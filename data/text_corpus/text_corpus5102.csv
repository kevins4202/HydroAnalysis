index,text
25510,earth observation eo technologies have played an increasingly important role in monitoring the sustainable development goals sdg these technologies often combined with machine learning ml models provide efficient means for achieving the sdgs the great progress of this combination is also demonstrated by the large number of software web tools and packages that have been made available for free use in this paper we introduce a software architecture to facilitate the generation of eo information targeted towards soil moisture that derive several challenges regarding the facilitation of satellite data processing thus this paper presents a web based tool for soil moisture estimation smetool designed for the soil moisture estimation using sentinel 1a and sentinel 2a data based on eo learn library smetool implements several ml techniques such as artificial neural network ann random forest rf convolutional neural network cnn etc the smetool could be very useful for decision makers in the region in assessing the effects of drought and desertification events experiments were carried out on two sites in tunisia during the period from 2016 to 2017 although the performance of the used models is very close it is clear that cnn and rf outperformed other ml models the achieved results reveal that the soil moisture was highly correlated to the in situ measurements with high pearson s correlation coefficient r r rf 0 86 r ann 0 75 r xgboost 0 79 r cnn 0 87 and low root mean square error rmse rmse rf 1 09 rmse ann 1 49 rmse xgboost 1 39 rmse cnn 1 12 respectively keywords soil moisture estimation open source data web based tool eo learn machine learning sentinel 1a and 2a data availability no data was used for the research described in the article 1 introduction soil moisture is an important indicator used in hydrological and agriculture applications petropoulos et al 2015 moradizadeh et al 2022 in agriculture soil moisture deficit directly leads to crop water stress therefore it is important to estimate soil moisture accurately during the crop main growth steps zhuang et al 2022 moreover it plays a key role in improving the overall efficiency and productivity of agriculture with the increasing development of remote sensing technology soil moisture estimation studies become more interesting essid et al 2012 a variety of microwave both active and passive sensors platforms have been recently employed for soil moisture retrieval jarray et al 2022 ben abbes and jarray 2022 in this context the launching of the constellation of synthetic aperture radar sar satellites sentinel 1 a since april 2014 and sentinel 2 a since june 2015 offers images with high spatial resolution up to 10 m and a high temporal resolution 6 12 days for sentinel 1 and 5 days for sentinel 2 el hajj et al 2017 it represents a major advance for the development of operational soil moisture estimation in addition sentinel data is free and open access recent literature studies demonstrated that sentinel 1 a and sentinel 2 a data can be used complementary to soil moisture estimation bousbih et al 2018 santi et al 2018 hachani et al 2019 foucras et al 2020 zribi et al 2019 jarray et al 2021 the advantage of the integration of satellite data fusion presents unprecedented opportunities for large scale soil moisture monitoring as sentinel 1 a and sentinel 2 a provide complementary information to estimate the soil moisture the latter retrieval tasks can take advantage of fusing both products leading usually to improve the estimation accuracy foucras et al 2020 zribi et al 2019 nonetheless the common problem faced by these models in practice is the lack of data to train machine learning ml models and the cost and expense of in situ collection various studies demonstrated the potential of retrieving soil moisture from sentinel 1 a and sentinel 2 a data bousbih et al 2018 santi et al 2018 hachani et al 2019 for example attarzadeh et al 2018 presented an approach for soil moisture retrieval by coupling sentinel 1 a and sentinel 2 a data at the plot scale in vegetated areas using support vector regression svr technique the results show that these approach in terms of estimation accuracy from 4 94 to 6 41 of root mean square error rmse and from 0 62 to 0 89 of coefficient of determination r 2 gangat et al 2020 estimated the soil moisture based on the random forest rf and support vector machine svm using sentinel 1 a and sentinel 2 a data the rf leading to an r 2 from 0 58 to 0 86 and rmse is lower than 18 whereas datta et al 2021 applied different ml and linear regression models to estimate the soil moisture using field observed soil moisture and sentinel 1 a data the input data used are the backscatter σ values in vv and vh band from sar data normalized difference vegetation index ndvi and normalized difference water index ndwi extracted from sentinel 2 a data the rf model was observed as the best performing ml model for soil moisture estimation r 2 0 87 and 0 93 during modeling and validation respectively rmse of 0 03 m 3 m 3 el hajj et al 2017 used the synergy of sentinel 1 a and sentinel 2 a data to develop a soil moisture retrieval technology based on water cloud model wcm and artificial neural network ann the input data for these models include the backscatter coefficient in vv polarization and ndvi this work showed promising results by wcm ann to estimate soil moisture the obtained results showed that vv alone gives better accuracy on the soil moisture retrieval than only vh meanwhile the use of both vv and vh furnish similar results comparable to vv the empirical semi empirical and physical models are widely used to estimate soil moisture they are highly linked to the specificity of the studied site it covers all scenarios of soil soil moisture soil roughness etc despite this the main weakness of physical models for soil moisture estimation especially over large areas consists in the fact that they cannot describe precisely the natural variability of the soil the physical behavior despite the availability of in situ samplings however ml approaches employ an efficient tool to solve the estimation issues based on data analysis they also handle the data with a limited number of assumptions about the physical behavior of the system the processing of earth observation eo data is complex and requires expertise in data storage organization processing and visualization rhif et al 2021 balti et al 2022 although the implementation of new technologies is essential for the non expert to remain competitive meanwhile producing useful information from eo data and using the processing capabilities of the platforms is not an easy task due to the advancement of novel technologies web based processing and analysis workflows are becoming web based to replace the download based approaches the recent increasing use of satellite image data has led to higher hardware requirements has allowed generating valuable information from eo data as a result the traditional processing workflows based on downloading and local processing of satellite image data have become quite inefficient in the last few years web based tools based solutions have been increasingly employed in a wide range of applications and domains especially involving the agricultural sector their main purpose is to help non experts visualize and analyze a large amount of data e g eo climate in situ etc in instance moderate resolution imaging spectroradiometer modis vegetation indices analysis esquerdo et al 2020 irrigation decision support system simionesei et al 2020 agro climatic index s analysis klein et al 2017 soil databases kim et al 2018 in spatial analysis phillips et al 2014 water resources monitoring khattar et al 2020 and hydrologic forecasts ashby et al 2021 in fact for soil moisture estimation zhang et al 2022 yang et al 2021 introduced the crop condition and soil moisture analytics tool crop casma which is a web based software employed to approximate the soil moisture at 1 and 9 km spatial resolution every 1 3 days using soil moisture active passive smap and modis in this context this paper presents a web based tool for soil moisture estimation smetool in order to automate the processing chain for soil moisture estimation smetool is an automated processing pipeline based on eo learn and ml techniques eo learn is an open source library developed in the python programming language eo learn library permits to rapidly and automatically build workflows in order to access process and extract the information from eo data eo learn it has been applied in several studies for instance peternelj et al 2020 compared the incremental learning algorithms to the traditional learning ones using eo data based on eo learn library the input data included sentinel 2 a images digital elevation model and in situ measurements the aim of this work was to provide methods and workflows that reduce the need for extensive hardware and processing power relying on the eo learn library this framework was evaluated in four case studies considering monitoring water bodies mahajan and martinez 2021 land cover classification koprivec et al 2019 račič et al 2020 to the best of our knowledge this is the first time that a web tool was developed for soil moisture retrieval through the eo learn library and ml methods using eo data the rest of this paper is as follows section 2 presents the study areas data and the structure of the smetool section 3 describes the smetool implementation for soil moisture estimation in southern tunisia finally section 4 presents some concluding remarks 2 materials and methods 2 1 materials 2 1 1 study area the governorate of medenine is located in the south eastern region of tunisia fig 1 its climate is typically mediterranean characterized by a long dry season with mild winter and hot summer the average annual rainfall does not exceed 200 mm the average temperature ranges from 21 c to 24 c two rainfed olive orchards within medenine governorate chammakh and dardhaoui were selected for soil moisture measurements fig 1 the first one is called chammakh it extends from 33 28 55 2 n to 33 36 46 8 n and 10 56 60 e to 11 3 7 2 e and covers 140 km2 the second one is dardhaoui extends from 33 14 16 8 n to 33 21 14 4 n and 10 42 10 8 e to 10 3 27 6 e and covers 204 km2 outside the olive groves the natural vegetation cover is made mainly of perennial species that are often found in the form of bushes and sometimes in the form of more or less developed shrubs see table 1 2 1 2 data a remote sensing data sentinel 1 a data was acquired in grd ground range detected format in iw imaging mode with the vv and vh polarization including 3 days at spatial resolution of 10 m hachani et al 2019 in addition sentinel 2 a data were obtained in level 1c l1c it produces one image every 5 days they are acquired in 13 spectral bands at three different spatial resolutions 10 20 and 60 m ambrosone et al 2020 these data are at the top of the atmosphere reflectance products after orthographic correction and geometric accuracy correction of the subpixel level indeed atmospheric correction is required to obtain the surface reflectance of each band the smetool provides the σ v v and σ v h from sentinel 1 a imagery available and normalized difference vegetation index ndvi computed from the sentinel 2 a imagery available with a resolution of 10 m b in situ data field campaigns have been conducted in the selected rainfed olive orchards the in situ data have been acquired at the same time as the sentinel 1 a and the closest for sentinel 2 a acquisitions in 32 test fields for each site presented by green dots in fig 1 with a depth of 0 10 cm within 6 m radius around 4 selected olive trees the in situ measurements of soil moisture were collected within 64 days from 2016 to 2017 the gravimetric soil moisture was determined using the drying process in the laboratory aniley et al 2018 in these case of the arid regions the sm values ranged between 0 1 and 12 some characteristics of the sm monitoring sites are reported in table 2 2 2 methods 2 2 1 architecture of smetool the smetool flowchart architecture is shown in fig 2 it is composed of two parts back end and front end the first is composed of the web interface by which the user interacts with a map display on the other hand the latter includes a postgis database formed after a set of procedures of data acquisition and processing applied to satellite images using on eo learn library and ml methods the development tools utilize mainly free software basically the user accesses the application through an interface and employs the map viewer to select any location and visualize the ndvi σ v v and σ v h time series with a chart the code and documentation are accessible on github https github com jarray01 smetool the main steps of the smetool system are illustrated below step 1 in the first step the login and password are chosen then the user selects the geographical coordinates of the region of interest the start and end dates via a dynamic web interface step 2 secondly the smetool system downloads automatically sentinel images preprocesses the sentinel data and extracts the features ndvi σ v v and σ v h using eopatch and eotask respectively eopatch is a data object that processes multi source and multi temporal images from a region of interest it includes unlimited amount of data to download and store however in eotask which is an instantiation of the eopatch object features extraction is performed on an existing eventually the indices are extracted subsequently they are used to training and validate the ml models with the in situ data the commonly used functions in eopatch and eotask are s eo learn 2021 eo learn coregistration a package dealing with the combined registration of images eo learn features a set of tools to collect and process the data properties eo learn geometry geometry package used to transform vector and raster data eo learn io i o utilized to obtain data from the sentinel hub services or local persistence and charging the data eo learn mask is the package used for data masking and cloud mask computation eo learn ml tools are used before or after the ml process eo learn visualization refers to visualization tools for the core elements of eo learn the inputs of these packages are a path followed to downloaded sentinel 1 a and sentinel 2 a images these images are processed one by one for each eopatch step 3 smetool estimates the soil moisture values using sentinel data with ml models step 4 the smetool automatically saves the estimated soil moisture data applying the different models in a geodatabase step 5 the web services that communicate with all platforms web mobile etc are introduced to visualize the soil moisture data in javascript object notation json format in this last step the user visualizes the soil moisture maps fig 3 shows a schematic overview of the data flow between the back end and the front end the requests sent and received by the user establish a link with several processes that are executed in the background without the intervention of the user steps 1 and 5 in fig 3 the services executed in the background are the operations applied to process the satellite images preprocessing the ml training models and the storage data in a geospatial database steps 2 3 and 4 depicted in fig 3 for example the requests from the client is the estimation of soil moisture in a selected region of interest a web tool furnishes functionalities and modules for asynchronous request handling and information delivery to the client eventually the result i e a vector of soil moisture information for a specified period is sent back to the user in the form of json data interchange format where it can be visualized in a map 2 2 2 system structure the smetool service is composed of six main components illustrated in fig 4 these components are presented below a remote sensing data module using this module the remote sensing images are acquired and processed the acquisition of the images is performed using eopatch in addition the radiometric calibration speckle filter terrain correction and backscattering coefficients were converted to the logarithm db scale to obtain geo referenced and calibrated backscattering values σ in two modes of polarization are applied to sentinel 1 a data using eopatch besides for sentinel 2 a data the atmospheric correction and resampling to resolution 10 m are applied using eotask we extract the σ v v and σ v h are extracted from sentinel 1 a images and ndvi from sentinel 2 a images b machine learning module the ml module is composed of four ml models rf extreme gradient boosting xgboost ann and convolution neural networks cnn the input data in each model are σ v v and σ v h and ndvi while the output of each model is soil moisture values the different used models are detailed as follows rf rf has been shown to be effective in a wide range of classification and regression problems carranza et al 2021 they consist of a set of binary trees each trained on random subsets of the data the randomness of the training process promotes the trees to generate independent estimates which can be merged to produce an accurate and robust result xgboost the framework uses xgboost as one of the key algorithms in the soil moisture estimation jia et al 2019 it has been widely employed thanks to its high problem solving performance it is easily applied in small data sets ann ann has been commonly utilized in many fields and has also become popular in soil moisture estimation hachani et al 2019 based on the concept of biological neurons we can classify the layers of ann into three types the input layer the hidden layer and the output layer to obtain the regression relationship between input and output variables cnn the fundamental structure of the cnn is made up of three steps convolution activation and pooling yu et al 2021 in convolution a kernel is used to obtain the features of an image afterward the activation function is executed to make a nonlinear mapping in the third step the extracted features are minimized which reduces overfitting the output of the cnn is usually treated as the input of a fully connected neural network c in situ measurements module in order to validate the ml models the in situ measurements were imported in the performed experiments into the geospatial database the in situ soil moisture measurements were carried at 66 different days at the same time as the radar acquisitions for each field the measurements of soil moisture were made at a depth of 10 cm the in situ measurement data were stored in the database together with the dates and their geographical coordinates d performance analysis module the performance of the different models was evaluated by using the correlation coefficient r root mean square error rmse and bias as well as by applying the following equations 1 r m s e i 1 n y i y i 2 n r is calculated as 2 r i 1 n y i y i y i y i i 1 n y i y i 2 y i y i 2 and bias is calculated as 3 b i a s i 1 n y i y i n where y i is the measured value of time i y i is the predicted value of time i obtained from a particular model and n is the number of sample data e geo database module smetool includes a relational database for data repository developed for the postgresql 13 0 https www postgresql org by adding its postgis extension for spatial data processing the database was divided into 9 tables and organized into two different groups as shown in fig 5 the first group is composed of 2 simple tables user and roles containing personal information of different types of users supported by the smetool user stores the usernames username names name first name last name contact information email phone and passwords password of users registered in the system the users table also contains a series of attributes used to manage their registration and the traceability of each connection finally the table further allows the users activation deactivation role lists the roles names and their description name description and their identification code id role their user identification code id user the second group covers six spatial tables study shape sm σ v v σ v h ndvi describing the general options available for process soil moisture estimation identified by their spatial connection to each other study shape characterizes the study area their identification code id study as well as the longitude and latitude of each pixel sm characterizes the soil moisture estimated using ml methods their identification code id sm as well as the longitude and latitude for each pixel and the value of soil moisture value σ v v characterizes the backscatter coefficient in mode polarization vv extracted from sentinel 1 a images the identification code id σ v v of the tables the longitude and latitude of each pixel and value of id σ v v value σ v h characterizes the backscatter coefficient in mode polarization vh extracted from sentinel 1 a images their identification code id σ v h longitude latitude and the value of σ v h value ndvi characterizes the ndvi extracted from sentinel 2 a images their identification code id ndvi longitude latitude and the value of ndvi value f visualization module the smetool is a web interface to visualize soil moisture maps in fact the access is free of charge through a registration process where the user provides basic information such as name email address username and password besides the user can visualize the soil moisture estimation results using ml models applied on geo referenced maps 3 experiments and results 3 1 experimental settings the performed consists of two phases the learning phase from 1 january to 31 december 2016 and the testing phase from 1 january to 31 december 2017 in terms of percentage around 70 of the data were used to train the model while the remaining 30 were employed to test the model in summary 2112 samples were utilized in the performed experiments in the training phase 1600 samples were used while the other samples were utilized in the testing phase to summarize we collect a total of 18 images 1700 1200 pixels and 16 images 1400 1000 pixels data for the training phase the rest ones about 18 images 1700 1200 pixels and 14 images 1400 1000 pixels are used in the testing phase in the training phase automatic preprocessing using the eo learn library was first conducted then hyperparameters were selected after this soil moisture estimates were obtained by applying several methods whose performance were comparable to that of the in situ measurements in the testing phase the trained models provided in the learning phases were applied to the test data subsequently the retrieval quality of several models was evaluated by comparing it with that of both in situ measurements 3 2 hyper parameters selection a hyperparameter is a parameter that sets a value before a learning process it is not the parameter obtained in the learning process table 3 shows the grid search of hyperparameters for all the methods the bold font denotes the best setting of each hyperparameter we used the grid search to find the best hyperparameter values to get the perfect estimation results from each model based on the grid search we used a different combination of all the hyperparameters then we calculated the performance for each combination and selected the best value for the hyperparameters which gives an efficient method to choose the best hyperparameters the cnn was implemented using tensorflow then the initial learning rate for the adam algorithm was set to 10 4 and the size of the batch was equal to 64 the parameters in the cnn were optimized by applying adam algorithm they used re lu activation dropout of probability 0 5 and training for 100 epochs cnn was used on the validation set to tune hyperparameters the cnn architecture consists of five convolutional blocks with relu activation at the end of the max pooling at the end of each convolutional block the number of filters is 64 128 256 512 and 512 respectively the linear activation function was applied adam is adopted as the optimizer in the experiments for the ann model three hidden layers were used with 32 32 and 16 neurones respectively the linear activation function was applied as the activation function in the output layer for the xgboost model the hyperparameters max depth 30 learning rate 101 n estimators 100 were utilized finally in the rfr model the rf was implemented with 100 decision trees using scikit learn with a maximum depth of 30 3 3 smetool web interfaces the aim of this paper is to develop and implement a web tool whose components can be interchanged in order to produce a generic pipeline for soil moisture estimation to help non experts in decision making it introduces a simple interface with strong geospatial capabilities such as zooming and polygon drawing in the smetool the user can i have connection to the smetool ii specify the input data select input data ii visualize data visualization map and chart iii manage ml models models iv access study areas study area and manage users users the menu is presented on the left of fig 6 the advantages of this interface consist in 1 allowing non expert users to utilize the tool and estimate soil moisture 2 providing an overview of soil moisture estimation output variables by the results page and 3 making the users able to visualize the results from the soil moisture map and chart outputs from the smetool the 1 0 version of the smetool web interface was proposed in fact access is available through a registration process where the user gives basic information such as name email address and password according to their registration the smetool home interface as presented in fig 6 is divided into three parts 1 a set of tools available on a control panel 2 a small dashboard on the data stored in the database and 3 a map area in the latter the user can select a region of interest anywhere by drawing the polygon on the map layer the smetool input selection interface is shown in fig 7 the user must define the box of the region of interest with four geographic coordinates presented in the selecting data box he also chooses the time interval start and end date when he sends his search request provided in the map box however the smetool downloads preprocesses extracts features and trains ml models from the sentinel images the smetool results which naturally includes all previous information can be further accessed online fig 8 the chart area in fig 8 shows the time series soil moisture of the selected location considering the selected pixel by the user besides the chart is drawn considering all the available dates in the database moreover the non experts access the soil moisture estimation map of each region of interest selected using different ml models first in the select filter data box the user has to specify the used name of the ml model and the date of the soil moisture retrieval then it has to draw a polygon of the area to search the estimated soil moisture information when it sends the search request a list of available maps is loaded in the map box and it can display moisture data on the maps in parallel with their drawn polygon via the small map icon presented in the last table column 3 4 web tools the web tools functionalities were implemented using the python framework in combination with the apache tomcat web server tomcat 2021 integrated into spring boot framework spring boot 2021 as the backend and the leaflet library leaflet js 2021 as the web gis interface on the client side moreover the visualization of the soil moisture modeling results was realized by the provided functions of the leaflet library the geospatial data was organized by the postgresql postgresql 2021 database with the postgis postgis 2021 plugin 3 5 the smetool estimation python library fig 9 shows a code snippet to illustrate the downloading preprocessing and features extraction of sentinel 1 a images from a sentinel hub instance the downloading parameters were used in order to identify the sentinel images fig 10 presents a code snippet to describe the usage of sentinel 2 a images to download preprocess and extract features from a sentinel hub instance 3 6 soil moisture estimation results to assess the performance of used models we compare the r rmse and bias between observed and estimated soil moisture an r ranging from 0 75 to 0 87 and an bias ranging from 0 010 to 0 020 was obtained the rmse ranged from 1 09 to 1 49 the performances of training and tests are shown in table 4 similarly a scatter plot between the estimated soil moisture values and in situ ones is presented in fig 11 this result was consistent with previous scientific publications bousbih et al 2018 rabiei et al 2021 liu et al 2021 in summary rf xgboost and cnn provided similar retrieval results as compared to in situ soil moisture data comparable result is reported by rabiei et al 2021 after validating the used models a series of soil moisture maps of the two study areas was generated the experiments were conducted during two seasons winter and the summer as demonstrated in fig 12 figs 12 a 12 b 12 c and 12 d presents the soil moisture maps obtained for dardhaoui by combining sentinel 1 a and sentinel 2 a images acquired on 18 september 2017 during which the climate is dry most of the soil moisture values vary between 0 and 8 due to the aridity of the studied region however in figs 12 e 12 f 12 g and 12 h reveal that the soil moisture maps for chammakh in the wet date was derived from the combination of sentinel 1 a and sentinel 2 a images acquired on 12 january 2017 on this date the value of soil moisture reached 12 4 discussion and conclusions in this paper we presented smetool which is a web based tool employed to monitor and estimate soil moisture utilizing sentinel data in arid regions it helps non experts to manage and visualize relevant information about the soil moisture this tool also allows estimating soil moisture using the eo learn framework and ml methods the back end architecture was implemented through an open source relational geospatial database to store the complete time series of sentinel 1 a and sentinel 2 a images the front end was developed to provide the user with a conviviality interface including a set of data filtering procedures applied to improve the visualization of the estimated soil moisture this system can be efficiently utilized in the agricultural sector for internal validity smetool was evaluated using sentinel data in arid regions obviously it provided acceptable results in addition it should be noted that the smetool is adaptable to many types and sizes of data providing high elasticity so that it may be used in different scenarios meanwhile for external validity experimentation was carried out in a small arid area despite the good obtained results the smetool suffers from some limitations in terms of development and testing to generalize smetool other case studies should be conducted to better evaluate all the steps of smetool moreover in order to avoid the lack of training data more ml strategies e g transfer learning and meta learning are required jarray et al 2022 ben abbes and jarray 2022 the future development plans for smetool include i integrating new data sources and formats climatic yield production etc ii connecting to the international soil moisture datasets and iii adding new time series forecasting tools software availability interested users can download the source code from github https github com jarray01 smetool and install it on their own computers the source code was developed by spring boot java and thymeleaf and was made available under the mit license eo learn is made available under the mit license declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25510,earth observation eo technologies have played an increasingly important role in monitoring the sustainable development goals sdg these technologies often combined with machine learning ml models provide efficient means for achieving the sdgs the great progress of this combination is also demonstrated by the large number of software web tools and packages that have been made available for free use in this paper we introduce a software architecture to facilitate the generation of eo information targeted towards soil moisture that derive several challenges regarding the facilitation of satellite data processing thus this paper presents a web based tool for soil moisture estimation smetool designed for the soil moisture estimation using sentinel 1a and sentinel 2a data based on eo learn library smetool implements several ml techniques such as artificial neural network ann random forest rf convolutional neural network cnn etc the smetool could be very useful for decision makers in the region in assessing the effects of drought and desertification events experiments were carried out on two sites in tunisia during the period from 2016 to 2017 although the performance of the used models is very close it is clear that cnn and rf outperformed other ml models the achieved results reveal that the soil moisture was highly correlated to the in situ measurements with high pearson s correlation coefficient r r rf 0 86 r ann 0 75 r xgboost 0 79 r cnn 0 87 and low root mean square error rmse rmse rf 1 09 rmse ann 1 49 rmse xgboost 1 39 rmse cnn 1 12 respectively keywords soil moisture estimation open source data web based tool eo learn machine learning sentinel 1a and 2a data availability no data was used for the research described in the article 1 introduction soil moisture is an important indicator used in hydrological and agriculture applications petropoulos et al 2015 moradizadeh et al 2022 in agriculture soil moisture deficit directly leads to crop water stress therefore it is important to estimate soil moisture accurately during the crop main growth steps zhuang et al 2022 moreover it plays a key role in improving the overall efficiency and productivity of agriculture with the increasing development of remote sensing technology soil moisture estimation studies become more interesting essid et al 2012 a variety of microwave both active and passive sensors platforms have been recently employed for soil moisture retrieval jarray et al 2022 ben abbes and jarray 2022 in this context the launching of the constellation of synthetic aperture radar sar satellites sentinel 1 a since april 2014 and sentinel 2 a since june 2015 offers images with high spatial resolution up to 10 m and a high temporal resolution 6 12 days for sentinel 1 and 5 days for sentinel 2 el hajj et al 2017 it represents a major advance for the development of operational soil moisture estimation in addition sentinel data is free and open access recent literature studies demonstrated that sentinel 1 a and sentinel 2 a data can be used complementary to soil moisture estimation bousbih et al 2018 santi et al 2018 hachani et al 2019 foucras et al 2020 zribi et al 2019 jarray et al 2021 the advantage of the integration of satellite data fusion presents unprecedented opportunities for large scale soil moisture monitoring as sentinel 1 a and sentinel 2 a provide complementary information to estimate the soil moisture the latter retrieval tasks can take advantage of fusing both products leading usually to improve the estimation accuracy foucras et al 2020 zribi et al 2019 nonetheless the common problem faced by these models in practice is the lack of data to train machine learning ml models and the cost and expense of in situ collection various studies demonstrated the potential of retrieving soil moisture from sentinel 1 a and sentinel 2 a data bousbih et al 2018 santi et al 2018 hachani et al 2019 for example attarzadeh et al 2018 presented an approach for soil moisture retrieval by coupling sentinel 1 a and sentinel 2 a data at the plot scale in vegetated areas using support vector regression svr technique the results show that these approach in terms of estimation accuracy from 4 94 to 6 41 of root mean square error rmse and from 0 62 to 0 89 of coefficient of determination r 2 gangat et al 2020 estimated the soil moisture based on the random forest rf and support vector machine svm using sentinel 1 a and sentinel 2 a data the rf leading to an r 2 from 0 58 to 0 86 and rmse is lower than 18 whereas datta et al 2021 applied different ml and linear regression models to estimate the soil moisture using field observed soil moisture and sentinel 1 a data the input data used are the backscatter σ values in vv and vh band from sar data normalized difference vegetation index ndvi and normalized difference water index ndwi extracted from sentinel 2 a data the rf model was observed as the best performing ml model for soil moisture estimation r 2 0 87 and 0 93 during modeling and validation respectively rmse of 0 03 m 3 m 3 el hajj et al 2017 used the synergy of sentinel 1 a and sentinel 2 a data to develop a soil moisture retrieval technology based on water cloud model wcm and artificial neural network ann the input data for these models include the backscatter coefficient in vv polarization and ndvi this work showed promising results by wcm ann to estimate soil moisture the obtained results showed that vv alone gives better accuracy on the soil moisture retrieval than only vh meanwhile the use of both vv and vh furnish similar results comparable to vv the empirical semi empirical and physical models are widely used to estimate soil moisture they are highly linked to the specificity of the studied site it covers all scenarios of soil soil moisture soil roughness etc despite this the main weakness of physical models for soil moisture estimation especially over large areas consists in the fact that they cannot describe precisely the natural variability of the soil the physical behavior despite the availability of in situ samplings however ml approaches employ an efficient tool to solve the estimation issues based on data analysis they also handle the data with a limited number of assumptions about the physical behavior of the system the processing of earth observation eo data is complex and requires expertise in data storage organization processing and visualization rhif et al 2021 balti et al 2022 although the implementation of new technologies is essential for the non expert to remain competitive meanwhile producing useful information from eo data and using the processing capabilities of the platforms is not an easy task due to the advancement of novel technologies web based processing and analysis workflows are becoming web based to replace the download based approaches the recent increasing use of satellite image data has led to higher hardware requirements has allowed generating valuable information from eo data as a result the traditional processing workflows based on downloading and local processing of satellite image data have become quite inefficient in the last few years web based tools based solutions have been increasingly employed in a wide range of applications and domains especially involving the agricultural sector their main purpose is to help non experts visualize and analyze a large amount of data e g eo climate in situ etc in instance moderate resolution imaging spectroradiometer modis vegetation indices analysis esquerdo et al 2020 irrigation decision support system simionesei et al 2020 agro climatic index s analysis klein et al 2017 soil databases kim et al 2018 in spatial analysis phillips et al 2014 water resources monitoring khattar et al 2020 and hydrologic forecasts ashby et al 2021 in fact for soil moisture estimation zhang et al 2022 yang et al 2021 introduced the crop condition and soil moisture analytics tool crop casma which is a web based software employed to approximate the soil moisture at 1 and 9 km spatial resolution every 1 3 days using soil moisture active passive smap and modis in this context this paper presents a web based tool for soil moisture estimation smetool in order to automate the processing chain for soil moisture estimation smetool is an automated processing pipeline based on eo learn and ml techniques eo learn is an open source library developed in the python programming language eo learn library permits to rapidly and automatically build workflows in order to access process and extract the information from eo data eo learn it has been applied in several studies for instance peternelj et al 2020 compared the incremental learning algorithms to the traditional learning ones using eo data based on eo learn library the input data included sentinel 2 a images digital elevation model and in situ measurements the aim of this work was to provide methods and workflows that reduce the need for extensive hardware and processing power relying on the eo learn library this framework was evaluated in four case studies considering monitoring water bodies mahajan and martinez 2021 land cover classification koprivec et al 2019 račič et al 2020 to the best of our knowledge this is the first time that a web tool was developed for soil moisture retrieval through the eo learn library and ml methods using eo data the rest of this paper is as follows section 2 presents the study areas data and the structure of the smetool section 3 describes the smetool implementation for soil moisture estimation in southern tunisia finally section 4 presents some concluding remarks 2 materials and methods 2 1 materials 2 1 1 study area the governorate of medenine is located in the south eastern region of tunisia fig 1 its climate is typically mediterranean characterized by a long dry season with mild winter and hot summer the average annual rainfall does not exceed 200 mm the average temperature ranges from 21 c to 24 c two rainfed olive orchards within medenine governorate chammakh and dardhaoui were selected for soil moisture measurements fig 1 the first one is called chammakh it extends from 33 28 55 2 n to 33 36 46 8 n and 10 56 60 e to 11 3 7 2 e and covers 140 km2 the second one is dardhaoui extends from 33 14 16 8 n to 33 21 14 4 n and 10 42 10 8 e to 10 3 27 6 e and covers 204 km2 outside the olive groves the natural vegetation cover is made mainly of perennial species that are often found in the form of bushes and sometimes in the form of more or less developed shrubs see table 1 2 1 2 data a remote sensing data sentinel 1 a data was acquired in grd ground range detected format in iw imaging mode with the vv and vh polarization including 3 days at spatial resolution of 10 m hachani et al 2019 in addition sentinel 2 a data were obtained in level 1c l1c it produces one image every 5 days they are acquired in 13 spectral bands at three different spatial resolutions 10 20 and 60 m ambrosone et al 2020 these data are at the top of the atmosphere reflectance products after orthographic correction and geometric accuracy correction of the subpixel level indeed atmospheric correction is required to obtain the surface reflectance of each band the smetool provides the σ v v and σ v h from sentinel 1 a imagery available and normalized difference vegetation index ndvi computed from the sentinel 2 a imagery available with a resolution of 10 m b in situ data field campaigns have been conducted in the selected rainfed olive orchards the in situ data have been acquired at the same time as the sentinel 1 a and the closest for sentinel 2 a acquisitions in 32 test fields for each site presented by green dots in fig 1 with a depth of 0 10 cm within 6 m radius around 4 selected olive trees the in situ measurements of soil moisture were collected within 64 days from 2016 to 2017 the gravimetric soil moisture was determined using the drying process in the laboratory aniley et al 2018 in these case of the arid regions the sm values ranged between 0 1 and 12 some characteristics of the sm monitoring sites are reported in table 2 2 2 methods 2 2 1 architecture of smetool the smetool flowchart architecture is shown in fig 2 it is composed of two parts back end and front end the first is composed of the web interface by which the user interacts with a map display on the other hand the latter includes a postgis database formed after a set of procedures of data acquisition and processing applied to satellite images using on eo learn library and ml methods the development tools utilize mainly free software basically the user accesses the application through an interface and employs the map viewer to select any location and visualize the ndvi σ v v and σ v h time series with a chart the code and documentation are accessible on github https github com jarray01 smetool the main steps of the smetool system are illustrated below step 1 in the first step the login and password are chosen then the user selects the geographical coordinates of the region of interest the start and end dates via a dynamic web interface step 2 secondly the smetool system downloads automatically sentinel images preprocesses the sentinel data and extracts the features ndvi σ v v and σ v h using eopatch and eotask respectively eopatch is a data object that processes multi source and multi temporal images from a region of interest it includes unlimited amount of data to download and store however in eotask which is an instantiation of the eopatch object features extraction is performed on an existing eventually the indices are extracted subsequently they are used to training and validate the ml models with the in situ data the commonly used functions in eopatch and eotask are s eo learn 2021 eo learn coregistration a package dealing with the combined registration of images eo learn features a set of tools to collect and process the data properties eo learn geometry geometry package used to transform vector and raster data eo learn io i o utilized to obtain data from the sentinel hub services or local persistence and charging the data eo learn mask is the package used for data masking and cloud mask computation eo learn ml tools are used before or after the ml process eo learn visualization refers to visualization tools for the core elements of eo learn the inputs of these packages are a path followed to downloaded sentinel 1 a and sentinel 2 a images these images are processed one by one for each eopatch step 3 smetool estimates the soil moisture values using sentinel data with ml models step 4 the smetool automatically saves the estimated soil moisture data applying the different models in a geodatabase step 5 the web services that communicate with all platforms web mobile etc are introduced to visualize the soil moisture data in javascript object notation json format in this last step the user visualizes the soil moisture maps fig 3 shows a schematic overview of the data flow between the back end and the front end the requests sent and received by the user establish a link with several processes that are executed in the background without the intervention of the user steps 1 and 5 in fig 3 the services executed in the background are the operations applied to process the satellite images preprocessing the ml training models and the storage data in a geospatial database steps 2 3 and 4 depicted in fig 3 for example the requests from the client is the estimation of soil moisture in a selected region of interest a web tool furnishes functionalities and modules for asynchronous request handling and information delivery to the client eventually the result i e a vector of soil moisture information for a specified period is sent back to the user in the form of json data interchange format where it can be visualized in a map 2 2 2 system structure the smetool service is composed of six main components illustrated in fig 4 these components are presented below a remote sensing data module using this module the remote sensing images are acquired and processed the acquisition of the images is performed using eopatch in addition the radiometric calibration speckle filter terrain correction and backscattering coefficients were converted to the logarithm db scale to obtain geo referenced and calibrated backscattering values σ in two modes of polarization are applied to sentinel 1 a data using eopatch besides for sentinel 2 a data the atmospheric correction and resampling to resolution 10 m are applied using eotask we extract the σ v v and σ v h are extracted from sentinel 1 a images and ndvi from sentinel 2 a images b machine learning module the ml module is composed of four ml models rf extreme gradient boosting xgboost ann and convolution neural networks cnn the input data in each model are σ v v and σ v h and ndvi while the output of each model is soil moisture values the different used models are detailed as follows rf rf has been shown to be effective in a wide range of classification and regression problems carranza et al 2021 they consist of a set of binary trees each trained on random subsets of the data the randomness of the training process promotes the trees to generate independent estimates which can be merged to produce an accurate and robust result xgboost the framework uses xgboost as one of the key algorithms in the soil moisture estimation jia et al 2019 it has been widely employed thanks to its high problem solving performance it is easily applied in small data sets ann ann has been commonly utilized in many fields and has also become popular in soil moisture estimation hachani et al 2019 based on the concept of biological neurons we can classify the layers of ann into three types the input layer the hidden layer and the output layer to obtain the regression relationship between input and output variables cnn the fundamental structure of the cnn is made up of three steps convolution activation and pooling yu et al 2021 in convolution a kernel is used to obtain the features of an image afterward the activation function is executed to make a nonlinear mapping in the third step the extracted features are minimized which reduces overfitting the output of the cnn is usually treated as the input of a fully connected neural network c in situ measurements module in order to validate the ml models the in situ measurements were imported in the performed experiments into the geospatial database the in situ soil moisture measurements were carried at 66 different days at the same time as the radar acquisitions for each field the measurements of soil moisture were made at a depth of 10 cm the in situ measurement data were stored in the database together with the dates and their geographical coordinates d performance analysis module the performance of the different models was evaluated by using the correlation coefficient r root mean square error rmse and bias as well as by applying the following equations 1 r m s e i 1 n y i y i 2 n r is calculated as 2 r i 1 n y i y i y i y i i 1 n y i y i 2 y i y i 2 and bias is calculated as 3 b i a s i 1 n y i y i n where y i is the measured value of time i y i is the predicted value of time i obtained from a particular model and n is the number of sample data e geo database module smetool includes a relational database for data repository developed for the postgresql 13 0 https www postgresql org by adding its postgis extension for spatial data processing the database was divided into 9 tables and organized into two different groups as shown in fig 5 the first group is composed of 2 simple tables user and roles containing personal information of different types of users supported by the smetool user stores the usernames username names name first name last name contact information email phone and passwords password of users registered in the system the users table also contains a series of attributes used to manage their registration and the traceability of each connection finally the table further allows the users activation deactivation role lists the roles names and their description name description and their identification code id role their user identification code id user the second group covers six spatial tables study shape sm σ v v σ v h ndvi describing the general options available for process soil moisture estimation identified by their spatial connection to each other study shape characterizes the study area their identification code id study as well as the longitude and latitude of each pixel sm characterizes the soil moisture estimated using ml methods their identification code id sm as well as the longitude and latitude for each pixel and the value of soil moisture value σ v v characterizes the backscatter coefficient in mode polarization vv extracted from sentinel 1 a images the identification code id σ v v of the tables the longitude and latitude of each pixel and value of id σ v v value σ v h characterizes the backscatter coefficient in mode polarization vh extracted from sentinel 1 a images their identification code id σ v h longitude latitude and the value of σ v h value ndvi characterizes the ndvi extracted from sentinel 2 a images their identification code id ndvi longitude latitude and the value of ndvi value f visualization module the smetool is a web interface to visualize soil moisture maps in fact the access is free of charge through a registration process where the user provides basic information such as name email address username and password besides the user can visualize the soil moisture estimation results using ml models applied on geo referenced maps 3 experiments and results 3 1 experimental settings the performed consists of two phases the learning phase from 1 january to 31 december 2016 and the testing phase from 1 january to 31 december 2017 in terms of percentage around 70 of the data were used to train the model while the remaining 30 were employed to test the model in summary 2112 samples were utilized in the performed experiments in the training phase 1600 samples were used while the other samples were utilized in the testing phase to summarize we collect a total of 18 images 1700 1200 pixels and 16 images 1400 1000 pixels data for the training phase the rest ones about 18 images 1700 1200 pixels and 14 images 1400 1000 pixels are used in the testing phase in the training phase automatic preprocessing using the eo learn library was first conducted then hyperparameters were selected after this soil moisture estimates were obtained by applying several methods whose performance were comparable to that of the in situ measurements in the testing phase the trained models provided in the learning phases were applied to the test data subsequently the retrieval quality of several models was evaluated by comparing it with that of both in situ measurements 3 2 hyper parameters selection a hyperparameter is a parameter that sets a value before a learning process it is not the parameter obtained in the learning process table 3 shows the grid search of hyperparameters for all the methods the bold font denotes the best setting of each hyperparameter we used the grid search to find the best hyperparameter values to get the perfect estimation results from each model based on the grid search we used a different combination of all the hyperparameters then we calculated the performance for each combination and selected the best value for the hyperparameters which gives an efficient method to choose the best hyperparameters the cnn was implemented using tensorflow then the initial learning rate for the adam algorithm was set to 10 4 and the size of the batch was equal to 64 the parameters in the cnn were optimized by applying adam algorithm they used re lu activation dropout of probability 0 5 and training for 100 epochs cnn was used on the validation set to tune hyperparameters the cnn architecture consists of five convolutional blocks with relu activation at the end of the max pooling at the end of each convolutional block the number of filters is 64 128 256 512 and 512 respectively the linear activation function was applied adam is adopted as the optimizer in the experiments for the ann model three hidden layers were used with 32 32 and 16 neurones respectively the linear activation function was applied as the activation function in the output layer for the xgboost model the hyperparameters max depth 30 learning rate 101 n estimators 100 were utilized finally in the rfr model the rf was implemented with 100 decision trees using scikit learn with a maximum depth of 30 3 3 smetool web interfaces the aim of this paper is to develop and implement a web tool whose components can be interchanged in order to produce a generic pipeline for soil moisture estimation to help non experts in decision making it introduces a simple interface with strong geospatial capabilities such as zooming and polygon drawing in the smetool the user can i have connection to the smetool ii specify the input data select input data ii visualize data visualization map and chart iii manage ml models models iv access study areas study area and manage users users the menu is presented on the left of fig 6 the advantages of this interface consist in 1 allowing non expert users to utilize the tool and estimate soil moisture 2 providing an overview of soil moisture estimation output variables by the results page and 3 making the users able to visualize the results from the soil moisture map and chart outputs from the smetool the 1 0 version of the smetool web interface was proposed in fact access is available through a registration process where the user gives basic information such as name email address and password according to their registration the smetool home interface as presented in fig 6 is divided into three parts 1 a set of tools available on a control panel 2 a small dashboard on the data stored in the database and 3 a map area in the latter the user can select a region of interest anywhere by drawing the polygon on the map layer the smetool input selection interface is shown in fig 7 the user must define the box of the region of interest with four geographic coordinates presented in the selecting data box he also chooses the time interval start and end date when he sends his search request provided in the map box however the smetool downloads preprocesses extracts features and trains ml models from the sentinel images the smetool results which naturally includes all previous information can be further accessed online fig 8 the chart area in fig 8 shows the time series soil moisture of the selected location considering the selected pixel by the user besides the chart is drawn considering all the available dates in the database moreover the non experts access the soil moisture estimation map of each region of interest selected using different ml models first in the select filter data box the user has to specify the used name of the ml model and the date of the soil moisture retrieval then it has to draw a polygon of the area to search the estimated soil moisture information when it sends the search request a list of available maps is loaded in the map box and it can display moisture data on the maps in parallel with their drawn polygon via the small map icon presented in the last table column 3 4 web tools the web tools functionalities were implemented using the python framework in combination with the apache tomcat web server tomcat 2021 integrated into spring boot framework spring boot 2021 as the backend and the leaflet library leaflet js 2021 as the web gis interface on the client side moreover the visualization of the soil moisture modeling results was realized by the provided functions of the leaflet library the geospatial data was organized by the postgresql postgresql 2021 database with the postgis postgis 2021 plugin 3 5 the smetool estimation python library fig 9 shows a code snippet to illustrate the downloading preprocessing and features extraction of sentinel 1 a images from a sentinel hub instance the downloading parameters were used in order to identify the sentinel images fig 10 presents a code snippet to describe the usage of sentinel 2 a images to download preprocess and extract features from a sentinel hub instance 3 6 soil moisture estimation results to assess the performance of used models we compare the r rmse and bias between observed and estimated soil moisture an r ranging from 0 75 to 0 87 and an bias ranging from 0 010 to 0 020 was obtained the rmse ranged from 1 09 to 1 49 the performances of training and tests are shown in table 4 similarly a scatter plot between the estimated soil moisture values and in situ ones is presented in fig 11 this result was consistent with previous scientific publications bousbih et al 2018 rabiei et al 2021 liu et al 2021 in summary rf xgboost and cnn provided similar retrieval results as compared to in situ soil moisture data comparable result is reported by rabiei et al 2021 after validating the used models a series of soil moisture maps of the two study areas was generated the experiments were conducted during two seasons winter and the summer as demonstrated in fig 12 figs 12 a 12 b 12 c and 12 d presents the soil moisture maps obtained for dardhaoui by combining sentinel 1 a and sentinel 2 a images acquired on 18 september 2017 during which the climate is dry most of the soil moisture values vary between 0 and 8 due to the aridity of the studied region however in figs 12 e 12 f 12 g and 12 h reveal that the soil moisture maps for chammakh in the wet date was derived from the combination of sentinel 1 a and sentinel 2 a images acquired on 12 january 2017 on this date the value of soil moisture reached 12 4 discussion and conclusions in this paper we presented smetool which is a web based tool employed to monitor and estimate soil moisture utilizing sentinel data in arid regions it helps non experts to manage and visualize relevant information about the soil moisture this tool also allows estimating soil moisture using the eo learn framework and ml methods the back end architecture was implemented through an open source relational geospatial database to store the complete time series of sentinel 1 a and sentinel 2 a images the front end was developed to provide the user with a conviviality interface including a set of data filtering procedures applied to improve the visualization of the estimated soil moisture this system can be efficiently utilized in the agricultural sector for internal validity smetool was evaluated using sentinel data in arid regions obviously it provided acceptable results in addition it should be noted that the smetool is adaptable to many types and sizes of data providing high elasticity so that it may be used in different scenarios meanwhile for external validity experimentation was carried out in a small arid area despite the good obtained results the smetool suffers from some limitations in terms of development and testing to generalize smetool other case studies should be conducted to better evaluate all the steps of smetool moreover in order to avoid the lack of training data more ml strategies e g transfer learning and meta learning are required jarray et al 2022 ben abbes and jarray 2022 the future development plans for smetool include i integrating new data sources and formats climatic yield production etc ii connecting to the international soil moisture datasets and iii adding new time series forecasting tools software availability interested users can download the source code from github https github com jarray01 smetool and install it on their own computers the source code was developed by spring boot java and thymeleaf and was made available under the mit license eo learn is made available under the mit license declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25511,this paper introduces post mordm a decision support framework that augments many objective robust decision making mordm mordm often creates an intractable number of environmental management policies characterized by decision variable objective and robustness values this large number of policies inhibits decision support causing disagreements among decision makers post mordm addresses these challenges via the self organizing map som synthesizing mordm data as layers organized in a map like coordinate system it uses the som to cluster policies discover salient characteristics and assess cause effect relationships between decision maker choices i e decision variable values and performance objective and robustness values overall the goal of post mordm is to create a structured platform that encourages negotiation and compromise we demonstrate post mordm with a case study of two illustrative decision makers for reservoir operation policy in the colorado river basin usa post mordm helps communicate tradeoffs between storage and delivery objectives relate tradeoffs to shortage policies and identify mutually feasible policies graphical abstract image 1 keywords self organizing maps som decision support robustness tradeoff analysis colorado river basin multi objective evolutionary algorithm moea data availability all data and code is available at the github link included in the manuscript file 1 introduction decision making for coupled human environmental systems is a paramount challenge of the 21st century decision makers dms need to identify policy actions that are simultaneously equitable balance competing objectives and are robust to future uncertainty un general assembly 2015 committee to advise the u s global change research program et al 2021 ipcc 2021 simulation models are often used to support dms by quantifying their system s key performance outcomes and elucidating how performance outcomes relate to policy decisions and exogenous driving forces of system behavior however analysts and dms often disagree on the relationships between the driving forces and performance outcomes of their systems further the probability distributions of driving forces are unknown and or disagreement exists on how to weigh performance outcomes of alternative decision actions such decision problems are described as deeply uncertain lempert et al 2003 kwakkel and haasnoot 2019 when facing deep uncertainty implementing a policy can require negotiation between dms but reaching a compromise can be difficult because of foundational disagreements such as divergent framings of the problem wheeler et al 2018 lempert and turner 2020 different prioritization of performance outcomes smith et al 2019 or different tolerances of uncertainty related risk mcphail et al 2018 2021 many objective robust decision making mordm is a simulation based decision support framework that helps dms identify promising policy actions when faced with deep uncertainty kasprzyk et al 2013 mordm has been shown to be effective for various human environmental systems with applications such as municipal water supply portfolios herman et al 2014 2015 gold et al 2019 irrigation and groundwater sustainability li and kinzelbach 2020 and reservoir operation policies alexander 2018 quinn et al 2018 mordm produces three types of decision relevant information for dms to consider decision variable values objective values and robustness values fig 1 a in panel i a simulation model of the system is coupled with an optimization algorithm to generate a set of many policy alternatives each policy is defined by a vector of decision variable values yellow box and each policy has corresponding data on its performance objective values green box mordm then stress tests each policy in a robustness analysis panel ii at this stage uncertainty regarding driving forces of the system is explicitly considered by simulating each policy in many plausible future states of the world which densely sample the range of plausible future scenarios robustness metrics are used to quantify how well a policy performs in performance objectives across all the sow this produces the third type of decision relevant data blue box several challenges hinder dms from utilizing the decision relevant information produced from mordm to select one or a small subset of policies for potential implementation first interpreting the cause effect relationships between dms actions i e dv values and performance robustness tradeoffs is non trivial because the policy set often consists of hundreds of policies characterized by complex interactions between decision variables objectives and robustness metrics miller 1956 lecompte 1999 saaty and ozdemir 2003 herman et al 2014 alexander 2018 quinn et al 2018 wheeler et al 2018 smith et al 2019 moreover in environmental systems that provide services to diverse stakeholders dms may struggle to use all this data to overcome foundational disagreements such as different framings of the decision problem wheeler et al 2018 lempert and turner 2020 different weighing of performance objectives smith et al 2019 or different risk tolerances towards uncertain future conditions mcphail et al 2018 2021 hadjimichael et al 2020 multiple studies in the engineering design domain have highlighted the self organizing map som as a promising machine learning algorithm to alleviate the cognitive burden faced by dms obayashi and sasaki 2003 koishi and shida 2006 li et al 2009 mosnier et al 2013 zhang et al 2018 in this research we uniquely couple the som with mordm introducing an alternative paradigm for how the relationships between decision variables objectives and robustness are interpreted as an analogy consider fig 1b panel i shows four geospatial map layers of lake mead located just east of las vegas nevada each layer shows a different type of data satellite imagery hydrologic drainage network administrative boundaries and major road network but importantly the data is organized by latitude and longitude in other words every coordinate pair has multiple layers of corresponding data types moreover the geographic arrangement of map layers helps explain the relationships between data layers for example as you move westward from lake mead leftward on the longitude axis the number of administrative units and major roads increases which is because you are moving away from lake mead and into downtown las vegas the som creates analogous means for interpreting the mordm data as multiple data layers organized by a two dimensional coordinate system the som learns the many dimensional patterns of the policy set groups and arranges policies into a two dimensional coordinate system and establishes a map like visualization for elucidating the relationships between decision variables objectives and robustness in this paper we refer to performance objective values decision variable values and robustness values as layers in accordance with this paradigm our review of the som literature has shown two critical research gaps as it pertains to mordm and decision support applications obayashi and sasaki 2003 koishi and shida 2006 li et al 2009 mosnier et al 2013 zhang et al 2018 first som applications have not considered robustness a critical component of mordm instead being limited to the decision variable and performance objective layers second som applications have been implemented in the context of a single analyst design team or organization gaining important system understanding or selecting an engineering design such applications are fundamentally different than the policy or management decisions in an environmental system where the problem is often characterized by negotiation between multiple local state or federal governments ngos landowners and various other interest groups reclamation 2007 2012 wheeler et al 2018 molina perez et al 2019 to our knowledge the som has not been implemented in such a negotiation context building on previous som applications this paper introduces post mordm a framework that assists dms and analysts interpret visualize and negotiate large sets of policies post mordm augments mordm by using the som to a elucidate the relationships between decision variables objectives and robustness b reduce the number of alternatives dms need to consider and c establish a visual structured platform whereby dms with divergent performance priorities and risk tolerances are assisted in a process of negotiation towards compromise policies post mordm contributes to the som literature by expanding layer visualization to robustness and demonstrating how the construction and utilization of the som can be implemented in negotiation contexts for human environmental systems the remainder of the paper is organized as follows section 2 provides background on mordm then motivates our use of the som with a review of machine learning algorithms that have been used to post process mordm data section 3 describes the som algorithm and its fundamental benefits before outlining the post mordm framework section 4 demonstrates post mordm in a case study of reservoir operation policies in the colorado river basin usa the discussion and conclusion follow in sections 5 6 2 background and motivation 2 1 many objective robust decision making mordm many objective robust decision making mordm is a simulation assisted decision support framework that helps dms identify promising policies when faced with deep uncertainty kasprzyk et al 2013 the framework consists of four steps problem formulation policy generation robustness analysis and scenario discovery this section describes the steps and how they result in three mordm data layers decision variable values objective values and robustness values mordm is broadly applicable to policy analysis and environmental problems for clarity the examples we provide in this section focus on a specific familiar human environmental system namely a river system managed by water storage reservoirs first problem formulation defines the scope of the decision problem in terms of decision variables performance objectives and driving forces characterized by exogenous uncertainties lempert et al 2003 lempert and collins 2007 kasprzyk et al 2013 exogenous uncertainties are factors outside the explicit control of the dms like hydroclimatic factors e g temperature precipitation runoff and socioeconomic factors e g water demand irrigation efficiency uncertainty is characterized using what if scenarios called states of the world sow each sow is a multivariate sample of the uncertainty factors and an ensemble of sow is created to extensively sample each factor s plausible bounds decision variables dvs x x 1 x 2 x l represent policy actions where x can be continuous e g volume of water to release from a reservoir at a given time step or discrete e g augment water supply by either expanding a reservoir building a desalination plant or purchasing water rights performance objectives f f 1 f 2 f m are metrics that quantify how well the system performs such as reliability of meeting water demands or average hydropower production for each policy and sow a simulation model calculates the values of the performance objectives after problem formulation policy generation is performed using multi objective simulation based optimization commonly using multi objective evolutionary algorithms moeas the moea is coupled with the simulation model in a loop where the moea generates policies and the model evaluates performance objectives in this paper we use the term policies to refer to moea solutions i e vectors of dv values giuliani et al 2014 at this stage in mordm a small number of sow is used to force the simulations and calculate performance objective values often using sow that reflect values of uncertain factors observed in the historical record kasprzyk et al 2013 alexander 2018 the moea creates new policies over thousands of iterations of the simulation optimization loop using operators inspired by concepts of evolutionary theory like genetic crossover random mutations and survival of the fittest hadka and reed 2013 maier et al 2019 the output of the moea is a set of non dominated policies policy a dominates policy b if the performance of policy a is equal to or better than the performance of b in all objectives while being better than the performance of b in at least one objective in the resulting policy set no policy dominates any other policy i e they are non dominated in effect the policies exhibit performance tradeoffs where improving performance in one objective necessitates inferior performance in one or more other objectives as a result of the policy generation step each policy has two corresponding mordm data layers objective values and dv values dms use interactive visualizations to explore tradeoffs and interpret the relationships between dv and objective layers visualization types include glyph plots kollat and reed 2007 kasprzyk et al 2013 and parallel axis pa plots inselberg 2009 which are commonly used because of the relative ease with which greater than three dimensions are visualized using pa plots dms apply their preferences by changing the order of the axes filtering policies that meet performance goals and clustering policies with similar dv and or objective values inselberg 2009 raseman et al 2019 to elucidate the relationships between objective and dv layers previous studies have linked a pa plot of objectives to a pa plot of dvs smith et al 2018 raseman et al 2019 li and kinzelbach 2020 raseman et al 2020 for example smith et al hosted a workshop with colorado water managers to query how moea results could improve decision making in their respective agencies 2019 participants identified policies of interest according to performance objective preferences in one plot and the corresponding dv values were highlighted in another linked plot demonstrating the types of actions needed to achieve their performance preferences the objectives layer is calculated using a small number of sow although policies are non dominated with respect to the objectives layer it is possible their performance deteriorates when the assumptions about deeply uncertain factors are incorrect therefore policy generation is followed by robustness analysis where policy alternatives are stress tested by simulating them in the ensemble of sow defined in the problem formulation step robustness is the degree to which a policy s performance is insensitive to this broad sampling of sow kasprzyk et al 2013 herman et al 2015 mcphail et al 2018 performance sensitivity is quantified with robustness metrics which are statistics that describe how well a policy performs across its distribution of performance for specified objectives in the sow ensemble robustness metric values define the third mordm data layer there exist different robustness metrics which use varying transformations and statistical calculations across the sampled sow mcphail et al 2018 2021 these different robustness metrics reflect differing prioritization of objectives minimum performance thresholds and risk tolerances of dms mcphail et al 2018 2021 quinn et al 2018 gold et al 2019 hadjimichael et al 2020 examples of robustness metrics include the expected value of performance laplace s principle of insufficient reason regret from best possible performance regret from best the worst case performance maximin or the fraction of sow where a dms expressed performance criteria are achieved satisficing mcphail et al 2018 the previously described interactive visualization techniques can also be used to explore the tradeoffs between different robustness metrics and their relationships to dvs giuliani et al 2014 herman et al 2015 cohen and herman 2021 notably in a study of four connected water utilities herman et al demonstrate how a different policy is the most robust for each utility because they each define robustness based on their respective performance criteria in other words there exists interutility robustness tradeoffs and settling on a single policy would require the utilities to negotiate and compromise 2014 this example underscores the challenge of selecting a policy in a system characterized by deep uncertainty and multiple dms plus the potential benefits of a structured negotiation platform that facilitates negotiation and compromise in the last step of mordm policies are interrogated further via scenario discovery scenario discovery is a type of vulnerability analysis where vulnerability is defined by the violation of dm defined performance criteria scenario discovery identifies the uncertainty factors sampled in the sow ensemble that are the best predictors of when a policy will be vulnerable and the corresponding values of the factors in which vulnerability occurs traditionally scenario discovery is performed on a small subset of policies selected based on the results of policy generation and robustness analysis kasprzyk et al 2013 giuliani et al 2014 quinn et al 2018 in this paper we focus on using mordm data layers objective dv and robustness values to identify a small subset of policies these policies would subsequently be input to a separate scenario discovery process and or real world implementation the workflow in this paper does not include scenario discovery 2 2 challenges and gaps to mordm decision support in the selection of policies using mordm techniques dms will want to consider the dv objective and robustness layers however several challenges arise when using mordm data layers to select policies for instance the quantity of policy alternatives and the dimensionality of mordm data layers exceeds average human processing limitations we use dimension to mean the numbers of dvs objectives and robustness metrics in each mordm data layer for example published applications of mordm have exhibited hundreds of policies herman et al 2014 zeff et al 2014 quinn et al 2018 wheeler et al 2018 less than five to over 100 dvs herman et al 2014 quinn et al 2018 three to eight objectives alexander 2018 quinn et al 2018 wheeler et al 2018 and one to four or more robustness metrics herman et al 2014 however studies suggest that fewer than three to a maximum of nine alternatives be ideally examined at one time miller 1956 brill et al 1982 lecompte 1999 saaty and ozdemir 2003 moreover in the context of negotiation between dms large quantities of information can have negative consequences by increasing egocentric interpretations of what constitutes a fair resolution thus increasing the time needed to identify a compromise policy thompson and loewenstein 1992 tsay and bazerman 2009 these studies highlight the cognitive burden dms face when selecting policies from large many dimensional policy sets selecting policies also requires dms to understand the cause effect relationships between their actions i e dv values and the performance robustness tradeoffs of the system however when the policy set is very large and or the relationships between mordm data layers is complex elucidating such relationships is non trivial although interactive visualization methods like pa plots can be effective for linking dv values to objective values for one or a small subset of policies it remains a challenge to synthesize the relationships across the entire policy set because of the large number of dimensions in each mordm data layer complex and non linear interactions within the system smith et al 2018 hadjimichael et al 2020 noisy or low signal dvs objectives or robustness metrics smith et al 2019 and surprising effects of challenging sow on robustness values indeed the participants in smith et al described that training personnel in their respective organizations to understand moea results and communicating them to dms remains a challenge and the study concluded that structured information about the relationships between decision levers variables and performance objectives would be beneficial for interpreting tradeoffs 2019 this study demonstrated the difficulty of synthesizing relationships between dv and objective layers moreover the addition of the robustness layer which potentially includes multiple definitions of robustness would likely exacerbate this challenge another challenge to selecting policies is that dms and stakeholders involved in environmental decision problems may have foundational disagreements that are not easily overcome by only exploring the objective dv and robustness layers as described in the previous section dms may hold conflicting prioritization of objectives which combined with different degrees of risk tolerance can result in different definitions of robustness further dms may not agree on a single problem formulation instead using multiple problem formulations that reflect their respective world views quinn et al 2017 wheeler et al 2018 lempert and turner 2020 for dms with foundational disagreements to identify compromise policies mordm data layers need to be presented to dms in such a way that facilitates discussion negotiation and compromise 2 3 clustering and dimension reduction methods with mordm previous studies have reduced the number of alternatives dms need to consider and improved the interpretability of moea derived objectives and dv layers via two classes of statistical techniques namely clustering and dimension reduction clustering is a method to group data such that data within a cluster are similar and data in different clusters are more dissimilar hastie et al 2009 chap 14 3 clustering has previously been used to group policies with similar dv and or objective values kansara et al 2015 raseman et al 2020 in this approach dms consider a small number of clusters rather than hundreds of individual policies which can help reduce the cognitive load faced by dms to be within the range of practical human processing limitations discussed in the previous section for example raseman et al group moea derived water treatment plant policies using k means clustering summarizing the dv and objective layers with three clusters and a representative policy from each 2020 although clustering techniques can reduce the number of alternatives dms consider it does not elucidate the interactions between objectives or the relationship between objectives and dvs further to our knowledge clustering applications have not considered robustness to elucidate the interactions between objectives and dvs clustering can be complemented with dimension reduction methods dimension reduction is the process of either selecting a subset of the most important features of a data set feature selection or creating a low dimensional representation of the data by linear or non linear combinations of features feature extraction khalid et al 2014 hira and gillies 2015 ghojogh et al 2019 in both cases the goal of dimension reduction is to simplify a dataset meanwhile preserving the information contained within it by removing or combining noisy redundant or irrelevant features in moea applications dimension reduction methods have helped dms relate dv values to objective outcomes for example smith et al perform feature selection via multivariate regression trees to identify the dvs that are most influential on objectives meanwhile using a tree based visualization to intuitively guide decision makers towards dv values that achieve their desired objective outcomes 2019 kansara et al utilize principal component analysis a feature extraction method to reduce 12 objectives into a two dimensional summary that highlights the most important objectives and the correlation structures between them 2015 dimension reduction also helps reduce the cognitive burden faced by dms instead of interpreting the relationships between many dvs and objectives the relationships can be synthesized in a low dimensional summary that captures the most decision relevant information these studies demonstrate the benefits of clustering and dimension reduction techniques to enhance decision support efficacy with dv and objective layers and they motivate similar techniques to be expanded to the robustness layer 2 4 motivation for self organizing maps in post mordm in summary mordm based decision support for environmental systems is characterized by many policies three multi dimensional data layers dvs objectives and robustness and negotiation between dms to overcome foundational disagreements previous studies have demonstrated the utility of clustering and dimension reduction techniques applied to dv and objective layers revealing two research gaps for this study first the robustness layer should also be considered in clustering and dimension reduction applications second decision support via mordm data layers also needs to address the foundational disagreements between dms described above providing a structured visual and easily interpretable methodology that encourages the discussion and negotiation needed to identify compromise policies to address these gaps we introduce the post mordm framework which augments mordm via a novel implementation of the self organizing maps som the som is a type of artificial neural network wherein the neurons learn the cluster structure and feature space patterns of a multivariate data set kohonen 1982 we apply the som to mordm data layers providing the benefits of policy clustering and dimension reduction in a single algorithm kohonen 1990 clark et al 2020 moreover the som enables dms to simultaneously visualize dv objective and robustness layers with an intuitive map like visualization of the relationships within and between each layer the post mordm framework builds on previous som applications by expanding to robustness a decision relevant data layer for human environmental systems previous studies in the mechanical engineering domain have trained a som on an moea objectives layer then used the resulting som to visualize the inverse relationship to the dv layer example design problems include aircraft wings obayashi and sasaki 2003 automotive tires koishi and shida 2006 mosnier et al 2013 and switched reluctance machines zhang et al 2018 in contrast environmental decision problems often consider objectives dvs and robustness because they are characterized by deeply uncertain hydro climatic and socioeconomic factors herman et al 2014 2015 quinn et al 2018 gold et al 2019 li and kinzelbach 2020 therefore we build on these previous moea som studies by expanding the som to the robustness layer further the post mordm framework implements the som to guide dms through a process of policy negotiation and compromise in the published moea som studies the purpose has been to support a single analyst or organization in the choice of a design conversely environmental decision problems often involve multiple dms who reflect the interests of local state or federal government environmental ngos and various other interest groups reclamation 2007 2012 wheeler et al 2018 molina perez et al 2019 the post mordm framework uniquely implements the som as a discussion and negotiation platform for multiple dms using the som to navigate to compromise policies moreover we desire the som to reduce the number of alternatives that dms consider from several hundred policies to a reasonably small number of neurons in this context a neuron is a model that represents one or more policies similar to a cluster the size of the som i e the number of neurons is determined by the user previous som applications have used a som with several hundred neurons which is often determined by heuristics that calculate the number of neurons given the sample size of the user s data set kohonen 2001a chap 3 obayashi and sasaki 2003 koishi and shida 2006 mosnier et al 2013 clark et al 2020 in contrast the post mordm framework uses the smallest som possible to adequately represent the cluster structure of mordm data layers see section 3 1 1 the result is a som that summarizes hundreds of moea policies with a relatively small number of neurons to reduce the cognitive burden faced by dms 3 self organizing maps and the post mordm framework this section is organized as follows we define the som algorithm in section 3 1 then describe the essential attributes of som via an example with dv and objective layers we describe the post mordm framework in sections 3 2 3 5 3 1 self organizing maps som a som is a type of two dimensional artificial neural network used for feature extraction clustering and topologic visualization of multidimensional data sets with many samples or points kohonen 1990 2001a 2013 clark et al 2020 a som consists of an interconnected grid of neurons where a neuron is a prototype data point each neuron is defined by a vector of values one value for each feature of the data each neuron is parametrized with an integer based coordinate pair that identifies the location of the neuron within the grid and the topologic or neighborhood based relationships between neurons kohonen 2001a hastie et al 2009 chap 14 4 during the training process the neurons are iteratively updated to better represent the multi dimensional structure of the input data meanwhile maintaining their topologic relationships to each other within the grid after training the data points are projected onto the two dimensional grid resulting in a topology map that visualizes the data s cluster structure and most salient patterns clark et al 2020 in this paper we describe the steps of creating a som in two categories pre training setup and the neuron update function 3 1 1 pre training setup before training a som the user must normalize or scale the features of the input data so that differences in their magnitudes and variances do not bias the training process next the length width ratio of the som is calculated such that it represents the shape of the data to accomplish this the user sets the length width ratio equal to the ratio of the first and second eigenvalues of the data s correlation matrix kohonen 2001a clark et al 2020 in effect this process allocates proportionally more neurons to the som along the direction of the data s feature space with the most variance after establishing the length width ratio the user chooses the total number of neurons and several hyperparameter values both of which requires training multiple soms and evaluating quality of fit metrics first to establish a practical upper and lower limit on the number of neurons to test the user estimates the number of clusters that best represents the intrinsic cluster structure of the input data this process can be performed via the k means clustering elbow method which requires calculating a cluster quality metric such as the silhouette or davies bouldin index for 1 to kmax k means clusters the user then identifies k such that larger numbers of clusters exhibit sharply diminishing marginal improvement in cluster quality hastie et al 2009 chap 14 3 rendón 2011 xiao et al 2017 clark et al 2020 the user then tests multiple soms of different sizes ranging from a lower and upper limit centered around k where the limits are based on the computational and time constraints of the user second the user sets the hyperparameter values hyperparameters include neighborhood radius neighborhood function distance function and edge neuron behavior we describe these hyperparameters in appendix a1 the user can also decide between a rectangular or hexagonal grid structure but in this study we use a hexagonal topology because they tend to outperform rectangular grids both in terms of visualization and quality of fit kohonen 2001a clark et al 2020 the user selects the number of neurons and hyperparameter set based on the tradeoff between two fit metrics percent of variance explained pve captures the degree to which neuron prototype vectors represent the input data topographic error te measures the degree to which the mapping of input data onto the two dimensional map preserves the many dimensional data structure clark et al 2020 boelaert et al 2021 for equations and further descriptions of these metrics see appendix a2 pve and te conflict where pve improves and te worsens with an increasing number of neurons thus the user tests multiple soms with different map sizes and hyperparameter sets then makes a selection that balances the metrics once the number of neurons and the hyperparameter values are set the user initializes the neurons by uniformly aligning them along the plane formed by the first and second principal components of the data s feature space principal component one pc1 is the linear projection of the feature space along which the data varies the most and the direction along which the longer edge of the som is aligned principal component two pc2 is orthogonal uncorrelated to pc1 and indicates the second greatest mode by which the data varies hastie et al 2009 chap 14 5 james et al 2013 chap 10 2 in this paper all visualizations of the som will be oriented such that pc1 and pc2 are aligned with the horizontal and vertical directions respectively because som neurons are initialized along pc1 and pc2 the resulting topology map can be navigated interpreting movement along the topology map according to the contributions of each feature to the pcs for practical guidance on all pre training steps described above see the code included in appendix a5 3 1 2 som update function during som training the neurons are iteratively fit to the data points via an update function whereby individual neurons compete to win data points and neurons within neighborhoods cooperate to win data points the neighborhood of a neuron is defined to be all neurons within a user defined neighborhood radius measured in two dimensional map space at every iteration each data point is assigned to its best matching unit bmu which is the neuron closest to the data point measured in data space according to a distance function neurons compete to be the bmu of each data point while also cooperating with neurons within their neighborhood via an update function the update function awards neurons and their neighbors by moving them closer to the data points effectively the grid of neurons is bent twisted and stretched from its original position on the principal component plane to better represent the non linear patterns of the data hastie et al 2009 chap 14 4 clark et al 2020 we employ the batch version of the som update function because it converges faster than the stepwise recursive function and has no random component kohonen 2013 for further information on the batch update function see appendix a1 after training iterations are complete the final step in creating a som is assigning the data points to their bmu clark et al 2020 the assignment of data points to neurons is analogous to k means clustering where neurons are akin to cluster centroids hastie et al 2009 chap 13 2 james et al 2013 chap 10 3 raseman et al 2020 importantly som neurons are arranged based on their similarities because of the neighborhood based cooperation during training clark et al 2020 for practical assistance in creating a som several packages are supported in r wehrens and kruisselbrink 2019 boelaert et al 2021 python smith 2021 vettigli 2021 and matlab cluster with self organizing map neural network matlab simulink no date after creating the som the user visualizes it on a topology map which is created by plotting the data onto the som s two dimensional grid within the topology map neurons close to each other are more similar than neurons far apart moreover the most significant data patterns along the horizontal and vertical dimensions of the topology map can be interpreted via the relative contribution of features to pc1 and pc2 clark et al 2020 in the next section we demonstrate a som applied to moea derived dv and objective layers we use the illustration to describe the benefits of the som and to introduce the plot types used in the post mordm framework 3 1 3 example som on moea created policies to demonstrate the benefits of the som we provide an example in fig 2 fig 2a shows moea derived objective values symbolized with a pa plot each vertical axis is an objective f i i 1 2 m where m is the number of objectives and each trace corresponds to one policy using the som the m dimensional objective space is summarized with a two dimensional topology map which we demonstrate with fig 2b in the topology map each neuron is represented by a radar plot and each axis of the radar plot is an objective pc1 summarizes the largest variations in objective values and the correlation between objectives for example moving from left to right in fig 2b f 1 and f 3 decrease and f 2 and f m increase from bottom to top f 2 decreases and f m increases slightly the objectives demonstrate greater variance along pc1 compared to pc2 which is determined via their eigenvalues but can also be observed visually for instance contrast the neuron on the bottom left to the neuron on the bottom right the blue surface area is markedly smaller then compare the bottom left neuron to the top left neuron the surface area is also smaller but with relatively less change thus pc1 is allocated four neurons compared to three neurons for pc2 to better capture the larger variation of objective values by utilizing principal component based dimension reduction som summarizes the most important patterns of the objective layer in two dimensions and creates a navigable visualization we use fig 2b to demonstrate the clustering of policies and the intuitive arrangement of clusters provided by som fig 2 symbolizes the clustering process via lines connecting fig 2a to neurons in fig 2b effectively som reduces the number of alternatives a dm would consider from the hundreds of moea derived policies to the number of neurons in the som moreover neurons close together in the topology map are more similar than those far apart for instance compare any two adjacent neurons in fig 2b the shape of the blue area is more similar than that of neurons on opposite ends of the map effectively som both clusters policies and arranges the clusters based on their similarities som topology maps are also a powerful tool for synthesizing and visualizing the relationships between various data layers we demonstrate the visualization of a dv layer in fig 2c each hexagon is one neuron and within each neuron are dvs the value of the dv is indicated by the position of the circle on its axis note that fig 2b and c are one and the same som meaning the assignment of each policy to a neuron and the location of the neurons are the same but they visualize two different mordm data layers therefore any patterns observed in the objective layer can be related to patterns observed in the dv layer for example we discussed earlier that f 2 decreases top to bottom in fig 2b in fig 2c x 2 decreases from top to bottom thus the conclusion is that decreasing the dv 2 results in the decrease of objective f 2 in the next section we describe how the post mordm expands on previous som applications to also visualize the robustness layer 3 2 the post mordm framework to implement post mordm three data layers are needed dv values performance objective values and robustness metric values fig 3 demonstrates the workflow of post mordm which we describe in the following subsections 3 2 1 train som on performance objective layer the first step of post mordm is training a som on the objective layer of a set of policies as described in section 3 1 1 creating the som first requires normalizing or scaling each objective calculating the length width ratio determining the number of neurons setting hyperparameter values initializing the neurons along pc1 and pc2 then implementing the training algorithm we recommend testing multiple soms with different sizes and hyperparameter values then selecting the smallest som that achieves sufficient pve and te after training the objective layer is plotted on the som in this paper we denote the mordm data layer being visualized with a subscript for example somobj shows the topology map of the objective layer fig 3 1 each neuron in somobj is depicted with a radar plot which we described in section 3 1 3 3 2 2 analyze inverse relationships to decision variables next the dv values for each neuron are visualized via somdv shown in fig 3 2 this visualization is the result of training som on the objective layer then projecting the dv layer onto the som we use the visual patterns in somdv and the principal components of somobj to investigate the relationships between the objective and dv layers as discussed earlier in section 2 3 2 with fig 2 b c in fig 3 we summarize each dv with a single value per neuron which could be the mean or median value however alternative visualizations that show the dv values of each policy assigned to the neuron see section 4 3 2 or a visualization of their distribution box plots violin plots etc can also be used 3 2 3 superposition robustness metrics after establishing somobj and somdv the dms define their robustness metrics of choice this includes the type of robustness metric such as regret or satisficing any performance objectives and corresponding thresholds and any considerations of risk tolerances that result in unique definitions of robustness see section 2 1 for a review of robustness metrics and guidance on selecting them we refer the reader to mcphail et al 2018 2021 we denote each dm and their robustness definition with an index 1 2 n because the som is trained on the objective layer not robustness layer dms can change or modify robustness metrics without needing to retrain the som we then superposition each unique robustness metric onto a topology map shown in fig 3 3 resulting in n robustness visualizations collectively called somrobust the creation of somrobust is similar to the visualization of dv with somdv the som was trained on the objective layer after which the robustness layer is superpositioned onto it in fig 3 3 we plot each dm s robustness value on an individual topology map coloring each hexagon by the robustness value averaged over policies assigned to each neuron in the som literature topology maps that show only one feature are called component planes clark et al 2020 the relationships between mordm data layers are explored visually and via the pcs of somobj for example we discussed earlier that moving from bottom to top dv x 2 decreases fig 3 2 resulting in the decrease of objective f 2 fig 3 1 now consider somrobust darker neurons represent better robustness values so when x 2 and f 2 both decrease this is related to decrease in robustness for dm1 increase in robustness for dm2 and decrease in robustness for dm n in a negotiation context each dm is presented their unique robustness visualization which they use collectively with somobj and somdv to identify their preferred neuron s in the example illustrated in fig 3 3 dms 1 2 and n maximize their individual robustness preferences in the lower left top right and bottom right neurons respectively we have highlighted each dm s preferred neuron with green circles blue squares and pink arrows respectively 3 2 4 navigate som to compromise neurons to encourage discussion negotiation and compromise we establish a topology map that is shared between the dms in fig 3 4 we begin with a colorless topology map meaning the neurons are not colored by robustness values but the assignment of policies to neurons and the position of neurons are the same as somobj somdv and somrobust then we project the dms robustness preferences established in fig 3 3 onto the colorless topology map establishing a shared negotiation platform called somnegotiation when the dms preferred neurons are located far apart this indicates conflicting preferences in the weighing of objectives dvs and robustness for example dm1 prefers the lower left neuron which is characterized by policies with high values of x 1 and f 3 but in contrast dm2 prefers the top right neuron where policies have a small x 1 and f 3 to negotiate and compromise dms navigate from their individual preferences towards a neuron between dms the tradeoffs of which are interpreted via somobj and its pcs somdv and somrobust for instance the example dms in fig 3 4 negotiate to a neuron that requires each to compromise a similar amount for dm1 this requires a decrease in objectives f 1 and f 3 left to right along pc1 decrease in f 2 and increase in f m moving upward along pc1 considering dvs the compromise neuron implements less of dvs x 1 and x 2 and more of x l dms may negotiate to a single neuron of mutual interest or several neurons of mutual interest in either case the number of policies under consideration is significantly reduced and the neurons can now be investigated further by analyzing the individual policies within them in the next section we provide an example with a case study of reservoir operation policy in the colorado river basin 4 post mordm case study reservoir operation policy in the colorado river basin 4 1 motivation the colorado river basin crb supplies municipal water for nearly 40 million people in seven us states basin states 29 federally recognized tribes and northern mexico the crb is a significant source of hydropower producing about ten billion kilowatt hours of electricity annually enough to power one million us households reclamation 2018b 2021c frequently asked questions faqs u s energy information administration eia 2020 moreover crb surface water is the primary source for the basin states agriculture sector which is responsible for 70 of the crb s consumptive use and losses reclamation 2018a the crb is regulated according to the law of the river a compilation of compacts treaties federal law and court decisions dating back to 1922 reclamation 2015 pursuant to the law of the river the basin states are divided into the upper basin ub colorado wyoming utah and new mexico and the lower basin lb arizona nevada and california divided by a streamflow gauge at lees ferry arizona each basin is allocated 7 5 million acre feet maf annually for consumptive use of which the ub is yet to fully utilize in addition mexico is allocated 1 5 maf totalling 16 5 maf basin wide during the 21st century persistent drought in the crb has exacerbated the risk of temporary or prolonged interruptions in water supplies buschatzke et al 2019 average annual streamflow has dwindled to 72 of the historical average lukas and payton 2020 and as of november 2021 system reservoirs are filled to only 38 of full capacity reclamation 2021b potential consequences of this drought include lb shortages curtailments of ub consumptive use and critically low reservoir levels which can also diminish hydropower production recreational services and environmental benefits reclamation 2007 in an effort to minimize these risks the us bureau of reclamation reclamation the basin states and mexico have recently legislated multiple shortage operation policies for lake mead the largest reservoir in the system the 2007 interim guidelines guidelines defined the pool elevations and corresponding volumes by which deliveries to the lb would be reduced during times of low reservoir levels i e shortage volumes further the guidelines dictate how lake powell the upstream reservoir from lake mead and the second largest in the system would be operated in coordination with lake mead reclamation 2007 three lake mead policy alternatives were considered for the guidelines two alternatives prioritized water delivery and storage respectively and the third alternative which was selected incorporates operational elements from both of the other two alternatives reclamation 2007 p 8 after 2007 drought persisted and reservoir levels continued to decline therefore crb stakeholders later augmented shortage volumes established in the guidelines via minute 323 between the us and mexico and the lb drought contingency plan dcp collectively the guidelines minute 323 and the lb dcp establish the cumulative shortage operations in effect at the time of writing this paper international boundary and water commission 2017 colorado river basin drought contingency plans bureau of reclamation 2019 although these policies differ in terms of whether or not users can recover delivery reductions when reservoir storage increases all policies functionally decrease the risk of pool elevations at lakes mead and powell declining to critically low levels we provide an overview of the lake mead shortage operations in fig 4 the projected pool elevation for january 1st of the coming year is shown on the y axis in feet above mean sea level msl the pool elevation determines the volume of water by which downstream deliveries are reduced for the calendar year i e the shortage volume as indicated by color the guidelines minute 323 and the lb dcp are ordered chronologically by year of implementation along the x axis and the cumulative shortage operation is shown on the right the policies expire december 31st 2025 thereafter a new policy will take effect in this case study we contribute to the negotiation of new lake mead shortage operations beginning in 2026 first we employ moea to identify a set of non dominated lake mead shortage operations and quantify objective values then we calculate multiple robustness metrics to reflect the conflicting interests of storage and delivery stakeholders finally we use post mordm to demonstrate a process of learning negotiation and compromise between two illustrative dms 4 2 implementation of mordm our mordm problem formulation is summarized in table 1 we describe the dvs simulation model and objectives when discussing moea optimization next we describe the sources of uncertainty and the methods for creating the sow ensemble then we define the robustness metrics of two example dms in the crb before demonstrating post mordm 4 2 1 policy alternatives we use a policy set adapted from alexander 2018 provided by reclamation policies were generated with the borg moea hadka and reed 2013 which was coupled with the colorado river simulation system crss a hydro policy model built in riverware that serves as reclamation s long term planning model for the crb zagona et al 2001 the simulation is 44 years long and uses a monthly timestep evaluating eight performance objectives the objectives quantify tradeoffs between ub and lb interests and delivery vs storage objectives see table 1 for definitions borg seeks to minimize the objectives by adjusting 14 dv table 1 right 12 dvs control shortage operations of which six define the pool elevations where shortage operations begin t1e t6e and six are the corresponding shortage volumes subtracted from lb deliveries t1v t6v the remaining two dvs are the elevations at which surplus operations begin but this case study will discuss shortage operations only the result is a set of 463 policy alternatives the objectives and tradeoffs of which are explored via post mordm in section 4 3 4 2 2 robustness analysis 4 2 2 1 sow ensemble generation our robustness analysis considers three sources of uncertainty 1 annual cumulative natural flow above lees ferry arizona 2 annual consumptive use in the ub which is sampled for each simulation but held constant with respect to time and 3 initial reservoir pool elevations at lake mead and lake powell this section describes how we sampled the uncertainty with a 500 member sow ensemble the result of which is shown in appendix a4 we considered four hydrology ensembles historically used by reclamation and thus familiar to crb dms reclamation 2007 2012 2018a groves et al 2013 1 the observed resampled ensemble is the result of the index sequential method ism applied to the observed 1906 2007 cumulative natural flow record 2 the global climate model gcm ensemble is based on bias corrected and spatially downscaled cmip3 climate projections of future high medium and low emission scenarios run through the variable infiltration capacity vic model 3 the paleo resampled ensemble applies the ism method to paleo reconstructions dating 762 to 2005 4 lastly the paleo conditioned ensemble uses a non parametric technique to blend the wet dry sequences from the paleo record with magnitudes from the observed record in sum there are 1963 streamflow traces that describe the envelope of hydrologic uncertainty consistent with the philosophy of mordm we use the traces to broadly sample the hydrologic uncertainty space as described below for more information on the ensembles we refer the reader to the 2012 crb supply and demand study reclamation 2012 next we created a 1000 sample latin hypercube lh of annual ub consumptive use initial pool elevation at lake mead and initial pool elevation at lake powell annual ub consumptive use ranges from 4 2 to 6 0 maf which considers both curtailments and growth for comparison in 2016 the upper colorado river commission estimated consumptive use at 4 33 maf and forecasted 5 22 maf in 2060 2016 initial reservoir levels consider lakes mead and powell because their combined storage accounts for about 87 of the entire system reclamation 2021a sampling ranges were informed from the 10th low end and 90th high end percentile values from reclamation s april 2020 five year projections rounding the low end down to the nearest 50 feet reclamation 2020 thus powell s initial pool elevation ranges from 3450 to 3675 feet above mean sea level msl and mead ranges from 1000 to 1185 feet msl the pool elevation projections end december 2026 accounting for the range of possible pool elevations at the expiration of the current operation policy after creating the lh we combine every sample of pool elevations and ub consumptive use with every hydrology trace to create a large set of sow from which to select a subset for robustness simulations to reduce computational costs we sample a subset of 500 sow using conditioned latin hypercube sampling clhs which is an extension of latin hypercube sampling lhs minasny and mcbratney 2006 instead of creating new multivariate samples that form a lh clhs employs an optimization algorithm to select existing observations that form a lh in the multivariate feature space while mimicking the distributional properties of the original population minasny and mcbratney 2010 brus 2019 roudier 2020 practically clhs allowed us to use existing hydrology traces our existing observations select a subset of sow with minimal repeats of hydrology traces and preserve the desired uncertainty ranges 4 2 2 2 decision maker robustness metrics our robustness analysis uses two illustrative dms with conflicting preferences namely delivery and storage delivery is of greatest concern to the lb since lb allocations are 100 utilized for irrigation municipalities groundwater recharge and all other uses the storage dm reflects hydropower interests at lake powell and lake mead moreover storage is of special concern for shoreline recreational services like boat ramps and marinas reclamation 2012 the robustness preferences of delivery and storage are both quantified with the satisficing metric satisficing is the fraction of sow where a policy satisfies minimum performance thresholds defined by the dm satisficing ranges from 0 performance thresholds satisfied in zero sow to 1 performance thresholds are satisfied in 100 of sow the performance requirements for delivery are shown in condition 1 delivery performance requirements lb avg 600 kaf and lb dur 10 years lb avg is the annual shortage volume in the lb averaged over the simulation lb dur is the maximum consecutive years the lb is in shortage conditions see table 1 thus delivery s performance thresholds require both acceptable average magnitudes and maximum duration of shortages the performance requirements for storage are shown in condition 2 storage performance requirements m 1000 10 and p 3490 5 m 1000 is the percentage of simulation months where lake mead s pool elevation is below 1000 feet msl and p 3490 is the percentage of simulation months where lake powell s pool elevation is below 3490 feet msl together the performance thresholds for storage require that both lake mead and lake powell consistently stay above critical pool elevations the dms performance thresholds are used to calculate satisficing for each policy and dm according to the equation s a t i s f i c i n g i p 1 s j 1 s g i j x p where s is the total number of sow j is a sow index x is the decision variable vector for policy p and g i j is an indicator function g i j 1 if the performance thresholds of dm i delivery or storage are satisfied in sow j and g i j 0 otherwise 4 3 implementation of post mordm 4 3 1 training the som on the objective layer after normalizing the objectives we selected the number of neurons and som hyperparameter values using a grid search of 1000 lh samples the tested hyperparameters include neighborhood radius neighborhood function distance function and edge behavior the number of neurons in length width directions of the som was calculated given the number of total neurons sampled in the lh and the calculated ratio of the first and second eigenvalues of the objective layer we chose a hexagonal neighborhood topology according to the recommendation of clark et al 2020 we evaluated the number of neurons and each hyperparameter set with pve and te som training was performed in r using the kohonen package wehrens and buydens 2007 wehrens and kruisselbrink 2018 2022 and fit metrics were calculated using the awesom package boelaert et al 2021 r core team 2021 the selected som is 5 3 neurons because larger maps achieved small increases in pve while smaller ones saw significant decrease in pve in both cases attaining similar te for additional details on the training process the number of neurons to use and the selection of hyperparameter set including code see appendix a5 fig 5 shows somobj which reveals the two most important modes by which policies vary with respect to the objective layer within the radar plots each objective is scaled 0 center to 1 outer edge where 0 is ideal because they are minimization objectives every policy is plotted in its assigned neuron using a transparent blue fill to visualize the number of policies and the degree to which their objective values are similar for example consider neuron 6 middle row far left these policies result in low average and maximum shortages lb avg and lb max at the expense of frequent long duration shortages lb freq and lb dur they also perform poorly in water storage reliability at lakes powell and mead m 1000 and p 3490 the black borders around each policy overlay each other almost exactly indicating high similarity among the policies in contrast consider neuron 15 top row far right the policies have large average and maximum shortages however they perform comparatively well with respect to shortage frequency shortage duration and reservoir storage the borders around each policy are easy to distinguish from one another especially for lf deficit this indicates notable variation of performance for the policies assigned to neuron 15 for the interested reader appendix a6 reports the average objective value in each neuron i e component planes to facilitate the interpretation of the pcs we include the loading scores of each objective shown in the bar plots loading scores quantify the degree to which each objective contributes to the pc where larger magnitudes mean the objectives contribute more to the change in performance james et al 2013 chap 10 2 green bars indicate decreasing values from left to right increase in performance whereas gold dashed bars indicate increasing values decrease in performance for example consider pc1 left to right p 3490 p wyr and m 1000 are improving but lb avg and lb max are worsening the other objectives have relatively little to no change in the horizontal direction moving from bottom to top along pc2 lb freq and lb dur are decreasing while lb max is increasing in other words the two most significant tradeoffs for the dms to navigate is first the tradeoff between reservoir storage reliability and lb shortage magnitudes average and maximum and second the tradeoff between shortage duration frequency and maximum magnitude 4 3 2 lake mead operation policies after we establish somobj we visualize the inverse relationship to lake mead dvs with somdv shown in fig 6 each vertical bar represents a lake mead policy where the y axis is water surface elevation and the colors show the magnitude of shortage the number of policies in each neuron is shown to the right of the neuron index in parentheses for neurons with more than 20 policies 20 policies are plotted at random to conserve space for simplicity the policies are ordered randomly along the x axis but could be ordered according to a dv robustness metric or objective to facilitate comparisons between neurons and between mordm data layers we report the tier 1 elevation t1e also indicated by the horizontal dashed line in feet msl and volume of the first shortage t1v kaf plus maximum shortage maxv kaf averaged across all policies in a neuron fig 6 shows salient patterns in shortage volumes and t1e corresponding to the pcs of somobj from left to right t1v maxv and t1e tend to increase this is shown by the gradient of green yellow red and the increasing bar height comparing to somobj in fig 5 the result is improved reliability of reservoir storage the tradeoff being increased average and maximum shortages from bottom to top t1e tends to decrease especially to the left side of somdv t1v and maxv also tend to increase the related performance outcome is decreasing frequency and duration of shortages at the expense of increasing maximum shortage interestingly t1v and maxv increase both left to right and bottom to top but the frequency and duration of shortages experienced by the lb responds almost exclusively in the vertical direction as indicated by loading scores of near one for the vertical pc compared to loading scores less than 0 25 for the horizontal pc considering that t1e tends to increase left to right but decreases bottom to top we conclude that reducing the frequency and duration of lb shortages meanwhile achieving reliable reservoir storage requires that larger lb shortages be paired with lower pool elevations this is the difference between neuron 15 and 5 both implement large shortage volumes an average t1v volume of 1825 and 1843 kaf respectively but policies in neuron 15 achieve less frequency and duration of lb shortages via being more patient waiting until lower t1e 1065 vs 1092 ft msl to implement the first shortage this difference in operation philosophy results in shortage conditions occurring in 12 01 less simulation months in neuron 15 compared to 5 not shown in fig 5 see appendix a6 4 3 3 decision maker robustness maps fig 7 a b shows the individual robustness values of the delivery and storage dms somrobust the color and label of each neuron indicates the average satisficing of the policies assigned to each neuron where darker colors are preferred clear topologic patterns exist demonstrating the effectiveness of robustness metrics superpositioned on a som fit to the objective layer in this example we present two satisficing robustness metrics moreover we demonstrate the effectiveness of this method using te and pve for three additional metrics in appendix a7 a13 for delivery neurons in the top left are the most robust with satisficing decreasing left to right on the top row decreasing robustness comparing to somdv in fig 6 the policies that result in the best delivery robustness do so by two operational strategies first consider neuron 11 these policies implement small shortage volumes maxv of 650 kaf at low pool elevations t1e less than 950 feet in all but one policy draining lake mead with minimal storage reliability safeguards alternatively a policy can be robust by delicately balancing shortage volume and elevation neurons 12 14 these neurons implement moderate to severe t1v 1385 1720 kaf at moderately high elevations t1e from 987 to 1028 feet msl other neurons do not exhibit this balance for example consider neurons 1 and 6 these policies implement small t1v shortage volumes 196 345 kaf implemented at moderate to high elevations 1039 1056 feet these neurons are not robust likely because the shortage volume is not large enough to raise lake mead s pool elevation out of shortage operations resulting in durations of lb shortage exceeding 10 years contrast neuron 15 top right to neuron 5 bottom right both implement large t1v shortage volumes 1825 and 1843 kaf respectively but neuron 15 policies wait until lower pool elevations to begin shortages t1e of 1065 vs 1092 feet msl satisfying delivery s performance thresholds in 10 more sow on average by exploring the relationships between somrobust and somdv the delivery stakeholder can identify policies that achieve this balancing act to satisfy both average shortage and shortage duration performance thresholds for storage neurons to the far right are the most robust and satisficing decreases towards the left in each row considering somdv in fig 6 the policies in neurons 5 10 and 15 have the most aggressive shortage operations implementing large t1v and maxv 1318 2182 kaf at high pool elevations t1e from 1064 to 1092 feet msl decreasing robustness is caused by smaller shortage volumes beginning at lower pool elevations in a negotiation context each dm is provided with the topology map showing their unique robustness definition upon which they identify neurons with preferred and unacceptable performance in fig 7 we define preferable performance as greater than 0 70 blue unacceptable below 0 5 dashed red and for neurons from 0 5 to 0 7 the dms have weak preferences then we project the intersection of their preferences onto a blank topology map shown in fig 7 c one neuron presides in both dms preferred areas shown in solid purple this is an intuitive neuron for both dms to investigate further however the adjacent neurons lie within either the preferred or weak preference areas of both dms thus we consider these neurons a feasible negotiation space 4 3 4 negotiation navigation next we visualize only the neurons defined to be inside the feasible negotiation space this enables the dms to investigate further the dv objective and robustness layers of mutually feasible policies shown in fig 8 here we have replaced the robustness topology maps from fig 7 with boxplots providing information on the spread of robustness in each neuron delivery is shown in orange and storage is shown in blue we have provided the boxplots of all neurons in appendix a14 neuron 14 mutually intersects both dms preferred areas however additional negotiation can be facilitated with fig 8 for example delivery may try to negotiate leftward to neuron 13 improving their satisficing score and avoiding comparative disadvantage to storage blount and bazerman 1996 tsay and bazerman 2009 in this neuron shortage volumes are smaller and implemented at lower pool elevations contrarily storage may negotiate to neuron 15 to increase robustness meaning that pool elevations and shortage volumes would increase the map based visualization of multiple mordm data layers such as demonstrated in fig 8 can also help dms overcome cognitive myopia in a negotiation context cognitive myopia can occur when the expressed interests of a dm such as their definition of robustness or weighing of objectives limits the exploration of mutually feasible policy alternatives kasprzyk et al 2013 giuliani et al 2014 for example when considering only robustness it does not appear that either dm would individually desire moving downward to neurons 9 or 10 because moving horizontally yields the greatest individual increases in satisficing however the consideration of objective and dv layers reveals other reasons these neurons may be of interest for instance lb max is improved by moving downward to neurons 9 and 10 perhaps the satisficing performance thresholds on lb avg and lb dur reflect delivery s highest and most well defined priorities but other findings like this could make these neurons interesting if so delivery may negotiate to neuron 10 which implements smaller shortage volumes at higher elevations storage may accept neuron 10 because their satisficing value is mostly unaffected by simultaneously visualizing multiple mordm data layers via topology maps dms are encouraged to explore policy alternatives they might not otherwise which can help dms negotiate to a compromise policy 4 4 robust shortage policies vs existing lake mead operations the combined lake mead shortage operation fig 4 does not resemble the neurons within the feasible negotiation space of delivery and storage fig 7 this section describes how post mordm can explore the potential reasons why and the consequences thereof compared to the feasible neurons the combined operation implements t1e at too high of an elevation with too small of a t1v further maxv of the combined operation is small in comparison to the feasible neurons except neuron 9 notably the combined operation resembles neuron 9 if the combined operation s shortage volumes of less than 1000 kaf are removed without these smaller shortage volumes t1e t1v and maxv of the combined operation are similar to the average values of neuron 9 1045 feet msl 1013 kaf 1375 kaf compared to 1046 feet msl 1009 kaf 1341 kaf by navigating somdv somobj and somrobust we can explore the potential consequences of these relatively high elevation low volume shortage operations used in the combined operation in this comparison we will use neuron 3 to represent the combined operation because of similar t1e t1v and maxv for a detailed comparison of neuron 3 and the combined operation please see appendix a15 using somdv fig 6 traverse from neuron 9 to neuron 3 t1e increases by 36 feet t1v decreases by 245 kaf and maxv decreases by 39 kaf this change in dv values resembles the difference between neuron 9 and the combined operation increasing t1e decreasing t1v and similar maxv the result of moving from neuron 9 to neuron 3 as indicated by somobj fig 5 is greater frequency duration and average volume of shortages the increased frequency duration and average volume of shortages explains why the policies within the dms feasible negotiation space are dissimilar to the combined operation policies that combine high t1e with small t1v fail to satisfy delivery s robustness criteria which requires that the maximum duration of shortage not exceed 10 consecutive years and that average shortage volume not exceed 600 kaf this conclusion is consistent with somrobust which shows that these policies satisfy both of delivery s performance criteria in only 44 of sow fig 7 to summarize the combined operation implements relatively small shortages at high pool elevations and our results suggest that this operational strategy can result in high frequency duration and average volume of shortages unfavorable to water users in contrast policies that begin shortages at lower elevations and larger volumes can reduce the frequency duration and average volume of shortage with the tradeoff being larger maximum shortages therefore this tradeoff should be emphasized during the renegotiation of lake mead shortage operations to solicit feedback from stakeholders further these results highlight that dms in the crb and other river systems facing deep uncertainty need to consider which types of water reductions high frequency vs high magnitude lend themselves towards more sustainable agriculture and public acceptance 5 discussion in sections 3 and 4 we have introduced the post mordm framework then demonstrated it with a case study of reservoir operation policy in the following discussion we describe several ways that post mordm allows for application specific flexibility meanwhile highlighting best practices then we discuss how flexibility in post mordm creates ample opportunity for future research to build on the case study presented in this paper 5 1 flexibility and best practices with post mordm one goal of the post mordm framework is reducing the number of alternatives that dms need to consider in section 4 3 4 dms negotiated between five neurons where each neuron summarizes the key attributes of a group of similar policies this number of neurons is consistent with the psychology literature that suggests 3 9 alternatives be ideally considered at one time see section 2 2 however in applications with more than two dms and or a large number of feasible neurons 10 or more neurons may be needed to represent the negotiation space after identifying one or more compromise neurons dms may want to consider the individual policies assigned to them however the number of policies assigned to each neuron may exceed 9 it does in section 4 3 4 in this case additional steps can be taken to reduce the number of policies if so desired for example policies can be filtered by dv objective or robustness values alternatively it is possible to reduce the number of policies the som is trained on by applying filters before implementing post mordm doing so could reduce the number of policies assigned to each neuron result in neurons that are better approximations of the policies and result in neurons that are more distinct from each other for instance the illustrative dms in our case study defined nine neurons as unacceptable which suggests that some policies could have been removed before training the som in other cases however defining the criteria by which policies are filtered may be non trivial and not agreed upon by dms thus we advocate for a posteriori defining of filter criteria if any based on exploration of topology maps in the post mordm framework we have used robustness values as the data layer which dms use to identify their policies of interest and the starting point for ensuing negotiation we encourage this method because robustness metrics take into account critical driving forces of the system that are characterized by deep uncertainty alternatively dms could express their preferences based on the objective or dv layer or a combination of layers we advocate that dms identify their preferred neurons beginning with the robustness layer however the post mordm framework allows for such flexibility the post mordm framework is also flexible with respect to the types of plots used in topology maps in our case study we have created topology maps with radar plots fig 5 reservoir operation diagrams fig 6 component planes fig 7 and appendix a6 and box plots fig 8c alternative visualizations can be used based on the application or preferences of dms and future research can explore alternative methods for visualizing the intra neuron variance besides the radar plots and box plots used in this case study regardless of the plot type we believe it is critical to maintain the topologic arrangement of neurons produced by the som and to use easily interpretable visualizations that facilitate the exploration of tradeoffs and the relationships between mordm data layers another fundamental goal of post mordm is to provide dms with a shared negotiation platform which we accomplish with navigable topology maps however we do not prescribe the exact topology map visualization type to be used in negotiation for instance in section 3 4 we facilitated dm negotiation with a blank topology map meaning the hexagons in somnegotiation were colorless except for the outlines that indicated the preferred neurons of each dm then the illustrative dms used somdv somrobust and somobj to interpret the results of moving on somnegotiation this plot type presents somnegotiation as a neutral topology map choosing not to visualize any objective robustness or dv values on it however we can imagine a circumstance where dms would benefit from including additional information on somnegotiation for instance an objective or robustness metric that represents a shared concern of the dms could be plotted to further encourage negotiation in the crb case study of the delivery and storage dms perhaps an environmental objective could be used we have demonstrated how post mordm facilitates negotiation and compromise via topologic visualization of multiple mordm data layers we do not define the exact procedure by which negotiation occurs instead post mordm can be implemented with formal negotiation rules that are application specific and agreed upon by the parties to a negotiation for example gold et al implement fallback bargaining to identify two compromise water portfolios in a case study of four interconnected water utilities 2019 building on the work of herman et al described in section 2 1 2014 under fallback bargaining dms first rank policies according to their individual preferences then dms consider the top ranking policy for each dm if they do not agree each dm falls back one level in their ranking of policies dms continue to fall back until there exists a policy that is acceptable to every dm and this policy is selected as a compromise brams and kilgour 2001 madani et al 2011 the post mordm framework could complement fallback bargaining by facilitating dms in the ranking of policies using post mordm dms could rank a tractable number of neurons as opposed to hundreds of policies in the context of moea further post mordm could help dms rank policies based on simultaneous consideration of the objective dv and robustness layers as facilitated with topology maps lastly dms could use somobj somdv and somrobust to track the tradeoffs resulting from each fallback step in the negotiation overall the post mordm framework provides navigable visualizations that promote understanding and negotiation but it does not prescribe a formal negotiation procedure thus post mordm could be used to enhance the efficacy of prescribed negotiation procedures such as fallback bargaining throughout this paper we have emphasized post mordm in negotiation contexts indeed we believe this to be a significant contribution to environmental modeling and decision support literature alternatively post mordm can be utilized by an individual analyst design group or organization because the benefits of post mordm clustering dimension reduction and map based visualization of multiple data layers can also help individual entities attain greater understanding of their system and make decisions 5 2 future research opportunities other data layers future research could implement post mordm to explore additional decision relevant data layers scenario discovery the last step in mordm is traditionally performed on a small subset of policies alternatively vulnerability information for each policy or a representative policy from each neuron could be calculated then displayed on a topology map then vulnerability topology maps could be visualized alongside somdv to analyze the relationships between dvs and vulnerability several recent publications have highlighted another potentially decision relevant data layer which is the degree to which robustness values are sensitive to statistical properties of the sow ensemble these properties include the number of sow the upper and lower bounds of the uncertain factors their correlation structure and their probability distributions hadjimichael et al 2020 reis and shortridge 2020 mcphail et al established a framework to quantify the sensitivity of robustness magnitude and robustness ranking 2020 which could be combined with the post mordm framework to identify dv and objective values that correspond to policies whose robustness is the most insensitive to sow ensemble design in this paper we have demonstrated the utility of the som for map like interpretation of objective dv and robustness layers and we discussed above how post mordm could be expanded to vulnerability and robustness sensitivity layers furthermore we believe the foundational methods and goals of post mordm could be implemented with layers relevant to decision support frameworks other than mordm of particular interest dynamic adaptive policy pathways dapp is another popular framework for systems faced with deep uncertainty because it frames policy implementation as conditional on future observations of uncertain factors haasnoot et al 2013 kwakkel and haasnoot 2019 p 359 like mordm dapp is characterized by several layers of decision relevant data where understanding the relationships between them is important to dms depending on how dapp is implemented these data layers can include long term policy decisions dynamic planning adaptive policy decisions short term contingency planning signpost variables signpost triggers and performance objectives haasnoot et al 2013 zeff et al 2016 the benefits of post mordm namely the simultaneous map based visualization of related data layers and a negotiation platform could also enhance the synthesis and communication of other decision support frameworks to dms 6 conclusion this paper presented the post mordm framework which enhances mordm based decision support via a novel implementation of the som post mordm constitutes an alternative paradigm for how policy relevant data is explored by interpreting mordm data as multiple layers arranged according to a two dimensional map system post mordm expands on previous applications of clustering dimension reduction and the som to 1 help dms elucidate the relationships between dvs objectives and robustness 2 reduce the number of alternatives dms need to consider and 3 establish a visual structured platform whereby dms with foundational disagreements are assisted in a process of negotiation and compromise we demonstrated post mordm with a case study of reservoir operation policy in the colorado river basin usa using a topology map of objective values somobj our results showed that the primary tradeoff dms need to navigate is the tradeoff between reservoir storage reliability and average magnitude of water delivery shortages the second strongest tradeoff is between frequency duration of shortages and maximum shortage magnitude we illustrated how topology maps can be used in a process of negotiation between two illustrative dms that represent delivery and storage interests in the crb based on individual definitions of robustness we showed that five neurons of policies could be mutually feasible and demonstrated how topology maps facilitate negotiation because of their map like navigation and interpretation we discuss how the combined lake mead shortage operation which is in effect until 2026 contrasts with the mutually feasible neurons because of a combination of high elevation low volume shortage tiers we used topology maps of dv and objective values somdv and somobj to describe how high elevation low volume shortage tiers increase the frequency and duration of delivery shortages while reducing the maximum shortage volume in the renegotiation of lake mead s shortage policy dms will need to consider which tradeoff the crb s diverse stakeholders can tolerate long persistent water shortages of smaller magnitude or less frequent shorter but harsher shortage magnitudes moreover future research should incorporate dvs for lake powell operations into moea optimization further investigating performance and robustness tradeoffs while maximizing the benefits of coordinated operation between lakes powell and mead the post mordm framework and the case study presented in this paper contribute to one of the grand challenges of the 21st century identifying policies for human environmental systems that balance competing objectives and are robust to uncertainty addressing the decision related challenges posed by deep uncertainty requires the integration of research across multiple disciplines therefore the post mordm framework is a demonstration of this integration pulling from research in the domains of machine learning engineering design psychology and water resources management in an effort to build a bridge between decision support systems originating in academia to dms our hope is that the post mordm framework will facilitate negotiation and compromise as decision support frameworks like mordm are increasingly implemented in real world applications moreover we believe this research offers an alternative paradigm through which tradeoff analyses and negotiations can occur encouraging future studies to expand upon our implementation of the som while also exploring other innovative approaches software and data availability all r code and data to reproduce the case study is available on github https github com nabocrb post mordm we have formatted the code to facilitate straightforward application of post mordm in other case studies including code for every step described in section 3 and the variety of topology maps used in this paper author contributions nathan bonham conceptualization analysis coding visualizations writing original draft joseph kasprzyk conceptualization writing review and editing supervision edith zagona conceptualization writing review and editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the bureau of reclamation for providing the lake mead objectives and decision variable data and the anonymous reviewers who contributed to the clarity and technical thoroughness of this article this material is based upon work supported by the national science foundation graduate research fellowship under grant no dge 2040434 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105491 
25511,this paper introduces post mordm a decision support framework that augments many objective robust decision making mordm mordm often creates an intractable number of environmental management policies characterized by decision variable objective and robustness values this large number of policies inhibits decision support causing disagreements among decision makers post mordm addresses these challenges via the self organizing map som synthesizing mordm data as layers organized in a map like coordinate system it uses the som to cluster policies discover salient characteristics and assess cause effect relationships between decision maker choices i e decision variable values and performance objective and robustness values overall the goal of post mordm is to create a structured platform that encourages negotiation and compromise we demonstrate post mordm with a case study of two illustrative decision makers for reservoir operation policy in the colorado river basin usa post mordm helps communicate tradeoffs between storage and delivery objectives relate tradeoffs to shortage policies and identify mutually feasible policies graphical abstract image 1 keywords self organizing maps som decision support robustness tradeoff analysis colorado river basin multi objective evolutionary algorithm moea data availability all data and code is available at the github link included in the manuscript file 1 introduction decision making for coupled human environmental systems is a paramount challenge of the 21st century decision makers dms need to identify policy actions that are simultaneously equitable balance competing objectives and are robust to future uncertainty un general assembly 2015 committee to advise the u s global change research program et al 2021 ipcc 2021 simulation models are often used to support dms by quantifying their system s key performance outcomes and elucidating how performance outcomes relate to policy decisions and exogenous driving forces of system behavior however analysts and dms often disagree on the relationships between the driving forces and performance outcomes of their systems further the probability distributions of driving forces are unknown and or disagreement exists on how to weigh performance outcomes of alternative decision actions such decision problems are described as deeply uncertain lempert et al 2003 kwakkel and haasnoot 2019 when facing deep uncertainty implementing a policy can require negotiation between dms but reaching a compromise can be difficult because of foundational disagreements such as divergent framings of the problem wheeler et al 2018 lempert and turner 2020 different prioritization of performance outcomes smith et al 2019 or different tolerances of uncertainty related risk mcphail et al 2018 2021 many objective robust decision making mordm is a simulation based decision support framework that helps dms identify promising policy actions when faced with deep uncertainty kasprzyk et al 2013 mordm has been shown to be effective for various human environmental systems with applications such as municipal water supply portfolios herman et al 2014 2015 gold et al 2019 irrigation and groundwater sustainability li and kinzelbach 2020 and reservoir operation policies alexander 2018 quinn et al 2018 mordm produces three types of decision relevant information for dms to consider decision variable values objective values and robustness values fig 1 a in panel i a simulation model of the system is coupled with an optimization algorithm to generate a set of many policy alternatives each policy is defined by a vector of decision variable values yellow box and each policy has corresponding data on its performance objective values green box mordm then stress tests each policy in a robustness analysis panel ii at this stage uncertainty regarding driving forces of the system is explicitly considered by simulating each policy in many plausible future states of the world which densely sample the range of plausible future scenarios robustness metrics are used to quantify how well a policy performs in performance objectives across all the sow this produces the third type of decision relevant data blue box several challenges hinder dms from utilizing the decision relevant information produced from mordm to select one or a small subset of policies for potential implementation first interpreting the cause effect relationships between dms actions i e dv values and performance robustness tradeoffs is non trivial because the policy set often consists of hundreds of policies characterized by complex interactions between decision variables objectives and robustness metrics miller 1956 lecompte 1999 saaty and ozdemir 2003 herman et al 2014 alexander 2018 quinn et al 2018 wheeler et al 2018 smith et al 2019 moreover in environmental systems that provide services to diverse stakeholders dms may struggle to use all this data to overcome foundational disagreements such as different framings of the decision problem wheeler et al 2018 lempert and turner 2020 different weighing of performance objectives smith et al 2019 or different risk tolerances towards uncertain future conditions mcphail et al 2018 2021 hadjimichael et al 2020 multiple studies in the engineering design domain have highlighted the self organizing map som as a promising machine learning algorithm to alleviate the cognitive burden faced by dms obayashi and sasaki 2003 koishi and shida 2006 li et al 2009 mosnier et al 2013 zhang et al 2018 in this research we uniquely couple the som with mordm introducing an alternative paradigm for how the relationships between decision variables objectives and robustness are interpreted as an analogy consider fig 1b panel i shows four geospatial map layers of lake mead located just east of las vegas nevada each layer shows a different type of data satellite imagery hydrologic drainage network administrative boundaries and major road network but importantly the data is organized by latitude and longitude in other words every coordinate pair has multiple layers of corresponding data types moreover the geographic arrangement of map layers helps explain the relationships between data layers for example as you move westward from lake mead leftward on the longitude axis the number of administrative units and major roads increases which is because you are moving away from lake mead and into downtown las vegas the som creates analogous means for interpreting the mordm data as multiple data layers organized by a two dimensional coordinate system the som learns the many dimensional patterns of the policy set groups and arranges policies into a two dimensional coordinate system and establishes a map like visualization for elucidating the relationships between decision variables objectives and robustness in this paper we refer to performance objective values decision variable values and robustness values as layers in accordance with this paradigm our review of the som literature has shown two critical research gaps as it pertains to mordm and decision support applications obayashi and sasaki 2003 koishi and shida 2006 li et al 2009 mosnier et al 2013 zhang et al 2018 first som applications have not considered robustness a critical component of mordm instead being limited to the decision variable and performance objective layers second som applications have been implemented in the context of a single analyst design team or organization gaining important system understanding or selecting an engineering design such applications are fundamentally different than the policy or management decisions in an environmental system where the problem is often characterized by negotiation between multiple local state or federal governments ngos landowners and various other interest groups reclamation 2007 2012 wheeler et al 2018 molina perez et al 2019 to our knowledge the som has not been implemented in such a negotiation context building on previous som applications this paper introduces post mordm a framework that assists dms and analysts interpret visualize and negotiate large sets of policies post mordm augments mordm by using the som to a elucidate the relationships between decision variables objectives and robustness b reduce the number of alternatives dms need to consider and c establish a visual structured platform whereby dms with divergent performance priorities and risk tolerances are assisted in a process of negotiation towards compromise policies post mordm contributes to the som literature by expanding layer visualization to robustness and demonstrating how the construction and utilization of the som can be implemented in negotiation contexts for human environmental systems the remainder of the paper is organized as follows section 2 provides background on mordm then motivates our use of the som with a review of machine learning algorithms that have been used to post process mordm data section 3 describes the som algorithm and its fundamental benefits before outlining the post mordm framework section 4 demonstrates post mordm in a case study of reservoir operation policies in the colorado river basin usa the discussion and conclusion follow in sections 5 6 2 background and motivation 2 1 many objective robust decision making mordm many objective robust decision making mordm is a simulation assisted decision support framework that helps dms identify promising policies when faced with deep uncertainty kasprzyk et al 2013 the framework consists of four steps problem formulation policy generation robustness analysis and scenario discovery this section describes the steps and how they result in three mordm data layers decision variable values objective values and robustness values mordm is broadly applicable to policy analysis and environmental problems for clarity the examples we provide in this section focus on a specific familiar human environmental system namely a river system managed by water storage reservoirs first problem formulation defines the scope of the decision problem in terms of decision variables performance objectives and driving forces characterized by exogenous uncertainties lempert et al 2003 lempert and collins 2007 kasprzyk et al 2013 exogenous uncertainties are factors outside the explicit control of the dms like hydroclimatic factors e g temperature precipitation runoff and socioeconomic factors e g water demand irrigation efficiency uncertainty is characterized using what if scenarios called states of the world sow each sow is a multivariate sample of the uncertainty factors and an ensemble of sow is created to extensively sample each factor s plausible bounds decision variables dvs x x 1 x 2 x l represent policy actions where x can be continuous e g volume of water to release from a reservoir at a given time step or discrete e g augment water supply by either expanding a reservoir building a desalination plant or purchasing water rights performance objectives f f 1 f 2 f m are metrics that quantify how well the system performs such as reliability of meeting water demands or average hydropower production for each policy and sow a simulation model calculates the values of the performance objectives after problem formulation policy generation is performed using multi objective simulation based optimization commonly using multi objective evolutionary algorithms moeas the moea is coupled with the simulation model in a loop where the moea generates policies and the model evaluates performance objectives in this paper we use the term policies to refer to moea solutions i e vectors of dv values giuliani et al 2014 at this stage in mordm a small number of sow is used to force the simulations and calculate performance objective values often using sow that reflect values of uncertain factors observed in the historical record kasprzyk et al 2013 alexander 2018 the moea creates new policies over thousands of iterations of the simulation optimization loop using operators inspired by concepts of evolutionary theory like genetic crossover random mutations and survival of the fittest hadka and reed 2013 maier et al 2019 the output of the moea is a set of non dominated policies policy a dominates policy b if the performance of policy a is equal to or better than the performance of b in all objectives while being better than the performance of b in at least one objective in the resulting policy set no policy dominates any other policy i e they are non dominated in effect the policies exhibit performance tradeoffs where improving performance in one objective necessitates inferior performance in one or more other objectives as a result of the policy generation step each policy has two corresponding mordm data layers objective values and dv values dms use interactive visualizations to explore tradeoffs and interpret the relationships between dv and objective layers visualization types include glyph plots kollat and reed 2007 kasprzyk et al 2013 and parallel axis pa plots inselberg 2009 which are commonly used because of the relative ease with which greater than three dimensions are visualized using pa plots dms apply their preferences by changing the order of the axes filtering policies that meet performance goals and clustering policies with similar dv and or objective values inselberg 2009 raseman et al 2019 to elucidate the relationships between objective and dv layers previous studies have linked a pa plot of objectives to a pa plot of dvs smith et al 2018 raseman et al 2019 li and kinzelbach 2020 raseman et al 2020 for example smith et al hosted a workshop with colorado water managers to query how moea results could improve decision making in their respective agencies 2019 participants identified policies of interest according to performance objective preferences in one plot and the corresponding dv values were highlighted in another linked plot demonstrating the types of actions needed to achieve their performance preferences the objectives layer is calculated using a small number of sow although policies are non dominated with respect to the objectives layer it is possible their performance deteriorates when the assumptions about deeply uncertain factors are incorrect therefore policy generation is followed by robustness analysis where policy alternatives are stress tested by simulating them in the ensemble of sow defined in the problem formulation step robustness is the degree to which a policy s performance is insensitive to this broad sampling of sow kasprzyk et al 2013 herman et al 2015 mcphail et al 2018 performance sensitivity is quantified with robustness metrics which are statistics that describe how well a policy performs across its distribution of performance for specified objectives in the sow ensemble robustness metric values define the third mordm data layer there exist different robustness metrics which use varying transformations and statistical calculations across the sampled sow mcphail et al 2018 2021 these different robustness metrics reflect differing prioritization of objectives minimum performance thresholds and risk tolerances of dms mcphail et al 2018 2021 quinn et al 2018 gold et al 2019 hadjimichael et al 2020 examples of robustness metrics include the expected value of performance laplace s principle of insufficient reason regret from best possible performance regret from best the worst case performance maximin or the fraction of sow where a dms expressed performance criteria are achieved satisficing mcphail et al 2018 the previously described interactive visualization techniques can also be used to explore the tradeoffs between different robustness metrics and their relationships to dvs giuliani et al 2014 herman et al 2015 cohen and herman 2021 notably in a study of four connected water utilities herman et al demonstrate how a different policy is the most robust for each utility because they each define robustness based on their respective performance criteria in other words there exists interutility robustness tradeoffs and settling on a single policy would require the utilities to negotiate and compromise 2014 this example underscores the challenge of selecting a policy in a system characterized by deep uncertainty and multiple dms plus the potential benefits of a structured negotiation platform that facilitates negotiation and compromise in the last step of mordm policies are interrogated further via scenario discovery scenario discovery is a type of vulnerability analysis where vulnerability is defined by the violation of dm defined performance criteria scenario discovery identifies the uncertainty factors sampled in the sow ensemble that are the best predictors of when a policy will be vulnerable and the corresponding values of the factors in which vulnerability occurs traditionally scenario discovery is performed on a small subset of policies selected based on the results of policy generation and robustness analysis kasprzyk et al 2013 giuliani et al 2014 quinn et al 2018 in this paper we focus on using mordm data layers objective dv and robustness values to identify a small subset of policies these policies would subsequently be input to a separate scenario discovery process and or real world implementation the workflow in this paper does not include scenario discovery 2 2 challenges and gaps to mordm decision support in the selection of policies using mordm techniques dms will want to consider the dv objective and robustness layers however several challenges arise when using mordm data layers to select policies for instance the quantity of policy alternatives and the dimensionality of mordm data layers exceeds average human processing limitations we use dimension to mean the numbers of dvs objectives and robustness metrics in each mordm data layer for example published applications of mordm have exhibited hundreds of policies herman et al 2014 zeff et al 2014 quinn et al 2018 wheeler et al 2018 less than five to over 100 dvs herman et al 2014 quinn et al 2018 three to eight objectives alexander 2018 quinn et al 2018 wheeler et al 2018 and one to four or more robustness metrics herman et al 2014 however studies suggest that fewer than three to a maximum of nine alternatives be ideally examined at one time miller 1956 brill et al 1982 lecompte 1999 saaty and ozdemir 2003 moreover in the context of negotiation between dms large quantities of information can have negative consequences by increasing egocentric interpretations of what constitutes a fair resolution thus increasing the time needed to identify a compromise policy thompson and loewenstein 1992 tsay and bazerman 2009 these studies highlight the cognitive burden dms face when selecting policies from large many dimensional policy sets selecting policies also requires dms to understand the cause effect relationships between their actions i e dv values and the performance robustness tradeoffs of the system however when the policy set is very large and or the relationships between mordm data layers is complex elucidating such relationships is non trivial although interactive visualization methods like pa plots can be effective for linking dv values to objective values for one or a small subset of policies it remains a challenge to synthesize the relationships across the entire policy set because of the large number of dimensions in each mordm data layer complex and non linear interactions within the system smith et al 2018 hadjimichael et al 2020 noisy or low signal dvs objectives or robustness metrics smith et al 2019 and surprising effects of challenging sow on robustness values indeed the participants in smith et al described that training personnel in their respective organizations to understand moea results and communicating them to dms remains a challenge and the study concluded that structured information about the relationships between decision levers variables and performance objectives would be beneficial for interpreting tradeoffs 2019 this study demonstrated the difficulty of synthesizing relationships between dv and objective layers moreover the addition of the robustness layer which potentially includes multiple definitions of robustness would likely exacerbate this challenge another challenge to selecting policies is that dms and stakeholders involved in environmental decision problems may have foundational disagreements that are not easily overcome by only exploring the objective dv and robustness layers as described in the previous section dms may hold conflicting prioritization of objectives which combined with different degrees of risk tolerance can result in different definitions of robustness further dms may not agree on a single problem formulation instead using multiple problem formulations that reflect their respective world views quinn et al 2017 wheeler et al 2018 lempert and turner 2020 for dms with foundational disagreements to identify compromise policies mordm data layers need to be presented to dms in such a way that facilitates discussion negotiation and compromise 2 3 clustering and dimension reduction methods with mordm previous studies have reduced the number of alternatives dms need to consider and improved the interpretability of moea derived objectives and dv layers via two classes of statistical techniques namely clustering and dimension reduction clustering is a method to group data such that data within a cluster are similar and data in different clusters are more dissimilar hastie et al 2009 chap 14 3 clustering has previously been used to group policies with similar dv and or objective values kansara et al 2015 raseman et al 2020 in this approach dms consider a small number of clusters rather than hundreds of individual policies which can help reduce the cognitive load faced by dms to be within the range of practical human processing limitations discussed in the previous section for example raseman et al group moea derived water treatment plant policies using k means clustering summarizing the dv and objective layers with three clusters and a representative policy from each 2020 although clustering techniques can reduce the number of alternatives dms consider it does not elucidate the interactions between objectives or the relationship between objectives and dvs further to our knowledge clustering applications have not considered robustness to elucidate the interactions between objectives and dvs clustering can be complemented with dimension reduction methods dimension reduction is the process of either selecting a subset of the most important features of a data set feature selection or creating a low dimensional representation of the data by linear or non linear combinations of features feature extraction khalid et al 2014 hira and gillies 2015 ghojogh et al 2019 in both cases the goal of dimension reduction is to simplify a dataset meanwhile preserving the information contained within it by removing or combining noisy redundant or irrelevant features in moea applications dimension reduction methods have helped dms relate dv values to objective outcomes for example smith et al perform feature selection via multivariate regression trees to identify the dvs that are most influential on objectives meanwhile using a tree based visualization to intuitively guide decision makers towards dv values that achieve their desired objective outcomes 2019 kansara et al utilize principal component analysis a feature extraction method to reduce 12 objectives into a two dimensional summary that highlights the most important objectives and the correlation structures between them 2015 dimension reduction also helps reduce the cognitive burden faced by dms instead of interpreting the relationships between many dvs and objectives the relationships can be synthesized in a low dimensional summary that captures the most decision relevant information these studies demonstrate the benefits of clustering and dimension reduction techniques to enhance decision support efficacy with dv and objective layers and they motivate similar techniques to be expanded to the robustness layer 2 4 motivation for self organizing maps in post mordm in summary mordm based decision support for environmental systems is characterized by many policies three multi dimensional data layers dvs objectives and robustness and negotiation between dms to overcome foundational disagreements previous studies have demonstrated the utility of clustering and dimension reduction techniques applied to dv and objective layers revealing two research gaps for this study first the robustness layer should also be considered in clustering and dimension reduction applications second decision support via mordm data layers also needs to address the foundational disagreements between dms described above providing a structured visual and easily interpretable methodology that encourages the discussion and negotiation needed to identify compromise policies to address these gaps we introduce the post mordm framework which augments mordm via a novel implementation of the self organizing maps som the som is a type of artificial neural network wherein the neurons learn the cluster structure and feature space patterns of a multivariate data set kohonen 1982 we apply the som to mordm data layers providing the benefits of policy clustering and dimension reduction in a single algorithm kohonen 1990 clark et al 2020 moreover the som enables dms to simultaneously visualize dv objective and robustness layers with an intuitive map like visualization of the relationships within and between each layer the post mordm framework builds on previous som applications by expanding to robustness a decision relevant data layer for human environmental systems previous studies in the mechanical engineering domain have trained a som on an moea objectives layer then used the resulting som to visualize the inverse relationship to the dv layer example design problems include aircraft wings obayashi and sasaki 2003 automotive tires koishi and shida 2006 mosnier et al 2013 and switched reluctance machines zhang et al 2018 in contrast environmental decision problems often consider objectives dvs and robustness because they are characterized by deeply uncertain hydro climatic and socioeconomic factors herman et al 2014 2015 quinn et al 2018 gold et al 2019 li and kinzelbach 2020 therefore we build on these previous moea som studies by expanding the som to the robustness layer further the post mordm framework implements the som to guide dms through a process of policy negotiation and compromise in the published moea som studies the purpose has been to support a single analyst or organization in the choice of a design conversely environmental decision problems often involve multiple dms who reflect the interests of local state or federal government environmental ngos and various other interest groups reclamation 2007 2012 wheeler et al 2018 molina perez et al 2019 the post mordm framework uniquely implements the som as a discussion and negotiation platform for multiple dms using the som to navigate to compromise policies moreover we desire the som to reduce the number of alternatives that dms consider from several hundred policies to a reasonably small number of neurons in this context a neuron is a model that represents one or more policies similar to a cluster the size of the som i e the number of neurons is determined by the user previous som applications have used a som with several hundred neurons which is often determined by heuristics that calculate the number of neurons given the sample size of the user s data set kohonen 2001a chap 3 obayashi and sasaki 2003 koishi and shida 2006 mosnier et al 2013 clark et al 2020 in contrast the post mordm framework uses the smallest som possible to adequately represent the cluster structure of mordm data layers see section 3 1 1 the result is a som that summarizes hundreds of moea policies with a relatively small number of neurons to reduce the cognitive burden faced by dms 3 self organizing maps and the post mordm framework this section is organized as follows we define the som algorithm in section 3 1 then describe the essential attributes of som via an example with dv and objective layers we describe the post mordm framework in sections 3 2 3 5 3 1 self organizing maps som a som is a type of two dimensional artificial neural network used for feature extraction clustering and topologic visualization of multidimensional data sets with many samples or points kohonen 1990 2001a 2013 clark et al 2020 a som consists of an interconnected grid of neurons where a neuron is a prototype data point each neuron is defined by a vector of values one value for each feature of the data each neuron is parametrized with an integer based coordinate pair that identifies the location of the neuron within the grid and the topologic or neighborhood based relationships between neurons kohonen 2001a hastie et al 2009 chap 14 4 during the training process the neurons are iteratively updated to better represent the multi dimensional structure of the input data meanwhile maintaining their topologic relationships to each other within the grid after training the data points are projected onto the two dimensional grid resulting in a topology map that visualizes the data s cluster structure and most salient patterns clark et al 2020 in this paper we describe the steps of creating a som in two categories pre training setup and the neuron update function 3 1 1 pre training setup before training a som the user must normalize or scale the features of the input data so that differences in their magnitudes and variances do not bias the training process next the length width ratio of the som is calculated such that it represents the shape of the data to accomplish this the user sets the length width ratio equal to the ratio of the first and second eigenvalues of the data s correlation matrix kohonen 2001a clark et al 2020 in effect this process allocates proportionally more neurons to the som along the direction of the data s feature space with the most variance after establishing the length width ratio the user chooses the total number of neurons and several hyperparameter values both of which requires training multiple soms and evaluating quality of fit metrics first to establish a practical upper and lower limit on the number of neurons to test the user estimates the number of clusters that best represents the intrinsic cluster structure of the input data this process can be performed via the k means clustering elbow method which requires calculating a cluster quality metric such as the silhouette or davies bouldin index for 1 to kmax k means clusters the user then identifies k such that larger numbers of clusters exhibit sharply diminishing marginal improvement in cluster quality hastie et al 2009 chap 14 3 rendón 2011 xiao et al 2017 clark et al 2020 the user then tests multiple soms of different sizes ranging from a lower and upper limit centered around k where the limits are based on the computational and time constraints of the user second the user sets the hyperparameter values hyperparameters include neighborhood radius neighborhood function distance function and edge neuron behavior we describe these hyperparameters in appendix a1 the user can also decide between a rectangular or hexagonal grid structure but in this study we use a hexagonal topology because they tend to outperform rectangular grids both in terms of visualization and quality of fit kohonen 2001a clark et al 2020 the user selects the number of neurons and hyperparameter set based on the tradeoff between two fit metrics percent of variance explained pve captures the degree to which neuron prototype vectors represent the input data topographic error te measures the degree to which the mapping of input data onto the two dimensional map preserves the many dimensional data structure clark et al 2020 boelaert et al 2021 for equations and further descriptions of these metrics see appendix a2 pve and te conflict where pve improves and te worsens with an increasing number of neurons thus the user tests multiple soms with different map sizes and hyperparameter sets then makes a selection that balances the metrics once the number of neurons and the hyperparameter values are set the user initializes the neurons by uniformly aligning them along the plane formed by the first and second principal components of the data s feature space principal component one pc1 is the linear projection of the feature space along which the data varies the most and the direction along which the longer edge of the som is aligned principal component two pc2 is orthogonal uncorrelated to pc1 and indicates the second greatest mode by which the data varies hastie et al 2009 chap 14 5 james et al 2013 chap 10 2 in this paper all visualizations of the som will be oriented such that pc1 and pc2 are aligned with the horizontal and vertical directions respectively because som neurons are initialized along pc1 and pc2 the resulting topology map can be navigated interpreting movement along the topology map according to the contributions of each feature to the pcs for practical guidance on all pre training steps described above see the code included in appendix a5 3 1 2 som update function during som training the neurons are iteratively fit to the data points via an update function whereby individual neurons compete to win data points and neurons within neighborhoods cooperate to win data points the neighborhood of a neuron is defined to be all neurons within a user defined neighborhood radius measured in two dimensional map space at every iteration each data point is assigned to its best matching unit bmu which is the neuron closest to the data point measured in data space according to a distance function neurons compete to be the bmu of each data point while also cooperating with neurons within their neighborhood via an update function the update function awards neurons and their neighbors by moving them closer to the data points effectively the grid of neurons is bent twisted and stretched from its original position on the principal component plane to better represent the non linear patterns of the data hastie et al 2009 chap 14 4 clark et al 2020 we employ the batch version of the som update function because it converges faster than the stepwise recursive function and has no random component kohonen 2013 for further information on the batch update function see appendix a1 after training iterations are complete the final step in creating a som is assigning the data points to their bmu clark et al 2020 the assignment of data points to neurons is analogous to k means clustering where neurons are akin to cluster centroids hastie et al 2009 chap 13 2 james et al 2013 chap 10 3 raseman et al 2020 importantly som neurons are arranged based on their similarities because of the neighborhood based cooperation during training clark et al 2020 for practical assistance in creating a som several packages are supported in r wehrens and kruisselbrink 2019 boelaert et al 2021 python smith 2021 vettigli 2021 and matlab cluster with self organizing map neural network matlab simulink no date after creating the som the user visualizes it on a topology map which is created by plotting the data onto the som s two dimensional grid within the topology map neurons close to each other are more similar than neurons far apart moreover the most significant data patterns along the horizontal and vertical dimensions of the topology map can be interpreted via the relative contribution of features to pc1 and pc2 clark et al 2020 in the next section we demonstrate a som applied to moea derived dv and objective layers we use the illustration to describe the benefits of the som and to introduce the plot types used in the post mordm framework 3 1 3 example som on moea created policies to demonstrate the benefits of the som we provide an example in fig 2 fig 2a shows moea derived objective values symbolized with a pa plot each vertical axis is an objective f i i 1 2 m where m is the number of objectives and each trace corresponds to one policy using the som the m dimensional objective space is summarized with a two dimensional topology map which we demonstrate with fig 2b in the topology map each neuron is represented by a radar plot and each axis of the radar plot is an objective pc1 summarizes the largest variations in objective values and the correlation between objectives for example moving from left to right in fig 2b f 1 and f 3 decrease and f 2 and f m increase from bottom to top f 2 decreases and f m increases slightly the objectives demonstrate greater variance along pc1 compared to pc2 which is determined via their eigenvalues but can also be observed visually for instance contrast the neuron on the bottom left to the neuron on the bottom right the blue surface area is markedly smaller then compare the bottom left neuron to the top left neuron the surface area is also smaller but with relatively less change thus pc1 is allocated four neurons compared to three neurons for pc2 to better capture the larger variation of objective values by utilizing principal component based dimension reduction som summarizes the most important patterns of the objective layer in two dimensions and creates a navigable visualization we use fig 2b to demonstrate the clustering of policies and the intuitive arrangement of clusters provided by som fig 2 symbolizes the clustering process via lines connecting fig 2a to neurons in fig 2b effectively som reduces the number of alternatives a dm would consider from the hundreds of moea derived policies to the number of neurons in the som moreover neurons close together in the topology map are more similar than those far apart for instance compare any two adjacent neurons in fig 2b the shape of the blue area is more similar than that of neurons on opposite ends of the map effectively som both clusters policies and arranges the clusters based on their similarities som topology maps are also a powerful tool for synthesizing and visualizing the relationships between various data layers we demonstrate the visualization of a dv layer in fig 2c each hexagon is one neuron and within each neuron are dvs the value of the dv is indicated by the position of the circle on its axis note that fig 2b and c are one and the same som meaning the assignment of each policy to a neuron and the location of the neurons are the same but they visualize two different mordm data layers therefore any patterns observed in the objective layer can be related to patterns observed in the dv layer for example we discussed earlier that f 2 decreases top to bottom in fig 2b in fig 2c x 2 decreases from top to bottom thus the conclusion is that decreasing the dv 2 results in the decrease of objective f 2 in the next section we describe how the post mordm expands on previous som applications to also visualize the robustness layer 3 2 the post mordm framework to implement post mordm three data layers are needed dv values performance objective values and robustness metric values fig 3 demonstrates the workflow of post mordm which we describe in the following subsections 3 2 1 train som on performance objective layer the first step of post mordm is training a som on the objective layer of a set of policies as described in section 3 1 1 creating the som first requires normalizing or scaling each objective calculating the length width ratio determining the number of neurons setting hyperparameter values initializing the neurons along pc1 and pc2 then implementing the training algorithm we recommend testing multiple soms with different sizes and hyperparameter values then selecting the smallest som that achieves sufficient pve and te after training the objective layer is plotted on the som in this paper we denote the mordm data layer being visualized with a subscript for example somobj shows the topology map of the objective layer fig 3 1 each neuron in somobj is depicted with a radar plot which we described in section 3 1 3 3 2 2 analyze inverse relationships to decision variables next the dv values for each neuron are visualized via somdv shown in fig 3 2 this visualization is the result of training som on the objective layer then projecting the dv layer onto the som we use the visual patterns in somdv and the principal components of somobj to investigate the relationships between the objective and dv layers as discussed earlier in section 2 3 2 with fig 2 b c in fig 3 we summarize each dv with a single value per neuron which could be the mean or median value however alternative visualizations that show the dv values of each policy assigned to the neuron see section 4 3 2 or a visualization of their distribution box plots violin plots etc can also be used 3 2 3 superposition robustness metrics after establishing somobj and somdv the dms define their robustness metrics of choice this includes the type of robustness metric such as regret or satisficing any performance objectives and corresponding thresholds and any considerations of risk tolerances that result in unique definitions of robustness see section 2 1 for a review of robustness metrics and guidance on selecting them we refer the reader to mcphail et al 2018 2021 we denote each dm and their robustness definition with an index 1 2 n because the som is trained on the objective layer not robustness layer dms can change or modify robustness metrics without needing to retrain the som we then superposition each unique robustness metric onto a topology map shown in fig 3 3 resulting in n robustness visualizations collectively called somrobust the creation of somrobust is similar to the visualization of dv with somdv the som was trained on the objective layer after which the robustness layer is superpositioned onto it in fig 3 3 we plot each dm s robustness value on an individual topology map coloring each hexagon by the robustness value averaged over policies assigned to each neuron in the som literature topology maps that show only one feature are called component planes clark et al 2020 the relationships between mordm data layers are explored visually and via the pcs of somobj for example we discussed earlier that moving from bottom to top dv x 2 decreases fig 3 2 resulting in the decrease of objective f 2 fig 3 1 now consider somrobust darker neurons represent better robustness values so when x 2 and f 2 both decrease this is related to decrease in robustness for dm1 increase in robustness for dm2 and decrease in robustness for dm n in a negotiation context each dm is presented their unique robustness visualization which they use collectively with somobj and somdv to identify their preferred neuron s in the example illustrated in fig 3 3 dms 1 2 and n maximize their individual robustness preferences in the lower left top right and bottom right neurons respectively we have highlighted each dm s preferred neuron with green circles blue squares and pink arrows respectively 3 2 4 navigate som to compromise neurons to encourage discussion negotiation and compromise we establish a topology map that is shared between the dms in fig 3 4 we begin with a colorless topology map meaning the neurons are not colored by robustness values but the assignment of policies to neurons and the position of neurons are the same as somobj somdv and somrobust then we project the dms robustness preferences established in fig 3 3 onto the colorless topology map establishing a shared negotiation platform called somnegotiation when the dms preferred neurons are located far apart this indicates conflicting preferences in the weighing of objectives dvs and robustness for example dm1 prefers the lower left neuron which is characterized by policies with high values of x 1 and f 3 but in contrast dm2 prefers the top right neuron where policies have a small x 1 and f 3 to negotiate and compromise dms navigate from their individual preferences towards a neuron between dms the tradeoffs of which are interpreted via somobj and its pcs somdv and somrobust for instance the example dms in fig 3 4 negotiate to a neuron that requires each to compromise a similar amount for dm1 this requires a decrease in objectives f 1 and f 3 left to right along pc1 decrease in f 2 and increase in f m moving upward along pc1 considering dvs the compromise neuron implements less of dvs x 1 and x 2 and more of x l dms may negotiate to a single neuron of mutual interest or several neurons of mutual interest in either case the number of policies under consideration is significantly reduced and the neurons can now be investigated further by analyzing the individual policies within them in the next section we provide an example with a case study of reservoir operation policy in the colorado river basin 4 post mordm case study reservoir operation policy in the colorado river basin 4 1 motivation the colorado river basin crb supplies municipal water for nearly 40 million people in seven us states basin states 29 federally recognized tribes and northern mexico the crb is a significant source of hydropower producing about ten billion kilowatt hours of electricity annually enough to power one million us households reclamation 2018b 2021c frequently asked questions faqs u s energy information administration eia 2020 moreover crb surface water is the primary source for the basin states agriculture sector which is responsible for 70 of the crb s consumptive use and losses reclamation 2018a the crb is regulated according to the law of the river a compilation of compacts treaties federal law and court decisions dating back to 1922 reclamation 2015 pursuant to the law of the river the basin states are divided into the upper basin ub colorado wyoming utah and new mexico and the lower basin lb arizona nevada and california divided by a streamflow gauge at lees ferry arizona each basin is allocated 7 5 million acre feet maf annually for consumptive use of which the ub is yet to fully utilize in addition mexico is allocated 1 5 maf totalling 16 5 maf basin wide during the 21st century persistent drought in the crb has exacerbated the risk of temporary or prolonged interruptions in water supplies buschatzke et al 2019 average annual streamflow has dwindled to 72 of the historical average lukas and payton 2020 and as of november 2021 system reservoirs are filled to only 38 of full capacity reclamation 2021b potential consequences of this drought include lb shortages curtailments of ub consumptive use and critically low reservoir levels which can also diminish hydropower production recreational services and environmental benefits reclamation 2007 in an effort to minimize these risks the us bureau of reclamation reclamation the basin states and mexico have recently legislated multiple shortage operation policies for lake mead the largest reservoir in the system the 2007 interim guidelines guidelines defined the pool elevations and corresponding volumes by which deliveries to the lb would be reduced during times of low reservoir levels i e shortage volumes further the guidelines dictate how lake powell the upstream reservoir from lake mead and the second largest in the system would be operated in coordination with lake mead reclamation 2007 three lake mead policy alternatives were considered for the guidelines two alternatives prioritized water delivery and storage respectively and the third alternative which was selected incorporates operational elements from both of the other two alternatives reclamation 2007 p 8 after 2007 drought persisted and reservoir levels continued to decline therefore crb stakeholders later augmented shortage volumes established in the guidelines via minute 323 between the us and mexico and the lb drought contingency plan dcp collectively the guidelines minute 323 and the lb dcp establish the cumulative shortage operations in effect at the time of writing this paper international boundary and water commission 2017 colorado river basin drought contingency plans bureau of reclamation 2019 although these policies differ in terms of whether or not users can recover delivery reductions when reservoir storage increases all policies functionally decrease the risk of pool elevations at lakes mead and powell declining to critically low levels we provide an overview of the lake mead shortage operations in fig 4 the projected pool elevation for january 1st of the coming year is shown on the y axis in feet above mean sea level msl the pool elevation determines the volume of water by which downstream deliveries are reduced for the calendar year i e the shortage volume as indicated by color the guidelines minute 323 and the lb dcp are ordered chronologically by year of implementation along the x axis and the cumulative shortage operation is shown on the right the policies expire december 31st 2025 thereafter a new policy will take effect in this case study we contribute to the negotiation of new lake mead shortage operations beginning in 2026 first we employ moea to identify a set of non dominated lake mead shortage operations and quantify objective values then we calculate multiple robustness metrics to reflect the conflicting interests of storage and delivery stakeholders finally we use post mordm to demonstrate a process of learning negotiation and compromise between two illustrative dms 4 2 implementation of mordm our mordm problem formulation is summarized in table 1 we describe the dvs simulation model and objectives when discussing moea optimization next we describe the sources of uncertainty and the methods for creating the sow ensemble then we define the robustness metrics of two example dms in the crb before demonstrating post mordm 4 2 1 policy alternatives we use a policy set adapted from alexander 2018 provided by reclamation policies were generated with the borg moea hadka and reed 2013 which was coupled with the colorado river simulation system crss a hydro policy model built in riverware that serves as reclamation s long term planning model for the crb zagona et al 2001 the simulation is 44 years long and uses a monthly timestep evaluating eight performance objectives the objectives quantify tradeoffs between ub and lb interests and delivery vs storage objectives see table 1 for definitions borg seeks to minimize the objectives by adjusting 14 dv table 1 right 12 dvs control shortage operations of which six define the pool elevations where shortage operations begin t1e t6e and six are the corresponding shortage volumes subtracted from lb deliveries t1v t6v the remaining two dvs are the elevations at which surplus operations begin but this case study will discuss shortage operations only the result is a set of 463 policy alternatives the objectives and tradeoffs of which are explored via post mordm in section 4 3 4 2 2 robustness analysis 4 2 2 1 sow ensemble generation our robustness analysis considers three sources of uncertainty 1 annual cumulative natural flow above lees ferry arizona 2 annual consumptive use in the ub which is sampled for each simulation but held constant with respect to time and 3 initial reservoir pool elevations at lake mead and lake powell this section describes how we sampled the uncertainty with a 500 member sow ensemble the result of which is shown in appendix a4 we considered four hydrology ensembles historically used by reclamation and thus familiar to crb dms reclamation 2007 2012 2018a groves et al 2013 1 the observed resampled ensemble is the result of the index sequential method ism applied to the observed 1906 2007 cumulative natural flow record 2 the global climate model gcm ensemble is based on bias corrected and spatially downscaled cmip3 climate projections of future high medium and low emission scenarios run through the variable infiltration capacity vic model 3 the paleo resampled ensemble applies the ism method to paleo reconstructions dating 762 to 2005 4 lastly the paleo conditioned ensemble uses a non parametric technique to blend the wet dry sequences from the paleo record with magnitudes from the observed record in sum there are 1963 streamflow traces that describe the envelope of hydrologic uncertainty consistent with the philosophy of mordm we use the traces to broadly sample the hydrologic uncertainty space as described below for more information on the ensembles we refer the reader to the 2012 crb supply and demand study reclamation 2012 next we created a 1000 sample latin hypercube lh of annual ub consumptive use initial pool elevation at lake mead and initial pool elevation at lake powell annual ub consumptive use ranges from 4 2 to 6 0 maf which considers both curtailments and growth for comparison in 2016 the upper colorado river commission estimated consumptive use at 4 33 maf and forecasted 5 22 maf in 2060 2016 initial reservoir levels consider lakes mead and powell because their combined storage accounts for about 87 of the entire system reclamation 2021a sampling ranges were informed from the 10th low end and 90th high end percentile values from reclamation s april 2020 five year projections rounding the low end down to the nearest 50 feet reclamation 2020 thus powell s initial pool elevation ranges from 3450 to 3675 feet above mean sea level msl and mead ranges from 1000 to 1185 feet msl the pool elevation projections end december 2026 accounting for the range of possible pool elevations at the expiration of the current operation policy after creating the lh we combine every sample of pool elevations and ub consumptive use with every hydrology trace to create a large set of sow from which to select a subset for robustness simulations to reduce computational costs we sample a subset of 500 sow using conditioned latin hypercube sampling clhs which is an extension of latin hypercube sampling lhs minasny and mcbratney 2006 instead of creating new multivariate samples that form a lh clhs employs an optimization algorithm to select existing observations that form a lh in the multivariate feature space while mimicking the distributional properties of the original population minasny and mcbratney 2010 brus 2019 roudier 2020 practically clhs allowed us to use existing hydrology traces our existing observations select a subset of sow with minimal repeats of hydrology traces and preserve the desired uncertainty ranges 4 2 2 2 decision maker robustness metrics our robustness analysis uses two illustrative dms with conflicting preferences namely delivery and storage delivery is of greatest concern to the lb since lb allocations are 100 utilized for irrigation municipalities groundwater recharge and all other uses the storage dm reflects hydropower interests at lake powell and lake mead moreover storage is of special concern for shoreline recreational services like boat ramps and marinas reclamation 2012 the robustness preferences of delivery and storage are both quantified with the satisficing metric satisficing is the fraction of sow where a policy satisfies minimum performance thresholds defined by the dm satisficing ranges from 0 performance thresholds satisfied in zero sow to 1 performance thresholds are satisfied in 100 of sow the performance requirements for delivery are shown in condition 1 delivery performance requirements lb avg 600 kaf and lb dur 10 years lb avg is the annual shortage volume in the lb averaged over the simulation lb dur is the maximum consecutive years the lb is in shortage conditions see table 1 thus delivery s performance thresholds require both acceptable average magnitudes and maximum duration of shortages the performance requirements for storage are shown in condition 2 storage performance requirements m 1000 10 and p 3490 5 m 1000 is the percentage of simulation months where lake mead s pool elevation is below 1000 feet msl and p 3490 is the percentage of simulation months where lake powell s pool elevation is below 3490 feet msl together the performance thresholds for storage require that both lake mead and lake powell consistently stay above critical pool elevations the dms performance thresholds are used to calculate satisficing for each policy and dm according to the equation s a t i s f i c i n g i p 1 s j 1 s g i j x p where s is the total number of sow j is a sow index x is the decision variable vector for policy p and g i j is an indicator function g i j 1 if the performance thresholds of dm i delivery or storage are satisfied in sow j and g i j 0 otherwise 4 3 implementation of post mordm 4 3 1 training the som on the objective layer after normalizing the objectives we selected the number of neurons and som hyperparameter values using a grid search of 1000 lh samples the tested hyperparameters include neighborhood radius neighborhood function distance function and edge behavior the number of neurons in length width directions of the som was calculated given the number of total neurons sampled in the lh and the calculated ratio of the first and second eigenvalues of the objective layer we chose a hexagonal neighborhood topology according to the recommendation of clark et al 2020 we evaluated the number of neurons and each hyperparameter set with pve and te som training was performed in r using the kohonen package wehrens and buydens 2007 wehrens and kruisselbrink 2018 2022 and fit metrics were calculated using the awesom package boelaert et al 2021 r core team 2021 the selected som is 5 3 neurons because larger maps achieved small increases in pve while smaller ones saw significant decrease in pve in both cases attaining similar te for additional details on the training process the number of neurons to use and the selection of hyperparameter set including code see appendix a5 fig 5 shows somobj which reveals the two most important modes by which policies vary with respect to the objective layer within the radar plots each objective is scaled 0 center to 1 outer edge where 0 is ideal because they are minimization objectives every policy is plotted in its assigned neuron using a transparent blue fill to visualize the number of policies and the degree to which their objective values are similar for example consider neuron 6 middle row far left these policies result in low average and maximum shortages lb avg and lb max at the expense of frequent long duration shortages lb freq and lb dur they also perform poorly in water storage reliability at lakes powell and mead m 1000 and p 3490 the black borders around each policy overlay each other almost exactly indicating high similarity among the policies in contrast consider neuron 15 top row far right the policies have large average and maximum shortages however they perform comparatively well with respect to shortage frequency shortage duration and reservoir storage the borders around each policy are easy to distinguish from one another especially for lf deficit this indicates notable variation of performance for the policies assigned to neuron 15 for the interested reader appendix a6 reports the average objective value in each neuron i e component planes to facilitate the interpretation of the pcs we include the loading scores of each objective shown in the bar plots loading scores quantify the degree to which each objective contributes to the pc where larger magnitudes mean the objectives contribute more to the change in performance james et al 2013 chap 10 2 green bars indicate decreasing values from left to right increase in performance whereas gold dashed bars indicate increasing values decrease in performance for example consider pc1 left to right p 3490 p wyr and m 1000 are improving but lb avg and lb max are worsening the other objectives have relatively little to no change in the horizontal direction moving from bottom to top along pc2 lb freq and lb dur are decreasing while lb max is increasing in other words the two most significant tradeoffs for the dms to navigate is first the tradeoff between reservoir storage reliability and lb shortage magnitudes average and maximum and second the tradeoff between shortage duration frequency and maximum magnitude 4 3 2 lake mead operation policies after we establish somobj we visualize the inverse relationship to lake mead dvs with somdv shown in fig 6 each vertical bar represents a lake mead policy where the y axis is water surface elevation and the colors show the magnitude of shortage the number of policies in each neuron is shown to the right of the neuron index in parentheses for neurons with more than 20 policies 20 policies are plotted at random to conserve space for simplicity the policies are ordered randomly along the x axis but could be ordered according to a dv robustness metric or objective to facilitate comparisons between neurons and between mordm data layers we report the tier 1 elevation t1e also indicated by the horizontal dashed line in feet msl and volume of the first shortage t1v kaf plus maximum shortage maxv kaf averaged across all policies in a neuron fig 6 shows salient patterns in shortage volumes and t1e corresponding to the pcs of somobj from left to right t1v maxv and t1e tend to increase this is shown by the gradient of green yellow red and the increasing bar height comparing to somobj in fig 5 the result is improved reliability of reservoir storage the tradeoff being increased average and maximum shortages from bottom to top t1e tends to decrease especially to the left side of somdv t1v and maxv also tend to increase the related performance outcome is decreasing frequency and duration of shortages at the expense of increasing maximum shortage interestingly t1v and maxv increase both left to right and bottom to top but the frequency and duration of shortages experienced by the lb responds almost exclusively in the vertical direction as indicated by loading scores of near one for the vertical pc compared to loading scores less than 0 25 for the horizontal pc considering that t1e tends to increase left to right but decreases bottom to top we conclude that reducing the frequency and duration of lb shortages meanwhile achieving reliable reservoir storage requires that larger lb shortages be paired with lower pool elevations this is the difference between neuron 15 and 5 both implement large shortage volumes an average t1v volume of 1825 and 1843 kaf respectively but policies in neuron 15 achieve less frequency and duration of lb shortages via being more patient waiting until lower t1e 1065 vs 1092 ft msl to implement the first shortage this difference in operation philosophy results in shortage conditions occurring in 12 01 less simulation months in neuron 15 compared to 5 not shown in fig 5 see appendix a6 4 3 3 decision maker robustness maps fig 7 a b shows the individual robustness values of the delivery and storage dms somrobust the color and label of each neuron indicates the average satisficing of the policies assigned to each neuron where darker colors are preferred clear topologic patterns exist demonstrating the effectiveness of robustness metrics superpositioned on a som fit to the objective layer in this example we present two satisficing robustness metrics moreover we demonstrate the effectiveness of this method using te and pve for three additional metrics in appendix a7 a13 for delivery neurons in the top left are the most robust with satisficing decreasing left to right on the top row decreasing robustness comparing to somdv in fig 6 the policies that result in the best delivery robustness do so by two operational strategies first consider neuron 11 these policies implement small shortage volumes maxv of 650 kaf at low pool elevations t1e less than 950 feet in all but one policy draining lake mead with minimal storage reliability safeguards alternatively a policy can be robust by delicately balancing shortage volume and elevation neurons 12 14 these neurons implement moderate to severe t1v 1385 1720 kaf at moderately high elevations t1e from 987 to 1028 feet msl other neurons do not exhibit this balance for example consider neurons 1 and 6 these policies implement small t1v shortage volumes 196 345 kaf implemented at moderate to high elevations 1039 1056 feet these neurons are not robust likely because the shortage volume is not large enough to raise lake mead s pool elevation out of shortage operations resulting in durations of lb shortage exceeding 10 years contrast neuron 15 top right to neuron 5 bottom right both implement large t1v shortage volumes 1825 and 1843 kaf respectively but neuron 15 policies wait until lower pool elevations to begin shortages t1e of 1065 vs 1092 feet msl satisfying delivery s performance thresholds in 10 more sow on average by exploring the relationships between somrobust and somdv the delivery stakeholder can identify policies that achieve this balancing act to satisfy both average shortage and shortage duration performance thresholds for storage neurons to the far right are the most robust and satisficing decreases towards the left in each row considering somdv in fig 6 the policies in neurons 5 10 and 15 have the most aggressive shortage operations implementing large t1v and maxv 1318 2182 kaf at high pool elevations t1e from 1064 to 1092 feet msl decreasing robustness is caused by smaller shortage volumes beginning at lower pool elevations in a negotiation context each dm is provided with the topology map showing their unique robustness definition upon which they identify neurons with preferred and unacceptable performance in fig 7 we define preferable performance as greater than 0 70 blue unacceptable below 0 5 dashed red and for neurons from 0 5 to 0 7 the dms have weak preferences then we project the intersection of their preferences onto a blank topology map shown in fig 7 c one neuron presides in both dms preferred areas shown in solid purple this is an intuitive neuron for both dms to investigate further however the adjacent neurons lie within either the preferred or weak preference areas of both dms thus we consider these neurons a feasible negotiation space 4 3 4 negotiation navigation next we visualize only the neurons defined to be inside the feasible negotiation space this enables the dms to investigate further the dv objective and robustness layers of mutually feasible policies shown in fig 8 here we have replaced the robustness topology maps from fig 7 with boxplots providing information on the spread of robustness in each neuron delivery is shown in orange and storage is shown in blue we have provided the boxplots of all neurons in appendix a14 neuron 14 mutually intersects both dms preferred areas however additional negotiation can be facilitated with fig 8 for example delivery may try to negotiate leftward to neuron 13 improving their satisficing score and avoiding comparative disadvantage to storage blount and bazerman 1996 tsay and bazerman 2009 in this neuron shortage volumes are smaller and implemented at lower pool elevations contrarily storage may negotiate to neuron 15 to increase robustness meaning that pool elevations and shortage volumes would increase the map based visualization of multiple mordm data layers such as demonstrated in fig 8 can also help dms overcome cognitive myopia in a negotiation context cognitive myopia can occur when the expressed interests of a dm such as their definition of robustness or weighing of objectives limits the exploration of mutually feasible policy alternatives kasprzyk et al 2013 giuliani et al 2014 for example when considering only robustness it does not appear that either dm would individually desire moving downward to neurons 9 or 10 because moving horizontally yields the greatest individual increases in satisficing however the consideration of objective and dv layers reveals other reasons these neurons may be of interest for instance lb max is improved by moving downward to neurons 9 and 10 perhaps the satisficing performance thresholds on lb avg and lb dur reflect delivery s highest and most well defined priorities but other findings like this could make these neurons interesting if so delivery may negotiate to neuron 10 which implements smaller shortage volumes at higher elevations storage may accept neuron 10 because their satisficing value is mostly unaffected by simultaneously visualizing multiple mordm data layers via topology maps dms are encouraged to explore policy alternatives they might not otherwise which can help dms negotiate to a compromise policy 4 4 robust shortage policies vs existing lake mead operations the combined lake mead shortage operation fig 4 does not resemble the neurons within the feasible negotiation space of delivery and storage fig 7 this section describes how post mordm can explore the potential reasons why and the consequences thereof compared to the feasible neurons the combined operation implements t1e at too high of an elevation with too small of a t1v further maxv of the combined operation is small in comparison to the feasible neurons except neuron 9 notably the combined operation resembles neuron 9 if the combined operation s shortage volumes of less than 1000 kaf are removed without these smaller shortage volumes t1e t1v and maxv of the combined operation are similar to the average values of neuron 9 1045 feet msl 1013 kaf 1375 kaf compared to 1046 feet msl 1009 kaf 1341 kaf by navigating somdv somobj and somrobust we can explore the potential consequences of these relatively high elevation low volume shortage operations used in the combined operation in this comparison we will use neuron 3 to represent the combined operation because of similar t1e t1v and maxv for a detailed comparison of neuron 3 and the combined operation please see appendix a15 using somdv fig 6 traverse from neuron 9 to neuron 3 t1e increases by 36 feet t1v decreases by 245 kaf and maxv decreases by 39 kaf this change in dv values resembles the difference between neuron 9 and the combined operation increasing t1e decreasing t1v and similar maxv the result of moving from neuron 9 to neuron 3 as indicated by somobj fig 5 is greater frequency duration and average volume of shortages the increased frequency duration and average volume of shortages explains why the policies within the dms feasible negotiation space are dissimilar to the combined operation policies that combine high t1e with small t1v fail to satisfy delivery s robustness criteria which requires that the maximum duration of shortage not exceed 10 consecutive years and that average shortage volume not exceed 600 kaf this conclusion is consistent with somrobust which shows that these policies satisfy both of delivery s performance criteria in only 44 of sow fig 7 to summarize the combined operation implements relatively small shortages at high pool elevations and our results suggest that this operational strategy can result in high frequency duration and average volume of shortages unfavorable to water users in contrast policies that begin shortages at lower elevations and larger volumes can reduce the frequency duration and average volume of shortage with the tradeoff being larger maximum shortages therefore this tradeoff should be emphasized during the renegotiation of lake mead shortage operations to solicit feedback from stakeholders further these results highlight that dms in the crb and other river systems facing deep uncertainty need to consider which types of water reductions high frequency vs high magnitude lend themselves towards more sustainable agriculture and public acceptance 5 discussion in sections 3 and 4 we have introduced the post mordm framework then demonstrated it with a case study of reservoir operation policy in the following discussion we describe several ways that post mordm allows for application specific flexibility meanwhile highlighting best practices then we discuss how flexibility in post mordm creates ample opportunity for future research to build on the case study presented in this paper 5 1 flexibility and best practices with post mordm one goal of the post mordm framework is reducing the number of alternatives that dms need to consider in section 4 3 4 dms negotiated between five neurons where each neuron summarizes the key attributes of a group of similar policies this number of neurons is consistent with the psychology literature that suggests 3 9 alternatives be ideally considered at one time see section 2 2 however in applications with more than two dms and or a large number of feasible neurons 10 or more neurons may be needed to represent the negotiation space after identifying one or more compromise neurons dms may want to consider the individual policies assigned to them however the number of policies assigned to each neuron may exceed 9 it does in section 4 3 4 in this case additional steps can be taken to reduce the number of policies if so desired for example policies can be filtered by dv objective or robustness values alternatively it is possible to reduce the number of policies the som is trained on by applying filters before implementing post mordm doing so could reduce the number of policies assigned to each neuron result in neurons that are better approximations of the policies and result in neurons that are more distinct from each other for instance the illustrative dms in our case study defined nine neurons as unacceptable which suggests that some policies could have been removed before training the som in other cases however defining the criteria by which policies are filtered may be non trivial and not agreed upon by dms thus we advocate for a posteriori defining of filter criteria if any based on exploration of topology maps in the post mordm framework we have used robustness values as the data layer which dms use to identify their policies of interest and the starting point for ensuing negotiation we encourage this method because robustness metrics take into account critical driving forces of the system that are characterized by deep uncertainty alternatively dms could express their preferences based on the objective or dv layer or a combination of layers we advocate that dms identify their preferred neurons beginning with the robustness layer however the post mordm framework allows for such flexibility the post mordm framework is also flexible with respect to the types of plots used in topology maps in our case study we have created topology maps with radar plots fig 5 reservoir operation diagrams fig 6 component planes fig 7 and appendix a6 and box plots fig 8c alternative visualizations can be used based on the application or preferences of dms and future research can explore alternative methods for visualizing the intra neuron variance besides the radar plots and box plots used in this case study regardless of the plot type we believe it is critical to maintain the topologic arrangement of neurons produced by the som and to use easily interpretable visualizations that facilitate the exploration of tradeoffs and the relationships between mordm data layers another fundamental goal of post mordm is to provide dms with a shared negotiation platform which we accomplish with navigable topology maps however we do not prescribe the exact topology map visualization type to be used in negotiation for instance in section 3 4 we facilitated dm negotiation with a blank topology map meaning the hexagons in somnegotiation were colorless except for the outlines that indicated the preferred neurons of each dm then the illustrative dms used somdv somrobust and somobj to interpret the results of moving on somnegotiation this plot type presents somnegotiation as a neutral topology map choosing not to visualize any objective robustness or dv values on it however we can imagine a circumstance where dms would benefit from including additional information on somnegotiation for instance an objective or robustness metric that represents a shared concern of the dms could be plotted to further encourage negotiation in the crb case study of the delivery and storage dms perhaps an environmental objective could be used we have demonstrated how post mordm facilitates negotiation and compromise via topologic visualization of multiple mordm data layers we do not define the exact procedure by which negotiation occurs instead post mordm can be implemented with formal negotiation rules that are application specific and agreed upon by the parties to a negotiation for example gold et al implement fallback bargaining to identify two compromise water portfolios in a case study of four interconnected water utilities 2019 building on the work of herman et al described in section 2 1 2014 under fallback bargaining dms first rank policies according to their individual preferences then dms consider the top ranking policy for each dm if they do not agree each dm falls back one level in their ranking of policies dms continue to fall back until there exists a policy that is acceptable to every dm and this policy is selected as a compromise brams and kilgour 2001 madani et al 2011 the post mordm framework could complement fallback bargaining by facilitating dms in the ranking of policies using post mordm dms could rank a tractable number of neurons as opposed to hundreds of policies in the context of moea further post mordm could help dms rank policies based on simultaneous consideration of the objective dv and robustness layers as facilitated with topology maps lastly dms could use somobj somdv and somrobust to track the tradeoffs resulting from each fallback step in the negotiation overall the post mordm framework provides navigable visualizations that promote understanding and negotiation but it does not prescribe a formal negotiation procedure thus post mordm could be used to enhance the efficacy of prescribed negotiation procedures such as fallback bargaining throughout this paper we have emphasized post mordm in negotiation contexts indeed we believe this to be a significant contribution to environmental modeling and decision support literature alternatively post mordm can be utilized by an individual analyst design group or organization because the benefits of post mordm clustering dimension reduction and map based visualization of multiple data layers can also help individual entities attain greater understanding of their system and make decisions 5 2 future research opportunities other data layers future research could implement post mordm to explore additional decision relevant data layers scenario discovery the last step in mordm is traditionally performed on a small subset of policies alternatively vulnerability information for each policy or a representative policy from each neuron could be calculated then displayed on a topology map then vulnerability topology maps could be visualized alongside somdv to analyze the relationships between dvs and vulnerability several recent publications have highlighted another potentially decision relevant data layer which is the degree to which robustness values are sensitive to statistical properties of the sow ensemble these properties include the number of sow the upper and lower bounds of the uncertain factors their correlation structure and their probability distributions hadjimichael et al 2020 reis and shortridge 2020 mcphail et al established a framework to quantify the sensitivity of robustness magnitude and robustness ranking 2020 which could be combined with the post mordm framework to identify dv and objective values that correspond to policies whose robustness is the most insensitive to sow ensemble design in this paper we have demonstrated the utility of the som for map like interpretation of objective dv and robustness layers and we discussed above how post mordm could be expanded to vulnerability and robustness sensitivity layers furthermore we believe the foundational methods and goals of post mordm could be implemented with layers relevant to decision support frameworks other than mordm of particular interest dynamic adaptive policy pathways dapp is another popular framework for systems faced with deep uncertainty because it frames policy implementation as conditional on future observations of uncertain factors haasnoot et al 2013 kwakkel and haasnoot 2019 p 359 like mordm dapp is characterized by several layers of decision relevant data where understanding the relationships between them is important to dms depending on how dapp is implemented these data layers can include long term policy decisions dynamic planning adaptive policy decisions short term contingency planning signpost variables signpost triggers and performance objectives haasnoot et al 2013 zeff et al 2016 the benefits of post mordm namely the simultaneous map based visualization of related data layers and a negotiation platform could also enhance the synthesis and communication of other decision support frameworks to dms 6 conclusion this paper presented the post mordm framework which enhances mordm based decision support via a novel implementation of the som post mordm constitutes an alternative paradigm for how policy relevant data is explored by interpreting mordm data as multiple layers arranged according to a two dimensional map system post mordm expands on previous applications of clustering dimension reduction and the som to 1 help dms elucidate the relationships between dvs objectives and robustness 2 reduce the number of alternatives dms need to consider and 3 establish a visual structured platform whereby dms with foundational disagreements are assisted in a process of negotiation and compromise we demonstrated post mordm with a case study of reservoir operation policy in the colorado river basin usa using a topology map of objective values somobj our results showed that the primary tradeoff dms need to navigate is the tradeoff between reservoir storage reliability and average magnitude of water delivery shortages the second strongest tradeoff is between frequency duration of shortages and maximum shortage magnitude we illustrated how topology maps can be used in a process of negotiation between two illustrative dms that represent delivery and storage interests in the crb based on individual definitions of robustness we showed that five neurons of policies could be mutually feasible and demonstrated how topology maps facilitate negotiation because of their map like navigation and interpretation we discuss how the combined lake mead shortage operation which is in effect until 2026 contrasts with the mutually feasible neurons because of a combination of high elevation low volume shortage tiers we used topology maps of dv and objective values somdv and somobj to describe how high elevation low volume shortage tiers increase the frequency and duration of delivery shortages while reducing the maximum shortage volume in the renegotiation of lake mead s shortage policy dms will need to consider which tradeoff the crb s diverse stakeholders can tolerate long persistent water shortages of smaller magnitude or less frequent shorter but harsher shortage magnitudes moreover future research should incorporate dvs for lake powell operations into moea optimization further investigating performance and robustness tradeoffs while maximizing the benefits of coordinated operation between lakes powell and mead the post mordm framework and the case study presented in this paper contribute to one of the grand challenges of the 21st century identifying policies for human environmental systems that balance competing objectives and are robust to uncertainty addressing the decision related challenges posed by deep uncertainty requires the integration of research across multiple disciplines therefore the post mordm framework is a demonstration of this integration pulling from research in the domains of machine learning engineering design psychology and water resources management in an effort to build a bridge between decision support systems originating in academia to dms our hope is that the post mordm framework will facilitate negotiation and compromise as decision support frameworks like mordm are increasingly implemented in real world applications moreover we believe this research offers an alternative paradigm through which tradeoff analyses and negotiations can occur encouraging future studies to expand upon our implementation of the som while also exploring other innovative approaches software and data availability all r code and data to reproduce the case study is available on github https github com nabocrb post mordm we have formatted the code to facilitate straightforward application of post mordm in other case studies including code for every step described in section 3 and the variety of topology maps used in this paper author contributions nathan bonham conceptualization analysis coding visualizations writing original draft joseph kasprzyk conceptualization writing review and editing supervision edith zagona conceptualization writing review and editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the bureau of reclamation for providing the lake mead objectives and decision variable data and the anonymous reviewers who contributed to the clarity and technical thoroughness of this article this material is based upon work supported by the national science foundation graduate research fellowship under grant no dge 2040434 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105491 
25512,climate matching allows comparisons of climatic conditions between different locations to understand location and species range climatic suitability the approach may be used as part of horizon scanning exercises such as those conducted for invasive species we implemented the climatch algorithm into an r package climatchr the package allows automated and scripted climate matching exercises across all steps from downloading data to summarizing species climate matches we also show how climatchr may be used with high throughput computing to process many species for example we were able to calculate climate scores for over 8 000 species in less than 3 days using this package this automation allows high throughput processing of species data a new development for improving the efficiency and speed of climate matching and horizon scanning keywords climate envelop invasive species risk assessment tool data availability no data was used for the research described in the article 1 software and data availability name of software and location climatchr available as a usgs software release erickson et al 2021a example htcondor application as a second u s geological survey usgs software release erickson et al 2021b required software availability and cost r r core team 2021 for climatchr high throughput computing uses htcondor and requires an htcondor pool see https htcondor org accessed 21 sept 2021 for guidance on obtaining htcondor both programs are free and open source required data we describe required data in detail in section 3 3 as a summary species names input data either as an input file such as a csv or data frame in r database of global administrative areas gadm files from https gadm org as rds files chelsa climate data from https chelsa climate org as tif files species location data from global biodiversity information facility gbif data downloaded by the package from https www gbif org for further instructions on how to run climatchr we refer the reader to the package s vignettes we have based these examples upon the vignettes as well 2 introduction climate matching provides a method for understanding species distributions and ranges furthermore climate matching may be used as part of horizon scanning the process of examining potential risks and threats to assess potential new invasive species sutherland and woodroof 2009 preventing new invasive species establishment requires less effort and fewer resources than attempting to control and eradicate existing infestations vander zanden and olden 2008 in invasion science horizon scanning allows resource managers and risk assessors to identify potentially new invasive species and subsequently to reduce the risk of invasion roy et al 2014 ricciardi et al 2017 as part of these efforts scientists and risk assessors often investigate the climatological similarities of existing species distributions and possible invasion areas during horizon scanning exercises and similar risk assessments e g hayes and barry 2008 bomford et al 2009 roy et al 2014 u s fish and wildlife service 2018 climate matching may be done before more labor intensive expert elicitation as part of scanning exercises current methods for climate matching can be labor intensive because they require users to hand select data and then manipulate the data across software programs e g u s fish and wildlife service 2018 however climate matching may be done relatively inexpensively and easily for large number of species because the process may be automated the climatch algorithm exists as one tool for conducting climate matching agencies such as the australian department of agriculture water and the environment crombie et al 2008 and u s fish and wildlife service usfws e g u s fish and wildlife service 2018 use climatch because validation studies have found the approach to be a consistent predictor of invasion risk hayes and barry 2008 bomford et al 2009 the australian government created the climatch software initially as desktop software that has since been replaced by a browser based application the climatch documentation provides a detailed history of the software and its creation crombie et al 2008 also worth noting is that climate matching compares each existing location to each possible invasion location and looks at the best match for each possible invasion location in a one to one association in contrast other methods such as niche modeling compare many to many associations for potential invasion a focus on the most climatically similar occupied location may provide a more precautionary perspective than models that account for associations across all points we provide a high level summary and relevant technical details as part of our introduction to the algorithm climatch produces a euclidean based score between 0 and 10 for how well a species climate matches in one location compared to another location different cutoff values may be used for determining species risk of invasion based upon score values e g u s fish and wildlife service 2018 we implemented a version of the climatch algorithm in r because of limitations of the browser based application notably the web application can be subject to continual internet connectivity issues such as outages and bandwidth limitations increasing the chance for disruption when processing large volumes of data for example we have a south african collaborator who has limited internet due to rolling blackouts the collaborator can download all inputs files during periods of internet connectivity and then run analysis offline at later times on a similar note the current climatch webpage https climatch cp1 agriculture gov au climatch jsp accessed 21 september 2021 does not have an application programming interface api that would facilitate scripting and high throughput processing by allowing the use of scripting and accessing data without downloads over the internet these needs emerged during the course of a horizon scanning project where we required high throughput computing htc in order to process 10 000s of species quickly using a reproducible method in this paper we start by introduce the climatchr package the first implementation of the climatch algorithm in r and provide an overview of the package and our specific methods then we describe how to use the package for a single species many users may find this application sufficient for their needs in calculating the climate matches for individual species e g a usfws biologist conducting an ecological risk screening summary following u s fish and wildlife service 2018 for users with a larger number of species we describe how the package may be used with parallel computing inside of r for mid sized batches e g 10s to 100s of species depending upon the amount of data for each species and local computing resources this example illustrates how a user may want to calculate many different climate matches given the resources of a single machine finally we provide an overview and worked example of how the package may be used for htc using the htcondor software htcondor allows for htc where individual jobs are processed one by one independently in contrast to high performance computing hpc where sub processes may be tightly coupled in one job erickson et al 2018 our example demonstrates how a user may calculate 1000s to 10 000s of scores given the constrains of an htcondor pool 3 methods the methods section contains seven sub sections first we provide an overview of the climatch algorithm second we provide an overview of how the climatchr package works fig 1 third we describe the needed inputs for climatchr fourth we demonstrate how to use the climatchr package to calculate climate matches one species at a time with two representative species fifth we describe how to use the climatchr package s internal parallel function for multiple species allowing for parallel computation inside of r the internal parallel functions allows for parallel computing inside of r within the constraints of r s memory limits based upon our own experience this function works well when processing about 300 species or less of course actually performance will depend upon the user s computational resources and amount of species data sixth we describe our workflow for using climatchr with htcondor for high throughput computing this approach scales for 1000s to 10 000s of species or possibly more depending on the constraints of a specific htcondor pool for reference other projects have run 320 000 concurrent jobs with htcondor such as sfiligoi et al 2020 finally we include an overview of the climatchr s functions and climatchr s vignettes which include complete code for examples 3 1 climatch algorithm the basic formula for climatch as presented by crombie et al 2008 is a euclidean distance based matching method the method calculates the distance d i between current sites both native and already invaded and possible target invasion sites the distance only uses climate variables y and is scaled by the standard deviation of the climate variables σ 2 the climatch algorithm keeps the closest i e smallest distance value using a minimum min function this value is then scaled by 10 and treated as an integer using a floor function using non rigorous mathematical notation the equation calculates d i as follows 1 d i floor 1 min 1 k k y i j y j k 2 σ k 2 10 the subscripts match the k th climate variables from site i to site j the climatchr package currently assumes user input data standardizes by dividing by σ k 2 for each climate input variable currently we follow the original climatch algorithm and do not include climate uncertainty through stochastic climate inputs the original climatch focused on animal invasion risk baker and bomford 2009 with a focus on weed or pest animal organisms crombie et al 2008 we have used our method with a diverse taxonomic groups included mammals reptiles and amphibians birds and fishes we have ongoing work exploring the use of the package with other taxa such as plants and focused on different climate data types e g water temperature rather than air temperature for aquatic invasive species existing validation of climatch see examples in crombie et al 2008 has demonstrated the package performs well across a wide range of taxa 3 2 package overview and reason for creation we created this package to meet our needs for an initial application conducting a horizon scan for 10 000s of possible invasive species in the united states and outlying territories we needed to rapidly scan through 10 000s of species based upon a list given to us by a partner agency our first step required us to compare the climates of reported observations outside the united states and its territories to locations within the united states and its territories we then summarized these results as part of our process to prioritize species for a taxonomic expert elicitation and review these needs and application guided our package design and nomenclature for example sub regions are referred to as states often within the code these sub units were used because states are a convenient method for splitting up large data about the united states and because our original stakeholders cared about state level response states correspond to gadm level 1 which are defined as state province equivalent https www usna edu users oceano pguth md help html gadm htm accessed 19 august 2022 for example in the united states this could a legal state e g wisconsin or hawaii or territory e g puerto rico or similar unit such as the district of columbia in other countries such as canada this would be a province or territory resources such as the wikipedia list of administrative divisions https en wikipedia org wiki list of administrative divisions by country accessed 19 august 2022 provide additional examples and context within the package cell dist serves as the core function of the package other functions serve as helper functions for data formatting or wrapper functions to make the core functions easier to use we include examples of how to use the functions in the package s documentation cell dist calculates distances between cells using eq 1 which is placed inside of an internal function euclidean score this function rounds scores less than 0 to be 0 0 and does not include a floor function because we wanted to have the option of continuous scores rather than integers cell dist keeps the two lowest scores to allow for sensitivity analysis unlike the climatch original algorithm the sensitivity analysis allows us to compare the importance of any individual observations by dropping cells and comparing the output we vectorize cell dist using the data table package dowle and srinivasan 2021 because the data table uses optimized c code that can scale and run efficiently for large data in contrast to base r functions or the tidyverse wickham and grolemund 2016 we use our own function rather than using base r s stats package s dist because dist calculates the distance between all points whereas we only wanted a one to many comparison i e one source cell to many possible target cells we also vectorized this calculation using the data table package this function currently only accepts chelsa climate data karger et al 2017 and this function would need to be changed for other climate data inputs the other functions in climatchr help users to modify and use inputs table 1 3 3 input data climatchr requires four inputs and reads all inputs in from files to avoid memory limits both software such as r and hardware such as a computer s ram this approach also ensures documentation of inputs used first a user needs to define a list of species for the analysis this file requires two columns a primary scientific species name with default column name valid name and a synonym species name default column name syn name field this input may either be read into r or created as a data frame object different column names may be specified as input to gbif pull clean if no synonyms are needed a value of na should be used if multiple synonyms are used multiple rows should be created for each synonym and the same primary name should be used for each row second climatchr uses global biodiversity information facility data gbif https www gbif org accessed 20 august 2021 for species source occurrence locations third climatchr uses the gadm data for geospatial boundaries available at https gadm org accessed 20 august 2021 fourth climatchr uses chelsa climate data for climate data available at https chelsa climate org accessed 20 august 2021 karger et al 2017 we include examples of all data as part of the package for examples and testing we strongly encourage users to download their own data to ensure they are using up to date data when running their own climatch assessments i e data included as part of the package may be outdated furthermore our example data only includes the united states and outlying territories users may want to do climatch assessments for other regions as well and would need to use different gadm inputs for their region of interest 3 4 single species examples we use two species to demonstrate the package we use the eastern rosella platycercus eximius across the contiguous united states to explore the results of an non established invasive species we use the term non established invasive species rather than invasive species not yet introduced because our example species are based upon species currently in trade in the united states from a list obtained from the u s fish and wildlife service thus the species are present in captivity domestic trade but not yet known to be established in the wild this bird is native to australia and is a known invasive species in new zealand galbraith et al 2011 next we use the coconut lorikeet trichoglossus haematodus across the contiguous united states to explore invasive species establishment risk this bird is native to eastern indonesia and new guinea and is a known invasive species in locations such as australia shukuroglou and mccarthy 2006 like the previous species the coconut lorikeet is also present in the us pet trade but not yet established for both species we use all existing locations both within the native ranges and other invaded locations none of these existing locations were within the united states these examples are based upon the vignettes and documentation in climatchr furthermore complete examples are included as part of the vignettes first the climatchr package and tidyverse packages wickham et al 2019a are loaded we use the tidyverse for manipulating inputs and plotting outputs however a user could also use other packages such as base r or data table second gbif pull clean uses a list of species to automatically obtain known species locations from the global biodiversity information facility api the results are saved to csv files we observed this step requiring between 5 and 45 min depending upon local processor speeds and internet connections the run time will also vary based upon the number of occurrence records available per species all input and output files are saved to csv files rather than internal r objects for multiple reasons notably using csv files for inputs allows input data to be read in from the local hard drive rather than regenerated from the internet each time likewise using csv files for inputs facilitates advanced computing specifically high throughput computing by keeping input files outside of r until needed so jobs may more easily be broken down into smaller jobs and run independently additionally using csv files for inputs decreases the memory used by r which can be important when large jobs are being run the main function for obtaining species data is gbif pull clean that downloads the gbif data third other data input locations are specified other data includes the database of global administrative areas gadm data available at https gadm org accessed 20 august 2021 and chelsa climate data available at https chelsa climate org accessed 20 august 2021 karger et al 2017 examples for both the gadm and chelsa are included in the r package for testing these example datasets are why our code contains system file extdat package climatchr users may want to update these input files to make sure they are using the most current data or to get gadm data for their target regions fourth we run an example for platycercus eximius across the contiguous united states u s we exclude outlying u s states and territories to speed up model results and simplify plotting we include basic results using ggplot2 however one could update this figure by adding details to create a high quality map fig 2 given the species s tropical native range the predicted habitat scores seem reasonable other than unexpected habitat matches occurring in the temperate state of michigan and over lake michigan fifth we run an example for trichoglossus haematodus across the contiguous united states this example is similar to the previous example but does not include unexpected suitable climates fig 3 this finding demonstrates the importance of expert review of climate matching scores when conducting risk assessments 3 5 parallel code the climatchr package also includes options to run species in parallel parallel computing uses multiple cores on the same machine to run code quicker than on a single core this allows one species to be run per core on a machine for our specific application for users unfamiliar with parallel processing we refer them to other resources such as documentation for the parallel package included as part of base r r core team 2021 the function takes the number of cores ncores as input as well as an input directory path indir containing gbif files described in other sections an output directory path out dir an input rds file state list and the data containing climate data clim folder 3 6 using high throughput computing with climatchr the dist from gbif parallel function works well for small numbers of species e g 10s and 100s depending upon the number of records for each species we needed to process 1000s of species and thus used high throughput computing we used htcondor thain et al 2005 given our previous experience with the software e g erickson et al 2018 and had access to an htcondor pool htcondor originally scavenged available computer resources such as idle desktop computer similar to how a condor scavenges carrion the software allows users to distribute many small computing jobs across large dedicated resources e g servers or idle resources e g desktop computers and workstations the process works well for embarrassingly or pleasantly parallel tasks that may be broken down into small jobs for readers seeking to know more about htcondor we refer them to the project s homepage https research cs wisc edu htcondor accessed 20 august 2021 and a tutorial written for the ecological and environmental sciences erickson et al 2018 we have published our high throughput working using htcondor as a usgs software release erickson et al 2021b the key for this project was to create software that allowed us to batch process each species as a batch iterator and then combine the results overall our example project uses a typical htcondor workflow erickson et al 2018 using docker to control the computing environment merkel 2014 for pre processing create rds file merges all gadm data files into one object and then creates an r data file rds file split gbif splits gbif data into single species files if needed a submit sub file runs the code on htcondor including calling a bourne again shell bash shell file run script sh that calls an r script file condor script r post processing is done with r scripts post processing includes extract condor out r that merges all species into one file for each data folder create ecoregion clim r that creates a table with the climate cell numbers for each eco region and summarize ecoregion r that uses the htcondor output with the previous output to summarize by ecoregions we have used this htcondor pool to process 8000 species on an htcondor pool with 200 cpus in less than 3 days a larger pool would have allowed more rapid scanning 3 7 numerical methods the code used for this manuscript is based upon the climatchr s vignettes the climatchr package has been peer reviewed and published as an r package erickson et al 2021a an example using climatchr with htcondor has also been published as a peer reviewed software release erickson et al 2021b we have tested and used climatchr with r 3 6 3 and the high throughput examples include dockerfiles locking down specific versions of r 4 discussion and future prospects we created the climatchr package to allow automation of climate matching using the climatch algorithm crombie et al 2008 using scripting such as our r package is an advantage over point and click based workflows because it allows for scripting thereby creating reproducible results and allowing the use of htc our examples also illustrate a pitfall of any type of climate matching exercise unrealistic results may occur requiring understanding of the biology of the system for example a tropical bird would not be able to survive in lake michigan under current climates this artifact likely emerged because climatch uses means rather than other climate measurements such as minimums or maximums hence any type of automated assessment requires expert oversight to ensure reasonable results for the taxa being assessed likewise exploring data beyond the bioclim variables used in the original climatch algorithm may be helpful because the bioclim variables may not be appropriate for all taxa groups we envision the package being extended in the future with expanded adaptability to other climate datasets currently a limitation of the climatchr is that the package is currently hard coded by having chelsa in the columns of data to use this could be updated by changing lines of code in cell dist that are hard coded to read chelsa columns i e lines with grep chelsa colnames clim native df allowing alternative sources of climate data would allow users to build multiple scenarios including the use of future climate projections furthermore almost all climate data models use surface air temperatures however other types of temperatures may be more important depending on the life history of species or taxa for example water temperatures may be more important for aquatic species and recent modeling efforts now allow these values to be readily predicted e g read et al 2021a b we also followed the climatch algorithm and used a euclidean based distance other distances and dissimilarities are possible and commonly used in fields such as community ecology c f legendre and legendre 2012 and other climate matching exercises have used different dissimilarity such as the bray curtis dissimilarity e g yoğurtçuoğlu et al 2021 research by bradie et al 2015 found euclidean based distances performed well but a research need exists to compare different dissimilarities for climate matching especially across different taxa and climatic predictor variables in our package new functions could be added similar to euclidean score and then given options for dissimilarities in cell dist we created this package as part of a broader horizon scanning effort in addition to the previously mentioned befits of automation e g allowing humans to avoid repetitive tasks that may also be error prone climatchr or similar tools may be used by future tools as statistical learning and text processing advances future tools could be developed to process invasive species literature to analyze the potential impact of invasive species on native species for example the geodeepdive modeling efforts allows for geologists to use machine learning to automatically extract information from literature zhang et al 2013 tools like geodeepdive may eventually emerge for ecological and species data as well the benefits of automation are that the approaches are reproducible less subjective or at least require explicit statement of assumptions in software code and scale with the addition of computational resources rather than human effort the climatchr package could also be expanded to be more accessible global including more non us examples would help with concepts such as state versus gdam level 1 additionally reducing dependencies would make the package more future proof we use data table dowle and srinivasan 2021 inside the package but the use of the tidyverse wickham et al 2019b requires many other packages which make our package more brittle for example we could use the poorman eastwood 2022 package to reduce remove our use of the tidyverse climatchr also uses the scrubr chamberlain 2022 but this package was removed from the comprehensive r archive network cran citation for cran hornik 2012 this limitation means that climatchr would fail a cran test and cannot be posted on cran lastly we see climatchr as being applicable to other types of assessments besides horizon scanning for invasive species a key assumption for these applications of climatch are that species are restricted by climate and that recorded observations capture the species range for example climate matching may be used with climate change projections for biosecurity risk assessments e g kriticos 2012 native species including threatened and endangered species e g broadmeadow et al 2005 or pathogen risk e g pethybridge et al 2003 however if species are not limited by climate but by other factors or their observations do not capture the species full climatic range then any climate matching approach may not accurately predict possible ranges more broadly any type of assessment that compares climate suitability would be amenable to automation through tools such as climatchr declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government we thank the usgs biological threats and invasive species research program and u s fish and wildlife service invasive species program for funding this research 
25512,climate matching allows comparisons of climatic conditions between different locations to understand location and species range climatic suitability the approach may be used as part of horizon scanning exercises such as those conducted for invasive species we implemented the climatch algorithm into an r package climatchr the package allows automated and scripted climate matching exercises across all steps from downloading data to summarizing species climate matches we also show how climatchr may be used with high throughput computing to process many species for example we were able to calculate climate scores for over 8 000 species in less than 3 days using this package this automation allows high throughput processing of species data a new development for improving the efficiency and speed of climate matching and horizon scanning keywords climate envelop invasive species risk assessment tool data availability no data was used for the research described in the article 1 software and data availability name of software and location climatchr available as a usgs software release erickson et al 2021a example htcondor application as a second u s geological survey usgs software release erickson et al 2021b required software availability and cost r r core team 2021 for climatchr high throughput computing uses htcondor and requires an htcondor pool see https htcondor org accessed 21 sept 2021 for guidance on obtaining htcondor both programs are free and open source required data we describe required data in detail in section 3 3 as a summary species names input data either as an input file such as a csv or data frame in r database of global administrative areas gadm files from https gadm org as rds files chelsa climate data from https chelsa climate org as tif files species location data from global biodiversity information facility gbif data downloaded by the package from https www gbif org for further instructions on how to run climatchr we refer the reader to the package s vignettes we have based these examples upon the vignettes as well 2 introduction climate matching provides a method for understanding species distributions and ranges furthermore climate matching may be used as part of horizon scanning the process of examining potential risks and threats to assess potential new invasive species sutherland and woodroof 2009 preventing new invasive species establishment requires less effort and fewer resources than attempting to control and eradicate existing infestations vander zanden and olden 2008 in invasion science horizon scanning allows resource managers and risk assessors to identify potentially new invasive species and subsequently to reduce the risk of invasion roy et al 2014 ricciardi et al 2017 as part of these efforts scientists and risk assessors often investigate the climatological similarities of existing species distributions and possible invasion areas during horizon scanning exercises and similar risk assessments e g hayes and barry 2008 bomford et al 2009 roy et al 2014 u s fish and wildlife service 2018 climate matching may be done before more labor intensive expert elicitation as part of scanning exercises current methods for climate matching can be labor intensive because they require users to hand select data and then manipulate the data across software programs e g u s fish and wildlife service 2018 however climate matching may be done relatively inexpensively and easily for large number of species because the process may be automated the climatch algorithm exists as one tool for conducting climate matching agencies such as the australian department of agriculture water and the environment crombie et al 2008 and u s fish and wildlife service usfws e g u s fish and wildlife service 2018 use climatch because validation studies have found the approach to be a consistent predictor of invasion risk hayes and barry 2008 bomford et al 2009 the australian government created the climatch software initially as desktop software that has since been replaced by a browser based application the climatch documentation provides a detailed history of the software and its creation crombie et al 2008 also worth noting is that climate matching compares each existing location to each possible invasion location and looks at the best match for each possible invasion location in a one to one association in contrast other methods such as niche modeling compare many to many associations for potential invasion a focus on the most climatically similar occupied location may provide a more precautionary perspective than models that account for associations across all points we provide a high level summary and relevant technical details as part of our introduction to the algorithm climatch produces a euclidean based score between 0 and 10 for how well a species climate matches in one location compared to another location different cutoff values may be used for determining species risk of invasion based upon score values e g u s fish and wildlife service 2018 we implemented a version of the climatch algorithm in r because of limitations of the browser based application notably the web application can be subject to continual internet connectivity issues such as outages and bandwidth limitations increasing the chance for disruption when processing large volumes of data for example we have a south african collaborator who has limited internet due to rolling blackouts the collaborator can download all inputs files during periods of internet connectivity and then run analysis offline at later times on a similar note the current climatch webpage https climatch cp1 agriculture gov au climatch jsp accessed 21 september 2021 does not have an application programming interface api that would facilitate scripting and high throughput processing by allowing the use of scripting and accessing data without downloads over the internet these needs emerged during the course of a horizon scanning project where we required high throughput computing htc in order to process 10 000s of species quickly using a reproducible method in this paper we start by introduce the climatchr package the first implementation of the climatch algorithm in r and provide an overview of the package and our specific methods then we describe how to use the package for a single species many users may find this application sufficient for their needs in calculating the climate matches for individual species e g a usfws biologist conducting an ecological risk screening summary following u s fish and wildlife service 2018 for users with a larger number of species we describe how the package may be used with parallel computing inside of r for mid sized batches e g 10s to 100s of species depending upon the amount of data for each species and local computing resources this example illustrates how a user may want to calculate many different climate matches given the resources of a single machine finally we provide an overview and worked example of how the package may be used for htc using the htcondor software htcondor allows for htc where individual jobs are processed one by one independently in contrast to high performance computing hpc where sub processes may be tightly coupled in one job erickson et al 2018 our example demonstrates how a user may calculate 1000s to 10 000s of scores given the constrains of an htcondor pool 3 methods the methods section contains seven sub sections first we provide an overview of the climatch algorithm second we provide an overview of how the climatchr package works fig 1 third we describe the needed inputs for climatchr fourth we demonstrate how to use the climatchr package to calculate climate matches one species at a time with two representative species fifth we describe how to use the climatchr package s internal parallel function for multiple species allowing for parallel computation inside of r the internal parallel functions allows for parallel computing inside of r within the constraints of r s memory limits based upon our own experience this function works well when processing about 300 species or less of course actually performance will depend upon the user s computational resources and amount of species data sixth we describe our workflow for using climatchr with htcondor for high throughput computing this approach scales for 1000s to 10 000s of species or possibly more depending on the constraints of a specific htcondor pool for reference other projects have run 320 000 concurrent jobs with htcondor such as sfiligoi et al 2020 finally we include an overview of the climatchr s functions and climatchr s vignettes which include complete code for examples 3 1 climatch algorithm the basic formula for climatch as presented by crombie et al 2008 is a euclidean distance based matching method the method calculates the distance d i between current sites both native and already invaded and possible target invasion sites the distance only uses climate variables y and is scaled by the standard deviation of the climate variables σ 2 the climatch algorithm keeps the closest i e smallest distance value using a minimum min function this value is then scaled by 10 and treated as an integer using a floor function using non rigorous mathematical notation the equation calculates d i as follows 1 d i floor 1 min 1 k k y i j y j k 2 σ k 2 10 the subscripts match the k th climate variables from site i to site j the climatchr package currently assumes user input data standardizes by dividing by σ k 2 for each climate input variable currently we follow the original climatch algorithm and do not include climate uncertainty through stochastic climate inputs the original climatch focused on animal invasion risk baker and bomford 2009 with a focus on weed or pest animal organisms crombie et al 2008 we have used our method with a diverse taxonomic groups included mammals reptiles and amphibians birds and fishes we have ongoing work exploring the use of the package with other taxa such as plants and focused on different climate data types e g water temperature rather than air temperature for aquatic invasive species existing validation of climatch see examples in crombie et al 2008 has demonstrated the package performs well across a wide range of taxa 3 2 package overview and reason for creation we created this package to meet our needs for an initial application conducting a horizon scan for 10 000s of possible invasive species in the united states and outlying territories we needed to rapidly scan through 10 000s of species based upon a list given to us by a partner agency our first step required us to compare the climates of reported observations outside the united states and its territories to locations within the united states and its territories we then summarized these results as part of our process to prioritize species for a taxonomic expert elicitation and review these needs and application guided our package design and nomenclature for example sub regions are referred to as states often within the code these sub units were used because states are a convenient method for splitting up large data about the united states and because our original stakeholders cared about state level response states correspond to gadm level 1 which are defined as state province equivalent https www usna edu users oceano pguth md help html gadm htm accessed 19 august 2022 for example in the united states this could a legal state e g wisconsin or hawaii or territory e g puerto rico or similar unit such as the district of columbia in other countries such as canada this would be a province or territory resources such as the wikipedia list of administrative divisions https en wikipedia org wiki list of administrative divisions by country accessed 19 august 2022 provide additional examples and context within the package cell dist serves as the core function of the package other functions serve as helper functions for data formatting or wrapper functions to make the core functions easier to use we include examples of how to use the functions in the package s documentation cell dist calculates distances between cells using eq 1 which is placed inside of an internal function euclidean score this function rounds scores less than 0 to be 0 0 and does not include a floor function because we wanted to have the option of continuous scores rather than integers cell dist keeps the two lowest scores to allow for sensitivity analysis unlike the climatch original algorithm the sensitivity analysis allows us to compare the importance of any individual observations by dropping cells and comparing the output we vectorize cell dist using the data table package dowle and srinivasan 2021 because the data table uses optimized c code that can scale and run efficiently for large data in contrast to base r functions or the tidyverse wickham and grolemund 2016 we use our own function rather than using base r s stats package s dist because dist calculates the distance between all points whereas we only wanted a one to many comparison i e one source cell to many possible target cells we also vectorized this calculation using the data table package this function currently only accepts chelsa climate data karger et al 2017 and this function would need to be changed for other climate data inputs the other functions in climatchr help users to modify and use inputs table 1 3 3 input data climatchr requires four inputs and reads all inputs in from files to avoid memory limits both software such as r and hardware such as a computer s ram this approach also ensures documentation of inputs used first a user needs to define a list of species for the analysis this file requires two columns a primary scientific species name with default column name valid name and a synonym species name default column name syn name field this input may either be read into r or created as a data frame object different column names may be specified as input to gbif pull clean if no synonyms are needed a value of na should be used if multiple synonyms are used multiple rows should be created for each synonym and the same primary name should be used for each row second climatchr uses global biodiversity information facility data gbif https www gbif org accessed 20 august 2021 for species source occurrence locations third climatchr uses the gadm data for geospatial boundaries available at https gadm org accessed 20 august 2021 fourth climatchr uses chelsa climate data for climate data available at https chelsa climate org accessed 20 august 2021 karger et al 2017 we include examples of all data as part of the package for examples and testing we strongly encourage users to download their own data to ensure they are using up to date data when running their own climatch assessments i e data included as part of the package may be outdated furthermore our example data only includes the united states and outlying territories users may want to do climatch assessments for other regions as well and would need to use different gadm inputs for their region of interest 3 4 single species examples we use two species to demonstrate the package we use the eastern rosella platycercus eximius across the contiguous united states to explore the results of an non established invasive species we use the term non established invasive species rather than invasive species not yet introduced because our example species are based upon species currently in trade in the united states from a list obtained from the u s fish and wildlife service thus the species are present in captivity domestic trade but not yet known to be established in the wild this bird is native to australia and is a known invasive species in new zealand galbraith et al 2011 next we use the coconut lorikeet trichoglossus haematodus across the contiguous united states to explore invasive species establishment risk this bird is native to eastern indonesia and new guinea and is a known invasive species in locations such as australia shukuroglou and mccarthy 2006 like the previous species the coconut lorikeet is also present in the us pet trade but not yet established for both species we use all existing locations both within the native ranges and other invaded locations none of these existing locations were within the united states these examples are based upon the vignettes and documentation in climatchr furthermore complete examples are included as part of the vignettes first the climatchr package and tidyverse packages wickham et al 2019a are loaded we use the tidyverse for manipulating inputs and plotting outputs however a user could also use other packages such as base r or data table second gbif pull clean uses a list of species to automatically obtain known species locations from the global biodiversity information facility api the results are saved to csv files we observed this step requiring between 5 and 45 min depending upon local processor speeds and internet connections the run time will also vary based upon the number of occurrence records available per species all input and output files are saved to csv files rather than internal r objects for multiple reasons notably using csv files for inputs allows input data to be read in from the local hard drive rather than regenerated from the internet each time likewise using csv files for inputs facilitates advanced computing specifically high throughput computing by keeping input files outside of r until needed so jobs may more easily be broken down into smaller jobs and run independently additionally using csv files for inputs decreases the memory used by r which can be important when large jobs are being run the main function for obtaining species data is gbif pull clean that downloads the gbif data third other data input locations are specified other data includes the database of global administrative areas gadm data available at https gadm org accessed 20 august 2021 and chelsa climate data available at https chelsa climate org accessed 20 august 2021 karger et al 2017 examples for both the gadm and chelsa are included in the r package for testing these example datasets are why our code contains system file extdat package climatchr users may want to update these input files to make sure they are using the most current data or to get gadm data for their target regions fourth we run an example for platycercus eximius across the contiguous united states u s we exclude outlying u s states and territories to speed up model results and simplify plotting we include basic results using ggplot2 however one could update this figure by adding details to create a high quality map fig 2 given the species s tropical native range the predicted habitat scores seem reasonable other than unexpected habitat matches occurring in the temperate state of michigan and over lake michigan fifth we run an example for trichoglossus haematodus across the contiguous united states this example is similar to the previous example but does not include unexpected suitable climates fig 3 this finding demonstrates the importance of expert review of climate matching scores when conducting risk assessments 3 5 parallel code the climatchr package also includes options to run species in parallel parallel computing uses multiple cores on the same machine to run code quicker than on a single core this allows one species to be run per core on a machine for our specific application for users unfamiliar with parallel processing we refer them to other resources such as documentation for the parallel package included as part of base r r core team 2021 the function takes the number of cores ncores as input as well as an input directory path indir containing gbif files described in other sections an output directory path out dir an input rds file state list and the data containing climate data clim folder 3 6 using high throughput computing with climatchr the dist from gbif parallel function works well for small numbers of species e g 10s and 100s depending upon the number of records for each species we needed to process 1000s of species and thus used high throughput computing we used htcondor thain et al 2005 given our previous experience with the software e g erickson et al 2018 and had access to an htcondor pool htcondor originally scavenged available computer resources such as idle desktop computer similar to how a condor scavenges carrion the software allows users to distribute many small computing jobs across large dedicated resources e g servers or idle resources e g desktop computers and workstations the process works well for embarrassingly or pleasantly parallel tasks that may be broken down into small jobs for readers seeking to know more about htcondor we refer them to the project s homepage https research cs wisc edu htcondor accessed 20 august 2021 and a tutorial written for the ecological and environmental sciences erickson et al 2018 we have published our high throughput working using htcondor as a usgs software release erickson et al 2021b the key for this project was to create software that allowed us to batch process each species as a batch iterator and then combine the results overall our example project uses a typical htcondor workflow erickson et al 2018 using docker to control the computing environment merkel 2014 for pre processing create rds file merges all gadm data files into one object and then creates an r data file rds file split gbif splits gbif data into single species files if needed a submit sub file runs the code on htcondor including calling a bourne again shell bash shell file run script sh that calls an r script file condor script r post processing is done with r scripts post processing includes extract condor out r that merges all species into one file for each data folder create ecoregion clim r that creates a table with the climate cell numbers for each eco region and summarize ecoregion r that uses the htcondor output with the previous output to summarize by ecoregions we have used this htcondor pool to process 8000 species on an htcondor pool with 200 cpus in less than 3 days a larger pool would have allowed more rapid scanning 3 7 numerical methods the code used for this manuscript is based upon the climatchr s vignettes the climatchr package has been peer reviewed and published as an r package erickson et al 2021a an example using climatchr with htcondor has also been published as a peer reviewed software release erickson et al 2021b we have tested and used climatchr with r 3 6 3 and the high throughput examples include dockerfiles locking down specific versions of r 4 discussion and future prospects we created the climatchr package to allow automation of climate matching using the climatch algorithm crombie et al 2008 using scripting such as our r package is an advantage over point and click based workflows because it allows for scripting thereby creating reproducible results and allowing the use of htc our examples also illustrate a pitfall of any type of climate matching exercise unrealistic results may occur requiring understanding of the biology of the system for example a tropical bird would not be able to survive in lake michigan under current climates this artifact likely emerged because climatch uses means rather than other climate measurements such as minimums or maximums hence any type of automated assessment requires expert oversight to ensure reasonable results for the taxa being assessed likewise exploring data beyond the bioclim variables used in the original climatch algorithm may be helpful because the bioclim variables may not be appropriate for all taxa groups we envision the package being extended in the future with expanded adaptability to other climate datasets currently a limitation of the climatchr is that the package is currently hard coded by having chelsa in the columns of data to use this could be updated by changing lines of code in cell dist that are hard coded to read chelsa columns i e lines with grep chelsa colnames clim native df allowing alternative sources of climate data would allow users to build multiple scenarios including the use of future climate projections furthermore almost all climate data models use surface air temperatures however other types of temperatures may be more important depending on the life history of species or taxa for example water temperatures may be more important for aquatic species and recent modeling efforts now allow these values to be readily predicted e g read et al 2021a b we also followed the climatch algorithm and used a euclidean based distance other distances and dissimilarities are possible and commonly used in fields such as community ecology c f legendre and legendre 2012 and other climate matching exercises have used different dissimilarity such as the bray curtis dissimilarity e g yoğurtçuoğlu et al 2021 research by bradie et al 2015 found euclidean based distances performed well but a research need exists to compare different dissimilarities for climate matching especially across different taxa and climatic predictor variables in our package new functions could be added similar to euclidean score and then given options for dissimilarities in cell dist we created this package as part of a broader horizon scanning effort in addition to the previously mentioned befits of automation e g allowing humans to avoid repetitive tasks that may also be error prone climatchr or similar tools may be used by future tools as statistical learning and text processing advances future tools could be developed to process invasive species literature to analyze the potential impact of invasive species on native species for example the geodeepdive modeling efforts allows for geologists to use machine learning to automatically extract information from literature zhang et al 2013 tools like geodeepdive may eventually emerge for ecological and species data as well the benefits of automation are that the approaches are reproducible less subjective or at least require explicit statement of assumptions in software code and scale with the addition of computational resources rather than human effort the climatchr package could also be expanded to be more accessible global including more non us examples would help with concepts such as state versus gdam level 1 additionally reducing dependencies would make the package more future proof we use data table dowle and srinivasan 2021 inside the package but the use of the tidyverse wickham et al 2019b requires many other packages which make our package more brittle for example we could use the poorman eastwood 2022 package to reduce remove our use of the tidyverse climatchr also uses the scrubr chamberlain 2022 but this package was removed from the comprehensive r archive network cran citation for cran hornik 2012 this limitation means that climatchr would fail a cran test and cannot be posted on cran lastly we see climatchr as being applicable to other types of assessments besides horizon scanning for invasive species a key assumption for these applications of climatch are that species are restricted by climate and that recorded observations capture the species range for example climate matching may be used with climate change projections for biosecurity risk assessments e g kriticos 2012 native species including threatened and endangered species e g broadmeadow et al 2005 or pathogen risk e g pethybridge et al 2003 however if species are not limited by climate but by other factors or their observations do not capture the species full climatic range then any climate matching approach may not accurately predict possible ranges more broadly any type of assessment that compares climate suitability would be amenable to automation through tools such as climatchr declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government we thank the usgs biological threats and invasive species research program and u s fish and wildlife service invasive species program for funding this research 
25513,climate is an essential component of environmental models over the last two decades many weather generators have been presented in the literature although their implementation into software has been of great help to environmental modellers their lack of integration into modelling frameworks still represents a challenge for end users in many cases end users have to retrieve the climate variables by themselves in order to use an environmental model in some other cases the weather generator software is embedded into the modelling framework but this increases the maintenance effort in this paper we present a different approach the deployment of a weather generator as a web api a few application examples are provided to illustrate the benefits of this implementation in summary a web api facilitates the integration into modelling frameworks decreases the maintenance effort and avoid interoperability issues due to different programming languages keywords weather generator web api client server architecture interoperability software maintenance software integration 1 introduction climate change has brought the attention of the scientific community on the effect of climate variables in environmental models climate sensitive models are needed especially in agriculture and forestry to assess the impacts of climate change on growth and crops however the integration of climate variables into environmental models poses two major challenges first meteorological time series mts are recorded in weather stations that is at punctual geographical locations in contrast models have a much larger geographical coverage and mts must be literally available wall to wall secondly in order to assess the impact of climate change climate projections must be available these projections are usually produced by general circulation models gcm that have a much coarser resolution than what is needed in environmental modelling in spite of these two challenges a significant number of weather generators have been published in the literature over the last two decades e g bannayan and hoogenboom 2008 birt et al 2010 burton et al 2013 the wall to wall availability of mts has been addressed through the interpolation or extrapolation of the observed mts at the nearest weather stations jolly et al 2005 as for regional and local climate projections these are obtained by downscaling the gcm projections e g wang et al 2013 although there are many weather generators available in the literature their implementation can cause some issues to ecological modellers and end users most weather generators are implemented as standalone applications programmed in various languages e g bannayan and hoogenboom 2008 kilsby et al 2007 liu et al 2009 for ecological modellers this usually implies downloading the standalone application exporting the geographic coordinates of interest from the modelling framework processing them in the standalone application and importing the mts back into the modelling framework when it comes to implementing climate sensitive ecological models into software and transferring them to end users the situation can get worse unless the weather generator can be embedded into the same software end users must download a weather generator hopefully the same that provided the mts for the model fitting and generate the mts by themselves in order to use the ecological model obviously this additional data processing reduces the probability of successful knowledge transfer to end users embedding a weather generator into the same software that implements the ecological model can be a solution however the different programming languages often represent a barrier moreover embedding a software into another has some limitations especially when it comes to updating unless the embedded weather generator can update itself modellers will have to publish a new software version containing the updated weather generator the end users will have to get aware of these new versions and to download them having disparate distributed versions increases the maintenance effort and makes the debugging process more complex another option consists of implementing a client server architecture a weather generator can be deployed on a server and the software implementing the ecological model can use a client to retrieve the mts from the server embedding a client into an existing software is simpler than embedding a whole weather generator it takes less space and requires fewer updates at the same time the weather generator on the server can be updated at any time ensuring that the clients are always dealing with the latest version the objective of this study was to develop this server client architecture in order to facilitate the use of weather generators for both environmental modellers and end users of environmental models more specifically we used an existing weather generator called biosim régnière et al 2017 which is currently used in north america and we deployed it on a server the server implementing the weather generator uses a representational state transfer rest application programming interface api that reads incoming hypertext transfer protocol http requests from the web we propose several clients in different languages that can easily be integrated into other software these clients can be used to produce the http requests and parse the incoming results of these requests this paper is structured as follows first we introduce the biosim weather generator and its implementation into a web api then some examples of applications are presented finally we discuss the advantages and weaknesses of the client server architecture in the context of weather generation 2 the biosim weather generator 2 1 geographical coverage by interpolation biosim is a weather generator initially designed to support the planning of pest management activities régnière et al 1995a b it has been in continuous development since the mid 1990s and has been used in many research projects focused on forest ecology forest fire dynamics and insect pest management e g tobin et al 2004 le goff et al 2009 coulombe et al 2010 biosim defines an mts as a series of daily values of the four basic climate variables minimum air temperature c maximum air temperature c precipitation mm and wind speed km h 1 depending on the weather station and the time period relative humidity and solar radiation w m 2 are also available if they are not they are simulated using the mt clim 4 3 model glassy and running 1994 thornton and running 1999 snow precipitation mm and snow depth accumulation cm are also simulated using a model adapted from that of brown et al 2003 the weather generator produces an mts for a particular location by interpolating from the mts in the k nearest weather stations the distance for identifying these k nearest stations is computed as a modified euclidean distance 1 d i δ x i 2 δ y i 2 100 δ z i 2 δ shore i 2 where δ x i δ y i δ z i and δ shore i are the differences km in longitude latitude elevation and distance to the shore between weather station i and the point of interest respectively note that the factor 100 in eq 1 gives a greater weight to the difference in elevation because of the environmental lapse rate i e the average temperature decrease with elevation which is about 6 c km 1 barry 2008 p 52 the mts used in the interpolation can be those observed or they can be simulated through the disaggregation of monthly 30 year normals the first option is mainly used to produce past mts whereas the second is the usual way of producing future mts note that future 30 year normals are themselves obtained through the downscaling of regional or general circulation model projections this downscaling is addressed in section 2 2 before interpolation an adjustment is performed on the mts of the k nearest weather stations based on local gradients the local gradients in minimum temperature maximum temperature and precipitation are evaluated through multiple linear regressions one for each climate variable fitted on the 30 year normals of the 25 nearest climate stations based on a distance function similar to eq 1 for instance for maximum temperature the local gradient would be evaluated through the following linear regression 2 t max i β 0 β 1 δ x i β 2 δ y i β 3 δ z i ɛ i δ x i δ y i and δ z i are the differences in longitude latitude and elevation between weather station i and the point of interest respectively and ɛ i is the residual error term once the regressions have been fitted they are used to adjust the minimum temperature maximum temperature and precipitation from the nearest weather stations to the coordinates of the point of interest then the values of these variables at the point of interest are calculated as the average of the adjusted values in the nearest climate stations weighted by the inverse squared distance i e 1 d i 2 2 2 simulating by disaggregation and future climate projections régnière and st amant 2007 improved an algorithm initially developed by régnière and bolstad 1994 with the aim of producing stochastic realizations of daily minimum and maximum air temperatures and precipitation from 30 year monthly normals the expected daily temperatures for a particular day are interpolated between the means of two successive months the algorithm uses the standard deviations of these variables the second order regressive terms and their cross correlation as observed during the 30 year period to compute a stochastic deviate that is added to the expectation mts generated through this disaggregation of 30 year monthly normals were found to reproduce seasonal patterns as well as the variability and extremes of temperature the predicted precipitation was not quite as accurate but remained realistic régnière and st amant 2007 simulating by disaggregation is mainly used to produce future mts however it assumes that future 30 year normals are available for each weather station biosim uses the delta method mosier et al 2014 also referred to as the perturbation method by some authors prudhomme et al 2002 fowler et al 2007 to scale climate projections from regional or general circulation models down to the weather stations the 30 year monthly normals for the period 1981 2010 are taken as reference at each weather station the difference between these 30 year monthly normals and the projections of a regional or a global circulation model for a future 30 year period is the so called delta the locally observed 30 year monthly normals are then updated using the delta in order to produce local future 30 year normals that biosim relies on to cover the whole 21st century future normals were produced using the delta method and climate projections from the hadley model the canadian global circulation model gcm4 and the canadian regional circulation model rcm4 collins et al 2011 salzen et al 2013 scinocca et al 2016 under the representative concentration pathways rcp 4 5 and 8 5 ipcc 2013 p 29 3 software implementation biosim was implemented in c as a standalone application for windows and it is freely available at ftp ftp cfl forestry ca regniere software biosim the core of the application is a dynamic link library dll implementing the weather generator this core is complemented by a series of smaller dlls that make it possible to derive information from the mts the annual monthly and daily degree days the soil moisture index and spruce budworm development count among the output of these additional dlls for the sake of clarity we will refer to the core dll as the generator and to these additional dlls as mts processing models the generator must get access to a dataset of 30 year normals to simulate by disaggregation it can also access a dataset of observed mts and generate mts from these given the number of weather stations in north america the weather generator can hardly be instantiated with all the observed mts the mts are rather broken down into smaller synchronous mts and a different weather generator is instantiated for each synchronous mts the same applies to the 30 year normals many weather generators are instantiated with different 30 year normals 3 1 the biosim web api 3 1 1 implementation in the context of this project we developed a rest api based on the net framework the api was implemented in c as an internet information services iis application which does the following tasks in order see fig 1 1 read incoming http requests 2 parse the requests and forward them to one or many instances of the generator 3 retrieve the mts generated by the generator instances 4 apply one or many mts processing models on the mts 5 send the result back to the client the biosim dlls are embedded in the api as a package the generator can be loaded with a dataset of 30 year normals and eventually a dataset of daily climate observations the combination of 30 year normals and daily climate observations is referred to as a context upon its initialization the api creates instances of the generator with different contexts in order to cover the whole 1900 2022 period for observed mts and as well as the 2022 2100 period for simulation by disaggregation the list of these contexts is provided in table 1 an incoming http request can refer to several contexts for instance a request for the period 1976 1986 will deal with generators related to contexts 2 and 3 in table 1 upon the uptake of a request the api determines which contexts are related to the request for past time series the range of data within the observed daily mts defines the time range covered by the context for example context 2 covers the 1950 1979 time range for future time series the different 30 year normals overlap the central 10 year period was set as the time range covered by each context associated to future time series for instance a request for the period 2041 2060 under rcp 4 5 with the hadley model would deal with context 7 for the time range 2041 2050 and with context 8 for the time range 2051 2060 the only exception to this is the last context that goes from 2071 to 2100 which was assumed to cover the time range 2081 2100 once the appropriate contexts have been identified the api retrieves their associated generators to produce mts the mts from different contexts are concatenated in order to create a single mts the api has several routes table 2 the main route is biosimweather which does the weather generation and processes the generated mts using one or many user specified mts processing models see table 3 an example of http request for the annual degree days over the 2000 2016 period at the eastern end of the reservoir gouin 48 30 n 74 30 w in the province of quebec canada using the main route is http repicea dynu net biosim biosimweather lat 48 5 long 74 5 from 2000 to 2016 model degreeday annual format json the result is returned in the javascript object notation json format multiple mts can be generated by specifying more than one points in the http request for example two mts are generated at once in the following request http repicea dynu net biosim biosimweather lat 48 5 2049 0 long 74 5 20 76 0 from 2000 to 2016 model degreeday annual format json note that the api allows for a maximum of 10 points per http request however the clients described in section 3 2 can handle a greater number of points by sending multiple requests of 10 points each the results of the multiple requests are concatenated into a single output the other routes shown in table 2 provide further information on the mts processing models and their parameters the api also implements an automatic update feature every day a new version of the 2021 2022 observed daily mts dataset is compiled with the latest meteorological observations and it is made available on a dedicated ftp site the api periodically checks if the version on the ftp site is more recent than the one used in context 4 table 1 whenever this is the case the new version of the dataset is downloaded and loaded in place of the previous version this feature ensures that the web api always relies on the latest observed mts 3 1 2 scalability and performance the c language allows for multithreading given that the api accepts several points per request this feature makes it possible to increase performance it is possible to instantiate more than one generator for each context listed in table 1 the different geographical points are then sent into a queue which feeds the different generator instances which in turn return the mts to the main thread the same implementation also applies to mts processing models see table 3 although a multithreading approach seemed promising preliminary trials showed that the marginal gain in adding new threads quickly decreased in fact there was little gain or no gain at all in performance when increasing the number of threads from five to 10 this was due to the memory allocation inside the threads because the api remained a single process application the memory allocation into the different threads is protected by locks which reduce the performance embedded staff 2006 especially when the threads must allocate large chunks of memory which was the case here to work around this memory issue we implemented a multiprocess architecture the different generators and mts processing models were instantiated in different processes and linked to the main process that of the api through pipes because each process is independent and contains a single generator or mts processing instance there is no constraint due to the locks associated to memory allocation preliminary results with this architecture showed that marginal gain in performance when adding new processes remained significant the performance of the web api was tested with 100 fictive requests sent on the biosimweather route each request had a random time span between 1950 and 2050 up to 100 locations and randomly asked for either the annual degree days or an extensive array of annual climatic variables these requests were sent to the web api running on an intel r xeon r cpu e5 1650 v3 3 50 ghz with each generator and mts processing model being instantiated three times in different processes the average time to process a single point in each request is shown in fig 2 by regression the average processing time was estimated at 28 8 and 11 4 milliseconds point 1 yr 1 for the extensive array of climate variables and for the degree days respectively 3 2 c java and r clients client applications are available in the c java and r languages they offer simple static methods that produce the http requests for the web api table 4 the r client actually uses the java client as back end and depends on the j4r package see fortin 2020 for the interoperability between r and java the r client exposes functions that bear the same names as the static methods of the java client table 4 the c client is a direct translation of the java client using one of the three clients has the advantage of breaking requests with a large number of points into smaller requests and concatenating the results which are then stored into an instance that provide accessors to easily retrieve the climate variables 4 application examples to illustrate the potential applications of the web api we provide four examples the first three are based on the r client whereas the last one uses the java client the r scripts to reproduce the first three examples can be found in the supporting information 4 1 spring frost and tomato seedlings tomato seedlings are sensitive to spring frost damage can occur at temperatures below 2 c drown 1985 therefore it would be helpful to identify the first calendar day for which the minimum temperature remained consistently above this threshold over the last 30 years let us focus on a location close to ottawa canada 45 25 n 76 01 w using the r client we can easily retrieve the minimum temperature from early may to the end of june for all the years from 1991 to 2021 and plot them against the calendar day fig 3 it turns out that the minimum temperature never went below 2 c after may 29 during the last 30 years and this date could be considered as safe to avoid spring frost when planting tomato seedlings outdoors although other factors will likely affect the subsequent growth and harvest 4 2 spruce budworm phenology spruce budworm is a major defoliator in the province of quebec canada it feeds mainly on balsam fir abies balsamea l mill and white spruce picea glauca moench voss and to a lesser extent on red spruce picea rubens sarg and black spruce picea mariana mill bsp outbreaks occur approximately once every 40 years boulanger and arsenault 2004 during outbreaks four consecutive years of moderate to severe defoliation significantly increase the mortality of balsam fir trees pothier and mailly 2006 spruce budworm larvae emerge in late april to mid may their population dynamics depend on many factors including foliage dynamics as well régnière 1982 developed a process oriented model to predict the seasonal development of spruce budworm populations this model which has undergone several improvements since its initial publication régnière et al 2012 was implemented as a mts processing model in biosim and it is part of the web api montmorency experimental forest 47 19 n 71 09 w is located north of quebec city canada this area is dominated by one of the host species namely balsam fir the predicted proportion of the population that reaches different successive development stages during spring and summer 2023 can be obtained by generating the mts for this location and processing it using the mts processing model fig 4 less than 10 of the initial population of overwintering larvae survive to the adult stage 4 3 stochastic predictions of maximum temperature heat waves are a concern for some populations as they are associated with health issues patz et al 2014 we simulated the annual highest temperature over the 2022 2100 period for the municipality of chibougamau canada 49 56 n 74 23 w the weather generation relies on the stochastic disaggregation of 30 year normals consequently generating several mts gives a better idea of the variability around the mean trend in this example we produced 10 realizations of mts for this period under two future climate scenarios rcp 4 5 and rcp 8 5 fig 5 the simulation shows that annual maximum temperature peaked at 35 c after 2060 under rcp 4 5 in contrast peaks as high as 37 c could be reached after 2060 under rcp 8 5 4 4 integration of climate variables into forest growth simulation capsis is a java programmed platform for forest growth simulators dufour kowalski et al 2012 one of its simulators is currently used to support forest management planning on publicly owned forest lands in the province of quebec this simulator artemis follows the typical architecture of individual based models in forestry porté and bartelink 2002 it predicts tree mortality the growth of survivors and the recruits of new individuals all these submodels were statistically fitted to a large dataset of permanent plot data some of which were sensitive to 30 year temperature and precipitation normals fortin and langevin 2012 these 30 year normals were produced by biosim the capsis platform aims at providing user friendly growth simulators assuming that the users will download biosim and retrieve the 30 year normals by themselves before simulating with artemis will certainly discourage many users without mentioning the possible errors in manipulating the data this issue was raised in the introduction of this paper to avoid such a situation the java client was embedded in the capsis platform upon the initialization of the artemis simulator the geographical coordinates of the points of interest are sent to the web api which sends the 30 year temperature and precipitation normals back to the capsis platform fig 6 the large area growth projections in melo et al 2019 and fortin et al 2021 relied on the java client to retrieve the climate variables before running the simulations with artemis we used a random plot from the fourth campaign of quebec s provincial forest inventory plot 1507602703 is located in a forest management unit near the municipality of chibougamau it was measured in 2015 following the inventory protocol mfwp 2016 it covers an area of 400 m 2 in which all trees with diameter at breast height 1 3 m in height greater than 9 cm were tallied black spruce was the dominant species accompanied by jack pine pinus banksiana lamb in a smaller proportion this plot was imported in the capsis platform and its growth was simulated with the artemis simulator for the 2015 2105 period under three climate scenarios no change rcp 4 5 and rcp 8 5 the simulation showed that this plot has a greater all species standing volume at the end of the 21st century under rcp 4 5 and 8 5 compared to the no change scenario fig 7 this simulation does not account for any natural or human made disturbance though 5 discussion and conclusions there are numerous weather generators available from the literature in addition to those we already mentioned e g ivanov et al 2007 semenov 2008 wang et al 2016 peleg et al 2017 all these weather generators share the same ideas of downscaling global projections and spatially interpolating mts the methods used to perform these two tasks differ across the weather generators for instance many downscaling techniques exist and they can be classified into four categories regression methods weather pattern approaches stochastic weather generators and limited area climate models wilby and wigley 1997 they all have limitations that can affect the reliability of the predictions in some contexts wilby et al 2002 our objective was not to present the pros and cons of biosim with respect to other weather generators it can be said that biosim has advantages and limitations compared to other simulators our focus was more on how weather generators can become more accessible and better integrated into a work flow there are plenty of examples of weather generators being used to provide mts as inputs for models in environmental sciences and in particular in forestry and hydrology e g le goff et al 2009 fatichi et al 2011 schlabing et al 2014 melo et al 2019 efforts have been made to provide user friendly access to these weather generators through standalone or web applications e g schlabing et al 2014 wang et al 2016 however to the best of our knowledge these always imply additional steps in the work flow namely saving the mts generated by the weather generator to disk and reimporting them into the simulation framework our solution to avoid these additional steps was to implement the weather generator into a web api and to provide some clients that can be used for a proper integration into a simulation framework our application examples show that climate variables can easily be retrieved in r and java acknowledging that r is a popular language tiobe 2022 and especially in the scientific community our web api makes it easy to integrate biosim into the work flow of an r script in addition to facilitating the use of a weather generator such a web api has at least two other benefits first it ensures that the users are always using the latest version of the weather generator as we pointed out in the introduction the web api avoids version conflicts since the clients are always connected to the latest version of biosim namely that embedded in the web api secondly it also facilitates the interoperability between the languages since the communication between the client and the web api is based on character strings our application example on the integration of climate variables into forest growth simulation is a good example of interoperability fig 6 as long as the client can produce http requests and parse the results the language does not matter for instance the language of two of our clients is different from that of the web api and this did not cause any problem rest apis such as the one shown in this study have become a standard for interoperability between languages and devices di martino et al 2017 the downside of a weather generator web api is the computational burden generating mts is not trivial depending on the time span and the computer cpu speed biosim can take up to half a second to produce an mts for a single point having the weather generator on a server means that there is only one application running for potentially many clients having many clients sending multiple requests at the same time is likely to increase the computational time we managed to control this delay by limiting the number of points in a single request to 10 and by having a multi process implementation on the web api side however each process instantiates a generator or a mts processing model and these instances especially the generators require a share of ram this results in a greater ram utilization moreover a multi process implementation requires the cpu to have a correspondingly larger number of cores as a consequence the deployment of a weather generator web api requires a greater capacity than that of most personal computers another limitation of such a web api is that it does not allow for rollback to a previous version of the biosim application by having the application on a server and making it accessible through a web api we ensure that the users are all using the latest version however some users might want to keep using a particular version of the biosim application for their own reasons this is impossible with the current web api a possible workaround consists of having different web api one for each version of the biosim application but this would imply additional resources given that the number of users interested in previous versions of biosim is limited we do not intent to create web api for these versions this could be envisaged if a new version of biosim was to be released we acknowledge that biosim has limitations just as any other weather generator the disaggregation of monthly normals produces stochastic mts although it can be expected that the occurrence of extreme events in stochastic mts resembles that of observed mts it has never been formally validated in order to avoid the pitfall of individual models it is recommended to use multimodel ensembles instead of a single model especially when generating regional and global climate projections krishnamurti et al 2000 following this line of thought we strongly encourage the implementation of other weather generators into web api like ours so that end users can get access to a large array of weather generators within their simulation framework software availability name of software biosim web api and clients developers jean françois lavoie rémi saint amant and mathieu fortin available since 2020 hardware required no specific requirement software required java c and r depending on the client availability clients are available on the web site website https github com rncan biosimclient csharp wiki license lgpl v3 cost free program languages c for the web api and c r and java for the clients program size java c and r clients 200 kb each declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful to the data hub team natural resources canada for the support in the migration of a preliminary version of the web api into the net environment this project was funded by the canadian forest service through its internal scientific program developing sustainable fibre solutions abbreviations api application programming interface cpu central processing unit dll dynamic link library ftp file transfer protocol gcm general circulation model http hypertext transfer protocol iis internet information services json javascript object notation mts meteorological time series ram random access memory rcp representative concentration pathway rest representational state transfer appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105476 appendix a supplementary data the following is the supplementary material related to this article mmc s1 r script tomato seedling example 1 mmc s2 r script spruce budworm example 2 mmc s3 r script maximum temperature example 3 
25513,climate is an essential component of environmental models over the last two decades many weather generators have been presented in the literature although their implementation into software has been of great help to environmental modellers their lack of integration into modelling frameworks still represents a challenge for end users in many cases end users have to retrieve the climate variables by themselves in order to use an environmental model in some other cases the weather generator software is embedded into the modelling framework but this increases the maintenance effort in this paper we present a different approach the deployment of a weather generator as a web api a few application examples are provided to illustrate the benefits of this implementation in summary a web api facilitates the integration into modelling frameworks decreases the maintenance effort and avoid interoperability issues due to different programming languages keywords weather generator web api client server architecture interoperability software maintenance software integration 1 introduction climate change has brought the attention of the scientific community on the effect of climate variables in environmental models climate sensitive models are needed especially in agriculture and forestry to assess the impacts of climate change on growth and crops however the integration of climate variables into environmental models poses two major challenges first meteorological time series mts are recorded in weather stations that is at punctual geographical locations in contrast models have a much larger geographical coverage and mts must be literally available wall to wall secondly in order to assess the impact of climate change climate projections must be available these projections are usually produced by general circulation models gcm that have a much coarser resolution than what is needed in environmental modelling in spite of these two challenges a significant number of weather generators have been published in the literature over the last two decades e g bannayan and hoogenboom 2008 birt et al 2010 burton et al 2013 the wall to wall availability of mts has been addressed through the interpolation or extrapolation of the observed mts at the nearest weather stations jolly et al 2005 as for regional and local climate projections these are obtained by downscaling the gcm projections e g wang et al 2013 although there are many weather generators available in the literature their implementation can cause some issues to ecological modellers and end users most weather generators are implemented as standalone applications programmed in various languages e g bannayan and hoogenboom 2008 kilsby et al 2007 liu et al 2009 for ecological modellers this usually implies downloading the standalone application exporting the geographic coordinates of interest from the modelling framework processing them in the standalone application and importing the mts back into the modelling framework when it comes to implementing climate sensitive ecological models into software and transferring them to end users the situation can get worse unless the weather generator can be embedded into the same software end users must download a weather generator hopefully the same that provided the mts for the model fitting and generate the mts by themselves in order to use the ecological model obviously this additional data processing reduces the probability of successful knowledge transfer to end users embedding a weather generator into the same software that implements the ecological model can be a solution however the different programming languages often represent a barrier moreover embedding a software into another has some limitations especially when it comes to updating unless the embedded weather generator can update itself modellers will have to publish a new software version containing the updated weather generator the end users will have to get aware of these new versions and to download them having disparate distributed versions increases the maintenance effort and makes the debugging process more complex another option consists of implementing a client server architecture a weather generator can be deployed on a server and the software implementing the ecological model can use a client to retrieve the mts from the server embedding a client into an existing software is simpler than embedding a whole weather generator it takes less space and requires fewer updates at the same time the weather generator on the server can be updated at any time ensuring that the clients are always dealing with the latest version the objective of this study was to develop this server client architecture in order to facilitate the use of weather generators for both environmental modellers and end users of environmental models more specifically we used an existing weather generator called biosim régnière et al 2017 which is currently used in north america and we deployed it on a server the server implementing the weather generator uses a representational state transfer rest application programming interface api that reads incoming hypertext transfer protocol http requests from the web we propose several clients in different languages that can easily be integrated into other software these clients can be used to produce the http requests and parse the incoming results of these requests this paper is structured as follows first we introduce the biosim weather generator and its implementation into a web api then some examples of applications are presented finally we discuss the advantages and weaknesses of the client server architecture in the context of weather generation 2 the biosim weather generator 2 1 geographical coverage by interpolation biosim is a weather generator initially designed to support the planning of pest management activities régnière et al 1995a b it has been in continuous development since the mid 1990s and has been used in many research projects focused on forest ecology forest fire dynamics and insect pest management e g tobin et al 2004 le goff et al 2009 coulombe et al 2010 biosim defines an mts as a series of daily values of the four basic climate variables minimum air temperature c maximum air temperature c precipitation mm and wind speed km h 1 depending on the weather station and the time period relative humidity and solar radiation w m 2 are also available if they are not they are simulated using the mt clim 4 3 model glassy and running 1994 thornton and running 1999 snow precipitation mm and snow depth accumulation cm are also simulated using a model adapted from that of brown et al 2003 the weather generator produces an mts for a particular location by interpolating from the mts in the k nearest weather stations the distance for identifying these k nearest stations is computed as a modified euclidean distance 1 d i δ x i 2 δ y i 2 100 δ z i 2 δ shore i 2 where δ x i δ y i δ z i and δ shore i are the differences km in longitude latitude elevation and distance to the shore between weather station i and the point of interest respectively note that the factor 100 in eq 1 gives a greater weight to the difference in elevation because of the environmental lapse rate i e the average temperature decrease with elevation which is about 6 c km 1 barry 2008 p 52 the mts used in the interpolation can be those observed or they can be simulated through the disaggregation of monthly 30 year normals the first option is mainly used to produce past mts whereas the second is the usual way of producing future mts note that future 30 year normals are themselves obtained through the downscaling of regional or general circulation model projections this downscaling is addressed in section 2 2 before interpolation an adjustment is performed on the mts of the k nearest weather stations based on local gradients the local gradients in minimum temperature maximum temperature and precipitation are evaluated through multiple linear regressions one for each climate variable fitted on the 30 year normals of the 25 nearest climate stations based on a distance function similar to eq 1 for instance for maximum temperature the local gradient would be evaluated through the following linear regression 2 t max i β 0 β 1 δ x i β 2 δ y i β 3 δ z i ɛ i δ x i δ y i and δ z i are the differences in longitude latitude and elevation between weather station i and the point of interest respectively and ɛ i is the residual error term once the regressions have been fitted they are used to adjust the minimum temperature maximum temperature and precipitation from the nearest weather stations to the coordinates of the point of interest then the values of these variables at the point of interest are calculated as the average of the adjusted values in the nearest climate stations weighted by the inverse squared distance i e 1 d i 2 2 2 simulating by disaggregation and future climate projections régnière and st amant 2007 improved an algorithm initially developed by régnière and bolstad 1994 with the aim of producing stochastic realizations of daily minimum and maximum air temperatures and precipitation from 30 year monthly normals the expected daily temperatures for a particular day are interpolated between the means of two successive months the algorithm uses the standard deviations of these variables the second order regressive terms and their cross correlation as observed during the 30 year period to compute a stochastic deviate that is added to the expectation mts generated through this disaggregation of 30 year monthly normals were found to reproduce seasonal patterns as well as the variability and extremes of temperature the predicted precipitation was not quite as accurate but remained realistic régnière and st amant 2007 simulating by disaggregation is mainly used to produce future mts however it assumes that future 30 year normals are available for each weather station biosim uses the delta method mosier et al 2014 also referred to as the perturbation method by some authors prudhomme et al 2002 fowler et al 2007 to scale climate projections from regional or general circulation models down to the weather stations the 30 year monthly normals for the period 1981 2010 are taken as reference at each weather station the difference between these 30 year monthly normals and the projections of a regional or a global circulation model for a future 30 year period is the so called delta the locally observed 30 year monthly normals are then updated using the delta in order to produce local future 30 year normals that biosim relies on to cover the whole 21st century future normals were produced using the delta method and climate projections from the hadley model the canadian global circulation model gcm4 and the canadian regional circulation model rcm4 collins et al 2011 salzen et al 2013 scinocca et al 2016 under the representative concentration pathways rcp 4 5 and 8 5 ipcc 2013 p 29 3 software implementation biosim was implemented in c as a standalone application for windows and it is freely available at ftp ftp cfl forestry ca regniere software biosim the core of the application is a dynamic link library dll implementing the weather generator this core is complemented by a series of smaller dlls that make it possible to derive information from the mts the annual monthly and daily degree days the soil moisture index and spruce budworm development count among the output of these additional dlls for the sake of clarity we will refer to the core dll as the generator and to these additional dlls as mts processing models the generator must get access to a dataset of 30 year normals to simulate by disaggregation it can also access a dataset of observed mts and generate mts from these given the number of weather stations in north america the weather generator can hardly be instantiated with all the observed mts the mts are rather broken down into smaller synchronous mts and a different weather generator is instantiated for each synchronous mts the same applies to the 30 year normals many weather generators are instantiated with different 30 year normals 3 1 the biosim web api 3 1 1 implementation in the context of this project we developed a rest api based on the net framework the api was implemented in c as an internet information services iis application which does the following tasks in order see fig 1 1 read incoming http requests 2 parse the requests and forward them to one or many instances of the generator 3 retrieve the mts generated by the generator instances 4 apply one or many mts processing models on the mts 5 send the result back to the client the biosim dlls are embedded in the api as a package the generator can be loaded with a dataset of 30 year normals and eventually a dataset of daily climate observations the combination of 30 year normals and daily climate observations is referred to as a context upon its initialization the api creates instances of the generator with different contexts in order to cover the whole 1900 2022 period for observed mts and as well as the 2022 2100 period for simulation by disaggregation the list of these contexts is provided in table 1 an incoming http request can refer to several contexts for instance a request for the period 1976 1986 will deal with generators related to contexts 2 and 3 in table 1 upon the uptake of a request the api determines which contexts are related to the request for past time series the range of data within the observed daily mts defines the time range covered by the context for example context 2 covers the 1950 1979 time range for future time series the different 30 year normals overlap the central 10 year period was set as the time range covered by each context associated to future time series for instance a request for the period 2041 2060 under rcp 4 5 with the hadley model would deal with context 7 for the time range 2041 2050 and with context 8 for the time range 2051 2060 the only exception to this is the last context that goes from 2071 to 2100 which was assumed to cover the time range 2081 2100 once the appropriate contexts have been identified the api retrieves their associated generators to produce mts the mts from different contexts are concatenated in order to create a single mts the api has several routes table 2 the main route is biosimweather which does the weather generation and processes the generated mts using one or many user specified mts processing models see table 3 an example of http request for the annual degree days over the 2000 2016 period at the eastern end of the reservoir gouin 48 30 n 74 30 w in the province of quebec canada using the main route is http repicea dynu net biosim biosimweather lat 48 5 long 74 5 from 2000 to 2016 model degreeday annual format json the result is returned in the javascript object notation json format multiple mts can be generated by specifying more than one points in the http request for example two mts are generated at once in the following request http repicea dynu net biosim biosimweather lat 48 5 2049 0 long 74 5 20 76 0 from 2000 to 2016 model degreeday annual format json note that the api allows for a maximum of 10 points per http request however the clients described in section 3 2 can handle a greater number of points by sending multiple requests of 10 points each the results of the multiple requests are concatenated into a single output the other routes shown in table 2 provide further information on the mts processing models and their parameters the api also implements an automatic update feature every day a new version of the 2021 2022 observed daily mts dataset is compiled with the latest meteorological observations and it is made available on a dedicated ftp site the api periodically checks if the version on the ftp site is more recent than the one used in context 4 table 1 whenever this is the case the new version of the dataset is downloaded and loaded in place of the previous version this feature ensures that the web api always relies on the latest observed mts 3 1 2 scalability and performance the c language allows for multithreading given that the api accepts several points per request this feature makes it possible to increase performance it is possible to instantiate more than one generator for each context listed in table 1 the different geographical points are then sent into a queue which feeds the different generator instances which in turn return the mts to the main thread the same implementation also applies to mts processing models see table 3 although a multithreading approach seemed promising preliminary trials showed that the marginal gain in adding new threads quickly decreased in fact there was little gain or no gain at all in performance when increasing the number of threads from five to 10 this was due to the memory allocation inside the threads because the api remained a single process application the memory allocation into the different threads is protected by locks which reduce the performance embedded staff 2006 especially when the threads must allocate large chunks of memory which was the case here to work around this memory issue we implemented a multiprocess architecture the different generators and mts processing models were instantiated in different processes and linked to the main process that of the api through pipes because each process is independent and contains a single generator or mts processing instance there is no constraint due to the locks associated to memory allocation preliminary results with this architecture showed that marginal gain in performance when adding new processes remained significant the performance of the web api was tested with 100 fictive requests sent on the biosimweather route each request had a random time span between 1950 and 2050 up to 100 locations and randomly asked for either the annual degree days or an extensive array of annual climatic variables these requests were sent to the web api running on an intel r xeon r cpu e5 1650 v3 3 50 ghz with each generator and mts processing model being instantiated three times in different processes the average time to process a single point in each request is shown in fig 2 by regression the average processing time was estimated at 28 8 and 11 4 milliseconds point 1 yr 1 for the extensive array of climate variables and for the degree days respectively 3 2 c java and r clients client applications are available in the c java and r languages they offer simple static methods that produce the http requests for the web api table 4 the r client actually uses the java client as back end and depends on the j4r package see fortin 2020 for the interoperability between r and java the r client exposes functions that bear the same names as the static methods of the java client table 4 the c client is a direct translation of the java client using one of the three clients has the advantage of breaking requests with a large number of points into smaller requests and concatenating the results which are then stored into an instance that provide accessors to easily retrieve the climate variables 4 application examples to illustrate the potential applications of the web api we provide four examples the first three are based on the r client whereas the last one uses the java client the r scripts to reproduce the first three examples can be found in the supporting information 4 1 spring frost and tomato seedlings tomato seedlings are sensitive to spring frost damage can occur at temperatures below 2 c drown 1985 therefore it would be helpful to identify the first calendar day for which the minimum temperature remained consistently above this threshold over the last 30 years let us focus on a location close to ottawa canada 45 25 n 76 01 w using the r client we can easily retrieve the minimum temperature from early may to the end of june for all the years from 1991 to 2021 and plot them against the calendar day fig 3 it turns out that the minimum temperature never went below 2 c after may 29 during the last 30 years and this date could be considered as safe to avoid spring frost when planting tomato seedlings outdoors although other factors will likely affect the subsequent growth and harvest 4 2 spruce budworm phenology spruce budworm is a major defoliator in the province of quebec canada it feeds mainly on balsam fir abies balsamea l mill and white spruce picea glauca moench voss and to a lesser extent on red spruce picea rubens sarg and black spruce picea mariana mill bsp outbreaks occur approximately once every 40 years boulanger and arsenault 2004 during outbreaks four consecutive years of moderate to severe defoliation significantly increase the mortality of balsam fir trees pothier and mailly 2006 spruce budworm larvae emerge in late april to mid may their population dynamics depend on many factors including foliage dynamics as well régnière 1982 developed a process oriented model to predict the seasonal development of spruce budworm populations this model which has undergone several improvements since its initial publication régnière et al 2012 was implemented as a mts processing model in biosim and it is part of the web api montmorency experimental forest 47 19 n 71 09 w is located north of quebec city canada this area is dominated by one of the host species namely balsam fir the predicted proportion of the population that reaches different successive development stages during spring and summer 2023 can be obtained by generating the mts for this location and processing it using the mts processing model fig 4 less than 10 of the initial population of overwintering larvae survive to the adult stage 4 3 stochastic predictions of maximum temperature heat waves are a concern for some populations as they are associated with health issues patz et al 2014 we simulated the annual highest temperature over the 2022 2100 period for the municipality of chibougamau canada 49 56 n 74 23 w the weather generation relies on the stochastic disaggregation of 30 year normals consequently generating several mts gives a better idea of the variability around the mean trend in this example we produced 10 realizations of mts for this period under two future climate scenarios rcp 4 5 and rcp 8 5 fig 5 the simulation shows that annual maximum temperature peaked at 35 c after 2060 under rcp 4 5 in contrast peaks as high as 37 c could be reached after 2060 under rcp 8 5 4 4 integration of climate variables into forest growth simulation capsis is a java programmed platform for forest growth simulators dufour kowalski et al 2012 one of its simulators is currently used to support forest management planning on publicly owned forest lands in the province of quebec this simulator artemis follows the typical architecture of individual based models in forestry porté and bartelink 2002 it predicts tree mortality the growth of survivors and the recruits of new individuals all these submodels were statistically fitted to a large dataset of permanent plot data some of which were sensitive to 30 year temperature and precipitation normals fortin and langevin 2012 these 30 year normals were produced by biosim the capsis platform aims at providing user friendly growth simulators assuming that the users will download biosim and retrieve the 30 year normals by themselves before simulating with artemis will certainly discourage many users without mentioning the possible errors in manipulating the data this issue was raised in the introduction of this paper to avoid such a situation the java client was embedded in the capsis platform upon the initialization of the artemis simulator the geographical coordinates of the points of interest are sent to the web api which sends the 30 year temperature and precipitation normals back to the capsis platform fig 6 the large area growth projections in melo et al 2019 and fortin et al 2021 relied on the java client to retrieve the climate variables before running the simulations with artemis we used a random plot from the fourth campaign of quebec s provincial forest inventory plot 1507602703 is located in a forest management unit near the municipality of chibougamau it was measured in 2015 following the inventory protocol mfwp 2016 it covers an area of 400 m 2 in which all trees with diameter at breast height 1 3 m in height greater than 9 cm were tallied black spruce was the dominant species accompanied by jack pine pinus banksiana lamb in a smaller proportion this plot was imported in the capsis platform and its growth was simulated with the artemis simulator for the 2015 2105 period under three climate scenarios no change rcp 4 5 and rcp 8 5 the simulation showed that this plot has a greater all species standing volume at the end of the 21st century under rcp 4 5 and 8 5 compared to the no change scenario fig 7 this simulation does not account for any natural or human made disturbance though 5 discussion and conclusions there are numerous weather generators available from the literature in addition to those we already mentioned e g ivanov et al 2007 semenov 2008 wang et al 2016 peleg et al 2017 all these weather generators share the same ideas of downscaling global projections and spatially interpolating mts the methods used to perform these two tasks differ across the weather generators for instance many downscaling techniques exist and they can be classified into four categories regression methods weather pattern approaches stochastic weather generators and limited area climate models wilby and wigley 1997 they all have limitations that can affect the reliability of the predictions in some contexts wilby et al 2002 our objective was not to present the pros and cons of biosim with respect to other weather generators it can be said that biosim has advantages and limitations compared to other simulators our focus was more on how weather generators can become more accessible and better integrated into a work flow there are plenty of examples of weather generators being used to provide mts as inputs for models in environmental sciences and in particular in forestry and hydrology e g le goff et al 2009 fatichi et al 2011 schlabing et al 2014 melo et al 2019 efforts have been made to provide user friendly access to these weather generators through standalone or web applications e g schlabing et al 2014 wang et al 2016 however to the best of our knowledge these always imply additional steps in the work flow namely saving the mts generated by the weather generator to disk and reimporting them into the simulation framework our solution to avoid these additional steps was to implement the weather generator into a web api and to provide some clients that can be used for a proper integration into a simulation framework our application examples show that climate variables can easily be retrieved in r and java acknowledging that r is a popular language tiobe 2022 and especially in the scientific community our web api makes it easy to integrate biosim into the work flow of an r script in addition to facilitating the use of a weather generator such a web api has at least two other benefits first it ensures that the users are always using the latest version of the weather generator as we pointed out in the introduction the web api avoids version conflicts since the clients are always connected to the latest version of biosim namely that embedded in the web api secondly it also facilitates the interoperability between the languages since the communication between the client and the web api is based on character strings our application example on the integration of climate variables into forest growth simulation is a good example of interoperability fig 6 as long as the client can produce http requests and parse the results the language does not matter for instance the language of two of our clients is different from that of the web api and this did not cause any problem rest apis such as the one shown in this study have become a standard for interoperability between languages and devices di martino et al 2017 the downside of a weather generator web api is the computational burden generating mts is not trivial depending on the time span and the computer cpu speed biosim can take up to half a second to produce an mts for a single point having the weather generator on a server means that there is only one application running for potentially many clients having many clients sending multiple requests at the same time is likely to increase the computational time we managed to control this delay by limiting the number of points in a single request to 10 and by having a multi process implementation on the web api side however each process instantiates a generator or a mts processing model and these instances especially the generators require a share of ram this results in a greater ram utilization moreover a multi process implementation requires the cpu to have a correspondingly larger number of cores as a consequence the deployment of a weather generator web api requires a greater capacity than that of most personal computers another limitation of such a web api is that it does not allow for rollback to a previous version of the biosim application by having the application on a server and making it accessible through a web api we ensure that the users are all using the latest version however some users might want to keep using a particular version of the biosim application for their own reasons this is impossible with the current web api a possible workaround consists of having different web api one for each version of the biosim application but this would imply additional resources given that the number of users interested in previous versions of biosim is limited we do not intent to create web api for these versions this could be envisaged if a new version of biosim was to be released we acknowledge that biosim has limitations just as any other weather generator the disaggregation of monthly normals produces stochastic mts although it can be expected that the occurrence of extreme events in stochastic mts resembles that of observed mts it has never been formally validated in order to avoid the pitfall of individual models it is recommended to use multimodel ensembles instead of a single model especially when generating regional and global climate projections krishnamurti et al 2000 following this line of thought we strongly encourage the implementation of other weather generators into web api like ours so that end users can get access to a large array of weather generators within their simulation framework software availability name of software biosim web api and clients developers jean françois lavoie rémi saint amant and mathieu fortin available since 2020 hardware required no specific requirement software required java c and r depending on the client availability clients are available on the web site website https github com rncan biosimclient csharp wiki license lgpl v3 cost free program languages c for the web api and c r and java for the clients program size java c and r clients 200 kb each declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful to the data hub team natural resources canada for the support in the migration of a preliminary version of the web api into the net environment this project was funded by the canadian forest service through its internal scientific program developing sustainable fibre solutions abbreviations api application programming interface cpu central processing unit dll dynamic link library ftp file transfer protocol gcm general circulation model http hypertext transfer protocol iis internet information services json javascript object notation mts meteorological time series ram random access memory rcp representative concentration pathway rest representational state transfer appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105476 appendix a supplementary data the following is the supplementary material related to this article mmc s1 r script tomato seedling example 1 mmc s2 r script spruce budworm example 2 mmc s3 r script maximum temperature example 3 
25514,forest landscapes pattern and development are affected by environment and disturbance disentangling their effects is important to understanding current landscape and predicting future changes such studies are limited by short term observation and sparse disturbance history data spatially explicit forest landscape modeling represents a solution to these limitations here we reconstructed the 300 year time series 1710 2010 of post volcanic eruption forest landscapes experiencing periodic typhoons in changbai mountain china using landis forest landscape model we used a factorial simulation design to quantify the main and interactive effects of environment and typhoon on forest landscape recovery results showed environment had dominant effects 80 on early recovery 1710 1760 suggesting early forest development follows deterministic community assembly processes governed by environment however as forest matured disturbance became dominant 50 at later recovery stages 1860 2010 this study showed that historical landscape reconstruction reveals the full spectrum of interplays of environment disturbance and succession in forest ecosystems which may not be captured by short term studies keywords changbai mountain environment historical landscape reconstruction landis pro typhoon disturbance post volcanic eruption forest landscape recovery data availability data will be made available on request software availability software name landis pro developer gis and spatial analysis laboratory at the university of missouri columbia in collaboration with the usda forest service northern research station operating system windows program languages c c cost free availability source code executable and model documentation are publicly available and can be cloned from the github repository at https github com landispro 1 introduction the formation of forest ecosystems across a landscape is affected by factors e g environment climate terrain and soil etc disturbance across a range of spatial and temporal scales oliver and larson 1996 svenning and others 2006 garzon lopez and others 2014 li and others 2016 climate is a dominant factor regulating distribution of forest ecosystems at regional scales pearson and dawson 2003 siefert and others 2012 osland and others 2017 under stable climatic conditions tree species gradually assemble into specific communities to adapt to regional climate clements 1916 1936 weiher and keddy 1999 and thus forests success along the determined pathways to reach vegetation environment equilibriums i e late successional climax stages bazzaz 1991 weiher and keddy 1999 turner 2010 at landscape scales disturbances are common processes that disrupt the deterministic succession pathways in forest ecosystems flynn and others 2010 johnstone and others 2010 marra and others 2018 huang and others 2017 vegetation environment equilibrium is predicted to occur only in systems with small and infrequent disturbances while large and frequent disturbances often lead to high variations in landscape pattern between pre and post disturbances and may even shift forest ecosystem successional pathways turner and others 1993 turner 2010 environment encapsulates regional sale climate and in situ terrain and soil to interact with site scale processes to determine tree species occurrence composition and distribution at broad scales chesson 1986 snyder and chesson 2004 meier and others 2010 cadotte and tucker 2017 the complex interactions between extrinsic ecological factors i e environment disturbances and inherent ecological processes i e succession competition at various temporal spatial scales may result in unknown and intractable changes in forest landscapes over time mccook 1994 oliver and larson 1996 kasel and others 2017 lucash and others 2018 understanding and separating the interacting effects of environment and disturbance on forest ecosystems across the landscape is important to understanding current forest landscapes and predicting future changes however such a task is challenging because the current forest landscape is the product of the interactive and cumulative effects of stochastic disturbances environment and forest succession across space studies of this sort are frequently limited by relatively short observations e g several decades of changes in forest landscapes and spatially sparse disturbance history data e g paleoecological and dendrochronological wimberly and spies 2001 franklin and others 2006 spatially explicit reconstruction of historical landscapes over time can be used to assess the effects of environment and disturbance forest landscape models flms are an effective tool for such tasks since they are designed to simulate forest dynamics at site scales with disturbance at landscape scales while incorporating variable environments across the landscape he 2008 wang and others 2013 wang and others 2014a flms simulate forest dynamics formed in time from a historical starting point which can be derived from historical data and are driven by the theories of forest stand dynamics and disturbance ecology wu and others 2020 xu and others 2020 the theories presented in oliver and larson 1996 generalizes forest development pattern and mechanism with and without disturbances the reconstructed forest landscapes can be benchmarked using current field data and forest patterns to check not only the end product but the intermediate processes are defensible wang and others 2014b by spatially explicit reconstruction of historical landscapes the effects of environment and disturbance and their interaction on a forest landscape can be separately quantified using a factorial simulation design changbai mountain experienced a massive volcanic eruption in 946 ad iacovino and others 2016 the eruption removed almost all forests in a 50 km radius and forced forest succession to restart zhao 1984 liu and others 1995 to date forest zonation has developed on the north side of changbai mountain with temperate boreal and alpine dwarf forest ecosystems growing along elevational zones that correspond to their latitudinal distributions yang 1981 xu and others 2004 however such vertical forest zones have not developed on the west and south sides of the mountain which are disturbed by periodic typhoons that might have been prevailing even before the volcanic eruption with succession and disturbance history after the volcanic eruption changbai mountain presents an ideal platform to reconstruct its historical landscapes and reveal the effects of typhoon and environment on forest landscape recovery in this study we reconstructed the post eruption forest landscape of changbai mountain in northeast china using landis pro forest landscape model landis pro forest landscape model is designed to simulate forest dynamics at landscape scales while incorporating variable environments across the landscape he 2008 recently a series studies used landis pro to investigate forest landscape dynamics under various environments and disturbance regimes and quantify the effects of disturbance environment and forest succession for example wang and others 2019 predicted future forest landscape dynamics to quantify the effects of climate change and timber harvest on forest dynamics huang and others 2021 and duan and others 2021 simulated forest landscape dynamics affected by fire and insect disturbances in the context of future climate change and both studies quantified the direct and indirect effects of climate change to future disturbance regimes in the context of these studies this study focused on the influence of environment and typhoon on the development of post volcanic eruption forest landscape our study is a complement to wang and others 2019 huang and others 2021 and duan and others 2021 in quantifying the effects of various drivers on forest landscape change specifically we hypothesized that 1 environment exerts dominant roles in the early stage of post volcanic eruption forest landscape recovery and its role reduces as forests reach the mid and late successional stages 2 the effects of typhoon increase over time and will overtake the environment to become dominant in the later recovery stage and 3 recurrent typhoons weaken the spatial pattern of forest climate equilibrium and lead to fragmented forest landscapes 2 approach and methods 2 1 study area our study area was in changbai mountain of china side with elevations ranging between 780 and 2 652 m 41 28 42 28 n 127 32 128 52 e fig 1 a as elevation increases the average climate shifts from temperate monsoon to cold zone with corresponding long severe winters and short warm summers the annual mean temperature ranges from 7 c at low elevations to 3 c at high elevations while the annual precipitation increases from about 760 mm at low elevations to about 2 000 mm at the mountain top distinctive ecosystems correspond with elevation with mixed korean pine and broadleaf forests dominating the low elevation zone mixed korean pine broadleaf forest zone 780 1100 m evergreen coniferous forests dominating the mid elevation zone evergreen coniferous forest zone 1100 1700 m subalpine forests typifying the higher elevation zone subalpine forest zone 1700 2100 m and tundra at the highest elevation zone above 2100 m the region contains fairly high levels of plant diversity in the mixed korean pine broadleaf forest zone the overstory is dominated by korean pine pinus koraiensis siebold and zucc basswood tilia amurensis rupr maple acer mono maxim elm ulmus davidiana planch var japonica rehd nakai ash fraxinus mandshurica rupr in the evergreen coniferous forest zone the dominant tree species include jezo spruce picea jezoensis siebold and zucc and manchurian fir abies nephrolepis trautv maxim in the subalpine forest zone mountain birch betula ermanii cham is the dominant tree species there are scattered early successional patches dominated by asian white birch betula platyphylla suk aspen poplus davidiana dode mongolian oak quercus mongolica fisch ledeb and olga bay larch larix olgensis a henry the above tree species account for 90 of stand volume in the region hao and others 2008 shao and deng 2000 in the old growth forests in this region coarse woody debris forms 39 56 of total aboveground detritus i e litter fine wood and coarse woody debris and the ratio of coarse woody debris to live tree mass is 0 04 0 07 harmon and hua 1991 with the total mass of coarse woody debris from 7 9 16 2 mg ha in the mixed korean pine and broadleaf forests harmon and hua 1991 to about 53 4 mg ha in the evergreen coniferous forests zhou and others 2011 our study area included the area disturbed by millennium eruption that occurred 946 a d iacovino and others 2016 and the adjacent undisturbed area extending 15 km down slope the spatial extent of the areas affected by the volcanic eruption were determined from previous studies fig 1 wu and others 2020 which was approximately 3 8 105 ha forest recovery after the volcano eruption relied largely on seed sources and seed dispersal mcclanahan 1986 tautenhahn et al 2016 the undisturbed habitats below the eruption area and isolated refugia within the eruption area served as sources of seed fig 1 we regarded the undisturbed area at lower elevations surrounding the eruption area as matrix seed source area which linked to the eruption area via seed dispersal and gene flow we referred to the surviving seed sources in the refugia as remnant seed sources the size of matrix seed sources and the location of remnant seed sources were determined in wu and others 2020 with a similar study from mount st helens antos and zobel 1986 2 2 the assumptions for historical conditions field investigations in the eruption area show that the forest stand age is less than 300 years and decreases with increasing elevation near the volcano crater xu and liang 2010 this evidence suggests that tree species established less than 300 years ago then moved upwards and thick volcanic ash from the millennium eruption and erosion hinder tree establishment and survival for the first 700 years after the eruption evidence elsewhere in northeast china suggests that post volcanic eruption forest recovery has lagged for centuries after volcanic eruption since volcanic ash and erosion prohibited trees from establishment for instance the wudalianchi volcano in northern china erupted in 1720 and the volcanic area is still dominated by lichen and brush zhang and others 2005 in the low altitude areas of changbai mountain zhao 1984 found dead larch an early successional species stumps about 200 years in the current 100 year korean pine broadleaf forests late successional forests confirming that the early successional species colonized around 1710 and were replaced by mixed korean pine broadleaf forests around 1910 the establishment in high elevation areas were even later xu and liang 2010 found that the age of mountain birch an early successional species in the area above 1500 m asl was less than 200 years colonized around 1810 and decreased with increasing altitude and only 90 years colonized around 1920 at 2000 m they also concluded that the current forest vegetation is the first regeneration after the eruption in changbai mountain based on this evidence we hypothesized that few trees establishment and colonization during the first 700 years due to volcanic ashes and erosion and the forest colonization started about 300 years ago palaeoecological studies of historical climate reconstruction in changbai mt found that climate has been relatively stable with only some fluctuations over past 300 years except for the warming from 1980s shao and wu 1997 du and others 2018 in addition xu and others 2018 reported few significant changes in tree species composition and structure in the old growth forests during the warming period 1981 2016 in changbai mt thus there seems to be no significant impact of the later climate warming on shaping current forest landscape and it is reasonable to assume that the climate has remained stable during the development of current forest landscape moreover the current geomorphology of changbai mt was formed from the millennium eruption in 946 ad and thus the terrain is also considered to be relatively stable additionally the successful colonization of tree species from around 300 years ago implies that the erosion of volcanic ash from the millennium eruption had largely completed and tree establishment started for the past 300 years based on the above evidence we assumed that environment was relatively stable during the development of forest landscape over the past 300 years additionally because of the absence of typhoons at the low elevation in the matrix seed source area it is reasonable to assume that undisturbed forests in matrix seed source area have reached vegetation climate equilibrium reflective of historical forests in 1710s charcoal study confirmed that pre eruption forest composition korean pine fir spruce larch basswood birch and ash was nearly the same as the current forests zhao 1984 thus we can use the current old growth forests to represent the 1710 forests in the matrix forests additionally to evaluate the uncertainties associated with this assumption we conducted a sensitivity analysis on the initial conditions species composition and structure of the old growth forests and found that 30 changes in dominant species abundance in the matrix forests did not significantly affect the reconstructed forest landscapes see supplements 2 3 the framework for reconstructing post volcanic eruption forest landscapes in this study we present a novel framework for reconstructing post volcanic eruption forest landscapes to our knowledge there are few studies of spatial and temporal reconstruction of historical landscapes of past centuries such a work involves using a forest landscape model that tracks spatially explicit information of parent tree location and abundance seed dispersal forest dynamics and succession and environment heterogeneity the novelty of our framework includes integrating old growth forests and disturbance history data with theories of forest stand dynamics and disturbance ecology through landis pro forest landscape model we also validated the reconstructed landscapes against contemporary forest inventory data and classified remote sensing data and verified the intermediate trajectories of stand dynamics against theories of stand dynamics 2 4 landis pro model landis pro is a raster based landscape model that tracks the number of trees by species and by age cohort within each pixel wang and others 2013 wang and others 2014a it integrates species stand and landscape scale processes e g wind to simulate forest succession dynamics over large spatial and temporal scales wang and others 2014a landis pro simulates forest succession by accounting for species specific demographic processes which are driven by the species vital attributes i e longevity age of reproductive maturity shade tolerance seed dispersal distance stand density index and species establishment probability sep and maximum growing space mgso for each environment in this study we modeled the 12 most dominant tree species mentioned above we derived the species biological attributes from previous studies and sep and mgso based on ecosystem process model linkages model he and others 2002 liang and others 2012 2014 2 5 environmental data and historical typhoon regimes in this study environment represents the assembly of various physical variables at various spatial scales that encapsulates regional scale climate and in situ terrain including multiple terrain factors and soil thus environment in our study encapsulates slope aspect and elevation as well as soil and climate the effects of environment on post volcanic eruption forest recovery refer to how given configurations of environmental factors climate terrain and soil combinations regulate the forest succession and landscape formation environment was derived from elevation zones corresponding to climate zones superimposed with topography 30 m digital elevation model and soil 1 km resolution soil data included thickness of soil layers soil texture field capacity soil organic matter and soil nitrogen content which derived from china soil database http www soil csdb cn thus the environment covered the entire study area as numerous small homogeneous units with similar environmental conditions i e climate terrain soil conditions fig 1c we classified the whole area into 168 climate topography soil types which were derived from the overlay of climate terrain and soil layers for at each climate terrain soil type we derived sep which represents the response of species to the environment the higher sep indicates the stronger adaptability of species to the environment and easier establishment and colonization of tree species in this environment type information referring to early typhoon disturbances was not readily available for the past 300 years thus we determined the historical typhoon regime by combining remote sensing data and field inventory first we identified historical typhoon disturbances by interpreting landsat tm etm images from 1980 to 2016 https earthexplorer usgs gov since there were no other stand replacing disturbances except typhoon the early successional patches scattered in the matured forests were deemed to be historical typhoon patches these patches were readily identified from the remote sensing images due to their distinct characteristics in the surrounding matrix shao and others 1996 there were 140 typhoon patches identified with small patches occurred much more frequently than large ones following lognormal distribution supplement figure s2 s3 they covered 26799 3 ha or 19 3 of the region supplement figure s2 next we conducted extensive field investigations on 50 of the identified patches to determine stand age which reflects the time typhoon occurred there were 114 plots of white birch aspen forests 129 plots of larch forests and 91 plots of meadows supplement figure s2 the oldest typhoon events could be dated back to 1900 110 years ago suggesting that all the typhoon patches identified are typhoons occurred within the last 110 years typhoon patches of the same occurrence year were counted as one typhoon event we calculated mean disturbance size using the typhoon patches with known time since disturbance which is 6968 3 ha or about 5 0 of the study area we calculated mean typhoon return interval which is 555 6 year following the approach from johnson 1992 to extrapolate the typhoon regime to the 300 years 1710 2010 we calculated that the typhoon disturbed area was 54 0 300 555 6 of the region with 11 54 0 5 0 main typhoon events the 1986 typhoon recorded a maximum sustained wind of about 30 m s guo and others 2015 which resulted in 80 tree mortality as a result of uprooting xue 2009 and 1 21 million cubic meters of fallen wood covering 71 2 of the forest stock in the windfall area yu and han 2016 thus we assumed the similar intensity and severity for other typhoons in this study especially to avoid the circular use of the historical typhoon data the disturbance parameters mean return interval mean disturbance size etc were inputted to landis which replicated 20 times to capture the stochastic behavior of typhoons 2 6 experimental design we designed 2 2 factorial experimental modeling scenarios for environment realistic heterogeneous vs hypothetical homogenous environment and typhoon recurrence vs absence to evaluate the individual and interactive effects of these factors the treatment combinations resulted in four simulation scenarios homogeneous environment without typhoon nn heterogenous environment with recurrent typhoons et heterogeneous environment without typhoon en and homogenous environment with recurrent typhoons nt the et scenario was realistic which was regarded as baseline scenario while the remaining three scenarios were unrealistic but used to complete the factorial model simulation design to quantify the effects of typhoon and environment on forest landscapes this design assumed that environment and typhoon were the two dominant extrinsic factors that affected forest landscape dynamics and typhoon is the main natural disturbance to forest ecosystem in our study area typically the forest landscape dynamics are affected by various disturbances which may include wildfire harvest in addition to typhoon however these disturbances such as wildfire and harvest have been insignificant in our study area wildfires rarely occurred and if occurred they are at very small scales due to the relatively high humidity abundant precipitation and long snow covered season large scale timber harvest has been absent due to low human density high elevation and remoteness of the region based on the realistic scenario we simulated the remaining three scenarios by disregarding environment or and typhoon while fixing other factors for the scenarios with homogenous environment we assumed that there is a uniform environment across the entire study area where all pixels were assigned the same species establishment probabilities seps the sep value for each species was the average across all ecoregions for each species the life attributes were kept the same among all simulation scenarios e g homogeneous environment and heterogeneous environment for the scenarios with heterogenous environment we determined the suitability of each environment by species for the scenarios with typhoons we incorporated the reconstructed typhoon regime in the simulation for each scenario we simulated spatio temporally explicit forest dynamics 1710 2010 at 10 year interval with 20 replicates additionally the intrinsic factors included tree establishment growth mortality competition and seed dispersal referred as forest dynamics which are the underlining forces simulated in all scenarios however the fundamental assumption here is that the simulated forest dynamics can reconstruct the post volcanic succession dating back to ca 900 ad for this we used the landis pro forest landscape model that is designed to simulate forest dynamics at species and stand scales and interactions with environment and disturbance in this study the reconstructed forest landscape the realistic scenario in our simulation design has gone through the verification and validation processes see below suggesting that the reconstructed landscape credibly captured the spatiotemporal trajectories of forest landscape change of the past 300 years wu and others 2020 in our current study framework the effects of disturbance on environment are not included indeed typhoon disturbance can temporarily alter the environment by increasing light availability through removing forest canopy reducing evapotranspiration and increasing soil water however from a long term perspective e g decades environmental conditions should remain relatively constant under the stable climate in our study we measured the effects of environmental factors by species establishment probability sep see above we assumed that seps remain unchanged before and after disturbance because our study is based on a long term simulation 2 7 field inventory for landis pro model parameterization and result verification we measured a total of 2055 trees diameter 5 cm at 41 old growth sites 20 20 m and recorded the species and size by each tree this information was used to parameterize the initial species composition of matrix forests in 1710 we calibrated model parameters and validated the simulated results using 135 20 20 m sample plots surveyed in 2010 and 2014 each plot recorded the number of trees of by species and diameters we used 2 3 of the data for model calibration and 1 3 of the data for result validation to calibrate the model we iteratively adjusted model parameters such as age dbh relationships by environment the number of seeds and maximum dbh per species until there were no significant differences paired t tests p 0 05 between the 2010 simulated species basal area and density and the 2010s inventory plots in different elevation zones to validate the results we compared the remaining 1 3 2010s forest inventory data to the simulated results in 2010 figure s5 to ensure simulated forest patterns were reasonable we compared the simulated distribution of forests types in 2010 with the forest cover type derived from remote sensing classification shao and others 1996 figure s6 to ensure the intermediate results 1710 2010 were credible we evaluated the simulated forest development trajectories with the expected trajectories using gingrich stocking charts figure s7 2 8 data analysis we classified tree species into four groups based on tree species assembly and distribution korean pine and broadleaf species group including korean pine and mid late successional broadleaf species such as maple ash basswood and elm spruce and fir group mountain birch group and early successional species group including white birch aspen and larch which are mainly related to historical typhoon and scattered in the study area the first three groups are specialists which represent specific tree species assembly within each forest zone while the last species group is a generalist because the distribution and abundance of early successional species group are relatively small we did not analyze the species group individually in addition we divided the forests into young 30 years near mature 30 70 years mature 70 120 years and old growth 120 years age groups within each pixel to analyze the forest landscape pattern we determined the combination of tree species groups and age groups at each pixel resulting in 16 possible forest types 4 tree species groups 4 age groups ex young korean pine and broadleaf forests we used basal area and tree density of each species group except for the early successional species group since it was uncommon as response variables to analyze forest dynamics at the stand pixel level the two attributes basal area and tree density reflect the size and abundance of tree species they are the two most common measurements in field based forest inventory other forest attributes e g biomass carbon stock are derived these two attributes we used contagion contag index as the response variable to evaluate forest landscape patterns li and reynolds 1993 contag represents the degree of clumping of patches that considers all patch types present in a landscape affected by both the dispersion and interspersion of patch types which ranges from 0 to 100 li and reynolds 1993 this metric was selected to assess patch diversity and sensitivity to fragmentation we estimated the absolute effect sizes and relative effects of environment typhoon disturbance and their interaction on the above response variables in early 1710 1760 middle 1760 1860 and late 1860 2010 recovery periods respectively we estimated the effect size of environment alone as the difference between scenarios en and nn scenario en scenario nn the effect size of typhoon disturbance alone as the differences between scenarios nt and nn scenario nt scenario nn and the combined effect as the difference between scenarios et and nn scenario et scenario nn the size of the interaction effect was calculated as the difference between the combined effects and the sum of the separate main effects of these two factors through the experimental modeling design we used repeated measures anova analysis of variance to quantify the relative effects of environment typhoon disturbance and their interactions by partitioning the proportion of total variation explained based on type iii sums of squares girden 1992 we used the relaimpo package grömping 2006 in r statistical software to compute the relative effects 3 results 3 1 result verification the reconstructed forest landscapes tracked the trajectories of basal area and density for all simulated species fig 2 a the simulated recovery pathways were supported by forest stand development theory fig s7 ensuring that the intermediate dynamics between 1710 and 2010 were realistic spatially the simulated distribution of various forests types in the et scenarios realistic in 2010 matched the distribution of current forest types figure s6 the simulated 2010 results showed high agreements with the current observed basal area and tree density in each elevation zone fig 2b and c especially the paired t tests results for species basal area at 2010 were t 1 07 df 11 p 0 31 at low elevation zone t 1 78 df 11 p 0 10 at mid elevation zone and t 1 46 df 11 p 0 17 at higher elevation zone the paired t tests results for density at 2010 were t 0 16 df 11 p 0 87 at low elevation zone t 1 54 df 11 p 0 15 at mid elevation zone and t 1 00 df 11 p 0 34 at higher elevation zone thus the end of the 300 year simulation in 2010 reasonably represented forest composition and structure in 2010s 3 2 the effects of environment and typhoon basal area relative effects of environment in the early recovery stage 1710 1760 were overwhelming 97 on total basal area of all species while the relative effects of typhoon were minimum 3 fig 3 a from the 1760 onward as tree size and forest cover increased over the landscape the relative effects of environment on total basal area decreased to 28 and the relative effects of typhoon increased to 72 surpassing environment in the mid recovery stage 1760 1860 fig 3a for each specialist species group in general the relative effects of both environment and disturbance on its basal area varied similarly to their effects on total basal area of all species while relative effects of typhoon remained lower than or close to those of environment throughout 300 year period fig 3b d the absolute effect sizes of both environment and typhoon disturbance generally coincided with the relative effects on total basal area for all tree species environment had a far greater effect size 0 43 m2 ha than typhoon 0 in the early recovery stage 1710 1760 fig 3e the effect sizes of environment on total basal area decreased to 0 26 m2 ha in the late recovery stage 1860 2010 the effect size of typhoon increased to 0 67 m2 ha and surpassed environment in the mid recovery stage 1760 1860 fig 3e at the species group level typhoons had a negative effect on the basal area in the korean pine and broadleaf species group and the mountain birch species group because of strong wind induced mortality among the large old trees fig 3f and h however typhoon had a positive effect on spruce and fir species group fig 3g because of the widespread regeneration of light demanding spruce seedlings in the areas affected by typhoons density environment had large effects on total tree density in the early recovery stage 1710 1760 and the relative effect was 98 fig 3i as forests steadily recovered the relative effect of environment gradually decreased to 48 while the relative effect of typhoon increased over time and became increasingly important 40 in the late recovery stage 1860 2010 fig 3i for each specialist species group the relative effects of both environment and typhoon on tree density varied consistently with those on total density of all species the relative effects of environment were over 80 in the early recovery stage 1710 1760 and then they decreased over time the combination of the relative effects of typhoon disturbance and its interaction with environment increased over time and exceeded the relative effects of environment except for the korean pine and broadleaf species group for which the combined effects were nearly equivalent to the effect of environment in the late recovery stage 1860 2010 fig 3j l the absolute effect sizes of both environment and typhoon disturbance generally coincided with the relative effects on total density for all tree species environment had a far larger effect size 1745 6 trees ha than typhoon 14 1 trees ha in the early recovery stage 1710 1760 fig 3m the effect size of environment on total density decreased to 342 9 trees ha in the late recovery stage 1860 2010 the effect size of typhoon increased to 357 8 trees ha and surpassed environment in the mid recovery stage 1760 1860 fig 3m after a typhoon event seedlings regenerated at a rate greatly exceeding tree mortality resulting in an increase in tree density for each specialist species group in the mid recovery stages reflected by positive effects of typhoon on tree density fig 3m p landscape fragmentation environment had an overwhelming effect on contag with the relative effect reaching 99 in the early recovery stage 1710 1760 fig 3q from the 1760 onward typhoon disturbance became increasingly important in shaping the forest landscape in the late recovery stage 1860 2010 typhoon exerted a dominant effect on contag and its relative effect reached 52 fig 3q the absolute effect sizes of both environment and typhoon disturbance generally varied consistently with their relative effects fig 3r the effect sizes of environment decreased from 5 0 in the early recovery stage 1710 2010 to 2 9 in the late recovery stage 1860 2010 fig 3r meanwhile the effect sizes of typhoon increased from 0 in the early recovery stage 1710 2010 to 3 3 in the late recovery stage 1860 2010 especially both environment and typhoon disturbance showed negative effects on contag fig 3r with the landscape being more fragmented smaller contag value under typhoon disturbances 4 discussion we presented an empirical case study of reconstructing historical forest landscapes and the disturbance regime using a forest landscape model the composition structure and spatial patterns of the reconstructed forest ecosystem reasonably represented current forest conditions after a 300 year reconstruction historical field data did not exist for a full verification of the reconstructed succession trajectories thus the trajectories were verified against the expected stand dynamics based on forest successional theory and an understanding of the natural regeneration dynamics of the study site oliver and larson 1996 wang and others 2014b moreover fluctuations in stand dynamics where total basal area decreased and total density increased corresponded to typhoon events which confirmed that the patterns of forest disturbance and recovery for typhoons were captured after 300 years of recovery the forests have differentiated into the three forest zones corresponding to climatic zones fig 4 suggesting that environment captured broad scale forest distribution patterns the above verifications suggested the validity of the reconstructed historical forest landscapes and disturbance regimes environment was shown as the dominant factor controlling forest ecosystem composition structure and landscape patterning during the early stage of forest recovery 1710 1760 although spatially explicit dispersal limitation and niche partitioning via environmental filtering was seen as the ultimate process responsible for the recovery fig 4 environment constrained the establishment and growth plotkin and others 2002 shen and others 2013 kraft and others 2015 within each elevation zone the environments suited one group of species over the other leading to an expected community assembly and stand structure xu and others 2004 lebrija trejos and others 2010 kraft and others 2015 environment drove the leading and trailing edges of the species distributions towards certain elevation zones corresponding to climatic zones where they thrive thus regulating the spatial distribution of the specialist tree species this is congruent with previous findings that environmental heterogeneity exerts the dominant influence on shaping forest patterns harms and others 2001 prada and stevenson 2016 however a study of second growth forests suggested that the effects of environment immediately after the disturbances may not be as dominant but would increase because of legacy effects of long term land use and two major hurricanes hogan and others 2016 our results further implies that early development of forest ecosystem tends to be a deterministic community assembly process and provides evidence that niche differences among species shape forest patterns kraft and others 2008 chase and myers 2011 our results showed that the dominance of environment declined in the later stages associated with the increased effect sizes of typhoon disturbance once forests reach the mid and late successional stages their composition generally remains stable and thus the effects of environment decrease in our case the increasing relative importance of typhoon disturbance was resulted from their cumulative effects over time however some studies have claimed that the impact of environment conditions increase with succession harrelson and matlack 2006 campetella and others 2011 this conclusion may have resulted from studies conducted within a relatively short timeframe like the early recovery stage in this study which would limit the understanding of the cumulative effects of disturbance or other factors we were able to spatially simulate that the effects of typhoons on forest ecosystem composition structure and landscape pattern through the reconstruction results showed that early in recovery period low tree density and small tree diameters low height made forests less susceptible to typhoons as tree density decreased and diameter increased forest ecosystems became more susceptible to typhoon damage rich and others 2007 lin and others 2020 the reconstructed forest landscapes revealed that typhoons had lasting impacts on forest composition and structure for up to many decades for instance the 1910 typhoon damaged older stands causing widespread mortality with affected forests changing from old to near mature forests which were not recovered by 1950 while the 1950 typhoon occurred that further reduced forest age and its legacy remained beyond 2010 likewise the impact of 1986 typhoon persisted through 2010 fig 4 when the recovery time from a disturbance exceeds the intervals between successive disturbances multiple stochastic disturbance events will overlap in space and create a complex landscape pattern as a result of the cumulative effects of disturbances turner 2010 other studies have also reported the compound effects of successive disturbances on forest ecosystems busing and others 2009 menges and others 2011 turner and others 2019 for example the cumulative effects of repeated hurricanes could change biomass and carbon balances busing and others 2009 and cause greater damage to the forest structure and function than a single disturbance event kim and others 2020 2019 similarly mclaren and others 2019 reconstructed the historical hurricanes over 155 years in two tropical montane rainforests in jamaica and also confirmed the cumulative effects of successive hurricanes on forest structure and diversity studies like ours involving 300 year landscape reconstruction offered a long term perspective of the cumulative effects of disturbance the time dependent cumulative effects highlighted the importance of historical disturbances in shaping current forest landscapes recent studies reported that climate change alters the frequency and intensity of disturbances which indirectly affects forest landscapes huang and others 2021 duan and others 2021 however in our study the effects of climate change on disturbance regimes and forest recovery have not been considered previous studies of historical climate in changbai mt found that climate has been relatively stable over the past 300 years except for the warming after 1980s shao and wu 1997 du and others 2018 so far there was only one typhoon event recorded in 1987 that destroyed large areas of forest guo and others 2015 thus although climate change can alter the frequency and intensity of disturbance such as fire huang and others 2021 2018 we hypothesized that the interactions of climate change and typhoon have been stable since our study focused on the historical landscapes additionally xu and others 2018 reported few significant changes in forest composition and structure in the old growth forests during the warming period 1981 2016 in changbai mt thus there was no significant impact of the later climate warming on the forest landscape thus our results should not be altered due to recent climate warming and the potential indirect effects of warming climate on typhoon disturbance our study is among the few that quantify the relative contribution of environment and typhoon disturbance over large spatial and temporal scales there are increasing concerns about the impact of global climate change and associated disturbance regime change on forest ecosystems vanderwel and purves 2014 lin and others 2020 knowledge about the relative impacts of environment and disturbance allows us to disentangle the two factors and examine their interactive effects this is especially important under changing environmental and disturbance regimes because changes in disturbance regimes predicted under climate warming such as increased disturbance intensity and frequency might lead to greater changes in forest ecosystem composition structure and landscape pattern johnstone and others 2010 turner and others 2019 lin and others 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the joint fund of national natural science foundation of china u19a2023 the national natural science foundation of china 31961133027 31971486 the natural science foundation of jilin scientific institute 20180520087jh and top notch young talents project of liaoning province xing liao talents project xlyc1907177 we thank sarah humfeld for improving the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105513 
25514,forest landscapes pattern and development are affected by environment and disturbance disentangling their effects is important to understanding current landscape and predicting future changes such studies are limited by short term observation and sparse disturbance history data spatially explicit forest landscape modeling represents a solution to these limitations here we reconstructed the 300 year time series 1710 2010 of post volcanic eruption forest landscapes experiencing periodic typhoons in changbai mountain china using landis forest landscape model we used a factorial simulation design to quantify the main and interactive effects of environment and typhoon on forest landscape recovery results showed environment had dominant effects 80 on early recovery 1710 1760 suggesting early forest development follows deterministic community assembly processes governed by environment however as forest matured disturbance became dominant 50 at later recovery stages 1860 2010 this study showed that historical landscape reconstruction reveals the full spectrum of interplays of environment disturbance and succession in forest ecosystems which may not be captured by short term studies keywords changbai mountain environment historical landscape reconstruction landis pro typhoon disturbance post volcanic eruption forest landscape recovery data availability data will be made available on request software availability software name landis pro developer gis and spatial analysis laboratory at the university of missouri columbia in collaboration with the usda forest service northern research station operating system windows program languages c c cost free availability source code executable and model documentation are publicly available and can be cloned from the github repository at https github com landispro 1 introduction the formation of forest ecosystems across a landscape is affected by factors e g environment climate terrain and soil etc disturbance across a range of spatial and temporal scales oliver and larson 1996 svenning and others 2006 garzon lopez and others 2014 li and others 2016 climate is a dominant factor regulating distribution of forest ecosystems at regional scales pearson and dawson 2003 siefert and others 2012 osland and others 2017 under stable climatic conditions tree species gradually assemble into specific communities to adapt to regional climate clements 1916 1936 weiher and keddy 1999 and thus forests success along the determined pathways to reach vegetation environment equilibriums i e late successional climax stages bazzaz 1991 weiher and keddy 1999 turner 2010 at landscape scales disturbances are common processes that disrupt the deterministic succession pathways in forest ecosystems flynn and others 2010 johnstone and others 2010 marra and others 2018 huang and others 2017 vegetation environment equilibrium is predicted to occur only in systems with small and infrequent disturbances while large and frequent disturbances often lead to high variations in landscape pattern between pre and post disturbances and may even shift forest ecosystem successional pathways turner and others 1993 turner 2010 environment encapsulates regional sale climate and in situ terrain and soil to interact with site scale processes to determine tree species occurrence composition and distribution at broad scales chesson 1986 snyder and chesson 2004 meier and others 2010 cadotte and tucker 2017 the complex interactions between extrinsic ecological factors i e environment disturbances and inherent ecological processes i e succession competition at various temporal spatial scales may result in unknown and intractable changes in forest landscapes over time mccook 1994 oliver and larson 1996 kasel and others 2017 lucash and others 2018 understanding and separating the interacting effects of environment and disturbance on forest ecosystems across the landscape is important to understanding current forest landscapes and predicting future changes however such a task is challenging because the current forest landscape is the product of the interactive and cumulative effects of stochastic disturbances environment and forest succession across space studies of this sort are frequently limited by relatively short observations e g several decades of changes in forest landscapes and spatially sparse disturbance history data e g paleoecological and dendrochronological wimberly and spies 2001 franklin and others 2006 spatially explicit reconstruction of historical landscapes over time can be used to assess the effects of environment and disturbance forest landscape models flms are an effective tool for such tasks since they are designed to simulate forest dynamics at site scales with disturbance at landscape scales while incorporating variable environments across the landscape he 2008 wang and others 2013 wang and others 2014a flms simulate forest dynamics formed in time from a historical starting point which can be derived from historical data and are driven by the theories of forest stand dynamics and disturbance ecology wu and others 2020 xu and others 2020 the theories presented in oliver and larson 1996 generalizes forest development pattern and mechanism with and without disturbances the reconstructed forest landscapes can be benchmarked using current field data and forest patterns to check not only the end product but the intermediate processes are defensible wang and others 2014b by spatially explicit reconstruction of historical landscapes the effects of environment and disturbance and their interaction on a forest landscape can be separately quantified using a factorial simulation design changbai mountain experienced a massive volcanic eruption in 946 ad iacovino and others 2016 the eruption removed almost all forests in a 50 km radius and forced forest succession to restart zhao 1984 liu and others 1995 to date forest zonation has developed on the north side of changbai mountain with temperate boreal and alpine dwarf forest ecosystems growing along elevational zones that correspond to their latitudinal distributions yang 1981 xu and others 2004 however such vertical forest zones have not developed on the west and south sides of the mountain which are disturbed by periodic typhoons that might have been prevailing even before the volcanic eruption with succession and disturbance history after the volcanic eruption changbai mountain presents an ideal platform to reconstruct its historical landscapes and reveal the effects of typhoon and environment on forest landscape recovery in this study we reconstructed the post eruption forest landscape of changbai mountain in northeast china using landis pro forest landscape model landis pro forest landscape model is designed to simulate forest dynamics at landscape scales while incorporating variable environments across the landscape he 2008 recently a series studies used landis pro to investigate forest landscape dynamics under various environments and disturbance regimes and quantify the effects of disturbance environment and forest succession for example wang and others 2019 predicted future forest landscape dynamics to quantify the effects of climate change and timber harvest on forest dynamics huang and others 2021 and duan and others 2021 simulated forest landscape dynamics affected by fire and insect disturbances in the context of future climate change and both studies quantified the direct and indirect effects of climate change to future disturbance regimes in the context of these studies this study focused on the influence of environment and typhoon on the development of post volcanic eruption forest landscape our study is a complement to wang and others 2019 huang and others 2021 and duan and others 2021 in quantifying the effects of various drivers on forest landscape change specifically we hypothesized that 1 environment exerts dominant roles in the early stage of post volcanic eruption forest landscape recovery and its role reduces as forests reach the mid and late successional stages 2 the effects of typhoon increase over time and will overtake the environment to become dominant in the later recovery stage and 3 recurrent typhoons weaken the spatial pattern of forest climate equilibrium and lead to fragmented forest landscapes 2 approach and methods 2 1 study area our study area was in changbai mountain of china side with elevations ranging between 780 and 2 652 m 41 28 42 28 n 127 32 128 52 e fig 1 a as elevation increases the average climate shifts from temperate monsoon to cold zone with corresponding long severe winters and short warm summers the annual mean temperature ranges from 7 c at low elevations to 3 c at high elevations while the annual precipitation increases from about 760 mm at low elevations to about 2 000 mm at the mountain top distinctive ecosystems correspond with elevation with mixed korean pine and broadleaf forests dominating the low elevation zone mixed korean pine broadleaf forest zone 780 1100 m evergreen coniferous forests dominating the mid elevation zone evergreen coniferous forest zone 1100 1700 m subalpine forests typifying the higher elevation zone subalpine forest zone 1700 2100 m and tundra at the highest elevation zone above 2100 m the region contains fairly high levels of plant diversity in the mixed korean pine broadleaf forest zone the overstory is dominated by korean pine pinus koraiensis siebold and zucc basswood tilia amurensis rupr maple acer mono maxim elm ulmus davidiana planch var japonica rehd nakai ash fraxinus mandshurica rupr in the evergreen coniferous forest zone the dominant tree species include jezo spruce picea jezoensis siebold and zucc and manchurian fir abies nephrolepis trautv maxim in the subalpine forest zone mountain birch betula ermanii cham is the dominant tree species there are scattered early successional patches dominated by asian white birch betula platyphylla suk aspen poplus davidiana dode mongolian oak quercus mongolica fisch ledeb and olga bay larch larix olgensis a henry the above tree species account for 90 of stand volume in the region hao and others 2008 shao and deng 2000 in the old growth forests in this region coarse woody debris forms 39 56 of total aboveground detritus i e litter fine wood and coarse woody debris and the ratio of coarse woody debris to live tree mass is 0 04 0 07 harmon and hua 1991 with the total mass of coarse woody debris from 7 9 16 2 mg ha in the mixed korean pine and broadleaf forests harmon and hua 1991 to about 53 4 mg ha in the evergreen coniferous forests zhou and others 2011 our study area included the area disturbed by millennium eruption that occurred 946 a d iacovino and others 2016 and the adjacent undisturbed area extending 15 km down slope the spatial extent of the areas affected by the volcanic eruption were determined from previous studies fig 1 wu and others 2020 which was approximately 3 8 105 ha forest recovery after the volcano eruption relied largely on seed sources and seed dispersal mcclanahan 1986 tautenhahn et al 2016 the undisturbed habitats below the eruption area and isolated refugia within the eruption area served as sources of seed fig 1 we regarded the undisturbed area at lower elevations surrounding the eruption area as matrix seed source area which linked to the eruption area via seed dispersal and gene flow we referred to the surviving seed sources in the refugia as remnant seed sources the size of matrix seed sources and the location of remnant seed sources were determined in wu and others 2020 with a similar study from mount st helens antos and zobel 1986 2 2 the assumptions for historical conditions field investigations in the eruption area show that the forest stand age is less than 300 years and decreases with increasing elevation near the volcano crater xu and liang 2010 this evidence suggests that tree species established less than 300 years ago then moved upwards and thick volcanic ash from the millennium eruption and erosion hinder tree establishment and survival for the first 700 years after the eruption evidence elsewhere in northeast china suggests that post volcanic eruption forest recovery has lagged for centuries after volcanic eruption since volcanic ash and erosion prohibited trees from establishment for instance the wudalianchi volcano in northern china erupted in 1720 and the volcanic area is still dominated by lichen and brush zhang and others 2005 in the low altitude areas of changbai mountain zhao 1984 found dead larch an early successional species stumps about 200 years in the current 100 year korean pine broadleaf forests late successional forests confirming that the early successional species colonized around 1710 and were replaced by mixed korean pine broadleaf forests around 1910 the establishment in high elevation areas were even later xu and liang 2010 found that the age of mountain birch an early successional species in the area above 1500 m asl was less than 200 years colonized around 1810 and decreased with increasing altitude and only 90 years colonized around 1920 at 2000 m they also concluded that the current forest vegetation is the first regeneration after the eruption in changbai mountain based on this evidence we hypothesized that few trees establishment and colonization during the first 700 years due to volcanic ashes and erosion and the forest colonization started about 300 years ago palaeoecological studies of historical climate reconstruction in changbai mt found that climate has been relatively stable with only some fluctuations over past 300 years except for the warming from 1980s shao and wu 1997 du and others 2018 in addition xu and others 2018 reported few significant changes in tree species composition and structure in the old growth forests during the warming period 1981 2016 in changbai mt thus there seems to be no significant impact of the later climate warming on shaping current forest landscape and it is reasonable to assume that the climate has remained stable during the development of current forest landscape moreover the current geomorphology of changbai mt was formed from the millennium eruption in 946 ad and thus the terrain is also considered to be relatively stable additionally the successful colonization of tree species from around 300 years ago implies that the erosion of volcanic ash from the millennium eruption had largely completed and tree establishment started for the past 300 years based on the above evidence we assumed that environment was relatively stable during the development of forest landscape over the past 300 years additionally because of the absence of typhoons at the low elevation in the matrix seed source area it is reasonable to assume that undisturbed forests in matrix seed source area have reached vegetation climate equilibrium reflective of historical forests in 1710s charcoal study confirmed that pre eruption forest composition korean pine fir spruce larch basswood birch and ash was nearly the same as the current forests zhao 1984 thus we can use the current old growth forests to represent the 1710 forests in the matrix forests additionally to evaluate the uncertainties associated with this assumption we conducted a sensitivity analysis on the initial conditions species composition and structure of the old growth forests and found that 30 changes in dominant species abundance in the matrix forests did not significantly affect the reconstructed forest landscapes see supplements 2 3 the framework for reconstructing post volcanic eruption forest landscapes in this study we present a novel framework for reconstructing post volcanic eruption forest landscapes to our knowledge there are few studies of spatial and temporal reconstruction of historical landscapes of past centuries such a work involves using a forest landscape model that tracks spatially explicit information of parent tree location and abundance seed dispersal forest dynamics and succession and environment heterogeneity the novelty of our framework includes integrating old growth forests and disturbance history data with theories of forest stand dynamics and disturbance ecology through landis pro forest landscape model we also validated the reconstructed landscapes against contemporary forest inventory data and classified remote sensing data and verified the intermediate trajectories of stand dynamics against theories of stand dynamics 2 4 landis pro model landis pro is a raster based landscape model that tracks the number of trees by species and by age cohort within each pixel wang and others 2013 wang and others 2014a it integrates species stand and landscape scale processes e g wind to simulate forest succession dynamics over large spatial and temporal scales wang and others 2014a landis pro simulates forest succession by accounting for species specific demographic processes which are driven by the species vital attributes i e longevity age of reproductive maturity shade tolerance seed dispersal distance stand density index and species establishment probability sep and maximum growing space mgso for each environment in this study we modeled the 12 most dominant tree species mentioned above we derived the species biological attributes from previous studies and sep and mgso based on ecosystem process model linkages model he and others 2002 liang and others 2012 2014 2 5 environmental data and historical typhoon regimes in this study environment represents the assembly of various physical variables at various spatial scales that encapsulates regional scale climate and in situ terrain including multiple terrain factors and soil thus environment in our study encapsulates slope aspect and elevation as well as soil and climate the effects of environment on post volcanic eruption forest recovery refer to how given configurations of environmental factors climate terrain and soil combinations regulate the forest succession and landscape formation environment was derived from elevation zones corresponding to climate zones superimposed with topography 30 m digital elevation model and soil 1 km resolution soil data included thickness of soil layers soil texture field capacity soil organic matter and soil nitrogen content which derived from china soil database http www soil csdb cn thus the environment covered the entire study area as numerous small homogeneous units with similar environmental conditions i e climate terrain soil conditions fig 1c we classified the whole area into 168 climate topography soil types which were derived from the overlay of climate terrain and soil layers for at each climate terrain soil type we derived sep which represents the response of species to the environment the higher sep indicates the stronger adaptability of species to the environment and easier establishment and colonization of tree species in this environment type information referring to early typhoon disturbances was not readily available for the past 300 years thus we determined the historical typhoon regime by combining remote sensing data and field inventory first we identified historical typhoon disturbances by interpreting landsat tm etm images from 1980 to 2016 https earthexplorer usgs gov since there were no other stand replacing disturbances except typhoon the early successional patches scattered in the matured forests were deemed to be historical typhoon patches these patches were readily identified from the remote sensing images due to their distinct characteristics in the surrounding matrix shao and others 1996 there were 140 typhoon patches identified with small patches occurred much more frequently than large ones following lognormal distribution supplement figure s2 s3 they covered 26799 3 ha or 19 3 of the region supplement figure s2 next we conducted extensive field investigations on 50 of the identified patches to determine stand age which reflects the time typhoon occurred there were 114 plots of white birch aspen forests 129 plots of larch forests and 91 plots of meadows supplement figure s2 the oldest typhoon events could be dated back to 1900 110 years ago suggesting that all the typhoon patches identified are typhoons occurred within the last 110 years typhoon patches of the same occurrence year were counted as one typhoon event we calculated mean disturbance size using the typhoon patches with known time since disturbance which is 6968 3 ha or about 5 0 of the study area we calculated mean typhoon return interval which is 555 6 year following the approach from johnson 1992 to extrapolate the typhoon regime to the 300 years 1710 2010 we calculated that the typhoon disturbed area was 54 0 300 555 6 of the region with 11 54 0 5 0 main typhoon events the 1986 typhoon recorded a maximum sustained wind of about 30 m s guo and others 2015 which resulted in 80 tree mortality as a result of uprooting xue 2009 and 1 21 million cubic meters of fallen wood covering 71 2 of the forest stock in the windfall area yu and han 2016 thus we assumed the similar intensity and severity for other typhoons in this study especially to avoid the circular use of the historical typhoon data the disturbance parameters mean return interval mean disturbance size etc were inputted to landis which replicated 20 times to capture the stochastic behavior of typhoons 2 6 experimental design we designed 2 2 factorial experimental modeling scenarios for environment realistic heterogeneous vs hypothetical homogenous environment and typhoon recurrence vs absence to evaluate the individual and interactive effects of these factors the treatment combinations resulted in four simulation scenarios homogeneous environment without typhoon nn heterogenous environment with recurrent typhoons et heterogeneous environment without typhoon en and homogenous environment with recurrent typhoons nt the et scenario was realistic which was regarded as baseline scenario while the remaining three scenarios were unrealistic but used to complete the factorial model simulation design to quantify the effects of typhoon and environment on forest landscapes this design assumed that environment and typhoon were the two dominant extrinsic factors that affected forest landscape dynamics and typhoon is the main natural disturbance to forest ecosystem in our study area typically the forest landscape dynamics are affected by various disturbances which may include wildfire harvest in addition to typhoon however these disturbances such as wildfire and harvest have been insignificant in our study area wildfires rarely occurred and if occurred they are at very small scales due to the relatively high humidity abundant precipitation and long snow covered season large scale timber harvest has been absent due to low human density high elevation and remoteness of the region based on the realistic scenario we simulated the remaining three scenarios by disregarding environment or and typhoon while fixing other factors for the scenarios with homogenous environment we assumed that there is a uniform environment across the entire study area where all pixels were assigned the same species establishment probabilities seps the sep value for each species was the average across all ecoregions for each species the life attributes were kept the same among all simulation scenarios e g homogeneous environment and heterogeneous environment for the scenarios with heterogenous environment we determined the suitability of each environment by species for the scenarios with typhoons we incorporated the reconstructed typhoon regime in the simulation for each scenario we simulated spatio temporally explicit forest dynamics 1710 2010 at 10 year interval with 20 replicates additionally the intrinsic factors included tree establishment growth mortality competition and seed dispersal referred as forest dynamics which are the underlining forces simulated in all scenarios however the fundamental assumption here is that the simulated forest dynamics can reconstruct the post volcanic succession dating back to ca 900 ad for this we used the landis pro forest landscape model that is designed to simulate forest dynamics at species and stand scales and interactions with environment and disturbance in this study the reconstructed forest landscape the realistic scenario in our simulation design has gone through the verification and validation processes see below suggesting that the reconstructed landscape credibly captured the spatiotemporal trajectories of forest landscape change of the past 300 years wu and others 2020 in our current study framework the effects of disturbance on environment are not included indeed typhoon disturbance can temporarily alter the environment by increasing light availability through removing forest canopy reducing evapotranspiration and increasing soil water however from a long term perspective e g decades environmental conditions should remain relatively constant under the stable climate in our study we measured the effects of environmental factors by species establishment probability sep see above we assumed that seps remain unchanged before and after disturbance because our study is based on a long term simulation 2 7 field inventory for landis pro model parameterization and result verification we measured a total of 2055 trees diameter 5 cm at 41 old growth sites 20 20 m and recorded the species and size by each tree this information was used to parameterize the initial species composition of matrix forests in 1710 we calibrated model parameters and validated the simulated results using 135 20 20 m sample plots surveyed in 2010 and 2014 each plot recorded the number of trees of by species and diameters we used 2 3 of the data for model calibration and 1 3 of the data for result validation to calibrate the model we iteratively adjusted model parameters such as age dbh relationships by environment the number of seeds and maximum dbh per species until there were no significant differences paired t tests p 0 05 between the 2010 simulated species basal area and density and the 2010s inventory plots in different elevation zones to validate the results we compared the remaining 1 3 2010s forest inventory data to the simulated results in 2010 figure s5 to ensure simulated forest patterns were reasonable we compared the simulated distribution of forests types in 2010 with the forest cover type derived from remote sensing classification shao and others 1996 figure s6 to ensure the intermediate results 1710 2010 were credible we evaluated the simulated forest development trajectories with the expected trajectories using gingrich stocking charts figure s7 2 8 data analysis we classified tree species into four groups based on tree species assembly and distribution korean pine and broadleaf species group including korean pine and mid late successional broadleaf species such as maple ash basswood and elm spruce and fir group mountain birch group and early successional species group including white birch aspen and larch which are mainly related to historical typhoon and scattered in the study area the first three groups are specialists which represent specific tree species assembly within each forest zone while the last species group is a generalist because the distribution and abundance of early successional species group are relatively small we did not analyze the species group individually in addition we divided the forests into young 30 years near mature 30 70 years mature 70 120 years and old growth 120 years age groups within each pixel to analyze the forest landscape pattern we determined the combination of tree species groups and age groups at each pixel resulting in 16 possible forest types 4 tree species groups 4 age groups ex young korean pine and broadleaf forests we used basal area and tree density of each species group except for the early successional species group since it was uncommon as response variables to analyze forest dynamics at the stand pixel level the two attributes basal area and tree density reflect the size and abundance of tree species they are the two most common measurements in field based forest inventory other forest attributes e g biomass carbon stock are derived these two attributes we used contagion contag index as the response variable to evaluate forest landscape patterns li and reynolds 1993 contag represents the degree of clumping of patches that considers all patch types present in a landscape affected by both the dispersion and interspersion of patch types which ranges from 0 to 100 li and reynolds 1993 this metric was selected to assess patch diversity and sensitivity to fragmentation we estimated the absolute effect sizes and relative effects of environment typhoon disturbance and their interaction on the above response variables in early 1710 1760 middle 1760 1860 and late 1860 2010 recovery periods respectively we estimated the effect size of environment alone as the difference between scenarios en and nn scenario en scenario nn the effect size of typhoon disturbance alone as the differences between scenarios nt and nn scenario nt scenario nn and the combined effect as the difference between scenarios et and nn scenario et scenario nn the size of the interaction effect was calculated as the difference between the combined effects and the sum of the separate main effects of these two factors through the experimental modeling design we used repeated measures anova analysis of variance to quantify the relative effects of environment typhoon disturbance and their interactions by partitioning the proportion of total variation explained based on type iii sums of squares girden 1992 we used the relaimpo package grömping 2006 in r statistical software to compute the relative effects 3 results 3 1 result verification the reconstructed forest landscapes tracked the trajectories of basal area and density for all simulated species fig 2 a the simulated recovery pathways were supported by forest stand development theory fig s7 ensuring that the intermediate dynamics between 1710 and 2010 were realistic spatially the simulated distribution of various forests types in the et scenarios realistic in 2010 matched the distribution of current forest types figure s6 the simulated 2010 results showed high agreements with the current observed basal area and tree density in each elevation zone fig 2b and c especially the paired t tests results for species basal area at 2010 were t 1 07 df 11 p 0 31 at low elevation zone t 1 78 df 11 p 0 10 at mid elevation zone and t 1 46 df 11 p 0 17 at higher elevation zone the paired t tests results for density at 2010 were t 0 16 df 11 p 0 87 at low elevation zone t 1 54 df 11 p 0 15 at mid elevation zone and t 1 00 df 11 p 0 34 at higher elevation zone thus the end of the 300 year simulation in 2010 reasonably represented forest composition and structure in 2010s 3 2 the effects of environment and typhoon basal area relative effects of environment in the early recovery stage 1710 1760 were overwhelming 97 on total basal area of all species while the relative effects of typhoon were minimum 3 fig 3 a from the 1760 onward as tree size and forest cover increased over the landscape the relative effects of environment on total basal area decreased to 28 and the relative effects of typhoon increased to 72 surpassing environment in the mid recovery stage 1760 1860 fig 3a for each specialist species group in general the relative effects of both environment and disturbance on its basal area varied similarly to their effects on total basal area of all species while relative effects of typhoon remained lower than or close to those of environment throughout 300 year period fig 3b d the absolute effect sizes of both environment and typhoon disturbance generally coincided with the relative effects on total basal area for all tree species environment had a far greater effect size 0 43 m2 ha than typhoon 0 in the early recovery stage 1710 1760 fig 3e the effect sizes of environment on total basal area decreased to 0 26 m2 ha in the late recovery stage 1860 2010 the effect size of typhoon increased to 0 67 m2 ha and surpassed environment in the mid recovery stage 1760 1860 fig 3e at the species group level typhoons had a negative effect on the basal area in the korean pine and broadleaf species group and the mountain birch species group because of strong wind induced mortality among the large old trees fig 3f and h however typhoon had a positive effect on spruce and fir species group fig 3g because of the widespread regeneration of light demanding spruce seedlings in the areas affected by typhoons density environment had large effects on total tree density in the early recovery stage 1710 1760 and the relative effect was 98 fig 3i as forests steadily recovered the relative effect of environment gradually decreased to 48 while the relative effect of typhoon increased over time and became increasingly important 40 in the late recovery stage 1860 2010 fig 3i for each specialist species group the relative effects of both environment and typhoon on tree density varied consistently with those on total density of all species the relative effects of environment were over 80 in the early recovery stage 1710 1760 and then they decreased over time the combination of the relative effects of typhoon disturbance and its interaction with environment increased over time and exceeded the relative effects of environment except for the korean pine and broadleaf species group for which the combined effects were nearly equivalent to the effect of environment in the late recovery stage 1860 2010 fig 3j l the absolute effect sizes of both environment and typhoon disturbance generally coincided with the relative effects on total density for all tree species environment had a far larger effect size 1745 6 trees ha than typhoon 14 1 trees ha in the early recovery stage 1710 1760 fig 3m the effect size of environment on total density decreased to 342 9 trees ha in the late recovery stage 1860 2010 the effect size of typhoon increased to 357 8 trees ha and surpassed environment in the mid recovery stage 1760 1860 fig 3m after a typhoon event seedlings regenerated at a rate greatly exceeding tree mortality resulting in an increase in tree density for each specialist species group in the mid recovery stages reflected by positive effects of typhoon on tree density fig 3m p landscape fragmentation environment had an overwhelming effect on contag with the relative effect reaching 99 in the early recovery stage 1710 1760 fig 3q from the 1760 onward typhoon disturbance became increasingly important in shaping the forest landscape in the late recovery stage 1860 2010 typhoon exerted a dominant effect on contag and its relative effect reached 52 fig 3q the absolute effect sizes of both environment and typhoon disturbance generally varied consistently with their relative effects fig 3r the effect sizes of environment decreased from 5 0 in the early recovery stage 1710 2010 to 2 9 in the late recovery stage 1860 2010 fig 3r meanwhile the effect sizes of typhoon increased from 0 in the early recovery stage 1710 2010 to 3 3 in the late recovery stage 1860 2010 especially both environment and typhoon disturbance showed negative effects on contag fig 3r with the landscape being more fragmented smaller contag value under typhoon disturbances 4 discussion we presented an empirical case study of reconstructing historical forest landscapes and the disturbance regime using a forest landscape model the composition structure and spatial patterns of the reconstructed forest ecosystem reasonably represented current forest conditions after a 300 year reconstruction historical field data did not exist for a full verification of the reconstructed succession trajectories thus the trajectories were verified against the expected stand dynamics based on forest successional theory and an understanding of the natural regeneration dynamics of the study site oliver and larson 1996 wang and others 2014b moreover fluctuations in stand dynamics where total basal area decreased and total density increased corresponded to typhoon events which confirmed that the patterns of forest disturbance and recovery for typhoons were captured after 300 years of recovery the forests have differentiated into the three forest zones corresponding to climatic zones fig 4 suggesting that environment captured broad scale forest distribution patterns the above verifications suggested the validity of the reconstructed historical forest landscapes and disturbance regimes environment was shown as the dominant factor controlling forest ecosystem composition structure and landscape patterning during the early stage of forest recovery 1710 1760 although spatially explicit dispersal limitation and niche partitioning via environmental filtering was seen as the ultimate process responsible for the recovery fig 4 environment constrained the establishment and growth plotkin and others 2002 shen and others 2013 kraft and others 2015 within each elevation zone the environments suited one group of species over the other leading to an expected community assembly and stand structure xu and others 2004 lebrija trejos and others 2010 kraft and others 2015 environment drove the leading and trailing edges of the species distributions towards certain elevation zones corresponding to climatic zones where they thrive thus regulating the spatial distribution of the specialist tree species this is congruent with previous findings that environmental heterogeneity exerts the dominant influence on shaping forest patterns harms and others 2001 prada and stevenson 2016 however a study of second growth forests suggested that the effects of environment immediately after the disturbances may not be as dominant but would increase because of legacy effects of long term land use and two major hurricanes hogan and others 2016 our results further implies that early development of forest ecosystem tends to be a deterministic community assembly process and provides evidence that niche differences among species shape forest patterns kraft and others 2008 chase and myers 2011 our results showed that the dominance of environment declined in the later stages associated with the increased effect sizes of typhoon disturbance once forests reach the mid and late successional stages their composition generally remains stable and thus the effects of environment decrease in our case the increasing relative importance of typhoon disturbance was resulted from their cumulative effects over time however some studies have claimed that the impact of environment conditions increase with succession harrelson and matlack 2006 campetella and others 2011 this conclusion may have resulted from studies conducted within a relatively short timeframe like the early recovery stage in this study which would limit the understanding of the cumulative effects of disturbance or other factors we were able to spatially simulate that the effects of typhoons on forest ecosystem composition structure and landscape pattern through the reconstruction results showed that early in recovery period low tree density and small tree diameters low height made forests less susceptible to typhoons as tree density decreased and diameter increased forest ecosystems became more susceptible to typhoon damage rich and others 2007 lin and others 2020 the reconstructed forest landscapes revealed that typhoons had lasting impacts on forest composition and structure for up to many decades for instance the 1910 typhoon damaged older stands causing widespread mortality with affected forests changing from old to near mature forests which were not recovered by 1950 while the 1950 typhoon occurred that further reduced forest age and its legacy remained beyond 2010 likewise the impact of 1986 typhoon persisted through 2010 fig 4 when the recovery time from a disturbance exceeds the intervals between successive disturbances multiple stochastic disturbance events will overlap in space and create a complex landscape pattern as a result of the cumulative effects of disturbances turner 2010 other studies have also reported the compound effects of successive disturbances on forest ecosystems busing and others 2009 menges and others 2011 turner and others 2019 for example the cumulative effects of repeated hurricanes could change biomass and carbon balances busing and others 2009 and cause greater damage to the forest structure and function than a single disturbance event kim and others 2020 2019 similarly mclaren and others 2019 reconstructed the historical hurricanes over 155 years in two tropical montane rainforests in jamaica and also confirmed the cumulative effects of successive hurricanes on forest structure and diversity studies like ours involving 300 year landscape reconstruction offered a long term perspective of the cumulative effects of disturbance the time dependent cumulative effects highlighted the importance of historical disturbances in shaping current forest landscapes recent studies reported that climate change alters the frequency and intensity of disturbances which indirectly affects forest landscapes huang and others 2021 duan and others 2021 however in our study the effects of climate change on disturbance regimes and forest recovery have not been considered previous studies of historical climate in changbai mt found that climate has been relatively stable over the past 300 years except for the warming after 1980s shao and wu 1997 du and others 2018 so far there was only one typhoon event recorded in 1987 that destroyed large areas of forest guo and others 2015 thus although climate change can alter the frequency and intensity of disturbance such as fire huang and others 2021 2018 we hypothesized that the interactions of climate change and typhoon have been stable since our study focused on the historical landscapes additionally xu and others 2018 reported few significant changes in forest composition and structure in the old growth forests during the warming period 1981 2016 in changbai mt thus there was no significant impact of the later climate warming on the forest landscape thus our results should not be altered due to recent climate warming and the potential indirect effects of warming climate on typhoon disturbance our study is among the few that quantify the relative contribution of environment and typhoon disturbance over large spatial and temporal scales there are increasing concerns about the impact of global climate change and associated disturbance regime change on forest ecosystems vanderwel and purves 2014 lin and others 2020 knowledge about the relative impacts of environment and disturbance allows us to disentangle the two factors and examine their interactive effects this is especially important under changing environmental and disturbance regimes because changes in disturbance regimes predicted under climate warming such as increased disturbance intensity and frequency might lead to greater changes in forest ecosystem composition structure and landscape pattern johnstone and others 2010 turner and others 2019 lin and others 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the joint fund of national natural science foundation of china u19a2023 the national natural science foundation of china 31961133027 31971486 the natural science foundation of jilin scientific institute 20180520087jh and top notch young talents project of liaoning province xing liao talents project xlyc1907177 we thank sarah humfeld for improving the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105513 
