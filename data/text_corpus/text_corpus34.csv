index,text
170,numerical simulations of groundwater flow are used to analyze and predict the response of an aquifer system to its change in state by approximating the solution of the fundamental groundwater physical equations the most used and classical methodologies such as finite difference fd and finite element fe methods use iterative solvers which are associated with high computational cost this study proposes a physics based convolutional encoder decoder neural network as a surrogate model to quickly calculate the response of the groundwater system holding strong promise in cross domain mappings encoder decoder networks are applicable for learning complex input output mappings of physical systems this manuscript presents an attention u net model that attempts to capture the fundamental input output relations of the groundwater system and generates solutions of hydraulic head in the whole domain given a set of physical parameters and boundary conditions the model accurately predicts the steady state response of a highly heterogeneous groundwater system given the locations and piezometric head of up to 3 wells as input the network learns to pay attention only in the relevant parts of the domain and the generated hydraulic head field corresponds to the target samples in great detail even relative to coarse finite difference approximations the proposed model is shown to be significantly faster than a comparative state of the art numerical solver thus providing a base for further development of the presented networks as surrogate models for groundwater prediction keywords surrogate modelling attention u net groundwater flow image to image regression 1 introduction groundwater resources are of major importance for residential industrial and agricultural use however the quality and availability of groundwater supplies are significantly affected by their overexploitation around the world population growth and climate extremes boretti and rosa 2019 consequently a demanding need exists for quick and accurate evaluation of multiple management alternatives over long time horizons the last 30 years have seen the development of several physics based numerical models for simulating groundwater systems with finite difference fd and finite element fe discretizations of the partial differential equations pdes as the most used and classical methodologies 2 diersch 2013 these techniques calculate the hydraulic head by iteratively solving an implicit system of equations at each time step in the discretized time and flow domains running the groundwater model in a complex system within a large domain and with reasonable accuracy incurs numerical challenges and an excessive computational demand pulido velazquez et al 2007 gorelick 1983 as the computational cost increases super linearly with the number of unknowns in the discretization long runtimes are a major challenge when high resolution is required or when many executions are necessary such as in uncertainty analysis sensitivity analysis and inverse modelling mens et al mens et al 2021 discuss the case of the national water model nwm that is used for national policy making on drought risk management in the netherlands and whose heavy computational burden poses limits to quickly responding to policy questions the authors advocate the need for a fast simple model that describes all relevant processes and is quick enough to explore many scenario and strategy combinations for long time series furthermore groundwater flow simulations require the reconstruction of subsurface heterogeneities and the physical properties of the aquifer as inputs to the model for which only limited direct observations are available inverse modelling is used to estimate the unknown parameters of the system taking into account their stochasticity traditional approaches to the inversion problem correspond to iterative techniques and necessitate a large number of forward model runs as the number of unknown parameters increases forward operations become extremely computationally demanding surrogate models are cheaper to run models which approximate the response of a complex and computationally intensive model surrogate models have been used in a number of groundwater studies such as for optimization design siade et al 2020 christelis et al 2018 and uncertainty quantification problems crevillén garcía et al 2019 gadd et al 2019 yu et al 2020 to name a few reduced fidelity models simplify the level of complexity of the physical processes of the full order model e g by projecting the governing equations into a transformed space of smaller dimension projection based techniques can accurately retain the underlying structure of the full order model however these methods can suffer stability and robustness issues lassila et al 2014 huang et al 2018 they are highly code intrusive and they cannot efficiently treat strong nonlinearity chaturantabut and sorensen 2010 data driven models learn the response of the system from the simulation data in a supervised manner gaussian processes have been successfully applied to uncertainty quantification tasks for which the training data are limited but they rely on specific a priori assumptions on the relationship between the input and the outputs and have high computational costs when dealing with large datasets kennedy and o hagan 2000 atkinson and zabaras 2019 deep neural networks are universal function approximators and are becoming increasingly common surrogate models for solving problems within the fields of physics and engineering these techniques have been applied for solving pdes in high dimensional settings and nonlinear systems with potential applications in parameter estimation and uncertainty quantification the reader is referred to karniadakis et al karniadakis et al 2021 for a review on the strengths limitations current applications and outlook of this class of deep learning algorithms recently interest has grown for learning complex nonlinear multiscale and high dimensional mappings of subsurface processes in the work of geneva and zabaras geneva and zabaras 2020 convolutional neural networks cnns for physics constrained learning show exceptional performance with solutions obtained an order of magnitude faster than with state of the art numerical solvers they train deep auto regressive convolutional neural network models to learn the dynamics of three transient pdes 1d kuramoto sivashinsky equation 1d burgers equation and the 2d coupled burgers system without any off line training data several studies adopted an adversarial network framework for surrogate methods for a single phase flow forward model and a multiphase flow forward model sun 2018 zhong et al 2019 dagasan et al dagasan et al 2020 argue that the use of a conditional generative adversarial network cgan as a surrogate forward model for groundwater systems can reduce the computational time by up to 80 compared to the numerical solver modflow deep neural networks were chosen in this study largely due to their scalability and their ability to learn based on a few a priori assumptions the first refers to the capacity to learn from massive amounts of data compared to a gaussian process whose runtime scales poorly with the size of the datasets deep neural networks can assimilate large amounts of multi fidelity observational data even in partly understood uncertain and high dimensional contexts compared to reduced order model techniques which aim to bring the physical relationships of full order models at a much lower dimension deep neural networks do not assume any prior assumption that constrains the relationship between input and output samples this flexibility can lead deep neural networks to learn complex relationships thus increasing their modelling power but at the cost of a lower interpretability the encoder decoder architecture consists of a contracting and an expansive path it shows robust and accurate performance in various tasks including machine translation problems wu et al 2016 semantic segmentation oktay et al 2018 or depth regression eigen et al 2014 initially developed for biomedical image segmentation ronneberger et al 2015 u net is an encoder decoder network which uses fully convolutional networks and requires highly limited training samples u net based architectures have been applied across a wide spectrum of application areas such as image super resolution style transfer text to image translation and image to image translation isola et al 2017 mo et al mo et al 2019 developed a deep convolutional encoder decoder network method as a surrogate model for transient multiphase flow models given the large approximation errors in the concentration fields near the source release location the authors assign an additional weight to the loss at the eight pixels around the source release location in order to improve the surrogate predictive capability attention models address this limitation by allowing the model to learn to focus selectively on the relevant parts of the input attention has recently become an essential component of neural architectures within diverse application domains chaudhari et al 2021 galassi et al 2020 khan et al 2021 attention u net makes use of attention gates in order to focus on specific parts of the image that are of importance while paying little attention to unnecessary areas oktay et al 2018 schlemper et al 2019 the purpose of this paper is to propose an attention u net network as a surrogate model for the forward operator in groundwater modelling the encoder decoder model learns the mapping between model inputs and output for deterministic steady state solutions of the two dimensional groundwater flow equation given a highly heterogeneous subsurface domain the surrogate model accurately captures the nonlinear relationship between the hydraulic conductivity and the subsurface groundwater map the model dynamically pays attention to only the parts of the input where flow can take place in a manner that helps the network in learning the mapping effectively the rest of the paper is organized as follows section 2 presents the adopted image to image deep learning approach and the architecture of the attention u net employed section 3 provides an overview of the problem formulation and model set up along with training of the surrogate model the proposed method is evaluated with and without attention gates in section 4 finally the conclusions are formulated in the last section 2 methodology 2 1 surrogate modelling as image to image regression a surrogate model f x θ y approximates the ground truth function y f x where f x y is the mapping between the input domain x and the output domain y x x is the input y y is the output and θ are the model parameters in the case of forward solving of pdes with machine learning the ground truth mapping represents some combination of the solution of the pdes governing the physical system and the surrogate model y f x θ is trained using a dataset d of n simulation data d x i y i i 1 n by adopting an image to image regression approach the surrogate modelling can be treated as an image regression problem by solving the pde over a spatial domain such as 2d regular grids the simulation data can be thought as images with inputs x i r d x h w and outputs y i r d y h w where dx and dy are the number of input and output images with a resolution of h w height width the surrogate modelling problem becomes an image to image regression problem with the regression function f r d x h w r d y h w zhu and zabaras 2018 2 2 encoder decoder model encoder decoder is a learning method with an analysis path encoder and a synthesis path decoder the encoder network transforms high dimensional unlabeled input data x into low dimensional embeddings z latent space and the decoder maps z to the intended output y decoder encoder x the input is passed through a series of layers that progressively down sample until a bottleneck layer at which point the decoder restores the spatial dimensions to produce the output images intuitively the model corresponds to a coarse refine process the encoder reduces the spatial dimension of the input image to high level coarse features and the decoder recovers the spatial dimension by refining the coarse features the assumption is that the input and output images share the underlying structure or they are different renderings of the same underlying structure that is their structures are roughly aligned isola et al 2017 as the goal of this study is to generate a targeted output image corresponding to given inputs the encoder decoder model learns the mapping x y from a conditioning input image x to the output image y the network converts images from the source to target domains where the first corresponds to the initial boundary conditions and model parameters and the latter to the resolution of the governing equation given those constrains 2 3 deep convolutional neural networks cnns lecun et al 1989 rumelhart et al 1985 are popular deep learning networks specialized in image processing krizhevsky et al 2012 szegedy et al 2015 while the first layers detect basic features deeper convolutional layers learn higher representation a convolution layer is a linear transformation that highlights the presence of a given feature in the map while preserving spatial information in the input image goodfellow et al 2016 given a 2 d input image and a square kernel ω with size m the convolutional layer outputs the value at location i j by summing up the contributions from the previous layer cells yl 1 weighted by the filter components then the nonlinearity σ is applied 1 y i j l σ a 0 m b 0 m w a b y i a j b l 1 the stride of the convolutional layer is a parameter that determines the number of pixel shifts between two successive moves of the filter while the padding indicates the amount of pixels with value zero added at each side of the boundaries of the input the rectified linear unit function relu is a piecewise linear function that outputs the input if it is positive and zero if negative the leaky relu with slope coefficient α modifies the function to allow a small negative output when the input is negative 2 σ x x i f x 0 α x o t h e r w i s e batch normalization and dropouts are used to stabilize training and mitigate overfitting salimans and kingma 2016 a dropout layer selects a random set of units from the preceding layer and ignores their output while batch normalization standardizes the layer s inputs by calculating the mean and standard deviation across the batch 3 application 3 1 groundwater model and datasets consider steady state groundwater flow in saturated media satisfying the fundamental governing equation todd 1980 3 k h q 0 the piezometric head h l is the field variable of interest k is the input hydraulic conductivity l t and q represents the source or sink terms l3 t 1 the problem of this study consists of steady state flow in a single layer model representing a heterogeneous confined aquifer initially in this work only dirichlet boundary conditions are considered and the groundwater head values are fixed in the cells in which the allocated head is known the model takes in an input image with three channels head values boundary markers and spatially varying hydraulic conductivity fig 1 dirichlet boundary conditions are imposed on the four sides of the square domain head is constant at up to three random locations across the domain representing wells the source term q is set to zero the second channel of the input image is a binary mask where the boundary markers identify the cells with a fixed value i e well locations and boundary cells as defined by the first source image the last input channel defines the heterogeneous media the conductivity field k of the highly heterogeneous aquifer is a gaussian random field vanmarcke 1983 in which the values of hydraulic conductivity are taken from a finite set of values this application example demonstrates the capability of an attention u net to successfully learn and simulate a common hydrologic situation using an image to image translation approach the model is trained to predict the output fields consisting of the spatial components of the groundwater head in the domain these predictions are compared against simulation results obtained by the fully implicit finite difference model modflow harbaugh 2005 here called target images bearing in mind that finite difference results provide an approximation of the partial differential equation and are not error free 3 2 network architecture the encoder decoder model is implemented as attention u net oktay et al 2018 ronneberger et al 2015 and the employed network architecture is shown in fig 2 for the case of a 64 64 input image as used in our computational tests this is an encoder decoder model with skip connections in the down sampling half the inputs are encoded with a series of cnns with kernels of size 4 stride of 2 and padding set to 1 in each block cnn is followed by a batch normalization layer dropout with rate 0 5 and a leaky relu with slope 0 3 as the number of filters increases to 512 and the size of the input images reduces to 4 4 the encoder captures high level abstract information in the up sampling half the representations are expanded spatially and the number of channels is reduced by a series of cnns and up sampling layers the last up sampling layer is followed by a transposed convolution layer with a sigmoid activation function to ensure predicted values between 0 and 1 skip connections link the layers in the encoder with corresponding layers with the same sized feature map in the decoder ronneberger et al 2015 the only difference between attention u net and the original u net architecture is that in the attention u net network skip connections are additionally passed through attention gates which use additive soft attention oktay et al 2018 the attention coefficients are larger if the vector from the next lowest layer of the network in the up sampling path and the corresponding vector from the encoder going through the skip connection are aligned the weights are multiplied element wise to the original vector which passes along in the skip connection in this way the attention gate ag mechanism allows the u net to suppress irrelevant regions and focus more on target structures of varying size and shape for reproducibility full details of the network architecture are described in appendix a 3 3 loss function the aim of the regression task is to minimize the mean square error mse between the generated samples and training data the network computes the average loss across a mini batch of size nb 4 l m s e 1 n b j 1 n b i 1 n y j i y j i 2 n where y is the training image y is the image generated by the network and n denote the total number of pixels of each image the networks tries to be near the ground truth output in an l2 sense 3 4 network training the model is trained in supervised fashion for the examples described in the following section a dataset consisting of 32000 training data is used and the development of the loss function is compared with 8000 validation data the size of the training dataset is big enough to ensure the model s ability to generalize and the generation time is less than 3 hours the losses are minimized using the adam optimizer kingma and ba 2014 with a starting learning rate α 8 10 4 the network is trained for 130 epochs training converged after approximately 2 hours by training the models on an intel r xeon r gpu tesla k80 the quality of the trained network is evaluated by reporting the coefficient of determination r2 and the root mean squared error rmse between each pixel value from the target image and each pixel value from the generated image 5 r 2 1 i 1 n y i y i 2 2 i 1 n y i y 2 2 r m s e 1 n i 1 n y i y i 2 2 where y is the target image y is the mean of the target images of the dataset y i 1 n y i n y is the network prediction and n is the total number of samples the two selected metrics between them yield complementary and representative information for the evaluation of the trained model 4 results and discussion 4 1 model predictions this section presents both qualitative and quantitative techniques to test the performance of the model the test case considers a square domain ω 0 64 0 64 consisting of 64 rows and 64 columns with the width of each cell equal to one the boundary is assumed to be constant head boundary with head of 1 while the imposed head values in the wells lie in the range 0 5 1 the number of wells their locations and their values are randomly selected and vary for each data sample the conductivity field k of the highly heterogeneous aquifer is generated as a continuous gaussian random field which is then discretized into a finite set of values the heterogeneous hydraulic conductivity field has values belonging to 5 different classes 0 1 0 325 0 55 0 775 1 which can be thought of as 5 soil types distributed within the model the model is trained with loss function in eq 4 and the target response is the finite difference simulation to illustrate the superior performance of the proposed attention u net architecture against the original u net network architecture the u net network without attention gates is also trained using the same training sets and parameters at the end of the training the attention u net network achieves a rmse of 1 98 10 3 and a r2 score of 0 996 while those obtained by u net are 3 78 10 3 and 0 986 respectively fig 3 fig 4 provides a comparison of generated images of groundwater head with the target images for 5 random examples taken from the test dataset with a set size of 4000 samples the attention u net model has learnt to map the flow patterns it generates accurate predictions for varied input samples that are unseen during training the predictions match the target images very well the model predicts the correct value of groundwater head and the pattern of its distribution the model is able to identify and focus on salient image regions the attention coefficients are highest at the boundary of the domain and near the well locations while they are low in the areas with small head distribution gradients when trained without attention gates u net can predict the values of the groundwater head in the domain but the generated outputs have some minor deviations especially at a distance from the source area and the head gradients are smaller the use of the attention mechanism significantly improves the accuracy of the results fig 5 visualizes the attention coefficients obtained from two test images with respect to training epochs during the first 20 epochs the loss function rapidly decreases fig 3 top and the attention gates learn to identify the location of the wells the boundaries and a rough outline of the area with large head distribution gradients by training the network for longer epochs the attention coefficients are gradually updated and refined to focus on areas with large head distribution gradients appendix b shows the 5 predictions with highest and lowest mean square error between the generated samples and training data out of 500 random samples from the test dataset the errors are localized near the wells and the difference between the generated and target images is almost negligible even for the samples with the highest error 4 2 model evaluation to test the performance of the model its computational power is compared with the modflow engine table 1 presents the processing time required for running the forward operators averaged on 10 examples in order to have a fair comparison between the two the tests are performed on the same hardware the cpu used is intel r xeon r cpu 2 20ghz and the gpu is tesla k80 the results demonstrate that attention u net requires less computational power than modflow this experiment reveals a 75 computational reduction for the data driven model showing its capability to be used in forward simulations with less computational demand than the state of the art numerical solver when applying the method to computationally more expensive forward models such as in large scale non linear system the computational cost of the neural network will remain low and significant computational savings can be expected dropout at inference time can be considered equivalent to bayesian approximation in deep gaussian processes and the neural network uncertainty can be quantified following the approach proposed by gal and ghahramani gal and ghahramani 2016 at test time the same input is passed 1000 times to the network with random dropout the mean and the standard deviation of the generated images give an estimation of the prediction interval fig 6 presents the results for 3 random samples the uncertainty is null at the boundaries and highest in the vicinity of the wells which is also the region with highest errors compared to the finite difference solver whose response is deterministic this method allows one to estimate the uncertainty of the model the generalization capabilities of the network are presented in appendix c the model is able to extrapolate to out of distribution inputs especially for different values of hydraulic conductivity and less so for increasing number of wells it is worth pointing out that the effect of using attention gates on the uncertainty and generalization capabilities of the model hasn t been addressed in the current study future work should investigate this relation and consequently explore how generalization on out of distribution input samples can be improved 5 conclusion and future work this paper presents a convolutional encoder decoder network to quickly calculate the steady state response of a groundwater system the data driven surrogate model is trained and tested in different scenarios in which the groundwater head values in the whole domain need to be inferred from the hydraulic head at the locations of the wells the square domain is a gaussian random field with a spatially varying hydraulic conductivity when trained by minimizing the departure from the target images the proposed u net model easily learns the nonlinear relation between inputs hydraulic conductivity fields and boundary conditions and output the hydraulic head field a significant contribution of the proposed framework is to incorporate attention gates which allow the network to identify and focus on the salient regions of the image the visualization of the attention coefficients demonstrate that the model has learnt to pay attention to areas with large head distribution gradients the attention mechanism improves the network s approximation accuracy and reduces the model uncertainty the application of the data driven surrogate method in solving forward simulations gives very accurate results but requiring much less computational time than the state of the art numerical solver one attractive property of this methodology is that the learning is carried out offline training converged after less than 3 hours on an intel r xeon r gpu tesla k80 which can be considered as a low training time compared to typical deep learning models once the model is trained its weights and parameters do not need to be further tuned the choice of the hyperparameters and the specificities of the u net architecture have been chosen based upon manual variation as opposed to systematic optimization to give accurate results with low computational time with little apparent sensitivity future work could include a more robust hyperparameter tuning study with a quantitative sensitivity analysis in the current study only dirichlet boundary conditions were applied to the borders of the domain and the locations of the wells an additional natural extension of our work is to investigate how well the model generalizes to different and mixed types of boundary conditions discretization is another important factor to consider the present work has been limited to data samples with the same resolution many questions remain open related to the discretization of the sample data e g the generalization of the trained model to different discretizations and the amount of training data required if the model needs to be retrained for different resolutions the authors plan to further develop the presented model for more complex larger and uncertain systems this could include time dependent problems three dimensional simulations and coupled transport through porous media all of which are likely to require larger training data sets and potentially deeper networks another potential extension is the incorporation of prior information directly into the learning process by imposing a physics constraint in the loss function physics informed learning could increase the speed of inference while requiring less data for the training process finally in this study the network has been trained using synthetic data but the potential use of the proposed model holds promises for the solution of practical applications due to its data driven nature declaration of competing interest none acknowledgments this work was supported by the leeds york hull natural environment research council nerc doctoral training partnership dtp panorama under grant ne s007458 1 and by rijkswaterstaat geert menting in the project fysica gebaseerde neurale netwerken in grondwater under kpp bo06 2018 and kpp bo06 2019 the authors acknowledge the additional guidance and practical expertise provided by the dutch research institute deltares appendix a network architecture this appendix discusses details in the models used both u net and attention u net have 4 downsampling layers and 4 upsampling layers table a 1 each layer consists of a series of cnns with kernels of size 4 stride of 2 and padding set to 1 followed by a batch normalization layer and dropout with rate 0 50 the nonlinear activation is leaky relu with slope 0 3 for the downsampling layers and relu for the upsampling ones table a 2 and table a 3 skip connections concatenate the layers in the encoder with corresponding layers in the decoder the network of attention u net additionally has attention gates which are implemented as according to the work of oktay et al oktay et al 2018 the total number of parameters of the network is 1 36 107 of which 7 35 106 are for the attention gates appendix b worst and best model predictions this appendix shows the 5 predictions with highest and lowest the mean square error out of 500 random samples from the test dataset the samples with highest errors present multiple wells with wide plumes which cover most of the domains fig b 1 on the contrary the best predictions are those in which the salient region is limited fig b 2 in all cases highest errors are localized near the wells it is worth noticing that even when the error is higher the mse is in the order of 10 5 and the difference between the generated and target images is almost negligible appendix c generalization so far the model has been both trained and tested for different scenarios in which the groundwater head values in the whole domain is inferred from the piezometric head at the locations of up to three wells and the spatially varying hydraulic conductivity has values belonging to 5 different classes between 0 and 1 here we consider testing the model on cases that have different numbers of well locations and different values for the hydraulic conductivity between 0 and 1 fig c 1 shows the mse error for the model tested on four new input distributions the figure shows that the model is able to generalize well given different values of the hydraulic conductivity but less so for increasing number of wells 
170,numerical simulations of groundwater flow are used to analyze and predict the response of an aquifer system to its change in state by approximating the solution of the fundamental groundwater physical equations the most used and classical methodologies such as finite difference fd and finite element fe methods use iterative solvers which are associated with high computational cost this study proposes a physics based convolutional encoder decoder neural network as a surrogate model to quickly calculate the response of the groundwater system holding strong promise in cross domain mappings encoder decoder networks are applicable for learning complex input output mappings of physical systems this manuscript presents an attention u net model that attempts to capture the fundamental input output relations of the groundwater system and generates solutions of hydraulic head in the whole domain given a set of physical parameters and boundary conditions the model accurately predicts the steady state response of a highly heterogeneous groundwater system given the locations and piezometric head of up to 3 wells as input the network learns to pay attention only in the relevant parts of the domain and the generated hydraulic head field corresponds to the target samples in great detail even relative to coarse finite difference approximations the proposed model is shown to be significantly faster than a comparative state of the art numerical solver thus providing a base for further development of the presented networks as surrogate models for groundwater prediction keywords surrogate modelling attention u net groundwater flow image to image regression 1 introduction groundwater resources are of major importance for residential industrial and agricultural use however the quality and availability of groundwater supplies are significantly affected by their overexploitation around the world population growth and climate extremes boretti and rosa 2019 consequently a demanding need exists for quick and accurate evaluation of multiple management alternatives over long time horizons the last 30 years have seen the development of several physics based numerical models for simulating groundwater systems with finite difference fd and finite element fe discretizations of the partial differential equations pdes as the most used and classical methodologies 2 diersch 2013 these techniques calculate the hydraulic head by iteratively solving an implicit system of equations at each time step in the discretized time and flow domains running the groundwater model in a complex system within a large domain and with reasonable accuracy incurs numerical challenges and an excessive computational demand pulido velazquez et al 2007 gorelick 1983 as the computational cost increases super linearly with the number of unknowns in the discretization long runtimes are a major challenge when high resolution is required or when many executions are necessary such as in uncertainty analysis sensitivity analysis and inverse modelling mens et al mens et al 2021 discuss the case of the national water model nwm that is used for national policy making on drought risk management in the netherlands and whose heavy computational burden poses limits to quickly responding to policy questions the authors advocate the need for a fast simple model that describes all relevant processes and is quick enough to explore many scenario and strategy combinations for long time series furthermore groundwater flow simulations require the reconstruction of subsurface heterogeneities and the physical properties of the aquifer as inputs to the model for which only limited direct observations are available inverse modelling is used to estimate the unknown parameters of the system taking into account their stochasticity traditional approaches to the inversion problem correspond to iterative techniques and necessitate a large number of forward model runs as the number of unknown parameters increases forward operations become extremely computationally demanding surrogate models are cheaper to run models which approximate the response of a complex and computationally intensive model surrogate models have been used in a number of groundwater studies such as for optimization design siade et al 2020 christelis et al 2018 and uncertainty quantification problems crevillén garcía et al 2019 gadd et al 2019 yu et al 2020 to name a few reduced fidelity models simplify the level of complexity of the physical processes of the full order model e g by projecting the governing equations into a transformed space of smaller dimension projection based techniques can accurately retain the underlying structure of the full order model however these methods can suffer stability and robustness issues lassila et al 2014 huang et al 2018 they are highly code intrusive and they cannot efficiently treat strong nonlinearity chaturantabut and sorensen 2010 data driven models learn the response of the system from the simulation data in a supervised manner gaussian processes have been successfully applied to uncertainty quantification tasks for which the training data are limited but they rely on specific a priori assumptions on the relationship between the input and the outputs and have high computational costs when dealing with large datasets kennedy and o hagan 2000 atkinson and zabaras 2019 deep neural networks are universal function approximators and are becoming increasingly common surrogate models for solving problems within the fields of physics and engineering these techniques have been applied for solving pdes in high dimensional settings and nonlinear systems with potential applications in parameter estimation and uncertainty quantification the reader is referred to karniadakis et al karniadakis et al 2021 for a review on the strengths limitations current applications and outlook of this class of deep learning algorithms recently interest has grown for learning complex nonlinear multiscale and high dimensional mappings of subsurface processes in the work of geneva and zabaras geneva and zabaras 2020 convolutional neural networks cnns for physics constrained learning show exceptional performance with solutions obtained an order of magnitude faster than with state of the art numerical solvers they train deep auto regressive convolutional neural network models to learn the dynamics of three transient pdes 1d kuramoto sivashinsky equation 1d burgers equation and the 2d coupled burgers system without any off line training data several studies adopted an adversarial network framework for surrogate methods for a single phase flow forward model and a multiphase flow forward model sun 2018 zhong et al 2019 dagasan et al dagasan et al 2020 argue that the use of a conditional generative adversarial network cgan as a surrogate forward model for groundwater systems can reduce the computational time by up to 80 compared to the numerical solver modflow deep neural networks were chosen in this study largely due to their scalability and their ability to learn based on a few a priori assumptions the first refers to the capacity to learn from massive amounts of data compared to a gaussian process whose runtime scales poorly with the size of the datasets deep neural networks can assimilate large amounts of multi fidelity observational data even in partly understood uncertain and high dimensional contexts compared to reduced order model techniques which aim to bring the physical relationships of full order models at a much lower dimension deep neural networks do not assume any prior assumption that constrains the relationship between input and output samples this flexibility can lead deep neural networks to learn complex relationships thus increasing their modelling power but at the cost of a lower interpretability the encoder decoder architecture consists of a contracting and an expansive path it shows robust and accurate performance in various tasks including machine translation problems wu et al 2016 semantic segmentation oktay et al 2018 or depth regression eigen et al 2014 initially developed for biomedical image segmentation ronneberger et al 2015 u net is an encoder decoder network which uses fully convolutional networks and requires highly limited training samples u net based architectures have been applied across a wide spectrum of application areas such as image super resolution style transfer text to image translation and image to image translation isola et al 2017 mo et al mo et al 2019 developed a deep convolutional encoder decoder network method as a surrogate model for transient multiphase flow models given the large approximation errors in the concentration fields near the source release location the authors assign an additional weight to the loss at the eight pixels around the source release location in order to improve the surrogate predictive capability attention models address this limitation by allowing the model to learn to focus selectively on the relevant parts of the input attention has recently become an essential component of neural architectures within diverse application domains chaudhari et al 2021 galassi et al 2020 khan et al 2021 attention u net makes use of attention gates in order to focus on specific parts of the image that are of importance while paying little attention to unnecessary areas oktay et al 2018 schlemper et al 2019 the purpose of this paper is to propose an attention u net network as a surrogate model for the forward operator in groundwater modelling the encoder decoder model learns the mapping between model inputs and output for deterministic steady state solutions of the two dimensional groundwater flow equation given a highly heterogeneous subsurface domain the surrogate model accurately captures the nonlinear relationship between the hydraulic conductivity and the subsurface groundwater map the model dynamically pays attention to only the parts of the input where flow can take place in a manner that helps the network in learning the mapping effectively the rest of the paper is organized as follows section 2 presents the adopted image to image deep learning approach and the architecture of the attention u net employed section 3 provides an overview of the problem formulation and model set up along with training of the surrogate model the proposed method is evaluated with and without attention gates in section 4 finally the conclusions are formulated in the last section 2 methodology 2 1 surrogate modelling as image to image regression a surrogate model f x θ y approximates the ground truth function y f x where f x y is the mapping between the input domain x and the output domain y x x is the input y y is the output and θ are the model parameters in the case of forward solving of pdes with machine learning the ground truth mapping represents some combination of the solution of the pdes governing the physical system and the surrogate model y f x θ is trained using a dataset d of n simulation data d x i y i i 1 n by adopting an image to image regression approach the surrogate modelling can be treated as an image regression problem by solving the pde over a spatial domain such as 2d regular grids the simulation data can be thought as images with inputs x i r d x h w and outputs y i r d y h w where dx and dy are the number of input and output images with a resolution of h w height width the surrogate modelling problem becomes an image to image regression problem with the regression function f r d x h w r d y h w zhu and zabaras 2018 2 2 encoder decoder model encoder decoder is a learning method with an analysis path encoder and a synthesis path decoder the encoder network transforms high dimensional unlabeled input data x into low dimensional embeddings z latent space and the decoder maps z to the intended output y decoder encoder x the input is passed through a series of layers that progressively down sample until a bottleneck layer at which point the decoder restores the spatial dimensions to produce the output images intuitively the model corresponds to a coarse refine process the encoder reduces the spatial dimension of the input image to high level coarse features and the decoder recovers the spatial dimension by refining the coarse features the assumption is that the input and output images share the underlying structure or they are different renderings of the same underlying structure that is their structures are roughly aligned isola et al 2017 as the goal of this study is to generate a targeted output image corresponding to given inputs the encoder decoder model learns the mapping x y from a conditioning input image x to the output image y the network converts images from the source to target domains where the first corresponds to the initial boundary conditions and model parameters and the latter to the resolution of the governing equation given those constrains 2 3 deep convolutional neural networks cnns lecun et al 1989 rumelhart et al 1985 are popular deep learning networks specialized in image processing krizhevsky et al 2012 szegedy et al 2015 while the first layers detect basic features deeper convolutional layers learn higher representation a convolution layer is a linear transformation that highlights the presence of a given feature in the map while preserving spatial information in the input image goodfellow et al 2016 given a 2 d input image and a square kernel ω with size m the convolutional layer outputs the value at location i j by summing up the contributions from the previous layer cells yl 1 weighted by the filter components then the nonlinearity σ is applied 1 y i j l σ a 0 m b 0 m w a b y i a j b l 1 the stride of the convolutional layer is a parameter that determines the number of pixel shifts between two successive moves of the filter while the padding indicates the amount of pixels with value zero added at each side of the boundaries of the input the rectified linear unit function relu is a piecewise linear function that outputs the input if it is positive and zero if negative the leaky relu with slope coefficient α modifies the function to allow a small negative output when the input is negative 2 σ x x i f x 0 α x o t h e r w i s e batch normalization and dropouts are used to stabilize training and mitigate overfitting salimans and kingma 2016 a dropout layer selects a random set of units from the preceding layer and ignores their output while batch normalization standardizes the layer s inputs by calculating the mean and standard deviation across the batch 3 application 3 1 groundwater model and datasets consider steady state groundwater flow in saturated media satisfying the fundamental governing equation todd 1980 3 k h q 0 the piezometric head h l is the field variable of interest k is the input hydraulic conductivity l t and q represents the source or sink terms l3 t 1 the problem of this study consists of steady state flow in a single layer model representing a heterogeneous confined aquifer initially in this work only dirichlet boundary conditions are considered and the groundwater head values are fixed in the cells in which the allocated head is known the model takes in an input image with three channels head values boundary markers and spatially varying hydraulic conductivity fig 1 dirichlet boundary conditions are imposed on the four sides of the square domain head is constant at up to three random locations across the domain representing wells the source term q is set to zero the second channel of the input image is a binary mask where the boundary markers identify the cells with a fixed value i e well locations and boundary cells as defined by the first source image the last input channel defines the heterogeneous media the conductivity field k of the highly heterogeneous aquifer is a gaussian random field vanmarcke 1983 in which the values of hydraulic conductivity are taken from a finite set of values this application example demonstrates the capability of an attention u net to successfully learn and simulate a common hydrologic situation using an image to image translation approach the model is trained to predict the output fields consisting of the spatial components of the groundwater head in the domain these predictions are compared against simulation results obtained by the fully implicit finite difference model modflow harbaugh 2005 here called target images bearing in mind that finite difference results provide an approximation of the partial differential equation and are not error free 3 2 network architecture the encoder decoder model is implemented as attention u net oktay et al 2018 ronneberger et al 2015 and the employed network architecture is shown in fig 2 for the case of a 64 64 input image as used in our computational tests this is an encoder decoder model with skip connections in the down sampling half the inputs are encoded with a series of cnns with kernels of size 4 stride of 2 and padding set to 1 in each block cnn is followed by a batch normalization layer dropout with rate 0 5 and a leaky relu with slope 0 3 as the number of filters increases to 512 and the size of the input images reduces to 4 4 the encoder captures high level abstract information in the up sampling half the representations are expanded spatially and the number of channels is reduced by a series of cnns and up sampling layers the last up sampling layer is followed by a transposed convolution layer with a sigmoid activation function to ensure predicted values between 0 and 1 skip connections link the layers in the encoder with corresponding layers with the same sized feature map in the decoder ronneberger et al 2015 the only difference between attention u net and the original u net architecture is that in the attention u net network skip connections are additionally passed through attention gates which use additive soft attention oktay et al 2018 the attention coefficients are larger if the vector from the next lowest layer of the network in the up sampling path and the corresponding vector from the encoder going through the skip connection are aligned the weights are multiplied element wise to the original vector which passes along in the skip connection in this way the attention gate ag mechanism allows the u net to suppress irrelevant regions and focus more on target structures of varying size and shape for reproducibility full details of the network architecture are described in appendix a 3 3 loss function the aim of the regression task is to minimize the mean square error mse between the generated samples and training data the network computes the average loss across a mini batch of size nb 4 l m s e 1 n b j 1 n b i 1 n y j i y j i 2 n where y is the training image y is the image generated by the network and n denote the total number of pixels of each image the networks tries to be near the ground truth output in an l2 sense 3 4 network training the model is trained in supervised fashion for the examples described in the following section a dataset consisting of 32000 training data is used and the development of the loss function is compared with 8000 validation data the size of the training dataset is big enough to ensure the model s ability to generalize and the generation time is less than 3 hours the losses are minimized using the adam optimizer kingma and ba 2014 with a starting learning rate α 8 10 4 the network is trained for 130 epochs training converged after approximately 2 hours by training the models on an intel r xeon r gpu tesla k80 the quality of the trained network is evaluated by reporting the coefficient of determination r2 and the root mean squared error rmse between each pixel value from the target image and each pixel value from the generated image 5 r 2 1 i 1 n y i y i 2 2 i 1 n y i y 2 2 r m s e 1 n i 1 n y i y i 2 2 where y is the target image y is the mean of the target images of the dataset y i 1 n y i n y is the network prediction and n is the total number of samples the two selected metrics between them yield complementary and representative information for the evaluation of the trained model 4 results and discussion 4 1 model predictions this section presents both qualitative and quantitative techniques to test the performance of the model the test case considers a square domain ω 0 64 0 64 consisting of 64 rows and 64 columns with the width of each cell equal to one the boundary is assumed to be constant head boundary with head of 1 while the imposed head values in the wells lie in the range 0 5 1 the number of wells their locations and their values are randomly selected and vary for each data sample the conductivity field k of the highly heterogeneous aquifer is generated as a continuous gaussian random field which is then discretized into a finite set of values the heterogeneous hydraulic conductivity field has values belonging to 5 different classes 0 1 0 325 0 55 0 775 1 which can be thought of as 5 soil types distributed within the model the model is trained with loss function in eq 4 and the target response is the finite difference simulation to illustrate the superior performance of the proposed attention u net architecture against the original u net network architecture the u net network without attention gates is also trained using the same training sets and parameters at the end of the training the attention u net network achieves a rmse of 1 98 10 3 and a r2 score of 0 996 while those obtained by u net are 3 78 10 3 and 0 986 respectively fig 3 fig 4 provides a comparison of generated images of groundwater head with the target images for 5 random examples taken from the test dataset with a set size of 4000 samples the attention u net model has learnt to map the flow patterns it generates accurate predictions for varied input samples that are unseen during training the predictions match the target images very well the model predicts the correct value of groundwater head and the pattern of its distribution the model is able to identify and focus on salient image regions the attention coefficients are highest at the boundary of the domain and near the well locations while they are low in the areas with small head distribution gradients when trained without attention gates u net can predict the values of the groundwater head in the domain but the generated outputs have some minor deviations especially at a distance from the source area and the head gradients are smaller the use of the attention mechanism significantly improves the accuracy of the results fig 5 visualizes the attention coefficients obtained from two test images with respect to training epochs during the first 20 epochs the loss function rapidly decreases fig 3 top and the attention gates learn to identify the location of the wells the boundaries and a rough outline of the area with large head distribution gradients by training the network for longer epochs the attention coefficients are gradually updated and refined to focus on areas with large head distribution gradients appendix b shows the 5 predictions with highest and lowest mean square error between the generated samples and training data out of 500 random samples from the test dataset the errors are localized near the wells and the difference between the generated and target images is almost negligible even for the samples with the highest error 4 2 model evaluation to test the performance of the model its computational power is compared with the modflow engine table 1 presents the processing time required for running the forward operators averaged on 10 examples in order to have a fair comparison between the two the tests are performed on the same hardware the cpu used is intel r xeon r cpu 2 20ghz and the gpu is tesla k80 the results demonstrate that attention u net requires less computational power than modflow this experiment reveals a 75 computational reduction for the data driven model showing its capability to be used in forward simulations with less computational demand than the state of the art numerical solver when applying the method to computationally more expensive forward models such as in large scale non linear system the computational cost of the neural network will remain low and significant computational savings can be expected dropout at inference time can be considered equivalent to bayesian approximation in deep gaussian processes and the neural network uncertainty can be quantified following the approach proposed by gal and ghahramani gal and ghahramani 2016 at test time the same input is passed 1000 times to the network with random dropout the mean and the standard deviation of the generated images give an estimation of the prediction interval fig 6 presents the results for 3 random samples the uncertainty is null at the boundaries and highest in the vicinity of the wells which is also the region with highest errors compared to the finite difference solver whose response is deterministic this method allows one to estimate the uncertainty of the model the generalization capabilities of the network are presented in appendix c the model is able to extrapolate to out of distribution inputs especially for different values of hydraulic conductivity and less so for increasing number of wells it is worth pointing out that the effect of using attention gates on the uncertainty and generalization capabilities of the model hasn t been addressed in the current study future work should investigate this relation and consequently explore how generalization on out of distribution input samples can be improved 5 conclusion and future work this paper presents a convolutional encoder decoder network to quickly calculate the steady state response of a groundwater system the data driven surrogate model is trained and tested in different scenarios in which the groundwater head values in the whole domain need to be inferred from the hydraulic head at the locations of the wells the square domain is a gaussian random field with a spatially varying hydraulic conductivity when trained by minimizing the departure from the target images the proposed u net model easily learns the nonlinear relation between inputs hydraulic conductivity fields and boundary conditions and output the hydraulic head field a significant contribution of the proposed framework is to incorporate attention gates which allow the network to identify and focus on the salient regions of the image the visualization of the attention coefficients demonstrate that the model has learnt to pay attention to areas with large head distribution gradients the attention mechanism improves the network s approximation accuracy and reduces the model uncertainty the application of the data driven surrogate method in solving forward simulations gives very accurate results but requiring much less computational time than the state of the art numerical solver one attractive property of this methodology is that the learning is carried out offline training converged after less than 3 hours on an intel r xeon r gpu tesla k80 which can be considered as a low training time compared to typical deep learning models once the model is trained its weights and parameters do not need to be further tuned the choice of the hyperparameters and the specificities of the u net architecture have been chosen based upon manual variation as opposed to systematic optimization to give accurate results with low computational time with little apparent sensitivity future work could include a more robust hyperparameter tuning study with a quantitative sensitivity analysis in the current study only dirichlet boundary conditions were applied to the borders of the domain and the locations of the wells an additional natural extension of our work is to investigate how well the model generalizes to different and mixed types of boundary conditions discretization is another important factor to consider the present work has been limited to data samples with the same resolution many questions remain open related to the discretization of the sample data e g the generalization of the trained model to different discretizations and the amount of training data required if the model needs to be retrained for different resolutions the authors plan to further develop the presented model for more complex larger and uncertain systems this could include time dependent problems three dimensional simulations and coupled transport through porous media all of which are likely to require larger training data sets and potentially deeper networks another potential extension is the incorporation of prior information directly into the learning process by imposing a physics constraint in the loss function physics informed learning could increase the speed of inference while requiring less data for the training process finally in this study the network has been trained using synthetic data but the potential use of the proposed model holds promises for the solution of practical applications due to its data driven nature declaration of competing interest none acknowledgments this work was supported by the leeds york hull natural environment research council nerc doctoral training partnership dtp panorama under grant ne s007458 1 and by rijkswaterstaat geert menting in the project fysica gebaseerde neurale netwerken in grondwater under kpp bo06 2018 and kpp bo06 2019 the authors acknowledge the additional guidance and practical expertise provided by the dutch research institute deltares appendix a network architecture this appendix discusses details in the models used both u net and attention u net have 4 downsampling layers and 4 upsampling layers table a 1 each layer consists of a series of cnns with kernels of size 4 stride of 2 and padding set to 1 followed by a batch normalization layer and dropout with rate 0 50 the nonlinear activation is leaky relu with slope 0 3 for the downsampling layers and relu for the upsampling ones table a 2 and table a 3 skip connections concatenate the layers in the encoder with corresponding layers in the decoder the network of attention u net additionally has attention gates which are implemented as according to the work of oktay et al oktay et al 2018 the total number of parameters of the network is 1 36 107 of which 7 35 106 are for the attention gates appendix b worst and best model predictions this appendix shows the 5 predictions with highest and lowest the mean square error out of 500 random samples from the test dataset the samples with highest errors present multiple wells with wide plumes which cover most of the domains fig b 1 on the contrary the best predictions are those in which the salient region is limited fig b 2 in all cases highest errors are localized near the wells it is worth noticing that even when the error is higher the mse is in the order of 10 5 and the difference between the generated and target images is almost negligible appendix c generalization so far the model has been both trained and tested for different scenarios in which the groundwater head values in the whole domain is inferred from the piezometric head at the locations of up to three wells and the spatially varying hydraulic conductivity has values belonging to 5 different classes between 0 and 1 here we consider testing the model on cases that have different numbers of well locations and different values for the hydraulic conductivity between 0 and 1 fig c 1 shows the mse error for the model tested on four new input distributions the figure shows that the model is able to generalize well given different values of the hydraulic conductivity but less so for increasing number of wells 
171,this work investigates numerical method and equivalent continuum approach eca of fluid flow in fractured porous media the commonly used discrete fracture model dfm without upscaling needs full discretization of all fractures it enjoys the merit of capturing each fracture accurately but will get in trouble with mesh partition and low computational efficiency especially when a complex geometry is involved in this study we develop an efficient implicit scheme with adaptive iteration in which an improved eca is devised and then integrated in this scheme numerical studies show that the proposed numerical scheme improves the convergence condition and computational efficiency then a test is conducted to demonstrate the feasibility of using superposition principle of permeability tensor in upscaling based on these different strategies are applied to simulate fluid flow in fracture networks with a complex geometry it is demonstrated that the proposed eca is able to reproduce the results computed by dfm the accuracy depends on resolution of background grids the presented method enjoys a low computational cost and desirable convergence performance compared with the standard dfm in which equivalent continuum is not considered keywords fractured porous media two phase flow upscaling equivalent permeability tensor implicit scheme 1 introduction previous studies have revealed that the presence of discrete fractures has significant influence on characteristics of fractured porous media berkowitz 2002 adler et al 2013 wang et al 2020 however modeling of flow and transport in fractured porous media raises numerical challenges in geoscience applications cotta et al 2020 medici et al 2021 wang et al 2022 in the community of geological modeling there are two main categories of the computational model namely the discrete fracture model dfm aliouache et al 2019 wang et al 2022 and the equivalent continuum model kottwitz et al 2021 lasseux et al 2021 this work intends to study the compromise between the accuracy of dfm and the convenience of equivalent continuum model we focus on two phase flow in fractured porous media and discuss the extension to a more general case namely fractured vuggy porous media which has gained much attention in recent years yan et al 2019 mohammed et al 2021 xu et al 2022 there are two main groups of numerical method dealing with the interaction between fractures and matrix namely conformal and non conformal mesh schemes the classical dfm is a widely used conformal mesh scheme dfm was presented to explicitly reproduce stochastic fractures based on field investigation normally the fractures are simulated by the low dimensional elements lying on the interface of matrix elements wang et al 2022 it enjoys several merits such as simulating each fracture accurately and a desirable flexibility to directly borrow the numerical discretizations from finite element and finite volume methods gläser et al 2017 wang et al 2019 2022 however it may encounter some difficulties of complicated mesh strategy required by the conformal grids and an expensive cost if a complex geometry is considered bahrainian et al 2015 fourno et al 2019 xie and edwards 2021 indeed the presence of fractures and vugs may introduce a complex topology of the geometry which reduces the quality of convergence condition and then leads to unnecessary cost to this end the non conformal mesh scheme and the equivalent continuum model were introduced as two representative methods to simplify the connection of fractures matrix ţene et al 2017 azizmohammadi and sedaghat 2020 lasseux et al 2021 these methods try to find a balance and compromise between accuracy and efficiency but none can be done without drawbacks in the scope of non conformal mesh edfm embedded dfm is developed as an alternative version of traditional dfm zhang et al 2017 hajibeygi et al 2020 in the last decade other non conformal methods have been proposed typically the extended finite element method ren et al 2018 phase field method lee et al 2018 and lagrangian multipliers schädle et al 2019 a transfer function is introduced to capture the connection between the embedded fractures and the surrounding matrix nevertheless accuracy of non conformal mesh schemes is highly relying on the interpolation manners on fractures and the flux transfer function it is a trade off between accuracy and the complicated conformal meshing in contrast dfm based methods allow directly employing the traditional numerical discretization schemes such that the accuracy can be ensured gläser et al 2017 tan et al 2021 wang et al 2022 the hierarchical modeling technique based on the concept of equivalent continuum model was introduced to upscale small size fractures and retaining the relative larger size fractures lee et al 2001 yan et al 2021 lasseux et al 2021 in this way the computational efficiency improves a lot but the accuracy highly depends on the grid resolution of the upscaled models in general the methods of calculating equivalent permeability tensor of the equivalent continuum model can be categorized into two main groups the first group is based on the analytical methods pioneered by snow 1969 and oda 1985 later there are some other analytical methods have been developed for instance critical path analysis adeyemi et al 2022 homogenization mezhoud et al 2020 and effective medium theory hosseini et al 2021 their application is restricted due to limitations of implementation and simplified assumptions the second group is flow based upscaling by numerical simulation kottwitz et al 2021 lasseux et al 2021 it has been widely used in reservoir engineering and geoscience applications its merit is to be applicable for complex geometry the presented work is based on the second strategy despite being vital importance there are still some open issues needed to be addressed one key point is the criterion of upscaling although the equivalent continuum model enjoys some conveniences in effortless meshing and acceptable computational cost azizmohammadi and sedaghat 2020 kottwitz et al 2021 lasseux et al 2021 it is sophisticated to determine which fractures should be upscaled or retained besides the grid resolution of the upscaled sub blocks has great impact on numerical simulation therefore it is worthwhile to study the impact of resolution on accuracy on the other hand the expensive computation cost of the dfm reduces the efficiency of simulation especially if the presence of multiple fractures is considered to solve this in this work we combine the advantages of the accuracy of dfm and the efficiency of equivalent continuum model an improved equivalent continuum approach is presented to reduce the complexity of fractured media in addition we introduce the superposition principle of equivalent permeability tensor and integrating it into the upscaling scheme at this point some researchers have developed the superposition principle in fractured media lei et al 2015 xu and yang 2020 however their discussions are restricted to single phase flow in fractures with regular patterns particularly upscaling of two phase flow in fracture vuggy network is rarely studied in the existing literature golfier et al 2015 yan et al 2019 wang et al 2022 in this work we investigate numerical method and equivalent continuum approach of fractured media modeling approach of fractured vuggy porous media and the fully upscaling strategy are presented section 2 provides the mathematical formulation of two phase flow in fractured vuggy porous media section 3 presents an efficient implicit scheme with adaptive iteration based on the hybrid dimensional modeling section 4 proposes an equivalent continuum approach as well as the treatment regarding upscaling of a complex geometry a general form of the superposition principle is introduced to evaluate equivalent permeability tensor in the equivalent model in section 5 a series of numerical tests is conducted to study the performance of the proposed scheme 2 mathematical model in this section formulation of the fractured media with complex geometrical structures is presented a complex fractured medium is decomposed into several basic components then the governing equations of two phase flow in fractured media are given for completeness 2 1 physical domain of fractured vuggy porous media assuming a domain ω consists of three sub domains with the matrix ω m fractures ω f i n f ω i f and cavities ω c i n c ω i c as displayed in fig 1 the numbers of fracture and cavity distributed in this domain are n f and n c respectively furthermore ω is confined by boundary γ including external and internal boundaries denoted γ e x γ e x n γ e x d and γ i n γ i n n γ i n d note that the subscripts n and d are neumann and dirichlet types respectively thus the system of a porous medium containing stochastic cavities and fractures is defined as 1 ω ω m ω f ω c γ γ e x γ i n it is worth mentioning that the state of ω c can be varied such as filled by geological materials ω f i l l e d c or empty ω e m p t y c in the former case the permeability of ω f i l l e d c is allowed to be assigned to different values compared to the rock matrix ω m in the later case ω e m p t y c is regarded as the sink or source in this work our approach allows the consideration of these states hereafter we will assume that the role of the empty cavity ω e m p t y c can be specified as an internal boundary γ i n imposed on the edge of it ω e m p t y c if a dirichlet type boundary condition pressure p is applied it reads 2 p p on ω e m p t y c 2 2 governing equations for two phase flow the formulation for two incompressible and immiscible fluids in porous media is given as follows for completeness the mass conservation for wetting phase w and non wetting phase n is described by 3 ϕ s α t u α f α s α q α w n where s α is saturation and s n s w 1 ϕ is porosity q volumetric flux the frictional flow and phase velocity are represented by f α s α and u α respectively aziz 1979 hoteit and firoozabadi 2008 the phase velocity is determined by darcy s law pickup and serbie 1996 kolditz et al 2012 the relative permeability k r α s α is involved as a function of s α 4 u α k r α s α μ α 1 k p α ρ α g z α w n where k is the absolute permeability tensor μ α and ρ α are the viscosity and density of phase α g and z denote the gravity acceleration and depth respectively the two pressures are related by the capillary pressure p c s w p n p w we refer to hoteit and firoozabadi 2008 for the detailed formulation for convenience phase mobility λ α s α is defined by k r α s α μ α 1 total mobility is λ t λ w s w λ n s n therefore f α s α λ α s α λ t the total velocity is calculated by the summation of phase velocities 5 u k λ t p n λ w p c λ w ρ w λ n ρ n g z initial and boundary conditions are required to confine the behavior on boundary and at the initial time t 0 6 s w s w on ω m ω f ω c t 0 p w p on γ e x d t e or ω e m p t y c t e λ t k p n 0 on γ e x n t e or ω e m p t y c t e where t e t 0 t n is the entire time period n the outward unit normal vector to γ s w and p are the prescribed saturation and pressure respectively combining eqs 3 4 and 6 a well posed system of partial differential equations pdes is constructed to capture two phase flow in a fractured vuggy porous medium 3 numerical method in this section the numerical method based on galerkin finite element method is presented to discretize the governing equations eqs 3 and 5 an efficient implicit scheme with adaptive iteration is devised to efficiently solve the system of coupled pdes 3 1 hybrid dimensional mesh partition the hybrid dimensional elements are introduced to partition the fractures and matrix separately tan et al 2021 wang et al 2022 we employed the commonly used conformal mesh where fracture elements ω f are lying along the edges of each pair of matrix elements ω m as displayed in fig 2 an arbitrary pair of matrix elements is denoted by ω i m ω j m the intersection set operation of them is a fracture element lying between them ω i m ω j m ω i j f the open source software triangle shewchuk 2014 is used to generate grids on the matrix ω m i 1 n e m ω i m and cavities ω c i 1 n e c ω i c the filled and empty cavities are treated separately and only filled cavities are meshed then the fracture elements are generated based on the connectivity of the matrix and cavity cells such that ω f i 1 n e f ω i f numbers of the matrix cavity and fracture elements are n e m n e c and n e f 3 2 formulation of the implicit scheme with adaptive iteration to discretize the transport equation eq 3 the temporal integral is taken over a time increment δ t and implicit difference scheme is used to discrete time dependent term for phase α it reads 7 ω ϕ x δ t s α n 1 s α n d v time dependent term ω u α n 1 d v flux term ω f α s α q n 1 d v source term the transport equation and darcy s law construct a coupled system the frictional velocity u α is obtained by the darcy s law for multi phase flow mentioned in eq 4 in implementation it is commonly represented by the product u α f α u such that f α is determined by the relative permeability k r α s α thereafter u is directly represented by eq 5 we employ the implicit time discretization and substitute eq 5 into the following equation 8 ω u n 1 d v ω q n 1 d v eqs 7 and 8 construct a coupled system of pdes we use eq 8 to calculate pressure then the total velocity and frictional velocities can be obtained the saturation is calculated from eq 7 based on the updated velocities according to the discussion in section 3 1 the matrix and cavity elements are of higher dimensional compared to fracture elements consequently the residuals for each type are defined to solve the nonlinear system using implicit iterative scheme following the notations defined in section 3 1 for each ω i m or ω i c the residual is updated at each iterative step 9 r i m c n 1 ϕ x v i m c ξ i δ t s α i n 1 s α i n f α i s α i q i n 1 v i m c ω i m c u α n 1 d v upwind term for ω i m or ω i c where i 1 n e m or n e c note that ω i m c is either ω i m or ω i c for each ω i f i 1 n e f the residual is written as 10 r i f n 1 ϕ x l i f a i f ξ i δ t s α i n 1 s α i n f α i s α i q i n 1 l i f a i f ω i f u α n 1 d v upwind term for ω i f where ω i represents an integral over the element v i m c is the volume of ω i m or ω i c l i f and a i f are length and aperture of ω i f the shape function ξ equals 1 if a cell centered scheme is applied the third term in right hand side of above equations is determined by the upwind scheme respective to different elements the expanded forms are given in appendix a the iteration will be terminated once a criterion is satisfied which is given by r n 1 2 ε the tolerance ε is user defined normally at a magnitude of 1 0 6 usually the time step δ t is a fixed value defined in the input however this preference sometimes induces that the system has difficulty to converge and reduces computational efficiency to solve this issue the adaptive iteration is devised to improve convergence condition such that δ t can be changed dynamically based on the condition of residual as displayed in fig 3 three values are introduced r c r 1 and r 2 the new time increment denoted by δ t depends on the condition evaluated by the three values 11 δ t r 1 δ t if r n 1 2 r c ε r 2 δ t if r n 1 2 ε δ t otherwise note that 0 r c 1 0 r 2 1 and r 1 1 eq 11 is evaluated at the end of each iteration at each iteration eqs 9 and 10 are calculated over all element until i n e such that a residual vector is constructed r r m r f r c t note that n e n e m n e c n e f then newton raphson approach is employed to obtain the updated saturation at each iteration 12 j m m j m f j m c j f m j f f j f c j c m j c f j c c ν j ν δ s m δ s f δ s c ν 1 δ s ν 1 r m r f r c ν r ν where components of jacobian j are calculated by j r s according to eqs 9 and 10 note that the subscripts m f c represent matrix fracture and cavity consequently the saturation in previous iteration s ν is updated by s ν 1 s ν δ s ν 1 ν is the iterative number the high contrast of permeabilities on fractures and matrix would lead to an ill conditioned jacobian j with a high condition number which would bring an inaccurate result to this end a preconditioning technique is proposed to better constrain the solution the preconditioned jacobian j is derived by a preconditioner p therefore the expanded forms of p can be presented it is easy to implement which is given in appendix b the operation eq b 1 provides an efficient and simple way to improve numerical quality of j 4 an equivalent continuum approach in practice the upscaling approach is usually used to simplify the complex fractured network and field heterogeneity consequently the discrete fractures are upscaled into many sub blocks with equivalent parameters in this section an equivalent continuum approach is devised based on the flow based upscaling technique superposition principle is introduced to compute permeability tensor then a treatment of complex geometries is addressed 4 1 upscaling for fluid flow in fractured porous media the results of upscaling should be satisfied with the equation of the original problem for convenience the capillary pressure is assumed to be zero substituting the calculated solution into the original equation it reads 13 q μ 1 k p where k is the equivalent permeability tensor p is the new solution calculated based on upscaled parameter it means that p should be close to the pressure in the original problem p chen et al 2015 the component form of k is 14 k k x x k x y k y x k y y the expanded forms given in appendix c are solved by the flow based approach to calculate k x x and k x y as shown in fig 4 a pair of constant pressures p l and p r are applied on the left and right hand sides while the linear pressures p l 1 x l x x p r l x are applied on the top and bottom sides therefore one can obtain the fluxes q x and q y here we use the calculation method proposed by chen et al 2015 in which the so called multiple boundary upscaling is used to calculate the fluxes if the inlet is placed on the left boundary l as illustrated in fig 4 we have 15 q x l l y v r n 1 d y l x v t n 1 d x l x v b n 1 d x q y l l x v t n 2 d x l x v b n 2 d x l y v r n 2 d y where v is the flow rate respective to top t bottom b and right r boundaries n 1 and n 2 are the unit normal vectors along vertical and horizontal axes therefore k x x and k x y can be computed the fluxes are computed for other boundaries in a similar way then k y y and k y x can be computed however the fluxes are not identical at the two parallel boundaries for a fractured medium to this end we introduce the treatment proposed by chen et al 2008 eq 15 is evaluated twice respectively to left and right sides denoted by q x l and q x r consequently q x q x l q x r 2 the same treatment is applied on top and bottom sides q y q y t q y b 2 the calculation of the upscaled permeability tensor is straightforward and derived from darcy s law sometimes the upscaled k is asymmetric the off diagonal components are set to k x y k y x 2 as advised by durlofsky 2005 and chen et al 2015 for a two phase flow we refer to the works by jackson et al 2018 and benham et al 2021 the equivalent relative permeability is defined as 16 k r α s α u α μ α l k 0 δ p α where is a type of spatial averaging δ p α is the pressure drop between inlet and outlet l is the distance between inlet and outlet k 0 is a typical dimensional scaling which is selected as a mean permeability in practice this expression is adapted from jackson et al 2018 benham et al 2021 used it to upscale fluid flow in a porous medium however this formula is usually involved in analytical studies and inconvenient to implement furthermore it is worth noting that rigorously the relative permeability is a tensor k r α pickup and serbie 1996 which is absent in eq 16 in general darcy s law for multi phase reads u α μ α 1 k r α k p to do this a result from matthai and nick 2009 is introduced in our work they provided a method to upscale k r α according to the upscaled k here we give the component forms as shown in appendix c 4 2 superposition principle of equivalent permeability tensor a superposition principle of equivalent permeability tensor is integrated in this work for upscaling discontinuities assuming a matrix block intersected completely by several fractures as shown in fig 5 each component of the equivalent tensor is equal to the summation contributed by these fractures respective to this component alternatively if we wish to calculate k of a complex pattern namely k one can simply conduct a summation operation based on the known basic patterns these basic patterns are constructed by single fracture cutting the porous rock with different geometrical parameters as illustrated in fig 5 k 1 of a simple pattern consists of the contributions from the rock matrix and the single fracture namely the superposition of them k 1 k m k f 1 note that k m is obtained in the case without fractures this idea can be extended to a more general case where multi fractures n f are considered 17 k i n f k i n f 1 k m to show how it works fig 5 provides an example when n f 2 an underlying assumption to this principle is that the interaction between fractures is neglected it produces an acceptable error compared to the accurate solution note that this principle has been used in fractured reservoir simulation zhang et al 1996 lei et al 2015 xu and yang 2020 the validation of this principle will be elaborated through numerical tests in section 5 4 3 treatment of complex configuration the preceding content discusses the treatment to a dual fracture pore medium where the cavities are absent for a fractured vuggy porous medium the presence of cavities would impose significantly to hydraulic property of this medium huang et al 2011 golfier et al 2015 yan et al 2019 in this section the treatment of a complex pattern with the fracture cavity networks is presented then it will be employed in section 5 for numerical study it is worthwhile to note that the superposition principle proposed in section 4 2 can be used in the upscaling procedure fig 6 shows a domain with complex geometry it is partitioned using a physical decomposition background grids and then upscaled by the flow based equivalent continuum method the pattern of physical decomposition is determined by the geometrical configuration in which the distribution of cavities and fractures is an important factor the cavities with a periodic pattern are upscaled hydraulically as solid squares by yan et al 2019 which is reliable when the cavities are filled by geological materials however as discussed in section 2 1 and eq 2 in some cases a cavity can be treated as a region of constant pressure in this case the upscaling of cavity which plays the role of an internal dirichlet boundary should be addressed as shown in fig 7 consequently this special situation is considered by imposing an internal pressure at the cavity walls the cavities are upscaled as several sub blocks for the convenience of meshing in the equivalent model and can reduces geometrical complexity each cavity covers several background grids which consist of an influenced area as depicted in fig 6 we follow the notations defined in section 2 fig 7 provides a novel treatment if a boundary condition p p is imposed on edge ω e m p t y c obviously grid resolution l g has great impact on upscaling as displayed in fig 6 in many remarkable works the large size fractures are retained since they have influence on flow lee et al 2001 li and lee 2008 it is termed as hierarchical upscaling in this paper different size fractures are classified by a ratio l r l f l g then the upscaling procedure is performed based on different types of fracture as illustrated in fig 8 an alternative scheme is applied for the sake of convenient implementation chen et al 2015 yan et al 2019 azizmohammadi and sedaghat 2020 the matrix and all fractures are fully discretized by the background grids it is termed as fully upscaling in this paper notably chen et al 2015 upscaled the large size fractures by partitioning them into several sub blocks which are intersected completely by fractures here as displayed in fig 8 we present these two different treatments regarding the large size fractures 5 numerical tests and discussion in this section the proposed efficient implicit scheme and equivalent continuum approach are synthetically combined to simulate flow in fractured media it should be noted that the capillary pressure is neglected for validation karimi fard et al 2004 moinfar et al 2014 jiang and tchelepi 2019 the superposition principle presented in section 4 2 is integrated into the upscaling procedure then fluid flow in a fractured medium with a complex geometry is studied using the approach introduced in section 4 3 note that the simulations are performed based on intel core i7 7950 processor at 2 6 ghz with 16 gb memory 5 1 numerical verification and convergence test the benchmark is extracted from a publication by karimi fard et al 2004 following the same setting in this literature parameters of this model are set to porosity ϕ 0 2 permeability of matrix k m 0 99 1 0 15 m 2 and fracture k f 8 33 1 0 10 m 2 such that the fractures are modeled as a conductive channel the coordinates of each fracture refer to karimi fard et al 2004 the result simulated by dfm as the reference solution is used to compare the result computed by the proposed numerical scheme fluid is injected at the bottom left corner of the model and the outlet is located at the top right corner as shown in fig 9 this figure depicts several stages of saturation during evolution at different pvi injected pore volume injection rate is 0 01 pvi d it appears that the result agrees well with the reference solution to test performance of the presented method under different conditions we define a ratio k r k f k m to measure the permeabilities of matrix and fractures as reported in literature the high contrast of permeability would induce an instability of simulators ţene et al 2017 the ratio k r is set to 1 0 5 and 10 5 to simulate two extreme situations note that k m 1 md the former case describes fractures as the conductive channels while the later case represents the fractures as the barriers because of the contrast of k f and k m the jacobian becomes ill conditioned which is improved by the preconditioner p proposed in section 3 2 furthermore the adaptive iterative scheme is applied to improve the convergence quality fig 10 displays a set of curves to compare the grid convergence with different k r it shows that k r has great impact on the convergence performance the convergence condition is gradually improved with a decreased k r obviously this test proves that the presented numerical scheme has a good convergence performance when the high contrast of permeability is involved another test is performed to evaluate performance of the proposed adaptive iterative scheme in section 3 2 to this end computations by the improved scheme and classical scheme are carried out the former scheme enjoys the advantage of adaptive iteration while the later one uses a fixed time step during the entire iterative process fig 11 shows a comparison of convergence performance during iteration using improved scheme and classical scheme with different k r also the total number of newton iterations of both schemes is given it appears that the presented method is able to improve the convergence condition and reduces the total time to study the grid convergence performance of this method the model as shown in fig 9 is simulated with different local grid refinements the maximum and minimum values of the grid resolution are denoted by h m a x and h m i n respectively then h m i n is allowed to be changed at a fixed h m a x as displayed in fig 12 the error ε h is defined by the normalized difference between the reference solution and the results calculated using different local refinements fig 13 provides the grid convergence with the refinement at a fixed h m a x it proves that the presented method has good performance of the grid convergence 5 2 study on superposition principle of equivalent tensor in section 4 2 the superposition principle is proposed for equivalent permeability tensor ept based on the flow based upscaling here we analyze this principle using the presented numerical scheme we use the dimensionless parameters the size of the model is 1 1 the permeability of the fractures and matrix are k f 1 0 5 and k m 1 the inlet is placed on left boundary p l 1 while the outlet is placed on right boundary p r 0 fig 5 provides an illustration of a fractured medium with a simple pattern to study the superposition principle ept of several basic patterns should be calculated firstly as displayed in fig 14 then a complex pattern can be easily obtained using these basic patterns at this point we use eq 17 note that when n f 2 the superposition computation should be operated carefully to explain it fig 14 shows a case with n f 3 where the ept of a complex pattern is calculated by k 123 i 1 3 k i 2 k m it is worthwhile to notice that k m should be subtracted twice which equals to factor n f 1 in eq 17 a comparison between results of the fluid flow simulation ffs and superposition principle sp is depicted in fig 14 the permeability tensors of the complex patterns are obtained from ept of basic patterns error of each pattern is computed by k i j k ˆ i j k i j where the symbols ˆ and represent sp and ffs respectively it shows that the error is very small therefore the assumption of sp is validated in this test the source of error is probably induced by the numerical discretization and grid resolution 5 3 flow in porous media with a fracture network to demonstrate the performance of the proposed method a porous medium with a fracture network is simulated this fracture network contains different size fractures the approach discussed in section 4 is employed as shown in fig 15 the background grids with shade are the covered grids of fractures obviously the resolution l g has great impact on the mesh pattern as well as the simulated results in this stage l g is determined by the fracture which has the smallest size the smallest size is l m i n f therefore l g l m i n f ω m e s h with a weight factor ω m e s h herein we present two different l g to show the effect of grid resolution on mesh patterns and simulation results as shown fig 15 the minimum length of fractures is 0 2 then the grid resolution l g is varied with the weight factor ω m e s h then l g 0 05 if ω m e s h 4 and l g 0 1 if ω m e s h 2 the inlet and outlet are p l 1 mpa and p r 0 on the left and right surfaces respectively the physical properties of two phase fluid are shown in table 1 note that the relative permeabilities follow the corey type model brooks and corey 1964 18 k r w s n n r k r n 1 s n n r where n r is the exponent of the corey type model the normalized saturation s n is defined in table 1 an obvious difference between the dfm and upscaling approach is that the upscaled model might fail to describe the sharp discontinuous pressure jump around the fractures as illustrated in the left hand side picture in fig 17 but their global distribution patterns agree well each other fig 16 shows the meshing strategy of different models as shown in this figure the unstructured grids are applied in the situation without upscaling the dfm where all fractures are retained and modeled explicitly if upscaling is applied these fractures are implicitly embedded inside the background grids by the equivalent continuum approach the solid lines are explicitly modeled fractures while the dashed lines are implicitly expressed the pressures distribution calculated by different models is displayed in fig 17 it demonstrates that the presence of fractures influences pressure field a significant pressure gradient is observed around the fractures it shows that the fully upscaling with different resolutions is able to reproduce this phenomenon note that the solution calculated by the hierarchical upscaling is very closed to that of the dfm to compare the results by different models a survey line is placed along the off diagonal crossing this domain this line cuts several fractures through its path as shown in fig 18 although the curve calculated by upscaling with l g 0 05 has a slight oscillation the global tendency agrees well with the curve of without upscaling however the accuracy is reduced if a coarse resolution for example l g 0 1 is employed this test proves that the simulated result is obviously impacted by resolution as pointed out in some studies karimi fard et al 2004 lee et al 2001 computation using dfm is more accurate than using upscaling approach but it needs to simulate all fractures explicitly and leads to an expensive cost compared to dfm the advantage of the equivalent continuum model is the use of structured grids instead of unstructured grids and the fractures are modeled implicitly as displayed in fig 16 to show their difference in aspect of convergence performance fig 18 provides a comparison of convergence history measured by l 2 norm of residual of these two different schemes it explains that the upscaling approach improves the performance of convergence compared with the dfm fig 20 displays saturation evolution simulated by different models to show the effect of fractures on hydraulic feature a high contrast permeability ratio is set to k r k f k m 1 0 5 it is observed that the fracture network plays a role of barrier to block fluid flow consequently the fluid accumulates at top and bottom sides since the fractures block the movement it shows that a fine resolution l g 0 05 in this case would lead to a more accurate result compared to a coarse resolution fig 19 displays a set of curves of the cumulative production versus pore volume injection the contrast of permeability between the fractures and the rock matrix is reflected by k r and it has significant influences on accuracy the high conductivity of fractures would lead to a relative high production compared to the low conductivity 5 4 a complex geometry a test is performed with a complex geometry to study fluid flow in a more complex porous medium this model is the so called fractured vuggy porous medium found in literature huang et al 2011 yan et al 2019 an essential difference between a cavity and a fracture is the spatial extension it results in different treatments of discretization for these two objects we follow the scheme proposed in section 4 3 to model the fractured media with complex geometry a fractured vuggy porous medium is shown in fig 21 the parameters are shown in table 1 note that the permeability of the filled cavity is set to 1 1 0 13 m 2 the dimensions in x and y axes are both 100 m in this simulation the filled and empty cavities are considered it reproduces a typical situation in carbonate reservoir engineering huang et al 2011 wei et al 2020 wang et al 2022 in which the connection of fractures and cavities constructs a connected channel system the inlet condition is imposed on the edge of a cavity with pressure p i n 1 mpa while the outlet is placed on the edge of another cavity p o u t 1 mpa these two cavities are connected by fracture networks two approaches dfm without upscaling and upscaling are applied to simulate fluid flow thus two different mesh strategies are adapted as shown in fig 21 the gray cells are embedded fracture cavity zones which need to be upscaled fig 22 provides saturation evolution of the porous medium as well as that of the fracture networks pressure distributions with respect to three different models are given in fig 23 it appears that discrete fracture model provides an accurate result but it requires explicitly simulation of all fractures the upscaling approach depicts similar result and the accuracy is increased with a finer resolution l g 0 025 is better than l g 0 05 a survey line is placed along the off diagonal of the domain pressure variation along this line is displayed in fig 24 correspondingly the relation of number of active grid cells versus pore volume injection is shown in fig 24 it demonstrates that the proposed equivalent continuum approach provides an efficient tool to model fluid flow in complex fractured media in a simple way the results simulated by equivalent continuum approach are shown in fig 26 different resolutions lead to different computational steps fig 25 provides a comparison analysis cell numbers of l g 0 1 0 05 0 025 and 0 0125 are 100 400 1600 and 6400 respectively correspondingly we select cell number n 108 398 1636 and 6423 for dfm on unstructured grids it shows that the total number of newton iterations n t o t a l of upscaling scheme is relatively smaller than that of dfm the adaptive time at reduces the computation steps compared to fixed time ft a test is analyzed with a more strict convergence criterion the l 2 norm of residual is 10 15 and initial δ t 1 0 3 1 0 4 the adaptive iteration enforces the time step more smaller compared to the previous iterative step as shown in fig 27 the error calculated by different resolutions l g is provided in this figure it is measured by the relative difference between the solutions of dfm and upscaling model s d f m s u p s d f m it shows that the error is relative small if a fine l g is applied the maximum error calculated based on an extremely fine resolution for example l g 0 0125 is 4 4 however it would produce an expensive computation cost which is greater than 1000 newton iteration number as shown in fig 25 in contrast if a more coarser grid resolution is applied for example l g 0 025 the error is acceptable but the cost is highly reduced to around 600 iterations it is more efficient than the finer grids thus finding a balance between accuracy and computational cost is a key point in practice the relation of cumulative production versus injection is shown in fig 28 the inlet and outlet are both placed on cavities it illustrates that the heterogeneity of permeability k r influences the production in this case the use of l g 0 025 in equivalent continuum model is sufficiently to recover the results of dfm 6 conclusions in this work an efficient implicit scheme with adaptive iteration is developed to simulate fluid flow in the fractured media with complex geometries an equivalent continuum approach is devised and then integrated in the simulator combining with the superposition principle of equivalent permeability tensor in the numerical scheme a preconditioner is proposed to improve the condition of jacobian and an adaptive iterative strategy is used to improve the convergence performance to model the fractured medium with a complex geometry especially the fracture vuggy network we introduce a novel treatment to upscale cavities which allows a cavity either filled or empty this scheme is able to simulate the inlet and outlet placed on the edge of cavity numerical tests are performed to verify the presented numerical scheme and the equivalent approach validation and grid convergence performance are evaluated through a benchmark study where different local grid refinements are considered a comparison study is conducted to show the merits of adaptive iteration to valid the feasibility of using superposition principle in upscaling a set of tests is carried out to demonstrate it then based on this we simulate fluid flow in a fractured vuggy porous medium different modeling strategies are employed to analyze and compare their performances it appears that the proposed equivalent continuum approach is able to reproduce the results computed by the dfm the accuracy depends on resolution of the background grids this method enjoys a low computational cost and good convergence performance compared with the dfm using fully discretized fractures several extensions of the presented approach deserve a further investigation it can be applied to a more general case such as two phase flow in three dimensional fractured vuggy porous media to release the huge computational cost for the fully discretized fractures especially in field scale simulations credit authorship contribution statement luyu wang conceptualization methodology software funding acquisition writing original draft fabrice golfier funding acquisition writing review editing anne julie tinet writing review editing weizhong chen funding acquisition writing review editing cornelis vuik methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was financially supported by the project of eurad european joint programme on radioactive waste management no 847593 and national natural science foundation of china no 51991392 luyu wang gratefully acknowledges that china scholarship council no 201904910310 partially supported this study appendix a upwind scheme for cell to cell connections an unified formulation of upwind scheme for different cells is applied but there are several slight differences in matrix and fracture cells following the discussion in section 3 2 for ω i m or ω i c as illustrated in fig a 29 the upwind term reads a 1 ω i m c u α n 1 d v j k m n i n e i g f u i n 1 a i where is the index of neighbors of element ω i m c the number of its neighbors is n i n e i g a i is the area of interface between elements ω i m c and ω m c in which the velocity placed at this interface is u i for ω i f it reads a 2 ω i f u α n 1 d v j k m n i n e i g f u i n 1 z i when the flux connection is fracture fracture z i a i f when it is matrix fracture z i l i f all notations are defined in section 3 2 both terms f u i in eqs a 1 and a 2 also eqs 9 and 10 are determined by the velocity direction or flux based on the so called upwind scheme as illustrated in fig a 29 for fracture elements a 3 f u i f i u i if flux ω i f ω f f u i if flux ω f ω i f it still validates for matrix and cavity elements appendix b formula of the preconditioner as discussed in section 3 2 the preconditioned jacobian j is derived by a preconditioner p b 1 j p j a 1 0 0 0 b 1 0 0 0 c 1 j m m j m f j m c j f m j f f j f c j c m j c f j c c with the sub matrices a b and c they are defined by the matrix norm of jacobian for convenience we denote each row of j as sub matrices d e and f such that j d e f t the sub block a in p is written as b 2 a j 1 n c d d 1 j 2 j 1 n c d d 2 j 2 j 1 n c d d n r d j 2 with the numbers of columns n c d and rows n r d of d similarly b and c can be expanded following the same way this preconditioner is easy to implement and flexible to integrate into a solver appendix c components of equivalent permeability tensor following the description and notations in section 4 1 the components of permeability tensor are expressed by c 1 k x x μ q x l x δ p l y k x y μ q y l x δ p l x k y y μ q y l y δ p l x k y x μ q x l y δ p l y the components forms of relative permeability tensor are expressed by c 2 k r α x x μ α q α x l x k x x δ p l y k r α x y μ α q α y l x k x y δ p l x k r α y y μ α q α y l y k y y δ p l x k r α y x μ α q α x l y k y x δ p l y where δ p is determined by boundary conditions l x and l y are the width and height of the upscaled block other notations are defined in the preceding sections 
171,this work investigates numerical method and equivalent continuum approach eca of fluid flow in fractured porous media the commonly used discrete fracture model dfm without upscaling needs full discretization of all fractures it enjoys the merit of capturing each fracture accurately but will get in trouble with mesh partition and low computational efficiency especially when a complex geometry is involved in this study we develop an efficient implicit scheme with adaptive iteration in which an improved eca is devised and then integrated in this scheme numerical studies show that the proposed numerical scheme improves the convergence condition and computational efficiency then a test is conducted to demonstrate the feasibility of using superposition principle of permeability tensor in upscaling based on these different strategies are applied to simulate fluid flow in fracture networks with a complex geometry it is demonstrated that the proposed eca is able to reproduce the results computed by dfm the accuracy depends on resolution of background grids the presented method enjoys a low computational cost and desirable convergence performance compared with the standard dfm in which equivalent continuum is not considered keywords fractured porous media two phase flow upscaling equivalent permeability tensor implicit scheme 1 introduction previous studies have revealed that the presence of discrete fractures has significant influence on characteristics of fractured porous media berkowitz 2002 adler et al 2013 wang et al 2020 however modeling of flow and transport in fractured porous media raises numerical challenges in geoscience applications cotta et al 2020 medici et al 2021 wang et al 2022 in the community of geological modeling there are two main categories of the computational model namely the discrete fracture model dfm aliouache et al 2019 wang et al 2022 and the equivalent continuum model kottwitz et al 2021 lasseux et al 2021 this work intends to study the compromise between the accuracy of dfm and the convenience of equivalent continuum model we focus on two phase flow in fractured porous media and discuss the extension to a more general case namely fractured vuggy porous media which has gained much attention in recent years yan et al 2019 mohammed et al 2021 xu et al 2022 there are two main groups of numerical method dealing with the interaction between fractures and matrix namely conformal and non conformal mesh schemes the classical dfm is a widely used conformal mesh scheme dfm was presented to explicitly reproduce stochastic fractures based on field investigation normally the fractures are simulated by the low dimensional elements lying on the interface of matrix elements wang et al 2022 it enjoys several merits such as simulating each fracture accurately and a desirable flexibility to directly borrow the numerical discretizations from finite element and finite volume methods gläser et al 2017 wang et al 2019 2022 however it may encounter some difficulties of complicated mesh strategy required by the conformal grids and an expensive cost if a complex geometry is considered bahrainian et al 2015 fourno et al 2019 xie and edwards 2021 indeed the presence of fractures and vugs may introduce a complex topology of the geometry which reduces the quality of convergence condition and then leads to unnecessary cost to this end the non conformal mesh scheme and the equivalent continuum model were introduced as two representative methods to simplify the connection of fractures matrix ţene et al 2017 azizmohammadi and sedaghat 2020 lasseux et al 2021 these methods try to find a balance and compromise between accuracy and efficiency but none can be done without drawbacks in the scope of non conformal mesh edfm embedded dfm is developed as an alternative version of traditional dfm zhang et al 2017 hajibeygi et al 2020 in the last decade other non conformal methods have been proposed typically the extended finite element method ren et al 2018 phase field method lee et al 2018 and lagrangian multipliers schädle et al 2019 a transfer function is introduced to capture the connection between the embedded fractures and the surrounding matrix nevertheless accuracy of non conformal mesh schemes is highly relying on the interpolation manners on fractures and the flux transfer function it is a trade off between accuracy and the complicated conformal meshing in contrast dfm based methods allow directly employing the traditional numerical discretization schemes such that the accuracy can be ensured gläser et al 2017 tan et al 2021 wang et al 2022 the hierarchical modeling technique based on the concept of equivalent continuum model was introduced to upscale small size fractures and retaining the relative larger size fractures lee et al 2001 yan et al 2021 lasseux et al 2021 in this way the computational efficiency improves a lot but the accuracy highly depends on the grid resolution of the upscaled models in general the methods of calculating equivalent permeability tensor of the equivalent continuum model can be categorized into two main groups the first group is based on the analytical methods pioneered by snow 1969 and oda 1985 later there are some other analytical methods have been developed for instance critical path analysis adeyemi et al 2022 homogenization mezhoud et al 2020 and effective medium theory hosseini et al 2021 their application is restricted due to limitations of implementation and simplified assumptions the second group is flow based upscaling by numerical simulation kottwitz et al 2021 lasseux et al 2021 it has been widely used in reservoir engineering and geoscience applications its merit is to be applicable for complex geometry the presented work is based on the second strategy despite being vital importance there are still some open issues needed to be addressed one key point is the criterion of upscaling although the equivalent continuum model enjoys some conveniences in effortless meshing and acceptable computational cost azizmohammadi and sedaghat 2020 kottwitz et al 2021 lasseux et al 2021 it is sophisticated to determine which fractures should be upscaled or retained besides the grid resolution of the upscaled sub blocks has great impact on numerical simulation therefore it is worthwhile to study the impact of resolution on accuracy on the other hand the expensive computation cost of the dfm reduces the efficiency of simulation especially if the presence of multiple fractures is considered to solve this in this work we combine the advantages of the accuracy of dfm and the efficiency of equivalent continuum model an improved equivalent continuum approach is presented to reduce the complexity of fractured media in addition we introduce the superposition principle of equivalent permeability tensor and integrating it into the upscaling scheme at this point some researchers have developed the superposition principle in fractured media lei et al 2015 xu and yang 2020 however their discussions are restricted to single phase flow in fractures with regular patterns particularly upscaling of two phase flow in fracture vuggy network is rarely studied in the existing literature golfier et al 2015 yan et al 2019 wang et al 2022 in this work we investigate numerical method and equivalent continuum approach of fractured media modeling approach of fractured vuggy porous media and the fully upscaling strategy are presented section 2 provides the mathematical formulation of two phase flow in fractured vuggy porous media section 3 presents an efficient implicit scheme with adaptive iteration based on the hybrid dimensional modeling section 4 proposes an equivalent continuum approach as well as the treatment regarding upscaling of a complex geometry a general form of the superposition principle is introduced to evaluate equivalent permeability tensor in the equivalent model in section 5 a series of numerical tests is conducted to study the performance of the proposed scheme 2 mathematical model in this section formulation of the fractured media with complex geometrical structures is presented a complex fractured medium is decomposed into several basic components then the governing equations of two phase flow in fractured media are given for completeness 2 1 physical domain of fractured vuggy porous media assuming a domain ω consists of three sub domains with the matrix ω m fractures ω f i n f ω i f and cavities ω c i n c ω i c as displayed in fig 1 the numbers of fracture and cavity distributed in this domain are n f and n c respectively furthermore ω is confined by boundary γ including external and internal boundaries denoted γ e x γ e x n γ e x d and γ i n γ i n n γ i n d note that the subscripts n and d are neumann and dirichlet types respectively thus the system of a porous medium containing stochastic cavities and fractures is defined as 1 ω ω m ω f ω c γ γ e x γ i n it is worth mentioning that the state of ω c can be varied such as filled by geological materials ω f i l l e d c or empty ω e m p t y c in the former case the permeability of ω f i l l e d c is allowed to be assigned to different values compared to the rock matrix ω m in the later case ω e m p t y c is regarded as the sink or source in this work our approach allows the consideration of these states hereafter we will assume that the role of the empty cavity ω e m p t y c can be specified as an internal boundary γ i n imposed on the edge of it ω e m p t y c if a dirichlet type boundary condition pressure p is applied it reads 2 p p on ω e m p t y c 2 2 governing equations for two phase flow the formulation for two incompressible and immiscible fluids in porous media is given as follows for completeness the mass conservation for wetting phase w and non wetting phase n is described by 3 ϕ s α t u α f α s α q α w n where s α is saturation and s n s w 1 ϕ is porosity q volumetric flux the frictional flow and phase velocity are represented by f α s α and u α respectively aziz 1979 hoteit and firoozabadi 2008 the phase velocity is determined by darcy s law pickup and serbie 1996 kolditz et al 2012 the relative permeability k r α s α is involved as a function of s α 4 u α k r α s α μ α 1 k p α ρ α g z α w n where k is the absolute permeability tensor μ α and ρ α are the viscosity and density of phase α g and z denote the gravity acceleration and depth respectively the two pressures are related by the capillary pressure p c s w p n p w we refer to hoteit and firoozabadi 2008 for the detailed formulation for convenience phase mobility λ α s α is defined by k r α s α μ α 1 total mobility is λ t λ w s w λ n s n therefore f α s α λ α s α λ t the total velocity is calculated by the summation of phase velocities 5 u k λ t p n λ w p c λ w ρ w λ n ρ n g z initial and boundary conditions are required to confine the behavior on boundary and at the initial time t 0 6 s w s w on ω m ω f ω c t 0 p w p on γ e x d t e or ω e m p t y c t e λ t k p n 0 on γ e x n t e or ω e m p t y c t e where t e t 0 t n is the entire time period n the outward unit normal vector to γ s w and p are the prescribed saturation and pressure respectively combining eqs 3 4 and 6 a well posed system of partial differential equations pdes is constructed to capture two phase flow in a fractured vuggy porous medium 3 numerical method in this section the numerical method based on galerkin finite element method is presented to discretize the governing equations eqs 3 and 5 an efficient implicit scheme with adaptive iteration is devised to efficiently solve the system of coupled pdes 3 1 hybrid dimensional mesh partition the hybrid dimensional elements are introduced to partition the fractures and matrix separately tan et al 2021 wang et al 2022 we employed the commonly used conformal mesh where fracture elements ω f are lying along the edges of each pair of matrix elements ω m as displayed in fig 2 an arbitrary pair of matrix elements is denoted by ω i m ω j m the intersection set operation of them is a fracture element lying between them ω i m ω j m ω i j f the open source software triangle shewchuk 2014 is used to generate grids on the matrix ω m i 1 n e m ω i m and cavities ω c i 1 n e c ω i c the filled and empty cavities are treated separately and only filled cavities are meshed then the fracture elements are generated based on the connectivity of the matrix and cavity cells such that ω f i 1 n e f ω i f numbers of the matrix cavity and fracture elements are n e m n e c and n e f 3 2 formulation of the implicit scheme with adaptive iteration to discretize the transport equation eq 3 the temporal integral is taken over a time increment δ t and implicit difference scheme is used to discrete time dependent term for phase α it reads 7 ω ϕ x δ t s α n 1 s α n d v time dependent term ω u α n 1 d v flux term ω f α s α q n 1 d v source term the transport equation and darcy s law construct a coupled system the frictional velocity u α is obtained by the darcy s law for multi phase flow mentioned in eq 4 in implementation it is commonly represented by the product u α f α u such that f α is determined by the relative permeability k r α s α thereafter u is directly represented by eq 5 we employ the implicit time discretization and substitute eq 5 into the following equation 8 ω u n 1 d v ω q n 1 d v eqs 7 and 8 construct a coupled system of pdes we use eq 8 to calculate pressure then the total velocity and frictional velocities can be obtained the saturation is calculated from eq 7 based on the updated velocities according to the discussion in section 3 1 the matrix and cavity elements are of higher dimensional compared to fracture elements consequently the residuals for each type are defined to solve the nonlinear system using implicit iterative scheme following the notations defined in section 3 1 for each ω i m or ω i c the residual is updated at each iterative step 9 r i m c n 1 ϕ x v i m c ξ i δ t s α i n 1 s α i n f α i s α i q i n 1 v i m c ω i m c u α n 1 d v upwind term for ω i m or ω i c where i 1 n e m or n e c note that ω i m c is either ω i m or ω i c for each ω i f i 1 n e f the residual is written as 10 r i f n 1 ϕ x l i f a i f ξ i δ t s α i n 1 s α i n f α i s α i q i n 1 l i f a i f ω i f u α n 1 d v upwind term for ω i f where ω i represents an integral over the element v i m c is the volume of ω i m or ω i c l i f and a i f are length and aperture of ω i f the shape function ξ equals 1 if a cell centered scheme is applied the third term in right hand side of above equations is determined by the upwind scheme respective to different elements the expanded forms are given in appendix a the iteration will be terminated once a criterion is satisfied which is given by r n 1 2 ε the tolerance ε is user defined normally at a magnitude of 1 0 6 usually the time step δ t is a fixed value defined in the input however this preference sometimes induces that the system has difficulty to converge and reduces computational efficiency to solve this issue the adaptive iteration is devised to improve convergence condition such that δ t can be changed dynamically based on the condition of residual as displayed in fig 3 three values are introduced r c r 1 and r 2 the new time increment denoted by δ t depends on the condition evaluated by the three values 11 δ t r 1 δ t if r n 1 2 r c ε r 2 δ t if r n 1 2 ε δ t otherwise note that 0 r c 1 0 r 2 1 and r 1 1 eq 11 is evaluated at the end of each iteration at each iteration eqs 9 and 10 are calculated over all element until i n e such that a residual vector is constructed r r m r f r c t note that n e n e m n e c n e f then newton raphson approach is employed to obtain the updated saturation at each iteration 12 j m m j m f j m c j f m j f f j f c j c m j c f j c c ν j ν δ s m δ s f δ s c ν 1 δ s ν 1 r m r f r c ν r ν where components of jacobian j are calculated by j r s according to eqs 9 and 10 note that the subscripts m f c represent matrix fracture and cavity consequently the saturation in previous iteration s ν is updated by s ν 1 s ν δ s ν 1 ν is the iterative number the high contrast of permeabilities on fractures and matrix would lead to an ill conditioned jacobian j with a high condition number which would bring an inaccurate result to this end a preconditioning technique is proposed to better constrain the solution the preconditioned jacobian j is derived by a preconditioner p therefore the expanded forms of p can be presented it is easy to implement which is given in appendix b the operation eq b 1 provides an efficient and simple way to improve numerical quality of j 4 an equivalent continuum approach in practice the upscaling approach is usually used to simplify the complex fractured network and field heterogeneity consequently the discrete fractures are upscaled into many sub blocks with equivalent parameters in this section an equivalent continuum approach is devised based on the flow based upscaling technique superposition principle is introduced to compute permeability tensor then a treatment of complex geometries is addressed 4 1 upscaling for fluid flow in fractured porous media the results of upscaling should be satisfied with the equation of the original problem for convenience the capillary pressure is assumed to be zero substituting the calculated solution into the original equation it reads 13 q μ 1 k p where k is the equivalent permeability tensor p is the new solution calculated based on upscaled parameter it means that p should be close to the pressure in the original problem p chen et al 2015 the component form of k is 14 k k x x k x y k y x k y y the expanded forms given in appendix c are solved by the flow based approach to calculate k x x and k x y as shown in fig 4 a pair of constant pressures p l and p r are applied on the left and right hand sides while the linear pressures p l 1 x l x x p r l x are applied on the top and bottom sides therefore one can obtain the fluxes q x and q y here we use the calculation method proposed by chen et al 2015 in which the so called multiple boundary upscaling is used to calculate the fluxes if the inlet is placed on the left boundary l as illustrated in fig 4 we have 15 q x l l y v r n 1 d y l x v t n 1 d x l x v b n 1 d x q y l l x v t n 2 d x l x v b n 2 d x l y v r n 2 d y where v is the flow rate respective to top t bottom b and right r boundaries n 1 and n 2 are the unit normal vectors along vertical and horizontal axes therefore k x x and k x y can be computed the fluxes are computed for other boundaries in a similar way then k y y and k y x can be computed however the fluxes are not identical at the two parallel boundaries for a fractured medium to this end we introduce the treatment proposed by chen et al 2008 eq 15 is evaluated twice respectively to left and right sides denoted by q x l and q x r consequently q x q x l q x r 2 the same treatment is applied on top and bottom sides q y q y t q y b 2 the calculation of the upscaled permeability tensor is straightforward and derived from darcy s law sometimes the upscaled k is asymmetric the off diagonal components are set to k x y k y x 2 as advised by durlofsky 2005 and chen et al 2015 for a two phase flow we refer to the works by jackson et al 2018 and benham et al 2021 the equivalent relative permeability is defined as 16 k r α s α u α μ α l k 0 δ p α where is a type of spatial averaging δ p α is the pressure drop between inlet and outlet l is the distance between inlet and outlet k 0 is a typical dimensional scaling which is selected as a mean permeability in practice this expression is adapted from jackson et al 2018 benham et al 2021 used it to upscale fluid flow in a porous medium however this formula is usually involved in analytical studies and inconvenient to implement furthermore it is worth noting that rigorously the relative permeability is a tensor k r α pickup and serbie 1996 which is absent in eq 16 in general darcy s law for multi phase reads u α μ α 1 k r α k p to do this a result from matthai and nick 2009 is introduced in our work they provided a method to upscale k r α according to the upscaled k here we give the component forms as shown in appendix c 4 2 superposition principle of equivalent permeability tensor a superposition principle of equivalent permeability tensor is integrated in this work for upscaling discontinuities assuming a matrix block intersected completely by several fractures as shown in fig 5 each component of the equivalent tensor is equal to the summation contributed by these fractures respective to this component alternatively if we wish to calculate k of a complex pattern namely k one can simply conduct a summation operation based on the known basic patterns these basic patterns are constructed by single fracture cutting the porous rock with different geometrical parameters as illustrated in fig 5 k 1 of a simple pattern consists of the contributions from the rock matrix and the single fracture namely the superposition of them k 1 k m k f 1 note that k m is obtained in the case without fractures this idea can be extended to a more general case where multi fractures n f are considered 17 k i n f k i n f 1 k m to show how it works fig 5 provides an example when n f 2 an underlying assumption to this principle is that the interaction between fractures is neglected it produces an acceptable error compared to the accurate solution note that this principle has been used in fractured reservoir simulation zhang et al 1996 lei et al 2015 xu and yang 2020 the validation of this principle will be elaborated through numerical tests in section 5 4 3 treatment of complex configuration the preceding content discusses the treatment to a dual fracture pore medium where the cavities are absent for a fractured vuggy porous medium the presence of cavities would impose significantly to hydraulic property of this medium huang et al 2011 golfier et al 2015 yan et al 2019 in this section the treatment of a complex pattern with the fracture cavity networks is presented then it will be employed in section 5 for numerical study it is worthwhile to note that the superposition principle proposed in section 4 2 can be used in the upscaling procedure fig 6 shows a domain with complex geometry it is partitioned using a physical decomposition background grids and then upscaled by the flow based equivalent continuum method the pattern of physical decomposition is determined by the geometrical configuration in which the distribution of cavities and fractures is an important factor the cavities with a periodic pattern are upscaled hydraulically as solid squares by yan et al 2019 which is reliable when the cavities are filled by geological materials however as discussed in section 2 1 and eq 2 in some cases a cavity can be treated as a region of constant pressure in this case the upscaling of cavity which plays the role of an internal dirichlet boundary should be addressed as shown in fig 7 consequently this special situation is considered by imposing an internal pressure at the cavity walls the cavities are upscaled as several sub blocks for the convenience of meshing in the equivalent model and can reduces geometrical complexity each cavity covers several background grids which consist of an influenced area as depicted in fig 6 we follow the notations defined in section 2 fig 7 provides a novel treatment if a boundary condition p p is imposed on edge ω e m p t y c obviously grid resolution l g has great impact on upscaling as displayed in fig 6 in many remarkable works the large size fractures are retained since they have influence on flow lee et al 2001 li and lee 2008 it is termed as hierarchical upscaling in this paper different size fractures are classified by a ratio l r l f l g then the upscaling procedure is performed based on different types of fracture as illustrated in fig 8 an alternative scheme is applied for the sake of convenient implementation chen et al 2015 yan et al 2019 azizmohammadi and sedaghat 2020 the matrix and all fractures are fully discretized by the background grids it is termed as fully upscaling in this paper notably chen et al 2015 upscaled the large size fractures by partitioning them into several sub blocks which are intersected completely by fractures here as displayed in fig 8 we present these two different treatments regarding the large size fractures 5 numerical tests and discussion in this section the proposed efficient implicit scheme and equivalent continuum approach are synthetically combined to simulate flow in fractured media it should be noted that the capillary pressure is neglected for validation karimi fard et al 2004 moinfar et al 2014 jiang and tchelepi 2019 the superposition principle presented in section 4 2 is integrated into the upscaling procedure then fluid flow in a fractured medium with a complex geometry is studied using the approach introduced in section 4 3 note that the simulations are performed based on intel core i7 7950 processor at 2 6 ghz with 16 gb memory 5 1 numerical verification and convergence test the benchmark is extracted from a publication by karimi fard et al 2004 following the same setting in this literature parameters of this model are set to porosity ϕ 0 2 permeability of matrix k m 0 99 1 0 15 m 2 and fracture k f 8 33 1 0 10 m 2 such that the fractures are modeled as a conductive channel the coordinates of each fracture refer to karimi fard et al 2004 the result simulated by dfm as the reference solution is used to compare the result computed by the proposed numerical scheme fluid is injected at the bottom left corner of the model and the outlet is located at the top right corner as shown in fig 9 this figure depicts several stages of saturation during evolution at different pvi injected pore volume injection rate is 0 01 pvi d it appears that the result agrees well with the reference solution to test performance of the presented method under different conditions we define a ratio k r k f k m to measure the permeabilities of matrix and fractures as reported in literature the high contrast of permeability would induce an instability of simulators ţene et al 2017 the ratio k r is set to 1 0 5 and 10 5 to simulate two extreme situations note that k m 1 md the former case describes fractures as the conductive channels while the later case represents the fractures as the barriers because of the contrast of k f and k m the jacobian becomes ill conditioned which is improved by the preconditioner p proposed in section 3 2 furthermore the adaptive iterative scheme is applied to improve the convergence quality fig 10 displays a set of curves to compare the grid convergence with different k r it shows that k r has great impact on the convergence performance the convergence condition is gradually improved with a decreased k r obviously this test proves that the presented numerical scheme has a good convergence performance when the high contrast of permeability is involved another test is performed to evaluate performance of the proposed adaptive iterative scheme in section 3 2 to this end computations by the improved scheme and classical scheme are carried out the former scheme enjoys the advantage of adaptive iteration while the later one uses a fixed time step during the entire iterative process fig 11 shows a comparison of convergence performance during iteration using improved scheme and classical scheme with different k r also the total number of newton iterations of both schemes is given it appears that the presented method is able to improve the convergence condition and reduces the total time to study the grid convergence performance of this method the model as shown in fig 9 is simulated with different local grid refinements the maximum and minimum values of the grid resolution are denoted by h m a x and h m i n respectively then h m i n is allowed to be changed at a fixed h m a x as displayed in fig 12 the error ε h is defined by the normalized difference between the reference solution and the results calculated using different local refinements fig 13 provides the grid convergence with the refinement at a fixed h m a x it proves that the presented method has good performance of the grid convergence 5 2 study on superposition principle of equivalent tensor in section 4 2 the superposition principle is proposed for equivalent permeability tensor ept based on the flow based upscaling here we analyze this principle using the presented numerical scheme we use the dimensionless parameters the size of the model is 1 1 the permeability of the fractures and matrix are k f 1 0 5 and k m 1 the inlet is placed on left boundary p l 1 while the outlet is placed on right boundary p r 0 fig 5 provides an illustration of a fractured medium with a simple pattern to study the superposition principle ept of several basic patterns should be calculated firstly as displayed in fig 14 then a complex pattern can be easily obtained using these basic patterns at this point we use eq 17 note that when n f 2 the superposition computation should be operated carefully to explain it fig 14 shows a case with n f 3 where the ept of a complex pattern is calculated by k 123 i 1 3 k i 2 k m it is worthwhile to notice that k m should be subtracted twice which equals to factor n f 1 in eq 17 a comparison between results of the fluid flow simulation ffs and superposition principle sp is depicted in fig 14 the permeability tensors of the complex patterns are obtained from ept of basic patterns error of each pattern is computed by k i j k ˆ i j k i j where the symbols ˆ and represent sp and ffs respectively it shows that the error is very small therefore the assumption of sp is validated in this test the source of error is probably induced by the numerical discretization and grid resolution 5 3 flow in porous media with a fracture network to demonstrate the performance of the proposed method a porous medium with a fracture network is simulated this fracture network contains different size fractures the approach discussed in section 4 is employed as shown in fig 15 the background grids with shade are the covered grids of fractures obviously the resolution l g has great impact on the mesh pattern as well as the simulated results in this stage l g is determined by the fracture which has the smallest size the smallest size is l m i n f therefore l g l m i n f ω m e s h with a weight factor ω m e s h herein we present two different l g to show the effect of grid resolution on mesh patterns and simulation results as shown fig 15 the minimum length of fractures is 0 2 then the grid resolution l g is varied with the weight factor ω m e s h then l g 0 05 if ω m e s h 4 and l g 0 1 if ω m e s h 2 the inlet and outlet are p l 1 mpa and p r 0 on the left and right surfaces respectively the physical properties of two phase fluid are shown in table 1 note that the relative permeabilities follow the corey type model brooks and corey 1964 18 k r w s n n r k r n 1 s n n r where n r is the exponent of the corey type model the normalized saturation s n is defined in table 1 an obvious difference between the dfm and upscaling approach is that the upscaled model might fail to describe the sharp discontinuous pressure jump around the fractures as illustrated in the left hand side picture in fig 17 but their global distribution patterns agree well each other fig 16 shows the meshing strategy of different models as shown in this figure the unstructured grids are applied in the situation without upscaling the dfm where all fractures are retained and modeled explicitly if upscaling is applied these fractures are implicitly embedded inside the background grids by the equivalent continuum approach the solid lines are explicitly modeled fractures while the dashed lines are implicitly expressed the pressures distribution calculated by different models is displayed in fig 17 it demonstrates that the presence of fractures influences pressure field a significant pressure gradient is observed around the fractures it shows that the fully upscaling with different resolutions is able to reproduce this phenomenon note that the solution calculated by the hierarchical upscaling is very closed to that of the dfm to compare the results by different models a survey line is placed along the off diagonal crossing this domain this line cuts several fractures through its path as shown in fig 18 although the curve calculated by upscaling with l g 0 05 has a slight oscillation the global tendency agrees well with the curve of without upscaling however the accuracy is reduced if a coarse resolution for example l g 0 1 is employed this test proves that the simulated result is obviously impacted by resolution as pointed out in some studies karimi fard et al 2004 lee et al 2001 computation using dfm is more accurate than using upscaling approach but it needs to simulate all fractures explicitly and leads to an expensive cost compared to dfm the advantage of the equivalent continuum model is the use of structured grids instead of unstructured grids and the fractures are modeled implicitly as displayed in fig 16 to show their difference in aspect of convergence performance fig 18 provides a comparison of convergence history measured by l 2 norm of residual of these two different schemes it explains that the upscaling approach improves the performance of convergence compared with the dfm fig 20 displays saturation evolution simulated by different models to show the effect of fractures on hydraulic feature a high contrast permeability ratio is set to k r k f k m 1 0 5 it is observed that the fracture network plays a role of barrier to block fluid flow consequently the fluid accumulates at top and bottom sides since the fractures block the movement it shows that a fine resolution l g 0 05 in this case would lead to a more accurate result compared to a coarse resolution fig 19 displays a set of curves of the cumulative production versus pore volume injection the contrast of permeability between the fractures and the rock matrix is reflected by k r and it has significant influences on accuracy the high conductivity of fractures would lead to a relative high production compared to the low conductivity 5 4 a complex geometry a test is performed with a complex geometry to study fluid flow in a more complex porous medium this model is the so called fractured vuggy porous medium found in literature huang et al 2011 yan et al 2019 an essential difference between a cavity and a fracture is the spatial extension it results in different treatments of discretization for these two objects we follow the scheme proposed in section 4 3 to model the fractured media with complex geometry a fractured vuggy porous medium is shown in fig 21 the parameters are shown in table 1 note that the permeability of the filled cavity is set to 1 1 0 13 m 2 the dimensions in x and y axes are both 100 m in this simulation the filled and empty cavities are considered it reproduces a typical situation in carbonate reservoir engineering huang et al 2011 wei et al 2020 wang et al 2022 in which the connection of fractures and cavities constructs a connected channel system the inlet condition is imposed on the edge of a cavity with pressure p i n 1 mpa while the outlet is placed on the edge of another cavity p o u t 1 mpa these two cavities are connected by fracture networks two approaches dfm without upscaling and upscaling are applied to simulate fluid flow thus two different mesh strategies are adapted as shown in fig 21 the gray cells are embedded fracture cavity zones which need to be upscaled fig 22 provides saturation evolution of the porous medium as well as that of the fracture networks pressure distributions with respect to three different models are given in fig 23 it appears that discrete fracture model provides an accurate result but it requires explicitly simulation of all fractures the upscaling approach depicts similar result and the accuracy is increased with a finer resolution l g 0 025 is better than l g 0 05 a survey line is placed along the off diagonal of the domain pressure variation along this line is displayed in fig 24 correspondingly the relation of number of active grid cells versus pore volume injection is shown in fig 24 it demonstrates that the proposed equivalent continuum approach provides an efficient tool to model fluid flow in complex fractured media in a simple way the results simulated by equivalent continuum approach are shown in fig 26 different resolutions lead to different computational steps fig 25 provides a comparison analysis cell numbers of l g 0 1 0 05 0 025 and 0 0125 are 100 400 1600 and 6400 respectively correspondingly we select cell number n 108 398 1636 and 6423 for dfm on unstructured grids it shows that the total number of newton iterations n t o t a l of upscaling scheme is relatively smaller than that of dfm the adaptive time at reduces the computation steps compared to fixed time ft a test is analyzed with a more strict convergence criterion the l 2 norm of residual is 10 15 and initial δ t 1 0 3 1 0 4 the adaptive iteration enforces the time step more smaller compared to the previous iterative step as shown in fig 27 the error calculated by different resolutions l g is provided in this figure it is measured by the relative difference between the solutions of dfm and upscaling model s d f m s u p s d f m it shows that the error is relative small if a fine l g is applied the maximum error calculated based on an extremely fine resolution for example l g 0 0125 is 4 4 however it would produce an expensive computation cost which is greater than 1000 newton iteration number as shown in fig 25 in contrast if a more coarser grid resolution is applied for example l g 0 025 the error is acceptable but the cost is highly reduced to around 600 iterations it is more efficient than the finer grids thus finding a balance between accuracy and computational cost is a key point in practice the relation of cumulative production versus injection is shown in fig 28 the inlet and outlet are both placed on cavities it illustrates that the heterogeneity of permeability k r influences the production in this case the use of l g 0 025 in equivalent continuum model is sufficiently to recover the results of dfm 6 conclusions in this work an efficient implicit scheme with adaptive iteration is developed to simulate fluid flow in the fractured media with complex geometries an equivalent continuum approach is devised and then integrated in the simulator combining with the superposition principle of equivalent permeability tensor in the numerical scheme a preconditioner is proposed to improve the condition of jacobian and an adaptive iterative strategy is used to improve the convergence performance to model the fractured medium with a complex geometry especially the fracture vuggy network we introduce a novel treatment to upscale cavities which allows a cavity either filled or empty this scheme is able to simulate the inlet and outlet placed on the edge of cavity numerical tests are performed to verify the presented numerical scheme and the equivalent approach validation and grid convergence performance are evaluated through a benchmark study where different local grid refinements are considered a comparison study is conducted to show the merits of adaptive iteration to valid the feasibility of using superposition principle in upscaling a set of tests is carried out to demonstrate it then based on this we simulate fluid flow in a fractured vuggy porous medium different modeling strategies are employed to analyze and compare their performances it appears that the proposed equivalent continuum approach is able to reproduce the results computed by the dfm the accuracy depends on resolution of the background grids this method enjoys a low computational cost and good convergence performance compared with the dfm using fully discretized fractures several extensions of the presented approach deserve a further investigation it can be applied to a more general case such as two phase flow in three dimensional fractured vuggy porous media to release the huge computational cost for the fully discretized fractures especially in field scale simulations credit authorship contribution statement luyu wang conceptualization methodology software funding acquisition writing original draft fabrice golfier funding acquisition writing review editing anne julie tinet writing review editing weizhong chen funding acquisition writing review editing cornelis vuik methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was financially supported by the project of eurad european joint programme on radioactive waste management no 847593 and national natural science foundation of china no 51991392 luyu wang gratefully acknowledges that china scholarship council no 201904910310 partially supported this study appendix a upwind scheme for cell to cell connections an unified formulation of upwind scheme for different cells is applied but there are several slight differences in matrix and fracture cells following the discussion in section 3 2 for ω i m or ω i c as illustrated in fig a 29 the upwind term reads a 1 ω i m c u α n 1 d v j k m n i n e i g f u i n 1 a i where is the index of neighbors of element ω i m c the number of its neighbors is n i n e i g a i is the area of interface between elements ω i m c and ω m c in which the velocity placed at this interface is u i for ω i f it reads a 2 ω i f u α n 1 d v j k m n i n e i g f u i n 1 z i when the flux connection is fracture fracture z i a i f when it is matrix fracture z i l i f all notations are defined in section 3 2 both terms f u i in eqs a 1 and a 2 also eqs 9 and 10 are determined by the velocity direction or flux based on the so called upwind scheme as illustrated in fig a 29 for fracture elements a 3 f u i f i u i if flux ω i f ω f f u i if flux ω f ω i f it still validates for matrix and cavity elements appendix b formula of the preconditioner as discussed in section 3 2 the preconditioned jacobian j is derived by a preconditioner p b 1 j p j a 1 0 0 0 b 1 0 0 0 c 1 j m m j m f j m c j f m j f f j f c j c m j c f j c c with the sub matrices a b and c they are defined by the matrix norm of jacobian for convenience we denote each row of j as sub matrices d e and f such that j d e f t the sub block a in p is written as b 2 a j 1 n c d d 1 j 2 j 1 n c d d 2 j 2 j 1 n c d d n r d j 2 with the numbers of columns n c d and rows n r d of d similarly b and c can be expanded following the same way this preconditioner is easy to implement and flexible to integrate into a solver appendix c components of equivalent permeability tensor following the description and notations in section 4 1 the components of permeability tensor are expressed by c 1 k x x μ q x l x δ p l y k x y μ q y l x δ p l x k y y μ q y l y δ p l x k y x μ q x l y δ p l y the components forms of relative permeability tensor are expressed by c 2 k r α x x μ α q α x l x k x x δ p l y k r α x y μ α q α y l x k x y δ p l x k r α y y μ α q α y l y k y y δ p l x k r α y x μ α q α x l y k y x δ p l y where δ p is determined by boundary conditions l x and l y are the width and height of the upscaled block other notations are defined in the preceding sections 
172,surface ponding is often used in combination with subsurface drainage systems to leach excessive salts from the saline soils of reclaimed coastal areas however soils that are not immediately proximal to subsurface drains are typically subjected to reduced leaching efficiency le this study evaluates an approach for increasing the le through the use of a low permeability surface mulch lpsm above the drain sand tank experiments and 2d numerical simulations were conducted to examine the effectiveness of lpsm configurations under continuous surface flooding the results show that the addition of a lpsm leads to more spatially uniform accelerated salt leaching towards subsurface drains as a result base case leaching periods and freshwater application rates were reduced by up to 60 and 80 respectively the sensitivity analysis based on field scale models revealed optimal values i e leading to maximum le of the lpsm hydraulic conductivity and length moreover an approximate method is applied to estimate the leaching period and to thereby assist in the design of drainage lpsm schemes we conclude that the addition of a lpsm is an effective strategy to enhance salt leaching under surface ponding adding to existing methods for ameliorating saline soils keywords soil management salinity sand tank experiment analytical solution 1 introduction coastal areas are being reclaimed in many parts of the world due mostly to the need to relieve population pressures martín antón et al 2016 sengupta et al 2018 however reclaimed coastal soils commonly require amelioration of high soil salinities li et al 2014 xiao et al 2019 which arise from prior inundation with seawater yang et al 2013 yu et al 2016 high salinity levels within the natural soil may induce various adverse effects on the habitats of soil microorganisms and the growth of crops leading to decreased agricultural production and disturbances to nutrient and biological cycling that otherwise benefit soil health eventually evolving into a sociocultural and environmental issue brevik et al 2015 pezeshki 2001 rengasamy 2006 thus lowering the soil salinity leads to enhanced agricultural productivity and improved nutrient and biological cycling that benefit soil health pezeshki 2001 xie et al 2017 the amelioration of reclaimed saline soils is often accelerated through the installation of drainage systems especially where salt flushing from rainfall is inhibited by high evaporation rates poor soil drainage and or where shallow saline groundwater occurs barua and alam 2013 daliakopoulos et al 2016 letey et al 2011 drainage systems may also be used to lower groundwater levels thereby decreasing the potential for waterlogging and the secondary salinization caused by the evapo concentration of soil salts liu et al 2017 subsurface drainage systems are often preferred to drainage channels due to advantages of low land occupation and limited interference with farming operations afruzi et al 2014 mirlas et al 2013 they are typically designed not only to lower salinities in the root zone steppuhn et al 2005 but also to maintain the groundwater level to an adequate depth to prevent the upward movement of salty capillary water from reaching the root zone qadir et al 2000 in practice the subsurface drain is usually suggested to be installed at the 0 75 to 1 2 m depth for satisfying the above mentioned requirements kladivko et al 1999 the performance of subsurface drainage systems in reducing soil salinity is routinely enhanced through the application of freshwater to the soil surface to supplement the flushing caused by rainfall ritzema et al 2008 sarmah and tiwari 2018 however previous studies of the subsurface drain performance have revealed that under continuous surface ponding only salts within the near drain area may be effectively leached mirjat et al 2014 siyal et al 2010 youngs and leeds harrison 2000 this is attributed to decreased seepage rates potentially by orders of magnitude in reclaimed areas furthest from the drains e g midway between drains xin et al 2016 this leads to inefficient salt leaching in those soils requiring longer timeframes than are required to flush salts from near drain soils and thereby wasting the unnecessary amount of freshwater in the near drain soils where the salts have been flushed out earlier the high spatial variability in salt leaching rates of subsurface drainage systems may be mitigated if the downward flow of freshwater from flood irrigation is more spatially uniform several methods have been developed to diminish spatial disparities in salt leaching rates within subsurface drainage schemes for example rao and leeds harrison 1991 numerically demonstrated that progressive ponding instead of continuous surface ponding can improve the leaching efficiency le in progressive ponding the soil surface between two drains is divided into several parallel strips which are successively flooded starting from midway between drains to the near drain area until the whole area is flooded youngs and leeds harrison 2000 a disadvantage of progressive ponding is the need for additional infrastructure to control the location and timing of flooding mirjat et al 2008 investigated leaching rates under progressive ponding using laboratory experimentation which included streamline tracing under four surface ponding scenarios the experimental results showed that higher pore water velocities occurred in the region midway between the two drains under progressive ponding than complete ponding leading to more effective leaching siyal et al 2010 used numerical modeling to estimate time and water savings from progressive ponding compared to complete ponding for various soil textures including layered soils subsurface drain depths and soil depths the results indicated that water and time savings in excess of 90 can be achieved through progressive ponding although water savings were only 13 when applied to a layered loam over sand soil an alternative to progressive ponding is intermittent ponding which has also been shown to improve the le darzi naftchali et al 2018 sibai et al 1997 previous studies found that intermittent ponding decreased the rate of water bypass compared to continuous ponding which reduces the le because flow through macropores and other preferential pathways tends to carry limited salts relative to diffuse flow through the soil matrix haws et al 2004 vogel et al 2000 however some studies have found that intermittent ponding requires considerably longer timeframes to achieve the same salt leaching as continuous ponding and under some conditions intermittent ponding may not lead to water savings due to the water loss and salt accumulation caused by effects of evapotranspiration minhas and khosla 1986 qadir et al 2000 spatial disparities in salt leaching rates under surface ponding may also be reduced by altering the permeability distribution of the soil yu et al 2018 proposed to add barriers within the soil profile above subsurface drains to create more spatially uniform salt leaching rates a conceptual model of the yu et al 2018 scheme is illustrated in fig 1 a yu et al 2018 used numerical modeling to show that the barrier greatly altered the pattern of infiltration under continuous ponding leading to increased le in comparison to the unmodified soil profile the barrier acted to both reduce the surface influx near the drains and enhance the infiltration through soils furthest from the drains the current study aims to extend the previous investigation by yu et al 2018 of the effects of adding a barrier to subsurface drainage surface ponding systems by evaluating the situation where the barrier to flow is placed at the land surface the surface barrier is referred to herein as a low permeability surface mulch lpsm and its conceptual model is illustrated in fig 1b the lpsm has two advantages over a subsurface barrier 1 the condition of the lpsm can be assessed visually and its placement does not require soil disturbance and 2 the lpsm avoids restrictions to plant rooting depths and salt flushing of near surface soils that arise with barriers placed below the land surface the analysis of the subsurface drainage lpsm system proposed here includes both laboratory experiments and numerical modeling these are used to 1 examine salt leaching processes associated with the subsurface drainage lpsm system within a sandy soil under controlled laboratory conditions 2 quantify the effectiveness in terms of salt leaching rates of different lpsm implementations and 3 apply and evaluate an existing approximate method for estimating seepage rates across the soil surface that allows for rapid estimation of the timeframe required for salt leaching to occur with and without a lpsm 2 methodology 2 1 laboratory experiments a schematic of the experimental setup is illustrated in fig 2 two dimensional laboratory experiments were carried out in a 100 cm length 6 cm width 30 cm height sand tank in the front of the laboratory setup which is similar to that adopted by mirjat et al 2008 for investigating leaching under progressive flooding situations a subsurface drain spanning the width of the sand tank was placed at 20 cm above the base of the sand tank and 50 cm from the left boundary the drain consisted of a perforated stainless steel tube with an internal diameter of 5 mm the tube was wrapped with a nylon screen to prevent sand from entering it the drain was connected to the atmosphere at its outlet allowing water to flow freely out of the sand tank the sand tank was filled with sand to 25 cm from the base surface ponding was maintained through a constant head tank connected to the backside of the sand tank via a connecting plane note that the connecting plane can assist in providing a steady flow from the constant head tank to the sand tank inflow to the constant head tank occurred from a variable overflow device which was connected to the constant head tank through a 50 mm internal diameter tubing here the overflow outlet depth in the variable overflow device can determine the ponding depth which was set to 2 cm in all experiments two different sands were used in the experiments well sorted medium sand i e the ambient soil and well sorted fine sand i e the lpsm grain size distribution curves produced d 50 1 76 mm and d 60 d 10 1 83 for the medium sand and d 50 0 09 mm and d 60 d 10 1 65 for the fine sand here d 10 d 50 and d 60 are the grain sizes that 10 50 and 60 of samples are finer by weight respectively sand porosities were found to be θ 0 0 40 0 01 medium sand and θ m 0 44 0 01 fine sand by oven drying saturated sand samples these values were taken as effective porosities θ e in numerical simulations darcy column tests three tests each for the medium and fine sands were undertaken leading to ranges of saturated hydraulic conductivities of 2 14 10 3 3 01 10 3 m s 1 medium sand and 1 04 10 6 8 04 10 6 m s 1 fine sand the fine sand lpsm had a thickness of 1 cm and length of 50 cm and was placed symmetrically above the subsurface drain fig 2 the sand tank was wet packed using incremental filling to avoid trapped air bubbles in the sand this included both the medium sand and the lpsm note that the subsurface drain was closed by a valve during filling the tank with sand follow on the ponding depth was maintained at 2 cm by adjusting the overflow outlet depth after the sand tank had been filled with sand as a result the sands are consistently saturated during the sand filling stage the drain would be opened immediately when the experiments started running the 10 000 ppm mass fraction parts per million saltwater in laboratory experiments was produced by dissolving 1 kg of sodium chloride nacl into 99 kg of deionized water at 15 c the saltwater and freshwater densities were determined by a densimeter to be 1006 8 kg m 3 and 999 2 kg m 3 note that two scenarios were conducted to evaluate the effectiveness of the lpsm in which case s1 is defined as the case without the lpsm and case s2 is referred to the case with the lpsm a conservative dye tracer allura red ac was added to the saltwater to allow for leaching experiments to track saltwater distributions the apparatus was photographed every 5 sec during experiments using a digital camera canon ixus 175 the effluent discharging from the subsurface drain was collected using a measuring cylinder to determine discharge rates the effluent salinity was measured by a handheld multiparameter meter ysi professional plus tracer experiments used to depict streamlines were conducted for cases s1 and s2 with stained tracers to allow for improved streamline visualization note that the saltwater of 10 000 ppm within the medium sand was not dye stained in the tracer experiments the stained tracers were injected at the soil surface points a to d fig 2 at low fluxes 10 ml min 1 using a peristaltic pump 2 2 numerical modeling the finite element sutra code voss and provost 2008 was employed to simulate density dependent flow and solute transport within the subsurface the scenarios that were evaluated involved only saturated groundwater flow due to the situation of continuous surface ponding therefore only saturated flow was considered in sutra the relevant governing equations describing flow and solute transport in sutra are given in the user manual voss and provost 2008 and are not repeated here for brevity sutra has been validated extensively against benchmark problems involving density dependent flow and transport kuan et al 2012 shen et al 2015 the sutra model of the sand tank represented only the right hand half of the apparatus due to flow symmetry about the drain the subsurface drain was assumed to be fully filled with water thereby the nodes related to the drain were taken as seepage face nodes with the zero pressure head i e atmospheric pressure as suggested by xin et al 2016 boundary nodes representing surface ponding were also designated as specified head conditions adopting constant values that reflected the depth of ponding other boundaries were set to no flow conditions surface nodes were assigned specified concentration boundary conditions of zero salinity while the drain was designated a zero concentration gradient boundary condition the hydrostatic conditions with a zero hydraulic head and the uniform concentration distributions of 10 000 ppm were used for the initial conditions of sutra models the groundwater flow can reach the steady state within a short period i e 15 to 20 sec for the sand tank scale models although the distributions of hydraulic head may have to vary with time for displacement from saltwater to freshwater the density effect of underlying saltwater may not affect significantly the groundwater flow therefore uncoupled flow and transport runs were performed to simplify the simulations mazi and koussis 2021 model parameters are listed in table 1 which shows the two scales of assessment used in the current study namely sand tank scale case s1 and case s2 and field scale scenario a and scenario b the longitudinal and transverse dispersivities of sand tank scale simulations were set to low values of 1 5 10 3 m and 1 5 10 4 m respectively which were intended to reflect the narrow mixing zones observed in laboratory experiments measured effluent volumes together with the plume images from laboratory experiments were used to calibrate the sand tank scale model as described in the following section hydraulic conductivity values for medium sand and fine sand were adjusted slightly within the bounds of measurement uncertainty through this calibration process the sensitivity analysis was used to explore the importance of key parameters in modifying salt leaching rates and timing two different numerical modeling cases scenarios a and b both involving field scale horizontal barriers to flow were compared scenario a represents a subsurface barrier and subsurface drain system i e fig 1a while scenario b is a subsurface drainage lpsm system i e the main focus of the present study i e fig 1b the sensitivity analysis was undertaken based on 240 field scale simulations including 120 drainage lpsm schemes scenario b and 120 drainage subsurface barrier schemes i e scenario a where the barrier is placed below the land surface the drain spacing and soil depth of field scale models table 1 are similar to the geometry of models used by xin et al 2016 to examine the effect of macropores on water flow and salt leaching in reclaimed areas note that the distance from the centerline of the subsurface barrier in scenario a models to the land surface was set to 0 6 m in all simulations as per yu et al 2018 as with the sand tank scale model only half the flow domain between two drains was simulated due to symmetry in field scale models the field scale drain was installed at a depth of 1 m consistent with yu et al 2018 the surface ponding depth was 10 cm in field scale models while other boundary conditions were similar to those of the sand tank model the soil used in field scale models was sandy loam with the homogeneous hydraulic conductivity of 1 23 10 3 m s 1 and the porosity of 0 41 similar to that adopted by xin et al 2013 other parameters given in table 1 reflect the values used in previous studies of salt leaching under subsurface drainage systems i e siyal et al 2010 xin et al 2016 the subsurface regime reached a steady state condition within a rather short period i e 1500 to 1800 sec in the field scale model steady state conditions were assumed to occur when differences in the pore water velocity between consecutive time steps were within 0 1 considering 36 sampling points distributed throughout the model domain thus the volume of effluent drain discharge was almost equal i e within 1 to the volume of applied freshwater across the soil surface over a given timeframe both sand tank scale and field scale models adopted finite element grids of 31 250 elements no lpsm and 31 750 elements with lpsm the node spacing δl l in sand tank scale and field scale numerical models was 2 10 3 m and 4 10 2 m respectively these result in grid péclet numbers δl αl of 1 3 sand tank scale model and 0 4 field scale model respectively which satisfy the grid péclet constraint of δl αl 4 voss and provost 2008 to avoid numerical instabilities models adopted a fixed time step of 1 sec and 60 sec for sand tank scale and field scale models respectively an additional simulation field scale case without a lpsm was undertaken using a refined grid resolution i e 80 000 elements to examine the effect of nodal spacing on the results heads in the two simulations i e 31 250 and 80 000 elements were within 0 2 and solute concentrations were within 0 8 considering 18 sampling points distributed throughout the model domain this provided confidence that numerical solutions presented in this study are effectively grid independent 2 3 approximate approach to leaching timeframe estimation here an existing analytical solution derived by youngs and leeds harrison 2000 for seepage rates across a flooded soil surface is applied to estimate the timeframe required for salt leaching to occur youngs and leeds harrison 2000 developed the solution for seepage under progressive ponding including with subsurface drainage in place using conformal mapping their solution is based on the assumptions that the drain acts as a line sink and only saturated groundwater flow occurs note that these assumptions are valid only for sufficiently high ponding depth and sufficiently deep drain depth otherwise a phreatic surface unsaturated zone emerges above the drain fortunately the ponding depth and drain depth are usually sufficiently deep e g h 5 cm and d 1 m for salt leaching under subsurface drainage systems the solution takes into account inundation of only a portion of the land surface which is an approximation of the conceptual model of the current study in which infiltration through the lpsm is impeded that is for simplicity we neglected the infiltration of ponded water through the lpsm in applying the solution of youngs and leeds harrison 2000 which was applied to approximate infiltration into the uncovered area i e x λ 1 fig 1b according to youngs and leeds harrison 2000 the complex seepage potential of groundwater flow ω l2 t is given by 1 ω f φ i ψ q 2 π cosh 1 α 1 1 β 1 2 α 1 α 1 1 2 β 1 β 2 α 1 1 β 2 β 1 where f is the complex position plane f x iz l φ is the seepage velocity potential l2 t ψ is the stream function l2 t q is the drain flow per unit length of drain l2 t and α 1 β 1 and β 2 are defined as 2 α 1 sn 2 f λ 1 l f 3 β 1 sn 2 i f d l f 4 β 2 sn 2 f f l f here sn u is the jacobian elliptic function of u f is the complete elliptic integral of the first kind with modulus k corresponding to the ratio f f 2h l youngs and leeds harrison 2000 where f is a complementary form of f f and f are defined by byrd and friedman 1971 5 f 0 π 2 d θ 1 k 2 sin 2 θ 6 f 0 π 2 d θ 1 1 k 2 sin 2 θ to obtain the value of q in eq 1 the drain is assumed to contain the atmospheric pressure at f i d r at the top of the drain giving a value of ω ik 0 d h r at that location subsequently the values of f and ω were substituted into eq 1 leading to 7 q 2 π k 0 d h r cosh 1 α 1 1 β 1 2 α 1 α 1 1 2 β 1 sn 2 i d r f l f α 1 1 sn 2 i d r f l f β 1 substituting eq 7 into eq 1 ω can be solved at any location f within the problem domain following youngs and leeds harrison 2000 the leaching timeframe tm was estimated by 8 t m 2 l λ 1 θ 0 d t im ω l im ω λ 1 here dt is the target leaching depth l assumed to be 1 m in the field scale model im refers to the imaginary part of the complex argument 2 4 measurable diagnostics salinity variations within the subsurface as obtained from numerical simulations were evaluated through the determination of the zero order and first order spatial moments of the salt plume these were determined by following the approach of freyberg 1986 as 9 m 00 θ 0 ρ c x z t d x d z 10 m f m 00 m i where m 00 is the zero order spatial moment m or the total solute mass at a given moment in time ρ is the fluid density m l 3 c x z t is the salt concentration mf is the normalized salt mass and mi is the subsurface salt mass m prior to installation of the drain and surface inundation with freshwater the first order spatial moments m 10 and m 01 represent moments about the horizontal and vertical axes m l respectively and are given by 11 m 10 θ 0 ρ c x z t x d x d z 12 m 01 θ 0 ρ c x z t z d x d z the centroid xc zc of the solute plume is obtained from 13 x c m 10 m 00 z c m 01 m 00 differences in mf xc and zc between cases without and with a lpsm i e cases s1 and s2 respectively were used as measures of the effects of the lpsm on the leaching process three additional dimensionless variables were adopted t v and le to describe reductions in the leaching timeframe savings in the volume of applied freshwater and the leaching efficiency respectively these are defined as 14 t t 0 tr t 0 100 15 v v 0 vr v 0 100 16 l e c 0 θ 0 v n ρ s v ρ f where t and v represent the leaching timeframe and the volume of effluent collected from the drain during the leaching timeframe respectively subscripts 0 and r signify cases without and with the lpsm respectively c 0 is the initial salt concentration and vn is the volume of the subsurface l3 between the soil surface and drain depth i e the target leaching area le represents the mass of salt removed from the target leaching area per mass of applied freshwater numerical modeling was used to explore the effect of key lpsm parameters on variations in t v and le where le le le min le max le min with le max and le min representing the maximum and minimum le values among the 240 field scale simulations e g including all scenarios a and b lpsm parameters included dimensionless variables k log10 km k 0 and λ λ 1 l which were varied between the limits of 5 0 to 0 50 and 0 20 to 0 70 respectively the lpsm was assumed to have a thickness of 8 cm 3 results and discussion 3 1 effect of lpsm on groundwater flow streamline tracer experiments were carried out to explore variations in groundwater flow between cases s1 and s2 thereby reflecting the flow effects of installing the lpsm the results are shown in fig 3 the release positions of tracers used to show streamlines in fig 3 are represented in fig 2 as points a b c and d which were located at 10 cm 20 cm 30 cm and 40 cm respectively from the subsurface drain particle tracking was used to capture the tracer paths and particle residence times in the simulation models particle tracking was undertaken using a matlab script that calculated particle displacements each time step by interpolating the velocity field created by sutra to particle locations results from fig 3 show that particle paths and particle residence times from sutra simulations agree well with laboratory experiment values that is differences in experimental and simulated particle residence times for points a to d were within 9 this indicates that the numerical models reproduced seepage flow processes reasonably well fig 3a and b show that tracer residence times and lengths of particle paths in case s1 increased with distance from the drain as expected for example the particle path length and residence time in case s1 were 14 5 cm and 104 sec respectively for streamline a and 61 1 cm and 3986 sec respectively for streamline d here streamline a and streamline d are the streamlines emanating from points a and d respectively thus in the absence of a lpsm the average velocity of 0 13 cm s 1 of streamline a was an order of magnitude more than the streamline d average velocity of 0 015 cm s 1 in a previous laboratory study by mirjat et al 2008 similar contrasts in groundwater flow velocities were observed under the situation of complete ponding including 0 26 cm s 1 at 10 cm and 0 03 cm s 1 at 40 cm from the drain the addition of the lpsm led to major changes in flow paths and particle residence times as evident in fig 3c and 3d for example the lengths of streamlines a b c and d of 14 5 28 7 44 9 and 61 1 cm respectively in case s1 decreased to 11 9 22 4 38 2 and 58 0 cm respectively in case s2 the shorter path lengths in case s2 were accompanied by shorter particle residence times whereby residence times of 104 611 1794 and 3986 sec for streamlines a b c and d respectively in case s1 reduced to 100 181 358 and 1015 sec respectively in case s2 consequently the mean flow velocities of case s1 0 131 0 048 0 026 and 0 015 cm s 1 streamlines a to d respectively changed in case s2 0 119 0 122 0 107 and 0 057 cm s 1 that is the flow velocities of streamlines b c and d were substantially higher in case s2 relative to case s1 whereas the streamline a velocity was slightly higher in case s1 case s2 shows smaller differences between the mean flow velocities of streamlines a and d relative to those of case s1 thereby achieving the stated aims of installing the lpsm i e to increase leaching velocities in regions furthest from the drain installing a lpsm above the drain also greatly altered the infiltration pattern across the soil surface as evident in fig 4 which compares the freshwater influx per unit area of soil surface for cases s1 and s2 after 30 sec model results in fig 4 show that the freshwater influx in case s1 ranged from 10 cm min 1 i e above the drain x 0 cm to 0 18 cm min 1 at the midway point between the two drains i e x 50 cm the addition of the lpsm reduced the total freshwater influx per unit width of soil surface over the region 0 cm x 25 cm the extent of the lpsm from 72 0 cm2 min 1 in case s1 to 1 5 cm2 min 1 in case s2 conversely the total freshwater influx in the region 25 cm x 50 cm uncovered soil in both cases increased from 6 5 cm2 min 1 in case s1 to 41 5 cm2 min 1 in case s2 this enhanced infiltration led to pore water velocities in soils furthest from the drain that were considerably higher in case s2 compared to case s1 despite the reduced influxes in the near drain region in case s2 the magnitude of flow velocities beneath the lpsm were not substantially reduced compared to case s1 although the flow direction changed towards the horizontal in case s2 fig 3 this arose from the variations in the hydraulic head distributions due to the installation of lpsm which creates horizontal flow instead of the mainly vertical flow in case s1 in the near drain region fig 5 shows the variations in hydraulic head converted to the equivalent freshwater heads from numerical modeling of cases s1 and s2 note that the distributions of hydraulic head for the sand tank scale models have completely reached the steady state after 20 sec as expected the hydraulic head decreases towards the drain in both cases distributions in hydraulic head near the soil surface are consistent with freshwater influx patterns i e fig 4 whereby head contours that are nearly horizontal reflect stronger rates of vertical flow and therefore higher rates of freshwater influx that is areas of near horizontal head contours near the soil surface occur over the inland half of the domain in case s2 whereas these occur close to the drain in case s1 thus highlighting the effect of the lpsm on near surface heads and influxes the hydraulic head contours for case s1 fig 5a fall steeply near the drain while small head gradients are apparent for x 35 cm this led to the 1 8 cm head contour black line in fig 5a occurring between the drain and the soil surface and being within 30 cm of the drain throughout the model domain of case s1 the 0 cm head contour i e white lines encircles the drain in case s1 the addition of the lpsm in case s2 caused the 1 8 cm head contour i e black lines to shift away from the drain due to more evenly distributed head gradients throughout the modelled region the 0 cm head occurs as a near vertical contour at approximately 12 13 cm from the drain indicating mostly lateral horizontal flow in the region beneath the lpsm the more evenly distributed head gradients and therefore groundwater flow in case s2 compared to case s1 provides a stronger potential for salt flushing in regions furthest from the drain 3 2 effects of the lpsm on salt distributions fig 6 shows salinity distributions and thereby patterns of salt leaching from laboratory experiments and corresponding numerical modeling of cases s1 and s2 while a larger freshwater saltwater mixing zone is apparent in numerical results relative to the sharp interface that appeared in laboratory experiments the geometry of the freshwater saltwater interface from laboratory experiments is closely approximated by the numerical model the close match between laboratory and numerical salinity distributions adds further validation of the leaching experimental and numerical methodologies and indicates that salt leaching processes are well represented in numerical models fig 6a shows that in case s1 the salt plume moved downward more rapidly within the near drain area than in soils more distant from the drain in both the laboratory and numerical results this is consistent with higher rates of freshwater influx above the drain as shown in fig 4 in the absence of the lpsm saltwater within the near drain area was rapidly leached 1 min to a depth of 5 cm i e z 5 cm whereas leaching of the salt plume near the right boundary was significantly slower 14 min to leach salts to a depth of 5 cm in case s2 the soil next to the edge of the lpsm i e at x 25 cm was rapidly leached during the first stages of the experiment only minor salt leaching occurred beneath the lpsm x 25 cm during the first 1 min of case s2 the lateral groundwater flow beneath the lpsm that is apparent from particle pathways e g fig 3c led to salt leaching from right to left in that region as evident in the transition in the freshwater saltwater interface in fig 6b salt leaching across the entire target leaching area in case s2 was considerably faster than that observed in case s1 whereby complete salt leaching of the target leaching area was about 14 min in case s1 and 5 min in case s2 leaching timeframes and efficiencies and effluent volumes i e presumed equal to freshwater influxes under steady state conditions for the two experiments are given in table 2 the results show that the case s1 leaching timeframe t and effluent volume v reduced by up to 60 and 80 respectively with the addition of the lpsm in case s2 le increased fivefold from 0 92 in case s1 to 4 87 in case s2 fig 7 a shows temporal variations in the normalized salt mass within the target leaching area mf for cases s1 and s2 mf trends indicate a slightly faster loss of salt mass in case s1 than in case s2 during the early stages i e 55 sec thereafter the reduction in mf was slower in case s1 than in case s2 such that by 5 min the saltwater had been completely removed from the target leaching area in case s2 while approximately 20 of the original salt mass remained in case s1 at that time removal of the remaining salt mass in case s1 required another 9 min of leaching fig 7b illustrates transient variations in the centroid of the salt plume based on numerical modeling of cases s1 and s2 the more rapid leaching of the total salt mass in case s2 is clearly shown the trajectory of the centroid movement in case s2 is closer to the domain s midpoint i e the orange dashed line in fig 7b than in case s1 indicating a more spatially uniform leaching pattern when the lpsm is in place the leftward trajectory of the centroid in case s2 arises because freshening occurs preferentially to the right of the lpsm whereas the opposite occurs in case s1 rightward trajectory due to preferential freshening above the drain 3 3 sensitivity analysis fig 8 depicts trends in t and v for scenario b i e the field scale models with a lpsm relative to the dimensionless variables k and λ using otherwise the base case parameters listed in table 1 results show that both t and v which reflect the influence of the lpsm on leaching timeframes and required volumes of irrigation water are non monotonic functions of k and λ whereby optimal values i e that lead to maximum t and v of the lpsm hydraulic conductivity and length are apparent the maximum t arises when k is 2 5 and λ is 0 5 while the peak v occurs with k and λ values of 3 and 0 5 respectively variations in both t and v with changes in k show asymmetric behavior i e both t and v sharply increase and then gradually decline with decreasing k for k values less than 2 5 the lpsm hydraulic conductivity has a somewhat similar effect on t and v indicating that the accompanying reduction in freshwater influx through the lpsm is inconsequential when k is low the lpsm length i e λ is the key parameter controlling freshwater influx and consequently t and v values of t and v plateau i e become insensitive when λ falls between 0 4 and 0 5 while both t and v are lower for smaller i e 0 4 and larger i e 0 5 values of λ note that when λ is set to small i e 0 2 or large i e 0 7 values the installation of the lpsm may lead to negative values for both t and v that is the lpsm may cause longer leaching timeframes and require larger volumes of irrigation water to leach salt from the target leaching area relative to the case without a lpsm this highlights the importance of applying lpsms with appropriate characteristics fig 9 illustrates trends in le in response to variations in k and λ for scenarios a i e field scale models with a subsurface barrier and b i e field scale models with a lpsm note that the base case parameters used in scenario a are consistent with those in scenario b which were listed in table 1 contours of le in fig 9 indicate that the lpsm system scenario b fig 9b outperforms the subsurface barrier scenario a fig 9a on the basis of the leaching efficiency in both cases maximum le values are apparent equal to 0 77 and 1 0 for scenarios a and b respectively le contours from scenario a tend towards the vertical fig 9a indicating greater sensitivity to k than λ whereas the opposite is apparent for scenario b fig 9b in which mostly horizontal contours indicate greater sensitivity to λ this arises primarily from the salt trapped above the barrier in scenario a which is linked directly to the barrier s hydraulic conductivity i e k additionally horizontal flow beneath the lpsm in scenario b instead of the vertical flow without the lpsm diminished the influence of increasingly smaller permeabilities of the lpsm relative to the optimal value on salt leaching within the near drain area given the flattening of gradients in plots of le versus k and λ it follows that the ideal lpsm length is one that is towards the lower end of the ideal range taking into account that construction costs will increase with λ i e λ 0 4 for the case considered scenario b to further reduce material costs a relatively thin lpsm thickness λ 2 with relatively low hydraulic conductivity km is also suggested 3 4 predictive solution of leaching timeframe the approximate analytical solution of youngs and leeds harrison 2000 was evaluated by comparing analytical and numerical modeling results for a range of hypothetical scenarios based on the field scale models of the drainage lpsm system this analysis adopted the lpsm with k 3 and λ 0 45 i e values that led to optimal values leaching performance see fig 9b and different values for k 0 d and s see table 3 the resulting leaching periods obtained from modeling results are given in fig 10 which reveals a generally reasonable match between the two approaches as differences between analytical and numerical results were within 20 the analytical solution was assumed to provide adequate estimates of leaching timeframes at least for the conditions considered in this study therefore even though the analytical solution neglects flow through the lpsm it appears that lpsm throughflow makes a relatively minor contribution to the leaching timeframes 4 conclusions this study explored the effects of adding a low permeability surface mulch lpsm to a subsurface drainage system for the purposes of enhancing the leaching efficiency of highly saline soils e g as might be found in reclaimed coastal regions under continuous freshwater ponding sand tank experiments to examine the effectiveness of the proposed method were found to be in good agreement with numerical simulations an approximate analytical approach applied previously to evaluate salt leaching using progressive ponding also showed good agreement with numerical model results the main conclusions are as follows 1 the lpsm acted to reduce hydraulic heads underneath the lpsm leading to more evenly distributed heads between subsurface drains than occurred without the lpsm 2 the modified head distribution caused by the lpsm led to more even distributed groundwater velocities which enhanced salt flushing in regions furthest from the drain where leaching was slowest without the lpsm 3 more uniform leaching led to shorter leaching periods and smaller volumes of applied freshwater by up to 60 and 80 respectively at the sand tank scale 4 the sensitivity analysis based on the field scale models revealed that the placement of a horizontal flow barrier at the surface led to improved performance in terms of salt leaching rates compared to a buried subsurface horizontal barrier which was explored previously by yu et al 2018 5 an existing solution was compared to numerical modeling results showing its applicability for estimating leaching timeframes for the design of drainage lpsm schemes the results from this study provide preliminary guidance to design subsurface drainage lpsm schemes however some practical factors such as soil heterogeneity transient conditions buoyancy evapotranspiration including evapo concentration of soil salts and three dimensional flow effects may affect the performance of the lpsm in terms of salt leaching further studies should aim to incorporate these factors to investigate the salt leaching process in situations of a lpsm credit authorship contribution statement jiaxu zhang conceptualization methodology investigation writing original draft adrian d werner investigation writing review editing chunhui lu conceptualization investigation supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments c lu acknowledges the financial support from the national natural science foundation of china 51679067 and 51879088 fundamental research funds for the central universities b200204002 and the natural science foundation of jiangsu province bk20190023 
172,surface ponding is often used in combination with subsurface drainage systems to leach excessive salts from the saline soils of reclaimed coastal areas however soils that are not immediately proximal to subsurface drains are typically subjected to reduced leaching efficiency le this study evaluates an approach for increasing the le through the use of a low permeability surface mulch lpsm above the drain sand tank experiments and 2d numerical simulations were conducted to examine the effectiveness of lpsm configurations under continuous surface flooding the results show that the addition of a lpsm leads to more spatially uniform accelerated salt leaching towards subsurface drains as a result base case leaching periods and freshwater application rates were reduced by up to 60 and 80 respectively the sensitivity analysis based on field scale models revealed optimal values i e leading to maximum le of the lpsm hydraulic conductivity and length moreover an approximate method is applied to estimate the leaching period and to thereby assist in the design of drainage lpsm schemes we conclude that the addition of a lpsm is an effective strategy to enhance salt leaching under surface ponding adding to existing methods for ameliorating saline soils keywords soil management salinity sand tank experiment analytical solution 1 introduction coastal areas are being reclaimed in many parts of the world due mostly to the need to relieve population pressures martín antón et al 2016 sengupta et al 2018 however reclaimed coastal soils commonly require amelioration of high soil salinities li et al 2014 xiao et al 2019 which arise from prior inundation with seawater yang et al 2013 yu et al 2016 high salinity levels within the natural soil may induce various adverse effects on the habitats of soil microorganisms and the growth of crops leading to decreased agricultural production and disturbances to nutrient and biological cycling that otherwise benefit soil health eventually evolving into a sociocultural and environmental issue brevik et al 2015 pezeshki 2001 rengasamy 2006 thus lowering the soil salinity leads to enhanced agricultural productivity and improved nutrient and biological cycling that benefit soil health pezeshki 2001 xie et al 2017 the amelioration of reclaimed saline soils is often accelerated through the installation of drainage systems especially where salt flushing from rainfall is inhibited by high evaporation rates poor soil drainage and or where shallow saline groundwater occurs barua and alam 2013 daliakopoulos et al 2016 letey et al 2011 drainage systems may also be used to lower groundwater levels thereby decreasing the potential for waterlogging and the secondary salinization caused by the evapo concentration of soil salts liu et al 2017 subsurface drainage systems are often preferred to drainage channels due to advantages of low land occupation and limited interference with farming operations afruzi et al 2014 mirlas et al 2013 they are typically designed not only to lower salinities in the root zone steppuhn et al 2005 but also to maintain the groundwater level to an adequate depth to prevent the upward movement of salty capillary water from reaching the root zone qadir et al 2000 in practice the subsurface drain is usually suggested to be installed at the 0 75 to 1 2 m depth for satisfying the above mentioned requirements kladivko et al 1999 the performance of subsurface drainage systems in reducing soil salinity is routinely enhanced through the application of freshwater to the soil surface to supplement the flushing caused by rainfall ritzema et al 2008 sarmah and tiwari 2018 however previous studies of the subsurface drain performance have revealed that under continuous surface ponding only salts within the near drain area may be effectively leached mirjat et al 2014 siyal et al 2010 youngs and leeds harrison 2000 this is attributed to decreased seepage rates potentially by orders of magnitude in reclaimed areas furthest from the drains e g midway between drains xin et al 2016 this leads to inefficient salt leaching in those soils requiring longer timeframes than are required to flush salts from near drain soils and thereby wasting the unnecessary amount of freshwater in the near drain soils where the salts have been flushed out earlier the high spatial variability in salt leaching rates of subsurface drainage systems may be mitigated if the downward flow of freshwater from flood irrigation is more spatially uniform several methods have been developed to diminish spatial disparities in salt leaching rates within subsurface drainage schemes for example rao and leeds harrison 1991 numerically demonstrated that progressive ponding instead of continuous surface ponding can improve the leaching efficiency le in progressive ponding the soil surface between two drains is divided into several parallel strips which are successively flooded starting from midway between drains to the near drain area until the whole area is flooded youngs and leeds harrison 2000 a disadvantage of progressive ponding is the need for additional infrastructure to control the location and timing of flooding mirjat et al 2008 investigated leaching rates under progressive ponding using laboratory experimentation which included streamline tracing under four surface ponding scenarios the experimental results showed that higher pore water velocities occurred in the region midway between the two drains under progressive ponding than complete ponding leading to more effective leaching siyal et al 2010 used numerical modeling to estimate time and water savings from progressive ponding compared to complete ponding for various soil textures including layered soils subsurface drain depths and soil depths the results indicated that water and time savings in excess of 90 can be achieved through progressive ponding although water savings were only 13 when applied to a layered loam over sand soil an alternative to progressive ponding is intermittent ponding which has also been shown to improve the le darzi naftchali et al 2018 sibai et al 1997 previous studies found that intermittent ponding decreased the rate of water bypass compared to continuous ponding which reduces the le because flow through macropores and other preferential pathways tends to carry limited salts relative to diffuse flow through the soil matrix haws et al 2004 vogel et al 2000 however some studies have found that intermittent ponding requires considerably longer timeframes to achieve the same salt leaching as continuous ponding and under some conditions intermittent ponding may not lead to water savings due to the water loss and salt accumulation caused by effects of evapotranspiration minhas and khosla 1986 qadir et al 2000 spatial disparities in salt leaching rates under surface ponding may also be reduced by altering the permeability distribution of the soil yu et al 2018 proposed to add barriers within the soil profile above subsurface drains to create more spatially uniform salt leaching rates a conceptual model of the yu et al 2018 scheme is illustrated in fig 1 a yu et al 2018 used numerical modeling to show that the barrier greatly altered the pattern of infiltration under continuous ponding leading to increased le in comparison to the unmodified soil profile the barrier acted to both reduce the surface influx near the drains and enhance the infiltration through soils furthest from the drains the current study aims to extend the previous investigation by yu et al 2018 of the effects of adding a barrier to subsurface drainage surface ponding systems by evaluating the situation where the barrier to flow is placed at the land surface the surface barrier is referred to herein as a low permeability surface mulch lpsm and its conceptual model is illustrated in fig 1b the lpsm has two advantages over a subsurface barrier 1 the condition of the lpsm can be assessed visually and its placement does not require soil disturbance and 2 the lpsm avoids restrictions to plant rooting depths and salt flushing of near surface soils that arise with barriers placed below the land surface the analysis of the subsurface drainage lpsm system proposed here includes both laboratory experiments and numerical modeling these are used to 1 examine salt leaching processes associated with the subsurface drainage lpsm system within a sandy soil under controlled laboratory conditions 2 quantify the effectiveness in terms of salt leaching rates of different lpsm implementations and 3 apply and evaluate an existing approximate method for estimating seepage rates across the soil surface that allows for rapid estimation of the timeframe required for salt leaching to occur with and without a lpsm 2 methodology 2 1 laboratory experiments a schematic of the experimental setup is illustrated in fig 2 two dimensional laboratory experiments were carried out in a 100 cm length 6 cm width 30 cm height sand tank in the front of the laboratory setup which is similar to that adopted by mirjat et al 2008 for investigating leaching under progressive flooding situations a subsurface drain spanning the width of the sand tank was placed at 20 cm above the base of the sand tank and 50 cm from the left boundary the drain consisted of a perforated stainless steel tube with an internal diameter of 5 mm the tube was wrapped with a nylon screen to prevent sand from entering it the drain was connected to the atmosphere at its outlet allowing water to flow freely out of the sand tank the sand tank was filled with sand to 25 cm from the base surface ponding was maintained through a constant head tank connected to the backside of the sand tank via a connecting plane note that the connecting plane can assist in providing a steady flow from the constant head tank to the sand tank inflow to the constant head tank occurred from a variable overflow device which was connected to the constant head tank through a 50 mm internal diameter tubing here the overflow outlet depth in the variable overflow device can determine the ponding depth which was set to 2 cm in all experiments two different sands were used in the experiments well sorted medium sand i e the ambient soil and well sorted fine sand i e the lpsm grain size distribution curves produced d 50 1 76 mm and d 60 d 10 1 83 for the medium sand and d 50 0 09 mm and d 60 d 10 1 65 for the fine sand here d 10 d 50 and d 60 are the grain sizes that 10 50 and 60 of samples are finer by weight respectively sand porosities were found to be θ 0 0 40 0 01 medium sand and θ m 0 44 0 01 fine sand by oven drying saturated sand samples these values were taken as effective porosities θ e in numerical simulations darcy column tests three tests each for the medium and fine sands were undertaken leading to ranges of saturated hydraulic conductivities of 2 14 10 3 3 01 10 3 m s 1 medium sand and 1 04 10 6 8 04 10 6 m s 1 fine sand the fine sand lpsm had a thickness of 1 cm and length of 50 cm and was placed symmetrically above the subsurface drain fig 2 the sand tank was wet packed using incremental filling to avoid trapped air bubbles in the sand this included both the medium sand and the lpsm note that the subsurface drain was closed by a valve during filling the tank with sand follow on the ponding depth was maintained at 2 cm by adjusting the overflow outlet depth after the sand tank had been filled with sand as a result the sands are consistently saturated during the sand filling stage the drain would be opened immediately when the experiments started running the 10 000 ppm mass fraction parts per million saltwater in laboratory experiments was produced by dissolving 1 kg of sodium chloride nacl into 99 kg of deionized water at 15 c the saltwater and freshwater densities were determined by a densimeter to be 1006 8 kg m 3 and 999 2 kg m 3 note that two scenarios were conducted to evaluate the effectiveness of the lpsm in which case s1 is defined as the case without the lpsm and case s2 is referred to the case with the lpsm a conservative dye tracer allura red ac was added to the saltwater to allow for leaching experiments to track saltwater distributions the apparatus was photographed every 5 sec during experiments using a digital camera canon ixus 175 the effluent discharging from the subsurface drain was collected using a measuring cylinder to determine discharge rates the effluent salinity was measured by a handheld multiparameter meter ysi professional plus tracer experiments used to depict streamlines were conducted for cases s1 and s2 with stained tracers to allow for improved streamline visualization note that the saltwater of 10 000 ppm within the medium sand was not dye stained in the tracer experiments the stained tracers were injected at the soil surface points a to d fig 2 at low fluxes 10 ml min 1 using a peristaltic pump 2 2 numerical modeling the finite element sutra code voss and provost 2008 was employed to simulate density dependent flow and solute transport within the subsurface the scenarios that were evaluated involved only saturated groundwater flow due to the situation of continuous surface ponding therefore only saturated flow was considered in sutra the relevant governing equations describing flow and solute transport in sutra are given in the user manual voss and provost 2008 and are not repeated here for brevity sutra has been validated extensively against benchmark problems involving density dependent flow and transport kuan et al 2012 shen et al 2015 the sutra model of the sand tank represented only the right hand half of the apparatus due to flow symmetry about the drain the subsurface drain was assumed to be fully filled with water thereby the nodes related to the drain were taken as seepage face nodes with the zero pressure head i e atmospheric pressure as suggested by xin et al 2016 boundary nodes representing surface ponding were also designated as specified head conditions adopting constant values that reflected the depth of ponding other boundaries were set to no flow conditions surface nodes were assigned specified concentration boundary conditions of zero salinity while the drain was designated a zero concentration gradient boundary condition the hydrostatic conditions with a zero hydraulic head and the uniform concentration distributions of 10 000 ppm were used for the initial conditions of sutra models the groundwater flow can reach the steady state within a short period i e 15 to 20 sec for the sand tank scale models although the distributions of hydraulic head may have to vary with time for displacement from saltwater to freshwater the density effect of underlying saltwater may not affect significantly the groundwater flow therefore uncoupled flow and transport runs were performed to simplify the simulations mazi and koussis 2021 model parameters are listed in table 1 which shows the two scales of assessment used in the current study namely sand tank scale case s1 and case s2 and field scale scenario a and scenario b the longitudinal and transverse dispersivities of sand tank scale simulations were set to low values of 1 5 10 3 m and 1 5 10 4 m respectively which were intended to reflect the narrow mixing zones observed in laboratory experiments measured effluent volumes together with the plume images from laboratory experiments were used to calibrate the sand tank scale model as described in the following section hydraulic conductivity values for medium sand and fine sand were adjusted slightly within the bounds of measurement uncertainty through this calibration process the sensitivity analysis was used to explore the importance of key parameters in modifying salt leaching rates and timing two different numerical modeling cases scenarios a and b both involving field scale horizontal barriers to flow were compared scenario a represents a subsurface barrier and subsurface drain system i e fig 1a while scenario b is a subsurface drainage lpsm system i e the main focus of the present study i e fig 1b the sensitivity analysis was undertaken based on 240 field scale simulations including 120 drainage lpsm schemes scenario b and 120 drainage subsurface barrier schemes i e scenario a where the barrier is placed below the land surface the drain spacing and soil depth of field scale models table 1 are similar to the geometry of models used by xin et al 2016 to examine the effect of macropores on water flow and salt leaching in reclaimed areas note that the distance from the centerline of the subsurface barrier in scenario a models to the land surface was set to 0 6 m in all simulations as per yu et al 2018 as with the sand tank scale model only half the flow domain between two drains was simulated due to symmetry in field scale models the field scale drain was installed at a depth of 1 m consistent with yu et al 2018 the surface ponding depth was 10 cm in field scale models while other boundary conditions were similar to those of the sand tank model the soil used in field scale models was sandy loam with the homogeneous hydraulic conductivity of 1 23 10 3 m s 1 and the porosity of 0 41 similar to that adopted by xin et al 2013 other parameters given in table 1 reflect the values used in previous studies of salt leaching under subsurface drainage systems i e siyal et al 2010 xin et al 2016 the subsurface regime reached a steady state condition within a rather short period i e 1500 to 1800 sec in the field scale model steady state conditions were assumed to occur when differences in the pore water velocity between consecutive time steps were within 0 1 considering 36 sampling points distributed throughout the model domain thus the volume of effluent drain discharge was almost equal i e within 1 to the volume of applied freshwater across the soil surface over a given timeframe both sand tank scale and field scale models adopted finite element grids of 31 250 elements no lpsm and 31 750 elements with lpsm the node spacing δl l in sand tank scale and field scale numerical models was 2 10 3 m and 4 10 2 m respectively these result in grid péclet numbers δl αl of 1 3 sand tank scale model and 0 4 field scale model respectively which satisfy the grid péclet constraint of δl αl 4 voss and provost 2008 to avoid numerical instabilities models adopted a fixed time step of 1 sec and 60 sec for sand tank scale and field scale models respectively an additional simulation field scale case without a lpsm was undertaken using a refined grid resolution i e 80 000 elements to examine the effect of nodal spacing on the results heads in the two simulations i e 31 250 and 80 000 elements were within 0 2 and solute concentrations were within 0 8 considering 18 sampling points distributed throughout the model domain this provided confidence that numerical solutions presented in this study are effectively grid independent 2 3 approximate approach to leaching timeframe estimation here an existing analytical solution derived by youngs and leeds harrison 2000 for seepage rates across a flooded soil surface is applied to estimate the timeframe required for salt leaching to occur youngs and leeds harrison 2000 developed the solution for seepage under progressive ponding including with subsurface drainage in place using conformal mapping their solution is based on the assumptions that the drain acts as a line sink and only saturated groundwater flow occurs note that these assumptions are valid only for sufficiently high ponding depth and sufficiently deep drain depth otherwise a phreatic surface unsaturated zone emerges above the drain fortunately the ponding depth and drain depth are usually sufficiently deep e g h 5 cm and d 1 m for salt leaching under subsurface drainage systems the solution takes into account inundation of only a portion of the land surface which is an approximation of the conceptual model of the current study in which infiltration through the lpsm is impeded that is for simplicity we neglected the infiltration of ponded water through the lpsm in applying the solution of youngs and leeds harrison 2000 which was applied to approximate infiltration into the uncovered area i e x λ 1 fig 1b according to youngs and leeds harrison 2000 the complex seepage potential of groundwater flow ω l2 t is given by 1 ω f φ i ψ q 2 π cosh 1 α 1 1 β 1 2 α 1 α 1 1 2 β 1 β 2 α 1 1 β 2 β 1 where f is the complex position plane f x iz l φ is the seepage velocity potential l2 t ψ is the stream function l2 t q is the drain flow per unit length of drain l2 t and α 1 β 1 and β 2 are defined as 2 α 1 sn 2 f λ 1 l f 3 β 1 sn 2 i f d l f 4 β 2 sn 2 f f l f here sn u is the jacobian elliptic function of u f is the complete elliptic integral of the first kind with modulus k corresponding to the ratio f f 2h l youngs and leeds harrison 2000 where f is a complementary form of f f and f are defined by byrd and friedman 1971 5 f 0 π 2 d θ 1 k 2 sin 2 θ 6 f 0 π 2 d θ 1 1 k 2 sin 2 θ to obtain the value of q in eq 1 the drain is assumed to contain the atmospheric pressure at f i d r at the top of the drain giving a value of ω ik 0 d h r at that location subsequently the values of f and ω were substituted into eq 1 leading to 7 q 2 π k 0 d h r cosh 1 α 1 1 β 1 2 α 1 α 1 1 2 β 1 sn 2 i d r f l f α 1 1 sn 2 i d r f l f β 1 substituting eq 7 into eq 1 ω can be solved at any location f within the problem domain following youngs and leeds harrison 2000 the leaching timeframe tm was estimated by 8 t m 2 l λ 1 θ 0 d t im ω l im ω λ 1 here dt is the target leaching depth l assumed to be 1 m in the field scale model im refers to the imaginary part of the complex argument 2 4 measurable diagnostics salinity variations within the subsurface as obtained from numerical simulations were evaluated through the determination of the zero order and first order spatial moments of the salt plume these were determined by following the approach of freyberg 1986 as 9 m 00 θ 0 ρ c x z t d x d z 10 m f m 00 m i where m 00 is the zero order spatial moment m or the total solute mass at a given moment in time ρ is the fluid density m l 3 c x z t is the salt concentration mf is the normalized salt mass and mi is the subsurface salt mass m prior to installation of the drain and surface inundation with freshwater the first order spatial moments m 10 and m 01 represent moments about the horizontal and vertical axes m l respectively and are given by 11 m 10 θ 0 ρ c x z t x d x d z 12 m 01 θ 0 ρ c x z t z d x d z the centroid xc zc of the solute plume is obtained from 13 x c m 10 m 00 z c m 01 m 00 differences in mf xc and zc between cases without and with a lpsm i e cases s1 and s2 respectively were used as measures of the effects of the lpsm on the leaching process three additional dimensionless variables were adopted t v and le to describe reductions in the leaching timeframe savings in the volume of applied freshwater and the leaching efficiency respectively these are defined as 14 t t 0 tr t 0 100 15 v v 0 vr v 0 100 16 l e c 0 θ 0 v n ρ s v ρ f where t and v represent the leaching timeframe and the volume of effluent collected from the drain during the leaching timeframe respectively subscripts 0 and r signify cases without and with the lpsm respectively c 0 is the initial salt concentration and vn is the volume of the subsurface l3 between the soil surface and drain depth i e the target leaching area le represents the mass of salt removed from the target leaching area per mass of applied freshwater numerical modeling was used to explore the effect of key lpsm parameters on variations in t v and le where le le le min le max le min with le max and le min representing the maximum and minimum le values among the 240 field scale simulations e g including all scenarios a and b lpsm parameters included dimensionless variables k log10 km k 0 and λ λ 1 l which were varied between the limits of 5 0 to 0 50 and 0 20 to 0 70 respectively the lpsm was assumed to have a thickness of 8 cm 3 results and discussion 3 1 effect of lpsm on groundwater flow streamline tracer experiments were carried out to explore variations in groundwater flow between cases s1 and s2 thereby reflecting the flow effects of installing the lpsm the results are shown in fig 3 the release positions of tracers used to show streamlines in fig 3 are represented in fig 2 as points a b c and d which were located at 10 cm 20 cm 30 cm and 40 cm respectively from the subsurface drain particle tracking was used to capture the tracer paths and particle residence times in the simulation models particle tracking was undertaken using a matlab script that calculated particle displacements each time step by interpolating the velocity field created by sutra to particle locations results from fig 3 show that particle paths and particle residence times from sutra simulations agree well with laboratory experiment values that is differences in experimental and simulated particle residence times for points a to d were within 9 this indicates that the numerical models reproduced seepage flow processes reasonably well fig 3a and b show that tracer residence times and lengths of particle paths in case s1 increased with distance from the drain as expected for example the particle path length and residence time in case s1 were 14 5 cm and 104 sec respectively for streamline a and 61 1 cm and 3986 sec respectively for streamline d here streamline a and streamline d are the streamlines emanating from points a and d respectively thus in the absence of a lpsm the average velocity of 0 13 cm s 1 of streamline a was an order of magnitude more than the streamline d average velocity of 0 015 cm s 1 in a previous laboratory study by mirjat et al 2008 similar contrasts in groundwater flow velocities were observed under the situation of complete ponding including 0 26 cm s 1 at 10 cm and 0 03 cm s 1 at 40 cm from the drain the addition of the lpsm led to major changes in flow paths and particle residence times as evident in fig 3c and 3d for example the lengths of streamlines a b c and d of 14 5 28 7 44 9 and 61 1 cm respectively in case s1 decreased to 11 9 22 4 38 2 and 58 0 cm respectively in case s2 the shorter path lengths in case s2 were accompanied by shorter particle residence times whereby residence times of 104 611 1794 and 3986 sec for streamlines a b c and d respectively in case s1 reduced to 100 181 358 and 1015 sec respectively in case s2 consequently the mean flow velocities of case s1 0 131 0 048 0 026 and 0 015 cm s 1 streamlines a to d respectively changed in case s2 0 119 0 122 0 107 and 0 057 cm s 1 that is the flow velocities of streamlines b c and d were substantially higher in case s2 relative to case s1 whereas the streamline a velocity was slightly higher in case s1 case s2 shows smaller differences between the mean flow velocities of streamlines a and d relative to those of case s1 thereby achieving the stated aims of installing the lpsm i e to increase leaching velocities in regions furthest from the drain installing a lpsm above the drain also greatly altered the infiltration pattern across the soil surface as evident in fig 4 which compares the freshwater influx per unit area of soil surface for cases s1 and s2 after 30 sec model results in fig 4 show that the freshwater influx in case s1 ranged from 10 cm min 1 i e above the drain x 0 cm to 0 18 cm min 1 at the midway point between the two drains i e x 50 cm the addition of the lpsm reduced the total freshwater influx per unit width of soil surface over the region 0 cm x 25 cm the extent of the lpsm from 72 0 cm2 min 1 in case s1 to 1 5 cm2 min 1 in case s2 conversely the total freshwater influx in the region 25 cm x 50 cm uncovered soil in both cases increased from 6 5 cm2 min 1 in case s1 to 41 5 cm2 min 1 in case s2 this enhanced infiltration led to pore water velocities in soils furthest from the drain that were considerably higher in case s2 compared to case s1 despite the reduced influxes in the near drain region in case s2 the magnitude of flow velocities beneath the lpsm were not substantially reduced compared to case s1 although the flow direction changed towards the horizontal in case s2 fig 3 this arose from the variations in the hydraulic head distributions due to the installation of lpsm which creates horizontal flow instead of the mainly vertical flow in case s1 in the near drain region fig 5 shows the variations in hydraulic head converted to the equivalent freshwater heads from numerical modeling of cases s1 and s2 note that the distributions of hydraulic head for the sand tank scale models have completely reached the steady state after 20 sec as expected the hydraulic head decreases towards the drain in both cases distributions in hydraulic head near the soil surface are consistent with freshwater influx patterns i e fig 4 whereby head contours that are nearly horizontal reflect stronger rates of vertical flow and therefore higher rates of freshwater influx that is areas of near horizontal head contours near the soil surface occur over the inland half of the domain in case s2 whereas these occur close to the drain in case s1 thus highlighting the effect of the lpsm on near surface heads and influxes the hydraulic head contours for case s1 fig 5a fall steeply near the drain while small head gradients are apparent for x 35 cm this led to the 1 8 cm head contour black line in fig 5a occurring between the drain and the soil surface and being within 30 cm of the drain throughout the model domain of case s1 the 0 cm head contour i e white lines encircles the drain in case s1 the addition of the lpsm in case s2 caused the 1 8 cm head contour i e black lines to shift away from the drain due to more evenly distributed head gradients throughout the modelled region the 0 cm head occurs as a near vertical contour at approximately 12 13 cm from the drain indicating mostly lateral horizontal flow in the region beneath the lpsm the more evenly distributed head gradients and therefore groundwater flow in case s2 compared to case s1 provides a stronger potential for salt flushing in regions furthest from the drain 3 2 effects of the lpsm on salt distributions fig 6 shows salinity distributions and thereby patterns of salt leaching from laboratory experiments and corresponding numerical modeling of cases s1 and s2 while a larger freshwater saltwater mixing zone is apparent in numerical results relative to the sharp interface that appeared in laboratory experiments the geometry of the freshwater saltwater interface from laboratory experiments is closely approximated by the numerical model the close match between laboratory and numerical salinity distributions adds further validation of the leaching experimental and numerical methodologies and indicates that salt leaching processes are well represented in numerical models fig 6a shows that in case s1 the salt plume moved downward more rapidly within the near drain area than in soils more distant from the drain in both the laboratory and numerical results this is consistent with higher rates of freshwater influx above the drain as shown in fig 4 in the absence of the lpsm saltwater within the near drain area was rapidly leached 1 min to a depth of 5 cm i e z 5 cm whereas leaching of the salt plume near the right boundary was significantly slower 14 min to leach salts to a depth of 5 cm in case s2 the soil next to the edge of the lpsm i e at x 25 cm was rapidly leached during the first stages of the experiment only minor salt leaching occurred beneath the lpsm x 25 cm during the first 1 min of case s2 the lateral groundwater flow beneath the lpsm that is apparent from particle pathways e g fig 3c led to salt leaching from right to left in that region as evident in the transition in the freshwater saltwater interface in fig 6b salt leaching across the entire target leaching area in case s2 was considerably faster than that observed in case s1 whereby complete salt leaching of the target leaching area was about 14 min in case s1 and 5 min in case s2 leaching timeframes and efficiencies and effluent volumes i e presumed equal to freshwater influxes under steady state conditions for the two experiments are given in table 2 the results show that the case s1 leaching timeframe t and effluent volume v reduced by up to 60 and 80 respectively with the addition of the lpsm in case s2 le increased fivefold from 0 92 in case s1 to 4 87 in case s2 fig 7 a shows temporal variations in the normalized salt mass within the target leaching area mf for cases s1 and s2 mf trends indicate a slightly faster loss of salt mass in case s1 than in case s2 during the early stages i e 55 sec thereafter the reduction in mf was slower in case s1 than in case s2 such that by 5 min the saltwater had been completely removed from the target leaching area in case s2 while approximately 20 of the original salt mass remained in case s1 at that time removal of the remaining salt mass in case s1 required another 9 min of leaching fig 7b illustrates transient variations in the centroid of the salt plume based on numerical modeling of cases s1 and s2 the more rapid leaching of the total salt mass in case s2 is clearly shown the trajectory of the centroid movement in case s2 is closer to the domain s midpoint i e the orange dashed line in fig 7b than in case s1 indicating a more spatially uniform leaching pattern when the lpsm is in place the leftward trajectory of the centroid in case s2 arises because freshening occurs preferentially to the right of the lpsm whereas the opposite occurs in case s1 rightward trajectory due to preferential freshening above the drain 3 3 sensitivity analysis fig 8 depicts trends in t and v for scenario b i e the field scale models with a lpsm relative to the dimensionless variables k and λ using otherwise the base case parameters listed in table 1 results show that both t and v which reflect the influence of the lpsm on leaching timeframes and required volumes of irrigation water are non monotonic functions of k and λ whereby optimal values i e that lead to maximum t and v of the lpsm hydraulic conductivity and length are apparent the maximum t arises when k is 2 5 and λ is 0 5 while the peak v occurs with k and λ values of 3 and 0 5 respectively variations in both t and v with changes in k show asymmetric behavior i e both t and v sharply increase and then gradually decline with decreasing k for k values less than 2 5 the lpsm hydraulic conductivity has a somewhat similar effect on t and v indicating that the accompanying reduction in freshwater influx through the lpsm is inconsequential when k is low the lpsm length i e λ is the key parameter controlling freshwater influx and consequently t and v values of t and v plateau i e become insensitive when λ falls between 0 4 and 0 5 while both t and v are lower for smaller i e 0 4 and larger i e 0 5 values of λ note that when λ is set to small i e 0 2 or large i e 0 7 values the installation of the lpsm may lead to negative values for both t and v that is the lpsm may cause longer leaching timeframes and require larger volumes of irrigation water to leach salt from the target leaching area relative to the case without a lpsm this highlights the importance of applying lpsms with appropriate characteristics fig 9 illustrates trends in le in response to variations in k and λ for scenarios a i e field scale models with a subsurface barrier and b i e field scale models with a lpsm note that the base case parameters used in scenario a are consistent with those in scenario b which were listed in table 1 contours of le in fig 9 indicate that the lpsm system scenario b fig 9b outperforms the subsurface barrier scenario a fig 9a on the basis of the leaching efficiency in both cases maximum le values are apparent equal to 0 77 and 1 0 for scenarios a and b respectively le contours from scenario a tend towards the vertical fig 9a indicating greater sensitivity to k than λ whereas the opposite is apparent for scenario b fig 9b in which mostly horizontal contours indicate greater sensitivity to λ this arises primarily from the salt trapped above the barrier in scenario a which is linked directly to the barrier s hydraulic conductivity i e k additionally horizontal flow beneath the lpsm in scenario b instead of the vertical flow without the lpsm diminished the influence of increasingly smaller permeabilities of the lpsm relative to the optimal value on salt leaching within the near drain area given the flattening of gradients in plots of le versus k and λ it follows that the ideal lpsm length is one that is towards the lower end of the ideal range taking into account that construction costs will increase with λ i e λ 0 4 for the case considered scenario b to further reduce material costs a relatively thin lpsm thickness λ 2 with relatively low hydraulic conductivity km is also suggested 3 4 predictive solution of leaching timeframe the approximate analytical solution of youngs and leeds harrison 2000 was evaluated by comparing analytical and numerical modeling results for a range of hypothetical scenarios based on the field scale models of the drainage lpsm system this analysis adopted the lpsm with k 3 and λ 0 45 i e values that led to optimal values leaching performance see fig 9b and different values for k 0 d and s see table 3 the resulting leaching periods obtained from modeling results are given in fig 10 which reveals a generally reasonable match between the two approaches as differences between analytical and numerical results were within 20 the analytical solution was assumed to provide adequate estimates of leaching timeframes at least for the conditions considered in this study therefore even though the analytical solution neglects flow through the lpsm it appears that lpsm throughflow makes a relatively minor contribution to the leaching timeframes 4 conclusions this study explored the effects of adding a low permeability surface mulch lpsm to a subsurface drainage system for the purposes of enhancing the leaching efficiency of highly saline soils e g as might be found in reclaimed coastal regions under continuous freshwater ponding sand tank experiments to examine the effectiveness of the proposed method were found to be in good agreement with numerical simulations an approximate analytical approach applied previously to evaluate salt leaching using progressive ponding also showed good agreement with numerical model results the main conclusions are as follows 1 the lpsm acted to reduce hydraulic heads underneath the lpsm leading to more evenly distributed heads between subsurface drains than occurred without the lpsm 2 the modified head distribution caused by the lpsm led to more even distributed groundwater velocities which enhanced salt flushing in regions furthest from the drain where leaching was slowest without the lpsm 3 more uniform leaching led to shorter leaching periods and smaller volumes of applied freshwater by up to 60 and 80 respectively at the sand tank scale 4 the sensitivity analysis based on the field scale models revealed that the placement of a horizontal flow barrier at the surface led to improved performance in terms of salt leaching rates compared to a buried subsurface horizontal barrier which was explored previously by yu et al 2018 5 an existing solution was compared to numerical modeling results showing its applicability for estimating leaching timeframes for the design of drainage lpsm schemes the results from this study provide preliminary guidance to design subsurface drainage lpsm schemes however some practical factors such as soil heterogeneity transient conditions buoyancy evapotranspiration including evapo concentration of soil salts and three dimensional flow effects may affect the performance of the lpsm in terms of salt leaching further studies should aim to incorporate these factors to investigate the salt leaching process in situations of a lpsm credit authorship contribution statement jiaxu zhang conceptualization methodology investigation writing original draft adrian d werner investigation writing review editing chunhui lu conceptualization investigation supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments c lu acknowledges the financial support from the national natural science foundation of china 51679067 and 51879088 fundamental research funds for the central universities b200204002 and the natural science foundation of jiangsu province bk20190023 
173,vegetation is one of the most important components of nature based coastal protection due to its ability to dissipate wave energy to quantify the wave attenuation by vegetation traditional analytical models assume rigid vegetation and use bulk drag coefficient cd or effective blade length le techniques to consider the effects of blade motion where cd and le are conventionally fitted as a function of kc the keulegan carpenter number and cal with ca the cauchy number and l the ratio of the blade length to wave excursion respectively these parameters do not include the full blade dynamics and so the empirical formulas of cd and le are different for varying vegetation with blade dynamics to obtain analytical solutions for cd and le an analytical wave attenuation model for flexible vegetation and kelp was developed in this study by simplifying and linearizing the blade motion compared with a wide range of experiments for both submerged vegetation and suspended kelp canopies the simplified analytical model underestimated the wave decay coefficient kd by 27 but with a small nrmse normalized root mean square error by the range of the measured data of 0 054 in comparison the numerical model with full nonlinearity underestimated the wave decay coefficient by 11 7 with nrmse 0 063 to reduce the underestimation of the analytical model due to the simplification and linearization a modification factor defined as the ratio of the numerically calculated kd and the analytically calculated kd was fitted with the modification factor the underestimation of the analytical model was reduced to 10 1 based on the analytical model analytical solutions for cd and le were derived which showed a similar precision with the experimentally fitted cd and le based on kc and cal respectively thus the analytical solutions for cd and le could be a reliable alternative when the experimentally calibrated cd and le are not available using the analytical wave attenuation model a case study showed the wave attenuation by cultivated saccharina latissima changes seasonally with the kelp growth when the kelp blade reaches 2 4 m long after 7 months of growth the kelp farms with 50 longlines over a distance of 200 m in the direction of wave propagation in 8 m deep water may attenuate wave energy by 29 for 1 m high waves with the period of 6 s the wave attenuation can be enhanced to 43 when the farms are located in 5 m deep water to provide considerable wave attenuation of kelp with adequate long blades around the year biennial and multiple partial harvesting techniques are recommended keywords wave attenuation vegetation kelp bulk drag coefficient effective blade length seasonal impacts 1 introduction coastal communities are exposed to the increasing risks of coastal erosion and flooding from storm tides and rising sea levels izaguirre et al 2011 tebaldi et al 2012 ondiviela et al 2014 weinkle et al 2018 conventional hard structures are recognized to have adverse impacts on the environment and are less sustainable in a changing climate syvitski et al 2009 currin et al 2010 pace 2011 temmerman et al 2013 sutton grier et al 2015 there is a growing need to use nature based infrastructures for coastal defense morris et al 2019 möller 2019 zhu et al 2020a nature based infrastructures include wetland plants mangroves aquatic vegetation kelp beds coral reefs and shellfish reefs based on the vertical position of the biomaterial in the water column nature based infrastructures are classified as either submerged e g submerged aquatic vegetation located at the seafloor emerged e g saltmarsh and mangroves located at the seafloor and emerged out of the water surface suspended e g kelp and mussel aquacultures suspended in the water column with gaps below and above the canopy or floating e g floating wetland canopies these canopies have the potential to protect coastal regions by damping wave energy without the adverse effects of hardened shorelines the wave attenuation by canopies has been investigated with laboratory and field experimental techniques as well as analytical and numerical models many of these methods are based on the wave attenuation theories developed by dalrymple et al 1984 and kobayashi et al 1993 assuming a rigid canopy component referred to as blade herein and after without motion wave dissipation is dependent on the work of the canopy drag and therefore is proportional to the cube of the relative velocity between the flow and the blade neglecting the blade motion can overestimate wave attenuation in an effort to represent these uncertainties a bulk drag coefficient cd approach has been applied e g kobayashi et al 1993 mendez and losada 2004 alternatively luhar and nepf 2016 and luhar et al 2017 proposed a technique that considers the effects of blade motion by using a reduced effective blade length le rather than the bulk drag coefficient cd that reduces the original drag coefficient cd for a rigid blade the effective blade length le is defined as the length of a rigid blade that dissipates the same wave energy as the flexible blade with the original length l the bulk drag coefficient and effective blade length methods reduce the complexity of modeling the wave vegetation interaction so that these models are computationally efficient and convenient to implement in large scale models however numerous experiments are required to calibrate cd and le conventionally cd is expressed as a function of the reynolds number re or keulegan carpenter number kc independent from vegetation flexural rigidity thus the re and kc based empirical formulas for cd are different for vegetation with different flexibilities e g the different formulas in mendez and losada 2004 bradley and houser 2009 sánchez gonzález et al 2011 jadhav et al 2013 anderson and smith 2014 and ozeren et al 2014 as summarized in chen et al 2018 and van veelen et al 2020 the empirical formula for le is expressed as a function of the cauchy number ca incorporating vegetation flexural rigidity accurate parameterizations of cd and le are important to predict wave attenuation fringer et al 2019 which requires a full understanding of wave vegetation interaction to quantify the blade motion in waves the flexible blade is modeled as a cantilever beam using the euler bernoulli beam approach by simplifying the blade motion to a balance of drag force and blade bending resistance mullarney and henderson 2010 obtained linear normal mode solutions for the blade displacement along the length of the blade the model was recently extended to include the effects of buoyancy by henderson 2019 with the normal mode technique zhu et al 2020a obtained frequency dependent analytical solutions for blade displacements in random waves considering the effects of inertial forces as the analytical solutions are limited to small amplitude blade motion a more precise solution for the large amplitude blade motion can be obtained with numerical techniques e g zeller et al 2014 zhu and chen 2015 luhar and nepf 2016 zhu et al 2018 chen and zou 2019 chen and zou 2020b the consistent mass cable model described in zhu et al 2020b captured an asymmetric whip like blade motion with accurate modeling of blade motion the cable model based wave attenuation model showed a good agreement with the experiments for suspended canopies though underestimating the wave decay coefficient by 10 zhu et al 2021 however the analytical solutions are still useful to describe the mechanisms that influence blade motion related to wave attenuation the analytical solutions are also computationally efficient and easy to implement into large scale models such as swan simulating waves nearshore suzuki et al 2012 wave attenuation by vegetation is determined by wave conditions and vegetation characteristics including morphological e g plant canopy density and blade length width and thickness and mechanical properties e g blade flexural rigidity the vegetation characteristics depend on the growth of vegetation and therefore show a seasonal variation möller and spencer 2002 möller 2006 koch et al 2009 most salt marsh grasses such as spartina alterniflora marsooli et al 2017 spartina anglica schulze et al 2019 spartina foliosa foster martinez et al 2018 salicornia pacific foster martinez et al 2018 and elymus athericus schulze et al 2019 contribute more wave energy dissipation during summer than winter and spring due to greater plant stem stiffness canopy height and aboveground biomass in summer than other seasons most seagrasses such as ruppia maritima chen et al 2007 zostera marina hansen and reidenbach 2013 and zostera noltii paul and amos 2011 also show a similar pattern to dissipate more wave energy in summer with greater blade length blade width canopy height and canopy density in summer than other seasons in winter more vegetation breaks or dies off reducing the wave attenuation capacity marsooli et al 2017 vuik et al 2018 therefore ondiviela et al 2014 proposed that the large long living and slow growing seagrass species e g posidonia oceanica with biomass largely independent of seasonal fluctuations may provide favorable coastal protection wave attenuation by kelp is also dependent on wave conditions and the morphological and mechanical properties of kelp for example sparse 0 1 plants m2 macrocystis pyrifera giant kelp forests was observed to have no significant effects on wave attenuation elwany et al 1995 similar results were also observed for highly flexible nereocystis luetkeana bull kelp gaylord et al 2003 and deeply submerged ecklonia radiata 10 of the water column morris et al 2019 however dense 25 plants m2 laminaria hyperborea tangle in shallow water 5 m could attenuate 70 85 of wave energy over 258 m into the canopy along the direction of wave propagation mork 1996 unlike the wild kelp that grows on the seafloor cultivated kelp is commonly suspended in the water column supported by a mooring system since the kinetic energy of intermediate and deep water waves is concentrated near the surface suspended kelp farms with a dense plant density are believed to dissipate more wave energy than naturally occurring kelp beds with a sparse plant density zhu et al 2021 with laboratory experiments zhu et al 2021 demonstrated that saccharina latissima sugar kelp aquaculture farms with 20 longlines of 1 m long blades and 100 blades m at 1 1 m below the water surface have the potential to dissipate 12 5 energy of 0 35 m high waves with the period of 6 3 s in 4 m deep water the wave attenuation can be further improved by planting the kelp more densely and adding more kelp longlines zhu et al 2021 cultivated kelp grows seasonally resulting in varying performances for wave attenuation which are not fully understood considering seasonal variations of natural materials is essential to understand their capacity for wave attenuation select appropriate species and design strategies for nature based coastal protection this study aims to develop an analytical wave attenuation model that resolves the blade motion and investigates the wave attenuation characteristics of submerged flexible vegetation and suspended cultivated kelp the analytical wave attenuation model was derived by simplifying and linearizing the blade motion the analytical model was compared with the experiments in luhar et al 2017 and lei and nepf 2019b for submerged vegetation and the experiments in zhu et al 2021 for suspended kelp canopies with the analytical model the analytical solutions for the bulk drag coefficient and effective blade length were derived and compared with the fitted values from the experimental data after the data model comparison the analytical model was then used to analyze the seasonal performance of submerged vegetation and suspended canopies for wave attenuation as well as the implications for nature based coastal protection 2 theory various vegetation and kelp species have different morphologies each with its own associated mechanics denny and gaylord 2002 which influence the selection of appropriate techniques to model the dynamics of vegetation and kelp in this study seagrass zostera marina and sugar kelp saccharina latissima common species in maine usa are selected as representatives for submerged vegetation and suspended kelp respectively fig 1 zostera marina is composed of roots rhizome sheath and leaves blades there is one or several leaves fixed on the sheath the sheath is short but much more rigid than the leaves sugar kelp saccharina latissima consists of holdfast stipe and blade the stipe of saccharina latissima is very short compared to the blade but much more rigid than the blade in the development of the wave attenuation model the seagrass sheath and kelp stipe are considered rigid while the seagrass leaves and kelp blade are considered flexible the seagrass leaf and kelp blade are commonly modeled using the cantilever beam theory for other species that have different morphologies other methods might be considered for example for the kelp species with a long stipe two dynamic models are commonly used mass damped spring model for eisenia arborea and pterygophora californica where the stipe is treated as a cantilever type spring with entire effective mass concentrated at the free end of the stipe denny et al 1998 and buoy on rope model for bull kelp nereocystis luetkeana and macrocystis pyrifera where the pneumatocyst is modeled as a buoy and the stipe is modeled as a thin straight non buoyant rope utter and denny 1996 denny et al 1997 stevens et al 2001 zhu et al 2020a different terminologies are used for seagrass and kelp although their physical forms are similar fig 1 for the convenience of description in the model development flexible blade is used to describe the vegetation leaf and kelp blade 2 1 model set up a three layer model is used to derive the wave attenuation for submerged and suspended canopies fig 2 the horizontal coordinate x is positive in the direction of wave propagation with x 0 at the leading edge of the canopy and x lv at the ending edge the vertical coordinate z is positive upward with z 0 at the still water level swl the canopy height d 2 is defined as the average submerged length l of the canopy blades the thicknesses of the water layers above and below the canopy are d 1 and d 3 respectively the water depth from the swl is h d 1 d 2 d 3 where the seafloor is located at z h and assumed horizontal this generalized three layer model can be used to analyze the wave attenuation characteristics of i submerged d 1 0 and d 3 0 ii emerged d 1 0 and d 3 0 iii suspended d 1 0 and d 3 0 and iv floating d 1 0 and d 3 0 canopies based on linear wave theory dean and dalrymple 1991 the frictionless horizontal wave orbital velocity u 0 is given by 1 u 0 h 2 ω γ z cos k x ω t where h is the wave height ω is the wave angular frequency t is the time γ z cosh k h z sinh kh is the vertical decay factor and k is the wave number obtained from the dispersion relation ω2 gktanh kh with g the gravitational acceleration due to the presence of a canopy the flow velocity in the canopy decreases and displays a phase shift relative to the flow velocity above the canopy lowe et al 2005 rosman et al 2013 henderson et al 2017 for simplicity the phase shift is not considered and the magnitude reduction is considered using a factor α w following lowe et al 2005 such that the within canopy velocity can be estimated by 2 u α w u 0 where the factor α w is calculated using the numerical solution in lowe et al 2005 2 2 blade motion the flexible blade can be modeled as a cantilever beam using the cable model described in zhu et al 2020b 2021 where the blade motion is governed by the balances of blade inertia blade bending resistance blade tension blade weight buoyancy friction and hydrodynamic force the vertical blade displacement geometrical nonlinearity of the blade deflection and the vertical forces including friction buoyancy and tension create a difficulty to obtain analytical solutions and therefore are neglected in this study the effects of neglecting these terms on blade motion and wave attenuation are assessed later in section 3 2 thus the governing equation for the horizontal blade displacement ξ is then simplified as the balance of blade inertia blade bending resistance and hydrodynamic force which is expressed as 3 ρ v a c ξ ei ξ f x where ξ is a function of time t and the distance s along the blade length from the fixed end s 0 the dot indicates the derivative with respect to t the prime indicates the derivative with respect to s ρ v is the blade mass density ac is the blade cross section area e is the elastic modulus i is the second moment of the cross section and fx is the hydrodynamic force per unit length the relation between s and z fig 2 is 4 z d 1 d 2 s blade fixed at the bottom end d 1 s blade fixed at the top end according to the morison equation morison et al 1950 the hydrodynamic force fx is decomposed into virtual buoyancy fvb drag fd and added mass force fam and is given by 5 f x f v b f d f a m with 6 f v b ρ w a c u 7 f d 1 2 c d ρ w b u ξ u ξ and 8 f am c m ρ w a c u ξ where ρ w is the water density b is the projected width of the blade cd is the drag coefficient and cm is the added mass coefficient for the blade with rectangular cross section the drag coefficient cd and added mass coefficient cm are 9 c d max 10 k c 1 3 1 95 and 10 c m min c m 1 c m 2 respectively with c m 1 1 0 35 k c 2 3 kc 20 1 0 15 k c 2 3 kc 20 and c m2 1 kc 18 2 49 luhar 2012 luhar and nepf 2016 in 9 and 10 kc umt b is the keulegan carpenter number with t 2π ω the wave period and um the magnitude of the water velocity relative to the blade yielding a non constant cd and cm the formulas are fitted from the experimental data in keulegan and carpenter 1958 and sarpkaya and o keefe 1996 for rigid flat plates in oscillatory flows with r 2 0 972 and 0 912 for cd and cm fig 3 respectively to obtain an analytical solution the nonlinear drag is linearized as 11 f d 1 2 c d ρ w b u ξ u ξ c u ξ where the linearization coefficient c is obtained from the lorentz s condition of equivalent work sollitt and cross 1972 this requires that the nonlinear and linear drag accounts for the same amount of energy dissipation averaged over one wave period such that d 1 d 1 d 2 1 2 c d ρ w b u ξ u ξ 2 d z d 1 d 1 d 2 c u ξ 2 d z yielding 12 c d 1 d 2 d 1 1 2 c d ρ w b u ξ u ξ 2 d z d 1 d 2 d 1 u ξ 2 d z 1 2 c d ρ w b u e r in 12 the overline indicates the time average over one wave period and the equivalent relative velocity magnitude u e r d 1 d 2 d 1 u ξ u ξ 2 d z d 1 d 2 d 1 u ξ 2 d z the linearization coefficient c is equivalent to the viscous damping coefficient per unit length with the unit of ns m2 eq 12 also shows that c is dependent on the equivalent relative velocity magnitude uer the linearization of the drag force in random waves is typically done using the borgman 1967 method based on the distribution of the wave orbital velocity resulting in a different expression of the linearization coefficient in zhu et al 2020a substituting 1 5 with linearized drag 11 into 3 yields 13 m ξ c ξ ei ξ h 2 ω γ c cos kx ω t ω m i sin kx ω t where m ρ v cmρ w ac and mi 1 cm ρ w ac the boundary conditions for a cantilever beam are set as ξ 0 t 0 ξ 0 t 0 ξ l t 0 and ξ l t 0 solving 13 with the normal mode approach rao 2007 yields 14 ξ h 2 γ γ s sin k x ω t γ c cos k x ω t in 13 γ s and γ c are the transfer functions and expressed as 15 γ s ω γ n 1 ϕ n ω i n λ n 2 ω 2 d n 2 ζ n λ n ω λ n 2 ω 2 2 2 ζ n λ n ω 2 and 16 γ c ω γ n 1 ϕ n d n λ n 2 ω 2 ω i n 2 ζ n λ n ω λ n 2 ω 2 2 2 ζ n λ n ω 2 where ϕ n cos μ n l cosh μ n l sin μ n s sinh μ n s sin μ n l sinh μ n l cosh μ n s cos μ n s is the nth normal mode of the cantilever beam with μ n the nth solution of 1 cos μlcosh μl 0 λ n μ n 2 e i m is the nth natural frequency of the blade 2ζ n λ n c m d n 0 l c γ ϕ n d s 0 l m ϕ n 2 d s and i n 0 l m i γ ϕ n d s 0 l m ϕ n 2 d s since γ is expressed in z and ϕ n is expressed in s the relation between s and z in 4 is required to calculate the integral 0 l γ ϕ n d s substituting 14 into 12 yields the expression of the linearization coefficient in terms of the transfer functions 17 c 1 2 c d ρ w b h 2 ω 8 3 π d 1 d 2 d 1 γ 3 1 γ s 2 γ c 2 3 2 d z d 1 d 2 d 1 γ 2 1 γ s 2 γ c 2 d z the linearization coefficient can be obtained iteratively starting from a static blade an initial c is calculated from eq 17 by assuming γ s 0 and γ c 0 once the transfer functions γ s and γ c are obtained from 15 and 16 c can be updated from 17 the procedure is repeated until a convergent solution is obtained 2 3 wave attenuation the wave energy dissipation is assumed to be attributed to the work of the canopy drag following zhu et al 2020a such that 18 e c g x d 1 d 1 d 2 n α ϵ f d u ξ dz where e ρ w gh 2 8 is the local wave energy per unit horizontal area cg ω 1 2kh sinh 2kh 2k is the wave group velocity n is the canopy density defined as the number of blades per unit horizontal area and αε 1 is a factor to consider the sheltering effects between blades with αε 1 for no sheltering the factor αε is calibrated through experiments substituting the quadratic drag force 7 into 18 yields the transmitted wave height at distance x in relation to the incident wave height h 0 at x 0 19 h x h 0 1 1 k d h 0 x where the wave decay coefficient kd is expressed as 20 k d 4 α c d b n k 2 3 π sinh k h 2 k h sinh 2 k h d 1 d 2 d 1 1 γ s 2 γ c 2 3 cosh 3 k h z d z it should be noted that eq 19 is obtained using quadratic drag using the linearized drag 11 yields an exponential decayed wave height as h h 0 e k d h 0 x which is also often used in practice e g kobayashi et al 1993 méndez et al 1999 zhu and zou 2017 the two wave decay forms are linked through a piecewise method appendix c of zhu 2020 the piecewise function method indicates that the exponential decay form with linearized drag would overestimate the wave attenuation therefore the fractional decay form in 19 is recommended zhu 2020 however for weak wave attenuation such that kdh 0 x 0 5 the difference between 1 1 kdh 0 x and e k d h 0 x is less than 10 for the canopies with unsheltered rigid blades such that αε 1 γ s 0 and γ c 0 the solution 20 reduces to the solution in zhu and zou 2017 i e 21 k d r c d bnk 9 π 9 sinh k d 2 d 3 9 sinh k d 3 sinh 3 k d 2 d 3 sinh 3 k d 3 sinh kh 2 kh sinh 2 kh for bottom rooted vegetation such that d 3 0 solution 21 can be further reduced to the solutions by dalrymple et al 1984 and kobayashi et al 1993 for shallow water waves with kh 0 1π 21 reduces to 22 k d r c d b n k l 3 π h 2 indicating that the wave decay coefficient is proportional to the drag coefficient canopy density blade width and blade length but inversely proportional to the wavelength and the square of water depth thus the wave attenuation decreases more dramatically with increasing water depth than other parameters for shallow water waves according to 22 the wave attenuation is independent from the vertical position of the canopy since the horizontal wave orbital velocity minimally decays to the bottom 2 4 bulk drag coefficient and effective blade length according to 18 the wave dissipation ratio is proportional to d 1 d 1 d 2 f d u ξ dz l 0 c d u r 3 ds with u r u ξ the relative velocity obtaining the wave dissipation ratio requires computation of the relative velocity in order to consider the wave blade interaction to reduce the computation for the relative velocity including resolving the blade motion the bulk drag coefficient and effective blade length methods with a rigid blade assumption are often used especially for implementation into large scale models the bulk drag coefficient cd is a reduced drag coefficient such that 0 l c d u 3 ds 0 l c d u r 3 ds while the effective blade length le is a reduced blade length such that 0 l e c d u 3 ds 0 l c d u r 3 ds fig 4 traditionally cd and le are calibrated with experiments or numerical models however with the analytical model developed in section 2 3 analytical solutions for cd and le can be obtained to derive the solution for the bulk drag coefficient replacing cd in 21 by cd and substituting the result into 20 yields 23 c d 12 k α ϵ d 1 d 2 d 1 cosh 3 k h z 1 γ s 2 γ c 2 3 dz 9 sinh k d 2 d 3 9 sinh k d 3 sinh 3 k d 2 d 3 sinh 3 k d 3 c d for shallow water waves with kh 0 1π 23 reduces to 24 c d α ϵ d 2 d 1 d 1 d 2 1 γ s 2 γ c 2 3 dz c d to derive the solution for the effective blade length replacing d 2 in 21 by le and substituting the result into 20 yields 25 9 sinh k l e d 3 sinh 3 k l e d 3 9 sinh k d 3 sinh 3 k d 3 12 k α ε l 0 cosh 3 k h z 1 γ s 2 γ c 2 3 ds for submerged vegetation with d 3 0 25 reduces to 26 9 sinh k l e sinh 3 k l e 12 k α ε l 0 cosh 3 k h z 1 γ s 2 γ c 2 3 ds when le is far less than the wave length with kle 0 1π 26 reduces to 27 l e α ε l 0 cosh 3 k h z 1 γ s 2 γ c 2 3 ds for shallow water waves with kh 0 1π 25 reduces to 28 l e α ε l 0 1 γ s 2 γ c 2 3 ds dividing 24 by 28 yields 29 l e l c d c d indicating that the rate of blade length reduction is equal to the rate of drag coefficient reduction in shallow water waves these reductions reflect the decreases in wave attenuation resulting from the blade motion eq 29 holds because the wave decay coefficient of rigid canopies is proportional to the drag coefficient and blade length in shallow water waves as shown in 22 3 results 3 1 model data comparison 3 1 1 experimental data description the analytical model was compared with the experiments in luhar et al 2017 and lei and nepf 2019b for submerged vegetation and the experiments in zhu et al 2021 for suspended kelp canopies in the experiments for submerged vegetation luhar et al 2017 lei and nepf 2019b the artificial vegetation consisted of six 13 cm long 0 3 cm wide and 0 1 mm thick flexible blades to model seagrass leaves and one 1 cm long solid wood cylinder to model seagrass sheath the diameters of the wood cylinders were 7 8 mm in luhar et al 2017 and 6 9 mm in lei and nepf 2019b the flexible blades were made from low density polyethylene ldpe film with ρ v 0 92 g cm3 and e 0 3 gpa the blades were located separately such that the sheltering factor αε 1 the canopy density was 280 to 1800 stems m2 with 1680 to 10 800 blades m2 the canopy lengths were 5 m in luhar et al 2017 and 3 5 7 m in lei and nepf 2019b the wave height was 1 to 11 2 cm the wave period was 0 8 to 2 s and the water depth was 16 to 45 cm thus the dimensionless parameters are kh 0 44 2 7 l h 0 29 0 8 kc 9 135 ca 64 3796 l 2 30 and γ 0 16 0 30 where c a ρ w b u m 2 l 3 e i is the cauchy number l lω um is the ratio of the blade length to the wave excursion um ω and γ βs 1 2 with β g ρ w ρ v a c t 0 5 ρ w c d l b u m the dimensionless buoyancy defined in henderson 2019 and s e i t 0 5 ρ w c d l 4 b u m the dimensionless stiffness defined in mullarney and henderson 2010 and henderson 2019 the value of γ determine the importance of buoyancy relative to stiffness with γ 1 indicating buoyancy is less important than stiffness and γ 1 indicating buoyancy is more important than stiffness details of the experiments for the submerged vegetation can be found in luhar et al 2017 and lei and nepf 2019b in the experiments for suspended kelp canopies the kelp blade was modeled using silicon film with ρ v 1 2 g cm3 and e 2 04 mpa the model blade was 10 16 cm long 0 95 cm wide and 0 1 mm thick the suspended kelp canopy consisted of 20 rows of blades and the rows were 20 cm apart for each row there were 31 aggregates of blades with one aggregate cm for each aggregate ten blades were fixed together at the top end of the blades the fixed part of the blade was lr 0 5 cm while the flexible free part was lf 9 66 cm the fixed part of the blades was used to model the effects of stipes the sheltering effects between the blades in an aggregate were considered using a sheltering factor αε 0 630 the canopy length was 3 8 m and the canopy density was 526 3 aggregates m2 with 5263 blades m2 three vertical positions of the suspended canopy beneath the still water level with d 1 6 11 and 16 cm were compared in the experiments the incident wave height was 1 8 to 3 8 cm wave period was 0 8 to 2 s and water depth was 30 to 40 cm with k h 0 58 2 33 lf h 0 24 0 32 kc 4 4 18 8 ca 9667 43 406 l 3 4 14 4 and γ 5 16 2 79 the details of the experiments can be found in zhu et al 2021 the canopy characteristics and wave conditions are also summarized in table 1 3 1 2 comparisons for wave attenuation the analytical wave attenuation model developed in this work is compared with the experiments in luhar et al 2017 and lei and nepf 2019b for submerged vegetation and the experiments in zhu et al 2021 for suspended kelp to evaluate the performance of the analytical model the calculations using the numerical model in zhu et al 2021 are also presented the comparisons for the wave height decay along the canopy are shown in fig 5 the measured wave height is featured with oscillations along the canopy the oscillation reflects the partially standing wave induced by the wave reflection and therefore is spatially periodic with the period of the scale of half wavelength luhar et al 2017 the analytical results are similar to the numerical results for the cases in fig 5 a c but for the cases in fig 5 b d the analytical model demonstrates a smaller wave decay than the numerical model this is because the simplification of the analytical model ignored the effects of the buoyancy tension and geometrical nonlinearity which will be discussed in section 3 2 to evaluate the overall performance of the analytical model the analytically and numerically calculated wave decay coefficients kd are compared with the experimental data in fig 6 the numerical results show an underestimation of 11 7 calculated using the slope of the linear fitted line between the calculated and measured values the same hereinafter with the normalized root mean square error nrmse 0 063 fig 6a the normalization is calculated using the range defined as the maximum value minus the minimum value of the measured data compared to the numerical model the analytical model underestimates the wave decay coefficient by 27 fig 6b however the nrmse of the analytical model is smaller at 0 054 indicating that the analytical model has a similar and even higher precision than the numerical model in calculating the wave attenuation 3 1 3 bulk drag coefficient and effective blade length bulk drag coefficient and effective blade length methods are simple ways to consider the effects of blade motion on wave attenuation the values of cd and le are for the flexible part of the blade with a length of lf in this section the analytical solutions for cd 23 and le 25 are presented and compared with the experiments in luhar et al 2017 and lei and nepf 2019b for submerged canopies as well as the experiments in zhu et al 2021 for suspended canopies the analytically calculated cd is compared with the fitted and measured cd in fig 7 where cd is expressed as a function of kc since kc does not include all the parameters governing the blade motion such as blade flexural rigidity the blade motions with different flexural rigidities might be different yielding different values of cd although with the same kc thus cd is scattered especially for the submerged vegetation dataset with more cases and wave conditions resulting in varying blade motion fig 7c for kc based cd the analytically calculated cd has a larger nrmse of 0 258 than the fitted cd with nrmse 0 181 for the suspended kelp canopy fig 7a similarly for submerged vegetation fig 7c the nrmse of the analytically calculated cd is 0 278 which is larger than the fitted cd with nrmse 0 191 in the comparisons for the wave decay coefficient fig 7b d the nrmse of the kd calculated using the analytically calculated cd are 0 187 for the suspended kelp and 0 054 for the submerged vegetation which are larger than those kd calculated using the fitted cd with nrmse 0 070 for the suspended kelp and 0 051 for the submerged vegetation the analytically calculated effective blade length of the flexible part of the blade l f e is compared with the measured and fitted l f e as well as the empirical formula in lei and nepf 2019b in fig 8 where l f e is expressed as a function of cal in the selected experiments the ratio of the blade length to the wave excursion was l 2 0 and the cauchy number was ca 64 for ca 1 and l 1 the effective blade length can be scaled as l f e l f c a l 1 4 l f e l f cal 1 4 by assuming the static balance between the drag and stiffness luhar and nepf 2016 for l 1 the effective blade length can be scaled as l f e l f c a 1 3 l f e l f c a 1 3 luhar and nepf 2011 however luhar et al 2017 and lei and nepf 2019b have shown that the scaling with cal 1 4 works better for the present experiments based on the measured horizontal forces of a single flexible blade with a wide range of cal from 0 292 to 1 88 104 lei and nepf 2019b developed the empirical formula for effective blade length given by 30 l f e l f 0 94 cal 0 25 where the effective blade length is defined as the reduced length of a rigid blade that provide the same horizontal force as the flexible blade with the full length although this formula is developed based on the horizontal force of a single blade it shows a good comparison with the measured l f e nrmse 0 224 fig 8c and performs well for calculating the decay coefficient kd nrmse 0 050 fig 8d similar to kc based cd cal also does not include all the parameters governing the blade motion therefore there might be several values of l f e for different blade motions with the same cal resulting in scattered l f e especially for the submerged vegetation experimental data with numerous types of blade motions fig 8c as the range of cl 1678 10 444 for the wave attenuation experiments is small the scattering of the data is more obvious for cal based l f e the analytically calculated l f e has a larger nrmse of 0 452 than the fitted l f e with nrmse 0 258 for suspended kelp canopies for submerged vegetation the nrmses are 0 205 for the fitted l f e 0 224 of the calculated l f e with the empirical formula 30 and 0 292 for the calculated l f e using the present analytical model fig 8c for the comparisons for the wave decay coefficients by suspended kelp fig 8b the nrmse of the kd calculated using the analytically calculated l f e are 0 187 which is larger than the kd calculated using the fitted l f e with nrmse 0 107 however for submerged vegetation fig 8d the nrmse of the kd calculated using the analytically calculated l f e is 0 054 which is smaller than the kd calculated using the fitted l f e with nrmse 0 067 the analytically calculated l f e shows a similar nrmse to the empirical formula in lei and nepf 2019b for calculating kd with nrmse of 0 050 fig 8d 3 2 blade dynamics and the simplified analytical wave attenuation model blade dynamics have a significant influence on wave attenuation performance to investigate the effects of blade motion on wave attenuation the blade postures relative velocities ur and drag force flux fdur are analyzed based on the simplified analytical solutions and nonlinear numerical solutions the wave energy dissipation is determined by the drag flux fdur ur 3 as shown in 18 which is proportional to the cube of the relative velocity the blade is so flexible that the free tip moves passively with the wave flow fig 9 a1 b1 resulting in a small relative velocity ur fig 9a2 a3 b2 b3 thus the blade segments near the free tip have little contribution to the wave dissipation take the submerged blade in fig 9b as an example the blade segments with s 0 24l have a small relative velocity ur 0 4max ur fig 9b2 and therefore a small drag flux f d u r 5 max f d u r fig 9b4 indicating little contribution to the wave dissipation in other words for the flexible blade only the blade segments near the fixed end s 0 19l for the suspended blade in fig 9a and s 0 24l for the submerged blade in fig 9b contribute to the wave dissipation this is different from a rigid blade that can dissipate wave energy along the whole blade length the blade segments near the fixed end contribute the most 95 wave dissipation which builds the foundation for the effective blade length method therefore accurate simulations of the dynamics of the blade segments near the fixed end are important to evaluate the wave dissipation the analytical model shows a symmetric motion fig 9a1 b1 due to neglecting the vertical wave orbital velocity and the phase lag induced by blade displacement the analytical model also neglected the vertical blade displacement geometrical nonlinearity of the blade deflection and vertical forces including friction buoyancy and tension by considering all the forces and nonlinearity the numerical model better simulates the blade motion by capturing the asymmetric whip like motion fig 9a1 b1 the asymmetry also reflects the phase shift of the relative velocity ur along the blade length fig 9a3 b3 although the analytically calculated displacements of the segments near the tip are much smaller than those calculated from the numerical model the discrepancy between these calculations for the blade displacements becomes smaller along the blade length toward the fixed end fig 9a1 b1 thus the analytical model predicts a similar drag force flux to the numerical model fig 9a4 b4 for the segments near the fixed end however above this region the analytical model underestimates the drag force flux and therefore underestimates the wave attenuation fig 6 to quantify the effects of the simplifications and linearization of the blade motion in the development of the analytical wave attenuation model the wave decay coefficient calculated from the analytical model is compared with that calculated from the numerical model using the ratio α m defined as 31 α m k d f numerical k d f analytical where k d f is the wave decay coefficient of the flexible blades without rigid sheaths or stipes such that k d f kd k d r with kd the wave decay coefficient of the canopy and k d r the wave decay coefficient of the rigid sheaths or stipes and the subscript numerical and analytical indicate the values are calculated numerically and analytically respectively for the cases in this study the ratio α m showed the following relations with the normalized blade motion amplitude at the blade tip by the blade length ξ ml l i e 32 α m 0 20 0 03 ξ ml l 0 50 0 04 0 015 ξ ml l 0 155 with r 2 0 933 for suspended kelp fig 10 a and 33 α m 0 92 0 03 ξ ml l 0 216 0 014 0 039 ξ ml l 0 449 with r 2 0 863 for submerged vegetation fig 10b since the numerical results reduce to the analytical results when ξ ml l 1 α m approaches to 1 as ξ ml l approaching to 0 unfortunately formulas 32 and 33 cannot capture this trend due to limited data therefore 32 and 33 are limited to apply for the given range of ξ ml l to address this issue an improved formula for α m will be developed with more data in the future with the ratio α m the analytical model can be modified by multiplying the modification factor α m the modified analytical results underestimate the wave decay coefficient by only 10 1 fig 10c while the original analytical results underestimate the wave decay coefficient by 27 fig 6b the modification improved the analytical wave attenuation model by reducing the underestimation by 27 10 1 27 62 6 however the nrmse increased 22 2 to 0 066 fig 10c from 0 054 fig 6b 3 3 case study for wave attenuation in different seasons the analytical model is then used to investigate the seasonal wave attenuation capacity of kelp farms compared with seagrass meadow over one year to represent the seagrass and kelp zostera marina eelgrass and saccharina latissima sugar kelp are used which are common species at the coast of maine in the usa regarding the parameters of seagrass the designed length of the seagrass leaf follows the measured data in gaeckle and short 2002 for the zostera marina at coastal maine usa fig 11 a the corresponding leaf width and thickness as well as the length and width of the sheaths are calculated using the relationship formulas in abdelrhman 2007 and shown in fig 11a the designed mass density and elastic modulus are 700 kg m3 abdelrhman 2007 and 0 26 gpa fonseca et al 2007 respectively the designed density is 335 shoots m2 and the leaf number is 3 for each shoot such that the density is 1005 leaves m2 mattila et al 1999 the water depth for the seagrass is designed as 5 m for the seagrass in this case study γ 0 04 0 16 indicating that the effects of buoyancy are small and negligible for the kelp the designed length l of the kelp blade follows the measured data in augyte et al 2017 for the saccharina latissima in maine usa fig 11b the corresponding blade width b maximum thickness dmax and elastic modulus e are calculated based on the relations with the blade length using the empirical formulas in zhu et al 2021 and shown in fig 11b in zhu et al 2021 e was fitted as an exponential function of dmax which may yield a very large e when dmax exceeds a critical value e g e 100 pa if dmax 1 26 mm to avoid too large values for e the maximum e is set as the measured maximum value 22 mpa in zhu et al 2021 this setting may underestimate the wave attenuation capacity of kelp in summer when the kelp blade is thick it is noted that the blade thickness d decreases from the center of the blade width to both edges following a normal like distribution along the blade width zhu et al 2021 34 d d max 0 797 0 011 e 1 2 s b b 0 118 0 003 2 0 203 0 011 where sb is the distance from the center of the blade width toward the blade edge based on the thickness distribution in 34 the second momentum of the cross section of saccharina latissima is 35 i 1 2 d m a x 1 2 d m a x 2 s b y 2 d y 0 2 b d m a x 3 12 indicating that the flexural rigidity of the real saccharina latissima blade is only 20 of the same wide blade but with the maximum thickness therefore an effective blade width with be 0 2b is used to calculate the flexural rigidity following zhu et al 2021 the designed mass density of kelp is 1053 kg m3 and the plant density is 405 plants m zhu et al 2021 the sheltering factor of α ε 0 630 zhu et al 2021 is assumed to be applicable for this case study due to limited data for the stipes the stipes were not considered in this study which may underestimate the wave attenuation of the kelp farms the kelp longline is designed at 1 2 m below the still water level and 4 m apart in 8 m deep water to compare with the seagrass meadow in the same water depth a shallower water depth of 5 m is also considered for kelp farms since the kelp blade grows longer after may the flexural rigidity decreases such that γ can reach up to 1 67 in may indicating a non negligible role of buoyancy consequently the neglecting of buoyancy might cause uncertainty in the wave attenuation for the wave conditions the designed wave height is 1 m and the wave period is designed as 6 s the drag coefficient cd and added mass coefficient cm are calculated using 9 and 10 the original analytical model without modification is used in this case study to evaluate the wave attenuation performance of the canopies the wave energy dissipation rate edr is used and defined as 36 e d r 1 h l v 2 h 0 2 1 1 1 k d h 0 l v 2 where lv is the canopy length in the direction of wave propagation in this case study the canopy length is designed as 200 m 50 longlines of kelp the wave attenuation by the designed suspended kelp farms and seagrass is shown in fig 12 the wave height decays when propagating through the canopy the transmitted wave height varies seasonally which is more obvious at the trailing edge of the canopy to investigate the wave attenuation capacities of suspended kelp farms and seagrass the edrs at the trailing edge of the canopies are compared in fig 12d the edr shows a similar pattern to the variance of blade length around the year the growth period of kelp has significant impacts on wave attenuation kelp is planted in late november after about 1 5 months of growth in a nursery and grows slowly over the first few months fig 11b resulting in a small edr 10 during this time fig 12d after about five months of growth the kelp blade length exceeds 1 5 m at the end of april yielding considerable wave attenuation with edr 10 in 8 m deep water and edr 20 in 5 m deep water when the kelp blade reaches 2 4 m long in may the wave attenuations of kelp farms in 8 m deep water and 5 m deep water are enhanced to 29 and 43 respectively the wave attenuation remains at this level until the kelp is harvested in june unlike the newly planted kelp with blades growing from millimeters to meters in the growing season the averaged leaf length for a perennial seagrass meadow changes from 15 to 61 cm fig 11a therefore seagrass is less impacted by seasonal growth patterns that are typical of kelp farms compared to kelp farms the seagrass has a larger elastic modulus and canopy density yielding a larger edr 20 than the kelp in the initial growing months with small blade lengths however the wave attenuation capacity of kelp farms surpasses the seagrass in the later months when the kelp blades become longer 1 5 m the edr by seagrass is most significant at 27 in august when the leaf and sheath lengths are the longest 4 discussion 4 1 evaluations and limitations of the analytical wave attenuation model the analytical wave attenuation model performed well in predicting the wave attenuation of suspended and submerged canopies with a small nrmse based on the analytical model the analytical solutions for the bulk drag coefficient cd and effective blade length le were obtained conventionally cd and le are usually fitted as a function of kc and cal respectively the results showed that the analytically calculated cd and le are in a similar precision to the kc based fitting cd and cal based fitting le the kc describes the relative importance of drag force over inertia force on a rigid body and the cal scaling is developed based on the static balance of blade stiffness and drag force the kc based and cal based fitting did not incorporate all the parameters governing the wave induced blade dynamics such as the frequency ratio and therefore influence the precision of the fittings however due to the complicated wave blade interaction sophisticated parameters to obtain good fits e g r 2 0 8 of cd and le for a wide range of wave conditions and blade properties are difficult to achieve thus the analytical solutions for cd and le can be an alternative when reliable cd and le are not available meanwhile the analytical solutions for cd and le have given the parameters that govern cd and le which may also provide insight into the appropriate parameters and relations to obtain a better fit for cd and le for example the ratio of the blade s natural frequency and the wave frequency influences the resonance and therefore is an important parameter that should be incorporated into the development of the empirical formulas for cd and le to obtain an explicit analytical solution for the wave attenuation the blade dynamics are simplified and linearized by assuming negligible vertical forces net buoyancy and friction and a small amplitude blade motion however the simplification and linearization decrease the precision for simulating the blade dynamics and the wave attenuation to improve the model a modification factor α m can be used in this study the formulas 32 and 33 for α m was fitted as a simple power function of the blade motion amplitude at the blade tip normalized by the blade length ξ ml l using limited data with 0 015 ξ ml l 0 155 and 0 039 ξ ml l 0 449 for suspended and submerged canopies respectively the flaw of this fitting is that the calculated α m using 32 and 33 cannot converge to 1 as ξ ml l approaches to 0 when the numerical results reduce to the analytical results for rigid vegetation thus the formulas 32 and 33 have limited application for the given range of ξ ml l a more sophisticated fitting of α m will be developed with more data in the future understanding the effects of neglecting buoyancy and the geometrical nonlinearity is helpful to improve the analytical solution the within canopy water velocity showed a phase lead and magnitude reduction relative to the above canopy water velocity especially for dense canopies lowe et al 2005 rosman et al 2013 henderson et al 2017 in this study the magnitude reduction of the water velocity was considered using the approach in lowe et al 2005 while the phase shift was ignored supposing the phase shift is a constant or varies slowly in time and location it is expected to have no effects on blade motion and wave attenuation because the phase shift is removed by time averaging in the model development however the phase shift changing with time and location may significantly affect blade dynamics and wave attenuation which warrants further investigation as waves propagate into the canopy interior the wave height and wave orbital velocity decrease with distance into the canopy thus the vegetation and kelp are under larger wave conditions at the leading edge of the canopy while under smaller wave conditions at the end of the canopy resulting in different blade dynamics with distance along the canopy the blade displacement is determined by the wave height h and transfer functions γ s and γ c as shown in 14 to obtain the wave attenuation by solving the energy conservation eq 18 the blade displacement is treated using a decaying wave height h x because the wave height is the unknown and distance independent transfer functions γ s and γ c since the effects of wave height decay on blade dynamics were incorporated into the wave attenuation model the effects induced by assuming distance independent γ s and γ c are expected to be small in the prediction of wave attenuation in this study using the wave conditions at the leading edge and trailing edge of the canopy to estimate γ s and γ c was observed to have no difference on the prediction of the wave attenuation this might be explained by the weak wave attenuation of flexible blades in the selected experiments with the minimum transmitted wave height as 75 of the incident wave height the contribution of the rigid part of the blades was removed for large wave attenuation the error induced by assuming distance independent γ s and γ c may appear but it is still expected to be small in wave attenuation prediction because the wave height decay is considered the flexible vegetation and kelp blade have shown significant asymmetric motions the asymmetry of the blade motion is induced by the vertical wave orbital velocity and the phase lag in encountering water velocity caused by blade displacement zhu et al 2020b this asymmetry occurs in linear waves and can be enhanced by wave nonlinearity the mean blade tilt is large at the blade tip and decreases dramatically to the fixed end unfortunately the analytical model cannot capture the asymmetric motion however as the blade segments near the tip with large mean tilt have very little contribution to the wave attenuation and only a few blade segments near the fixed end with small mean tilt dominate the wave attenuation the effects of the large mean blade tilt may be small thus the analytical model could perform well in predicting wave attenuation although the asymmetric blade motion is not well predicted nevertheless the effects of the mean blade tilt of the blade segments near the fixed end on wave attenuation might be significant and worth further study the mean blade tilt might reduce the drag force and therefore reduce the wave attenuation thus neglecting the mean blade tilt might overestimate the wave attenuation for a dense canopy the blades are close to each other yielding sheltering and interaction between blades the sheltering effects can reduce the drag of the sheltered blades and therefore reduce the wave attenuation for long kelp blades blade interaction may result in the entanglement of the kelp blades enhancing blade breakage koehl and wainwright 1977 and therefore influencing the wave attenuation in this study the kelp blade sheltering and interaction was considered using a constant factor which should be improved by scaling with blade properties and configuration as well as wave conditions the analytical model was developed assuming monochromatic waves in the real world waves can be described using random waves with different frequencies the form of the present analytical wave attenuation model for random waves is referred to zhu et al 2020a however for waves with strong nonlinearity and nonlinear wave wave interaction such as infragravity waves mullarney and pilditch 2017 the application of the present model warrants careful consideration in the field the background current also has significant influences on the reconfiguration of vegetation and kelp and waves gaylord et al 2003 rosman et al 2003 losada et al 2016 lei and nepf 2019a fredriksson et al 2020 zhang and nepf 2020 2021 lei et al 2021 which should be also implemented in the future work 4 2 nature based coastal protection strategies the case study showed the wave attenuation capacity of saccharina latissima varies seasonally with the kelp growth after about 6 5 months of growth 1 5 months in the nursery and five months in the ocean the kelp blade grows longer than 1 5 m and can provide considerable wave attenuation for different locations e g different water temperatures and salinities different water depths exposed sites and sheltered sites etc the growth rate may be different peteiro and freire 2013 nielsen et al 2014 augyte et al 2017 vettori and nikora 2017 azevedo et al 2019 large wave conditions with significant wave height greater than 1 5 m associated with storm events typically occur in the gulf of maine for example in winter and spring from october to may data from maine epscor seanet buoy c0502 in saco bay to develop large kelp blades to provide considerable wave attenuation in winter it is proposed that the kelp should be seeded several months earlier as kelp grows faster in cold water the kelp can be submerged near the seafloor during summer and then moved upward to the surface in winter to keep considerable wave attenuation of kelp farms for coastal protection around the year the biennial harvesting technique peteiro et al 2006 could be adopted as a result the kelp older than one year can be several meters long providing more favorable wave attenuation one recommended strategy is to harvest every other longline every other year to improve commercial values the multiple partial harvesting technique bak et al 2018 is also recommended such that only part of the blades is cut off leaving an adequate length of the blade to allow regrowth and provide considerable wave attenuation since the kelp may get biofouled degraded and ragged in summer førde et al 2016 one solution is to submerge the kelp canopy deep in water to improve the survivability peteiro et al 2006 in addition to dissipating wave energy kelp farms can also play a substantial role in marine carbon sequestration for climate change mitigation and adaptation krause jensen and duarte 2016 duarte et al 2017 unlike newly seeded kelp naturally occurring seagrass is less impacted by seasonality implementing seagrass with kelp aquaculture may mitigate the seasonal impacts in addition combining seagrass and suspended kelp aquaculture structures can improve their wave attenuation capacity for a wider range of wave periods and water levels zhu et al 2020a enhancing the size of kelp aquaculture farms and the plant density can also improve the wave attenuation capacity as expected zhu et al 2021 5 conclusion in this study an analytical wave attenuation model for flexible vegetation and kelp was developed by simplifying and linearizing the blade motion compared with a wide range of experiments for submerged and suspended canopies with flexible blades the simplified analytical model underestimated the wave decay coefficient by 27 but with a small nrmse of 0 054 in comparison the numerical model with full nonlinearity underestimated the wave decay coefficient by 11 7 with nrmse 0 063 to reduce the underestimation of the analytical model due to the simplification and linearization a modification factor was developed reducing the underestimation of the analytical model to 10 1 based on the analytical model analytical solutions for the bulk drag coefficient cd and effective blade length le were derived which showed a similar precision with the experimentally fitted cd and le indicating that the analytical solutions for cd and le could be a reliable alternative when the experimentally calibrated cd and le are not available the analytical solutions are computationally efficient and easy to implement into large scale models to analyze the influences of wave attenuation on processes such as coastal morphology inner shelf circulation and material transport a case study showed the wave attenuation of cultivated saccharina latissima changes seasonally with the kelp growth when the kelp blade reaches 2 4 m long after seven months of growth the kelp farms with 50 longlines over a distance of 200 m in the direction of wave propagation in 8 m deep water may attenuate wave energy by 29 for 1 m high waves with the period of 6 s the wave attenuation can be enhanced to 43 when the farms are located in 5 m deep water to provide considerable wave attenuation of kelp with adequate long blades around the year biennial and multiple partial harvesting techniques are recommended the research of using kelp aquaculture structures as nature based coastal protection is still in its infancy field observations of wave attenuation by kelp aquaculture structure are an important research gap to consider for future studies credit authorship contribution statement longhuan zhu conceptualization methodology software validation formal analysis investigation resources data curation writing original draft writing review editing visualization kimberly huguenard writing review editing supervision project administration david w fredriksson writing review editing jiarui lei conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was completed as part of the phd research of longhuan zhu who was supported by national science foundation award iia 1355457 to maine epscor at the university of maine the authors gratefully acknowledge the assistance of the advanced computing group of the university of maine system in producing the numerical data longhuan zhu was benefited from the discussions with gretchen grebe and zhilong liu finally the authors would like to thank the anonymous reviewers and associate editor for constructive comments that really helped to improve this manuscript greatly 
173,vegetation is one of the most important components of nature based coastal protection due to its ability to dissipate wave energy to quantify the wave attenuation by vegetation traditional analytical models assume rigid vegetation and use bulk drag coefficient cd or effective blade length le techniques to consider the effects of blade motion where cd and le are conventionally fitted as a function of kc the keulegan carpenter number and cal with ca the cauchy number and l the ratio of the blade length to wave excursion respectively these parameters do not include the full blade dynamics and so the empirical formulas of cd and le are different for varying vegetation with blade dynamics to obtain analytical solutions for cd and le an analytical wave attenuation model for flexible vegetation and kelp was developed in this study by simplifying and linearizing the blade motion compared with a wide range of experiments for both submerged vegetation and suspended kelp canopies the simplified analytical model underestimated the wave decay coefficient kd by 27 but with a small nrmse normalized root mean square error by the range of the measured data of 0 054 in comparison the numerical model with full nonlinearity underestimated the wave decay coefficient by 11 7 with nrmse 0 063 to reduce the underestimation of the analytical model due to the simplification and linearization a modification factor defined as the ratio of the numerically calculated kd and the analytically calculated kd was fitted with the modification factor the underestimation of the analytical model was reduced to 10 1 based on the analytical model analytical solutions for cd and le were derived which showed a similar precision with the experimentally fitted cd and le based on kc and cal respectively thus the analytical solutions for cd and le could be a reliable alternative when the experimentally calibrated cd and le are not available using the analytical wave attenuation model a case study showed the wave attenuation by cultivated saccharina latissima changes seasonally with the kelp growth when the kelp blade reaches 2 4 m long after 7 months of growth the kelp farms with 50 longlines over a distance of 200 m in the direction of wave propagation in 8 m deep water may attenuate wave energy by 29 for 1 m high waves with the period of 6 s the wave attenuation can be enhanced to 43 when the farms are located in 5 m deep water to provide considerable wave attenuation of kelp with adequate long blades around the year biennial and multiple partial harvesting techniques are recommended keywords wave attenuation vegetation kelp bulk drag coefficient effective blade length seasonal impacts 1 introduction coastal communities are exposed to the increasing risks of coastal erosion and flooding from storm tides and rising sea levels izaguirre et al 2011 tebaldi et al 2012 ondiviela et al 2014 weinkle et al 2018 conventional hard structures are recognized to have adverse impacts on the environment and are less sustainable in a changing climate syvitski et al 2009 currin et al 2010 pace 2011 temmerman et al 2013 sutton grier et al 2015 there is a growing need to use nature based infrastructures for coastal defense morris et al 2019 möller 2019 zhu et al 2020a nature based infrastructures include wetland plants mangroves aquatic vegetation kelp beds coral reefs and shellfish reefs based on the vertical position of the biomaterial in the water column nature based infrastructures are classified as either submerged e g submerged aquatic vegetation located at the seafloor emerged e g saltmarsh and mangroves located at the seafloor and emerged out of the water surface suspended e g kelp and mussel aquacultures suspended in the water column with gaps below and above the canopy or floating e g floating wetland canopies these canopies have the potential to protect coastal regions by damping wave energy without the adverse effects of hardened shorelines the wave attenuation by canopies has been investigated with laboratory and field experimental techniques as well as analytical and numerical models many of these methods are based on the wave attenuation theories developed by dalrymple et al 1984 and kobayashi et al 1993 assuming a rigid canopy component referred to as blade herein and after without motion wave dissipation is dependent on the work of the canopy drag and therefore is proportional to the cube of the relative velocity between the flow and the blade neglecting the blade motion can overestimate wave attenuation in an effort to represent these uncertainties a bulk drag coefficient cd approach has been applied e g kobayashi et al 1993 mendez and losada 2004 alternatively luhar and nepf 2016 and luhar et al 2017 proposed a technique that considers the effects of blade motion by using a reduced effective blade length le rather than the bulk drag coefficient cd that reduces the original drag coefficient cd for a rigid blade the effective blade length le is defined as the length of a rigid blade that dissipates the same wave energy as the flexible blade with the original length l the bulk drag coefficient and effective blade length methods reduce the complexity of modeling the wave vegetation interaction so that these models are computationally efficient and convenient to implement in large scale models however numerous experiments are required to calibrate cd and le conventionally cd is expressed as a function of the reynolds number re or keulegan carpenter number kc independent from vegetation flexural rigidity thus the re and kc based empirical formulas for cd are different for vegetation with different flexibilities e g the different formulas in mendez and losada 2004 bradley and houser 2009 sánchez gonzález et al 2011 jadhav et al 2013 anderson and smith 2014 and ozeren et al 2014 as summarized in chen et al 2018 and van veelen et al 2020 the empirical formula for le is expressed as a function of the cauchy number ca incorporating vegetation flexural rigidity accurate parameterizations of cd and le are important to predict wave attenuation fringer et al 2019 which requires a full understanding of wave vegetation interaction to quantify the blade motion in waves the flexible blade is modeled as a cantilever beam using the euler bernoulli beam approach by simplifying the blade motion to a balance of drag force and blade bending resistance mullarney and henderson 2010 obtained linear normal mode solutions for the blade displacement along the length of the blade the model was recently extended to include the effects of buoyancy by henderson 2019 with the normal mode technique zhu et al 2020a obtained frequency dependent analytical solutions for blade displacements in random waves considering the effects of inertial forces as the analytical solutions are limited to small amplitude blade motion a more precise solution for the large amplitude blade motion can be obtained with numerical techniques e g zeller et al 2014 zhu and chen 2015 luhar and nepf 2016 zhu et al 2018 chen and zou 2019 chen and zou 2020b the consistent mass cable model described in zhu et al 2020b captured an asymmetric whip like blade motion with accurate modeling of blade motion the cable model based wave attenuation model showed a good agreement with the experiments for suspended canopies though underestimating the wave decay coefficient by 10 zhu et al 2021 however the analytical solutions are still useful to describe the mechanisms that influence blade motion related to wave attenuation the analytical solutions are also computationally efficient and easy to implement into large scale models such as swan simulating waves nearshore suzuki et al 2012 wave attenuation by vegetation is determined by wave conditions and vegetation characteristics including morphological e g plant canopy density and blade length width and thickness and mechanical properties e g blade flexural rigidity the vegetation characteristics depend on the growth of vegetation and therefore show a seasonal variation möller and spencer 2002 möller 2006 koch et al 2009 most salt marsh grasses such as spartina alterniflora marsooli et al 2017 spartina anglica schulze et al 2019 spartina foliosa foster martinez et al 2018 salicornia pacific foster martinez et al 2018 and elymus athericus schulze et al 2019 contribute more wave energy dissipation during summer than winter and spring due to greater plant stem stiffness canopy height and aboveground biomass in summer than other seasons most seagrasses such as ruppia maritima chen et al 2007 zostera marina hansen and reidenbach 2013 and zostera noltii paul and amos 2011 also show a similar pattern to dissipate more wave energy in summer with greater blade length blade width canopy height and canopy density in summer than other seasons in winter more vegetation breaks or dies off reducing the wave attenuation capacity marsooli et al 2017 vuik et al 2018 therefore ondiviela et al 2014 proposed that the large long living and slow growing seagrass species e g posidonia oceanica with biomass largely independent of seasonal fluctuations may provide favorable coastal protection wave attenuation by kelp is also dependent on wave conditions and the morphological and mechanical properties of kelp for example sparse 0 1 plants m2 macrocystis pyrifera giant kelp forests was observed to have no significant effects on wave attenuation elwany et al 1995 similar results were also observed for highly flexible nereocystis luetkeana bull kelp gaylord et al 2003 and deeply submerged ecklonia radiata 10 of the water column morris et al 2019 however dense 25 plants m2 laminaria hyperborea tangle in shallow water 5 m could attenuate 70 85 of wave energy over 258 m into the canopy along the direction of wave propagation mork 1996 unlike the wild kelp that grows on the seafloor cultivated kelp is commonly suspended in the water column supported by a mooring system since the kinetic energy of intermediate and deep water waves is concentrated near the surface suspended kelp farms with a dense plant density are believed to dissipate more wave energy than naturally occurring kelp beds with a sparse plant density zhu et al 2021 with laboratory experiments zhu et al 2021 demonstrated that saccharina latissima sugar kelp aquaculture farms with 20 longlines of 1 m long blades and 100 blades m at 1 1 m below the water surface have the potential to dissipate 12 5 energy of 0 35 m high waves with the period of 6 3 s in 4 m deep water the wave attenuation can be further improved by planting the kelp more densely and adding more kelp longlines zhu et al 2021 cultivated kelp grows seasonally resulting in varying performances for wave attenuation which are not fully understood considering seasonal variations of natural materials is essential to understand their capacity for wave attenuation select appropriate species and design strategies for nature based coastal protection this study aims to develop an analytical wave attenuation model that resolves the blade motion and investigates the wave attenuation characteristics of submerged flexible vegetation and suspended cultivated kelp the analytical wave attenuation model was derived by simplifying and linearizing the blade motion the analytical model was compared with the experiments in luhar et al 2017 and lei and nepf 2019b for submerged vegetation and the experiments in zhu et al 2021 for suspended kelp canopies with the analytical model the analytical solutions for the bulk drag coefficient and effective blade length were derived and compared with the fitted values from the experimental data after the data model comparison the analytical model was then used to analyze the seasonal performance of submerged vegetation and suspended canopies for wave attenuation as well as the implications for nature based coastal protection 2 theory various vegetation and kelp species have different morphologies each with its own associated mechanics denny and gaylord 2002 which influence the selection of appropriate techniques to model the dynamics of vegetation and kelp in this study seagrass zostera marina and sugar kelp saccharina latissima common species in maine usa are selected as representatives for submerged vegetation and suspended kelp respectively fig 1 zostera marina is composed of roots rhizome sheath and leaves blades there is one or several leaves fixed on the sheath the sheath is short but much more rigid than the leaves sugar kelp saccharina latissima consists of holdfast stipe and blade the stipe of saccharina latissima is very short compared to the blade but much more rigid than the blade in the development of the wave attenuation model the seagrass sheath and kelp stipe are considered rigid while the seagrass leaves and kelp blade are considered flexible the seagrass leaf and kelp blade are commonly modeled using the cantilever beam theory for other species that have different morphologies other methods might be considered for example for the kelp species with a long stipe two dynamic models are commonly used mass damped spring model for eisenia arborea and pterygophora californica where the stipe is treated as a cantilever type spring with entire effective mass concentrated at the free end of the stipe denny et al 1998 and buoy on rope model for bull kelp nereocystis luetkeana and macrocystis pyrifera where the pneumatocyst is modeled as a buoy and the stipe is modeled as a thin straight non buoyant rope utter and denny 1996 denny et al 1997 stevens et al 2001 zhu et al 2020a different terminologies are used for seagrass and kelp although their physical forms are similar fig 1 for the convenience of description in the model development flexible blade is used to describe the vegetation leaf and kelp blade 2 1 model set up a three layer model is used to derive the wave attenuation for submerged and suspended canopies fig 2 the horizontal coordinate x is positive in the direction of wave propagation with x 0 at the leading edge of the canopy and x lv at the ending edge the vertical coordinate z is positive upward with z 0 at the still water level swl the canopy height d 2 is defined as the average submerged length l of the canopy blades the thicknesses of the water layers above and below the canopy are d 1 and d 3 respectively the water depth from the swl is h d 1 d 2 d 3 where the seafloor is located at z h and assumed horizontal this generalized three layer model can be used to analyze the wave attenuation characteristics of i submerged d 1 0 and d 3 0 ii emerged d 1 0 and d 3 0 iii suspended d 1 0 and d 3 0 and iv floating d 1 0 and d 3 0 canopies based on linear wave theory dean and dalrymple 1991 the frictionless horizontal wave orbital velocity u 0 is given by 1 u 0 h 2 ω γ z cos k x ω t where h is the wave height ω is the wave angular frequency t is the time γ z cosh k h z sinh kh is the vertical decay factor and k is the wave number obtained from the dispersion relation ω2 gktanh kh with g the gravitational acceleration due to the presence of a canopy the flow velocity in the canopy decreases and displays a phase shift relative to the flow velocity above the canopy lowe et al 2005 rosman et al 2013 henderson et al 2017 for simplicity the phase shift is not considered and the magnitude reduction is considered using a factor α w following lowe et al 2005 such that the within canopy velocity can be estimated by 2 u α w u 0 where the factor α w is calculated using the numerical solution in lowe et al 2005 2 2 blade motion the flexible blade can be modeled as a cantilever beam using the cable model described in zhu et al 2020b 2021 where the blade motion is governed by the balances of blade inertia blade bending resistance blade tension blade weight buoyancy friction and hydrodynamic force the vertical blade displacement geometrical nonlinearity of the blade deflection and the vertical forces including friction buoyancy and tension create a difficulty to obtain analytical solutions and therefore are neglected in this study the effects of neglecting these terms on blade motion and wave attenuation are assessed later in section 3 2 thus the governing equation for the horizontal blade displacement ξ is then simplified as the balance of blade inertia blade bending resistance and hydrodynamic force which is expressed as 3 ρ v a c ξ ei ξ f x where ξ is a function of time t and the distance s along the blade length from the fixed end s 0 the dot indicates the derivative with respect to t the prime indicates the derivative with respect to s ρ v is the blade mass density ac is the blade cross section area e is the elastic modulus i is the second moment of the cross section and fx is the hydrodynamic force per unit length the relation between s and z fig 2 is 4 z d 1 d 2 s blade fixed at the bottom end d 1 s blade fixed at the top end according to the morison equation morison et al 1950 the hydrodynamic force fx is decomposed into virtual buoyancy fvb drag fd and added mass force fam and is given by 5 f x f v b f d f a m with 6 f v b ρ w a c u 7 f d 1 2 c d ρ w b u ξ u ξ and 8 f am c m ρ w a c u ξ where ρ w is the water density b is the projected width of the blade cd is the drag coefficient and cm is the added mass coefficient for the blade with rectangular cross section the drag coefficient cd and added mass coefficient cm are 9 c d max 10 k c 1 3 1 95 and 10 c m min c m 1 c m 2 respectively with c m 1 1 0 35 k c 2 3 kc 20 1 0 15 k c 2 3 kc 20 and c m2 1 kc 18 2 49 luhar 2012 luhar and nepf 2016 in 9 and 10 kc umt b is the keulegan carpenter number with t 2π ω the wave period and um the magnitude of the water velocity relative to the blade yielding a non constant cd and cm the formulas are fitted from the experimental data in keulegan and carpenter 1958 and sarpkaya and o keefe 1996 for rigid flat plates in oscillatory flows with r 2 0 972 and 0 912 for cd and cm fig 3 respectively to obtain an analytical solution the nonlinear drag is linearized as 11 f d 1 2 c d ρ w b u ξ u ξ c u ξ where the linearization coefficient c is obtained from the lorentz s condition of equivalent work sollitt and cross 1972 this requires that the nonlinear and linear drag accounts for the same amount of energy dissipation averaged over one wave period such that d 1 d 1 d 2 1 2 c d ρ w b u ξ u ξ 2 d z d 1 d 1 d 2 c u ξ 2 d z yielding 12 c d 1 d 2 d 1 1 2 c d ρ w b u ξ u ξ 2 d z d 1 d 2 d 1 u ξ 2 d z 1 2 c d ρ w b u e r in 12 the overline indicates the time average over one wave period and the equivalent relative velocity magnitude u e r d 1 d 2 d 1 u ξ u ξ 2 d z d 1 d 2 d 1 u ξ 2 d z the linearization coefficient c is equivalent to the viscous damping coefficient per unit length with the unit of ns m2 eq 12 also shows that c is dependent on the equivalent relative velocity magnitude uer the linearization of the drag force in random waves is typically done using the borgman 1967 method based on the distribution of the wave orbital velocity resulting in a different expression of the linearization coefficient in zhu et al 2020a substituting 1 5 with linearized drag 11 into 3 yields 13 m ξ c ξ ei ξ h 2 ω γ c cos kx ω t ω m i sin kx ω t where m ρ v cmρ w ac and mi 1 cm ρ w ac the boundary conditions for a cantilever beam are set as ξ 0 t 0 ξ 0 t 0 ξ l t 0 and ξ l t 0 solving 13 with the normal mode approach rao 2007 yields 14 ξ h 2 γ γ s sin k x ω t γ c cos k x ω t in 13 γ s and γ c are the transfer functions and expressed as 15 γ s ω γ n 1 ϕ n ω i n λ n 2 ω 2 d n 2 ζ n λ n ω λ n 2 ω 2 2 2 ζ n λ n ω 2 and 16 γ c ω γ n 1 ϕ n d n λ n 2 ω 2 ω i n 2 ζ n λ n ω λ n 2 ω 2 2 2 ζ n λ n ω 2 where ϕ n cos μ n l cosh μ n l sin μ n s sinh μ n s sin μ n l sinh μ n l cosh μ n s cos μ n s is the nth normal mode of the cantilever beam with μ n the nth solution of 1 cos μlcosh μl 0 λ n μ n 2 e i m is the nth natural frequency of the blade 2ζ n λ n c m d n 0 l c γ ϕ n d s 0 l m ϕ n 2 d s and i n 0 l m i γ ϕ n d s 0 l m ϕ n 2 d s since γ is expressed in z and ϕ n is expressed in s the relation between s and z in 4 is required to calculate the integral 0 l γ ϕ n d s substituting 14 into 12 yields the expression of the linearization coefficient in terms of the transfer functions 17 c 1 2 c d ρ w b h 2 ω 8 3 π d 1 d 2 d 1 γ 3 1 γ s 2 γ c 2 3 2 d z d 1 d 2 d 1 γ 2 1 γ s 2 γ c 2 d z the linearization coefficient can be obtained iteratively starting from a static blade an initial c is calculated from eq 17 by assuming γ s 0 and γ c 0 once the transfer functions γ s and γ c are obtained from 15 and 16 c can be updated from 17 the procedure is repeated until a convergent solution is obtained 2 3 wave attenuation the wave energy dissipation is assumed to be attributed to the work of the canopy drag following zhu et al 2020a such that 18 e c g x d 1 d 1 d 2 n α ϵ f d u ξ dz where e ρ w gh 2 8 is the local wave energy per unit horizontal area cg ω 1 2kh sinh 2kh 2k is the wave group velocity n is the canopy density defined as the number of blades per unit horizontal area and αε 1 is a factor to consider the sheltering effects between blades with αε 1 for no sheltering the factor αε is calibrated through experiments substituting the quadratic drag force 7 into 18 yields the transmitted wave height at distance x in relation to the incident wave height h 0 at x 0 19 h x h 0 1 1 k d h 0 x where the wave decay coefficient kd is expressed as 20 k d 4 α c d b n k 2 3 π sinh k h 2 k h sinh 2 k h d 1 d 2 d 1 1 γ s 2 γ c 2 3 cosh 3 k h z d z it should be noted that eq 19 is obtained using quadratic drag using the linearized drag 11 yields an exponential decayed wave height as h h 0 e k d h 0 x which is also often used in practice e g kobayashi et al 1993 méndez et al 1999 zhu and zou 2017 the two wave decay forms are linked through a piecewise method appendix c of zhu 2020 the piecewise function method indicates that the exponential decay form with linearized drag would overestimate the wave attenuation therefore the fractional decay form in 19 is recommended zhu 2020 however for weak wave attenuation such that kdh 0 x 0 5 the difference between 1 1 kdh 0 x and e k d h 0 x is less than 10 for the canopies with unsheltered rigid blades such that αε 1 γ s 0 and γ c 0 the solution 20 reduces to the solution in zhu and zou 2017 i e 21 k d r c d bnk 9 π 9 sinh k d 2 d 3 9 sinh k d 3 sinh 3 k d 2 d 3 sinh 3 k d 3 sinh kh 2 kh sinh 2 kh for bottom rooted vegetation such that d 3 0 solution 21 can be further reduced to the solutions by dalrymple et al 1984 and kobayashi et al 1993 for shallow water waves with kh 0 1π 21 reduces to 22 k d r c d b n k l 3 π h 2 indicating that the wave decay coefficient is proportional to the drag coefficient canopy density blade width and blade length but inversely proportional to the wavelength and the square of water depth thus the wave attenuation decreases more dramatically with increasing water depth than other parameters for shallow water waves according to 22 the wave attenuation is independent from the vertical position of the canopy since the horizontal wave orbital velocity minimally decays to the bottom 2 4 bulk drag coefficient and effective blade length according to 18 the wave dissipation ratio is proportional to d 1 d 1 d 2 f d u ξ dz l 0 c d u r 3 ds with u r u ξ the relative velocity obtaining the wave dissipation ratio requires computation of the relative velocity in order to consider the wave blade interaction to reduce the computation for the relative velocity including resolving the blade motion the bulk drag coefficient and effective blade length methods with a rigid blade assumption are often used especially for implementation into large scale models the bulk drag coefficient cd is a reduced drag coefficient such that 0 l c d u 3 ds 0 l c d u r 3 ds while the effective blade length le is a reduced blade length such that 0 l e c d u 3 ds 0 l c d u r 3 ds fig 4 traditionally cd and le are calibrated with experiments or numerical models however with the analytical model developed in section 2 3 analytical solutions for cd and le can be obtained to derive the solution for the bulk drag coefficient replacing cd in 21 by cd and substituting the result into 20 yields 23 c d 12 k α ϵ d 1 d 2 d 1 cosh 3 k h z 1 γ s 2 γ c 2 3 dz 9 sinh k d 2 d 3 9 sinh k d 3 sinh 3 k d 2 d 3 sinh 3 k d 3 c d for shallow water waves with kh 0 1π 23 reduces to 24 c d α ϵ d 2 d 1 d 1 d 2 1 γ s 2 γ c 2 3 dz c d to derive the solution for the effective blade length replacing d 2 in 21 by le and substituting the result into 20 yields 25 9 sinh k l e d 3 sinh 3 k l e d 3 9 sinh k d 3 sinh 3 k d 3 12 k α ε l 0 cosh 3 k h z 1 γ s 2 γ c 2 3 ds for submerged vegetation with d 3 0 25 reduces to 26 9 sinh k l e sinh 3 k l e 12 k α ε l 0 cosh 3 k h z 1 γ s 2 γ c 2 3 ds when le is far less than the wave length with kle 0 1π 26 reduces to 27 l e α ε l 0 cosh 3 k h z 1 γ s 2 γ c 2 3 ds for shallow water waves with kh 0 1π 25 reduces to 28 l e α ε l 0 1 γ s 2 γ c 2 3 ds dividing 24 by 28 yields 29 l e l c d c d indicating that the rate of blade length reduction is equal to the rate of drag coefficient reduction in shallow water waves these reductions reflect the decreases in wave attenuation resulting from the blade motion eq 29 holds because the wave decay coefficient of rigid canopies is proportional to the drag coefficient and blade length in shallow water waves as shown in 22 3 results 3 1 model data comparison 3 1 1 experimental data description the analytical model was compared with the experiments in luhar et al 2017 and lei and nepf 2019b for submerged vegetation and the experiments in zhu et al 2021 for suspended kelp canopies in the experiments for submerged vegetation luhar et al 2017 lei and nepf 2019b the artificial vegetation consisted of six 13 cm long 0 3 cm wide and 0 1 mm thick flexible blades to model seagrass leaves and one 1 cm long solid wood cylinder to model seagrass sheath the diameters of the wood cylinders were 7 8 mm in luhar et al 2017 and 6 9 mm in lei and nepf 2019b the flexible blades were made from low density polyethylene ldpe film with ρ v 0 92 g cm3 and e 0 3 gpa the blades were located separately such that the sheltering factor αε 1 the canopy density was 280 to 1800 stems m2 with 1680 to 10 800 blades m2 the canopy lengths were 5 m in luhar et al 2017 and 3 5 7 m in lei and nepf 2019b the wave height was 1 to 11 2 cm the wave period was 0 8 to 2 s and the water depth was 16 to 45 cm thus the dimensionless parameters are kh 0 44 2 7 l h 0 29 0 8 kc 9 135 ca 64 3796 l 2 30 and γ 0 16 0 30 where c a ρ w b u m 2 l 3 e i is the cauchy number l lω um is the ratio of the blade length to the wave excursion um ω and γ βs 1 2 with β g ρ w ρ v a c t 0 5 ρ w c d l b u m the dimensionless buoyancy defined in henderson 2019 and s e i t 0 5 ρ w c d l 4 b u m the dimensionless stiffness defined in mullarney and henderson 2010 and henderson 2019 the value of γ determine the importance of buoyancy relative to stiffness with γ 1 indicating buoyancy is less important than stiffness and γ 1 indicating buoyancy is more important than stiffness details of the experiments for the submerged vegetation can be found in luhar et al 2017 and lei and nepf 2019b in the experiments for suspended kelp canopies the kelp blade was modeled using silicon film with ρ v 1 2 g cm3 and e 2 04 mpa the model blade was 10 16 cm long 0 95 cm wide and 0 1 mm thick the suspended kelp canopy consisted of 20 rows of blades and the rows were 20 cm apart for each row there were 31 aggregates of blades with one aggregate cm for each aggregate ten blades were fixed together at the top end of the blades the fixed part of the blade was lr 0 5 cm while the flexible free part was lf 9 66 cm the fixed part of the blades was used to model the effects of stipes the sheltering effects between the blades in an aggregate were considered using a sheltering factor αε 0 630 the canopy length was 3 8 m and the canopy density was 526 3 aggregates m2 with 5263 blades m2 three vertical positions of the suspended canopy beneath the still water level with d 1 6 11 and 16 cm were compared in the experiments the incident wave height was 1 8 to 3 8 cm wave period was 0 8 to 2 s and water depth was 30 to 40 cm with k h 0 58 2 33 lf h 0 24 0 32 kc 4 4 18 8 ca 9667 43 406 l 3 4 14 4 and γ 5 16 2 79 the details of the experiments can be found in zhu et al 2021 the canopy characteristics and wave conditions are also summarized in table 1 3 1 2 comparisons for wave attenuation the analytical wave attenuation model developed in this work is compared with the experiments in luhar et al 2017 and lei and nepf 2019b for submerged vegetation and the experiments in zhu et al 2021 for suspended kelp to evaluate the performance of the analytical model the calculations using the numerical model in zhu et al 2021 are also presented the comparisons for the wave height decay along the canopy are shown in fig 5 the measured wave height is featured with oscillations along the canopy the oscillation reflects the partially standing wave induced by the wave reflection and therefore is spatially periodic with the period of the scale of half wavelength luhar et al 2017 the analytical results are similar to the numerical results for the cases in fig 5 a c but for the cases in fig 5 b d the analytical model demonstrates a smaller wave decay than the numerical model this is because the simplification of the analytical model ignored the effects of the buoyancy tension and geometrical nonlinearity which will be discussed in section 3 2 to evaluate the overall performance of the analytical model the analytically and numerically calculated wave decay coefficients kd are compared with the experimental data in fig 6 the numerical results show an underestimation of 11 7 calculated using the slope of the linear fitted line between the calculated and measured values the same hereinafter with the normalized root mean square error nrmse 0 063 fig 6a the normalization is calculated using the range defined as the maximum value minus the minimum value of the measured data compared to the numerical model the analytical model underestimates the wave decay coefficient by 27 fig 6b however the nrmse of the analytical model is smaller at 0 054 indicating that the analytical model has a similar and even higher precision than the numerical model in calculating the wave attenuation 3 1 3 bulk drag coefficient and effective blade length bulk drag coefficient and effective blade length methods are simple ways to consider the effects of blade motion on wave attenuation the values of cd and le are for the flexible part of the blade with a length of lf in this section the analytical solutions for cd 23 and le 25 are presented and compared with the experiments in luhar et al 2017 and lei and nepf 2019b for submerged canopies as well as the experiments in zhu et al 2021 for suspended canopies the analytically calculated cd is compared with the fitted and measured cd in fig 7 where cd is expressed as a function of kc since kc does not include all the parameters governing the blade motion such as blade flexural rigidity the blade motions with different flexural rigidities might be different yielding different values of cd although with the same kc thus cd is scattered especially for the submerged vegetation dataset with more cases and wave conditions resulting in varying blade motion fig 7c for kc based cd the analytically calculated cd has a larger nrmse of 0 258 than the fitted cd with nrmse 0 181 for the suspended kelp canopy fig 7a similarly for submerged vegetation fig 7c the nrmse of the analytically calculated cd is 0 278 which is larger than the fitted cd with nrmse 0 191 in the comparisons for the wave decay coefficient fig 7b d the nrmse of the kd calculated using the analytically calculated cd are 0 187 for the suspended kelp and 0 054 for the submerged vegetation which are larger than those kd calculated using the fitted cd with nrmse 0 070 for the suspended kelp and 0 051 for the submerged vegetation the analytically calculated effective blade length of the flexible part of the blade l f e is compared with the measured and fitted l f e as well as the empirical formula in lei and nepf 2019b in fig 8 where l f e is expressed as a function of cal in the selected experiments the ratio of the blade length to the wave excursion was l 2 0 and the cauchy number was ca 64 for ca 1 and l 1 the effective blade length can be scaled as l f e l f c a l 1 4 l f e l f cal 1 4 by assuming the static balance between the drag and stiffness luhar and nepf 2016 for l 1 the effective blade length can be scaled as l f e l f c a 1 3 l f e l f c a 1 3 luhar and nepf 2011 however luhar et al 2017 and lei and nepf 2019b have shown that the scaling with cal 1 4 works better for the present experiments based on the measured horizontal forces of a single flexible blade with a wide range of cal from 0 292 to 1 88 104 lei and nepf 2019b developed the empirical formula for effective blade length given by 30 l f e l f 0 94 cal 0 25 where the effective blade length is defined as the reduced length of a rigid blade that provide the same horizontal force as the flexible blade with the full length although this formula is developed based on the horizontal force of a single blade it shows a good comparison with the measured l f e nrmse 0 224 fig 8c and performs well for calculating the decay coefficient kd nrmse 0 050 fig 8d similar to kc based cd cal also does not include all the parameters governing the blade motion therefore there might be several values of l f e for different blade motions with the same cal resulting in scattered l f e especially for the submerged vegetation experimental data with numerous types of blade motions fig 8c as the range of cl 1678 10 444 for the wave attenuation experiments is small the scattering of the data is more obvious for cal based l f e the analytically calculated l f e has a larger nrmse of 0 452 than the fitted l f e with nrmse 0 258 for suspended kelp canopies for submerged vegetation the nrmses are 0 205 for the fitted l f e 0 224 of the calculated l f e with the empirical formula 30 and 0 292 for the calculated l f e using the present analytical model fig 8c for the comparisons for the wave decay coefficients by suspended kelp fig 8b the nrmse of the kd calculated using the analytically calculated l f e are 0 187 which is larger than the kd calculated using the fitted l f e with nrmse 0 107 however for submerged vegetation fig 8d the nrmse of the kd calculated using the analytically calculated l f e is 0 054 which is smaller than the kd calculated using the fitted l f e with nrmse 0 067 the analytically calculated l f e shows a similar nrmse to the empirical formula in lei and nepf 2019b for calculating kd with nrmse of 0 050 fig 8d 3 2 blade dynamics and the simplified analytical wave attenuation model blade dynamics have a significant influence on wave attenuation performance to investigate the effects of blade motion on wave attenuation the blade postures relative velocities ur and drag force flux fdur are analyzed based on the simplified analytical solutions and nonlinear numerical solutions the wave energy dissipation is determined by the drag flux fdur ur 3 as shown in 18 which is proportional to the cube of the relative velocity the blade is so flexible that the free tip moves passively with the wave flow fig 9 a1 b1 resulting in a small relative velocity ur fig 9a2 a3 b2 b3 thus the blade segments near the free tip have little contribution to the wave dissipation take the submerged blade in fig 9b as an example the blade segments with s 0 24l have a small relative velocity ur 0 4max ur fig 9b2 and therefore a small drag flux f d u r 5 max f d u r fig 9b4 indicating little contribution to the wave dissipation in other words for the flexible blade only the blade segments near the fixed end s 0 19l for the suspended blade in fig 9a and s 0 24l for the submerged blade in fig 9b contribute to the wave dissipation this is different from a rigid blade that can dissipate wave energy along the whole blade length the blade segments near the fixed end contribute the most 95 wave dissipation which builds the foundation for the effective blade length method therefore accurate simulations of the dynamics of the blade segments near the fixed end are important to evaluate the wave dissipation the analytical model shows a symmetric motion fig 9a1 b1 due to neglecting the vertical wave orbital velocity and the phase lag induced by blade displacement the analytical model also neglected the vertical blade displacement geometrical nonlinearity of the blade deflection and vertical forces including friction buoyancy and tension by considering all the forces and nonlinearity the numerical model better simulates the blade motion by capturing the asymmetric whip like motion fig 9a1 b1 the asymmetry also reflects the phase shift of the relative velocity ur along the blade length fig 9a3 b3 although the analytically calculated displacements of the segments near the tip are much smaller than those calculated from the numerical model the discrepancy between these calculations for the blade displacements becomes smaller along the blade length toward the fixed end fig 9a1 b1 thus the analytical model predicts a similar drag force flux to the numerical model fig 9a4 b4 for the segments near the fixed end however above this region the analytical model underestimates the drag force flux and therefore underestimates the wave attenuation fig 6 to quantify the effects of the simplifications and linearization of the blade motion in the development of the analytical wave attenuation model the wave decay coefficient calculated from the analytical model is compared with that calculated from the numerical model using the ratio α m defined as 31 α m k d f numerical k d f analytical where k d f is the wave decay coefficient of the flexible blades without rigid sheaths or stipes such that k d f kd k d r with kd the wave decay coefficient of the canopy and k d r the wave decay coefficient of the rigid sheaths or stipes and the subscript numerical and analytical indicate the values are calculated numerically and analytically respectively for the cases in this study the ratio α m showed the following relations with the normalized blade motion amplitude at the blade tip by the blade length ξ ml l i e 32 α m 0 20 0 03 ξ ml l 0 50 0 04 0 015 ξ ml l 0 155 with r 2 0 933 for suspended kelp fig 10 a and 33 α m 0 92 0 03 ξ ml l 0 216 0 014 0 039 ξ ml l 0 449 with r 2 0 863 for submerged vegetation fig 10b since the numerical results reduce to the analytical results when ξ ml l 1 α m approaches to 1 as ξ ml l approaching to 0 unfortunately formulas 32 and 33 cannot capture this trend due to limited data therefore 32 and 33 are limited to apply for the given range of ξ ml l to address this issue an improved formula for α m will be developed with more data in the future with the ratio α m the analytical model can be modified by multiplying the modification factor α m the modified analytical results underestimate the wave decay coefficient by only 10 1 fig 10c while the original analytical results underestimate the wave decay coefficient by 27 fig 6b the modification improved the analytical wave attenuation model by reducing the underestimation by 27 10 1 27 62 6 however the nrmse increased 22 2 to 0 066 fig 10c from 0 054 fig 6b 3 3 case study for wave attenuation in different seasons the analytical model is then used to investigate the seasonal wave attenuation capacity of kelp farms compared with seagrass meadow over one year to represent the seagrass and kelp zostera marina eelgrass and saccharina latissima sugar kelp are used which are common species at the coast of maine in the usa regarding the parameters of seagrass the designed length of the seagrass leaf follows the measured data in gaeckle and short 2002 for the zostera marina at coastal maine usa fig 11 a the corresponding leaf width and thickness as well as the length and width of the sheaths are calculated using the relationship formulas in abdelrhman 2007 and shown in fig 11a the designed mass density and elastic modulus are 700 kg m3 abdelrhman 2007 and 0 26 gpa fonseca et al 2007 respectively the designed density is 335 shoots m2 and the leaf number is 3 for each shoot such that the density is 1005 leaves m2 mattila et al 1999 the water depth for the seagrass is designed as 5 m for the seagrass in this case study γ 0 04 0 16 indicating that the effects of buoyancy are small and negligible for the kelp the designed length l of the kelp blade follows the measured data in augyte et al 2017 for the saccharina latissima in maine usa fig 11b the corresponding blade width b maximum thickness dmax and elastic modulus e are calculated based on the relations with the blade length using the empirical formulas in zhu et al 2021 and shown in fig 11b in zhu et al 2021 e was fitted as an exponential function of dmax which may yield a very large e when dmax exceeds a critical value e g e 100 pa if dmax 1 26 mm to avoid too large values for e the maximum e is set as the measured maximum value 22 mpa in zhu et al 2021 this setting may underestimate the wave attenuation capacity of kelp in summer when the kelp blade is thick it is noted that the blade thickness d decreases from the center of the blade width to both edges following a normal like distribution along the blade width zhu et al 2021 34 d d max 0 797 0 011 e 1 2 s b b 0 118 0 003 2 0 203 0 011 where sb is the distance from the center of the blade width toward the blade edge based on the thickness distribution in 34 the second momentum of the cross section of saccharina latissima is 35 i 1 2 d m a x 1 2 d m a x 2 s b y 2 d y 0 2 b d m a x 3 12 indicating that the flexural rigidity of the real saccharina latissima blade is only 20 of the same wide blade but with the maximum thickness therefore an effective blade width with be 0 2b is used to calculate the flexural rigidity following zhu et al 2021 the designed mass density of kelp is 1053 kg m3 and the plant density is 405 plants m zhu et al 2021 the sheltering factor of α ε 0 630 zhu et al 2021 is assumed to be applicable for this case study due to limited data for the stipes the stipes were not considered in this study which may underestimate the wave attenuation of the kelp farms the kelp longline is designed at 1 2 m below the still water level and 4 m apart in 8 m deep water to compare with the seagrass meadow in the same water depth a shallower water depth of 5 m is also considered for kelp farms since the kelp blade grows longer after may the flexural rigidity decreases such that γ can reach up to 1 67 in may indicating a non negligible role of buoyancy consequently the neglecting of buoyancy might cause uncertainty in the wave attenuation for the wave conditions the designed wave height is 1 m and the wave period is designed as 6 s the drag coefficient cd and added mass coefficient cm are calculated using 9 and 10 the original analytical model without modification is used in this case study to evaluate the wave attenuation performance of the canopies the wave energy dissipation rate edr is used and defined as 36 e d r 1 h l v 2 h 0 2 1 1 1 k d h 0 l v 2 where lv is the canopy length in the direction of wave propagation in this case study the canopy length is designed as 200 m 50 longlines of kelp the wave attenuation by the designed suspended kelp farms and seagrass is shown in fig 12 the wave height decays when propagating through the canopy the transmitted wave height varies seasonally which is more obvious at the trailing edge of the canopy to investigate the wave attenuation capacities of suspended kelp farms and seagrass the edrs at the trailing edge of the canopies are compared in fig 12d the edr shows a similar pattern to the variance of blade length around the year the growth period of kelp has significant impacts on wave attenuation kelp is planted in late november after about 1 5 months of growth in a nursery and grows slowly over the first few months fig 11b resulting in a small edr 10 during this time fig 12d after about five months of growth the kelp blade length exceeds 1 5 m at the end of april yielding considerable wave attenuation with edr 10 in 8 m deep water and edr 20 in 5 m deep water when the kelp blade reaches 2 4 m long in may the wave attenuations of kelp farms in 8 m deep water and 5 m deep water are enhanced to 29 and 43 respectively the wave attenuation remains at this level until the kelp is harvested in june unlike the newly planted kelp with blades growing from millimeters to meters in the growing season the averaged leaf length for a perennial seagrass meadow changes from 15 to 61 cm fig 11a therefore seagrass is less impacted by seasonal growth patterns that are typical of kelp farms compared to kelp farms the seagrass has a larger elastic modulus and canopy density yielding a larger edr 20 than the kelp in the initial growing months with small blade lengths however the wave attenuation capacity of kelp farms surpasses the seagrass in the later months when the kelp blades become longer 1 5 m the edr by seagrass is most significant at 27 in august when the leaf and sheath lengths are the longest 4 discussion 4 1 evaluations and limitations of the analytical wave attenuation model the analytical wave attenuation model performed well in predicting the wave attenuation of suspended and submerged canopies with a small nrmse based on the analytical model the analytical solutions for the bulk drag coefficient cd and effective blade length le were obtained conventionally cd and le are usually fitted as a function of kc and cal respectively the results showed that the analytically calculated cd and le are in a similar precision to the kc based fitting cd and cal based fitting le the kc describes the relative importance of drag force over inertia force on a rigid body and the cal scaling is developed based on the static balance of blade stiffness and drag force the kc based and cal based fitting did not incorporate all the parameters governing the wave induced blade dynamics such as the frequency ratio and therefore influence the precision of the fittings however due to the complicated wave blade interaction sophisticated parameters to obtain good fits e g r 2 0 8 of cd and le for a wide range of wave conditions and blade properties are difficult to achieve thus the analytical solutions for cd and le can be an alternative when reliable cd and le are not available meanwhile the analytical solutions for cd and le have given the parameters that govern cd and le which may also provide insight into the appropriate parameters and relations to obtain a better fit for cd and le for example the ratio of the blade s natural frequency and the wave frequency influences the resonance and therefore is an important parameter that should be incorporated into the development of the empirical formulas for cd and le to obtain an explicit analytical solution for the wave attenuation the blade dynamics are simplified and linearized by assuming negligible vertical forces net buoyancy and friction and a small amplitude blade motion however the simplification and linearization decrease the precision for simulating the blade dynamics and the wave attenuation to improve the model a modification factor α m can be used in this study the formulas 32 and 33 for α m was fitted as a simple power function of the blade motion amplitude at the blade tip normalized by the blade length ξ ml l using limited data with 0 015 ξ ml l 0 155 and 0 039 ξ ml l 0 449 for suspended and submerged canopies respectively the flaw of this fitting is that the calculated α m using 32 and 33 cannot converge to 1 as ξ ml l approaches to 0 when the numerical results reduce to the analytical results for rigid vegetation thus the formulas 32 and 33 have limited application for the given range of ξ ml l a more sophisticated fitting of α m will be developed with more data in the future understanding the effects of neglecting buoyancy and the geometrical nonlinearity is helpful to improve the analytical solution the within canopy water velocity showed a phase lead and magnitude reduction relative to the above canopy water velocity especially for dense canopies lowe et al 2005 rosman et al 2013 henderson et al 2017 in this study the magnitude reduction of the water velocity was considered using the approach in lowe et al 2005 while the phase shift was ignored supposing the phase shift is a constant or varies slowly in time and location it is expected to have no effects on blade motion and wave attenuation because the phase shift is removed by time averaging in the model development however the phase shift changing with time and location may significantly affect blade dynamics and wave attenuation which warrants further investigation as waves propagate into the canopy interior the wave height and wave orbital velocity decrease with distance into the canopy thus the vegetation and kelp are under larger wave conditions at the leading edge of the canopy while under smaller wave conditions at the end of the canopy resulting in different blade dynamics with distance along the canopy the blade displacement is determined by the wave height h and transfer functions γ s and γ c as shown in 14 to obtain the wave attenuation by solving the energy conservation eq 18 the blade displacement is treated using a decaying wave height h x because the wave height is the unknown and distance independent transfer functions γ s and γ c since the effects of wave height decay on blade dynamics were incorporated into the wave attenuation model the effects induced by assuming distance independent γ s and γ c are expected to be small in the prediction of wave attenuation in this study using the wave conditions at the leading edge and trailing edge of the canopy to estimate γ s and γ c was observed to have no difference on the prediction of the wave attenuation this might be explained by the weak wave attenuation of flexible blades in the selected experiments with the minimum transmitted wave height as 75 of the incident wave height the contribution of the rigid part of the blades was removed for large wave attenuation the error induced by assuming distance independent γ s and γ c may appear but it is still expected to be small in wave attenuation prediction because the wave height decay is considered the flexible vegetation and kelp blade have shown significant asymmetric motions the asymmetry of the blade motion is induced by the vertical wave orbital velocity and the phase lag in encountering water velocity caused by blade displacement zhu et al 2020b this asymmetry occurs in linear waves and can be enhanced by wave nonlinearity the mean blade tilt is large at the blade tip and decreases dramatically to the fixed end unfortunately the analytical model cannot capture the asymmetric motion however as the blade segments near the tip with large mean tilt have very little contribution to the wave attenuation and only a few blade segments near the fixed end with small mean tilt dominate the wave attenuation the effects of the large mean blade tilt may be small thus the analytical model could perform well in predicting wave attenuation although the asymmetric blade motion is not well predicted nevertheless the effects of the mean blade tilt of the blade segments near the fixed end on wave attenuation might be significant and worth further study the mean blade tilt might reduce the drag force and therefore reduce the wave attenuation thus neglecting the mean blade tilt might overestimate the wave attenuation for a dense canopy the blades are close to each other yielding sheltering and interaction between blades the sheltering effects can reduce the drag of the sheltered blades and therefore reduce the wave attenuation for long kelp blades blade interaction may result in the entanglement of the kelp blades enhancing blade breakage koehl and wainwright 1977 and therefore influencing the wave attenuation in this study the kelp blade sheltering and interaction was considered using a constant factor which should be improved by scaling with blade properties and configuration as well as wave conditions the analytical model was developed assuming monochromatic waves in the real world waves can be described using random waves with different frequencies the form of the present analytical wave attenuation model for random waves is referred to zhu et al 2020a however for waves with strong nonlinearity and nonlinear wave wave interaction such as infragravity waves mullarney and pilditch 2017 the application of the present model warrants careful consideration in the field the background current also has significant influences on the reconfiguration of vegetation and kelp and waves gaylord et al 2003 rosman et al 2003 losada et al 2016 lei and nepf 2019a fredriksson et al 2020 zhang and nepf 2020 2021 lei et al 2021 which should be also implemented in the future work 4 2 nature based coastal protection strategies the case study showed the wave attenuation capacity of saccharina latissima varies seasonally with the kelp growth after about 6 5 months of growth 1 5 months in the nursery and five months in the ocean the kelp blade grows longer than 1 5 m and can provide considerable wave attenuation for different locations e g different water temperatures and salinities different water depths exposed sites and sheltered sites etc the growth rate may be different peteiro and freire 2013 nielsen et al 2014 augyte et al 2017 vettori and nikora 2017 azevedo et al 2019 large wave conditions with significant wave height greater than 1 5 m associated with storm events typically occur in the gulf of maine for example in winter and spring from october to may data from maine epscor seanet buoy c0502 in saco bay to develop large kelp blades to provide considerable wave attenuation in winter it is proposed that the kelp should be seeded several months earlier as kelp grows faster in cold water the kelp can be submerged near the seafloor during summer and then moved upward to the surface in winter to keep considerable wave attenuation of kelp farms for coastal protection around the year the biennial harvesting technique peteiro et al 2006 could be adopted as a result the kelp older than one year can be several meters long providing more favorable wave attenuation one recommended strategy is to harvest every other longline every other year to improve commercial values the multiple partial harvesting technique bak et al 2018 is also recommended such that only part of the blades is cut off leaving an adequate length of the blade to allow regrowth and provide considerable wave attenuation since the kelp may get biofouled degraded and ragged in summer førde et al 2016 one solution is to submerge the kelp canopy deep in water to improve the survivability peteiro et al 2006 in addition to dissipating wave energy kelp farms can also play a substantial role in marine carbon sequestration for climate change mitigation and adaptation krause jensen and duarte 2016 duarte et al 2017 unlike newly seeded kelp naturally occurring seagrass is less impacted by seasonality implementing seagrass with kelp aquaculture may mitigate the seasonal impacts in addition combining seagrass and suspended kelp aquaculture structures can improve their wave attenuation capacity for a wider range of wave periods and water levels zhu et al 2020a enhancing the size of kelp aquaculture farms and the plant density can also improve the wave attenuation capacity as expected zhu et al 2021 5 conclusion in this study an analytical wave attenuation model for flexible vegetation and kelp was developed by simplifying and linearizing the blade motion compared with a wide range of experiments for submerged and suspended canopies with flexible blades the simplified analytical model underestimated the wave decay coefficient by 27 but with a small nrmse of 0 054 in comparison the numerical model with full nonlinearity underestimated the wave decay coefficient by 11 7 with nrmse 0 063 to reduce the underestimation of the analytical model due to the simplification and linearization a modification factor was developed reducing the underestimation of the analytical model to 10 1 based on the analytical model analytical solutions for the bulk drag coefficient cd and effective blade length le were derived which showed a similar precision with the experimentally fitted cd and le indicating that the analytical solutions for cd and le could be a reliable alternative when the experimentally calibrated cd and le are not available the analytical solutions are computationally efficient and easy to implement into large scale models to analyze the influences of wave attenuation on processes such as coastal morphology inner shelf circulation and material transport a case study showed the wave attenuation of cultivated saccharina latissima changes seasonally with the kelp growth when the kelp blade reaches 2 4 m long after seven months of growth the kelp farms with 50 longlines over a distance of 200 m in the direction of wave propagation in 8 m deep water may attenuate wave energy by 29 for 1 m high waves with the period of 6 s the wave attenuation can be enhanced to 43 when the farms are located in 5 m deep water to provide considerable wave attenuation of kelp with adequate long blades around the year biennial and multiple partial harvesting techniques are recommended the research of using kelp aquaculture structures as nature based coastal protection is still in its infancy field observations of wave attenuation by kelp aquaculture structure are an important research gap to consider for future studies credit authorship contribution statement longhuan zhu conceptualization methodology software validation formal analysis investigation resources data curation writing original draft writing review editing visualization kimberly huguenard writing review editing supervision project administration david w fredriksson writing review editing jiarui lei conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was completed as part of the phd research of longhuan zhu who was supported by national science foundation award iia 1355457 to maine epscor at the university of maine the authors gratefully acknowledge the assistance of the advanced computing group of the university of maine system in producing the numerical data longhuan zhu was benefited from the discussions with gretchen grebe and zhilong liu finally the authors would like to thank the anonymous reviewers and associate editor for constructive comments that really helped to improve this manuscript greatly 
174,floods are among the most devastating natural hazards in the world with climate change and growing urbanisation floods are expected to become more frequent and severe in the future hydrodynamic models are powerful tools for flood hazard assessment but face numerous challenges especially when operating at a large scale the downside of discretising an area using a fine mesh yielding more accurate results is the expensive computational cost of simulations moreover critical input information such as bathymetry i e riverbed geometry are required but cannot be easily collected by field measurements or remote sensing observations during the past few years the development of sub grid models has gained a growing interest as these enable faster simulations by using coarser cells and at the same time preserve small scale topography variations within the cell in this study we propose and evaluate a modelling framework based on the shallow water 2d model with depth dependent porosity enabling to represent floodplain and riverbed topography through porosity functions to enable a careful and meaningful evaluation of the model we set up a 2d classical model and use it as a benchmark we also exploit ground truth data and remote sensing derived flood inundation maps to evaluate the proposed modelling framework and use as test cases the 2007 and 2012 flood events of the river severn our empirical results demonstrate a high performance and low computational cost of the proposed model for fast flood simulations at a large scale keywords shallow water model porosity based model flood inundation large scale high resolution data fast simulations 1 introduction with the increasing risk of more frequent and severe floods arnell and gosling 2016 due to climate change and growing urbanisation there is a crucial need to make more investments in flood management impacts of floods include human socio economical and environmental losses poorly conducted hazard assessments can lead to inefficient risk management from insufficient protective mitigation measures to expensive rebuilding of devastated areas baan and klijn 2004 meyer et al 2009 instead well conducted flood risk assessments provide a valuable support for decision making related to urban planning and emergency response preparedness therefore it is essential to improve flood management systems to better anticipate and further reduce potential flood risk pradhan et al 2014 tehrany et al 2014 in this context hydrological and hydraulic models play a central role in flood forecasting as they provide predictions of water streamflows and levels across various temporal and spatial scales revilla romero et al 2016 hostache et al 2018 the most common flood inundation hydraulic models are based on the depth averaged navier stokes equations also called de saint venant or shallow water equations swe the resolution of these equations can be carried out in one 1d or two dimensions 2d 1d models solve the 1d formulation of the swe bates and de roo 2000 where the flow is assumed to be unidirectional and water levels are assumed to be constant across sections although they are relatively easy to setup and fast to run cunge 1980 these models fail to provide accurate predictions of overbanking flow and in presence of complex topographies especially because the momentum transfers between the channel and the floodplain are neglected 1d storage area models also often referred to as 1d or quasi 2d are sometimes preferred as they include a representation of floodplain storage using a series of user defined polygonal compartments into which overbank flows can spill the flow between the main channel and the floodplain storage cells is modelled using stage discharge equations such as weir gate or orifice laws these can also be used to link storage cells to one another and the water level is then computed using volume conservation in each storage cell however these models also neglect the momentum conservation between storage areas in floodplains to tackle the previously described issues 2d approaches are adopted in 2d flood modelling a fine discretisation of the area of interest including main channel and floodplains is required to accurately represent topography consequently the main limitation of accurate modelling of large scale floods is associated with a very expensive computational cost an alternative approach relies on the coupling of 1d and 2d models this approach is not completely satisfying as it only accounts for mass transfers between the two models the actual key for this approach to be reliable is correctly representing edges of the 1d and 2d models making it possible to keep the spatial and temporal correlations of 1d and 2d models consistent zhang et al 2020 for instance willems et al 2002 represents the floodplain by fictitious river branches for which the calibration of friction coefficients is required to account for momentum a precise mapping of these branches is necessary to accurately delineate the flood extent that is otherwise often overestimated finaud guyot et al 2011 proposed a shallow water based model for river floodplain interactions using 1d and 2d elements in the main channel and floodplain respectively this allowed to improve the portraying head loss phenomena that can happen due to channel bends or meander shortcuts thanks to the inclusion of lateral momentum transfer between the river and the floodplain to correctly capture flood dynamics there is a need to further reduce the computational time while ensuring precise representation of river floodplain connections sub grid modelling approaches have tackled this challenge and gained a growing interest as they are a good compromise between accuracy and high computational efficiency indeed they enable faster simulations as they use coarser computation cells while accounting for small scale topography variations within the cells for example lisflood fp is a fast running and relatively easy to set up model and its standard version was introduced by bates and de roo 2000 it was further developed by neal et al 2012 who proposed a version enabling subgrid capability in the channel this assumes a simplified channel shape and uses a 1d approach for the channel flow simulation and a 2d approach for the floodplain the adopted subgrid channel approach allows representing any river channel size even below the grid resolution however the subgrid approach only applies to the river and the resolution in the floodplain has to be rather high to accurately represent floodplain flows and inundation extent other modelling techniques rely on two dimensional shallow water models including the porosity concept as a way to upscale the traditional shallow water equations porosity is defined as the fraction of a computational cell edge available to the flow porosity based models have evolved over the past twenty years first defina et al 1994 introduces the shallow water model with isotropic porosity the formulation for partially wet dry areas over irregular domains is later improved by defina 2000 casulli 2015 and hervouet et al 2002 in the single porosity model guinot and soares frazão 2006 a differential formulation is derived using a finite volume scheme which is further evaluated by soares frazão et al 2008 later sanders et al 2008 shifts the focus from isotropic to anisotropic porosity by proposing the integral porosity model where connective porosity through edges is distinguished from storage porosity within cells guinot 2012 then merges the isotropic and anisotropic models and introduces a multiple porosity approach for applications in urban areas kim et al 2015 investigates the porosity based model errors and show that they are lower for anisotropic models than for isotropic approaches this approach is further extended by introducing a dual integral porosity dip model guinot et al 2017 the sw2d ddp model was presented and evaluated in guinot et al 2018 in this article synthetic test cases a series of dam break configurations and a meandering channel and an experimental test case were used to evaluate and validate the model results two closure laws integral porosity and the depth dependent dual integral porosity were compared to a fine 2d model that solves the classical shallow water equations with the second order muscl evr scheme results show the superiority of the proposed dip closure model on the ip model moreover the paper presented a shallow water model based on the dip approach with depth variable porosity fields sw2d ddp and it has been found that although porosity approach cannot represent details within the cells it shows good agreement with the average values of the fine 2d model moreover the cpu ratio between ddp and fine 2d models ranges from 310 to 2900 in this paper we apply and validate the sw2d ddp model on a real large scale test case since it was already evaluated on synthetic test cases in guinot et al 2018 the provision of accurate bathymetric data is critical in hydrodynamic modelling yet obtaining this information where in situ measurements are lacking is not always possible in this context hostache et al 2015 proposes a method for retrieving effective riverbed bathymetry based on the assimilation of water level measurements acquired by a drifting gps buoy into a 1d hydraulic model and many bathymetry retrieval methods have been recently developed in the framework of swot satellite mission preparation e g oubanas et al 2018 yoon et al 2012 durand et al 2014 in the perspective of retrieving bathymetry that is often unknown larnier et al 2021 delenne et al 2021 we propose here to represent it via depth dependent porosity functions we therefore hypothesise that bathymetry can be effectively represented through depth dependent porosity parameters in this context the main objective of this study is to develop and evaluate a modelling framework based on sw2d ddp enabling fast flood inundation simulations at a large scale while representing for the first time both bathymetry and small scale floodplain topography using depth dependent porosity within comparatively large computational cells to evaluate the proposed modelling framework with scrutiny we compare the sw2d ddp simulation result with a standard 2d shallow water model on one hand moreover we evaluate both model results using ground truth data in situ measured and remote sensing derived benchmarking the sw2d ddp model against a high resolution 2d model enables the evaluation of our approach across space and time it also helps to explore strengths and limitations of the proposed approach in comparison with state of the art approaches our study site is a 1500 km 2 floodplain located at the confluence of the severn and avon rivers in the united kingdom which has frequently experienced flooding especially in the last decades the 2007 and 2012 flood events are used as test cases the remainder of this paper is organised as follows in section 2 we present the proposed modelling framework enabling the simplified representation of bathymetry and topography via porosity and then the design of the experiment to evaluate the model performance next the study site and available data as well as the models set up are described in section 3 in section 4 we evaluate the simulated flood extent and water level maps finally section 5 discusses the main outcomes of the study 2 methodology and experimental setup this section first describes the proposed modelling approach based on the shallow water 2d model with sw2d ddp next it presents the experimental setup for evaluating the model performance using a standard 2d shallow water sw2d model as a reference as well as in situ measured and remote sensing derived data 2 1 modelling framework sw2d 1 1 https sw2d inria fr is a modelling suite that has been progressively developed and further improved since 2002 it solves the 2d shallow water equations with a finite volume scheme on structured or unstructured grids the sw2d ddp model guinot et al 2018 introduces a depth dependent porosity that accounts for small scale effects of obstacles to the flow in a macroscopic way without the need to detail their geometry in the mesh although the whole domain is represented as flat in the mesh a bottom elevation z b is provided inside each cell via a porosity distribution as a function of the water depth within a domain d the model distinguishes storage cell porosity ϕ ω from edge connective porosity ϕ γ the storage porosity for a given cell is the adimensional area available for water at the elevation z s as detailed in guinot et al 2018 the standard shallow water equations are multiplied by the phase function ɛ defined as 1 ɛ x y z 1 if z z b 0 otherwise where ɛ x y z is the phase indicator for the point coordinates x y z that is equal to 0 if the point is in the solid phase i e lower than the bottom elevation z b the porosities represent the amount of water that can be stored per unit domain and boundary for a unit variation in the free surface elevation z s x y which is assumed to be known thus the porosities over cells ω and edges γ are defined as 2 ϕ d z 1 d d ɛ x y z d d d ω γ where d stands for either a cell ω or an edge γ this allows to uniquely define the volume of water stored per unit area length in the sub domain d between the ground and the elevation z as 3 θ d z z ϕ d ζ d ζ small scale topography information is therefore taken into account via porosity laws 15 in the sw2d ddp software four law types are proposed in this model setup we have chosen to use only two types of porosity law 0 and 3 for the sake of simplicity and in order to show how porosity represents and preserves high resolution topographic data law 0 is used for defining storage porosity in the floodplain fig 1 the distribution of ground elevations z b x y within each cell is first retrieved from the digital elevation model dem next it is discretised using a piecewise constant function of n segments with equidistant porosity values associated to elevation values with the following relation 4 ϕ z i i n i 1 n where z i is the subgrid water depth associated to a porosity ϕ i and n is the number of segments law type 3 allows us to handle porosities inside riverbed cells in line with the objective of minimising the number of cells in the model mesh and therefore reducing computational time we propose to define cells with dimensions larger than the riverbed width moreover to avoid elongated cells that can be responsible for model instabilities we maintain the length of the computational cells along the streamflow direction at maximum twice its width fig 2 since bathymetric data is rarely available we propose to represent riverbed geometry using a simplified trapezoidal shape assumption via the porosity law type 3 fig 1 first storage porosities are computed then the porosity law type used for the edges is selected depending on the location of their adjacent cells law type 3 is used inside the riverbed cross sections i e edges between two cells of type 3 and law type 0 is used in the floodplain between two cells of type 0 and on river banks between a riverbed and a floodplain cells to accurately represent overbank flows the nodes of the river bed cells are positioned on the dikes in both models indeed when positioning interfaces upon constrictions and obstacles these latter are implicitly considered in the interface flux calculation while the same obstacles disappear from the numerical representation when they are located inside the cells moreover to ensure that high points are correctly taken into account without too much overloading the mesh design process we choose to automatically compute the edge porosity values as the minimum of the porosities of their neighbouring cells it is worth mentioning that the parameter retrieval of the porosity law in each cell and edge is carried out automatically using the available dem and bathymetric information 2 2 experimental design to the best of our knowledge our modelling framework enables for the first time to represent both riverbed and floodplain subgrid topography using porosity laws to evaluate its advantages and limitations we compare the sw2d ddp model with a standard fine 2d model namely sw2d inria lemon team 2022 in terms of simulated water depths and inundation extents moreover to further assess our modelling approach we evaluate it against observed flood extents from aerial photographs and satellite images and against observed water level time series from in situ measurements when available 2 2 1 the standard 2d shallow water model to enable a meaningful comparison between the two approaches the standard model has to use exactly the same input data as the porosity model topography bathymetry boundary and initial conditions and parameters e g friction coefficient numerical scheme the two models differ only in the way they represent the floodplain and riverbed topography the standard model being based on a classical finite volume scheme the bottom elevation inside a computational cell has a unique value equal to the average elevation of the cell s nodes as a consequence topography can be smoothed out within each cell when flow obstructions drains or structures e g dikes roads streams are not intrinsically represented via cells smaller than their dimensions indeed adequately representing dikes drains or river channels requires to include several mesh cells within each of these structuring elements therefore in the standard sw2d model the mesh needs to be designed in a way that entire cells are placed explicitly on hydraulic structures or singularities for instance when representing a drain of 5 m width cells have to be well placed to capture its effect otherwise it would be transparent for the model having many of these structures in large scale areas would require a long time to represent them in a standard sw2d model 2 2 2 evaluation method to evaluate the porosity model performance we propose an approach composed of several successive steps detailed in the following paragraphs i post process model results to derive flood extent and water depth maps in the same format ii compare flood extent and water level maps extracted from both models on a daily basis iii evaluate flood maps extracted from both models using remote sensing derived data and iv evaluate simulated water level time series against in situ observation data in this study we chose to evaluate the proposed modelling approach in terms of simulated water levels first using punctual in situ water level measurements next as spatially distributed water level cannot be derived from in situ observation we also compare the sw2d ddp results with those obtained using the standard sw2d model whether we use the sw2d model results or the measurements provided by a camera or a gauging station to evaluate water levels we computed the root mean squared deviations rmsd eq 7 when we evaluate the model in terms of flood extents two types of references are used i the flood extent maps simulated by sw2d and ii the ones derived from a satellite imagery post processing of model results we aim to compare the results of the porosity and the reference models in terms of flood extents and water levels by definition the bottom elevation of the cells in the porosity and standard model meshes are taken into account differently in the standard sw2d model as previously mentioned the bottom elevation of a cell corresponds to the average elevation of its nodes in the porosity model the subgrid elevation variability is accounted for via the porosity since the edges of the fine and coarse grid cells do not overlay the flood extent maps derived from the two models are resampled to the original dem resolution 2 m to enable a pixel to pixel comparison to do so the cell is considered flooded when the simulated water depth reaches a minimum value h min i e when z s z b h min where z b is the cell bottom elevation for the standard model and the dem elevation for the porosity model the h min is set to 0 1 m which corresponds approximately to the vertical accuracy of the lidar dem evaluation of simulated flood extent maps the simulated flood extents evaluation is carried out twice using as a reference either i the standard model or ii the available earth observation data based on a pixel by pixel comparison we compute a confusion matrix composed of four metrics 1 the number of pixels that are unflooded in both maps i e tn true negatives 2 the number of pixels flooded only in the standard model i e fn false negatives 3 the number of pixels flooded only in the porosity sw2d ddp model i e fp false positives and 4 the number of pixels flooded in both maps i e tp true positives to compare the simulated and the reference maps we compute contingency maps as overall performance metrics we use the critical success index csi schaefer 1990 and the overall accuracy oa that are both derived from the confusion matrix csi and oa quantify the goodness of fit between the evaluated map and the reference maps see eqs 5 and 6 the csi represents the ratio of the number of pixels correctly predicted as flooded tp over the number of all flooded pixels 5 csi tp tp fp fn the oa takes into account the agreement of non flooded areas and is defined as follows 6 oa tp tn tp fp fn tn these scores vary between 0 and 1 with the highest value attained when the predictions present a perfect fit with the reference evaluation of simulated water level maps to quantitatively measure discrepancies between the simulations and reference water level maps we use the root mean square deviations rmsd eq 7 between the porosity model predicted water levels z i sim and the reference water levels z i ref eq 7 resampled at the dem resolution 2 m and for each of the n inundated pixels of the entire domain 7 rmsd 1 n i 1 n z i sim z i ref 2 moreover to further evaluate the distribution of the differences between the simulated and reference water levels we make use of boxplots showing the deviation distribution based on statistical metrics 1 the lower bound 2 the first quartile q1 25th percentile 3 the median q2 50th percentile 4 the third quartile q3 75th percentile and 5 the upper bound the interquartile range iqr goes from the 25th to the 75th percentile and therefore represents 50 of the data values the maximum value of the boxplot is defined as q3 1 5 iqr and the minimum value q1 1 5 iqr outlier points are thus eliminated from the plot for the sake of readability evaluation of simulated water levels time series water level time series obtained from each of the porosity and standard models are evaluated against in situ observation data for visual comparison these time series are plotted then to quantitatively measure the discrepancies we compute the root mean square deviations rmsds as described in the previous section 3 study area experimental data and model setup the severn the longest river in great britain extends from its source at plynlimon in the welsh hills to the mouth of the bristol channel the overall catchment area covers approximately 11 000 km2 and is predominantly rural apart from some urban settlements like worcester tewkesbury and evesham fig 3 shows the model domain and river network with the location of the available gauging stations and the camera location offering live imagery on the river severn the study site is located at the confluence of rivers severn and avon around the city of tewkesbury and has been subject to frequent flooding due to intense precipitation the area of interest covers approximately 15 10 km2 two flood events of different magnitude will be simulated and analysed to better understand the model behaviour with changes in boundary conditions the july 2007 and november 2012 flood events hydrometric data two suitable gauging stations are located at saxons lode along the severn river and evesham along the avon river upstream of the confluence due to the backwater effect observed at bredon the streamflow time series is estimated there from that recorded at evesham gauging station located upstream of bredon and delayed in time based on an estimated wave travel time mythe bridge is situated upstream the confluence of the severn avon rivers and deerhurst is situated downstream hydrometric data are provided by the uk environmental agency ea at 15 min intervals moreover the tewkesbury stationary camera fig 3 mounted on the wall of a building in march 2011 provided a view on the avon river which allowed taking hourly daylight images during the 2012 flood event this camera enabled the estimation of water levels in the river vetra carvalho et al 2020 which are used to evaluate the hydraulic model performance inside the domain earth observation data the flood event of july 2007 is particularly interesting because an airborne campaign imaged the flood at a very high resolution 50 cm on july 24 close to the flood peak giustarini et al 2012 flood extents were manually digitised on this imagery this extracted flood map allows evaluating the simulated flood extents at the same date 24 july the hierarchical split based approach proposed by chini et al 2017 is used to derive flood extent maps from the cosmos skymed images acquired on the following dates 27 28 29 30 november and 01 02 04 december 2012 these flood maps are considered for evaluating synchronous flood extent maps simulated by the porosity and the standard models topographic and bathymetric data a lidar dem at 2m spatial resolution with a vertical accuracy of 0 10 m provided by the uk environmental agency ea wood et al 2016 is used to provide the model with ground elevation bathymetric data is reconstructed using three river cross section measurements at the upstream saxons lode and bredon and downstream deerhurst boundaries of the model to do so first we approximate the observed cross sections using a trapezoidal shape the bank lines are manually digitised along the avon and severn river streams and river stream bottom lines are automatically generated as parallels to the bank lines using a distance estimated based on the observed cross sections next the bank elevations are estimated by extracting ground elevation provided by the lidar dem along the bank lines then the river bottom elevation is linearly interpolated between the three trapezoidal cross sections along the avon and severn bottom lines based on the river banks and bottom lines with associated elevation values we interpolate river bathymetry finally the interpolated bathymetric data is merged with topographic data to form a single model input 3 1 model setup while the standard model mesh is composed of 29 772 cells the porosity model mesh contains only 1042 cells concerning the mesh design in the sw2d ddp model just like in any other hydraulic model the cell including the river should not be too large as the porosity law used in river cells considers the flood plain as horizontal rectangular above trapezoidal shape for other cells no brutal variations in terms of surface should be found between adjacent cells the influence of the number of tabulations n inside a cell has been investigated in guinot et al 2018 since the spatial information is lost within a coarse grid cell it is essential to ensure that obstacles are captured by the 5 tabulation levels discharge time series are imposed as upstream boundary conditions of the hydraulic model severn at saxons lode and avon at bredon the streamflow time series in saxons lode are derived from water surface elevation records using a rating curve water level time series are used as downstream boundary condition at deerhurst fig 4 the initial condition is a fixed water level equal to the downstream condition a uniform strickler coefficient k s 50 m 1 3 s 1 is used for the riverbed and the floodplain spatially distributed parameters could easily be prescribed but a sensitivity analysis not shown in this paper showed that the influence of the friction coefficient was limited for the studied flood event the durations of 2007 and 2012 flood event simulations are 17 days 18 july 04 august and 15 days 21 november 06 december respectively 4 results 4 1 evaluation of simulated flood extent maps fig 5 shows the csi and oa time series computed on a daily frequency for evaluating the sw2d ddp simulated flood maps using the sw2d simulated flood maps as reference it can be seen that both simulated flood extent maps are most of the time in agreement for both flood events at the flood peak in figs 6b and 7b there is a very good agreement between the two models accuracy of 95 the model agreement is slightly lower in the rising limb and decreases more in the falling limb this implies that the draining dynamic in the sw2d ddp model is different from that in the sw2d model figs 6 and 7 show a series of contingency maps obtained by comparing the simulated flood extent maps by the sw2d ddp and sw2d models during the 2007 and 2012 flood events during the rising limbs figs 6a and 7a the porosity model exhibits a good agreement with the standard model while locally inundating slightly larger areas especially in the upstream part as well as in little drains in the urban zone at the severn avon confluence see box in fig 6a this indicates the porosity model induces overbanking earlier than the standard model oppositely a smaller inundation extent is visible locally nearby the avon river figs 6c and 7c show a substantially larger flood extent simulated by sw2d this indicates that almost all floodplain water came back to the stream in the sw2d ddp simulation while a substantial volume of water remains present in the floodplain in the sw2d simulation this effect is dominant in the eastern severn floodplain and around the urban settlements overall figs 6c and 7c suggest that the porosity model fills in and drains floodplain water faster than the standard model to better understand and assess this aspect we also compare both model results to remote sensing derived data for the 2007 event the porosity and standard model derived flood maps were evaluated against the flood map extracted from aerial photography and showed similar levels of agreement csi 0 92 oa 0 95 and csi 0 9 oa 0 94 respectively for sw2d ddp and sw2d during the 2012 flood simulation both models are in good agreement with lower scores for the last satellite image 04 december see table 1 csi and oa are rather similar for the two models but it is worth highlighting that the metrics of the porosity model are always exceeding those of the standard model fig 8 shows the contingency maps computed by each of the models using as a reference the satellite flood map acquired on december 04 the most important differences between the two simulated flood extent maps are exhibited close to tewkesbury where sw2d ddp drains water faster than sw2d the flood extent map derived from sw2d therefore exhibit in fig 8a a substantial overestimation when compared to the flood extent map derived from a cosmo skymed image however this overestimation has to be interpreted carefully as sar backscatter images do not enable floodwater detection in dense urban areas chini et al 2019 moreover giustarini et al 2012 showed for the same study area that part of the floodwater was detectable during the 2007 flood event inside tewkesbury using a high resolution sar backscatter image i e a terrassar x image as a consequence one can argue that the absence of floodwater within tewkesbury in the cosmo skymed images acquired in 2012 lends more weight to the sw2d ddp flood extent map reliability 4 2 evaluation of simulated water level maps fig 9 shows time series of root mean square deviations rmsd calculated between the porosity and standard model derived water levels at a daily frequency across the inundated areas the corresponding time averaged rmsds are equal to 12 32 cm and 6 3 cm for the 2007 and the 2012 flood events respectively the highest deviations are observed in the falling and rising limbs during the flood peaks are reduced and vary between 3 and 9 cm from a practical point of view depth deviations ranging from 10 to 15 cm in flood predictions can arguably be considered as acceptable given the vertical accuracy of the lidar used 10 cm sanders 2007 mason et al 2003 furthermore boxplots are used to assess the distribution of differences between water levels simulated by the porosity and the standard models at a daily time step fig 10 at first sight it is found that the model results present a very good agreement at the flood peak since the boxplot height is very small positive values in the boxplots refer to higher water levels simulated by the porosity model this is mainly observed during the rising limb and at the flood peak this indicates that the porosity model simulates the overbanking earlier this result is in agreement with the larger water extent simulated by the porosity model see figs 6a and 7a during the falling limb lower values of water levels simulated by the porosity model express a larger inundation extent computed by the standard model as obtained in figs 6c and 7c 4 3 evaluation of simulated water levels time series simulated porosity and standard model water level time series are first evaluated using in situ observations at the mythe bridge hydrometric station and second using water levels estimated from the tewkesbury camera images when inter comparing the two models the results present a very good agreement figs 11a 11b and 11c the highest discrepancies of simulated water levels compared to the gauge observations 0 90 and 0 60 m are reached just before the 2007 flood peak fig 11a and on the first day of the 2012 flood simulation fig 11b respectively the evaluation further shows reduced model errors during the falling limb of the 2012 flood event where the porosity model exhibit an error of less than 5 cm approximately this is probably related to the initial condition fixed in the simulation that is set as uniform and fits the downstream level deerhurst on another note rmsds are slightly improved albeit not significantly in the porosity model fig 9 both model results are also assessed using the camera images at tewkesbury see location in fig 3 the highest discrepancies with the gauge data are found in the rising limb they are reduced when approaching the flood peak and almost fit the model results at the falling limb table 2 shows the porosity and standard model scores using the rmsd metrics computed on water level time series the considered reference is the data observed at mythe bridge for the 2007 and 2012 events and at tewkesbury for the 2012 event 5 discussion as described in section 4 2 the water depth deviations of the porosity based model with respect to the standard sw2d model are acceptable given the vertical accuracy of the lidar used c a 10 cm the average flow depth in the rivers estimated over the entirety of the flood event is about 7 m generally speaking the average errors c a 6 to 12 cm are not substantial high errors reaching a maximum of 25 cm are observed in the rising and the falling limb where the porosity model seems to fill in and evacuate faster than the standard model on the other hand errors with respect to the gauge camera data reach a maximum of 60 to 90 cm respectively since the real bathymetry and bed shape of the river are unknown this potentially affects the simulation results in general and can be further improved in fig 11 the simulated levels at mythe bridge are lower than the observed ones especially during the rising limb and the flood peak this is arguably due to the simplified representation of the bathymetry in the models and to an underestimated upstream inflow for the severn river at saxon s lode under high flow conditions indeed as the river burst its banks around saxon s lode the floodplain starts conveying a part of the flow that is not accounted for in the corresponding model boundary condition derived from the riverstream gauging station in terms of flood extents results show the porosity model fills in and drains floodplain water faster than the standard model to better assess this behaviour we compared both model results to a series of remote sensing derived flood maps it was shown that during the falling limb the observed inundation extent is closer to the one simulated with the porosity model especially in the areas around tewkesbury this faster flooding and receding dynamic in the porosity model is mainly related to its ability to represent small scale topography and drains via porosity as mentioned in section 2 2 1 representing small drains in the standard model requires cells with dimensions smaller than that of the drains this means that drains should in theory be finely discretised by very small cells fig 12 in the sw2d model these drains are visible in the lidar topographic data fig 3 but they are not captured by our standard model mesh because the mesh cells are comparatively large for example the size of a cell capturing a drain would have dimensions smaller than 5 m which would increase the number of computational cells along the drainage network the standard mesh designed in this study consists of 29 772 cells representing approximately 28 times more cells than that in the porosity mesh 1042 cells moreover the simulation run time in that case would escalate drastically with the decrease of the simulation time step becoming inconvenient for large scale applications table 3 summarises the cpu times necessary for the standard and the porosity model simulations carried out on a computer with an i7 4770 cpu processor and a memory of 16 gb ram for an area of 1500 km 2 and in both test cases the cpu time required for the model simulation is 13 min vs 3 2 days for the standard model for a 17 days flood simulation and 12 min vs 2 9 days for a 15 flood day event the porosity model therefore offers the advantage of a fast model setup while preserving high resolution data by using coarse grid cells thus enabling reduced computational efforts this paves the way for real time applications and long terms simulations over large areas all singularities and types of cross section can be taken into account in the porosity laws as long as they are visible in the dtm however the spatial localisation of the singularity inside a coarse grid cell is lost this is why it is preferable to place the interfaces on the singularities so as not to create artificial links between the cells since cross sections are rarely available along the entire river and only punctual measurements are provided a riverbed shape approximation must be made which is facilitated by the use of porosity laws the interpolation of river bathymetry between observed cross sections certainly has an influence on the model results but it is the only available information in this study we compared both models using the same bathymetric data the linear interpolation of these profiles along the river appeared to be reasonable since no brutal variations of the slopes were observed while examining longitudinal profiles however further improvements are expected when having more precise bathymetry data in this study the topography information is derived from a high resolution lidar dtm originally at 2 m resolution and resampled at 10 m with increasing availability of global dems e g srtm 30 m at a global scale the modelling approach can be applied in poorly gauged areas for the specific case of urban areas the building location map could be used to improve the dem based on widely available databases such as openstreetmap the vegetation remains more complex to be accounted for and for the moment we may consider vegetation effects through an increased friction coefficient as usually done in hydrodynamic modelling in the sw2d software classical or ddp all parameters such as infiltration rates and friction can be spatially distributed e g based on land use maps as discussed in guinot et al 2018 one main limitation of the porosity modelling approach is the definition of a unique water level per computational cell which is equivalent to considering a horizontal free surface elevation in each cell although the consequences of such an assumption are limited when dealing with large scale and slow floods they may not be negligible and should be assessed the porosity based approach also leads to a loss of spatial information inside coarse grids this can potentially create artificial links between cells unless the edges are carefully placed upon local highest points however one should keep in mind that this is also true for other hydrodynamic models such as classical 2d ones moreover as seen before it is possible to recover spatial information by resampling the results on the dtm as proposed in this paper therefore preserving the original dtm data at its original resolution 6 conclusion in this paper we proposed an innovative modelling framework based on porosity to rapidly simulate flood inundations this framework enables for the first time to represent both bathymetry and small scale floodplain topography using depth dependent porosity within comparatively large computational cells simulating two real test case floods over a 1500 km 2 area around the severn and avon confluence has shown the following 1 the proposed modelling approach enable to simulate flood extent maps very similar to the one simulated by the standard sw2d model with 90 agreement 2 the evaluation based on in situ measurements indicates that the porosity model is exhibiting levels of performance comparable to and even higher than those of a standard model 3 it is found that the porosity model is able to account for small drains within comparatively very large cells representing these small drains in a standard model would require very small cells therefore leading to a much higher number of cells and a large computational demand 4 our experiment shows that the sw2d ddp model simulations are c a 350 times faster than that of the standard model thereby substantially reducing computational costs in perspective the proposed modelling framework facilitates the retrieval of an effective bathymetry as this is represented via the porosity parameters this opens up new perspectives for large scale applications over areas where bathymetric data are not available declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by the national research fund of luxembourg through the cascade project grant no c17 sr 11682050 the hydrometric data is provided by the uk environment agency all authors approved the version of the manuscript to be published 
174,floods are among the most devastating natural hazards in the world with climate change and growing urbanisation floods are expected to become more frequent and severe in the future hydrodynamic models are powerful tools for flood hazard assessment but face numerous challenges especially when operating at a large scale the downside of discretising an area using a fine mesh yielding more accurate results is the expensive computational cost of simulations moreover critical input information such as bathymetry i e riverbed geometry are required but cannot be easily collected by field measurements or remote sensing observations during the past few years the development of sub grid models has gained a growing interest as these enable faster simulations by using coarser cells and at the same time preserve small scale topography variations within the cell in this study we propose and evaluate a modelling framework based on the shallow water 2d model with depth dependent porosity enabling to represent floodplain and riverbed topography through porosity functions to enable a careful and meaningful evaluation of the model we set up a 2d classical model and use it as a benchmark we also exploit ground truth data and remote sensing derived flood inundation maps to evaluate the proposed modelling framework and use as test cases the 2007 and 2012 flood events of the river severn our empirical results demonstrate a high performance and low computational cost of the proposed model for fast flood simulations at a large scale keywords shallow water model porosity based model flood inundation large scale high resolution data fast simulations 1 introduction with the increasing risk of more frequent and severe floods arnell and gosling 2016 due to climate change and growing urbanisation there is a crucial need to make more investments in flood management impacts of floods include human socio economical and environmental losses poorly conducted hazard assessments can lead to inefficient risk management from insufficient protective mitigation measures to expensive rebuilding of devastated areas baan and klijn 2004 meyer et al 2009 instead well conducted flood risk assessments provide a valuable support for decision making related to urban planning and emergency response preparedness therefore it is essential to improve flood management systems to better anticipate and further reduce potential flood risk pradhan et al 2014 tehrany et al 2014 in this context hydrological and hydraulic models play a central role in flood forecasting as they provide predictions of water streamflows and levels across various temporal and spatial scales revilla romero et al 2016 hostache et al 2018 the most common flood inundation hydraulic models are based on the depth averaged navier stokes equations also called de saint venant or shallow water equations swe the resolution of these equations can be carried out in one 1d or two dimensions 2d 1d models solve the 1d formulation of the swe bates and de roo 2000 where the flow is assumed to be unidirectional and water levels are assumed to be constant across sections although they are relatively easy to setup and fast to run cunge 1980 these models fail to provide accurate predictions of overbanking flow and in presence of complex topographies especially because the momentum transfers between the channel and the floodplain are neglected 1d storage area models also often referred to as 1d or quasi 2d are sometimes preferred as they include a representation of floodplain storage using a series of user defined polygonal compartments into which overbank flows can spill the flow between the main channel and the floodplain storage cells is modelled using stage discharge equations such as weir gate or orifice laws these can also be used to link storage cells to one another and the water level is then computed using volume conservation in each storage cell however these models also neglect the momentum conservation between storage areas in floodplains to tackle the previously described issues 2d approaches are adopted in 2d flood modelling a fine discretisation of the area of interest including main channel and floodplains is required to accurately represent topography consequently the main limitation of accurate modelling of large scale floods is associated with a very expensive computational cost an alternative approach relies on the coupling of 1d and 2d models this approach is not completely satisfying as it only accounts for mass transfers between the two models the actual key for this approach to be reliable is correctly representing edges of the 1d and 2d models making it possible to keep the spatial and temporal correlations of 1d and 2d models consistent zhang et al 2020 for instance willems et al 2002 represents the floodplain by fictitious river branches for which the calibration of friction coefficients is required to account for momentum a precise mapping of these branches is necessary to accurately delineate the flood extent that is otherwise often overestimated finaud guyot et al 2011 proposed a shallow water based model for river floodplain interactions using 1d and 2d elements in the main channel and floodplain respectively this allowed to improve the portraying head loss phenomena that can happen due to channel bends or meander shortcuts thanks to the inclusion of lateral momentum transfer between the river and the floodplain to correctly capture flood dynamics there is a need to further reduce the computational time while ensuring precise representation of river floodplain connections sub grid modelling approaches have tackled this challenge and gained a growing interest as they are a good compromise between accuracy and high computational efficiency indeed they enable faster simulations as they use coarser computation cells while accounting for small scale topography variations within the cells for example lisflood fp is a fast running and relatively easy to set up model and its standard version was introduced by bates and de roo 2000 it was further developed by neal et al 2012 who proposed a version enabling subgrid capability in the channel this assumes a simplified channel shape and uses a 1d approach for the channel flow simulation and a 2d approach for the floodplain the adopted subgrid channel approach allows representing any river channel size even below the grid resolution however the subgrid approach only applies to the river and the resolution in the floodplain has to be rather high to accurately represent floodplain flows and inundation extent other modelling techniques rely on two dimensional shallow water models including the porosity concept as a way to upscale the traditional shallow water equations porosity is defined as the fraction of a computational cell edge available to the flow porosity based models have evolved over the past twenty years first defina et al 1994 introduces the shallow water model with isotropic porosity the formulation for partially wet dry areas over irregular domains is later improved by defina 2000 casulli 2015 and hervouet et al 2002 in the single porosity model guinot and soares frazão 2006 a differential formulation is derived using a finite volume scheme which is further evaluated by soares frazão et al 2008 later sanders et al 2008 shifts the focus from isotropic to anisotropic porosity by proposing the integral porosity model where connective porosity through edges is distinguished from storage porosity within cells guinot 2012 then merges the isotropic and anisotropic models and introduces a multiple porosity approach for applications in urban areas kim et al 2015 investigates the porosity based model errors and show that they are lower for anisotropic models than for isotropic approaches this approach is further extended by introducing a dual integral porosity dip model guinot et al 2017 the sw2d ddp model was presented and evaluated in guinot et al 2018 in this article synthetic test cases a series of dam break configurations and a meandering channel and an experimental test case were used to evaluate and validate the model results two closure laws integral porosity and the depth dependent dual integral porosity were compared to a fine 2d model that solves the classical shallow water equations with the second order muscl evr scheme results show the superiority of the proposed dip closure model on the ip model moreover the paper presented a shallow water model based on the dip approach with depth variable porosity fields sw2d ddp and it has been found that although porosity approach cannot represent details within the cells it shows good agreement with the average values of the fine 2d model moreover the cpu ratio between ddp and fine 2d models ranges from 310 to 2900 in this paper we apply and validate the sw2d ddp model on a real large scale test case since it was already evaluated on synthetic test cases in guinot et al 2018 the provision of accurate bathymetric data is critical in hydrodynamic modelling yet obtaining this information where in situ measurements are lacking is not always possible in this context hostache et al 2015 proposes a method for retrieving effective riverbed bathymetry based on the assimilation of water level measurements acquired by a drifting gps buoy into a 1d hydraulic model and many bathymetry retrieval methods have been recently developed in the framework of swot satellite mission preparation e g oubanas et al 2018 yoon et al 2012 durand et al 2014 in the perspective of retrieving bathymetry that is often unknown larnier et al 2021 delenne et al 2021 we propose here to represent it via depth dependent porosity functions we therefore hypothesise that bathymetry can be effectively represented through depth dependent porosity parameters in this context the main objective of this study is to develop and evaluate a modelling framework based on sw2d ddp enabling fast flood inundation simulations at a large scale while representing for the first time both bathymetry and small scale floodplain topography using depth dependent porosity within comparatively large computational cells to evaluate the proposed modelling framework with scrutiny we compare the sw2d ddp simulation result with a standard 2d shallow water model on one hand moreover we evaluate both model results using ground truth data in situ measured and remote sensing derived benchmarking the sw2d ddp model against a high resolution 2d model enables the evaluation of our approach across space and time it also helps to explore strengths and limitations of the proposed approach in comparison with state of the art approaches our study site is a 1500 km 2 floodplain located at the confluence of the severn and avon rivers in the united kingdom which has frequently experienced flooding especially in the last decades the 2007 and 2012 flood events are used as test cases the remainder of this paper is organised as follows in section 2 we present the proposed modelling framework enabling the simplified representation of bathymetry and topography via porosity and then the design of the experiment to evaluate the model performance next the study site and available data as well as the models set up are described in section 3 in section 4 we evaluate the simulated flood extent and water level maps finally section 5 discusses the main outcomes of the study 2 methodology and experimental setup this section first describes the proposed modelling approach based on the shallow water 2d model with sw2d ddp next it presents the experimental setup for evaluating the model performance using a standard 2d shallow water sw2d model as a reference as well as in situ measured and remote sensing derived data 2 1 modelling framework sw2d 1 1 https sw2d inria fr is a modelling suite that has been progressively developed and further improved since 2002 it solves the 2d shallow water equations with a finite volume scheme on structured or unstructured grids the sw2d ddp model guinot et al 2018 introduces a depth dependent porosity that accounts for small scale effects of obstacles to the flow in a macroscopic way without the need to detail their geometry in the mesh although the whole domain is represented as flat in the mesh a bottom elevation z b is provided inside each cell via a porosity distribution as a function of the water depth within a domain d the model distinguishes storage cell porosity ϕ ω from edge connective porosity ϕ γ the storage porosity for a given cell is the adimensional area available for water at the elevation z s as detailed in guinot et al 2018 the standard shallow water equations are multiplied by the phase function ɛ defined as 1 ɛ x y z 1 if z z b 0 otherwise where ɛ x y z is the phase indicator for the point coordinates x y z that is equal to 0 if the point is in the solid phase i e lower than the bottom elevation z b the porosities represent the amount of water that can be stored per unit domain and boundary for a unit variation in the free surface elevation z s x y which is assumed to be known thus the porosities over cells ω and edges γ are defined as 2 ϕ d z 1 d d ɛ x y z d d d ω γ where d stands for either a cell ω or an edge γ this allows to uniquely define the volume of water stored per unit area length in the sub domain d between the ground and the elevation z as 3 θ d z z ϕ d ζ d ζ small scale topography information is therefore taken into account via porosity laws 15 in the sw2d ddp software four law types are proposed in this model setup we have chosen to use only two types of porosity law 0 and 3 for the sake of simplicity and in order to show how porosity represents and preserves high resolution topographic data law 0 is used for defining storage porosity in the floodplain fig 1 the distribution of ground elevations z b x y within each cell is first retrieved from the digital elevation model dem next it is discretised using a piecewise constant function of n segments with equidistant porosity values associated to elevation values with the following relation 4 ϕ z i i n i 1 n where z i is the subgrid water depth associated to a porosity ϕ i and n is the number of segments law type 3 allows us to handle porosities inside riverbed cells in line with the objective of minimising the number of cells in the model mesh and therefore reducing computational time we propose to define cells with dimensions larger than the riverbed width moreover to avoid elongated cells that can be responsible for model instabilities we maintain the length of the computational cells along the streamflow direction at maximum twice its width fig 2 since bathymetric data is rarely available we propose to represent riverbed geometry using a simplified trapezoidal shape assumption via the porosity law type 3 fig 1 first storage porosities are computed then the porosity law type used for the edges is selected depending on the location of their adjacent cells law type 3 is used inside the riverbed cross sections i e edges between two cells of type 3 and law type 0 is used in the floodplain between two cells of type 0 and on river banks between a riverbed and a floodplain cells to accurately represent overbank flows the nodes of the river bed cells are positioned on the dikes in both models indeed when positioning interfaces upon constrictions and obstacles these latter are implicitly considered in the interface flux calculation while the same obstacles disappear from the numerical representation when they are located inside the cells moreover to ensure that high points are correctly taken into account without too much overloading the mesh design process we choose to automatically compute the edge porosity values as the minimum of the porosities of their neighbouring cells it is worth mentioning that the parameter retrieval of the porosity law in each cell and edge is carried out automatically using the available dem and bathymetric information 2 2 experimental design to the best of our knowledge our modelling framework enables for the first time to represent both riverbed and floodplain subgrid topography using porosity laws to evaluate its advantages and limitations we compare the sw2d ddp model with a standard fine 2d model namely sw2d inria lemon team 2022 in terms of simulated water depths and inundation extents moreover to further assess our modelling approach we evaluate it against observed flood extents from aerial photographs and satellite images and against observed water level time series from in situ measurements when available 2 2 1 the standard 2d shallow water model to enable a meaningful comparison between the two approaches the standard model has to use exactly the same input data as the porosity model topography bathymetry boundary and initial conditions and parameters e g friction coefficient numerical scheme the two models differ only in the way they represent the floodplain and riverbed topography the standard model being based on a classical finite volume scheme the bottom elevation inside a computational cell has a unique value equal to the average elevation of the cell s nodes as a consequence topography can be smoothed out within each cell when flow obstructions drains or structures e g dikes roads streams are not intrinsically represented via cells smaller than their dimensions indeed adequately representing dikes drains or river channels requires to include several mesh cells within each of these structuring elements therefore in the standard sw2d model the mesh needs to be designed in a way that entire cells are placed explicitly on hydraulic structures or singularities for instance when representing a drain of 5 m width cells have to be well placed to capture its effect otherwise it would be transparent for the model having many of these structures in large scale areas would require a long time to represent them in a standard sw2d model 2 2 2 evaluation method to evaluate the porosity model performance we propose an approach composed of several successive steps detailed in the following paragraphs i post process model results to derive flood extent and water depth maps in the same format ii compare flood extent and water level maps extracted from both models on a daily basis iii evaluate flood maps extracted from both models using remote sensing derived data and iv evaluate simulated water level time series against in situ observation data in this study we chose to evaluate the proposed modelling approach in terms of simulated water levels first using punctual in situ water level measurements next as spatially distributed water level cannot be derived from in situ observation we also compare the sw2d ddp results with those obtained using the standard sw2d model whether we use the sw2d model results or the measurements provided by a camera or a gauging station to evaluate water levels we computed the root mean squared deviations rmsd eq 7 when we evaluate the model in terms of flood extents two types of references are used i the flood extent maps simulated by sw2d and ii the ones derived from a satellite imagery post processing of model results we aim to compare the results of the porosity and the reference models in terms of flood extents and water levels by definition the bottom elevation of the cells in the porosity and standard model meshes are taken into account differently in the standard sw2d model as previously mentioned the bottom elevation of a cell corresponds to the average elevation of its nodes in the porosity model the subgrid elevation variability is accounted for via the porosity since the edges of the fine and coarse grid cells do not overlay the flood extent maps derived from the two models are resampled to the original dem resolution 2 m to enable a pixel to pixel comparison to do so the cell is considered flooded when the simulated water depth reaches a minimum value h min i e when z s z b h min where z b is the cell bottom elevation for the standard model and the dem elevation for the porosity model the h min is set to 0 1 m which corresponds approximately to the vertical accuracy of the lidar dem evaluation of simulated flood extent maps the simulated flood extents evaluation is carried out twice using as a reference either i the standard model or ii the available earth observation data based on a pixel by pixel comparison we compute a confusion matrix composed of four metrics 1 the number of pixels that are unflooded in both maps i e tn true negatives 2 the number of pixels flooded only in the standard model i e fn false negatives 3 the number of pixels flooded only in the porosity sw2d ddp model i e fp false positives and 4 the number of pixels flooded in both maps i e tp true positives to compare the simulated and the reference maps we compute contingency maps as overall performance metrics we use the critical success index csi schaefer 1990 and the overall accuracy oa that are both derived from the confusion matrix csi and oa quantify the goodness of fit between the evaluated map and the reference maps see eqs 5 and 6 the csi represents the ratio of the number of pixels correctly predicted as flooded tp over the number of all flooded pixels 5 csi tp tp fp fn the oa takes into account the agreement of non flooded areas and is defined as follows 6 oa tp tn tp fp fn tn these scores vary between 0 and 1 with the highest value attained when the predictions present a perfect fit with the reference evaluation of simulated water level maps to quantitatively measure discrepancies between the simulations and reference water level maps we use the root mean square deviations rmsd eq 7 between the porosity model predicted water levels z i sim and the reference water levels z i ref eq 7 resampled at the dem resolution 2 m and for each of the n inundated pixels of the entire domain 7 rmsd 1 n i 1 n z i sim z i ref 2 moreover to further evaluate the distribution of the differences between the simulated and reference water levels we make use of boxplots showing the deviation distribution based on statistical metrics 1 the lower bound 2 the first quartile q1 25th percentile 3 the median q2 50th percentile 4 the third quartile q3 75th percentile and 5 the upper bound the interquartile range iqr goes from the 25th to the 75th percentile and therefore represents 50 of the data values the maximum value of the boxplot is defined as q3 1 5 iqr and the minimum value q1 1 5 iqr outlier points are thus eliminated from the plot for the sake of readability evaluation of simulated water levels time series water level time series obtained from each of the porosity and standard models are evaluated against in situ observation data for visual comparison these time series are plotted then to quantitatively measure the discrepancies we compute the root mean square deviations rmsds as described in the previous section 3 study area experimental data and model setup the severn the longest river in great britain extends from its source at plynlimon in the welsh hills to the mouth of the bristol channel the overall catchment area covers approximately 11 000 km2 and is predominantly rural apart from some urban settlements like worcester tewkesbury and evesham fig 3 shows the model domain and river network with the location of the available gauging stations and the camera location offering live imagery on the river severn the study site is located at the confluence of rivers severn and avon around the city of tewkesbury and has been subject to frequent flooding due to intense precipitation the area of interest covers approximately 15 10 km2 two flood events of different magnitude will be simulated and analysed to better understand the model behaviour with changes in boundary conditions the july 2007 and november 2012 flood events hydrometric data two suitable gauging stations are located at saxons lode along the severn river and evesham along the avon river upstream of the confluence due to the backwater effect observed at bredon the streamflow time series is estimated there from that recorded at evesham gauging station located upstream of bredon and delayed in time based on an estimated wave travel time mythe bridge is situated upstream the confluence of the severn avon rivers and deerhurst is situated downstream hydrometric data are provided by the uk environmental agency ea at 15 min intervals moreover the tewkesbury stationary camera fig 3 mounted on the wall of a building in march 2011 provided a view on the avon river which allowed taking hourly daylight images during the 2012 flood event this camera enabled the estimation of water levels in the river vetra carvalho et al 2020 which are used to evaluate the hydraulic model performance inside the domain earth observation data the flood event of july 2007 is particularly interesting because an airborne campaign imaged the flood at a very high resolution 50 cm on july 24 close to the flood peak giustarini et al 2012 flood extents were manually digitised on this imagery this extracted flood map allows evaluating the simulated flood extents at the same date 24 july the hierarchical split based approach proposed by chini et al 2017 is used to derive flood extent maps from the cosmos skymed images acquired on the following dates 27 28 29 30 november and 01 02 04 december 2012 these flood maps are considered for evaluating synchronous flood extent maps simulated by the porosity and the standard models topographic and bathymetric data a lidar dem at 2m spatial resolution with a vertical accuracy of 0 10 m provided by the uk environmental agency ea wood et al 2016 is used to provide the model with ground elevation bathymetric data is reconstructed using three river cross section measurements at the upstream saxons lode and bredon and downstream deerhurst boundaries of the model to do so first we approximate the observed cross sections using a trapezoidal shape the bank lines are manually digitised along the avon and severn river streams and river stream bottom lines are automatically generated as parallels to the bank lines using a distance estimated based on the observed cross sections next the bank elevations are estimated by extracting ground elevation provided by the lidar dem along the bank lines then the river bottom elevation is linearly interpolated between the three trapezoidal cross sections along the avon and severn bottom lines based on the river banks and bottom lines with associated elevation values we interpolate river bathymetry finally the interpolated bathymetric data is merged with topographic data to form a single model input 3 1 model setup while the standard model mesh is composed of 29 772 cells the porosity model mesh contains only 1042 cells concerning the mesh design in the sw2d ddp model just like in any other hydraulic model the cell including the river should not be too large as the porosity law used in river cells considers the flood plain as horizontal rectangular above trapezoidal shape for other cells no brutal variations in terms of surface should be found between adjacent cells the influence of the number of tabulations n inside a cell has been investigated in guinot et al 2018 since the spatial information is lost within a coarse grid cell it is essential to ensure that obstacles are captured by the 5 tabulation levels discharge time series are imposed as upstream boundary conditions of the hydraulic model severn at saxons lode and avon at bredon the streamflow time series in saxons lode are derived from water surface elevation records using a rating curve water level time series are used as downstream boundary condition at deerhurst fig 4 the initial condition is a fixed water level equal to the downstream condition a uniform strickler coefficient k s 50 m 1 3 s 1 is used for the riverbed and the floodplain spatially distributed parameters could easily be prescribed but a sensitivity analysis not shown in this paper showed that the influence of the friction coefficient was limited for the studied flood event the durations of 2007 and 2012 flood event simulations are 17 days 18 july 04 august and 15 days 21 november 06 december respectively 4 results 4 1 evaluation of simulated flood extent maps fig 5 shows the csi and oa time series computed on a daily frequency for evaluating the sw2d ddp simulated flood maps using the sw2d simulated flood maps as reference it can be seen that both simulated flood extent maps are most of the time in agreement for both flood events at the flood peak in figs 6b and 7b there is a very good agreement between the two models accuracy of 95 the model agreement is slightly lower in the rising limb and decreases more in the falling limb this implies that the draining dynamic in the sw2d ddp model is different from that in the sw2d model figs 6 and 7 show a series of contingency maps obtained by comparing the simulated flood extent maps by the sw2d ddp and sw2d models during the 2007 and 2012 flood events during the rising limbs figs 6a and 7a the porosity model exhibits a good agreement with the standard model while locally inundating slightly larger areas especially in the upstream part as well as in little drains in the urban zone at the severn avon confluence see box in fig 6a this indicates the porosity model induces overbanking earlier than the standard model oppositely a smaller inundation extent is visible locally nearby the avon river figs 6c and 7c show a substantially larger flood extent simulated by sw2d this indicates that almost all floodplain water came back to the stream in the sw2d ddp simulation while a substantial volume of water remains present in the floodplain in the sw2d simulation this effect is dominant in the eastern severn floodplain and around the urban settlements overall figs 6c and 7c suggest that the porosity model fills in and drains floodplain water faster than the standard model to better understand and assess this aspect we also compare both model results to remote sensing derived data for the 2007 event the porosity and standard model derived flood maps were evaluated against the flood map extracted from aerial photography and showed similar levels of agreement csi 0 92 oa 0 95 and csi 0 9 oa 0 94 respectively for sw2d ddp and sw2d during the 2012 flood simulation both models are in good agreement with lower scores for the last satellite image 04 december see table 1 csi and oa are rather similar for the two models but it is worth highlighting that the metrics of the porosity model are always exceeding those of the standard model fig 8 shows the contingency maps computed by each of the models using as a reference the satellite flood map acquired on december 04 the most important differences between the two simulated flood extent maps are exhibited close to tewkesbury where sw2d ddp drains water faster than sw2d the flood extent map derived from sw2d therefore exhibit in fig 8a a substantial overestimation when compared to the flood extent map derived from a cosmo skymed image however this overestimation has to be interpreted carefully as sar backscatter images do not enable floodwater detection in dense urban areas chini et al 2019 moreover giustarini et al 2012 showed for the same study area that part of the floodwater was detectable during the 2007 flood event inside tewkesbury using a high resolution sar backscatter image i e a terrassar x image as a consequence one can argue that the absence of floodwater within tewkesbury in the cosmo skymed images acquired in 2012 lends more weight to the sw2d ddp flood extent map reliability 4 2 evaluation of simulated water level maps fig 9 shows time series of root mean square deviations rmsd calculated between the porosity and standard model derived water levels at a daily frequency across the inundated areas the corresponding time averaged rmsds are equal to 12 32 cm and 6 3 cm for the 2007 and the 2012 flood events respectively the highest deviations are observed in the falling and rising limbs during the flood peaks are reduced and vary between 3 and 9 cm from a practical point of view depth deviations ranging from 10 to 15 cm in flood predictions can arguably be considered as acceptable given the vertical accuracy of the lidar used 10 cm sanders 2007 mason et al 2003 furthermore boxplots are used to assess the distribution of differences between water levels simulated by the porosity and the standard models at a daily time step fig 10 at first sight it is found that the model results present a very good agreement at the flood peak since the boxplot height is very small positive values in the boxplots refer to higher water levels simulated by the porosity model this is mainly observed during the rising limb and at the flood peak this indicates that the porosity model simulates the overbanking earlier this result is in agreement with the larger water extent simulated by the porosity model see figs 6a and 7a during the falling limb lower values of water levels simulated by the porosity model express a larger inundation extent computed by the standard model as obtained in figs 6c and 7c 4 3 evaluation of simulated water levels time series simulated porosity and standard model water level time series are first evaluated using in situ observations at the mythe bridge hydrometric station and second using water levels estimated from the tewkesbury camera images when inter comparing the two models the results present a very good agreement figs 11a 11b and 11c the highest discrepancies of simulated water levels compared to the gauge observations 0 90 and 0 60 m are reached just before the 2007 flood peak fig 11a and on the first day of the 2012 flood simulation fig 11b respectively the evaluation further shows reduced model errors during the falling limb of the 2012 flood event where the porosity model exhibit an error of less than 5 cm approximately this is probably related to the initial condition fixed in the simulation that is set as uniform and fits the downstream level deerhurst on another note rmsds are slightly improved albeit not significantly in the porosity model fig 9 both model results are also assessed using the camera images at tewkesbury see location in fig 3 the highest discrepancies with the gauge data are found in the rising limb they are reduced when approaching the flood peak and almost fit the model results at the falling limb table 2 shows the porosity and standard model scores using the rmsd metrics computed on water level time series the considered reference is the data observed at mythe bridge for the 2007 and 2012 events and at tewkesbury for the 2012 event 5 discussion as described in section 4 2 the water depth deviations of the porosity based model with respect to the standard sw2d model are acceptable given the vertical accuracy of the lidar used c a 10 cm the average flow depth in the rivers estimated over the entirety of the flood event is about 7 m generally speaking the average errors c a 6 to 12 cm are not substantial high errors reaching a maximum of 25 cm are observed in the rising and the falling limb where the porosity model seems to fill in and evacuate faster than the standard model on the other hand errors with respect to the gauge camera data reach a maximum of 60 to 90 cm respectively since the real bathymetry and bed shape of the river are unknown this potentially affects the simulation results in general and can be further improved in fig 11 the simulated levels at mythe bridge are lower than the observed ones especially during the rising limb and the flood peak this is arguably due to the simplified representation of the bathymetry in the models and to an underestimated upstream inflow for the severn river at saxon s lode under high flow conditions indeed as the river burst its banks around saxon s lode the floodplain starts conveying a part of the flow that is not accounted for in the corresponding model boundary condition derived from the riverstream gauging station in terms of flood extents results show the porosity model fills in and drains floodplain water faster than the standard model to better assess this behaviour we compared both model results to a series of remote sensing derived flood maps it was shown that during the falling limb the observed inundation extent is closer to the one simulated with the porosity model especially in the areas around tewkesbury this faster flooding and receding dynamic in the porosity model is mainly related to its ability to represent small scale topography and drains via porosity as mentioned in section 2 2 1 representing small drains in the standard model requires cells with dimensions smaller than that of the drains this means that drains should in theory be finely discretised by very small cells fig 12 in the sw2d model these drains are visible in the lidar topographic data fig 3 but they are not captured by our standard model mesh because the mesh cells are comparatively large for example the size of a cell capturing a drain would have dimensions smaller than 5 m which would increase the number of computational cells along the drainage network the standard mesh designed in this study consists of 29 772 cells representing approximately 28 times more cells than that in the porosity mesh 1042 cells moreover the simulation run time in that case would escalate drastically with the decrease of the simulation time step becoming inconvenient for large scale applications table 3 summarises the cpu times necessary for the standard and the porosity model simulations carried out on a computer with an i7 4770 cpu processor and a memory of 16 gb ram for an area of 1500 km 2 and in both test cases the cpu time required for the model simulation is 13 min vs 3 2 days for the standard model for a 17 days flood simulation and 12 min vs 2 9 days for a 15 flood day event the porosity model therefore offers the advantage of a fast model setup while preserving high resolution data by using coarse grid cells thus enabling reduced computational efforts this paves the way for real time applications and long terms simulations over large areas all singularities and types of cross section can be taken into account in the porosity laws as long as they are visible in the dtm however the spatial localisation of the singularity inside a coarse grid cell is lost this is why it is preferable to place the interfaces on the singularities so as not to create artificial links between the cells since cross sections are rarely available along the entire river and only punctual measurements are provided a riverbed shape approximation must be made which is facilitated by the use of porosity laws the interpolation of river bathymetry between observed cross sections certainly has an influence on the model results but it is the only available information in this study we compared both models using the same bathymetric data the linear interpolation of these profiles along the river appeared to be reasonable since no brutal variations of the slopes were observed while examining longitudinal profiles however further improvements are expected when having more precise bathymetry data in this study the topography information is derived from a high resolution lidar dtm originally at 2 m resolution and resampled at 10 m with increasing availability of global dems e g srtm 30 m at a global scale the modelling approach can be applied in poorly gauged areas for the specific case of urban areas the building location map could be used to improve the dem based on widely available databases such as openstreetmap the vegetation remains more complex to be accounted for and for the moment we may consider vegetation effects through an increased friction coefficient as usually done in hydrodynamic modelling in the sw2d software classical or ddp all parameters such as infiltration rates and friction can be spatially distributed e g based on land use maps as discussed in guinot et al 2018 one main limitation of the porosity modelling approach is the definition of a unique water level per computational cell which is equivalent to considering a horizontal free surface elevation in each cell although the consequences of such an assumption are limited when dealing with large scale and slow floods they may not be negligible and should be assessed the porosity based approach also leads to a loss of spatial information inside coarse grids this can potentially create artificial links between cells unless the edges are carefully placed upon local highest points however one should keep in mind that this is also true for other hydrodynamic models such as classical 2d ones moreover as seen before it is possible to recover spatial information by resampling the results on the dtm as proposed in this paper therefore preserving the original dtm data at its original resolution 6 conclusion in this paper we proposed an innovative modelling framework based on porosity to rapidly simulate flood inundations this framework enables for the first time to represent both bathymetry and small scale floodplain topography using depth dependent porosity within comparatively large computational cells simulating two real test case floods over a 1500 km 2 area around the severn and avon confluence has shown the following 1 the proposed modelling approach enable to simulate flood extent maps very similar to the one simulated by the standard sw2d model with 90 agreement 2 the evaluation based on in situ measurements indicates that the porosity model is exhibiting levels of performance comparable to and even higher than those of a standard model 3 it is found that the porosity model is able to account for small drains within comparatively very large cells representing these small drains in a standard model would require very small cells therefore leading to a much higher number of cells and a large computational demand 4 our experiment shows that the sw2d ddp model simulations are c a 350 times faster than that of the standard model thereby substantially reducing computational costs in perspective the proposed modelling framework facilitates the retrieval of an effective bathymetry as this is represented via the porosity parameters this opens up new perspectives for large scale applications over areas where bathymetric data are not available declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by the national research fund of luxembourg through the cascade project grant no c17 sr 11682050 the hydrometric data is provided by the uk environment agency all authors approved the version of the manuscript to be published 
