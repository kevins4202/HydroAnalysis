index,text
26120,the distributed hydrology soil vegetation model dhsvm code was parallelized for distributed memory computers using the global arrays ga programming model to analyze parallel performance dhsvm was used to simulate the hydrology in two river basins of significant size located in the northwest continental united states and southwest canada at 90 m resolution the 1 clearwater 25 000 km2 and 2 columbia 668 000 km2 river basins meteorological forcing applied to both basins was dynamically down scaled from a regional reanalysis using the weather research and forecasting wrf model and read into dhsvm as 2d maps for each time step parallel code speedup was significant run times for 1 year simulations were reduced by an order of magnitude for both test basins a maximum parallel speedup of 105 was attained with 480 processors while simulating the columbia river basin speedup was limited by input dominated tasks particularly the input of meteorological forcing data keywords watershed hydrology distributed hydrology model high performance computing parallel computing columbia river basin clearwater river basin software availability program title parallel dhsvm description distributed hydrology vegetation soil model dhsvm platform linux mac os x source language c c cost free license public domain availability source code available on github https github com pnnl dhsvm pnnl parallel branch 1 introduction the distributed hydrology vegetation soil model dhsvm wigmosta et al 1994 is a spatially distributed physics based hydrology model that simulates the overland and subsurface hydrological processes influenced by climate topography soil and vegetation dhsvm is composed of a two layer canopy model an energy balance two layer snow model a multi layer soil model and three dimensional surface and subsurface flow routing models these models allow for characterization of hydrological processes including canopy and topographic shading canopy interception evapotranspiration snow accumulation and melt and water movement overland and through the soil to streams and rivers in an extensive review of 30 hydrological models beckers et al 2009 dhsvm was identified to be best suited for modeling mountain hydrology in forested environments initially developed in the early 1990s wigmosta et al 1994 dhsvm has been applied extensively particularly in forested mountainous snowfall dominated regions to characterize the hydrologic regime and project potential changes with changing climate and landscape storck et al 1998 storck and lettenmaier 1999 leung and wigmosta 1999 thyer et al 2004 cuo et al 2009 cristea et al 2014 livneh et al 2015 cao et al 2016 sun et al 2018 subsequent adaptations have extended the capability of dhsvm to represent urban landscapes with impervious surfaces and runoff detention cuo et al 2008 glacio hydrological dynamics naz et al 2014 frans et al 2015 2018 river thermal dynamics sun et al 2015 cao et al 2016 urban water quality sun et al 2016 and forest snow interactions in canopy gaps sun et al 2018 with the increasing availability of high resolution satellite products e g light detecting and ranging and advances in high performance computing systems and data storage there is evolving interest in exploring hydrologic fluxes and state variables at progressively higher spatial resolutions for applications ranging from regional to global scales lettenmaier et al 2015 high resolution spatially distributed modeling capabilities are particularly important for representing complex mountain hydrology that is highly affected by heterogeneous terrain and strong climate gradients with elevation a spatially lumped modeling approach with sparsely distributed observation networks can limit our ability to understand and predict the implications of changing climate and landscape on available water for extreme runoff events regional water supplies and associated reservoir operations for hydropower and other water allocations bales et al 2006 while dhsvm has been under constant development since its inception it has always been a serial code its computational performance has been tied to the performance of a single processor parallelization is a good strategy for helping meet these and future simulations needs a number of examples in the literature describe parallel hydrological models the majority e g hwang et al 2014 liu et al 2014 2016 adriance et al 2019 seem to favor small shared memory platforms using openmp dagum and menon 1998 a few vivoni et al 2011 kumar and duffy 2016 target distributed memory systems using the message passing interface mpi 29 in this work dhsvm was made into a parallel code while maintaining most of its existing capability the parallel code development was aimed at large distributed memory clusters but portability to smaller multiprocessor shared memory systems such as desktops and laptops was maintained an alternate interprocess communication programming model global arrays ga nieplocha et al 2006 manojkumar et al 2012 was used ga provides a partitioned global address space pgas and implements one sided communication protocols to demonstrate its utility and scalability the parallel dhsvm was used to simulate runoff from two basins of significant size this work is limited to demonstrating the performance of parallel dhsvm 2 methods 2 1 hydrologic process representation the dhsvm domain is divided into an array of rectangular cells fig 1 cell size is determined by the resolution of the digital elevation model dem used a mask is used to denote which cells in the domain are active typically encompassing a watershed that drains to a single point within each cell a water mass and energy balance is maintained excess surface water is routed down slope overland excess drainage to the subsoil layer is routed downgradient to neighboring cells until reaching the channel network as much as possible dhsvm uses physically based representations to compute the movement of water and energy through the domain the details of dhsvm hydrologic process representation are presented elsewhere e g wigmosta et al 1994 wigmosta and lettenmaier 1999 wigmosta et al 2002 cuo et al 2009 naz et al 2014 frans et al 2018 sun et al 2018 brief descriptions of some processes important to code parallelization are presented here 2 1 1 cell energy water balance a dhsvm cell consists of a set of soil layers a set of snowpack layers when present and a multi level vegetation canopy meteorological forcing data are used to drive the energy balances in the snowpack resulting in melt and or accumulation and in the vegetation canopy resulting in evapotranspiration movement of water in the cell s soil layers is simulated this includes infiltration or exfiltration evaporation from the soil surface evapotranspiration from soil layers in which vegetation has roots vertical saturated and unsaturated water movement between layers and drainage to a subsurface soil layer all of these calculations take place within the cell independent of its neighbors the results are volumes of water in each soil and snow layer and on the surface 2 1 2 surface and subsurface routing the surface and subsurface volumes computed in the cell energy water balance are routed to neighboring cells the dhsvm routing schemes are documented by wigmosta et al 1994 wigmosta and lettenmaier 1999 and wigmosta et al 2002 both surface and subsurface routing work with a similar algorithm a gradient based on the ground surface or water table surface is used to determine the direction and magnitude of flow for each cell in a cell discharge to each neighboring cell is computed and stored surface water flux from active cell i j to its k th down slope neighbor is computed as 1 q o i j k w i j k v i j k y i j where w i j k is the flow width in the k direction v i j k is the overland flow velocity and y i j is the overland flow depth subsurface flow from active cell i j to its k th downgradient neighbor is computed as 2 q s i j k w i j k β i j k t i j z d where β i j k is the cell water table or land slope and t i j z d is the soil transmissivity computed as t i j z d k i j f i j e f i j z i j e f i j d i j where k i j is the cell lateral saturated hydraulic conductivity z i j is the depth to the water table f i j is a decay coefficient and d i j is the cell soil thickness these fluxes are computed for every cell in the domain each cell accumulates the inflow from its upgradient neighbors discharges to adjacent down gradient cells and adjusts surface and subsurface volumes accordingly 2 1 3 stream channel network dhsvm uses a stream channel network to route excess surface water and intercepted subsurface flow to the watershed outlet the stream channel network is represented by a cascade of linear reservoirs wigmosta et al 2002 after surface and subsurface routing is complete computed stream channel interception of surface and subsurface flow is accumulated for each cell in which a stream channel lies the intercepted water volume is summed and used as lateral inflow for each stream segment the lateral inflow is then routed through the network the outflow rate of segment i at time t 1 is given by 3 o i t 1 i i t 1 l i t 1 s i t 1 s i t 1 δ t where i i t is the inflow rate at time t to segment i from upstream segment s l i t is the lateral inflow at time t into segment i δ t is the time step between t and t 1 and s i t is the segment storage at time t computed using 4 s i t 1 1 k i i t 1 l i t 1 x s i t 1 k i i t 1 l i t 1 in which k s o r 2 3 n l and x e k δ t where s o is the channel slope n is manning s coefficient l is the channel length r is the hydraulic radius which is assumed to be a constant 75 of the bank height and δ t is the time step 2 2 code parallelization the multiple instruction multiple data mimd wilkinson and allen 1998 parallel model was used this approach targets large distributed memory systems i e clusters but the approach should work fine for smaller shared memory systems multi processor desktops and laptops without modification in the mimd model each processor is assigned its own data to work on independently and some communication layer is required to exchange data between processors when needed in this case each dhsvm process is assigned a non overlapping rectangular subset of the active cells in domain the goal was to make dhsvm as fast as practical while retaining as much of its existing behavior as possible dhsvm is a relatively large and complicated code resources were not available to design and code a parallel dhsvm from the ground up this in some ways limited the parallelization approach and results 2 2 1 interprocess communication inter process communication in dhsvm was implemented through the use of ga nieplocha et al 2006 manojkumar et al 2012 ga is a partitioned global address space library for distributed arrays ga provides a distributed random access multi dimensional array data structure such an array is consistent with the internal dhsvm data structures so most of the serial code structure could be retained in addition nearly all of the required interprocess communication consists of floating point values which simplifies coding in general dhsvm interprocess communication is all cell based numeric values i e rectangular arrays in a typical communication scenario a ga structure is created transfers of values are made from local memory to the ga put and from the ga to local memory get other operations are available like accumulate where values in local memory are summed into the ga ga can use several underlying communication protocols depending on the underlying hardware the most commonly used are based on mpi and can be used on almost any platform that supports mpi these range from large clusters to laptops any shared or distributed memory system for which mpi is available dinan et al 2012 dhsvm relies entirely on the ga application programming interface api there are no direct calls to any other parallel communication interface 2 2 2 domain decomposition the most straightforward approach to parallelization was to distribute cell based calculations across processors a divide and conquer strategy was implemented that has some similarity to the strategy used by hwang et al 2014 each process was assigned a non overlapping rectangular region of the original domain as shown in fig 2 the region assigned to a process may be a collection of rows stripey or a collection of columns stripex an algorithm similar to simeone s 1986 is used to evenly distribute the active cells among the processors when splitting the domain by rows for example the number of active cells in each row are summed and summed again into a cumulative histogram if the rows are to be divided into p groups the cumulative histogram is searched for the splits closest to 1 p 2 p p 1 p a similar search of the columns active cell cumulative histogram is done to split the columns the decomposition described is used only for the cell based calculations the channel network is not divided among processes each process is assigned complete representation of the domain s entire channel network 2 2 3 input output strategy a distributed hydrology model like dhsvm requires considerable input data and can produce simulation results of considerable size the choice of how the data are input and output can significantly affect parallel performance the input output i o strategy used here was relatively simple and largely emphasized maintaining existing behavior such that the serial code structure was mostly maintained when i o bottlenecks are identified in future applications a more complex strategy may be deployed all processes read the configuration file so that at startup all processes have a complete description of the simulation without further communication other text files like the stream network description are also read by all processes these files are typically small in size and the time to read them is usually inconsequential dhsvm requires several input data sets that vary cell by cell these data sets are input in the form of a two dimensional 2d raster map in the parallel dhsvm 2d map data are input through the root process serially then distributed via a global array at the time of this writing parallel i o was not used but may be supported in the future for this work dhsvm required considerable reworking of 2d data i o to be able to work efficiently over a wide range of computational resources two 2d map resolutions are necessary the first is at the resolution of the dhsvm cell size and contain a single value for each cell data sets input at this resolution include the dem soil type and vegetation type maps of this resolution are partitioned and distributed to processes according to the domain decomposition the second map resolution is much coarser and not necessarily aligned with dhsvm cell boundaries this was used for input of meteorological data fields for data sets at this coarser resolution the entire 2d map is mirrored on all processes i e all processes receive an identical copy of the map mirroring the entire map in this way avoids a more complicated decomposition that would require overlapping sub domains fig 3 shows a schematic of i o for 2d maps that are partitioned as they are distributed a global array is created with a size to store values for the entire domain a single process opens and reads serially a 2d map for the entire domain into local memory this process then puts 1 1 here put and get are operations defined by the ga api another operation accumulate is mentioned below those data in the global array all processes including the process reading the map data then get that portion of the global array it has been assigned for this work dhsvm required considerable reworking of 2d data i o to be able to work efficiently over a range of computational resources at the time of writing parallel i o was not used but may be supported in the future fig 4 shows a schematic of the input of maps that are mirrored across all processes the input process is nearly the same as in fig 3 except that each compute process gets the entire map and loads it into local memory in this work meteorological data were supplied as a series of 2d maps of each required fields as discussed in more detail below at the beginning of each time step the maps for that time step are read as described above all output both 2d map data and text files is through the root process writing 2d map data is the reverse of reading fig 3 each process puts its local values in the global array the root process gets the entire set of values and writes them to a file output other than 2d maps e g mass balance summary requires a more traditional mpi like all reduce operation using the ga api though 2 2 4 hydrologic processes adaptation fig 5 shows a simplified depiction of the parallel algorithm for a single simulation time step each simulation time step starts with time step initialization tsi several things are initialized at the beginning of a time step each process prepares the cells it owns for the next time step the most important part of tsi is the assignment of meteorological data to individual cells dhsvm has several available approaches to make this assignment but each of eight meteorological data fields were read as mirrored 2d maps section 2 2 3 this is a significant amount of data that needs to be read every time step once cells are initialized energy water balance ewb calculations proceed each process updates the hydrologic and thermal state of the snowpack vegetation canopy and soil layers section 2 1 1 within the active cells assigned to it the computations for a single cell do not require any communication with its neighbors so this part of the simulation is most amenable to parallelization unlike the ewb subsurface and surface routing ssr and sr calculations section 2 1 2 require interaction with neighboring cells and that interaction needed to extend between processors when neighboring cells were not owned by the same processor surface and subsurface routing have very similar algorithms so the parallelization of those processes is handled in a similar manner fig 6 depicts the inter process communication used for ssr and sr the key issue with these processes is that a cell assigned to one processor may drain to a cell on another processor this is handled by extending the calculated local domain by one cell a temporary array is created on a local processor to hold the results of sr or ssr routing eqs 1 and 2 respectively as shown in fig 6a the array is sized to be one cell larger in all valid directions than the domain assigned to the processor shaded gray in fig 6a that extra cell captures ssr or sr flux to the off processor cell s as routing calculations proceed surface water is routed to a cell outside the processor s domain and the result is stored on the edge of the array after all processes complete local sr and ssr calculations a global array for the entire domain is initialized to zero fig 6b each process accumulates the local array of routing results into the global array in this way water routed outside of the processors local domain is correctly captured and delivered to the neighboring domain a get operation fig 6c returns complete sr or ssr routing results from the global array to each processor s local memory part of the sr and ssr algorithms is to compute the lateral inflow l in eqs 3 and 4 into each channel segment however each processor only computes lateral inflow contribution from the cells assigned it and it is necessary to add contributions from multiple processes consequently lateral inflow for each channel segment is totaled from the contributions computed by all processes an all reduce summation typically an expensive operation is used to sum lateral inflow over all processors after the all reduce all processes have an identical array of lateral inflow to all segments the simulation time step ends with channel routing cr section 2 1 3 eqs 3 and 4 require that a segment can only be routed after all upstream segments have been routed consequently it was decided to keep cr a serial algorithm and that all processes would carry out identical computations all processes then perform cr on the same network with the same inflow producing identical results only the root process outputs cr results an alternate approach would be to have the root process alone do the cr calculations then distribute the cr results back to the other processors with a broadcast the chosen approach avoids this second possibly expensive communication other calculations are performed during a time step that are not depicted in fig 5 the most important is a mass balance check this requires each process to accumulate the mass balance components of its cells then an all reduce operation is used to sum the components over the domain 2 3 comparison to serial dhsvm throughout the development of parallel dhsvm it was periodically checked against the serial code which was considered correct several smaller basins were used to ensure that the serial and parallel codes produced identical results one such case was rainy creek rainy creek is a small tributary 44 km2 to the wenatchee river in western washington usa the rainy creek data set used in this comparison was that distributed as the dhsvm 3 0 tutorial 2 2 https dhsvm pnnl gov tutorials stm limited changes were made to configuration file s to conform to the current dhsvm code but spatial and meteorologic data remained unchanged from the downloaded tutorial the basin was simulated for 18 months on an 8 core workstation running linux simulation output from serial dhsvm code master branch commit 59e3230 was compared to that from the parallel code parallel branch commit e24b11c 2 4 case studies two basins of different size were chosen to measure parallel dhsvm performance minimal simulation results are presented here because the focus of this work is parallel performance we emphasize however that these are real detailed applications that use real data and are undergoing calibration and validation at the time of writing the calibration validation and use of these cases will be documented in detail in future publications the columbia river basin is located in the northwest continental united states and southern british columbia canada fig 7 and it drains an area of 668 000 km2 table 1 dhsvm was configured to simulate the columbia river basin at a resolution of 90 m resulting in about 83 million active cells and 20 800 stream segments the columbia river basin dem fig 7 was derived from u s geological survey dem usgs 2017 and canada dem cdem nrc 2015 soil types were taken from soil survey geographic database ssurgo nrcs 2019a digital general soil map of the united states statsgo2 nrcs 2019b and national soil database cansis 2017 vegetation land cover was derived from the national land cover data set usgs 2014 and earth observation for sustainable development of forests eosd wood et al 2002 the columbia river basin stream network fig 8 was generated using the python based dhsvm pre processing module which calculates and extracts accumulated flow lines based on flow direction as derived from the dem the columbia river basin was simulated in two ways in addition to complete simulation mode dhsvm s snow only mode was also used in snow only mode dhsvm does not perform runoff related computations but instead concentrates on snowpack accumulation and melt this is useful in snow dominated applications because it allows calibration and validation of the snowpack simulation at a significantly lower computational cost the clearwater river is an upland tributary in the columbia river basin located in northern idaho usa fig 7 the basin area is 25 000 km2 about 4 of the columbia river basin table 1 and it produces about 7 5 of the columbia river basin s average annual discharge the clearwater river basin dhsvm application was extracted as a subset of the columbia river basin so it has the same computational resolution 90 m and uses the same source data the clearwater application consisted of about 3 million active cells and 2600 stream segments 2 4 1 meteorological forcing in the case studies described above meteorological forcings were generated using the weather research and forecasting wrf model skamarock et al 2008 the advanced research wrf version 3 8 was used in this study and the model was configured using physics options identical to those used by gao et al 2017 the historical climate simulation covered the period of 1981 2015 and wrf was driven by the large scale circulations of the north american regional reanalysis mesinger et al 2006 at 32 km grid resolution the details of this simulation are provided by chen et al 2018 meteorological forcings for dhsvm precipitation humidity air temperature wind speed shortwave and long wave radiation were archived hourly and transformed into the geographic projection used by dhsvm within dhsvm all data fields were assumed to be spatially constant over each 6 km cell with the exception of temperature and precipitation despite relatively high resolution wrf data the 6 km grids were still insufficient to capture some of the spatial variance of precipitation inside a wrf grid given the dhsvm resolution of 90 m therefore wrf precipitation was down scaled by introducing a monthly adjustment ratio at each dhsvm grid cell based on the parameter elevation regressions on independent slopes model prism pcg 2004 precipitation data temperature was lapsed over each 6 km cell based on the elevation at each dhsvm grid cell and the elevation of the wrf cell wrf to dhsvm down scaling details will be documented in future publications wrf model results were prepared for a region large enough to encompass the entire columbia river basin 220 191 6 6 km cells using one file for each of the 6 variables containing maps for every hour of the simulation 1 year 8760 h from those results a subset was also extracted that covered the clearwater river basin 32 37 6 6 km cells 2 5 parallel performance the two basins were simulated repeatedly for the 1982 water year with varying numbers of processors and the execution time was recorded the simulations were carried out on the pacific northwest national laboratory institutional computing pic constance cluster this cluster consists of 528 compute nodes each having 24 cores intel haswell e5 2670 and 64 gb of ram the nodes use fdr infiniband connectivity dhsvm and ga were built with intel version 15 compilers and intel mpi implementation ga was built to use infiniband connectivity directly rather than mpi at the time of this writing this was considered to be the fastest transport layer for ga on systems using infiniband interconnect 3 3 bruce palmer lead ga developer personal communication the main disk storage system pic cluster is a lustre file system consisting of 42 lustre object storage server oss nodes with one object storage target per oss and a capacity of 3 5 petabytes simulation times were used to compute parallel speedup defined as 5 s t s t n where t s is the execution time for a given problem on a single processor and t n is the execution time for the same problem on n processors also computed was parallel efficiency 6 e t s n t n to provide key diagnostics dhsvm was instrumented to report execution times of important tasks some of these tasks are described in detail in section 2 2 4 startup su the su task is all activity prior to simulating the first time step the computational cost consists of memory allocation and initialization of the domain state during this task considerable input including the dem and related data and initial model state is read and distributed among processors as partitioned 2d maps section 2 2 3 domain decomposition section 2 2 2 is carried out during this task time step initialization tsi the tsi task performs all necessary preparation for a new time step the most costly of which in this case was the reading and distribution of meteorological data as mirrored 2d maps section 2 2 3 energy water balance ewb ewb is the main computational task and was easily parallelized little or no interprocess communication or i o happens during this task subsurface routing ssr the ssr task should be a mostly computational where cell subsurface fluxes eq 2 are computed for the cells a processor owns the communication cost is the accumulation of a flux array over all processors surface routing sr as with ssr sr computational cost is flux computation eq 1 and communication cost is flux accumulation channel routing cr in the cr task all processes perform identical routing calculations on the entire network which means that the routing has a fixed computational cost regardless of the number of processors used the main communication cost of this task is an all reduce operation required to sum lateral inflow to the channel network across all processors output out this task includes all significant output except stream flow minimal output was specified for these simulations output of 2d maps was not specified consequently the major cost of this task was the computation and output of an overall mass balance for the purposes of discussion these tasks are grouped into primarily computational tasks ewb ssr sr cr that are dominated by numerical computations communication tasks su tsi out are those dominated by i o and or interprocess communication 3 results 3 1 comparison to serial dhsvm fig 9 visually compares rainy creek snow water equivalent depth and basin discharge simulated the parallel dhsvm using 4 processors to the current serial version of dhsvm the parallel dhsvm simulation output was identical to that produced by the serial code to 5 significant figures dhsvm generates a summary mass balance at the end of a simulation mass balance components are summed each time step over all active cells at the end of the simulation a mass balance error is computed table 2 compares this mass balance summary for serial and parallel simulations of rainy creek here too the mass balance components were consistent to 5 significant figures dhsvm computations are all single precision so the differences between the simulations in table 2 are easily explained by rounding and truncation errors 3 2 hydrologic simulation at the time of this writing calibration and validation were under way for the columbia river basin of which the clearwater river basin is a significant part that will be documented in subsequent publications the original serial dhsvm was not able to simulate either of these cases mainly because of the problem size but also because of some alterations that were not back ported to the serial code for this work which focuses on computational performance sufficient hydrological results are presented here to demonstrate that parallel dhsvm simulations show acceptable agreement with observations the columbia river and nearly all of its larger tributaries have some kind of regulation and or irrigation withdrawals because dhsvm at this time cannot represent this regulation dhsvm is being calibrated and validated against estimates of no regulation no irrigation nrni streamflow for various locations in the columbia river basin bpa 2011 fig 10 compares preliminary simulated columbia river daily average discharge with nrni at the dalles for water years 1981 through 1985 the dalles is significant gage location on the columbia river approximately 300 km from the mouth often used as representative of the basin fig 11 shows similar comparisons for discharges from some significant tributaries the clearwater and salmon rivers are tributaries to the snake river the willamette river a large tributary that enters the columbia river below the dalles the pend oreille river enters the columbia near the u s canada border all time series shown were extracted from the same simulation of the entire columbia basin these time series show that the columbia basin simulation has produced reasonable results at several locations and much better matches can be expected with calibration fig 12 shows simulated snow water equivalent swe depths over the entire columbia river basin on april 1 1982 3 3 parallel performance with the clearwater simulation a maximum speedup of about 32 was attained using 128 processors fig 16 about 23 000 active cells per processor one year s simulation time was reduced from almost 4 hours using a single processor to 8 minutes using 128 processors table 3 with one processor run time was dominated by computational tasks ewb 70 sr 7 and ssr 13 table 3 fig 13 computational tasks took a similar fraction of run time until about 32 processors were used at this point i o tasks specifically su and tsi started to take a larger fraction this is the point at which parallel efficiency dropped quickly fig 17 at maximum speedup 128 processors run time was split with about 40 for computational and 60 for i o tasks the columbia river basin simulations required a minimum of four nodes to run due to the large memory requirements of the application the smallest number of cores we could use was four run time for one processor was assumed to be four times that of four processors so that between one and four processors speedup assumed to be ideal and parallel efficiency was assumed to be 1 0 with the columbia snow only simulation the maximum speedup was about 93 using 480 processors at about 173 000 active cells per processor fig 16 the 1 year simulation time was reduced from about 10 days using one processor estimated to 2 5 hours using 480 processors with four processors the run time was dominated by ewb 80 and that task dominates until 120 processors are used 56 when the main i o tasks su and tsi begin to dominate at maximum speedup ewb takes about 29 and su and tsi take about 63 of the simulation time table 3 fig 14 maximum speedup for the full columbia simulation which included the water routing tasks was about 105 also using 480 processors fig 16 the 1 year simulation time was reduced from about 19 days with one processor estimated to about 4 hours table 3 with 480 processors with four processors run time was dominated by computational tasks 90 with ewb dominating that 67 at maximum speedup 480 processors the run time was split with about 60 for computational and 40 for i o tasks this is the reverse of the split for the clearwater at maximum speedup note that cr took a much larger part of the simulation time for the columbia 21 than for the clearwater 5 4 discussion in this work we modified dhsvm to run in parallel using ga for interprocess communication targeting large distributed memory systems simulation run times for our test cases were reduced enough to make long term decades high spatial resolution simulations of significantly sized basins manageable as expected the run times with low numbers of processes were dominated by the computational tasks namely ewb sr and ssr io intensive tasks namely su and tsi become dominant at higher core counts indicating more interprocess communication it was not straightforward to compare our results with those in the literature different models use different methods that have different computational and communication costs our speedup was on par with that measured by vivoni et al 2011 and not nearly as good as that of kumar and duffy 2016 the only other examples we could find using a parallel distributed hydrology model running on 100 s of processors a previous effort with dhsvm adriance et al 2019 attained speed ups of 440 using compiler optimization and openmp the kumar and duffy application appears to be designed and implemented as a parallel application as opposed to our retrofit of dhsvm our speedup was also comparable to some others e g liu et al 2014 2016 that used 10 20 processors and had significantly smaller sized problems our approach is distinct from other approaches in two ways the first is exclusive reliance on the ga one sided communication api other parallel distributed hydrology models tend to use mpi on distributed memory platforms e g vivoni et al 2011 kumar and duffy 2016 or openmp on shared memory systems e g li et al 2010 2011 hwang et al 2014 liu et al 2014 2016 dhsvm has an advantage here in that it has a rectangular domain which fits the data structure model of ga nicely the use of ga expands the range of computational platforms that can be brought to bear on these hydrologic problems the second distinction is the straightforward domain decomposition technique dhsvm domain decomposition simply divides rows or columns of the rectangular domain unlike other approaches that divide the domain by drainage network apostolopoulos and georgakakos 1997 grübsch and david 2001 li et al 2011 liu et al 2016 the triangular cell networks used by vivoni et al 2011 and kumar and duffy 2016 required more complicated algorithms to decompose the domain our approach is simple and requires very little computational effort but could perhaps be improved while not explicitly investigated here load balancing likely plays a significant role in dhsvm parallel performance and should be examined as part of further code improvements dhsvm parallel efficiency falls off as more processors are used fig 17 decreasing efficiency is due to the parts of the simulation that are serial and require the same amount of time regardless of the number of processes involved parallel efficiency indicates the relative benefit of adding more processors to the calculation as more processors are added the fixed time tasks take a larger proportion of the simulation time which reduces the relative benefit of the additional processors in this case the most prominent culprits as shown in fig 15 are tsi input and cr in order to get better efficiency those parts of simulation need to either more efficient i e take less time or be parallelized in some way some improvements to these are being considered as discussed below the parallel performance also indicates that running dhsvm at the point of maximum speedup may not be ideal run time needs to be balanced with the availability and cost of computational resources for example the columbia simulation had a maximum speedup with 480 processors with a simulation time of about 4 h if the same simulation is run with 120 processors it would take 8 h while the run time would be doubled the computational cost would only be one quarter additionally a set of 480 processors is most likely less available than 120 which may lead to longer job queue times it may also be more efficient to simulate a case like the columbia river basin in several large subbasins particularly for calibration and validation once calibrated the parameters could be used in a production simulation of the entire columbia basin in this initial parallel version of dhsvm we emphasized maintenance of current capabilities and avoiding large structural changes to the code parallel performance may have been limited by the emphasis on limiting code changes this is particularly true with the input of 2d maps 5 future work the authors are generally pleased with the performance improvements attained with this work dhsvm s parallel performance was good enough to tackle the task at hand namely the entire columbia river basin however a larger application at this ultra fine 90 m resolution still may be challenging at this time the timings clearly indicate that the way meteorological forcing was read and applied was the largest single obstacle to higher parallel performance several ideas may be investigated to reduce this load but they would probably require significant structural changes to the current dhsvm code reading the meteorological data in larger blocks a day or month at a time say rather than one time step at a time may reduce input time reading 2d map data in parallel instead of through the root process section 2 2 3 may also be a solution stream routing took a significant part of the total run time for the columbia simulation the choice to keep this a serial process executed by all processes may be acceptable for smaller basins and was acceptable for the cases here but may become a barrier with larger applications parallel methods to perform channel routing will be investigated in future work we have used a straightforward and relatively simple domain decomposition scheme here a more extensive investigation of domain decomposition would likely yield further performance improvements we have assumed that the simulation of each active cell has an equivalent computational cost this is not strictly true cells with snow definitely have a higher computational cost than cells without snow such an investigation would require some detailed analysis of run times and how snow increases the computational cost of an active cell acknowledgments the study was primarily funded by the u s department of energy doe office of energy efficiency and renewable energy water power technologies office the material is also based upon work funded by the strategic environmental research and development program under contract rc 2546 and the doe office of science biological and environmental research as part of the regional and global climate modeling and multi sector energy and environmental dynamics programs the wrf simulations described here were performed using the facilities of the pacific northwest national laboratory pnnl institutional computing center pic and the national energy research supercomputing center which is supported by the doe office of science under contract de ac02 05ch11231 dhsvm simulations described here were also performed using pnnl pic facilities pnnl is operated by battelle for the doe under contract de ac06 76rlo 1830 dhsvm is maintained jointly by the hydrology group at pnnl and the civil engineering department at the university of washington more information can be found at http dhsvmdev pnl gov dhsvm source code can be obtained from its github repository https github com pnnl dhsvm pnnl the code used for this work is in the parallel branch 
26120,the distributed hydrology soil vegetation model dhsvm code was parallelized for distributed memory computers using the global arrays ga programming model to analyze parallel performance dhsvm was used to simulate the hydrology in two river basins of significant size located in the northwest continental united states and southwest canada at 90 m resolution the 1 clearwater 25 000 km2 and 2 columbia 668 000 km2 river basins meteorological forcing applied to both basins was dynamically down scaled from a regional reanalysis using the weather research and forecasting wrf model and read into dhsvm as 2d maps for each time step parallel code speedup was significant run times for 1 year simulations were reduced by an order of magnitude for both test basins a maximum parallel speedup of 105 was attained with 480 processors while simulating the columbia river basin speedup was limited by input dominated tasks particularly the input of meteorological forcing data keywords watershed hydrology distributed hydrology model high performance computing parallel computing columbia river basin clearwater river basin software availability program title parallel dhsvm description distributed hydrology vegetation soil model dhsvm platform linux mac os x source language c c cost free license public domain availability source code available on github https github com pnnl dhsvm pnnl parallel branch 1 introduction the distributed hydrology vegetation soil model dhsvm wigmosta et al 1994 is a spatially distributed physics based hydrology model that simulates the overland and subsurface hydrological processes influenced by climate topography soil and vegetation dhsvm is composed of a two layer canopy model an energy balance two layer snow model a multi layer soil model and three dimensional surface and subsurface flow routing models these models allow for characterization of hydrological processes including canopy and topographic shading canopy interception evapotranspiration snow accumulation and melt and water movement overland and through the soil to streams and rivers in an extensive review of 30 hydrological models beckers et al 2009 dhsvm was identified to be best suited for modeling mountain hydrology in forested environments initially developed in the early 1990s wigmosta et al 1994 dhsvm has been applied extensively particularly in forested mountainous snowfall dominated regions to characterize the hydrologic regime and project potential changes with changing climate and landscape storck et al 1998 storck and lettenmaier 1999 leung and wigmosta 1999 thyer et al 2004 cuo et al 2009 cristea et al 2014 livneh et al 2015 cao et al 2016 sun et al 2018 subsequent adaptations have extended the capability of dhsvm to represent urban landscapes with impervious surfaces and runoff detention cuo et al 2008 glacio hydrological dynamics naz et al 2014 frans et al 2015 2018 river thermal dynamics sun et al 2015 cao et al 2016 urban water quality sun et al 2016 and forest snow interactions in canopy gaps sun et al 2018 with the increasing availability of high resolution satellite products e g light detecting and ranging and advances in high performance computing systems and data storage there is evolving interest in exploring hydrologic fluxes and state variables at progressively higher spatial resolutions for applications ranging from regional to global scales lettenmaier et al 2015 high resolution spatially distributed modeling capabilities are particularly important for representing complex mountain hydrology that is highly affected by heterogeneous terrain and strong climate gradients with elevation a spatially lumped modeling approach with sparsely distributed observation networks can limit our ability to understand and predict the implications of changing climate and landscape on available water for extreme runoff events regional water supplies and associated reservoir operations for hydropower and other water allocations bales et al 2006 while dhsvm has been under constant development since its inception it has always been a serial code its computational performance has been tied to the performance of a single processor parallelization is a good strategy for helping meet these and future simulations needs a number of examples in the literature describe parallel hydrological models the majority e g hwang et al 2014 liu et al 2014 2016 adriance et al 2019 seem to favor small shared memory platforms using openmp dagum and menon 1998 a few vivoni et al 2011 kumar and duffy 2016 target distributed memory systems using the message passing interface mpi 29 in this work dhsvm was made into a parallel code while maintaining most of its existing capability the parallel code development was aimed at large distributed memory clusters but portability to smaller multiprocessor shared memory systems such as desktops and laptops was maintained an alternate interprocess communication programming model global arrays ga nieplocha et al 2006 manojkumar et al 2012 was used ga provides a partitioned global address space pgas and implements one sided communication protocols to demonstrate its utility and scalability the parallel dhsvm was used to simulate runoff from two basins of significant size this work is limited to demonstrating the performance of parallel dhsvm 2 methods 2 1 hydrologic process representation the dhsvm domain is divided into an array of rectangular cells fig 1 cell size is determined by the resolution of the digital elevation model dem used a mask is used to denote which cells in the domain are active typically encompassing a watershed that drains to a single point within each cell a water mass and energy balance is maintained excess surface water is routed down slope overland excess drainage to the subsoil layer is routed downgradient to neighboring cells until reaching the channel network as much as possible dhsvm uses physically based representations to compute the movement of water and energy through the domain the details of dhsvm hydrologic process representation are presented elsewhere e g wigmosta et al 1994 wigmosta and lettenmaier 1999 wigmosta et al 2002 cuo et al 2009 naz et al 2014 frans et al 2018 sun et al 2018 brief descriptions of some processes important to code parallelization are presented here 2 1 1 cell energy water balance a dhsvm cell consists of a set of soil layers a set of snowpack layers when present and a multi level vegetation canopy meteorological forcing data are used to drive the energy balances in the snowpack resulting in melt and or accumulation and in the vegetation canopy resulting in evapotranspiration movement of water in the cell s soil layers is simulated this includes infiltration or exfiltration evaporation from the soil surface evapotranspiration from soil layers in which vegetation has roots vertical saturated and unsaturated water movement between layers and drainage to a subsurface soil layer all of these calculations take place within the cell independent of its neighbors the results are volumes of water in each soil and snow layer and on the surface 2 1 2 surface and subsurface routing the surface and subsurface volumes computed in the cell energy water balance are routed to neighboring cells the dhsvm routing schemes are documented by wigmosta et al 1994 wigmosta and lettenmaier 1999 and wigmosta et al 2002 both surface and subsurface routing work with a similar algorithm a gradient based on the ground surface or water table surface is used to determine the direction and magnitude of flow for each cell in a cell discharge to each neighboring cell is computed and stored surface water flux from active cell i j to its k th down slope neighbor is computed as 1 q o i j k w i j k v i j k y i j where w i j k is the flow width in the k direction v i j k is the overland flow velocity and y i j is the overland flow depth subsurface flow from active cell i j to its k th downgradient neighbor is computed as 2 q s i j k w i j k β i j k t i j z d where β i j k is the cell water table or land slope and t i j z d is the soil transmissivity computed as t i j z d k i j f i j e f i j z i j e f i j d i j where k i j is the cell lateral saturated hydraulic conductivity z i j is the depth to the water table f i j is a decay coefficient and d i j is the cell soil thickness these fluxes are computed for every cell in the domain each cell accumulates the inflow from its upgradient neighbors discharges to adjacent down gradient cells and adjusts surface and subsurface volumes accordingly 2 1 3 stream channel network dhsvm uses a stream channel network to route excess surface water and intercepted subsurface flow to the watershed outlet the stream channel network is represented by a cascade of linear reservoirs wigmosta et al 2002 after surface and subsurface routing is complete computed stream channel interception of surface and subsurface flow is accumulated for each cell in which a stream channel lies the intercepted water volume is summed and used as lateral inflow for each stream segment the lateral inflow is then routed through the network the outflow rate of segment i at time t 1 is given by 3 o i t 1 i i t 1 l i t 1 s i t 1 s i t 1 δ t where i i t is the inflow rate at time t to segment i from upstream segment s l i t is the lateral inflow at time t into segment i δ t is the time step between t and t 1 and s i t is the segment storage at time t computed using 4 s i t 1 1 k i i t 1 l i t 1 x s i t 1 k i i t 1 l i t 1 in which k s o r 2 3 n l and x e k δ t where s o is the channel slope n is manning s coefficient l is the channel length r is the hydraulic radius which is assumed to be a constant 75 of the bank height and δ t is the time step 2 2 code parallelization the multiple instruction multiple data mimd wilkinson and allen 1998 parallel model was used this approach targets large distributed memory systems i e clusters but the approach should work fine for smaller shared memory systems multi processor desktops and laptops without modification in the mimd model each processor is assigned its own data to work on independently and some communication layer is required to exchange data between processors when needed in this case each dhsvm process is assigned a non overlapping rectangular subset of the active cells in domain the goal was to make dhsvm as fast as practical while retaining as much of its existing behavior as possible dhsvm is a relatively large and complicated code resources were not available to design and code a parallel dhsvm from the ground up this in some ways limited the parallelization approach and results 2 2 1 interprocess communication inter process communication in dhsvm was implemented through the use of ga nieplocha et al 2006 manojkumar et al 2012 ga is a partitioned global address space library for distributed arrays ga provides a distributed random access multi dimensional array data structure such an array is consistent with the internal dhsvm data structures so most of the serial code structure could be retained in addition nearly all of the required interprocess communication consists of floating point values which simplifies coding in general dhsvm interprocess communication is all cell based numeric values i e rectangular arrays in a typical communication scenario a ga structure is created transfers of values are made from local memory to the ga put and from the ga to local memory get other operations are available like accumulate where values in local memory are summed into the ga ga can use several underlying communication protocols depending on the underlying hardware the most commonly used are based on mpi and can be used on almost any platform that supports mpi these range from large clusters to laptops any shared or distributed memory system for which mpi is available dinan et al 2012 dhsvm relies entirely on the ga application programming interface api there are no direct calls to any other parallel communication interface 2 2 2 domain decomposition the most straightforward approach to parallelization was to distribute cell based calculations across processors a divide and conquer strategy was implemented that has some similarity to the strategy used by hwang et al 2014 each process was assigned a non overlapping rectangular region of the original domain as shown in fig 2 the region assigned to a process may be a collection of rows stripey or a collection of columns stripex an algorithm similar to simeone s 1986 is used to evenly distribute the active cells among the processors when splitting the domain by rows for example the number of active cells in each row are summed and summed again into a cumulative histogram if the rows are to be divided into p groups the cumulative histogram is searched for the splits closest to 1 p 2 p p 1 p a similar search of the columns active cell cumulative histogram is done to split the columns the decomposition described is used only for the cell based calculations the channel network is not divided among processes each process is assigned complete representation of the domain s entire channel network 2 2 3 input output strategy a distributed hydrology model like dhsvm requires considerable input data and can produce simulation results of considerable size the choice of how the data are input and output can significantly affect parallel performance the input output i o strategy used here was relatively simple and largely emphasized maintaining existing behavior such that the serial code structure was mostly maintained when i o bottlenecks are identified in future applications a more complex strategy may be deployed all processes read the configuration file so that at startup all processes have a complete description of the simulation without further communication other text files like the stream network description are also read by all processes these files are typically small in size and the time to read them is usually inconsequential dhsvm requires several input data sets that vary cell by cell these data sets are input in the form of a two dimensional 2d raster map in the parallel dhsvm 2d map data are input through the root process serially then distributed via a global array at the time of this writing parallel i o was not used but may be supported in the future for this work dhsvm required considerable reworking of 2d data i o to be able to work efficiently over a wide range of computational resources two 2d map resolutions are necessary the first is at the resolution of the dhsvm cell size and contain a single value for each cell data sets input at this resolution include the dem soil type and vegetation type maps of this resolution are partitioned and distributed to processes according to the domain decomposition the second map resolution is much coarser and not necessarily aligned with dhsvm cell boundaries this was used for input of meteorological data fields for data sets at this coarser resolution the entire 2d map is mirrored on all processes i e all processes receive an identical copy of the map mirroring the entire map in this way avoids a more complicated decomposition that would require overlapping sub domains fig 3 shows a schematic of i o for 2d maps that are partitioned as they are distributed a global array is created with a size to store values for the entire domain a single process opens and reads serially a 2d map for the entire domain into local memory this process then puts 1 1 here put and get are operations defined by the ga api another operation accumulate is mentioned below those data in the global array all processes including the process reading the map data then get that portion of the global array it has been assigned for this work dhsvm required considerable reworking of 2d data i o to be able to work efficiently over a range of computational resources at the time of writing parallel i o was not used but may be supported in the future fig 4 shows a schematic of the input of maps that are mirrored across all processes the input process is nearly the same as in fig 3 except that each compute process gets the entire map and loads it into local memory in this work meteorological data were supplied as a series of 2d maps of each required fields as discussed in more detail below at the beginning of each time step the maps for that time step are read as described above all output both 2d map data and text files is through the root process writing 2d map data is the reverse of reading fig 3 each process puts its local values in the global array the root process gets the entire set of values and writes them to a file output other than 2d maps e g mass balance summary requires a more traditional mpi like all reduce operation using the ga api though 2 2 4 hydrologic processes adaptation fig 5 shows a simplified depiction of the parallel algorithm for a single simulation time step each simulation time step starts with time step initialization tsi several things are initialized at the beginning of a time step each process prepares the cells it owns for the next time step the most important part of tsi is the assignment of meteorological data to individual cells dhsvm has several available approaches to make this assignment but each of eight meteorological data fields were read as mirrored 2d maps section 2 2 3 this is a significant amount of data that needs to be read every time step once cells are initialized energy water balance ewb calculations proceed each process updates the hydrologic and thermal state of the snowpack vegetation canopy and soil layers section 2 1 1 within the active cells assigned to it the computations for a single cell do not require any communication with its neighbors so this part of the simulation is most amenable to parallelization unlike the ewb subsurface and surface routing ssr and sr calculations section 2 1 2 require interaction with neighboring cells and that interaction needed to extend between processors when neighboring cells were not owned by the same processor surface and subsurface routing have very similar algorithms so the parallelization of those processes is handled in a similar manner fig 6 depicts the inter process communication used for ssr and sr the key issue with these processes is that a cell assigned to one processor may drain to a cell on another processor this is handled by extending the calculated local domain by one cell a temporary array is created on a local processor to hold the results of sr or ssr routing eqs 1 and 2 respectively as shown in fig 6a the array is sized to be one cell larger in all valid directions than the domain assigned to the processor shaded gray in fig 6a that extra cell captures ssr or sr flux to the off processor cell s as routing calculations proceed surface water is routed to a cell outside the processor s domain and the result is stored on the edge of the array after all processes complete local sr and ssr calculations a global array for the entire domain is initialized to zero fig 6b each process accumulates the local array of routing results into the global array in this way water routed outside of the processors local domain is correctly captured and delivered to the neighboring domain a get operation fig 6c returns complete sr or ssr routing results from the global array to each processor s local memory part of the sr and ssr algorithms is to compute the lateral inflow l in eqs 3 and 4 into each channel segment however each processor only computes lateral inflow contribution from the cells assigned it and it is necessary to add contributions from multiple processes consequently lateral inflow for each channel segment is totaled from the contributions computed by all processes an all reduce summation typically an expensive operation is used to sum lateral inflow over all processors after the all reduce all processes have an identical array of lateral inflow to all segments the simulation time step ends with channel routing cr section 2 1 3 eqs 3 and 4 require that a segment can only be routed after all upstream segments have been routed consequently it was decided to keep cr a serial algorithm and that all processes would carry out identical computations all processes then perform cr on the same network with the same inflow producing identical results only the root process outputs cr results an alternate approach would be to have the root process alone do the cr calculations then distribute the cr results back to the other processors with a broadcast the chosen approach avoids this second possibly expensive communication other calculations are performed during a time step that are not depicted in fig 5 the most important is a mass balance check this requires each process to accumulate the mass balance components of its cells then an all reduce operation is used to sum the components over the domain 2 3 comparison to serial dhsvm throughout the development of parallel dhsvm it was periodically checked against the serial code which was considered correct several smaller basins were used to ensure that the serial and parallel codes produced identical results one such case was rainy creek rainy creek is a small tributary 44 km2 to the wenatchee river in western washington usa the rainy creek data set used in this comparison was that distributed as the dhsvm 3 0 tutorial 2 2 https dhsvm pnnl gov tutorials stm limited changes were made to configuration file s to conform to the current dhsvm code but spatial and meteorologic data remained unchanged from the downloaded tutorial the basin was simulated for 18 months on an 8 core workstation running linux simulation output from serial dhsvm code master branch commit 59e3230 was compared to that from the parallel code parallel branch commit e24b11c 2 4 case studies two basins of different size were chosen to measure parallel dhsvm performance minimal simulation results are presented here because the focus of this work is parallel performance we emphasize however that these are real detailed applications that use real data and are undergoing calibration and validation at the time of writing the calibration validation and use of these cases will be documented in detail in future publications the columbia river basin is located in the northwest continental united states and southern british columbia canada fig 7 and it drains an area of 668 000 km2 table 1 dhsvm was configured to simulate the columbia river basin at a resolution of 90 m resulting in about 83 million active cells and 20 800 stream segments the columbia river basin dem fig 7 was derived from u s geological survey dem usgs 2017 and canada dem cdem nrc 2015 soil types were taken from soil survey geographic database ssurgo nrcs 2019a digital general soil map of the united states statsgo2 nrcs 2019b and national soil database cansis 2017 vegetation land cover was derived from the national land cover data set usgs 2014 and earth observation for sustainable development of forests eosd wood et al 2002 the columbia river basin stream network fig 8 was generated using the python based dhsvm pre processing module which calculates and extracts accumulated flow lines based on flow direction as derived from the dem the columbia river basin was simulated in two ways in addition to complete simulation mode dhsvm s snow only mode was also used in snow only mode dhsvm does not perform runoff related computations but instead concentrates on snowpack accumulation and melt this is useful in snow dominated applications because it allows calibration and validation of the snowpack simulation at a significantly lower computational cost the clearwater river is an upland tributary in the columbia river basin located in northern idaho usa fig 7 the basin area is 25 000 km2 about 4 of the columbia river basin table 1 and it produces about 7 5 of the columbia river basin s average annual discharge the clearwater river basin dhsvm application was extracted as a subset of the columbia river basin so it has the same computational resolution 90 m and uses the same source data the clearwater application consisted of about 3 million active cells and 2600 stream segments 2 4 1 meteorological forcing in the case studies described above meteorological forcings were generated using the weather research and forecasting wrf model skamarock et al 2008 the advanced research wrf version 3 8 was used in this study and the model was configured using physics options identical to those used by gao et al 2017 the historical climate simulation covered the period of 1981 2015 and wrf was driven by the large scale circulations of the north american regional reanalysis mesinger et al 2006 at 32 km grid resolution the details of this simulation are provided by chen et al 2018 meteorological forcings for dhsvm precipitation humidity air temperature wind speed shortwave and long wave radiation were archived hourly and transformed into the geographic projection used by dhsvm within dhsvm all data fields were assumed to be spatially constant over each 6 km cell with the exception of temperature and precipitation despite relatively high resolution wrf data the 6 km grids were still insufficient to capture some of the spatial variance of precipitation inside a wrf grid given the dhsvm resolution of 90 m therefore wrf precipitation was down scaled by introducing a monthly adjustment ratio at each dhsvm grid cell based on the parameter elevation regressions on independent slopes model prism pcg 2004 precipitation data temperature was lapsed over each 6 km cell based on the elevation at each dhsvm grid cell and the elevation of the wrf cell wrf to dhsvm down scaling details will be documented in future publications wrf model results were prepared for a region large enough to encompass the entire columbia river basin 220 191 6 6 km cells using one file for each of the 6 variables containing maps for every hour of the simulation 1 year 8760 h from those results a subset was also extracted that covered the clearwater river basin 32 37 6 6 km cells 2 5 parallel performance the two basins were simulated repeatedly for the 1982 water year with varying numbers of processors and the execution time was recorded the simulations were carried out on the pacific northwest national laboratory institutional computing pic constance cluster this cluster consists of 528 compute nodes each having 24 cores intel haswell e5 2670 and 64 gb of ram the nodes use fdr infiniband connectivity dhsvm and ga were built with intel version 15 compilers and intel mpi implementation ga was built to use infiniband connectivity directly rather than mpi at the time of this writing this was considered to be the fastest transport layer for ga on systems using infiniband interconnect 3 3 bruce palmer lead ga developer personal communication the main disk storage system pic cluster is a lustre file system consisting of 42 lustre object storage server oss nodes with one object storage target per oss and a capacity of 3 5 petabytes simulation times were used to compute parallel speedup defined as 5 s t s t n where t s is the execution time for a given problem on a single processor and t n is the execution time for the same problem on n processors also computed was parallel efficiency 6 e t s n t n to provide key diagnostics dhsvm was instrumented to report execution times of important tasks some of these tasks are described in detail in section 2 2 4 startup su the su task is all activity prior to simulating the first time step the computational cost consists of memory allocation and initialization of the domain state during this task considerable input including the dem and related data and initial model state is read and distributed among processors as partitioned 2d maps section 2 2 3 domain decomposition section 2 2 2 is carried out during this task time step initialization tsi the tsi task performs all necessary preparation for a new time step the most costly of which in this case was the reading and distribution of meteorological data as mirrored 2d maps section 2 2 3 energy water balance ewb ewb is the main computational task and was easily parallelized little or no interprocess communication or i o happens during this task subsurface routing ssr the ssr task should be a mostly computational where cell subsurface fluxes eq 2 are computed for the cells a processor owns the communication cost is the accumulation of a flux array over all processors surface routing sr as with ssr sr computational cost is flux computation eq 1 and communication cost is flux accumulation channel routing cr in the cr task all processes perform identical routing calculations on the entire network which means that the routing has a fixed computational cost regardless of the number of processors used the main communication cost of this task is an all reduce operation required to sum lateral inflow to the channel network across all processors output out this task includes all significant output except stream flow minimal output was specified for these simulations output of 2d maps was not specified consequently the major cost of this task was the computation and output of an overall mass balance for the purposes of discussion these tasks are grouped into primarily computational tasks ewb ssr sr cr that are dominated by numerical computations communication tasks su tsi out are those dominated by i o and or interprocess communication 3 results 3 1 comparison to serial dhsvm fig 9 visually compares rainy creek snow water equivalent depth and basin discharge simulated the parallel dhsvm using 4 processors to the current serial version of dhsvm the parallel dhsvm simulation output was identical to that produced by the serial code to 5 significant figures dhsvm generates a summary mass balance at the end of a simulation mass balance components are summed each time step over all active cells at the end of the simulation a mass balance error is computed table 2 compares this mass balance summary for serial and parallel simulations of rainy creek here too the mass balance components were consistent to 5 significant figures dhsvm computations are all single precision so the differences between the simulations in table 2 are easily explained by rounding and truncation errors 3 2 hydrologic simulation at the time of this writing calibration and validation were under way for the columbia river basin of which the clearwater river basin is a significant part that will be documented in subsequent publications the original serial dhsvm was not able to simulate either of these cases mainly because of the problem size but also because of some alterations that were not back ported to the serial code for this work which focuses on computational performance sufficient hydrological results are presented here to demonstrate that parallel dhsvm simulations show acceptable agreement with observations the columbia river and nearly all of its larger tributaries have some kind of regulation and or irrigation withdrawals because dhsvm at this time cannot represent this regulation dhsvm is being calibrated and validated against estimates of no regulation no irrigation nrni streamflow for various locations in the columbia river basin bpa 2011 fig 10 compares preliminary simulated columbia river daily average discharge with nrni at the dalles for water years 1981 through 1985 the dalles is significant gage location on the columbia river approximately 300 km from the mouth often used as representative of the basin fig 11 shows similar comparisons for discharges from some significant tributaries the clearwater and salmon rivers are tributaries to the snake river the willamette river a large tributary that enters the columbia river below the dalles the pend oreille river enters the columbia near the u s canada border all time series shown were extracted from the same simulation of the entire columbia basin these time series show that the columbia basin simulation has produced reasonable results at several locations and much better matches can be expected with calibration fig 12 shows simulated snow water equivalent swe depths over the entire columbia river basin on april 1 1982 3 3 parallel performance with the clearwater simulation a maximum speedup of about 32 was attained using 128 processors fig 16 about 23 000 active cells per processor one year s simulation time was reduced from almost 4 hours using a single processor to 8 minutes using 128 processors table 3 with one processor run time was dominated by computational tasks ewb 70 sr 7 and ssr 13 table 3 fig 13 computational tasks took a similar fraction of run time until about 32 processors were used at this point i o tasks specifically su and tsi started to take a larger fraction this is the point at which parallel efficiency dropped quickly fig 17 at maximum speedup 128 processors run time was split with about 40 for computational and 60 for i o tasks the columbia river basin simulations required a minimum of four nodes to run due to the large memory requirements of the application the smallest number of cores we could use was four run time for one processor was assumed to be four times that of four processors so that between one and four processors speedup assumed to be ideal and parallel efficiency was assumed to be 1 0 with the columbia snow only simulation the maximum speedup was about 93 using 480 processors at about 173 000 active cells per processor fig 16 the 1 year simulation time was reduced from about 10 days using one processor estimated to 2 5 hours using 480 processors with four processors the run time was dominated by ewb 80 and that task dominates until 120 processors are used 56 when the main i o tasks su and tsi begin to dominate at maximum speedup ewb takes about 29 and su and tsi take about 63 of the simulation time table 3 fig 14 maximum speedup for the full columbia simulation which included the water routing tasks was about 105 also using 480 processors fig 16 the 1 year simulation time was reduced from about 19 days with one processor estimated to about 4 hours table 3 with 480 processors with four processors run time was dominated by computational tasks 90 with ewb dominating that 67 at maximum speedup 480 processors the run time was split with about 60 for computational and 40 for i o tasks this is the reverse of the split for the clearwater at maximum speedup note that cr took a much larger part of the simulation time for the columbia 21 than for the clearwater 5 4 discussion in this work we modified dhsvm to run in parallel using ga for interprocess communication targeting large distributed memory systems simulation run times for our test cases were reduced enough to make long term decades high spatial resolution simulations of significantly sized basins manageable as expected the run times with low numbers of processes were dominated by the computational tasks namely ewb sr and ssr io intensive tasks namely su and tsi become dominant at higher core counts indicating more interprocess communication it was not straightforward to compare our results with those in the literature different models use different methods that have different computational and communication costs our speedup was on par with that measured by vivoni et al 2011 and not nearly as good as that of kumar and duffy 2016 the only other examples we could find using a parallel distributed hydrology model running on 100 s of processors a previous effort with dhsvm adriance et al 2019 attained speed ups of 440 using compiler optimization and openmp the kumar and duffy application appears to be designed and implemented as a parallel application as opposed to our retrofit of dhsvm our speedup was also comparable to some others e g liu et al 2014 2016 that used 10 20 processors and had significantly smaller sized problems our approach is distinct from other approaches in two ways the first is exclusive reliance on the ga one sided communication api other parallel distributed hydrology models tend to use mpi on distributed memory platforms e g vivoni et al 2011 kumar and duffy 2016 or openmp on shared memory systems e g li et al 2010 2011 hwang et al 2014 liu et al 2014 2016 dhsvm has an advantage here in that it has a rectangular domain which fits the data structure model of ga nicely the use of ga expands the range of computational platforms that can be brought to bear on these hydrologic problems the second distinction is the straightforward domain decomposition technique dhsvm domain decomposition simply divides rows or columns of the rectangular domain unlike other approaches that divide the domain by drainage network apostolopoulos and georgakakos 1997 grübsch and david 2001 li et al 2011 liu et al 2016 the triangular cell networks used by vivoni et al 2011 and kumar and duffy 2016 required more complicated algorithms to decompose the domain our approach is simple and requires very little computational effort but could perhaps be improved while not explicitly investigated here load balancing likely plays a significant role in dhsvm parallel performance and should be examined as part of further code improvements dhsvm parallel efficiency falls off as more processors are used fig 17 decreasing efficiency is due to the parts of the simulation that are serial and require the same amount of time regardless of the number of processes involved parallel efficiency indicates the relative benefit of adding more processors to the calculation as more processors are added the fixed time tasks take a larger proportion of the simulation time which reduces the relative benefit of the additional processors in this case the most prominent culprits as shown in fig 15 are tsi input and cr in order to get better efficiency those parts of simulation need to either more efficient i e take less time or be parallelized in some way some improvements to these are being considered as discussed below the parallel performance also indicates that running dhsvm at the point of maximum speedup may not be ideal run time needs to be balanced with the availability and cost of computational resources for example the columbia simulation had a maximum speedup with 480 processors with a simulation time of about 4 h if the same simulation is run with 120 processors it would take 8 h while the run time would be doubled the computational cost would only be one quarter additionally a set of 480 processors is most likely less available than 120 which may lead to longer job queue times it may also be more efficient to simulate a case like the columbia river basin in several large subbasins particularly for calibration and validation once calibrated the parameters could be used in a production simulation of the entire columbia basin in this initial parallel version of dhsvm we emphasized maintenance of current capabilities and avoiding large structural changes to the code parallel performance may have been limited by the emphasis on limiting code changes this is particularly true with the input of 2d maps 5 future work the authors are generally pleased with the performance improvements attained with this work dhsvm s parallel performance was good enough to tackle the task at hand namely the entire columbia river basin however a larger application at this ultra fine 90 m resolution still may be challenging at this time the timings clearly indicate that the way meteorological forcing was read and applied was the largest single obstacle to higher parallel performance several ideas may be investigated to reduce this load but they would probably require significant structural changes to the current dhsvm code reading the meteorological data in larger blocks a day or month at a time say rather than one time step at a time may reduce input time reading 2d map data in parallel instead of through the root process section 2 2 3 may also be a solution stream routing took a significant part of the total run time for the columbia simulation the choice to keep this a serial process executed by all processes may be acceptable for smaller basins and was acceptable for the cases here but may become a barrier with larger applications parallel methods to perform channel routing will be investigated in future work we have used a straightforward and relatively simple domain decomposition scheme here a more extensive investigation of domain decomposition would likely yield further performance improvements we have assumed that the simulation of each active cell has an equivalent computational cost this is not strictly true cells with snow definitely have a higher computational cost than cells without snow such an investigation would require some detailed analysis of run times and how snow increases the computational cost of an active cell acknowledgments the study was primarily funded by the u s department of energy doe office of energy efficiency and renewable energy water power technologies office the material is also based upon work funded by the strategic environmental research and development program under contract rc 2546 and the doe office of science biological and environmental research as part of the regional and global climate modeling and multi sector energy and environmental dynamics programs the wrf simulations described here were performed using the facilities of the pacific northwest national laboratory pnnl institutional computing center pic and the national energy research supercomputing center which is supported by the doe office of science under contract de ac02 05ch11231 dhsvm simulations described here were also performed using pnnl pic facilities pnnl is operated by battelle for the doe under contract de ac06 76rlo 1830 dhsvm is maintained jointly by the hydrology group at pnnl and the civil engineering department at the university of washington more information can be found at http dhsvmdev pnl gov dhsvm source code can be obtained from its github repository https github com pnnl dhsvm pnnl the code used for this work is in the parallel branch 
26121,the management of multi use landscapes is challenging but essential when aiming at preserving the potential for ecosystem service provision land use decisions lay at the center of this challenge while land use decision models may help to transparently grasp land use decisions the parameterization of such models is difficult as human decision making is often not rational we show here how we used a serious game to parameterize a bayesian network based land use decision model to elicit validation game outputs are transformed to conditional probabilities and compared to conditional probabilities parameterized via a questionnaire and workshop exercises the analysis of four types of validity shows encouraging results for criterion respondent related and practice related validation however content validation sensitivity analysis was disappointing initially we discuss how the success in validation quality may be related to the design of the game and conclude that the transfer from a game to bayesian networks could improve the parameterization quality keywords bayesian network serious game participation validation parameterization 1 introduction globally preserving forests and their potential for ecosystem provision is seen as a central challenge for sustainability lambin and meyfroidt 2011 in madagascar the search for strategies to sustainably protect forests including the perspectives of local actors and their livelihood needs led to a shift from strictly conserved forest areas to multi use sites gardner et al 2013 in multi use sites land use decisions are central to land use and landscape changes influencing the state of a socio ecological system mcginnis and ostrom 2014 hence land use decisions are key when defining such strategies and frame the trade offs among different land use types such as upland rice paddy rice and agroforestry systems however land use decisions are made following rationales and heuristics which are difficult to grasp cf pahl wostl 2007 among others individual preferences and mental models are introducing complexity into land use change luc processes as they cannot be fully determined in addition interactions among decision makers are influencing their decisions and social networks have an impact on individuals decisions isaac and matous 2017 manson et al 2016 as personal preferences may alter the development of a parcel and subsequently a farm and a landscape these individual decisions are of particular importance whenever a landscape management problem is approached blanco et al 2017 primdahl and kristensen 2011 in such landscape management processes we need methods supporting local actors self evaluation a possibility for making land use decisions more tangible and for considering them in landscape management is the development of land use decision models however their ability for prediction is by no means a revelation of truth but rather an approach to make possible decision making rationales explicit modeling helps clarify land use decisions and their inherent trade offs land use decisions and related trade offs may be captured in a serious game we use the term serious game to reflect that besides the joyful part of a game a game may be used for learning instruction or deliberations of challenges wouters et al 2013 according to wouters and colleagues characteristics of a game are interaction agreed upon rules goal orientation often in connection with a challenge and the provision of feedback these characteristics are applicable for analog games too additionally for role playing games rpgs participants adopt specific roles in the iterative development process the game was inspired by other serious games in the broader field of land system science cleland et al 2012 used a game board with cards representing the characteristics of a location or livelihood options to explore interactions in subsistence fishing communities salvini et al 2016 aimed at showing the learning effect of a game and induced the effects of sustainable management in the game demonstrating its effect villamor and van noordwijk 2011 designed a game including villagers and external agents providing financial incentives and let them interact in the context of forest conservation and management similar to many other game approaches they introduced exogenous changes i e population increase forest fire rubber price decrease in the rounds of the game the potential added value of combining a game with a computer based model was recognized by the companion modeling community étienne 2014 and their extensive use of agent based models abms the rpgs may be designed after the abm existed campo et al 2009 castella et al 2005 or before salvini et al 2016 in the latter case the transfer from the game to the abm was based on the choices played in the game thus model mechanisms could be iteratively calibrated and validated with the help of a game the main research needs are related to the translation from a game to a computational model and the challenge grasping often intangible decision making processes in many publications the transfer from a role playing or tabletop game to a computer model has not been specified in more detail barnaud et al 2013 even distinguishes between the computer model producing scientific knowledge and the game aiming at facilitating communication however both a well defined transfer from a game to a computer model and the production of scientific knowledge with a game help to ensure credibility of such processes mallampalli et al 2016 compared 10 methods helping translate narrative scenarios to quantitative models while both the bayesian network bn and rpgs are seen as such a translation method a possible combination of these two approaches was not mentioned they suggested rpgs to be most compatible with an abm here we would like to consider the usefulness of combining rpgs with bns using the ability of games to reveal insights that rarely emerge in interviews pak and brieva 2010 to combine a serious game with modeling is regarded as advantageous as in depth deliberation in the game may be combined with the forecasting capabilities of a modeling exercise barreteau et al 2001 voinov et al 2016 here we focus on the nexus between a game and our modeling tool which is a bn more specifically connecting a game to our bn seems promising as game participants bring their habits and mental models with them to the game and we can elicit participants system knowledge taking into account their ideas and intentions castella et al 2005 lamarque et al 2013 in addition rpgs may help to jointly define the problems to be addressed pak and brieva 2010 we test a serious game which mimics land use change decision making participants play multiple rounds to explore different scenarios and the results are used to parameterize a bn node methodologically we describe a connection that was to the best of our knowledge not made until now this paper focuses on the transfer from a tabletop rpg to a computer model the paper aims at making this procedure transparent and critically reviewing the added value for the specific case of bns hence evidence for reliability and validity of this transfer needs to be presented we would like to answer the following question how valid is the presented game as a method to parameterize a bn 2 methods 2 1 the game in this research and learning game van den hoogen et al 2016 players cultivate land in order to achieve a good living for the household in the game local actors were confronted with land use decision situations players had to allocate labor force units to their land use portfolio therefore we called the game sandry game as sandry in the local malagasy dialect means labor force and strength of a person to create something the allocation of labor force units pawns in the game served as a proxy for the players intentions the game set up was based on field data observations gained through field missions discussions with madagascar experts as well as trial and error to adjust game mechanisms fig 1 shows the game set up left players and the tokens for cloves and vanilla as well as the fake money top right three possible roles were attributed to the players a four farmers were the key actors of the game they made labor force decisions by allocating labor force units to available land use plots the farmers were characterized by available land use plots and household size both of which varied in number however the money available was the same for all farmers these characterizations were based on real life situations but adapted to a game situation by reducing complexity e g biophysical mechanisms number of land use categories and increasing wealth of underprivileged roles to increase playability others classified households in a neighboring region in more detail but similarly laney and turner 2015 zaehringer et al 2017 they used farming area per household the number of commercial plants income from cash crop production production of rice and others to create household groups b the collectors are a type of intermediary actor who buys cloves from villages and sells them to a next level of the value chain in the game they sell cloves directly to the exporter two collectors sell to the same exporter for vanilla markets are organized in central villages and prices are determined by the world market game coordinator the game coordinator is buying vanilla directly from farmers at a standard but varying price for cloves collectors visit each household to buy the harvest c the role of the exporter is scripted and played by a game facilitator in the game the exporter is responsible for the demand of cash crops and has capital stock to buy all the cloves on the market the exporter discusses contracts with collectors exporters receive demand and price signals from the world market the game coordinator secretly provides this information in the first year a normal average season was revealed to participants the characteristics of the subsequent seasons followed a sequence confronting farmers with a theft a cyclone a season with good yields a pest and three average seasons in which only market prices for cash crops would change the labor force allocations were recorded in an excel sheet after each season in other words the number of pawns for every land use parcel players cultivated were recorded each game session ended with a debriefing session to reflect on the victory condition the degree of reality and the strategy played by participants after each year all participants had to complete an annual accounting and report to the game coordinator the amount of crop stock and the amount of money additionally players reported whether they want to send children to school and lastly they were asked to rate with a sign of their thumbs their overall satisfaction with their current game performance the full game lasted between 3 and 3 5 h playing three years with two seasons each the thin red line between being close to reality and at the same time enable playability was constantly reflected within the larger research team and in the debriefing of the game to reflect on this challenge we analyzed the game and its output to demonstrate the prototypical transfer of the game data to a conditional probability table cpt and to compare the parameterization from the game with other data sources i e questionnaire and workshop outputs 2 2 bayesian networks and the land use decision model bns are directed acyclic graphs network nodes represent variables and arcs represent causal relationships all nodes and their relationships are specified with cpts the bayesian rule of conditional probability is applicable to infer posterior probabilities focusing on two nodes in a bn posterior probability is calculated via p x e p x p e x p e where x is the probability of a parent node state and e is the evidence of a child node state to solve an entire network structure chain rule is applied e g for three nodes in a bn equation p a b c p a b c p b c p c shows the chain rule and determines the network structure next marginalization or variable elimination allows us to remove variables from a joint probability distribution jensen and nielsen 2007 p 10 as all possible probabilities of one variable are summed up to determine the marginal contribution of another in bn solving marginalization helps to efficiently determine posterior probability using the conditional independence properties or equivalently the directed global markov property in a bn increases efficiency as factorization of distributions into distributions with lower dimensions is possible kjaerulff and madsen 2008 algorithms based on the bayesian rule are used in software that provides graphical user or application programming interfaces jensen and nielsen 2007 kjaerulff and madsen 2008 norsys 2011 with these types of software entire network structures can be calculated we used norsys netica version 5 18 64 bit the game gave us input to one specific and central node in the bn called land use intention compared to abm approaches our game does not mirror the computer model or vice versa we rather use the game a to inform one node in the bn and b to create a learning environment for participants and researchers the bn reflects luc decisions and the mentioned central node land use intention is the child node that captures influence factors a farmer takes into account optimizing his farm level decisions land use intention in turn is parent to the actual land use category at plot level that is influenced by biophysical characteristics and policy or regulations this structure is strongly motivated by isaac ajzen s theory of planned behavior ajzen 1991 and the bn is linked dynamically to spatial data celio et al 2012 2014 the full bn is shown in appendix b the model is used to predict luc and to show different time steps of land use in a spatially explicit manner the bn was set up in a participatory process involving farmers in our study sites and land use experts in the district capital of maroantsetra the produced maps are intended to support strategic decision making at village level in regard of dynamic cash crop markets and nearby protected areas for parameterization we used two approaches a expectation maximization em learning and b workshops to get knowledge on key cps in the tables we used netica s em algorithm to learn conditional probabilities as it is viewed as more robust than gradient descent norsys 2011 before learning all probability distributions are uniform for learning we used two case files derived from a questionnaire conducted in fv3 that collected data on land use and land use decision making we first used the farm level data n 71 and in a second step the plot level data n 171 for learning 2 3 transfer from the game to a bn to establish a transfer from the game outputs to the bn node intention we used the game as an arena where certain conditions were set and land use decisions were made hence we gained knowledge under conditions framed in the game scenarios fig 2 shows the conditions that needed to be controlled in the game need of money referred to the perception whether gained money is sufficient to cover daily needs traditional land use reflected the self conception of the households dominantly cultivated land use local price level reflected the clove price water availability referred to the sum of precipitation and irrigation possibilities and rice self sufficiency showed how many months per year a household is rice self sufficient due to different player characteristics and game scenarios all except one price level states of the parent nodes could be covered however the number of game sessions was too small to cover all the state combinations in the game participants placed their pawns on different land use plots this number per specific land use was used as an input to calculate our proxy for conditional probabilities formula 1 and 2 we assumed that the revealed intention in the game may be transferred to conditional probabilities labor input per lu type was adjusted by the total number of plots in this category and normalized with the sum of these ratios per land use type normalized labor intensity l a b o u r i n t e n s i t y l i c f o r m u l a 1 n o r m a l i z e d l a b o r i n t e n s i t y n l i c f o r m u l a 2 l i c s a n d r y i n p u t i n c a t e g o r y p l o t s i n c a t e g o r y i c p c n l i c l i c c 1 c l i c we played five game sessions that were judged to be valid three times six seasons and two times four seasons sixteen players labor force allocations were analyzed for seasons one to four and nine for seasons five and six the calculated normalized labor intensity is equal to p x e and reflects one cp in the cpt intention we obtained 82 seasons or probability distributions for one set of conditions due to overlap of identical conditions we averaged entries from identical sets of conditions and deduced 28 final probability distributions 2 4 methods for validation as validation is interpreted from different perspectives bowen 2008 a clear framing for this study is needed in order to adopt a broader view of validation bowen 2008 we present evidence for validity 1 showing that the scores obtained from the game accurately reflect the land use change decision making and 2 showing the usefulness of the scores obtained from the game bowen 2008 land use change decision making is captured by the game and thereon based scores therefore we pose sub questions to the abovementioned guiding question which are related to different perspectives of validation table 1 validity focuses on how accurately and completely a measure captures its target construct bowen 2008 to validate the quantitative inputs from the game we used the above mentioned debriefing of game participants and in addition a workshop exercise we called imagine exercise as well as questionnaire data which are used as parameterizing methods for the overall bn too for each of the research questions we used a specific procedure that was also related to specific perspectives on validation a criterion validity was evaluated by comparing if measured scores the game luc decision making were related to scores from other measures of the same construct luc decision making measured in questionnaire and in workshops b content validation aimed at evaluating if all dimensions of a construct the luc decision making were captured c respondent related validation analyzed if content and format of a measure the game was appropriate to caption the players luc decision making d practice related validation evaluated whether scores the game luc decision making were relevant to luc decision making in reality and relevant to fill a knowledge gap of the target group bowen 2008 in the results section each result is accompanied by paragraphs contextualizing the results as this information helps to understand the elaborated validation results in the following we will explain the methods for each of the four different perspectives of validation in detail 2 4 1 criterion validation comparison of outputs from the game with triangulation methods questionnaire 1 and workshop 2 1 using questionnaire data a farmer questionnaire provided data for parameter learning of the bn for each node included in the network we gathered farmer characteristics the analysis applying chi square test and anova on the questionnaire data revealed that water availability and traditional land use were not significantly related to intention hence we removed these two nodes from the bn for this analysis to be consistent we also adapted the cpt of the game for this analysis and removed these two nodes from the 82 player seasons rounds and players played we aggregated identical condition settings and obtained 14 distinctive cp distributions to compare with the respective cp distributions learned from the questionnaire data to compare the game luc decision making with the questionnaire the answers of the questionnaire were classified according to the node states we could use 71 cases on farm level and 171 cases on plot level for em learning that reflected the empirical combinations of intention and the three remaining parent nodes need of money local price level rice self sufficiency in a next step the obtained conditional probability entries in the cpt for the node intention could be subtracted from the conditional probability entries obtained via the game and we displayed the 14 cp that we compared in a boxplot 2 using workshop outputs in workshops we conducted a specific exercise which we called imagine exercise the aim of the imagine exercise was the elicitation of conditional probabilities by evoking typical situations of farmers in that region members of the stakeholder platform were asked to respond to the following type of questions imagine 10 households in geographically lower levels of the district e g mahalevona and these households are not indebted water availability does not pose any difficulties and the family produces enough rice to be rice sufficient all year long the price of cloves is at the level of 2015 a what is the intention of the majority of the households in this situation the five possible intention categories are more paddy rice more upland rice more pasture more pasture clove and more agroforestry system b how many of these 10 households would have this identical idea intention combining the answers of questions a and b allowed us to determine a conditional probability entry for the node intention we repeated this procedure for an additional four times changing condition each time one to obtain a comparison dataset the imagine exercise was conducted with eleven members of our regional stakeholder platform that is reflecting and consulting on the project s plans and results this group consisted of agriculture experts such as farmer representatives farmer cooperatives farming consultancy ngos as well as district representatives of forest and agriculture this exercise was analyzed by summing up the estimated household counts with the same decisions over all participants and normalizing them over the five possible land use intentions hence we obtained a conditional probability estimate for five conditional probability distributions 2 4 2 content validation 1 sensitivity analysis to check for game driving forces in land use decisions 2 post game interviews focusing on victory conditions 1 sensitivity analysis via the game we specified 28 cpt entries and all conditions were checked for their influence on the cp to do so the mean over all cpt entries with the same varying condition e g price rice self sufficiency etc per land use categories were calculated thus by comparing these means we obtained the influence of one varying condition on the game luc decision making for example looking at the intention more paddy rice the mean of all cps given a local clove price of 5 5 6 5 was compared to the mean of all cps given a local clove price of 6 5 7 5 2 post game interviews after the game sessions group debriefings were conducted the game did not focus on winning however competition was part of the game the triggering question was who won the game and why this question triggered discussion and reflections on the victory condition answers were recorded transcribed and content analyzed in total 18 participants of six game sessions discussed the questions 2 4 3 respondent related validation analysis of consistency in game behavior and strategy statements in post game interviews with this validation step we determined if the game is an appropriate tool to elicit luc decision making to do so players were asked in the debriefing of the game sessions in a person to person interview what their game strategies were these statements were compared to their actual decisions in the game i e their allocation of labor force units to the different land use categories their preferred land use category was approximated by determining the highest probability of a land use intention over a game session we could distinguish 1 whether players were consistent in their statement and in their game decision making or not observation of game play and 2 whether the content of the statement aligned to their actual game behavior or not self declaration of game play fully consistent players precisely pointed out a certain strategy e g optimizing for cash crop production in their statements and this strategy could have been recognized in their game decisions marginally inconsistent players formulated either their strategy ambiguously or their game decisions allowed interpreting a certain range of strategies without specifying that the strategy was diversification inconsistent players were contradictory e g stating that their strategy was focused on cash crop production but showing a clear priority for paddy rice in their game decisions 2 4 4 practice related validation content analysis of post game group debriefing during the group debriefing mentioned in 2 5 2 we also asked questions about the degree of reality of the game the questions triggered discussions and reflections on the degree of reality which is also called external validity in game settings van den hoogen et al 2016 two triggering questions were used in this discussion a how do you judge the degree of reality concerning productivity increase through labor force intensification b how do you judge the degree of reality concerning the implementation of soil fertility in the game in total 18 participants of six game sessions discussed the questions discussions were recorded transcribed and content analyzed we evaluated how the participants judged on the game mechanisms and how they perceived these mechanisms in reality 2 5 study area this study belonged to a six year research project that aims to collaboratively define and test strategies for securing ecosystem services and develop local livelihoods the study areas were chosen enabling comparison between the study areas in laos madagascar and myanmar here we report from the study area located in northeastern madagascar where trade offs between land use categories are pronounced as this area is situated between two major protected areas makira natural park and masoala national park fig 3 this area is judged a hotspot of biodiversity ganzhorn et al 2001 hence many existing publications shed light into the ecological system e g andrianjakarivelo et al 2005 merenlender et al 1998 rakotomanana et al 2001 fewer publications take an anthropological e g keller 2008 or a combined perspective e g fedele et al 2016 golden and comaroff 2015 zaehringer et al 2017 to test the game bn translation we studied a socio ecological system ses in which actors in our case farmers are confronted with a context of increasing complexity cf seto and reenberg 2014 complexity rises as global market interactions intensify through market integration adger et al 2009 liu et al 2013 local access to land resources becomes relatively scarce e g by delimiting protected areas ward et al 2018 and the mutually reinforcing mechanisms of increased financial means and locally diversified demand for globally distributed consumer goods in our study area farmers are confronted with trade offs between feeding a household through subsistence rice cultivation and earning money through cash crop cultivation to buy daily needs products and invest in the farm this is a common challenge for all farmers in northeastern madagascar pfund et al 2011 laney and turner 2015 propose an endogenous and an exogenous view why farmers keep subsistence production the endogenous perspective emphasizes traditions or norms while the exogenous perspective emphasizes market imperfections or constraints induced by rising prices clove and vanilla production gained a dynamic and the trade off between subsistence and cash crop farming was emphasized in recent years in the region of analanjirofo clove cultivation has historically been and is still an important part of the agricultural sector danthu et al 2014 the therein included district of maroantsetra is a rural area with about 14 of the population living in the district capital of maroantsetra and about 84 of the population in the district being farmers rasolofomanana 2009 we concentrate on two areas each composed of two villages in the district of maroantsetra fig 3 the northern site corresponds to the villages mahalevona mah and fizono fiz municipality of mahalevona and the southern site to the villages morafeno mor and beanana bea municipality of morafeno from both case study sites inhabitants travel about one day to the district capital called maroantsetra main land uses in the areas are the cultivation of irrigated paddy rice rainfed upland rice agroforestry systems including cash and subsistence crops and pastures for zebus table 2 analyzing the landscape mosaic in 1995 and 2011 farmers in mah developed from cultivating mostly a mixed upland rice landscape in 1995 to a mixed paddy rice landscape in 2011 farmers in fiz developed with the same pattern however farmers in mor cultivated constantly a mixed paddy landscape and bea cultivated constantly a mixed upland rice landscape in this period based on landscape mosaic approach detailed in zaehringer et al 2016 see appendix a for more details we collected data for this study during four field visits fvs the exploratory fv1 took place from end of august to mid september 2015 we visited three neighboring villages of mah being part of a former study to report its results these villages were not intended to be part of the actual study sites for this study at this occasion we conducted 13 interviews with farmers in the villages navana mahafidina and marofototra getting insight on their ways of cultivation and the value chain they are taking part fv2 took place from mid april to the end of april 2016 and aimed at setting up a game on land use decision making using interviews and four pre test game sessions fv3 took place from the beginning of november to mid december 2016 and focused on playing the game and debriefing players in order to create a learning environment and to evaluate the game s potential for the parameterization of bns fv4 took place in february 2018 and was used to verify patterns land use decision patterns revealed in the game session in each site we conducted a focus group discussion to critically discuss preliminary findings in each group 10 19 female 26 and male 74 farmers were present 3 results 3 1 is the land use decision making in the game similar to interview or workshop measures 3 1 1 questionnaire data the mean difference between the game and the questionnaire s cp was in absolute values from 0 03 to 0 28 fig 4 for the three land use categories which were discussed and played most often in the actual game paddy rice was more frequently chosen in land use decisions made by the players than in the questionnaire higher median by 24 while agroforestry systems afss was a less selected choice in the game compared to the cp resulting from the questionnaire data median 04 contextualization we observed a high interest in paddy rice during the game sessions compared to the questionnaire data a primary reason for this difference could be the credo rice is madagascar s foundation for food security which was frequently mentioned during game sessions to exclude the possibility that the different interest in paddy rice in the game and the questionnaire result from a bias due to location we checked from which villages the data was sourced and used zaehringer et al 2016 to check for the dominant land use in 2011 we played 12 seasons in mah 10 seasons in mor and fiz all three nowadays mixed paddy rice based and 4 seasons in bea mixed upland rice based questionnaire participants were sampled according to the relative number of the population per village this resulted in 56 cases farm households based in mixed paddy rice landscapes mah mor fiz and 15 cases from mixed upland rice landscapes bea thus the trend toward rice production in the game sessions cannot be explained due to a different database e g a bias due to location but the game seems to represent the current cultivation the intuitive game play shows rather the current strategies as the decision making is immediate and similar to the player s operational decision making the questionnaire however could be influenced by responses to assumed value systems triggered by the interaction with the researcher 3 1 2 workshop outputs table 3 shows the five decision situations brought up in the imagine exercise the revealed intention as cps and the deviation of the imagine exercise s values from the game s values three times we found a remarkably high deviation in case two participants in the imagine exercise emphasized the combined system of pastures with clove trees which represents a cash crop based production in the game we found a relatively high probability 0 23 for pure pasture without clove trees that is categorized neither as cash crop nor as rice based production in case three the workshop participants did not consider the possible cultivation of upland rice but concentrated on the cultivation of cash crops therefore there was a shift in weight from rice based to cash crop based cultivation contextualization there is coherence in the way farmers decide in the game and how experts think farmers decide revealed in the imagine exercise however the comparison reveals a difference of about 15 20 between game decisions and estimated decisions or intentions by regional agroforestry and farming experts in the imagine exercise farming experts overestimated the intention of cash crop based production systems compared to the game output 3 2 does the game capture all the major dimensions of luc decision making 3 2 1 sensitivity analysis to understand if the game captures major dimensions of luc decision making we checked how strongly the game induced conditions influenced game decisions table 4 shows the mean for all cpt entries separated by the varying conditions e g price rice self sufficiency etc and land use categories thus the table reveals how players decisions have changed being confronted with different conditions in the game e g state 1 state 2 we focus on players decisions regarding paddy rice upland rice and afs the last column reveals our expectations and rationale for potential divergence contextualization for price rice self sufficiency and need for money the game behavior was unexpectedly unrelated to the game induced conditions the price signal was revealed at the end of the year as in reality but players did not seem to rely on this information we checked this non correlation on our fourth field visit in a focus group discussion four discussions were held and the farmer groups stated that even if clove prices would decrease they would continue planting clove seedlings reasons for this rationale were that this investment pays off only after 4 7 years that they want to compensate for the lower price with higher production and that clove trees serve as an insurance as well as a value to bequest for the other two conditions rice self sufficiency and need for money we think participants risk averse thinking following their credo of rice is the foundation of our food security influenced their game behavior in addition water condition did not have an influence on the game decisions to include the water condition each participant was asked about how they experienced water availability this feature was surveyed for each participant referring to his or her reality at the beginning of the game we hypothesize that this spatially explicit location factor was not brought to the game the strongest influence on the game decisions had the condition traditional cultivation in the introductory phase of the game players were asked what their most prominent traditional land use was traditional was framed as the production system that they used most often looking back in time for about one generation and were proud of their declarations indicated upland rice for mor and bea and paddy rice for mah and fiz this traditional production system was brought to the game and players made decisions in the game according to these statements for mor however their statements contradict our analysis of the main land use shares see table 2 this could mean that their declarations or their perceptions were more relevant for the game play than the actual luc trajectories that are based on remote sensing based analyses 3 2 2 post game interviews the content analysis of the group debriefing revealed that in all game sessions the players suggested players to be winners when they were well off in monetary terms criteria for winning were related to monetary success and to savings high number of zebus a lot of land and stock of commodity only once education was mentioned which is related to financial means for paying school fees 3 3 is the game an appropriate tool to elicit luc decision making table 5 shows that players were judged to act fully consistently or only marginally inconsistently in 82 of the cases fig 5 shows that agroforestry including the cash crops of clove and vanilla was the most attractive strategy in the game and that players would refer to this strategy in their statements implementing a mixture of paddy rice agroforestry systems and even other land uses such as pasture or pasture with single clove trees was the second most frequently chosen option interestingly in the game many players focused on paddy rice having the highest labor input for this land use category but nobody admitted to this strategy in the debriefing importantly no player decided to invest a labor force in upland rice nor stated such a strategy in the debriefing contextualization in summary farmers were not fully consistent but their stated reflections on their strategy most often aligned with their decisions in the game setting hence there was alignment between played decisions and stated strategies as the foundation of their game play while this alignment could be seen as a pure memory test it was nevertheless a test for internal validity even if participants only tried to remember how they played they had to understand the game to answer consistently 3 4 is the game of use for research and the participants the group debriefing confirmed a relationship between productivity and labor input rationales to support this relationship focused on the potential delay of plant growth if labor input is too low this delay would lead to reduced productivity regarding fertility participants supported the relationship between non cultivation and increasing fertility hence to not cultivate a plot for a certain period would increase fertility interestingly players did not distinguish between paddy rice and upland rice in both cultivation types non cultivation for the period of one year in the game had led to increased productivity in the game in general the game was judged as being close to reality and participants were able to play their role contextualization in the interviews in the neighboring villages of our case study site mah in 2015 see also section 2 5 study area the link between labor intensity and paddy rice production was an important topic farmers referred to their shift from one cultivation period per year to a second one this change was triggered by increased demand for subsistence rice however they reported a decreasing yield since they are now cultivating paddy rice two times per year this relates to the abovementioned mechanism having a fallow state in paddy rice plots as well if in the second season the plot remains in a fallow state production of the producing season is higher in contrast in agroforestry systems and more specifically in clove cultivation labor input was not emphasized fluctuating yields were explained by productivity cycles inherent to clove plants 4 discussion and conclusions with the four validation perspectives analyzed we could show that our game bn link is suitable when comparing it to other parameterizing methods criterion validation comparing players actions during the game with players strategy statements respondent related validation as well as the stated degree of reality in the game was encouraging however sensitivity analysis reflecting content validation was disappointing at first sight potentially the game mechanisms were too subtle and embedded in a multitude of signals resulting in an uncertain situation for the players game players had to cope with this the coping strategy manifested itself in risk reducing behavior and a focus on rice production this reaction however was subconscious as discussions were still focused on the cash crop topic potentially fulfilling the supposed interest of the researchers nevertheless there is evidence for such behavior as rice is central to the livelihoods of northeastern madagascar rakotoarison 2014 the analysis provided insights in the game bn connection and hence we validated this transfer by triangulating different methods of validation a potential next step could be the validation at the level of the land use however this step is conditional to the understanding of the relationship between the game and the bn node outputs observing risk reducing behavior could be an added value of the game as this subconscious behavior could not be determined from the questionnaire in contrast a critical review must suggest ways to reduce complexity and increase the strength of the signals in the game by doing so we would expect more pronounced game reactions and consistent results in a sensitivity analysis however feedback from the participants also pointed to the fact that they recognized their decision making situation in which complexity is an essential part this aligns well with the diagnosis that due to contextual influences games may tend to more strongly support exploration than explanation van den hoogen et al 2016 to obtain higher production independent of productivity the expansion of cultivated land could be a strategy laney 2004 however as the resource of land is restricted in our case study area due to adjacent protected areas a next step could be to intensify labor and or capital input taking a long term perspective on the induced intensification thesis a next step could include higher investments infrastructure such as terraces and irrigation or definition of rules for water access turner and ali 1996 especially in a situation in which expensive investments in technology are not an option due to missing financial means inputs are increased within the existing strategy instead of innovating from a rain fed to an irrigated rice cultivation system farmers could increase cropping frequency styger et al 2007 this non change of cultivation technique could occur due to the perceived absence of alternatives the results of the sensitivity analysis showed that game induced conditions had little effect on players decision making we found explanations for such game behavior but we still would hypothesize that farmers follow rising clove and vanilla prices and adjust their labor force input along this line the game reveals a new perspective by asking which criteria within the broader topic of price development a farmer considers and to what extent e g difference in past price development expected price development expected price stability opportunity costs etc in addition by analyzing the rationales provided by participants in the group debriefing we found that the criteria for winning were strongly related to monetary success and to savings zebus land and commodity this leads to the conclusion that the game insufficiently triggered the monetary incentives participants judged the game to be close to reality in their statements however social desirability effects cannot be fully denied in addition game designers should be cautious to not imply ready made strategies with the game castella et al 2005 we tried to avoid this by balancing the game as much as possible with the help of real data such as prices for goods without reducing too much the criterion of playability in order to not push players toward a certain direction or strategy participants played the game with enthusiasm and never complained about unrealistic game settings for all players it was the first time they participated in such a game we observed that the game as a simplified version of reality triggered reflections and discussions about the players own farming strategies however starting from discussions about victory conditions and the degree of reality we did not capture any reflections going beyond the triggered topics possibly due to our questions focusing on labor input and fertility participants felt strongly guided in their responding a pre game interview or focus group would have been helpful to distinguish the bias induced by the game from their rationales in their daily life what parts of the answers in the post game debriefing were directly a result of the game remains an open question comparing the lessons learned from this study with abm approaches using games leads to additional insights abm and rpgs are conceptually closely related in both models rules are defined and interactions are taking place the abm profits by opening the black box of the model to obtain rules for parameterization in contrast using a game to parameterize a bn needs to conceptually and methodologically bridge a wider gap conceptually we step from game rules and player behavior to a causal system methodologically a translation of land use decisions manifesting themselves through labor input to cps is needed this may add plausibility as inconsistencies are discovered in contrast abm and rpgs are a close representation of each other and flaws might remain undiscovered furthermore this application provides lessons learned for other attempts to facilitate bn parameterization by methods that are more intuitive further research should focus on the learning aspects of this game by monitoring several rounds and time distant sessions feeding back results and inquiring into in depth relationships with reality and their cultivation as a few examples additionally the learning experience could be increased if players reported their experiences and insights to a wider public e g a sounding board this would support researchers seeking to better understand the learning effect of serious games e g sitzmann 2011 wouters et al 2013 these authors however focus on computer games in which social interactions are potentially less established here methods are required that specifically tackle the more complex situation of real social interactions additionally further research could investigate how multiple cpts could be populated with the same game by including additional game elements for example the node need for money in the land use decision bn could be elicited by implementing a bank that provides loans or grants translation of this game behavior to probabilities could be achieved by recording played choices and calculating percentages given the identical set of conditions we conclude that the applied validation measures depicted in three of four cases produced encouraging results the content validity sensitivity analysis was lacking in consistency hence the use of a game as a means to parameterize nodes in a bn must be further analyzed to show how complexity in decision making in reality and potentially in a game may be transferred to a bn given that game decision making is closely related to the actors mental models applied in their daily life a serious game may then help make decision making transparent here the transfer of a game to a bn may help bridge the game environment and the decision making in reality as the outputs of a bn based land use decision model may connect the game with land use changes and reality based maps software and or data availability section name of software or data set netica 5 18 64 bit developer and contact address norsys software corp 3512 west 23rd avenuevancouver bc canadav6s 1k5 telephone 1 604 221 2223 fax and e mail numbers info norsys com year first available na hardware required pc software required netica availability and cost free version with limited number of nodes commercial version 685 also for software program language program size for data form of repository data base files spreadsheet netica proprietary size of archive na access form https www norsys com netica html acknowledgements the authors would like to thank all players in the pre testing sessions and in the case study sites the project team of managing telecoupled landscapes with special thanks to the madagascar team from essa forêts bruno ramamonjisoa zo rabemananjara and laby patrick and to harimalala paul clément for his guidance translations and feedback jorge llopis cde university of berne for providing the case study site boundaries the fordev group at eth zürich for their support in game development special thanks to anne dray and claude garcia this research was supported by the swiss programme for research on global issues for development r4d programme supported by the swiss national science foundation snsf and the swiss agency for development and cooperation sdc grant number 100400 152167 appendix a methodology of the landscape mosaic approach the following details are based on zaehringer et al 2016 we summarize briefly the methodology fig a 1 and explain the land use categories used table a 1 as land use cannot be directly deduced from pixel based remote sensing data the landscape mosaic approach interpreted pixel information by taking into account human environmental interactions the locational information of a pixel was interpreted in a zonal perspective including context information i e the surroundings of a pixel with the landscape mosaic approach landscape type information was provided along two gradients a staple crop intensity and b tree cover subsequently using abundance criteria in a decision tree each pixel in a 5 5 km moving window was categorized along these two gradients the basic input data were landsat imagery land cover maps for 1995 and 2011 figure a 1 shows the resulting matrix of possible landscape types fig a 1 landscape types categorized by staple crop intensity and tree cover fd forest dominated htc high intensity tree crop ltc low intensity tree crop nc no staple source zaehringer et al 2016 fig a 1 table a 1 shows the analysis for the four case study sites of the landscape mosaic table a 1 number of pixels per case study site pixel size 30 30 m source of vector data zaehringer et al 2016 table a 1 fizono mahalevona morafeno beanana lu cat 1995 2011 1995 2011 1995 2011 1995 2011 nc fd 12 6289 nc htc nc ltc s fd 10518 5027 2 s htc s ltc ms fd 17604 12485 380 23 15168 ms htc 450 ms ltc mp fd 22586 3646 12774 3366 3009 1115 mp htc 3591 4182 4239 11 1041 mp ltc p fd 1945 3610 1012 7743 p htc 5167 29 p ltc appendix b bayesian network of the land use decision model fig a 2 shows the bn structure of the bn based land use decision model grey colored nodes were controlled during the game session to determine cp of the central node intention the bn has no populated cpts included as cpts were changing depending on the parameterization method e g game questionnaire in total 23 nodes were used resulting in 12 108 conditional probabilities for node definitions see table a 3 fig a 2 bn structure of the bn based land use decision model grey colored nodes were controlled during the game session to determine cp of the central node intention bn is shown without populated cpts as they were changing depending on the parameterization method e g game questionnaire fig a 2 table a 3 shows the node definitions table a 3 nodes and their definition table a 3 node name type of knowledge states definition land use t0 t1 no priors using geodata for updating rice paddy land use categories defined in collaboration with local actors and regional expert group shifting cultivation pasture pasture cloves dense plantation clove mixed agroforestry primary forest not modelled housing not modelled road not modelled slope no priors using geodata for updating flat flat slop smaller than 10 sloping sloping slope higher than 10 events no priors except that discussion were held under the assumption that there is no theft or cyclone no event shows the case of events cyclone theft water priors from questionnaire ok aggregated water situation including precipitation and irrigation systems lack protected area no prior except that discussion were held under the assumption that there is no protected area where farmers cultivate no strongly protected areas such as national parks or nature parks makira or masoala and community forests voi influence land use decision making given such an area the yes protected area at this location yes community forest voi at this location revenue questionnaire priors less than 50 sobika total amount of rice produces in one household measured in sobika 1 sobika approx 15 20 kg 30 48 l 50 100 sobika 100 600 sobika living costs questionnaire priors continuous discretized living costs including rice schooling medical expenses transportation and other expenses of daily needs reference yearly living costs 2 3 kapoaka rice day 1000 ariary value of rice per year 365000 ar 120 yearly income from other cash crops such as vanilla coffee questionnaire priors continuous discretized cash income from cash crops vanilla and coffee other than cloves reference local vanilla price in 2017 clove price farmer at farm gate questionnaire prior continuous discretized clove price per kg at local level farm gate in dollar reference clove price reached 20 000 ariary kg in 2017 approximately 5 6 5 9 clove price global market export price no data continuous discretized price at the exporter in madagascar in dollar per kg world no data over supply status of world clove market taking into account major producers importers and exporters a supply demand ratio reflects the potential push of the clove s market price over demand yield per stand questionnaire prior continuous discretized cloves harvested from one tree proxy to estimate total amount of cloves harvested are varying strongly an average seem to be 1 3 kg per tree clove income intermediate with equation continuous discretized equation price yieldperstand stands need for money intermediate with equation continuous discretized equation yearlyincome other cc cloveincome yearlyincome other livingcosts personshousehold rice self sufficiency intermediate with em learning continuous discretized comparing the number of persons with the rice revenue to estimate how many months a household may eat their own rice persons questionnaire prior continuous discretized number of persons in the same household supported by the same chef de ménage persons perhousehold questionnaire prior continuous discretized number of persons in the same household supported by the same chef de ménage stands questionnaire prior continuous discretized number of clove trees a household has tradition cultivation production no prior based on jinja shifting cultivation village based indicator depending on the location a different tradition applies based on horaka paddy rice cultivation based on tanimboly mixed agroforestry intention intermediate with em learning more rice paddy intention motivated by ajzen s theory of planned behavior ajzen 1991 more upland rice more pasture more pasture clove more afs more dense plantation clove 
26121,the management of multi use landscapes is challenging but essential when aiming at preserving the potential for ecosystem service provision land use decisions lay at the center of this challenge while land use decision models may help to transparently grasp land use decisions the parameterization of such models is difficult as human decision making is often not rational we show here how we used a serious game to parameterize a bayesian network based land use decision model to elicit validation game outputs are transformed to conditional probabilities and compared to conditional probabilities parameterized via a questionnaire and workshop exercises the analysis of four types of validity shows encouraging results for criterion respondent related and practice related validation however content validation sensitivity analysis was disappointing initially we discuss how the success in validation quality may be related to the design of the game and conclude that the transfer from a game to bayesian networks could improve the parameterization quality keywords bayesian network serious game participation validation parameterization 1 introduction globally preserving forests and their potential for ecosystem provision is seen as a central challenge for sustainability lambin and meyfroidt 2011 in madagascar the search for strategies to sustainably protect forests including the perspectives of local actors and their livelihood needs led to a shift from strictly conserved forest areas to multi use sites gardner et al 2013 in multi use sites land use decisions are central to land use and landscape changes influencing the state of a socio ecological system mcginnis and ostrom 2014 hence land use decisions are key when defining such strategies and frame the trade offs among different land use types such as upland rice paddy rice and agroforestry systems however land use decisions are made following rationales and heuristics which are difficult to grasp cf pahl wostl 2007 among others individual preferences and mental models are introducing complexity into land use change luc processes as they cannot be fully determined in addition interactions among decision makers are influencing their decisions and social networks have an impact on individuals decisions isaac and matous 2017 manson et al 2016 as personal preferences may alter the development of a parcel and subsequently a farm and a landscape these individual decisions are of particular importance whenever a landscape management problem is approached blanco et al 2017 primdahl and kristensen 2011 in such landscape management processes we need methods supporting local actors self evaluation a possibility for making land use decisions more tangible and for considering them in landscape management is the development of land use decision models however their ability for prediction is by no means a revelation of truth but rather an approach to make possible decision making rationales explicit modeling helps clarify land use decisions and their inherent trade offs land use decisions and related trade offs may be captured in a serious game we use the term serious game to reflect that besides the joyful part of a game a game may be used for learning instruction or deliberations of challenges wouters et al 2013 according to wouters and colleagues characteristics of a game are interaction agreed upon rules goal orientation often in connection with a challenge and the provision of feedback these characteristics are applicable for analog games too additionally for role playing games rpgs participants adopt specific roles in the iterative development process the game was inspired by other serious games in the broader field of land system science cleland et al 2012 used a game board with cards representing the characteristics of a location or livelihood options to explore interactions in subsistence fishing communities salvini et al 2016 aimed at showing the learning effect of a game and induced the effects of sustainable management in the game demonstrating its effect villamor and van noordwijk 2011 designed a game including villagers and external agents providing financial incentives and let them interact in the context of forest conservation and management similar to many other game approaches they introduced exogenous changes i e population increase forest fire rubber price decrease in the rounds of the game the potential added value of combining a game with a computer based model was recognized by the companion modeling community étienne 2014 and their extensive use of agent based models abms the rpgs may be designed after the abm existed campo et al 2009 castella et al 2005 or before salvini et al 2016 in the latter case the transfer from the game to the abm was based on the choices played in the game thus model mechanisms could be iteratively calibrated and validated with the help of a game the main research needs are related to the translation from a game to a computational model and the challenge grasping often intangible decision making processes in many publications the transfer from a role playing or tabletop game to a computer model has not been specified in more detail barnaud et al 2013 even distinguishes between the computer model producing scientific knowledge and the game aiming at facilitating communication however both a well defined transfer from a game to a computer model and the production of scientific knowledge with a game help to ensure credibility of such processes mallampalli et al 2016 compared 10 methods helping translate narrative scenarios to quantitative models while both the bayesian network bn and rpgs are seen as such a translation method a possible combination of these two approaches was not mentioned they suggested rpgs to be most compatible with an abm here we would like to consider the usefulness of combining rpgs with bns using the ability of games to reveal insights that rarely emerge in interviews pak and brieva 2010 to combine a serious game with modeling is regarded as advantageous as in depth deliberation in the game may be combined with the forecasting capabilities of a modeling exercise barreteau et al 2001 voinov et al 2016 here we focus on the nexus between a game and our modeling tool which is a bn more specifically connecting a game to our bn seems promising as game participants bring their habits and mental models with them to the game and we can elicit participants system knowledge taking into account their ideas and intentions castella et al 2005 lamarque et al 2013 in addition rpgs may help to jointly define the problems to be addressed pak and brieva 2010 we test a serious game which mimics land use change decision making participants play multiple rounds to explore different scenarios and the results are used to parameterize a bn node methodologically we describe a connection that was to the best of our knowledge not made until now this paper focuses on the transfer from a tabletop rpg to a computer model the paper aims at making this procedure transparent and critically reviewing the added value for the specific case of bns hence evidence for reliability and validity of this transfer needs to be presented we would like to answer the following question how valid is the presented game as a method to parameterize a bn 2 methods 2 1 the game in this research and learning game van den hoogen et al 2016 players cultivate land in order to achieve a good living for the household in the game local actors were confronted with land use decision situations players had to allocate labor force units to their land use portfolio therefore we called the game sandry game as sandry in the local malagasy dialect means labor force and strength of a person to create something the allocation of labor force units pawns in the game served as a proxy for the players intentions the game set up was based on field data observations gained through field missions discussions with madagascar experts as well as trial and error to adjust game mechanisms fig 1 shows the game set up left players and the tokens for cloves and vanilla as well as the fake money top right three possible roles were attributed to the players a four farmers were the key actors of the game they made labor force decisions by allocating labor force units to available land use plots the farmers were characterized by available land use plots and household size both of which varied in number however the money available was the same for all farmers these characterizations were based on real life situations but adapted to a game situation by reducing complexity e g biophysical mechanisms number of land use categories and increasing wealth of underprivileged roles to increase playability others classified households in a neighboring region in more detail but similarly laney and turner 2015 zaehringer et al 2017 they used farming area per household the number of commercial plants income from cash crop production production of rice and others to create household groups b the collectors are a type of intermediary actor who buys cloves from villages and sells them to a next level of the value chain in the game they sell cloves directly to the exporter two collectors sell to the same exporter for vanilla markets are organized in central villages and prices are determined by the world market game coordinator the game coordinator is buying vanilla directly from farmers at a standard but varying price for cloves collectors visit each household to buy the harvest c the role of the exporter is scripted and played by a game facilitator in the game the exporter is responsible for the demand of cash crops and has capital stock to buy all the cloves on the market the exporter discusses contracts with collectors exporters receive demand and price signals from the world market the game coordinator secretly provides this information in the first year a normal average season was revealed to participants the characteristics of the subsequent seasons followed a sequence confronting farmers with a theft a cyclone a season with good yields a pest and three average seasons in which only market prices for cash crops would change the labor force allocations were recorded in an excel sheet after each season in other words the number of pawns for every land use parcel players cultivated were recorded each game session ended with a debriefing session to reflect on the victory condition the degree of reality and the strategy played by participants after each year all participants had to complete an annual accounting and report to the game coordinator the amount of crop stock and the amount of money additionally players reported whether they want to send children to school and lastly they were asked to rate with a sign of their thumbs their overall satisfaction with their current game performance the full game lasted between 3 and 3 5 h playing three years with two seasons each the thin red line between being close to reality and at the same time enable playability was constantly reflected within the larger research team and in the debriefing of the game to reflect on this challenge we analyzed the game and its output to demonstrate the prototypical transfer of the game data to a conditional probability table cpt and to compare the parameterization from the game with other data sources i e questionnaire and workshop outputs 2 2 bayesian networks and the land use decision model bns are directed acyclic graphs network nodes represent variables and arcs represent causal relationships all nodes and their relationships are specified with cpts the bayesian rule of conditional probability is applicable to infer posterior probabilities focusing on two nodes in a bn posterior probability is calculated via p x e p x p e x p e where x is the probability of a parent node state and e is the evidence of a child node state to solve an entire network structure chain rule is applied e g for three nodes in a bn equation p a b c p a b c p b c p c shows the chain rule and determines the network structure next marginalization or variable elimination allows us to remove variables from a joint probability distribution jensen and nielsen 2007 p 10 as all possible probabilities of one variable are summed up to determine the marginal contribution of another in bn solving marginalization helps to efficiently determine posterior probability using the conditional independence properties or equivalently the directed global markov property in a bn increases efficiency as factorization of distributions into distributions with lower dimensions is possible kjaerulff and madsen 2008 algorithms based on the bayesian rule are used in software that provides graphical user or application programming interfaces jensen and nielsen 2007 kjaerulff and madsen 2008 norsys 2011 with these types of software entire network structures can be calculated we used norsys netica version 5 18 64 bit the game gave us input to one specific and central node in the bn called land use intention compared to abm approaches our game does not mirror the computer model or vice versa we rather use the game a to inform one node in the bn and b to create a learning environment for participants and researchers the bn reflects luc decisions and the mentioned central node land use intention is the child node that captures influence factors a farmer takes into account optimizing his farm level decisions land use intention in turn is parent to the actual land use category at plot level that is influenced by biophysical characteristics and policy or regulations this structure is strongly motivated by isaac ajzen s theory of planned behavior ajzen 1991 and the bn is linked dynamically to spatial data celio et al 2012 2014 the full bn is shown in appendix b the model is used to predict luc and to show different time steps of land use in a spatially explicit manner the bn was set up in a participatory process involving farmers in our study sites and land use experts in the district capital of maroantsetra the produced maps are intended to support strategic decision making at village level in regard of dynamic cash crop markets and nearby protected areas for parameterization we used two approaches a expectation maximization em learning and b workshops to get knowledge on key cps in the tables we used netica s em algorithm to learn conditional probabilities as it is viewed as more robust than gradient descent norsys 2011 before learning all probability distributions are uniform for learning we used two case files derived from a questionnaire conducted in fv3 that collected data on land use and land use decision making we first used the farm level data n 71 and in a second step the plot level data n 171 for learning 2 3 transfer from the game to a bn to establish a transfer from the game outputs to the bn node intention we used the game as an arena where certain conditions were set and land use decisions were made hence we gained knowledge under conditions framed in the game scenarios fig 2 shows the conditions that needed to be controlled in the game need of money referred to the perception whether gained money is sufficient to cover daily needs traditional land use reflected the self conception of the households dominantly cultivated land use local price level reflected the clove price water availability referred to the sum of precipitation and irrigation possibilities and rice self sufficiency showed how many months per year a household is rice self sufficient due to different player characteristics and game scenarios all except one price level states of the parent nodes could be covered however the number of game sessions was too small to cover all the state combinations in the game participants placed their pawns on different land use plots this number per specific land use was used as an input to calculate our proxy for conditional probabilities formula 1 and 2 we assumed that the revealed intention in the game may be transferred to conditional probabilities labor input per lu type was adjusted by the total number of plots in this category and normalized with the sum of these ratios per land use type normalized labor intensity l a b o u r i n t e n s i t y l i c f o r m u l a 1 n o r m a l i z e d l a b o r i n t e n s i t y n l i c f o r m u l a 2 l i c s a n d r y i n p u t i n c a t e g o r y p l o t s i n c a t e g o r y i c p c n l i c l i c c 1 c l i c we played five game sessions that were judged to be valid three times six seasons and two times four seasons sixteen players labor force allocations were analyzed for seasons one to four and nine for seasons five and six the calculated normalized labor intensity is equal to p x e and reflects one cp in the cpt intention we obtained 82 seasons or probability distributions for one set of conditions due to overlap of identical conditions we averaged entries from identical sets of conditions and deduced 28 final probability distributions 2 4 methods for validation as validation is interpreted from different perspectives bowen 2008 a clear framing for this study is needed in order to adopt a broader view of validation bowen 2008 we present evidence for validity 1 showing that the scores obtained from the game accurately reflect the land use change decision making and 2 showing the usefulness of the scores obtained from the game bowen 2008 land use change decision making is captured by the game and thereon based scores therefore we pose sub questions to the abovementioned guiding question which are related to different perspectives of validation table 1 validity focuses on how accurately and completely a measure captures its target construct bowen 2008 to validate the quantitative inputs from the game we used the above mentioned debriefing of game participants and in addition a workshop exercise we called imagine exercise as well as questionnaire data which are used as parameterizing methods for the overall bn too for each of the research questions we used a specific procedure that was also related to specific perspectives on validation a criterion validity was evaluated by comparing if measured scores the game luc decision making were related to scores from other measures of the same construct luc decision making measured in questionnaire and in workshops b content validation aimed at evaluating if all dimensions of a construct the luc decision making were captured c respondent related validation analyzed if content and format of a measure the game was appropriate to caption the players luc decision making d practice related validation evaluated whether scores the game luc decision making were relevant to luc decision making in reality and relevant to fill a knowledge gap of the target group bowen 2008 in the results section each result is accompanied by paragraphs contextualizing the results as this information helps to understand the elaborated validation results in the following we will explain the methods for each of the four different perspectives of validation in detail 2 4 1 criterion validation comparison of outputs from the game with triangulation methods questionnaire 1 and workshop 2 1 using questionnaire data a farmer questionnaire provided data for parameter learning of the bn for each node included in the network we gathered farmer characteristics the analysis applying chi square test and anova on the questionnaire data revealed that water availability and traditional land use were not significantly related to intention hence we removed these two nodes from the bn for this analysis to be consistent we also adapted the cpt of the game for this analysis and removed these two nodes from the 82 player seasons rounds and players played we aggregated identical condition settings and obtained 14 distinctive cp distributions to compare with the respective cp distributions learned from the questionnaire data to compare the game luc decision making with the questionnaire the answers of the questionnaire were classified according to the node states we could use 71 cases on farm level and 171 cases on plot level for em learning that reflected the empirical combinations of intention and the three remaining parent nodes need of money local price level rice self sufficiency in a next step the obtained conditional probability entries in the cpt for the node intention could be subtracted from the conditional probability entries obtained via the game and we displayed the 14 cp that we compared in a boxplot 2 using workshop outputs in workshops we conducted a specific exercise which we called imagine exercise the aim of the imagine exercise was the elicitation of conditional probabilities by evoking typical situations of farmers in that region members of the stakeholder platform were asked to respond to the following type of questions imagine 10 households in geographically lower levels of the district e g mahalevona and these households are not indebted water availability does not pose any difficulties and the family produces enough rice to be rice sufficient all year long the price of cloves is at the level of 2015 a what is the intention of the majority of the households in this situation the five possible intention categories are more paddy rice more upland rice more pasture more pasture clove and more agroforestry system b how many of these 10 households would have this identical idea intention combining the answers of questions a and b allowed us to determine a conditional probability entry for the node intention we repeated this procedure for an additional four times changing condition each time one to obtain a comparison dataset the imagine exercise was conducted with eleven members of our regional stakeholder platform that is reflecting and consulting on the project s plans and results this group consisted of agriculture experts such as farmer representatives farmer cooperatives farming consultancy ngos as well as district representatives of forest and agriculture this exercise was analyzed by summing up the estimated household counts with the same decisions over all participants and normalizing them over the five possible land use intentions hence we obtained a conditional probability estimate for five conditional probability distributions 2 4 2 content validation 1 sensitivity analysis to check for game driving forces in land use decisions 2 post game interviews focusing on victory conditions 1 sensitivity analysis via the game we specified 28 cpt entries and all conditions were checked for their influence on the cp to do so the mean over all cpt entries with the same varying condition e g price rice self sufficiency etc per land use categories were calculated thus by comparing these means we obtained the influence of one varying condition on the game luc decision making for example looking at the intention more paddy rice the mean of all cps given a local clove price of 5 5 6 5 was compared to the mean of all cps given a local clove price of 6 5 7 5 2 post game interviews after the game sessions group debriefings were conducted the game did not focus on winning however competition was part of the game the triggering question was who won the game and why this question triggered discussion and reflections on the victory condition answers were recorded transcribed and content analyzed in total 18 participants of six game sessions discussed the questions 2 4 3 respondent related validation analysis of consistency in game behavior and strategy statements in post game interviews with this validation step we determined if the game is an appropriate tool to elicit luc decision making to do so players were asked in the debriefing of the game sessions in a person to person interview what their game strategies were these statements were compared to their actual decisions in the game i e their allocation of labor force units to the different land use categories their preferred land use category was approximated by determining the highest probability of a land use intention over a game session we could distinguish 1 whether players were consistent in their statement and in their game decision making or not observation of game play and 2 whether the content of the statement aligned to their actual game behavior or not self declaration of game play fully consistent players precisely pointed out a certain strategy e g optimizing for cash crop production in their statements and this strategy could have been recognized in their game decisions marginally inconsistent players formulated either their strategy ambiguously or their game decisions allowed interpreting a certain range of strategies without specifying that the strategy was diversification inconsistent players were contradictory e g stating that their strategy was focused on cash crop production but showing a clear priority for paddy rice in their game decisions 2 4 4 practice related validation content analysis of post game group debriefing during the group debriefing mentioned in 2 5 2 we also asked questions about the degree of reality of the game the questions triggered discussions and reflections on the degree of reality which is also called external validity in game settings van den hoogen et al 2016 two triggering questions were used in this discussion a how do you judge the degree of reality concerning productivity increase through labor force intensification b how do you judge the degree of reality concerning the implementation of soil fertility in the game in total 18 participants of six game sessions discussed the questions discussions were recorded transcribed and content analyzed we evaluated how the participants judged on the game mechanisms and how they perceived these mechanisms in reality 2 5 study area this study belonged to a six year research project that aims to collaboratively define and test strategies for securing ecosystem services and develop local livelihoods the study areas were chosen enabling comparison between the study areas in laos madagascar and myanmar here we report from the study area located in northeastern madagascar where trade offs between land use categories are pronounced as this area is situated between two major protected areas makira natural park and masoala national park fig 3 this area is judged a hotspot of biodiversity ganzhorn et al 2001 hence many existing publications shed light into the ecological system e g andrianjakarivelo et al 2005 merenlender et al 1998 rakotomanana et al 2001 fewer publications take an anthropological e g keller 2008 or a combined perspective e g fedele et al 2016 golden and comaroff 2015 zaehringer et al 2017 to test the game bn translation we studied a socio ecological system ses in which actors in our case farmers are confronted with a context of increasing complexity cf seto and reenberg 2014 complexity rises as global market interactions intensify through market integration adger et al 2009 liu et al 2013 local access to land resources becomes relatively scarce e g by delimiting protected areas ward et al 2018 and the mutually reinforcing mechanisms of increased financial means and locally diversified demand for globally distributed consumer goods in our study area farmers are confronted with trade offs between feeding a household through subsistence rice cultivation and earning money through cash crop cultivation to buy daily needs products and invest in the farm this is a common challenge for all farmers in northeastern madagascar pfund et al 2011 laney and turner 2015 propose an endogenous and an exogenous view why farmers keep subsistence production the endogenous perspective emphasizes traditions or norms while the exogenous perspective emphasizes market imperfections or constraints induced by rising prices clove and vanilla production gained a dynamic and the trade off between subsistence and cash crop farming was emphasized in recent years in the region of analanjirofo clove cultivation has historically been and is still an important part of the agricultural sector danthu et al 2014 the therein included district of maroantsetra is a rural area with about 14 of the population living in the district capital of maroantsetra and about 84 of the population in the district being farmers rasolofomanana 2009 we concentrate on two areas each composed of two villages in the district of maroantsetra fig 3 the northern site corresponds to the villages mahalevona mah and fizono fiz municipality of mahalevona and the southern site to the villages morafeno mor and beanana bea municipality of morafeno from both case study sites inhabitants travel about one day to the district capital called maroantsetra main land uses in the areas are the cultivation of irrigated paddy rice rainfed upland rice agroforestry systems including cash and subsistence crops and pastures for zebus table 2 analyzing the landscape mosaic in 1995 and 2011 farmers in mah developed from cultivating mostly a mixed upland rice landscape in 1995 to a mixed paddy rice landscape in 2011 farmers in fiz developed with the same pattern however farmers in mor cultivated constantly a mixed paddy landscape and bea cultivated constantly a mixed upland rice landscape in this period based on landscape mosaic approach detailed in zaehringer et al 2016 see appendix a for more details we collected data for this study during four field visits fvs the exploratory fv1 took place from end of august to mid september 2015 we visited three neighboring villages of mah being part of a former study to report its results these villages were not intended to be part of the actual study sites for this study at this occasion we conducted 13 interviews with farmers in the villages navana mahafidina and marofototra getting insight on their ways of cultivation and the value chain they are taking part fv2 took place from mid april to the end of april 2016 and aimed at setting up a game on land use decision making using interviews and four pre test game sessions fv3 took place from the beginning of november to mid december 2016 and focused on playing the game and debriefing players in order to create a learning environment and to evaluate the game s potential for the parameterization of bns fv4 took place in february 2018 and was used to verify patterns land use decision patterns revealed in the game session in each site we conducted a focus group discussion to critically discuss preliminary findings in each group 10 19 female 26 and male 74 farmers were present 3 results 3 1 is the land use decision making in the game similar to interview or workshop measures 3 1 1 questionnaire data the mean difference between the game and the questionnaire s cp was in absolute values from 0 03 to 0 28 fig 4 for the three land use categories which were discussed and played most often in the actual game paddy rice was more frequently chosen in land use decisions made by the players than in the questionnaire higher median by 24 while agroforestry systems afss was a less selected choice in the game compared to the cp resulting from the questionnaire data median 04 contextualization we observed a high interest in paddy rice during the game sessions compared to the questionnaire data a primary reason for this difference could be the credo rice is madagascar s foundation for food security which was frequently mentioned during game sessions to exclude the possibility that the different interest in paddy rice in the game and the questionnaire result from a bias due to location we checked from which villages the data was sourced and used zaehringer et al 2016 to check for the dominant land use in 2011 we played 12 seasons in mah 10 seasons in mor and fiz all three nowadays mixed paddy rice based and 4 seasons in bea mixed upland rice based questionnaire participants were sampled according to the relative number of the population per village this resulted in 56 cases farm households based in mixed paddy rice landscapes mah mor fiz and 15 cases from mixed upland rice landscapes bea thus the trend toward rice production in the game sessions cannot be explained due to a different database e g a bias due to location but the game seems to represent the current cultivation the intuitive game play shows rather the current strategies as the decision making is immediate and similar to the player s operational decision making the questionnaire however could be influenced by responses to assumed value systems triggered by the interaction with the researcher 3 1 2 workshop outputs table 3 shows the five decision situations brought up in the imagine exercise the revealed intention as cps and the deviation of the imagine exercise s values from the game s values three times we found a remarkably high deviation in case two participants in the imagine exercise emphasized the combined system of pastures with clove trees which represents a cash crop based production in the game we found a relatively high probability 0 23 for pure pasture without clove trees that is categorized neither as cash crop nor as rice based production in case three the workshop participants did not consider the possible cultivation of upland rice but concentrated on the cultivation of cash crops therefore there was a shift in weight from rice based to cash crop based cultivation contextualization there is coherence in the way farmers decide in the game and how experts think farmers decide revealed in the imagine exercise however the comparison reveals a difference of about 15 20 between game decisions and estimated decisions or intentions by regional agroforestry and farming experts in the imagine exercise farming experts overestimated the intention of cash crop based production systems compared to the game output 3 2 does the game capture all the major dimensions of luc decision making 3 2 1 sensitivity analysis to understand if the game captures major dimensions of luc decision making we checked how strongly the game induced conditions influenced game decisions table 4 shows the mean for all cpt entries separated by the varying conditions e g price rice self sufficiency etc and land use categories thus the table reveals how players decisions have changed being confronted with different conditions in the game e g state 1 state 2 we focus on players decisions regarding paddy rice upland rice and afs the last column reveals our expectations and rationale for potential divergence contextualization for price rice self sufficiency and need for money the game behavior was unexpectedly unrelated to the game induced conditions the price signal was revealed at the end of the year as in reality but players did not seem to rely on this information we checked this non correlation on our fourth field visit in a focus group discussion four discussions were held and the farmer groups stated that even if clove prices would decrease they would continue planting clove seedlings reasons for this rationale were that this investment pays off only after 4 7 years that they want to compensate for the lower price with higher production and that clove trees serve as an insurance as well as a value to bequest for the other two conditions rice self sufficiency and need for money we think participants risk averse thinking following their credo of rice is the foundation of our food security influenced their game behavior in addition water condition did not have an influence on the game decisions to include the water condition each participant was asked about how they experienced water availability this feature was surveyed for each participant referring to his or her reality at the beginning of the game we hypothesize that this spatially explicit location factor was not brought to the game the strongest influence on the game decisions had the condition traditional cultivation in the introductory phase of the game players were asked what their most prominent traditional land use was traditional was framed as the production system that they used most often looking back in time for about one generation and were proud of their declarations indicated upland rice for mor and bea and paddy rice for mah and fiz this traditional production system was brought to the game and players made decisions in the game according to these statements for mor however their statements contradict our analysis of the main land use shares see table 2 this could mean that their declarations or their perceptions were more relevant for the game play than the actual luc trajectories that are based on remote sensing based analyses 3 2 2 post game interviews the content analysis of the group debriefing revealed that in all game sessions the players suggested players to be winners when they were well off in monetary terms criteria for winning were related to monetary success and to savings high number of zebus a lot of land and stock of commodity only once education was mentioned which is related to financial means for paying school fees 3 3 is the game an appropriate tool to elicit luc decision making table 5 shows that players were judged to act fully consistently or only marginally inconsistently in 82 of the cases fig 5 shows that agroforestry including the cash crops of clove and vanilla was the most attractive strategy in the game and that players would refer to this strategy in their statements implementing a mixture of paddy rice agroforestry systems and even other land uses such as pasture or pasture with single clove trees was the second most frequently chosen option interestingly in the game many players focused on paddy rice having the highest labor input for this land use category but nobody admitted to this strategy in the debriefing importantly no player decided to invest a labor force in upland rice nor stated such a strategy in the debriefing contextualization in summary farmers were not fully consistent but their stated reflections on their strategy most often aligned with their decisions in the game setting hence there was alignment between played decisions and stated strategies as the foundation of their game play while this alignment could be seen as a pure memory test it was nevertheless a test for internal validity even if participants only tried to remember how they played they had to understand the game to answer consistently 3 4 is the game of use for research and the participants the group debriefing confirmed a relationship between productivity and labor input rationales to support this relationship focused on the potential delay of plant growth if labor input is too low this delay would lead to reduced productivity regarding fertility participants supported the relationship between non cultivation and increasing fertility hence to not cultivate a plot for a certain period would increase fertility interestingly players did not distinguish between paddy rice and upland rice in both cultivation types non cultivation for the period of one year in the game had led to increased productivity in the game in general the game was judged as being close to reality and participants were able to play their role contextualization in the interviews in the neighboring villages of our case study site mah in 2015 see also section 2 5 study area the link between labor intensity and paddy rice production was an important topic farmers referred to their shift from one cultivation period per year to a second one this change was triggered by increased demand for subsistence rice however they reported a decreasing yield since they are now cultivating paddy rice two times per year this relates to the abovementioned mechanism having a fallow state in paddy rice plots as well if in the second season the plot remains in a fallow state production of the producing season is higher in contrast in agroforestry systems and more specifically in clove cultivation labor input was not emphasized fluctuating yields were explained by productivity cycles inherent to clove plants 4 discussion and conclusions with the four validation perspectives analyzed we could show that our game bn link is suitable when comparing it to other parameterizing methods criterion validation comparing players actions during the game with players strategy statements respondent related validation as well as the stated degree of reality in the game was encouraging however sensitivity analysis reflecting content validation was disappointing at first sight potentially the game mechanisms were too subtle and embedded in a multitude of signals resulting in an uncertain situation for the players game players had to cope with this the coping strategy manifested itself in risk reducing behavior and a focus on rice production this reaction however was subconscious as discussions were still focused on the cash crop topic potentially fulfilling the supposed interest of the researchers nevertheless there is evidence for such behavior as rice is central to the livelihoods of northeastern madagascar rakotoarison 2014 the analysis provided insights in the game bn connection and hence we validated this transfer by triangulating different methods of validation a potential next step could be the validation at the level of the land use however this step is conditional to the understanding of the relationship between the game and the bn node outputs observing risk reducing behavior could be an added value of the game as this subconscious behavior could not be determined from the questionnaire in contrast a critical review must suggest ways to reduce complexity and increase the strength of the signals in the game by doing so we would expect more pronounced game reactions and consistent results in a sensitivity analysis however feedback from the participants also pointed to the fact that they recognized their decision making situation in which complexity is an essential part this aligns well with the diagnosis that due to contextual influences games may tend to more strongly support exploration than explanation van den hoogen et al 2016 to obtain higher production independent of productivity the expansion of cultivated land could be a strategy laney 2004 however as the resource of land is restricted in our case study area due to adjacent protected areas a next step could be to intensify labor and or capital input taking a long term perspective on the induced intensification thesis a next step could include higher investments infrastructure such as terraces and irrigation or definition of rules for water access turner and ali 1996 especially in a situation in which expensive investments in technology are not an option due to missing financial means inputs are increased within the existing strategy instead of innovating from a rain fed to an irrigated rice cultivation system farmers could increase cropping frequency styger et al 2007 this non change of cultivation technique could occur due to the perceived absence of alternatives the results of the sensitivity analysis showed that game induced conditions had little effect on players decision making we found explanations for such game behavior but we still would hypothesize that farmers follow rising clove and vanilla prices and adjust their labor force input along this line the game reveals a new perspective by asking which criteria within the broader topic of price development a farmer considers and to what extent e g difference in past price development expected price development expected price stability opportunity costs etc in addition by analyzing the rationales provided by participants in the group debriefing we found that the criteria for winning were strongly related to monetary success and to savings zebus land and commodity this leads to the conclusion that the game insufficiently triggered the monetary incentives participants judged the game to be close to reality in their statements however social desirability effects cannot be fully denied in addition game designers should be cautious to not imply ready made strategies with the game castella et al 2005 we tried to avoid this by balancing the game as much as possible with the help of real data such as prices for goods without reducing too much the criterion of playability in order to not push players toward a certain direction or strategy participants played the game with enthusiasm and never complained about unrealistic game settings for all players it was the first time they participated in such a game we observed that the game as a simplified version of reality triggered reflections and discussions about the players own farming strategies however starting from discussions about victory conditions and the degree of reality we did not capture any reflections going beyond the triggered topics possibly due to our questions focusing on labor input and fertility participants felt strongly guided in their responding a pre game interview or focus group would have been helpful to distinguish the bias induced by the game from their rationales in their daily life what parts of the answers in the post game debriefing were directly a result of the game remains an open question comparing the lessons learned from this study with abm approaches using games leads to additional insights abm and rpgs are conceptually closely related in both models rules are defined and interactions are taking place the abm profits by opening the black box of the model to obtain rules for parameterization in contrast using a game to parameterize a bn needs to conceptually and methodologically bridge a wider gap conceptually we step from game rules and player behavior to a causal system methodologically a translation of land use decisions manifesting themselves through labor input to cps is needed this may add plausibility as inconsistencies are discovered in contrast abm and rpgs are a close representation of each other and flaws might remain undiscovered furthermore this application provides lessons learned for other attempts to facilitate bn parameterization by methods that are more intuitive further research should focus on the learning aspects of this game by monitoring several rounds and time distant sessions feeding back results and inquiring into in depth relationships with reality and their cultivation as a few examples additionally the learning experience could be increased if players reported their experiences and insights to a wider public e g a sounding board this would support researchers seeking to better understand the learning effect of serious games e g sitzmann 2011 wouters et al 2013 these authors however focus on computer games in which social interactions are potentially less established here methods are required that specifically tackle the more complex situation of real social interactions additionally further research could investigate how multiple cpts could be populated with the same game by including additional game elements for example the node need for money in the land use decision bn could be elicited by implementing a bank that provides loans or grants translation of this game behavior to probabilities could be achieved by recording played choices and calculating percentages given the identical set of conditions we conclude that the applied validation measures depicted in three of four cases produced encouraging results the content validity sensitivity analysis was lacking in consistency hence the use of a game as a means to parameterize nodes in a bn must be further analyzed to show how complexity in decision making in reality and potentially in a game may be transferred to a bn given that game decision making is closely related to the actors mental models applied in their daily life a serious game may then help make decision making transparent here the transfer of a game to a bn may help bridge the game environment and the decision making in reality as the outputs of a bn based land use decision model may connect the game with land use changes and reality based maps software and or data availability section name of software or data set netica 5 18 64 bit developer and contact address norsys software corp 3512 west 23rd avenuevancouver bc canadav6s 1k5 telephone 1 604 221 2223 fax and e mail numbers info norsys com year first available na hardware required pc software required netica availability and cost free version with limited number of nodes commercial version 685 also for software program language program size for data form of repository data base files spreadsheet netica proprietary size of archive na access form https www norsys com netica html acknowledgements the authors would like to thank all players in the pre testing sessions and in the case study sites the project team of managing telecoupled landscapes with special thanks to the madagascar team from essa forêts bruno ramamonjisoa zo rabemananjara and laby patrick and to harimalala paul clément for his guidance translations and feedback jorge llopis cde university of berne for providing the case study site boundaries the fordev group at eth zürich for their support in game development special thanks to anne dray and claude garcia this research was supported by the swiss programme for research on global issues for development r4d programme supported by the swiss national science foundation snsf and the swiss agency for development and cooperation sdc grant number 100400 152167 appendix a methodology of the landscape mosaic approach the following details are based on zaehringer et al 2016 we summarize briefly the methodology fig a 1 and explain the land use categories used table a 1 as land use cannot be directly deduced from pixel based remote sensing data the landscape mosaic approach interpreted pixel information by taking into account human environmental interactions the locational information of a pixel was interpreted in a zonal perspective including context information i e the surroundings of a pixel with the landscape mosaic approach landscape type information was provided along two gradients a staple crop intensity and b tree cover subsequently using abundance criteria in a decision tree each pixel in a 5 5 km moving window was categorized along these two gradients the basic input data were landsat imagery land cover maps for 1995 and 2011 figure a 1 shows the resulting matrix of possible landscape types fig a 1 landscape types categorized by staple crop intensity and tree cover fd forest dominated htc high intensity tree crop ltc low intensity tree crop nc no staple source zaehringer et al 2016 fig a 1 table a 1 shows the analysis for the four case study sites of the landscape mosaic table a 1 number of pixels per case study site pixel size 30 30 m source of vector data zaehringer et al 2016 table a 1 fizono mahalevona morafeno beanana lu cat 1995 2011 1995 2011 1995 2011 1995 2011 nc fd 12 6289 nc htc nc ltc s fd 10518 5027 2 s htc s ltc ms fd 17604 12485 380 23 15168 ms htc 450 ms ltc mp fd 22586 3646 12774 3366 3009 1115 mp htc 3591 4182 4239 11 1041 mp ltc p fd 1945 3610 1012 7743 p htc 5167 29 p ltc appendix b bayesian network of the land use decision model fig a 2 shows the bn structure of the bn based land use decision model grey colored nodes were controlled during the game session to determine cp of the central node intention the bn has no populated cpts included as cpts were changing depending on the parameterization method e g game questionnaire in total 23 nodes were used resulting in 12 108 conditional probabilities for node definitions see table a 3 fig a 2 bn structure of the bn based land use decision model grey colored nodes were controlled during the game session to determine cp of the central node intention bn is shown without populated cpts as they were changing depending on the parameterization method e g game questionnaire fig a 2 table a 3 shows the node definitions table a 3 nodes and their definition table a 3 node name type of knowledge states definition land use t0 t1 no priors using geodata for updating rice paddy land use categories defined in collaboration with local actors and regional expert group shifting cultivation pasture pasture cloves dense plantation clove mixed agroforestry primary forest not modelled housing not modelled road not modelled slope no priors using geodata for updating flat flat slop smaller than 10 sloping sloping slope higher than 10 events no priors except that discussion were held under the assumption that there is no theft or cyclone no event shows the case of events cyclone theft water priors from questionnaire ok aggregated water situation including precipitation and irrigation systems lack protected area no prior except that discussion were held under the assumption that there is no protected area where farmers cultivate no strongly protected areas such as national parks or nature parks makira or masoala and community forests voi influence land use decision making given such an area the yes protected area at this location yes community forest voi at this location revenue questionnaire priors less than 50 sobika total amount of rice produces in one household measured in sobika 1 sobika approx 15 20 kg 30 48 l 50 100 sobika 100 600 sobika living costs questionnaire priors continuous discretized living costs including rice schooling medical expenses transportation and other expenses of daily needs reference yearly living costs 2 3 kapoaka rice day 1000 ariary value of rice per year 365000 ar 120 yearly income from other cash crops such as vanilla coffee questionnaire priors continuous discretized cash income from cash crops vanilla and coffee other than cloves reference local vanilla price in 2017 clove price farmer at farm gate questionnaire prior continuous discretized clove price per kg at local level farm gate in dollar reference clove price reached 20 000 ariary kg in 2017 approximately 5 6 5 9 clove price global market export price no data continuous discretized price at the exporter in madagascar in dollar per kg world no data over supply status of world clove market taking into account major producers importers and exporters a supply demand ratio reflects the potential push of the clove s market price over demand yield per stand questionnaire prior continuous discretized cloves harvested from one tree proxy to estimate total amount of cloves harvested are varying strongly an average seem to be 1 3 kg per tree clove income intermediate with equation continuous discretized equation price yieldperstand stands need for money intermediate with equation continuous discretized equation yearlyincome other cc cloveincome yearlyincome other livingcosts personshousehold rice self sufficiency intermediate with em learning continuous discretized comparing the number of persons with the rice revenue to estimate how many months a household may eat their own rice persons questionnaire prior continuous discretized number of persons in the same household supported by the same chef de ménage persons perhousehold questionnaire prior continuous discretized number of persons in the same household supported by the same chef de ménage stands questionnaire prior continuous discretized number of clove trees a household has tradition cultivation production no prior based on jinja shifting cultivation village based indicator depending on the location a different tradition applies based on horaka paddy rice cultivation based on tanimboly mixed agroforestry intention intermediate with em learning more rice paddy intention motivated by ajzen s theory of planned behavior ajzen 1991 more upland rice more pasture more pasture clove more afs more dense plantation clove 
26122,developing simulation and optimisation models for resource networks like water or energy systems increasingly involves integrating multiple data sources and software connecting multiple models and managing data accessed by different groups of analysts is a software challenge many resource systems are represented in computer models as networks of nodes and links driven by a range of objectives and rules we present a data storage platform written in python which exploits the commonality of network representations to store data for multiple model types within a single deployment this open source platform provides a common source of data to multiple models using consistent data formats reducing likelihood of error compared to file based data management when deployed as a web service it allows data to be shared securely among authorised users over the internet facilitating collaboration a case study describes the hosting of a water utility planning model with an accompanying worked example keywords model platform web services python open source software framework network modelling software availability program title hydra platform developer stephen knox contact address stephen knox manchester ac uk software access https www github com hydraplatform year first available 2015 hardware required windows 7 10 tested linux macosx program language python program size 116k availability and cost open source license lgpl for hydra base hydra server mit for clients 1 introduction recent advances in simulation and optimisation of environmental systems have relied on increasingly detailed models running many scenarios herman et al 2015 cluster and cloud computing also has enabled larger and more complex models to be used for increasingly sophisticated decision making analyses reed et al 2013 kasprzyk et al 2013 maier et al 2014 huskova et al 2016 eker and kwakkel 2018 in addition to increasing the data intensive nature of environmental modelling the sharing of data among collaborators and stakeholders has become common and will become increasingly important in future carr et al 2012 voinov et al 2016 van bruggen et al 2019 however translating data formats and managing data itself can be a barrier to effective and efficient modelling this can result in model developers spending more time transforming data than with the actual modelling and developing system insights multi disciplinary and remote collaboration is now a common requirement for environmental modelling and is often essential for environmental system modelling laniak et al 2013 manually managing multiple input and output files and synchronising among collaborators is inefficient and error prone as it requires continuous communication to ensure all parties remain up to date asynchronous collaboration on the same data or model is cumbersome or impossible manually centralising data storage where users control access to data eliminates this issue providing a suite of data management functions such as scenario management and history tracking through a multi user asynchronous system further reduces the likelihood of data becoming out of sync several approaches have been used in the last two decades to address the model data management problems below we present some approaches and identify some gaps in this area many existing modelling platforms e g commercially available decision support systems for water and energy planning use an all in one single user design where a software installed on a user s machine contains the user interface data storage and models all bundled together johnson et al 1995 kuczera 1997 ltd 2000 sieber et al 2005 modern web technologies can now provide a similar experience with several additional advantages some benefits of using a web framework over traditional desktop based software include 1 upgrades to the database code base and user interface ui can be done transparently across collaborators 2 if the models are made accessible via a web application programming interface api or user interface ui features can be added and upgraded transparently without the need to reinstall or reconfigure software 3 sharing data is a simple process as it requires relaxing access permissions on existing data 4 having a centralised data management system with a standardised api ensures consistent data formats allows for access control and allows model results to be more easily investigated and reproduced 5 scalability to large scale model runs can be simplified as the server can be upgraded or connected to a cloud service without user disruption 6 web user interfaces such as google docs google maps lucidchart etc have become common and are considered by some to match desktop based approaches for usability blau and caspi 2009 having data accessible through the web allows users to choose how they work with the data be it with such online tools or with desktop solutions use of web services for managing environmental data is well established buytaert et al 2012 with projects focussing on deploying models through web services kralisch and krause 2006 gregersen et al 2007 cetinkaya et al 2008 kralisch et al 2012 peckham et al 2013 standardisation of time series data ames et al 2009 management of large quantities of data vitolo et al 2015 folk et al 2011 rew and davis 1990 coupling of models at runtime gregersen et al 2007 and management of data for specific model runs knox et al 2014 meier et al 2014 zander and kralisch 2011 there are few enforced general standards or requirements for environmental data which means there is no single solution to data management many projects specialise in one area be it serving large quantities of time series data ames et al 2009 focussing on supporting water resource modelling chalh et al 2015 or standardising model run time data communication a focus of recent research has been facilitating model sharing and result publishing to better ensure model reproducibility tarboton et al 2013 by exposing a model platform through a web service it is possible to make particular model runs and result sets public either by relaxing access permissions on data in the system or by exporting the data to public repositories such as hydroshare using this approach transparency is ensured as the public data is exactly what is used by the model mckinney and cai 2002 presented an object oriented approach to data management of data for network based models within a gis geographic information system environment this used a generalised node link structure for networks where model specific subclasses could be created this allowed the structure to be applied to multiple network types harou et al 2010 describe a model platform as a generic software which separates models from data management and visualisation such software stores data in one location and exports data for use by different models model platforms build on the idea of heterogeneous data collection and model access by integrating data from multiple sources and models into a single platform model developers can read data from different sources for use in multiple models a model platform need not itself perform any modelling tasks or provide ability to build models instead a model platform combines several data sources aided by a plug in architecture and tools to aggregate heterogeneous interfaces with the ability to run or export data to various models from the same source model platforms facilitate model integration through loose model coupling jiang et al 2017 goodall et al 2013 loose coupling allows models to communicate through a third party to store intermediate data in a standardised way eliminating the need for direct transfers between models which can require custom unscalable solutions kelly et al 2013 this approach introduces an overhead and is more suitable to model chaining meier et al 2014 where models run in their entirety in a given order in contrast to a solution like pynsim or openmi knox et al 2018b gregersen et al 2007 which allows the models to be connected on a per timestep basis a centralised data store relies on the structure of environmental modelling data being predictable enough to store data generally and the system being flexible enough to deal with the idiosyncrasies and exceptions of individual systems and model requirements in addition it relies on a common understanding of how data is represented many environmental management models use networks of nodes and links to represent systems and use largely predictable data formats such as timeseries arrays scalars despite advances described above the ability to easily connect different models to the same data manager share and collaborate on model runs and make data public is still needed with no current technologies having yet made these tasks easy the model platform concept goes some way to achieving this by separating data management from the models themselves in this paper we add web services which enable the collaborative aspects of model development and analysis we present a solution for storing network structures and associated datasets in a general way with the addition of version management multi user sharing including data ownership control the presented platform manages network based data to support modelling tasks such as uploading new inputs data managing scenarios and extracting results while data is stored in a general way this is not a single public data repository rather it is a software platform that can be used with specific models and does not enforce metadata rules to enable searching of datasets for example this platform supports a modeller who has compiled the necessary data from the appropriate sources and built a network based model it allows the modeller to store manage and share the inputs and outputs of their model with other users support for scenario analysis and data sharing gives the user the possibility to compare results and collaborate with other users we call this system hydra platform the rest of this paper is structured as follows section 2 describes the design of hydra platform section 3 outlines a case study which illustrates a real world deployment of hydra platform the appendix has a worked example of a water energy simulation model and two further partially implemented case studies 2 design hydra platform referred to here on as hydra for brevity is a model platform and aims to provide data support for network based models the software stores inputs and outputs for models which use a network structure including data metadata and model topology models which use network structure include for example the simulation or optimisation of water energy transport and trading systems networks are not the only way to represent a modelled domain some models for example are raster based hydrologic models marsh et al 2018 and would not be suitable for a network based system 2 1 principle hydra is designed to provide a standardised means to store and access data for network based modelling and to allow data from multiple models model types and modelling disciplines to be stored and accessed from the same place to achieve this hydra has two main design principles decouple data storage and management from application logic and provide support for a wide range of applications to achieve the first an architecture is required which contains minimal application logic and is flexible enough to store different types of data to achieve the second a programming interface api is required which links to the data storage and provides logical and standardised protocols for accessing data these are often competing requirements supporting a wide range of applications requires a design which understands the scope of possible applications while maximising its utility within that scope in the case of hydra this is network based resource modelling within this broad application area hydra should be useable by any modeller from different disciplines equally it should be designed from the outset to not favour any one this principle relies on a generalised design where application specific content can be entered as data rather than requiring changes to hydra s source code or architecture 2 2 methodology hydra uses a high level representation of networks to support storing a range of network types and employs a templating system which mimics object oriented programming by allowing users to define user configurable network types and then create instances of those types using this approach a user can define types of nodes and links with properties relevant to specific modelling problems instances of those nodes and link classes along with their datasets are the inputs for modelling a range of dataset formats are supported along with ability to define custom data formats for non standard values the customizability of the system facilitates integration with a range of models so long as they use a network nodes and links structure hydra is a software library written in python that provides an application programming interface api the api contains high level operations to create and edit networks manage scenarios and share networks with other users the user has no direct interaction with the database i e they must use the api to perform queries on the database this separation of the logical layer from the storage allows the database implementation to be upgraded or altered while data operations remain unchanged a web api an additional python library connects to the core python library to allow remote data management using a web api allows multiple users to use and access the same data limiting duplication in collaborative modelling remote access through a web api also allows software developers to build applications in any mainstream programming language separating database from logical operations and providing a web api all serve to broaden the application areas in which hydra can operate from a simple desktop installation to a large scale cloud enabled service fig 1 gives an overview of the architecture 2 3 usage four use cases for interaction with hydra include 1 importing data from a 3rd party file format for example output of a model run 2 exporting data to a 3rd party file format for example for use in a model run 3 running a model with data stored in hydra this involves exporting data to the appropriate file format running a model instance then importing results back to hydra 4 using a user interface i e a graphical front end so non developers can interact with the data all use cases require software that interact with hydra a client application or client for brevity is a software application that makes requests to an api clients are not part of hydra but independent software packages which provide functionality such as the use cases described above this section describes how clients interact with hydra to store and access data the generalised data storage design and how it enables accessing diverse network types and collaborative features a client application requests data from or sends data to hydra by interacting with the api for example api functions get network or update network allow a client to retrieve and modify a network respectively a client application may interact with the api to read write update or delete data from hydra users of hydra can write their own custom client applications provided they interact with the api appropriately hydra has its own data format which the client api must use this format closely reflects the database schema structure the data structures can be explored in the examples on the hydra base github repository knox et al 2018a or in the json client application knox 2018 2 3 1 data import export data import and export io relates to data in any format which can be stored in hydra this category includes data not necessarily linked to any one modelling system or method but which can be communicated these might include network topology contained in shapefiles time series data such as that available from the cuahsi his ames et al 2009 and encoded as waterml valentine et al 2012 data stored in microsoft excel files csv files etc 2 3 2 model clients model clients allow communication with specific models or modelling frameworks for example such as matlab gams rosenthal 2008 pyomo these require input and output in specific data formats a model client can also include the function to run the model itself in which case the client algorithm would be 1 export the data to file input modelformat 2 run a model instance using input modelformat 3 once complete import results from output modelformat the process of import and export are identical to a data import export client a user could use the export and import clients in isolation running the model manually but the run client allows this step to be automated 2 3 3 user interfaces hydra is a web server api and so does not have a graphical interface a user interface ui is a client application which allows a user to visually edit network topology data initiate model runs share results etc 2 4 architecture hydra has two components hydra base and hydra server hydra base is a python package library and performs all computational tasks such as database interaction and validation hydra base can be installed as a standard python package and used as such with no requirement to act as a web server hydra server is an additional python package which exposes the functions within hydra base as remote procedure calls rpcs this separation of the base from the server allows for flexible deployment with the same instance being accessible remotely locally or both depending on the requirement of the application figs 2 and 3 demonstrate the different ways hydra can be deployed where clients applications interact through a web service or directly with the python api respectively 2 4 1 hydra object types terminology the following terms refer to concepts within hydra this nomenclature is reflected in the database structure and api and will be referred to throughout this paper capitalisation of these terms later in the paper indicates that they relate to this list a project is a container for networks a project can contain multiple networks and have multiple owners each with configurable levels of access a network describes a topology and contains nodes links and groups nodes links form the topology of a network and can have data associated with them through attributes these can have types like reservoir or water diversion for nodes or river or pipe line for links groups are containers of nodes links and other groups these can be used to define interdependencies among nodes links or to construct a non physical entity such as a governance hierarchy an attribute is defined by a name and dimension for example water flow volumetric flow rate to illustrate consider two attributes water flow volumetric flow rate and storage volume node a might have a water flow associated with it while node b might have both water flow and storage a dataset is an individual data value it is defined by a name data type unit value and metadata when associated with an attribute the unit of the dataset must match the dimension of the attribute it refers to hydra supports multiple datasets for the same attribute this is done through scenarios a network can contain multiple scenarios each of which contains mappings between an attribute and a dataset as hydra can support data for multiple models it uses a template to define the types of nodes links and groups required for each model a template type is defined by a name e g reservoir and a set of attributes e g storage max head etc this can be thought of as the equivalent of a class in object oriented programming oop a node or link within a network will be an instance of a template type 2 5 scenarios data a scenario in hydra represents data associated with a network scenarios allow attributes of nodes links and groups in a network to have multiple datasets associated to them at once any node link or group attribute can be connected to a dataset through a scenario a reference table in the database allows multiple attributes to be connected to the same dataset to ensure datasets are not duplicated datasets are stored independently of networks in hydra this allows the system to store and manage datasets without the need for network topology this is useful for example when there is a need to import time series from cuahsi his ames et al 2009 or other data without having a network this also avoids the duplication of data 2 5 1 units hydra manages units and dimensions by providing a default set including length area volume density etc with a set of default units for each dimension new dimensions and units can be added on a per user basis functions such as add unit and get all dimensions allow units to be managed and searched units are not required for datasets but it is encouraged to specify a unit if a dataset has no dimension the unit dimensionless is used hydra can convert units on datasets the get dataset function call has an optional unit argument which will return a dataset object with the requested dataset converted to the specified unit an error is thrown if units are not of the same dimension 2 6 client interaction hydra is object oriented and uses objects for network and data management and this is reflected in the web api which is also object based that is the user sends receives objects to from the web server or the native python library in the same way the web api is a wrapper for the functions within hydra base these functions have the same names and accept the same arguments and pass these arguments into the equivalent function within hydra base the api is implemented as a json remote procedure call protocol rpc using the spyne python library spyne allows for validation of argument types and definition of the object types which the api can accept spyne also allows the same api to be deployed for several protocols at once for example soap and json this provides flexibility for different client requirements for example using soap simple object access protocol with c net can automatically build classes and perform validation within the client an example of how the web api wraps hydra functions can be found in listing 2 appendix a 4 there are two ways in which clients can interact with hydra local or remote hydra provides client libraries for python java and c the python implementation is the most complete and provides two classes which enable interactions jsonconnection and remotejsonconnection as the names imply these classes allow interaction with hydra using json based data structures either directly with the python library or using http requests the data sent and received from each connection type and the function calls are identical meaning the client code would require little or no alteration when using the different connection types listing 1 in appendix a 4 presents an example of an interaction with hydra using a remote connection with the python client library 2 7 relational database hydra uses a relational database for data storage with sqlite mysql and postresql so far tested the python library sqlalchemy is used for concurrency and transaction management upgrades queries and allows support of multiple database types sqlalchemy is a popular open source well supported library proven to support the security concurrency and scalability requirements of large scale web applications the summary database schema appears in fig 4 and the full version appears in fig 11 in the appendix 2 8 templates hydra is a generalised data management platform which can support multiple types of networks concurrently for example multiple users might work on different projects using different models written in different languages without knowledge of any other supporting this means allowing users to define custom node link and group types which relate to their model for example user a s model deals with supply and demand nodes while user b deals with reservoir desalination and treatment nodes hydra achieves this with templates a template allows a user to create definitions of the node link types required by their model including the attributes which each type requires see figs 5 7 templates allow hydra to support multiple different types of networks within the same system supporting multiple types of model concurrently in addition to defining the node link types required by a model hydra supports data validation in the template api these are implemented as restrictions on the attributes of a type and can include enumerations e g the value must be 10 20 or 30 data ranges the value must be between 10 and 30 date ranges e g the time series must be between january 2018 and december 2018 2 9 collaboration and security hydra supports user and data management by 1 global management of users roles and permissions and 2 management of entities by their owner hydra uses a user role permission structure to define what a user can and cannot do this is done by assigning a user a role which has some permissions such as add network add user in these examples an attempt to call the add network or add user function by a user without these permissions will result in a permission error using this system a user can be given general permissions across the whole platform but may still be restricted on a case by case basis in addition to global permission management hydra supports the sharing and management of individual projects networks scenarios and datasets the user who creates a project network or dataset is deemed its owner and has control to delete hide or share it here we describe how each can be managed 1 projects a user can share a project with other users through the share project function call there are three modes of sharing view edit and share these flags indicate whether the receiving user can view edit or re share the project once shared the receiving user can see all networks within the shared project 2 networks like projects networks can be shared with other users in this case the user will be able to see the project containing the shared network but any unshared networks within the same project will not be visible the same view edit share rules apply to the network 3 datasets datasets are by default independent of any network project or scenario datasets are then linked to networks through scenarios as explained in section 2 5 to reduce duplication an important aspect of data management is the ability to store confidential data to this end hydra allows datasets to be hidden such that only the owner can see them the owner can then share these datasets with other users while keeping them hidden from other users hidden dataset are stored in the same way to other dataset their access is controlled by the hidden flag in the tdataset table which when set to y checks the tdatasetowner table this table is a whitelist of users who have access to this dataset security is important in multi user systems which convey data hydra employs well established security protocols such as encrypting passwords using the bcrypt hashing function in hydra multiple users may share potentially sensitive data with other users hydra s security protocols are designed to ensure only users with the appropriate privileges may view and edit data a series of unit tests available on github were created to ensure sharing functions operate as expected and these are tested automatically whenever changes are made to hydra s code security features such as secure web connections https and altering hydra s default password hashing key are the responsibility of each deployment 3 case study hydra is operational and has been used in regional scale modelling efforts within the uk we present a case study to illustrate an actual deployment in an industrial setting where a consultancy manages large numbers of model runs and scenario analyses for a utility company and where efficient sharing of data was necessary hydra enabled efficient sharing of data and simplified the modelling workflow a common modelling workflow in an industrial setting involves a water institution such as a water utility or river basin agency hiring a consultant to build a model to analyse for instance the levels of service impacts of a new infrastructure asset or management policy the model used in this project is the economic balance of supply and demand ebsd model padula et al 2013 a least cost water supply planning optimisation model used by uk water companies for infrastructure investment planning this model is used to minimise investment costs over a 25 year planning horizon subject to maintaining supply demand balance at a particular level of service the problem is formulated as a mixed integer linear programming milp with binary decision variables for each feasible investment option in each year the solution is subject to constraints that ensure the supply demand balance is satisfied in each water resource zone planning area and investment dependencies or exclusivities are enforced in our case the ebsd model is written in gams rosenthal 2008 a scripting language for mathematical programming optimisation models the model requires a single text input file containing the input network topology and scenario data for a large water utility with many feasible options solving the milp is computationally expensive a run containing all possible options can take several hours as per regulatory requirements the water utility supplies both supply and demand data this is generated independently prior to and during the modelling process supply data includes feasible infrastructure options including their location size and outputs demand data is derived from for example population forecasts and analysis of historic per capita consumption prior projects with this model involved the utility company requesting the consultant to run the model located on their pc then waiting for the results to be returned via email in microsoft excel format reporting was done by the consultant using powerpoint this process proved slow and cumbersome limiting the amount of iteration and refinement of the modelling and ultimately reducing business intelligence on different investment strategies as regulatory requirements change uk water utilities are expected to perform more analysis model runs with more input scenarios and sensitivity analysis the traditional workflow described gave the utility little ability to perform such detailed analysis as running models and accessing results was too time consuming hydra gave the utility more autonomy to perform model runs themselves and for reporting and data analysis to be shared between consultant and utility and potentially with regulators the original ebsd model was expanded to employ the modelling to generate alternatives mga technique chang et al 1983 brill et al 1990 which relaxes the least cost requirement of the model to return the n least cost alternatives within a specified threshold for example within 2 of least cost this enabled the water utility to explore a range of results which may perform better in criteria other than cost such as environmental impact resilience to drought etc this additional requirement caused each model run to be significantly longer ranging from the original 15 min run time to several hours running such models on a local workstation was infeasible and for efficiency multiple runs should be configured and deployed at the same time in addition to the mga addition the utility required that constraints be configurable such as pre requisites must build x before y and mutually exclusivities can only build x or y but not both to implement these constraints hydra s groups were used where nodes were grouped by constraint type the groups were then exported to the model as constraints using the client application the additional requirements of this project compared to prior ebsd projects were as follows make data and model accessible online to enable collaborative modelling allow the water utility to upload data and run scenarios independently configure the mga model runs e g the number of optimised configurations to save allow mga results to be explored and downloaded for analysis allow multiple model runs to be launched concurrently make constraints user configurable the software facilitated and enabled the following project roles 1 the water utility seeking to run the model and who must present results to the regulator 2 an analyst either in the water company or a consultant who performs the actual model runs and analyses the results 3 the developer of the model code and specialist in ebsd and mga provides support to the analyst in interpreting results and expanding the model 4 the developer of the ui hydra and technical support after initial development has little interaction with the project for this project hydra was installed on an amazon ec2 instance along with the database gams client application and the gams model file a user interface for hydra was deployed on an amazon ec2 instance connected to hydra import and export functions were developed in the ui to allow uploading and downloading of model data in the formats required by the water utility the analyst received input data from multiple sources for existing and proposed infrastructure the network topology weather forecasts cost information and demand forecasts a custom excel client application compiled these sources into a single hydra compatible input json string which was uploaded to hydra using the add network function through the ui the analyst ran the model cloned the scenarios to tweak input data such as interest rates or constraints and re ran the model storing outputs in different scenarios once model runs were complete the consultant was given access to the network using share network the consultant and utility analysts then performed analyses on the model results before presenting them to the regulator by using hydra in this instance a transparent modelling workflow was enabled with input data and modelling results available to both consultants and utility analysts via a web browser analysts could consolidate input and output data and share results within a single platform enabled by having a single shared place to select and refine data and manage model scenarios the translation tool required for compatibility with gams is open source gpl3 and available on github knox et al 2011 table 1 lists requirements for the project and the solution to those requirements provided either by hydra directly or through web services generally the hydra features highlighted in this case study are demonstrated in a worked example in appendix a 1 the example illustrates how a model can be uploaded run and shared through hydra the full code for this example is on github the worked example also demonstrates features used in the two supplementary case studies described in appendices a 2 and a 3 these proposed case studies differ from this one by focussing on collaborative model development in a research setting and managing data privacy in an industrial setting respectively 4 discussion this paper has presented hydra platform also referred to as hydra for short a data management software system which helps modellers collaboratively manage share and analyse input and output data to network models as models and modelling efforts expand in scope and importance within resource system decision making processes there is a growing need to share data and collaborate on models which require contributions from multiple people in different physical locations the old paradigm of having the model on a user s local machine and using a file naming system spreadsheets and email for data management is often too limiting to ensure reproducibility and consistency amongst collaborators hydra addresses data management and sharing issues by using a web server which contains functions for storing network structures in a generic way this allows the storage of multiple types of networks while remaining within the well defined sphere of network simulation and optimisation by focusing on network based models there is enough flexibility to support a cross section of environmental modelling while being focussed enough to provide a meaningful and enabling platform for these models the web server provides several features for creating and editing data while supporting multiple degrees of user control these range from limiting user roles user a cannot create networks to a user being able to control access to specific aspects of their work user a can see my network but not edit it data within hydra are by default only visible to the user who creates it but a user can change this setting by using the same data for model input and using the same containers to store model outputs there is consistency amongst users which could improve understanding and reproducibility of results the use of scenarios allows users to create multiple versions of a network thus allowing different inputs and associated outputs to be compared and to avoid conflicts as multiple types of network can be stored in the same software system and shared among users hydra supports model integration this can be achieved by linking two networks together for example an energy network and a water network having run one model to completion and stored the results these results can be transferred to the other network and used as input to a second model this process is termed loose coupling and is supported by hydra with its attribute linking process which allows a node of one network to be connected to that of another as outputs of one are immediately available as an input to the next this approach reduce both the time needed to run an integrated model and errors in data transfer our approach to facilitating this kind of model integration is use of a common generalised data store combined with translation tools these tools create the appropriate input formats for each model and stores their outputs for further use or analysis while this requires the researcher to translate data to and from the format of this centralised system this enables them to make their tools available to others reducing this burden on future modellers currently publicly available tools exist to translate network data to and from the following formats and modelling tools gams pyomo waterml via cuahsi his microsoft excel csv and gis shapefiles to demonstrate that hydra is a functioning system we presented a case study of its use in an industrial setting the case study describes an existing deployment of hydra within a water utility hydra reduces reliance on model developers performing the model runs and allows the water utility to run the model themselves this case study illustrates how the model itself can be deployed remotely connecting to hydra and managed through a custom user interface two additional proposed case studies are presented in the appendix with example code showing how they can be achieved early hydra projects have pointed to needed future developments these include lowering the barrier to entry for the use of hydra by non software developers allowing hydra to handle the storage of large datasets multiple gb and to improve the accessibility and quality documentation comments etc of existing client applications as the number of deployments of hydra increases we will continue to develop its collaboration and security features for example by allowing encryption of datasets by individual users to further ensure sensitive data is protected to encourage adoption of hydra as a solution to data management needs it has become clear that a ui would be helpful we are building on the work presented here to create a generalised ui for managing networks and data the ui will feature the ability to create network types and build networks manually using these types the ui will also feature the ability to run models and activate client applications to integrate input and output of data more than one ui using hydra may emerge the data required to run some models and the results produced by large models can become too large to store in hydra in the order of gb per dataset in order to accommodate the potentially huge quantities of data we are investigating using nosql approaches and big data techniques for accessing huge datasets hydra then will be used to store references to dataset locations which reside externally as with the ui a key to the adoption of hydra is the availability of a range of pre existing client applications where users can quickly get data into and out of the system with minimal effort we intend to continue developing the existing client applications for gams matlab excel as well as documenting their development to reduce barriers to entry for prospective client developers hydra does not contain the facility to directly make a dataset or network publicly available this is an important improvement to support publications and to encourage scientific reproducibility to this end we are investigating the development of a connection to hydro share tarboton et al 2013 an online repository of data and models for water resources and other fields all tools presented here are open source and available on github we will continue developing documentation examples tests of hydra and its applications with the goal of encouraging adoption 5 conclusions the environmental systems models being developed are increasingly complex and there is an increasing need to manage data for such models to facilitate collaboration and sharing across users and locations reduce errors and promote scientific reproducibility and transparency we have presented hydra platform hydra a web based data management platform specialising in storing network based data some features of hydra include open source python library with an extension enabling it to be deployed as a web service client libraries to simplify interaction with local and remote instances of hydra networks of multiple types can be stored in the same instance of hydra allowing it to serve multiple model types concurrently so long as all models use a node link structure flexible data support built in support for data types data frames timeseries arrays scalars and free text expandable to support other types client applications connect to hydra to translate the data into the formats required by a modelling framework and to send back results of model runs includes a user permission system and supports data ownership this enables data confidentiality while providing the ability to share data amongst users multiple modellers can access the same inputs or model results simplifying collaboration and minimising the likelihood of the errors common in traditional data sharing the case study described a successful deployment of hydra in an industrial setting and highlighted some features of hydra such as its facility for sharing data between users running models through client applications and management of confidential data in future work we have proposed development of a user interface to simplify data interaction an approach to managing big data and a way to publish input data and model results acknowledgements support for the software development was partially funded by the uk government through innovate uk file reference number 101338 the uk economic and social research council esrc provided support through the futuredams project es p011373 1 any opinions findings and conclusions or recommendations expressed in this material are those of the authors but not necessarily the funders appendix this appendix contains a worked example which provides a compact demonstration of how the case study and two proposed case studies can be implemented with code the worked example shows an application to an energy water system simulator the appendix also contains two further case studies which are partially designed and implemented showing further examples of hydra s use which 1 supports collaborative development of a model across academic institutions appendix a 2 and 2 allows secure control of data sharing within a large industrial project appendix a 3 finally two small snippets of code are presented which demonstrate how to perform a basic hydra task of updating an existing network and an illustration of the hydra server design a 1 worked example the following example shows a step by step process for creating and importing a network running a model sharing results while keeping input data private and inspecting results visually the demonstration involves simple scripts which perform actions within hydra such as creating projects retrieving network details and so on the reader is free and encouraged to amend these scripts it is important to note that these scripts are for illustrative purposes only and as such are kept as simple as possible they are not designed to provide a full comprehensive example of how a client application should be built for hydra for example almost no error checking is performed this example uses a modelling tool called pywr a python based simulation system a client application called the hydra pywr app is used to load network data into hydra to run the model and to store results to ensure maximum compatibility of this demo the app is run through pipenv a virtual environment system used in python the model in question simulates coupled water and energy systems and is applied to a small synthetic resource system network the simulation model includes different water demands irrigation water supply and hydropower and energy demands energy demands are supplied by hydropower plants and conventional thermal power plants a simple transmission system connects the different energy supplies to the energy demands we note that this energy system model does not include any dispatch constraints the model schematic is show in fig 8 it is shown in a web based front end interface for hydra which is currently in development the formulation of the model itself is beyond the purposes of this example and has been submitted for publication data workflow is our focus here a more detailed description of this model and the example models are available at the same location on github the outputs of the code in this example have been shortened for clarity the code contained in the scripts shown below can be found at https github com umwrg hydra pywr demos a 2 proposed case study 1 this proposed case study case was designed to manage the difficulties that can arise when working collaboratively on multiple models or multiple instances of input data consider a model being developed by two researchers in different organisations for example two separate model formulations are developed of a water system each model capable of being run with the same input data but with differing outputs this project requires keeping track of multiple evolving models and input data files to minimise the disorder of multiple researchers developing input data and analysing output data of the different models hydra is used to store the data centrally using the project network structure for the separate model formulations and its scenario management capabilities for parameterising data in this application the model is an optimisation driven water resources simulation written in the generic algebraic modelling system gams a scripting language for optimisation models different variations formulations of the optimisation drive simulation model are created and compared the gams models require a single text input file containing the network topology and scenario data the goal is to study the differences in results resulting from different formulations the models share most data but some parameters are particular to each optimisation model formulation to manage the inputs to each different model and the many possible outputs the two researchers user a and user b deploy an instance of hydra on an amazon ec2 server using a mysql database and an apache web server user a first puts the network and the baseline scenario into an excel file in the format required by the excel client application knox and mohamed 2018 she then uses an excel client application to upload the file to hydra user a then shares the network with user b granting full edit access using the share network function both users can then clone the baseline scenario into for example climate change within the network using the clone scenario function in this case the input data is identical but the scenario can be used to store different outputs once the respective models have been run this allows them to manage different instances of data relating to the same network should they wish to work on fully separate networks or even projects the clone network function could also be used to run the model a gams client application is used which calls get network to retrieve the network and scenario in question this is returned as a json string which is then converted by the client into input txt ready to be imported in the gams code upon completion the output output gdx file is read by the client converted back into the hydra friendly json format and update network is called with the new result values included following the same procedure for each model results in three scenarios stored against the network which can be compared stored for later and shared with user b which in turn can re run the model with new input data the translation tool required for compatibility with gams is open source gpl2 and available on github knox et al 2011 a 3 proposed case study 2 regional infrastructure planning increasingly encourages watercompanies to build regional inter utility models to explore the use of common assets and transfers to cut costs and increase reliability matrosov et al 2013 zeff et al 2014 multiple data sources and need for transparency while maintaining confidentiality of cost data a regulatory requirement can become difficult without dedicated management of the data this proposed case study presents an example of how hydra could be used to improve data sharing and management in this project the stakeholders engage a consultant for modelling an analyst within the consultancy runs the model the model in this proposed case study is a single regional scale simulation written using the python water resource simulator pywr this model accepts a single json input file containing topology and scenario data data on supplies demands and costs is compiled by each water company each water company may view their own data and overall results but cost data from other utilities must be hidden from them given the uk has regulated private utilities hydra can consolidate all the data so it can be input to the single regional model the model run itself is done by a trusted third party analyst who has permission to view all datasets but each water company is only able to see its own see figs 9 and 10 to allow controlled data access to all parties hydra is deployed on an amazon ec2 instance which also hosts the pywr software and a custom ui this ensures the model can be run from any location removing dependency on specific computers or institutions an analyst in the consultancy compiles the data from multiple sources into a json file ensuring that sensitive datasets have a hidden flag this ensures nobody except the analyst can see the datasets using a json client application the network is uploaded to the server next the sensitive datasets are shared with the appropriate user groups followed by the network being shared with all parties when accessing the network each stakeholder can see all non hidden datasets as well as the hidden datasets to which it has been granted access a 4 examples listing 1 shows pseudocode to create a remotejsonconnection to enable a client to communicate with a remote installation of hydra server using a jsonconnection the client would attempt to connect to a local instance of hydra base a client application developer can easily switch between a local and remote connection with both connections interacting in an identical fashion this gives the developer flexibility for testing or to allow the client to interact with different end points with little or no changes to the client side code the full operational code is available on github knox 2018 other examples can be found in the hydra base repository located on github knox et al 2018a listing 2 demonstrates using pseudocode how the web api works the spyne library exposes the get network function as a remote procedure call rpc accepting an integer as an argument it then calls the get network function in hydra base returning a network object in this example the network object comes from a class defined by the web api which ensures the incoming and outgoing structures are valid for every function in hydra base which should be exposed there is an identical function in the server with this workflow 
26122,developing simulation and optimisation models for resource networks like water or energy systems increasingly involves integrating multiple data sources and software connecting multiple models and managing data accessed by different groups of analysts is a software challenge many resource systems are represented in computer models as networks of nodes and links driven by a range of objectives and rules we present a data storage platform written in python which exploits the commonality of network representations to store data for multiple model types within a single deployment this open source platform provides a common source of data to multiple models using consistent data formats reducing likelihood of error compared to file based data management when deployed as a web service it allows data to be shared securely among authorised users over the internet facilitating collaboration a case study describes the hosting of a water utility planning model with an accompanying worked example keywords model platform web services python open source software framework network modelling software availability program title hydra platform developer stephen knox contact address stephen knox manchester ac uk software access https www github com hydraplatform year first available 2015 hardware required windows 7 10 tested linux macosx program language python program size 116k availability and cost open source license lgpl for hydra base hydra server mit for clients 1 introduction recent advances in simulation and optimisation of environmental systems have relied on increasingly detailed models running many scenarios herman et al 2015 cluster and cloud computing also has enabled larger and more complex models to be used for increasingly sophisticated decision making analyses reed et al 2013 kasprzyk et al 2013 maier et al 2014 huskova et al 2016 eker and kwakkel 2018 in addition to increasing the data intensive nature of environmental modelling the sharing of data among collaborators and stakeholders has become common and will become increasingly important in future carr et al 2012 voinov et al 2016 van bruggen et al 2019 however translating data formats and managing data itself can be a barrier to effective and efficient modelling this can result in model developers spending more time transforming data than with the actual modelling and developing system insights multi disciplinary and remote collaboration is now a common requirement for environmental modelling and is often essential for environmental system modelling laniak et al 2013 manually managing multiple input and output files and synchronising among collaborators is inefficient and error prone as it requires continuous communication to ensure all parties remain up to date asynchronous collaboration on the same data or model is cumbersome or impossible manually centralising data storage where users control access to data eliminates this issue providing a suite of data management functions such as scenario management and history tracking through a multi user asynchronous system further reduces the likelihood of data becoming out of sync several approaches have been used in the last two decades to address the model data management problems below we present some approaches and identify some gaps in this area many existing modelling platforms e g commercially available decision support systems for water and energy planning use an all in one single user design where a software installed on a user s machine contains the user interface data storage and models all bundled together johnson et al 1995 kuczera 1997 ltd 2000 sieber et al 2005 modern web technologies can now provide a similar experience with several additional advantages some benefits of using a web framework over traditional desktop based software include 1 upgrades to the database code base and user interface ui can be done transparently across collaborators 2 if the models are made accessible via a web application programming interface api or user interface ui features can be added and upgraded transparently without the need to reinstall or reconfigure software 3 sharing data is a simple process as it requires relaxing access permissions on existing data 4 having a centralised data management system with a standardised api ensures consistent data formats allows for access control and allows model results to be more easily investigated and reproduced 5 scalability to large scale model runs can be simplified as the server can be upgraded or connected to a cloud service without user disruption 6 web user interfaces such as google docs google maps lucidchart etc have become common and are considered by some to match desktop based approaches for usability blau and caspi 2009 having data accessible through the web allows users to choose how they work with the data be it with such online tools or with desktop solutions use of web services for managing environmental data is well established buytaert et al 2012 with projects focussing on deploying models through web services kralisch and krause 2006 gregersen et al 2007 cetinkaya et al 2008 kralisch et al 2012 peckham et al 2013 standardisation of time series data ames et al 2009 management of large quantities of data vitolo et al 2015 folk et al 2011 rew and davis 1990 coupling of models at runtime gregersen et al 2007 and management of data for specific model runs knox et al 2014 meier et al 2014 zander and kralisch 2011 there are few enforced general standards or requirements for environmental data which means there is no single solution to data management many projects specialise in one area be it serving large quantities of time series data ames et al 2009 focussing on supporting water resource modelling chalh et al 2015 or standardising model run time data communication a focus of recent research has been facilitating model sharing and result publishing to better ensure model reproducibility tarboton et al 2013 by exposing a model platform through a web service it is possible to make particular model runs and result sets public either by relaxing access permissions on data in the system or by exporting the data to public repositories such as hydroshare using this approach transparency is ensured as the public data is exactly what is used by the model mckinney and cai 2002 presented an object oriented approach to data management of data for network based models within a gis geographic information system environment this used a generalised node link structure for networks where model specific subclasses could be created this allowed the structure to be applied to multiple network types harou et al 2010 describe a model platform as a generic software which separates models from data management and visualisation such software stores data in one location and exports data for use by different models model platforms build on the idea of heterogeneous data collection and model access by integrating data from multiple sources and models into a single platform model developers can read data from different sources for use in multiple models a model platform need not itself perform any modelling tasks or provide ability to build models instead a model platform combines several data sources aided by a plug in architecture and tools to aggregate heterogeneous interfaces with the ability to run or export data to various models from the same source model platforms facilitate model integration through loose model coupling jiang et al 2017 goodall et al 2013 loose coupling allows models to communicate through a third party to store intermediate data in a standardised way eliminating the need for direct transfers between models which can require custom unscalable solutions kelly et al 2013 this approach introduces an overhead and is more suitable to model chaining meier et al 2014 where models run in their entirety in a given order in contrast to a solution like pynsim or openmi knox et al 2018b gregersen et al 2007 which allows the models to be connected on a per timestep basis a centralised data store relies on the structure of environmental modelling data being predictable enough to store data generally and the system being flexible enough to deal with the idiosyncrasies and exceptions of individual systems and model requirements in addition it relies on a common understanding of how data is represented many environmental management models use networks of nodes and links to represent systems and use largely predictable data formats such as timeseries arrays scalars despite advances described above the ability to easily connect different models to the same data manager share and collaborate on model runs and make data public is still needed with no current technologies having yet made these tasks easy the model platform concept goes some way to achieving this by separating data management from the models themselves in this paper we add web services which enable the collaborative aspects of model development and analysis we present a solution for storing network structures and associated datasets in a general way with the addition of version management multi user sharing including data ownership control the presented platform manages network based data to support modelling tasks such as uploading new inputs data managing scenarios and extracting results while data is stored in a general way this is not a single public data repository rather it is a software platform that can be used with specific models and does not enforce metadata rules to enable searching of datasets for example this platform supports a modeller who has compiled the necessary data from the appropriate sources and built a network based model it allows the modeller to store manage and share the inputs and outputs of their model with other users support for scenario analysis and data sharing gives the user the possibility to compare results and collaborate with other users we call this system hydra platform the rest of this paper is structured as follows section 2 describes the design of hydra platform section 3 outlines a case study which illustrates a real world deployment of hydra platform the appendix has a worked example of a water energy simulation model and two further partially implemented case studies 2 design hydra platform referred to here on as hydra for brevity is a model platform and aims to provide data support for network based models the software stores inputs and outputs for models which use a network structure including data metadata and model topology models which use network structure include for example the simulation or optimisation of water energy transport and trading systems networks are not the only way to represent a modelled domain some models for example are raster based hydrologic models marsh et al 2018 and would not be suitable for a network based system 2 1 principle hydra is designed to provide a standardised means to store and access data for network based modelling and to allow data from multiple models model types and modelling disciplines to be stored and accessed from the same place to achieve this hydra has two main design principles decouple data storage and management from application logic and provide support for a wide range of applications to achieve the first an architecture is required which contains minimal application logic and is flexible enough to store different types of data to achieve the second a programming interface api is required which links to the data storage and provides logical and standardised protocols for accessing data these are often competing requirements supporting a wide range of applications requires a design which understands the scope of possible applications while maximising its utility within that scope in the case of hydra this is network based resource modelling within this broad application area hydra should be useable by any modeller from different disciplines equally it should be designed from the outset to not favour any one this principle relies on a generalised design where application specific content can be entered as data rather than requiring changes to hydra s source code or architecture 2 2 methodology hydra uses a high level representation of networks to support storing a range of network types and employs a templating system which mimics object oriented programming by allowing users to define user configurable network types and then create instances of those types using this approach a user can define types of nodes and links with properties relevant to specific modelling problems instances of those nodes and link classes along with their datasets are the inputs for modelling a range of dataset formats are supported along with ability to define custom data formats for non standard values the customizability of the system facilitates integration with a range of models so long as they use a network nodes and links structure hydra is a software library written in python that provides an application programming interface api the api contains high level operations to create and edit networks manage scenarios and share networks with other users the user has no direct interaction with the database i e they must use the api to perform queries on the database this separation of the logical layer from the storage allows the database implementation to be upgraded or altered while data operations remain unchanged a web api an additional python library connects to the core python library to allow remote data management using a web api allows multiple users to use and access the same data limiting duplication in collaborative modelling remote access through a web api also allows software developers to build applications in any mainstream programming language separating database from logical operations and providing a web api all serve to broaden the application areas in which hydra can operate from a simple desktop installation to a large scale cloud enabled service fig 1 gives an overview of the architecture 2 3 usage four use cases for interaction with hydra include 1 importing data from a 3rd party file format for example output of a model run 2 exporting data to a 3rd party file format for example for use in a model run 3 running a model with data stored in hydra this involves exporting data to the appropriate file format running a model instance then importing results back to hydra 4 using a user interface i e a graphical front end so non developers can interact with the data all use cases require software that interact with hydra a client application or client for brevity is a software application that makes requests to an api clients are not part of hydra but independent software packages which provide functionality such as the use cases described above this section describes how clients interact with hydra to store and access data the generalised data storage design and how it enables accessing diverse network types and collaborative features a client application requests data from or sends data to hydra by interacting with the api for example api functions get network or update network allow a client to retrieve and modify a network respectively a client application may interact with the api to read write update or delete data from hydra users of hydra can write their own custom client applications provided they interact with the api appropriately hydra has its own data format which the client api must use this format closely reflects the database schema structure the data structures can be explored in the examples on the hydra base github repository knox et al 2018a or in the json client application knox 2018 2 3 1 data import export data import and export io relates to data in any format which can be stored in hydra this category includes data not necessarily linked to any one modelling system or method but which can be communicated these might include network topology contained in shapefiles time series data such as that available from the cuahsi his ames et al 2009 and encoded as waterml valentine et al 2012 data stored in microsoft excel files csv files etc 2 3 2 model clients model clients allow communication with specific models or modelling frameworks for example such as matlab gams rosenthal 2008 pyomo these require input and output in specific data formats a model client can also include the function to run the model itself in which case the client algorithm would be 1 export the data to file input modelformat 2 run a model instance using input modelformat 3 once complete import results from output modelformat the process of import and export are identical to a data import export client a user could use the export and import clients in isolation running the model manually but the run client allows this step to be automated 2 3 3 user interfaces hydra is a web server api and so does not have a graphical interface a user interface ui is a client application which allows a user to visually edit network topology data initiate model runs share results etc 2 4 architecture hydra has two components hydra base and hydra server hydra base is a python package library and performs all computational tasks such as database interaction and validation hydra base can be installed as a standard python package and used as such with no requirement to act as a web server hydra server is an additional python package which exposes the functions within hydra base as remote procedure calls rpcs this separation of the base from the server allows for flexible deployment with the same instance being accessible remotely locally or both depending on the requirement of the application figs 2 and 3 demonstrate the different ways hydra can be deployed where clients applications interact through a web service or directly with the python api respectively 2 4 1 hydra object types terminology the following terms refer to concepts within hydra this nomenclature is reflected in the database structure and api and will be referred to throughout this paper capitalisation of these terms later in the paper indicates that they relate to this list a project is a container for networks a project can contain multiple networks and have multiple owners each with configurable levels of access a network describes a topology and contains nodes links and groups nodes links form the topology of a network and can have data associated with them through attributes these can have types like reservoir or water diversion for nodes or river or pipe line for links groups are containers of nodes links and other groups these can be used to define interdependencies among nodes links or to construct a non physical entity such as a governance hierarchy an attribute is defined by a name and dimension for example water flow volumetric flow rate to illustrate consider two attributes water flow volumetric flow rate and storage volume node a might have a water flow associated with it while node b might have both water flow and storage a dataset is an individual data value it is defined by a name data type unit value and metadata when associated with an attribute the unit of the dataset must match the dimension of the attribute it refers to hydra supports multiple datasets for the same attribute this is done through scenarios a network can contain multiple scenarios each of which contains mappings between an attribute and a dataset as hydra can support data for multiple models it uses a template to define the types of nodes links and groups required for each model a template type is defined by a name e g reservoir and a set of attributes e g storage max head etc this can be thought of as the equivalent of a class in object oriented programming oop a node or link within a network will be an instance of a template type 2 5 scenarios data a scenario in hydra represents data associated with a network scenarios allow attributes of nodes links and groups in a network to have multiple datasets associated to them at once any node link or group attribute can be connected to a dataset through a scenario a reference table in the database allows multiple attributes to be connected to the same dataset to ensure datasets are not duplicated datasets are stored independently of networks in hydra this allows the system to store and manage datasets without the need for network topology this is useful for example when there is a need to import time series from cuahsi his ames et al 2009 or other data without having a network this also avoids the duplication of data 2 5 1 units hydra manages units and dimensions by providing a default set including length area volume density etc with a set of default units for each dimension new dimensions and units can be added on a per user basis functions such as add unit and get all dimensions allow units to be managed and searched units are not required for datasets but it is encouraged to specify a unit if a dataset has no dimension the unit dimensionless is used hydra can convert units on datasets the get dataset function call has an optional unit argument which will return a dataset object with the requested dataset converted to the specified unit an error is thrown if units are not of the same dimension 2 6 client interaction hydra is object oriented and uses objects for network and data management and this is reflected in the web api which is also object based that is the user sends receives objects to from the web server or the native python library in the same way the web api is a wrapper for the functions within hydra base these functions have the same names and accept the same arguments and pass these arguments into the equivalent function within hydra base the api is implemented as a json remote procedure call protocol rpc using the spyne python library spyne allows for validation of argument types and definition of the object types which the api can accept spyne also allows the same api to be deployed for several protocols at once for example soap and json this provides flexibility for different client requirements for example using soap simple object access protocol with c net can automatically build classes and perform validation within the client an example of how the web api wraps hydra functions can be found in listing 2 appendix a 4 there are two ways in which clients can interact with hydra local or remote hydra provides client libraries for python java and c the python implementation is the most complete and provides two classes which enable interactions jsonconnection and remotejsonconnection as the names imply these classes allow interaction with hydra using json based data structures either directly with the python library or using http requests the data sent and received from each connection type and the function calls are identical meaning the client code would require little or no alteration when using the different connection types listing 1 in appendix a 4 presents an example of an interaction with hydra using a remote connection with the python client library 2 7 relational database hydra uses a relational database for data storage with sqlite mysql and postresql so far tested the python library sqlalchemy is used for concurrency and transaction management upgrades queries and allows support of multiple database types sqlalchemy is a popular open source well supported library proven to support the security concurrency and scalability requirements of large scale web applications the summary database schema appears in fig 4 and the full version appears in fig 11 in the appendix 2 8 templates hydra is a generalised data management platform which can support multiple types of networks concurrently for example multiple users might work on different projects using different models written in different languages without knowledge of any other supporting this means allowing users to define custom node link and group types which relate to their model for example user a s model deals with supply and demand nodes while user b deals with reservoir desalination and treatment nodes hydra achieves this with templates a template allows a user to create definitions of the node link types required by their model including the attributes which each type requires see figs 5 7 templates allow hydra to support multiple different types of networks within the same system supporting multiple types of model concurrently in addition to defining the node link types required by a model hydra supports data validation in the template api these are implemented as restrictions on the attributes of a type and can include enumerations e g the value must be 10 20 or 30 data ranges the value must be between 10 and 30 date ranges e g the time series must be between january 2018 and december 2018 2 9 collaboration and security hydra supports user and data management by 1 global management of users roles and permissions and 2 management of entities by their owner hydra uses a user role permission structure to define what a user can and cannot do this is done by assigning a user a role which has some permissions such as add network add user in these examples an attempt to call the add network or add user function by a user without these permissions will result in a permission error using this system a user can be given general permissions across the whole platform but may still be restricted on a case by case basis in addition to global permission management hydra supports the sharing and management of individual projects networks scenarios and datasets the user who creates a project network or dataset is deemed its owner and has control to delete hide or share it here we describe how each can be managed 1 projects a user can share a project with other users through the share project function call there are three modes of sharing view edit and share these flags indicate whether the receiving user can view edit or re share the project once shared the receiving user can see all networks within the shared project 2 networks like projects networks can be shared with other users in this case the user will be able to see the project containing the shared network but any unshared networks within the same project will not be visible the same view edit share rules apply to the network 3 datasets datasets are by default independent of any network project or scenario datasets are then linked to networks through scenarios as explained in section 2 5 to reduce duplication an important aspect of data management is the ability to store confidential data to this end hydra allows datasets to be hidden such that only the owner can see them the owner can then share these datasets with other users while keeping them hidden from other users hidden dataset are stored in the same way to other dataset their access is controlled by the hidden flag in the tdataset table which when set to y checks the tdatasetowner table this table is a whitelist of users who have access to this dataset security is important in multi user systems which convey data hydra employs well established security protocols such as encrypting passwords using the bcrypt hashing function in hydra multiple users may share potentially sensitive data with other users hydra s security protocols are designed to ensure only users with the appropriate privileges may view and edit data a series of unit tests available on github were created to ensure sharing functions operate as expected and these are tested automatically whenever changes are made to hydra s code security features such as secure web connections https and altering hydra s default password hashing key are the responsibility of each deployment 3 case study hydra is operational and has been used in regional scale modelling efforts within the uk we present a case study to illustrate an actual deployment in an industrial setting where a consultancy manages large numbers of model runs and scenario analyses for a utility company and where efficient sharing of data was necessary hydra enabled efficient sharing of data and simplified the modelling workflow a common modelling workflow in an industrial setting involves a water institution such as a water utility or river basin agency hiring a consultant to build a model to analyse for instance the levels of service impacts of a new infrastructure asset or management policy the model used in this project is the economic balance of supply and demand ebsd model padula et al 2013 a least cost water supply planning optimisation model used by uk water companies for infrastructure investment planning this model is used to minimise investment costs over a 25 year planning horizon subject to maintaining supply demand balance at a particular level of service the problem is formulated as a mixed integer linear programming milp with binary decision variables for each feasible investment option in each year the solution is subject to constraints that ensure the supply demand balance is satisfied in each water resource zone planning area and investment dependencies or exclusivities are enforced in our case the ebsd model is written in gams rosenthal 2008 a scripting language for mathematical programming optimisation models the model requires a single text input file containing the input network topology and scenario data for a large water utility with many feasible options solving the milp is computationally expensive a run containing all possible options can take several hours as per regulatory requirements the water utility supplies both supply and demand data this is generated independently prior to and during the modelling process supply data includes feasible infrastructure options including their location size and outputs demand data is derived from for example population forecasts and analysis of historic per capita consumption prior projects with this model involved the utility company requesting the consultant to run the model located on their pc then waiting for the results to be returned via email in microsoft excel format reporting was done by the consultant using powerpoint this process proved slow and cumbersome limiting the amount of iteration and refinement of the modelling and ultimately reducing business intelligence on different investment strategies as regulatory requirements change uk water utilities are expected to perform more analysis model runs with more input scenarios and sensitivity analysis the traditional workflow described gave the utility little ability to perform such detailed analysis as running models and accessing results was too time consuming hydra gave the utility more autonomy to perform model runs themselves and for reporting and data analysis to be shared between consultant and utility and potentially with regulators the original ebsd model was expanded to employ the modelling to generate alternatives mga technique chang et al 1983 brill et al 1990 which relaxes the least cost requirement of the model to return the n least cost alternatives within a specified threshold for example within 2 of least cost this enabled the water utility to explore a range of results which may perform better in criteria other than cost such as environmental impact resilience to drought etc this additional requirement caused each model run to be significantly longer ranging from the original 15 min run time to several hours running such models on a local workstation was infeasible and for efficiency multiple runs should be configured and deployed at the same time in addition to the mga addition the utility required that constraints be configurable such as pre requisites must build x before y and mutually exclusivities can only build x or y but not both to implement these constraints hydra s groups were used where nodes were grouped by constraint type the groups were then exported to the model as constraints using the client application the additional requirements of this project compared to prior ebsd projects were as follows make data and model accessible online to enable collaborative modelling allow the water utility to upload data and run scenarios independently configure the mga model runs e g the number of optimised configurations to save allow mga results to be explored and downloaded for analysis allow multiple model runs to be launched concurrently make constraints user configurable the software facilitated and enabled the following project roles 1 the water utility seeking to run the model and who must present results to the regulator 2 an analyst either in the water company or a consultant who performs the actual model runs and analyses the results 3 the developer of the model code and specialist in ebsd and mga provides support to the analyst in interpreting results and expanding the model 4 the developer of the ui hydra and technical support after initial development has little interaction with the project for this project hydra was installed on an amazon ec2 instance along with the database gams client application and the gams model file a user interface for hydra was deployed on an amazon ec2 instance connected to hydra import and export functions were developed in the ui to allow uploading and downloading of model data in the formats required by the water utility the analyst received input data from multiple sources for existing and proposed infrastructure the network topology weather forecasts cost information and demand forecasts a custom excel client application compiled these sources into a single hydra compatible input json string which was uploaded to hydra using the add network function through the ui the analyst ran the model cloned the scenarios to tweak input data such as interest rates or constraints and re ran the model storing outputs in different scenarios once model runs were complete the consultant was given access to the network using share network the consultant and utility analysts then performed analyses on the model results before presenting them to the regulator by using hydra in this instance a transparent modelling workflow was enabled with input data and modelling results available to both consultants and utility analysts via a web browser analysts could consolidate input and output data and share results within a single platform enabled by having a single shared place to select and refine data and manage model scenarios the translation tool required for compatibility with gams is open source gpl3 and available on github knox et al 2011 table 1 lists requirements for the project and the solution to those requirements provided either by hydra directly or through web services generally the hydra features highlighted in this case study are demonstrated in a worked example in appendix a 1 the example illustrates how a model can be uploaded run and shared through hydra the full code for this example is on github the worked example also demonstrates features used in the two supplementary case studies described in appendices a 2 and a 3 these proposed case studies differ from this one by focussing on collaborative model development in a research setting and managing data privacy in an industrial setting respectively 4 discussion this paper has presented hydra platform also referred to as hydra for short a data management software system which helps modellers collaboratively manage share and analyse input and output data to network models as models and modelling efforts expand in scope and importance within resource system decision making processes there is a growing need to share data and collaborate on models which require contributions from multiple people in different physical locations the old paradigm of having the model on a user s local machine and using a file naming system spreadsheets and email for data management is often too limiting to ensure reproducibility and consistency amongst collaborators hydra addresses data management and sharing issues by using a web server which contains functions for storing network structures in a generic way this allows the storage of multiple types of networks while remaining within the well defined sphere of network simulation and optimisation by focusing on network based models there is enough flexibility to support a cross section of environmental modelling while being focussed enough to provide a meaningful and enabling platform for these models the web server provides several features for creating and editing data while supporting multiple degrees of user control these range from limiting user roles user a cannot create networks to a user being able to control access to specific aspects of their work user a can see my network but not edit it data within hydra are by default only visible to the user who creates it but a user can change this setting by using the same data for model input and using the same containers to store model outputs there is consistency amongst users which could improve understanding and reproducibility of results the use of scenarios allows users to create multiple versions of a network thus allowing different inputs and associated outputs to be compared and to avoid conflicts as multiple types of network can be stored in the same software system and shared among users hydra supports model integration this can be achieved by linking two networks together for example an energy network and a water network having run one model to completion and stored the results these results can be transferred to the other network and used as input to a second model this process is termed loose coupling and is supported by hydra with its attribute linking process which allows a node of one network to be connected to that of another as outputs of one are immediately available as an input to the next this approach reduce both the time needed to run an integrated model and errors in data transfer our approach to facilitating this kind of model integration is use of a common generalised data store combined with translation tools these tools create the appropriate input formats for each model and stores their outputs for further use or analysis while this requires the researcher to translate data to and from the format of this centralised system this enables them to make their tools available to others reducing this burden on future modellers currently publicly available tools exist to translate network data to and from the following formats and modelling tools gams pyomo waterml via cuahsi his microsoft excel csv and gis shapefiles to demonstrate that hydra is a functioning system we presented a case study of its use in an industrial setting the case study describes an existing deployment of hydra within a water utility hydra reduces reliance on model developers performing the model runs and allows the water utility to run the model themselves this case study illustrates how the model itself can be deployed remotely connecting to hydra and managed through a custom user interface two additional proposed case studies are presented in the appendix with example code showing how they can be achieved early hydra projects have pointed to needed future developments these include lowering the barrier to entry for the use of hydra by non software developers allowing hydra to handle the storage of large datasets multiple gb and to improve the accessibility and quality documentation comments etc of existing client applications as the number of deployments of hydra increases we will continue to develop its collaboration and security features for example by allowing encryption of datasets by individual users to further ensure sensitive data is protected to encourage adoption of hydra as a solution to data management needs it has become clear that a ui would be helpful we are building on the work presented here to create a generalised ui for managing networks and data the ui will feature the ability to create network types and build networks manually using these types the ui will also feature the ability to run models and activate client applications to integrate input and output of data more than one ui using hydra may emerge the data required to run some models and the results produced by large models can become too large to store in hydra in the order of gb per dataset in order to accommodate the potentially huge quantities of data we are investigating using nosql approaches and big data techniques for accessing huge datasets hydra then will be used to store references to dataset locations which reside externally as with the ui a key to the adoption of hydra is the availability of a range of pre existing client applications where users can quickly get data into and out of the system with minimal effort we intend to continue developing the existing client applications for gams matlab excel as well as documenting their development to reduce barriers to entry for prospective client developers hydra does not contain the facility to directly make a dataset or network publicly available this is an important improvement to support publications and to encourage scientific reproducibility to this end we are investigating the development of a connection to hydro share tarboton et al 2013 an online repository of data and models for water resources and other fields all tools presented here are open source and available on github we will continue developing documentation examples tests of hydra and its applications with the goal of encouraging adoption 5 conclusions the environmental systems models being developed are increasingly complex and there is an increasing need to manage data for such models to facilitate collaboration and sharing across users and locations reduce errors and promote scientific reproducibility and transparency we have presented hydra platform hydra a web based data management platform specialising in storing network based data some features of hydra include open source python library with an extension enabling it to be deployed as a web service client libraries to simplify interaction with local and remote instances of hydra networks of multiple types can be stored in the same instance of hydra allowing it to serve multiple model types concurrently so long as all models use a node link structure flexible data support built in support for data types data frames timeseries arrays scalars and free text expandable to support other types client applications connect to hydra to translate the data into the formats required by a modelling framework and to send back results of model runs includes a user permission system and supports data ownership this enables data confidentiality while providing the ability to share data amongst users multiple modellers can access the same inputs or model results simplifying collaboration and minimising the likelihood of the errors common in traditional data sharing the case study described a successful deployment of hydra in an industrial setting and highlighted some features of hydra such as its facility for sharing data between users running models through client applications and management of confidential data in future work we have proposed development of a user interface to simplify data interaction an approach to managing big data and a way to publish input data and model results acknowledgements support for the software development was partially funded by the uk government through innovate uk file reference number 101338 the uk economic and social research council esrc provided support through the futuredams project es p011373 1 any opinions findings and conclusions or recommendations expressed in this material are those of the authors but not necessarily the funders appendix this appendix contains a worked example which provides a compact demonstration of how the case study and two proposed case studies can be implemented with code the worked example shows an application to an energy water system simulator the appendix also contains two further case studies which are partially designed and implemented showing further examples of hydra s use which 1 supports collaborative development of a model across academic institutions appendix a 2 and 2 allows secure control of data sharing within a large industrial project appendix a 3 finally two small snippets of code are presented which demonstrate how to perform a basic hydra task of updating an existing network and an illustration of the hydra server design a 1 worked example the following example shows a step by step process for creating and importing a network running a model sharing results while keeping input data private and inspecting results visually the demonstration involves simple scripts which perform actions within hydra such as creating projects retrieving network details and so on the reader is free and encouraged to amend these scripts it is important to note that these scripts are for illustrative purposes only and as such are kept as simple as possible they are not designed to provide a full comprehensive example of how a client application should be built for hydra for example almost no error checking is performed this example uses a modelling tool called pywr a python based simulation system a client application called the hydra pywr app is used to load network data into hydra to run the model and to store results to ensure maximum compatibility of this demo the app is run through pipenv a virtual environment system used in python the model in question simulates coupled water and energy systems and is applied to a small synthetic resource system network the simulation model includes different water demands irrigation water supply and hydropower and energy demands energy demands are supplied by hydropower plants and conventional thermal power plants a simple transmission system connects the different energy supplies to the energy demands we note that this energy system model does not include any dispatch constraints the model schematic is show in fig 8 it is shown in a web based front end interface for hydra which is currently in development the formulation of the model itself is beyond the purposes of this example and has been submitted for publication data workflow is our focus here a more detailed description of this model and the example models are available at the same location on github the outputs of the code in this example have been shortened for clarity the code contained in the scripts shown below can be found at https github com umwrg hydra pywr demos a 2 proposed case study 1 this proposed case study case was designed to manage the difficulties that can arise when working collaboratively on multiple models or multiple instances of input data consider a model being developed by two researchers in different organisations for example two separate model formulations are developed of a water system each model capable of being run with the same input data but with differing outputs this project requires keeping track of multiple evolving models and input data files to minimise the disorder of multiple researchers developing input data and analysing output data of the different models hydra is used to store the data centrally using the project network structure for the separate model formulations and its scenario management capabilities for parameterising data in this application the model is an optimisation driven water resources simulation written in the generic algebraic modelling system gams a scripting language for optimisation models different variations formulations of the optimisation drive simulation model are created and compared the gams models require a single text input file containing the network topology and scenario data the goal is to study the differences in results resulting from different formulations the models share most data but some parameters are particular to each optimisation model formulation to manage the inputs to each different model and the many possible outputs the two researchers user a and user b deploy an instance of hydra on an amazon ec2 server using a mysql database and an apache web server user a first puts the network and the baseline scenario into an excel file in the format required by the excel client application knox and mohamed 2018 she then uses an excel client application to upload the file to hydra user a then shares the network with user b granting full edit access using the share network function both users can then clone the baseline scenario into for example climate change within the network using the clone scenario function in this case the input data is identical but the scenario can be used to store different outputs once the respective models have been run this allows them to manage different instances of data relating to the same network should they wish to work on fully separate networks or even projects the clone network function could also be used to run the model a gams client application is used which calls get network to retrieve the network and scenario in question this is returned as a json string which is then converted by the client into input txt ready to be imported in the gams code upon completion the output output gdx file is read by the client converted back into the hydra friendly json format and update network is called with the new result values included following the same procedure for each model results in three scenarios stored against the network which can be compared stored for later and shared with user b which in turn can re run the model with new input data the translation tool required for compatibility with gams is open source gpl2 and available on github knox et al 2011 a 3 proposed case study 2 regional infrastructure planning increasingly encourages watercompanies to build regional inter utility models to explore the use of common assets and transfers to cut costs and increase reliability matrosov et al 2013 zeff et al 2014 multiple data sources and need for transparency while maintaining confidentiality of cost data a regulatory requirement can become difficult without dedicated management of the data this proposed case study presents an example of how hydra could be used to improve data sharing and management in this project the stakeholders engage a consultant for modelling an analyst within the consultancy runs the model the model in this proposed case study is a single regional scale simulation written using the python water resource simulator pywr this model accepts a single json input file containing topology and scenario data data on supplies demands and costs is compiled by each water company each water company may view their own data and overall results but cost data from other utilities must be hidden from them given the uk has regulated private utilities hydra can consolidate all the data so it can be input to the single regional model the model run itself is done by a trusted third party analyst who has permission to view all datasets but each water company is only able to see its own see figs 9 and 10 to allow controlled data access to all parties hydra is deployed on an amazon ec2 instance which also hosts the pywr software and a custom ui this ensures the model can be run from any location removing dependency on specific computers or institutions an analyst in the consultancy compiles the data from multiple sources into a json file ensuring that sensitive datasets have a hidden flag this ensures nobody except the analyst can see the datasets using a json client application the network is uploaded to the server next the sensitive datasets are shared with the appropriate user groups followed by the network being shared with all parties when accessing the network each stakeholder can see all non hidden datasets as well as the hidden datasets to which it has been granted access a 4 examples listing 1 shows pseudocode to create a remotejsonconnection to enable a client to communicate with a remote installation of hydra server using a jsonconnection the client would attempt to connect to a local instance of hydra base a client application developer can easily switch between a local and remote connection with both connections interacting in an identical fashion this gives the developer flexibility for testing or to allow the client to interact with different end points with little or no changes to the client side code the full operational code is available on github knox 2018 other examples can be found in the hydra base repository located on github knox et al 2018a listing 2 demonstrates using pseudocode how the web api works the spyne library exposes the get network function as a remote procedure call rpc accepting an integer as an argument it then calls the get network function in hydra base returning a network object in this example the network object comes from a class defined by the web api which ensures the incoming and outgoing structures are valid for every function in hydra base which should be exposed there is an identical function in the server with this workflow 
26123,papyrus cyperus papyrus wetlands support millions through food provisioning which leads to loss of regulating ecosystem services this study aimed at understanding the impact of changes in water regime and vegetation harvesting on nutrient retention in rooted papyrus wetlands a simulation model papyrus simulator developed and calibrated with data from african wetlands produced reasonable estimates of productivity and nutrient retention phosphorus retention was lower than nitrogen retention leading to a nitrogen limited environment by reducing the n p ratio in the water absence of surface water during part of the year caused a reduction of biomass harvesting increased nitrogen retention from 7 to over 40 and phosphorus retention from 4 to 40 sensitivity analysis revealed assimilation mortality decay re translocation nutrient inflow and soil porosity as the most influential factors papyrus simulator is suitable for studying nutrient retention and harvesting in wetlands and contributes to quantification of ecosystem services and sustainable wetland management keywords papyrus nitrogen retention phosphorus retention modelling ecosystem services wetlands 1 introduction in africa where so many people depend directly on wetlands for their livelihoods schuyt 2005 maclean et al 2014 population growth climate change the need for food security conceição et al 2016 the suitability of wetlands for food production and weak implementation of wetland conservation policies create an enormous pressure on wetlands rebelo et al 2010 davidson 2014 current per capita food production in africa is at the level of the 1960s pretty et al 2011 more than one in four africans are undernourished undp 2012 pretty et al 2011 defined sustainable agricultural intensification as producing more output from the same area of land while reducing negative environmental impacts and increasing contributions to natural capital and the value of environmental services and identified nutrient cycling as one of its attributes to apply this concept to agriculture in and around wetlands more knowledge is needed about the dynamics of nutrient retention both in relation to human activities e g agriculture and vegetation harvesting or extreme weather events due to climate change and to natural variation in wetland processes e g seasonal changes in hydrology this knowledge can be used to develop more effective management practices to increase food security while protecting important ecosystem services the still widespread papyrus cyperus papyrus wetlands in east africa van dam et al 2014 ajwang ondiek et al 2016 provide a wide range of ecosystem services these wetlands support the livelihoods of millions of people through provisioning services like seasonal agriculture papyrus harvesting drinking water clay and sand mining fishing and fuel production morrison et al 2012 2014 jones et al 2016 van dam and kipkemboi 2018 the regulating services in these wetlands reduce the runoff of nutrients sediments and trace elements into lakes and rivers kansiime et al 2007 and regulate water quantity flood protection and water storage and local climate e g by influencing rainfall patterns due to its c4 photosynthesis papyrus vegetation can sequester up to 0 48 kg c m 2 y 1 storing up to 8 8 kg c m 2 in biomass and 64 kg c m 2 in detrital and peat deposits saunders et al 2007 2014 after clearing the aboveground biomass a full stand of papyrus can grow back within 6 9 months muthuri et al 1989 terer et al 2012a 2012b this high productivity provides opportunity for seasonal crop production in the dry season with recovery of the papyrus vegetation through rhizomes and continuation of regulating services like nitrogen and phosphorus retention in the wet season van dam et al 2014 the processes underlying nitrogen n and phosphorus p retention accumulation of organic matter uptake nitrification denitrification adsorption depend on conditions resulting from the hydrology and the presence of vegetation in the wetland and their interaction powers et al 2012 permanently flooded areas favour peat formation and denitrification while seasonally flooded zones favour nitrification and are more prone to anthropogenic disturbance in papyrus between 45 and 105 g n m 2 is stored in living biomass and 105 457 g n m 2 in dead biomass and peat gaudet 1977 gaudet and muthuri 1981 boar et al 1999 boar 2006 potential denitrification in floating papyrus in lake naivasha ranged from 2 3 to 6 4 g n m 2 y 1 viner 1982 reported values for p storage in living biomass were 5 4 g p m 2 and in detritus and peat 3 57 g p m 2 gaudet 1977 gaudet and muthuri 1981 boar 2006 for p adsorption kelderman et al 2007 found a maximum adsorption of 4 mg p g 1 sediment in kirinya wetland uganda lakes are often downstream of papyrus wetlands and guildford and hecky 2000 found lake victoria to be n limited and therefore favouring n fixing cyanobacterial blooms at higher p concentrations lakes are more often n limited but not exclusively and limitation can shift from n to p with increased global n deposition rates elser et al 2009 it is therefore crucial to understand n and p cycling in papyrus wetlands and how wetlands influence the n p ratio in runoff water simulation models can help to improve understanding of the complex dynamics of nutrient retention as influenced by natural e g seasonal hydrology and anthropogenic e g agriculture and harvesting drivers of change earlier model studies analysed the retention capacity of lake victoria fringing wetlands with a focus on processes for n p and organic matter mwanuzi et al 2003 and n processes for floating papyrus wetlands van dam et al 2007 while some studies have concluded that papyrus wetlands are effective in removing or retaining n and p e g up to 35 of total n from the inflowing water kanyiginya et al 2010 and inorganic p up to 90 mwanuzi et al 2003 the understanding of underlying processes is still limited there is no evidence that these conclusions are valid over a longer period or merely a result of temporal n and p storage besides simulating changing conditions on a local scale e g vegetation harvesting process models can improve regional and global models to estimate the impact of human activities on biodiversity and ecosystem services sjögersten et al 2014 janse et al 2015 beusen et al 2016 costanza et al 2017 in a previous study we constructed a simulation model for n cycling in natural rooted papyrus wetlands hes et al 2014 the model now called papyrus simulator showed that n retention increased from 13 at low harvesting to a maximum of 50 per year at intermediate harvesting rates if harvesting was increased further n retention dropped dramatically 5 as harvesting exceeded regrowth the model did not incorporate p and was not able to compare permanently and seasonally flooded zones independently the overall objective of the current study is to understand the impact of changes in water regime and papyrus harvesting on n and p retention in rooted papyrus wetlands the specific objectives are 1 to further develop papyrus simulator by incorporating p processes and a hydrology section that enables independent comparison of different hydrological conditions and make it generally applicable to papyrus dominated wetlands 2 to conduct a sensitivity analysis to identify which processes and parameters are most important in retaining n and p and to which parameters the model is most sensitive and 3 to compare papyrus simulator outputs with published field data 2 methods 2 1 model description and development the original papyrus simulator described a rooted papyrus wetland hes et al 2014 and modelled the influence of papyrus growth and harvesting under different hydrological conditions on the cycling of n fig 1 to achieve this the model comprised three interacting sections hydrology carbon and nitrogen fig 2 in the hydrology section water level and soil moisture were calculated based on a simple water balance model soil moisture conditions determined the factor mode representing conditions from anaerobic to aerobic which influenced the nitrification and denitrification rates in the carbon section the assimilation by papyrus was estimated on the basis of maximum photosynthetic rate and irradiance and limited by the n concentration in the papyrus biomass other processes in the carbon section were mortality fragmentation leaching and harvesting biomass c and n content were linked through an optimum c n ratio so that changes in carbon for example when harvesting occurs led to proportional changes in the amount of n in the n section of the model in the n section mortality of papyrus biomass led to dead organic n which was further degraded to nh4 which could be converted to no3 and enter denitrification depending on moisture conditions in the current study papyrus simulator was developed further in two areas a phosphorus p section was added 1 and the model was generalized by merging the original seasonal and permanent wetland zones based on lake naivasha into one wetland zone that can be inundated based on the prevailing conditions of surface water inflow a combination of stream and overland flows in the model called river inflow lake inflow in the model this can be switched on and off depending on whether a seasonally inundated or permanently inundated wetland is simulated and precipitation 2 all state and rate variables processes of the new model are given in annex 1 and 2 respectively with their numbers between brackets in the text all other parameters and variables are given in annex 3 the key processes and conceptual diagrams of the four sections fig 2a are presented and described below 2 2 hydrology the hydrology section fig 2b calculates the water level as the volume of water per m2 of surface area based on inflow from river discharge outflow to and backflow from the lake precipitation evapotranspiration and groundwater recharge backflow from the lake occurs when the sum of water outputs is greater than the sum of water inputs keeping the surface water level constant at 0 5 m 72 and can be switched off to simulate wetlands that have an outflow but do not receive backflow allowing the surface water to drop in the dry season when the inputs are higher than the outputs the difference flows to the lake keeping the surface water level at a maxmimum of 0 5 m 100 lake naivasha conditions gaudet 1979 were used as a basis to describe an annual discharge precipitation and evapotranspiration regime groundwater recharge is modelled with first order equations resulting in a maximum recharge of 6 mm day 1 when soil is saturated 133 the factor mode based on van der peijl and verhoeven 1999 is calculated as the proportion of soil porosity that is filled with water annex 3 ranging from 0 soil completely saturated anaerobic to 1 soil at field capacity or less aerobic conditions mode influences nitrification and denitrification rates in the soil pore water conditions in the surface water are assumed to be aerobic the soil depth considered in the model was 0 2 m 2 3 carbon papyrus biomass is modelled as carbon c in aboveground biomass agb culms and umbels and belowground biomass bgb rhizome and roots fig 2 carbon in agb cagb results from net growth assimilation minus mortality minus respiration and translocation of c to and from the rhizome assimilation 42 was described using a logistic model depending on total aboveground biomass and limited by irradiance and the n and p concentrations in the plant both described by monod type equations a s s i m i l a t i o n max a s s i m i l a t i o n c o n s t a n t c a g b 1 c a g b c c o n c a g b k a s s i m r a d i a n c e r a d i a n c e k r a d i a n c e l i m i t n a s s l i m i t p a s s 0 81 in which max assimilation constant is the maximum relative assimilation rate day 1 cagb is carbon in agb g c m 2 c conc agb is carbon in agb g c g 1 dw k assim is papyrus biomass at which assimilation stops g dw m 2 radiance is irradiance mj m 2 day 1 k radiance is the half saturation constant of irradiance for assimilation mj m 2 day 1 limit n ass and limit p ass are limiting factors of n and p respectively for c assimilation annex 3 the factor 0 81 ensures that maximum growth limitation can be reached van der peijl and verhoeven 1999 at n or p concentrations in papyrus below the minimum needed for assimilation 1 6 10 3 g n g 1 dw and 8 10 5 g p g 1 dw no growth is assumed so a monod type function with a cut off is used van der peijl and verhoeven 1999 the dead agb is fragmented and hydrolysed according to first order kinetics 48 and 113 respiration in agb 45 is the sum of maintenance respiration proportional to biomass and growth respiration proportional to assimilation as c a g b r e s p i r a t i o n c a g b m a i n t c o e f f a g b a s s i m i l a t i o n g r o w t h c o e f f a g b in which maint coeff agb is the maintenance respiration coefficient for agb day 1 and growth coeff agb is the respiration coefficient for assimilation translocation of c between agb and bgb 41 is based on an assumed optimal ratio between aboveground and belowground c c agb to bgb optimal ratio annex 3 of 1 2 boar et al 1999 jones and humphries 2002 whenever c ratio differs from this optimal ratio c is translocated to restore the optimum i f c a g b t o b g b o p t i m a l r a t i o c a g b c b g b t h e n t r a n s l o c a t i o n t l d o w n m a x c c a g b c b g b c a g b c b g b k t l d o w n c i f c a g b t o b g b o p t i m a l r a t i o c a g b c b g b t h e n t r a n s l o c a t i o n t l u p m a x c c b g b c a g b c b g b c a g b k t l u p c in which cagb and cbgb are c in agb and bgb respectively g c m 2 c agb to bgb optimal ratio is the optimal ratio between c in agb and bgb tldownmax c is the maximum rate for translocation of c from agb to bgb g c day 1 tlupmax c is the maximum rate for translocation of carbon from bgb to agb g c day 1 ktldown c is the half saturation constant for downward translocation and ktlup c is the half saturation constant for upward translocation carbon in bgb cbgb is reduced by mortality following a first order equation depending on cbgb and a cbgb death constant 46 the dead bgb is fragmented and hydrolysed according to first order equations 50 and 112 respiration by bgb 47 is calculated as for agb with growth respiration proportional to downward translocation and maintenance respiration proportional to cbgb 2 4 nitrogen the n section fig 3 expresses the same aboveground and belowground alive and dead biomass compartments as the carbon model expressed in g m 2 of n added to these are the main components of the n cycle in the wetland particulate and dissolved organic n nh4 and no3 nitrate and ammonium are taken up by the bgb 84 and 94 and then passed on to the agb by translocation 74 n in dead biomass is re translocated 73 or passes through fragmentation 78 and 80 hydrolysis 116 and 119 mineralisation 56 and 59 nitrification 90 and 91 and denitrification 52 exchange of n between the aboveground and belowground layers takes place through diffusion 53 82 and 92 driven by concentration differences of soluble compounds no3 nh4 and dissolved organic n and settling of particulate organic n 115 the uptake of nh4 and no3 by papyrus 84 and 94 depends on the carrying capacity for papyrus and the concentration of n in the biomass and is limited by the concentration of nh4 and no3 respectively this limitation is modelled with a monod type equation only the equation for nh4 uptake is given here for illustration n h 4 u p t a k e m a x n h 4 u p t a k e n b i o m a s s 1 n b i o m a s s n max b i o m a s s n h 4 p c o n c n h 4 p c o n c k n h 4 in which max nh4 uptake is the maximum uptake rate of nh4 by papyrus day 1 n biomass is the amount of n in agb and bgb g n m 2 n max biomass the maximum amount of n stored in agb and bgb g n m 2 nh4p conc is the nh4 concentration in the pore water g n m 3 and k nh4 is a half saturation constant g n m 3 n max biomass n max agb n max bgb was calculated based on literature values for n content in both agb and bgb as 0 013 and 0 008 g n g 1 dw respectively annex 3 and the maximum papyrus density of agb and bgb found in literature 8118 g dw m 2 muthuri et al 1989 jones and muthuri 1997 2 5 phosphorus the p section is similar in structure to the n section fig 4 with the main exception being the adsorption and release of p to the sediment 31 and 32 these are modelled with a langmuir equation and a factor that decreases adsorption and increases the release rate when the amount of adsorbed p approaches the maximum of adsorbable p van der peijl and verhoeven 1999 i f o p a d s o p a d s e q t h e n a d s o r p t i o n 1 o p a d s o p a d s m a x o p a d s e q o p a d s i f o p a d s o p a d s e q t h e n r e l e a s e o p a d s o p a d s m a x o p a d s o p a d s e q in which opads is the amount of p adsorbed g p m 2 opads max is the maximum amount that can be adsorbed g p m 2 and opads eq is the amount adsorbed in equilibrium with the orthophosphate op concentration g p m 2 2 6 model assumptions and implementation it was assumed that the distribution of water in the wetland is uniform without preferential flow floating papyrus mats at the edge of the lake were not considered because harvesting is assumed to take place in rooted papyrus zones all elements needed for growth that are not included in the model were assumed not to be limiting uptake of no3 nh4 and op are not taking place when pore water is not available the model currently does not include ammonia volatilization n fixation and n deposition considerations were discussed in hes et al 2014 the model was implemented in stella 10 0 6 isee systems inc lebanon nh us and run for a period of 5 years with rectangular euler integration and a time step of 0 0625 days 1 5 h for 1 m2 of wetland a complete listing of the equation layer of the stella model is given in annex 4 2 7 parameterization and calibration the model was parameterized and calibrated with data from the literature for lake naivasha annex 1 and 3 when values from lake naivasha were not available data from other east african papyrus wetlands were used for parameters that were never studied or measured in papyrus wetlands literature values from other wetland types were used or estimated for details see annex 1 and 3 seasonal variability solar radiation and hydrological inputs in the lake naivasha wetland was described using monthly averages from the period 1970 1982 for irradiance muthuri et al 1989 and 1974 1976 for evaporation and precipitation gaudet 1979 and river inflow evaporation was multiplied with a factor 1 25 to estimate evapotranspiration for papyrus saunders et al 2007 monthly river inflow based on the flow regime of the malewa river gaudet 1979 was calibrated to achieve realistic flow rates and n and p concentrations with a main rainy season in the months march may and a short rainy season in december evapotranspiration 4 1 6 2 mm day 1 and precipitation 0 1 6 2 mm day 1 varied throughout the year fig 5 c two flooding conditions were simulated in the permanently flooded wetland zone it was assumed that there was a backflow from the lake of between 0 and 0 12 m3 m 2 day 1 whenever river flow and rainfall were low in the seasonally flooded zone it was assumed that no lake backflow occurred resulting in lower water levels during periods with low river flow and rainfall this hydrological regime was repeated annually for the 5 year simulation fig 5 2 8 retention and harvesting absolute n and p retention g n or p m 2 yr 1 were calculated as ninfow noutflow and pinflow poutflow both over the 5th year when the model had stabilised fig 6 a ninflow was the sum of n in river discharge 55 58 85 87 95 97 117 and 120 and n flowing in from the lake 54 83 93 and 114 and noutflow was the sum of n outflow to the lake 60 88 98 and 121 and groundwater recharge 57 86 96 and 118 pinflow 34 37 38 63 66 67 123 127 and 129 and poutflow 36 39 65 69 126 and 130 were calculated in the same way relative retention wt n or p m 2 yr 1 was also calculated over the 5th year as ninfow noutflow ninflow and pinflow poutflow pinflow two types of harvesting scenarios were applied 44 76 and 104 daily harvesting in g papyrus dw m 2 d 1 and annual harvesting in a percentage of standing biomass the annual harvest took place during the dry season on day 230 when the surface water levels go down fig 5a and seasonal agriculture is most likely 2 9 sensitivity analysis initially a one at a time oat local sensitivity analysis was done by running the model with calibrated initial values of state variables and model parameters and values of 10 and 10 of each of these the effect of these variations on the output variables papyrus biomass nutrient concentrations in surface water nutrient retention were observed and 28 out of 81 parameters to which the model was most sensitive were selected for a global sensitivity analysis based on the approach outlined in saltelli et al 2000 for each of the 28 parameters a range of possible values minimum maximum was determined based on what was assessed as being most realistic table 1 and a rectangular distribution was set within the sensitivity settings of stella the model was then run 500 times both for permanently and seasonally flooded conditions with parameter values drawn from the 28 distributions for each run as output variables the following ten were selected papyrus agb papyrus bgb both in g dw m 2 nh4 n no3 n and op p in the surface and pore water in g n or p and n and p retention for year 5 in g m 2 y 1 for the first eight variables the mean value for the inundated period days 32 322 of year 5 of the simulation run was computed for n and p retention the end values of year 5 were used the resulting output dataset of two times 500 model runs with combinations of random parameters and output variables was submitted to multiple regression analysis with the output variable as dependent variable for each regression model that had a sufficiently high coefficient of determination preferably r2 0 7 standardized regression coefficients or beta weights were calculated and compared to assess the contribution of each input variable in explaining the variation in the output variable as a measure of sensitivity of the output variables to the inputs only model parameters that had a significant regression coefficient t test p 0 05 were included all regression models were calculated using functions lm and lm beta in r version 3 5 0 r core team 2018 2 10 comparison with field data to compare the simulation results with field data papyrus wetland studies that reported the main output variables of the model agb and bgb n and p concentrations in biomass water quality net primary production of biomass were reviewed wetland characteristics location altitude and type floating or rooted were also included as the model was aimed at papyrus wetlands in general and not at one wetland site in particular model output was compared with the ranges of values found in the literature 3 results 3 1 water levels in and outflows and factor mode under permanently flooded conditions the surface water level was constant at 0 5 m above the substrate fig 5a resulting in a value of 0 for the controlling factor mode fig 5b implicating saturation and anaerobic conditions in the pore water under seasonally flooded conditions the surface water level was 0 5 m during the rainy season and dropped to zero in the dry season while the pore water was below saturation for part of the dry season fig 5a this resulted in peaks of the mode factor fig 5b in the dry season when the sum of river inflow and rainfall was smaller than the combined evapotranspiration and recharge the seasonal wetland did not receive backflow from the lake while the permanent wetland did fig 5d and e 3 2 above and belowground biomass and effects of harvesting the simulated papyrus biomass reached a maximum of 8127 g dw m 2 in year 5 when the model was assumed stable belowground biomass bgb remained higher than aboveground biomass agb fig 6a and b without harvesting and under permanently flooded conditions agb and bgb fluctuated annually between 3809 and 3824 g dw m 2 and 4287 and 4304 g dw m 2 respectively with seasonally flooded conditions fluctuation was slightly higher 3798 3823 g dw m 2 agb and 4276 4303 g dw m 2 bgb once per year there was a small drop in biomass coinciding with the dry season figs 5a and 6b the agb bgb ratio remained similar at 0 9 throughout the year in both zones when a harvesting rate of 25 g dw m 2 d 1 of agb was applied the negative impact of the dry season on agb and bgb increased and total biomass was reduced by 13 37 in the permanent zone and 14 39 in the seasonal zone fig 6c and d if harvesting was increased to 35 g dw m 2 d 1 the agb was reduced to zero after only 3 years in the seasonally flooded wetland and about 20 days later in the permanently flooded wetland with a one time annual harvest of 50 or 100 the biomass recovered fully in about 11 months in contrast with the daily harvesting scenarios the agb bgb ratio changed during the recovery period increasing from very low just after the harvest to the original ratio 0 9 after recovery for the 100 harvest the agb bgb ratio increased to more than unity before returning to the original value fig 6i and j this was a result of temporary slow recovery of bgb owing to nutrient limitation during the absence of river inflow fig 5d e 7i and 7 j 3 3 effects of harvesting on surface water concentrations of no3 nh4 and op under permanently flooded conditions the concentrations in year 5 of no3 n nh4 n and op p in the surface water reached a peak of 1 6 3 0 and 0 5 g m 3 respectively fig 7a these peaks coincided with the wet season fig 5 the lowest concentrations in year 5 of zero no3 n 0 2 nh4 n and zero op g m 3 occurred at the end of the year just before the start of the wet season and at the end of a period with low n and p inputs from river inflow fig 5d e and 7a the patterns in the permanently flooded and seasonally flooded systems were similar as were maximum concentrations recorded the lowest concentrations were close to zero due to the absence of input of n and p from the lake during the dry seasons fig 7b with a harvesting rate of 25 g dw m 2 d 1 under permanently flooded conditions the highest concentrations of no3 n 0 8 g m 3 nh4 n 1 2 g m 3 and op 0 2 g m 3 dropped and occurred earlier after the start of the dry season compared with no harvesting fig 7c for seasonally flooded conditions nutrient concentrations were similar fig 7d when harvesting was increased to 35 g dw m 2 d 1 under permanently flooded conditions the minimum concentration in year 5 for all compounds were higher than without harvesting no3 n 2 0 nh4 n 2 3 and op 0 4 all g m 3 maxima of no3 3 2 g n m 3 and op 0 5 g p m 3 occurred at the end of year 5 ammonium maxima 3 1 g n m 3 occurred early in the year under seasonally flooded conditions the effects on the lowest concentrations were the same as under permanently flooded conditions for no3 and op but higher for nh4 3 0 g n m 3 the highest concentrations for no3 and nh4 were around 5 g n m 3 just before and after the dry periods when surface water dropped fig 7e and f when 50 of the agb was harvested under both hydrological conditions all nutrient concentrations dropped from the moment of harvesting to the point of biomass recovery fig 6g h 7g and 7h with 100 harvesting the trend was similar under both seasonally and permanently flooded conditions just after the harvest there was an increase in all concentrations which was slightly lower under seasonally flooded conditions due to the absence of backflow with nutrients from the lake after this increase concentrations decreased and remained lower compared with the situation without harvesting until the biomass recovered fig 6i j 7i and 7j 3 4 effects of harvesting on n and p retention n and p retention increased with increasing harvesting rates in both permanently and seasonally flooded conditions until a dramatic drop when the papyrus was over harvested fig 6e and f and 8 for the permanently flooded wetland this point for both n and p was at a harvesting rate of about 34 g dw m 2 d 1 in seasonally flooded conditions this was similar at around 33 g dw m 2 d 1 n retention under both flooding conditions ranged from 10 g n m 2 yr 1 without harvesting to 67 g n m 2 yr 1 at a harvesting rate of 33 g dw m 2 d 1 dropping to a net maximum release of 3 9 g n m 2 yr 1 under permanently flooded conditions under seasonally flooded conditions n retention was 12 g n m 2 yr 1 without harvesting increasing to 66 g n m 2 yr 1 and then falling to a release of 3 4 g n m 2 yr 1 at higher harvesting rates p retention increased from 0 6 to 6 3 and from 0 6 to 6 2 g p m 2 yr 1 in permanently and seasonally flooded conditions respectively thereafter declining with faster harvesting to around zero for both conditions without harvesting fig 8 n and p were retained mainly in accumulating dead agb 7 1 g n m 2 yr 1 and 0 3 g p m 2 yr 1 and 8 7 g n m 2 yr 1 and 0 4 g p m 2 yr 1 for permanent and seasonal systems respectively to a lesser extent n and p were also retained in dead bgb and in particulate and dissolved organic matter with increased harvesting rates the accumulation in organic matter n and p gradually decreased while overall retention increased as a consequence of uptake by recovering papyrus fig 8 denitrification was marginal in both zones until papyrus was overharvested when it increased to 1 8 g n m 2 yr 1 permanent and 2 0 g n m 2 yr 1 seasonal and became the sole process responsible for retaining or more correctly removing n fig 8a and c the denitrification was slightly higher under seasonal conditions due to the higher no3 concentrations fig 7e and f compared with permanent flooding conditions and the short aerobic period under seasonally flooded conditions fig 5b at harvesting rates leading to the absence of papyrus fig 6e and f the main factor for p retention was adsorption at 1 0 g p m 2 yr 1 in both systems fig 8 b and d the values taken are from the 34 g dw m 2 d 1 harvesting scenario when harvesting was increased further adsorption rates were lower 3 5 sensitivity analysis table 2 shows the results of the twenty regression models and the ten output dependent variables for both permanently flooded and seasonally flooded systems all models were significant but the four models for n and p retention had adjusted r2 below 0 7 0 53 0 60 despite this all models where used for further analysis 3 5 1 sensitivity analysis for biomass outputs there was little difference between responses of biomass under permanently and seasonally flooded conditions and between agb and bgb fig 9 a and b biomass responded most positively to an increase in the maximum assimilation constant and the k assimilation with beta values between 0 46 and 0 55 for both constants responses where higher for permanent than for seasonally flooded conditions and for agb than for bgb positive responses beta between 0 10 and 0 15 were also observed with increased nh4 and no3 concentrations in the inflow for agb and bgb and for n translocation constant on bgb for both conditions biomass responded negatively to an increased aboveground death constant beta between 0 35 and 0 41 with the responses to bgb more negative than to agb and also slightly more negative for seasonal than for permanently flooded conditions for n fig 9c d e and f the differences between dry and wet conditions were modest the concentrations in the incoming river water had the highest impact on the concentrations in both wetland systems especially on surface water concentrations for surface water nh4 river concentrations explained the nh4 concentration in the surface water with a beta value of 0 85 and 0 79 for permanently flooded and seasonally flooded conditions respectively for the pore water concentrations the beta values were lower 0 69 and 0 65 the incoming no3 concentrations positively influenced no3 surface water concentrations in both systems beta 0 77 for permanent and beta 0 74 for seasonal and in the pore water beta values of 0 62 and 0 60 respectively after the river concentrations the nh4 and no3 concentrations were most sensitive to changes in the n re translocation constant beta values for the re translocation constant influencing no3 concentrations were higher than for nh4 and re translocation beta values in pore water for both no3 and nh4 concentrations were higher than those in surface water fig 9c d e and f there were also positive responses betas around 0 20 of the nh4 concentration in surface and pore water in both systems to an increased belowground leaching constant higher soil porosity dilution resulted in lower no3 and nh4 concentrations in the pore water with betas around 0 15 increased death constants for agb and more so for bgb had a negative effect on both n species but more on no3 than nh4 fig 9c d e and f for p there was also not much difference in sensitivity between the permanently and seasonally flooded systems fig 9g and h the op concentrations in the wetland were most sensitive to the river concentrations for surface water with beta values of 0 94 permanent and 0 92 seasonal and pore water 0 84 permanent and 0 83 seasonal the p re translocation constant explained op in pore water with beta of 0 28 for both systems and in surface water with beta 0 20 permanent and beta 0 22 seasonal similar to the n concentrations also the op concentrations in the pore water were sensitive to changes in porosity beta 0 20 for both systems like nh4 the op concentrations in both surface and pore water increased with increasing belowground leaching constant beta values around 0 15 similar to nh4 and no3 the concentrations of op decreased with higher death constants beta values between 0 11 and 0 16 3 5 2 sensitivity analysis for nitrogen and phosphorus retention n retention was largely sensitive to the same parameters in both systems the most influential factor was the n re translocation constant beta values 0 48 and 0 52 for the permanently and seasonally flooded systems respectively fig 9i and j higher aboveground fragmentation and leaching constants led to lower n retention beta values 0 26 and 0 23 respectively permanent and 0 25 and 0 20 seasonal n retention increased with a higher aboveground death constant 0 28 permanent and 0 29 seasonal increasing inflow nh4 and no3 concentrations increased retention in both systems beta values 0 14 0 16 p retention fig 9i and j was mainly sensitive to the op concentration in the influent beta 0 25 permanent 0 33 seasonal and similar to n the p re translocation constant beta 0 39 permanent and 0 38 seasonal and the aboveground death constant beta 0 28 permanent and 0 29 seasonal p retention was also sensitive to increase in fragmentation and leaching constants beta 0 28 and 0 21 permanent and 0 26 and 0 18 seasonal however in contrast to n retention of mainly bgb and much less of agb fig 9i and j this was likely a consequence of the higher re translocation constant for p annex 3 3 6 n p ratio in the surface water the n p ratio coming into the wetland was 9 9 for permanent and 10 0 for seasonally flooded conditions table 3 without harvesting and with sustainable harvesting rates d25 a50 and a100 the tn tp ratio was reduced to values between 8 2 a100 and 9 6 no harvesting for both flooding conditions indicating that relatively more n than p was retained with overharvesting d35 the tn tp ratio increased to 10 3 the main change occurred with dissolved inorganic n and p din dip ratios dropped between 6 4 and 8 9 table 3 the particulate n to p ratio increased with no harvesting and annual harvesting a50 and a100 and did not change much with daily harvesting d25 and over harvesting d35 while the ratio of dissolved organic compounds was hardly affected by the wetland table 3 3 7 comparison with field data since the 1970s frequent measurements have been made across a range of wetlands on biomass water quality and productivity table 4 a wide range of agb and bgb values were reported in studies ranging from south africa to egypt and from sea level in the nile delta to 1883 m altitude at lake naivasha maximum values were higher in rooted systems table 4 water quality values vary from low concentrations for e g lake naivasha and the shire river in malawi gaudet 1975 in the 1970s to high concentrations in wetlands with wastewater input like namiiro uganda kipkemboi et al 2002 and nakivibo uganda kansiime er al 2007 unfortunately productivity values were reported from studies that did not include water quality data and could not be related to nutrient inputs productivity values above 20 g m 2 d 1 were reported at altitudes ranging from 700 upemba dr congo to 1883 at lake naivasha table 4 values simulated with the model for agb and bgb n and p concentrations in biomass water quality and net primary production of biomass were all well within the ranges found in the literature table 4 4 discussion and conclusion 4 1 model performance the current model focussed on n and p processes related to papyrus vegetation growth mortality nutrient uptake and release but also included microbiological and physico chemical processes the model studied the impact of different hydrological permanent and seasonal flooding and harvesting regimes on n and p retention the hydrology section enabled studying the effect of hydrological regimes independently for example by including or excluding backflow from the lake the development of the p model enabled studying retention at process level in comparison with n and simulating changes in n p ratios in the water from inflow to outflow comparison of model outcomes with results of field studies table 4 confirmed that the model produced reasonable estimates of biomass n and p in biomass productivity and the concentrations of nutrients in the water the simulated time of 11 months for re growth of agb fig 6i and j was realistic compared with literature values of 6 12 months muthuri et al 1989 kansiime and nalubega 1999 terer et al 2012b 4 2 n and p retention n and p retention were mainly a result of accumulation in dead biomass fig 8 both relative and absolute p retention 4 wt m 2 yr 1 and 0 6 g p m 2 yr 1 were lower than n retention 7 wt m 2 yr 1 and 11 g n m 2 yr 1 the absolute retention was lower due to lower p content in dead biomass and not compensated by p adsorption due to the low simulated p concentrations in the pore water gaudet 1977 also identified peat accumulation as the main mechanism for retention of n and p in floating papyrus but found higher values 65 g n m 2 and 0 7 g p m 2 yr 1 indicating either a higher mortality or lower decomposition rate than used for model simulation relative n retention also exceeded p retention this was caused by a relatively high amount of n n p 20 in dead agb compared with living agb n p 15 and to a lesser extent denitrification the relatively low amount of p in agb was caused by a higher re translocation constant 0 77 for p vs 0 7 for n higher resorption of p under p limiting conditions is not exceptional for tropical wetland macrophytes as shown for eleocharis cellulosa and typha domingensis in belize rejmánková 2005 rejmánková and snyder 2008 but this remains to be confirmed for cyperus papyrus dominated wetlands under the environmental conditions at lake naivasha simulated papyrus growth was not p limited nevertheless due to n fixation not currently modelled in the root zone gaudet 1979 boar et al 1999 p may still be the limiting nutrient in reality justifying a higher p re translocation given the considerable impact of re translocation on retention of n and p asaeda et al 2008 as confirmed by the sensitivity analysis fig 9 empirical research into re translocation rates in c papyrus is recommended 4 3 effect of moisture on nutrient cycling differences in moisture conditions in the wetland permanent soil saturation and standing water under permanent flooding unsaturated soil and absence of water during part of the year under seasonal flooding fig 5 led to differences in papyrus biomass nutrient retention and water quality under seasonal flooding biomass was lower than under permanent flooding fig 6 the low water level led to both n and p limitation in the model less uptake belowground and less translocation of n and p to the aboveground parts of the plant the difference in impact on agb compared with bgb was amplified by re translocation of n and p resulting from agb mortality the dependence of re translocation on water depth is known from eleocharis sphacelata asaeda et al 2008 with harvesting the impact of reduced water levels on papyrus biomass increased fig 6 seasonally flooded conditions led to higher retention fig 8 and lower no3 nh4 and op concentrations in the water fig 7 than permanent flooding due to the higher mortality in the dry season and the fast recovery during the wet periods of the year this leads to a higher net n and p uptake lower concentrations and a larger accumulation of dead agb higher retention the effects are small with the two hydrological scenarios presented here but likely of greater importance with longer dry periods and lower concentrations of n and p in the inflow while the temporary nutrient limitation in the dry season leads to a higher simulated net annual retention in reality this may be less under water stress re translocation would likely increase resulting in lower concentrations of n and p in agb asaeda et al 2008 and decomposition and n and p release would increase because of more aerobic conditions denitrification would also be lower with a longer dry period under the current seasonal flooding regime there was little difference fig 8a and c and with over harvesting denitrification was even higher under seasonal conditions as a result of a higher no3 concentration fig 6e f 8a and 8c 4 4 effects of harvesting harvesting increased both n 6 times and p retention 10 times and decreased the tn tp mass ratio of the water from 10 to 8 this is in line with findings on n and p retention in a c papyrus dominated wetland receiving wastewater runoff kanyiginya et al 2010 as total n was retained more than total p fig 8 the tn tp ratio in the water decreased this effect was stronger with harvesting however with overharvesting retention decreased and n p ratio increased fig 8 and table 3 the ratio of dissolved organic n to dissolved organic p did not change much and the ratio for particulate n to particulate p in the water was even higher in the outflow reduction in tn tp in the water therefore could be attributed to dissolved inorganic n p ratio particulate n p in the water increased due to mortality because n content in biomass is higher than the p content and therefore more particulate n is released in the outflow harvesting reduced the absolute mortality rate because of a reduction in biomass fig 6 and consequently the particulate n p in the wetland outflow the reduction of the inorganic n p ratio in the water is caused by the uptake of papyrus plants to replace vegetation that has died off with harvesting this increased as the demand for inorganic n and p increases for re growth and fast translocation from rhizome to agb as the uptake of n is higher than p the n p ratio in the water is reduced a higher n than p retention was also found in field experiments gaudet 1977 kansiime et al 2007 kanyiginya et al 2010 with over harvesting adsorption of op increased as a result of higher op concentrations in the pore water fig 7e and f and 8b and d explaining the higher n p ratio in the outflow compared with inflow table 3 intact wetlands would therefore reduce both n and p and push the system to be more n limited the tn tp outflow ratio without harvesting was 9 6 g g equivalent to a 21 3 n p molar ratio and with harvesting this decreased to 18 n p molar ratio this is in between values found for lake victoria 13 6 n p molar and lake malawi 28 4 n p molar guildford and hecky 2000 a tn tp molar ratio in the water below 20 is considered n limiting and favours blooms of n fixing cyanobacteria at high p concentrations as they outcompete non n fixing algal species guildford and hecky 2000 wetlands are valued for their water purifying characteristics costanza et al 2017 this model suggests that this ecosystem service can be enhanced by harvesting however while n and p retention increase with harvesting the ratio at which n and p are retained pushes the system to be more n limited making it more sensitive to cyanobacterial blooms on the other hand elser et al 2009 showed a shift towards more p limited lake systems in europe and north america as a result of increased n deposition for africa n deposition has been lower but is now increasing faster compared with europe and north america dentener et al 2006 4 5 sensitivity analysis the global sensitivity analysis method that was used here saltelli et al 2000 is novel in ecological modelling an advantage over the more commonly used oat approach was the identification of a set of parameters that relative to each other explained the outputs fig 9 this is valuable in identifying which parameters need further attention and which can be left as they are cariboni et al 2007 the method also provides quality assurance by facilitating discussion on the biological or physico chemical explanation behind the dependence of the output on the input parameters saltelli et al 2000 the relatively low r2 values for the models describing n and p retention may indicate that the oat pre screening did not identify the most influential parameters for retention it is worth investigating this further by looking at other global sa methods in a future study e g makler pick et al 2011 the results of the sensitivity analysis fig 9 illustrated mostly logical biologically meaningful relationships and confirmed the importance of re translocation as expected higher assimilation led to higher biomass and higher mortality to lower biomass for water quality higher re translocation led to higher concentrations in the water by keeping more n and p in the vegetation when it senesced and reducing uptake from the water similarly retention was lower with higher re translocation because of less n and p in aboveground dead biomass based on a study of macrophytes in wetlands in various regions and with different nutrient status rejmánková 2005 suggested that re translocation in emergent macrophytes depends on the inorganic n and p concentrations in the water with lower concentrations increasing re translocation as a survival strategy higher mortality led to higher retention more dead biomass and higher fragmentation and leaching to lower retention less accumulating dead biomass all parameters that define papyrus growth mortality and decay processes are also related to system conditions e g temperature which were not all modelled because some of the parameter values were derived from other wetland types or plant species or calibrated or estimated annex 3 empirical studies on papyrus systems are needed to obtain more evidence based values for these parameters other influential parameters such as inflow concentrations and porosity can be measured easily and the values used in the model were realistic small increases or decreases of no3 nh4 and op in the inflow led to longer or shorter periods of nutrient limitation which highlights the impact of inflow concentrations on water quality this impact was higher under permanent flooding conditions than with seasonal flooding with seasonal flooding there was more nutrient limitation higher mortality more re growth a higher n and p uptake and lower concentrations in the water compared with permanent flooding similarly retention was more sensitive to inflow concentrations with seasonal flooding when longer periods of nutrient limitation led to more mortality and more accumulation of dead biomass 4 6 model evaluation and future development an evaluation of the quality and credibility of a model encompasses the whole process of model development calibration analysis and application augusiak et al 2014 based on the available data from field studies of papyrus the numerical model used and the results of the sensitivity analysis and comparison with literature data it can be concluded that papyrus simulator provides a good representation of the main nutrient cycles in rooted papyrus wetlands and allows a better understanding of the impact of water regime and harvesting on n and p retention if validation means that a model is acceptable for its intended use rykiel 1996 the model could be considered validated however validation in the sense of model output corroboration augusiak et al 2014 would require a more rigorous comparison of model output with independent time series data from different wetland sites wider application of papyrus simulator to specific wetland sites for decision making purposes would require coupling of the model to a spatially defined hydrology model while currently good datasets from sufficiently long time periods are not available we hope that increasing availability of data from monitoring programmes and remote sensing will provide opportunities for further model development and a full validation the model may also be developed further in other directions in the current version of the model oxygen only influences nitrification and denitrification other processes e g fragmentation leaching and assimilation are also dependent on environmental factors like temperature oxygen ph and irradiance for application of the model to all papyrus wetlands from the middle east to southern africa knowledge of how these factors vary with altitude and longitude and how they influence biomass retention and water quality is crucial for example under the mediterranean climate serag 2003 found a low agb in winter low temperatures and radiation and high agb in summer high temperatures and radiation and vice versa for bgb the relationship between altitude and biomass is also complex temperature is lower at higher altitude naivasha in kenya at 1900 m on average below 20 c compared with the sudd wetland in south sudan at 400 m average 30 c but radiation may be higher which favours c4 photosynthesis higher temperature stimulates growth but also leads to higher respiratory losses jones and muthuri 1985 higher temperature may increase decomposition and therefore reduce retention but assimilation may also be higher if this would result in higher mortality this could increase retention the empirical data from literature did not reveal a clear relationship between altitude biomass and productivity table 4 which emphasizes the need for experiments targeting the processes related to growth and mortality 4 7 potential for application the potential application of papyrus simulator for scientists and decision makers can be local regional and global for a specific papyrus wetland the model can be linked to a site specific hydrological model and developed into a spatially explicit tool for spatial planning or economic valuation of n and p retention the model could also be applied to design constructed wetlands for wastewater treatment to predict n and p removal rates and evaluate harvesting regimes at a regional level the model can improve our understanding of n and p cycling and retention and how they are affected by changes in climate or land use identify knowledge gaps and help focus empirical research with a more elaborated carbon section the model can address questions on the role of papyrus wetlands in the carbon cycle and quantify their expected role as net carbon sink moomaw et al 2018 a comparison with growth models of other emergent plants e g phragmites australis and typha sp asaeda and karunaratne 2000 asaeda et al 2008 tanaka et al 2004 could identify strength and weaknesses of these respective modelling approaches and lead to mutual benefits combined with existing vegetation models and global hydrological and climate models papyrus simulator could contribute to a global model which quantifies ecosystem services that contribute to the sustainable development goals janse et al 2019 such a global model can quantify the loss of economic value from conversion of wetlands for food security conceição et al 2016 and provide evidence of impact on human well being 5 conclusion in conclusion papyrus simulator is suitable for studying processes related to n and p retention and the effect of harvesting in papyrus wetlands absolute and relative retention of p was lower than n retention and the main mechanism for both was peat accumulation this led to a predominantly n limited environment by reducing the n p ratio in the water absence of surface water during part of the year resulted in a reduction of biomass mainly aboveground harvesting increased retention from 7 to more than 40 for n and for p from 4 to 40 the global sensitivity analysis was successful in identifying the relative contribution of the model inputs to explaining model outputs the most influential parameters were related to assimilation mortality decay re translocation inflow concentrations and soil porosity the main mechanism for retention was peat formation which would be relatively unaffected when combined with seasonal agriculture and sustainable harvesting of papyrus but would be completely lost when wetlands are converted to agriculture more permanently papyrus simulator can contribute to a global modelling effort to quantify ecosystem services and contribute to achieving the sdgs related to food water climate and biodiversity declaration of competing interest none acknowledgments the authors would like to thank ihe delft institute for water education for time allocated to this research prof kenneth irvine ihe delft for his support in general and his valuable reviews of various versions of the manuscript and two anonymous reviewers for their constructive feedback that helped to improve the quality of the manuscript considerably appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104531 
26123,papyrus cyperus papyrus wetlands support millions through food provisioning which leads to loss of regulating ecosystem services this study aimed at understanding the impact of changes in water regime and vegetation harvesting on nutrient retention in rooted papyrus wetlands a simulation model papyrus simulator developed and calibrated with data from african wetlands produced reasonable estimates of productivity and nutrient retention phosphorus retention was lower than nitrogen retention leading to a nitrogen limited environment by reducing the n p ratio in the water absence of surface water during part of the year caused a reduction of biomass harvesting increased nitrogen retention from 7 to over 40 and phosphorus retention from 4 to 40 sensitivity analysis revealed assimilation mortality decay re translocation nutrient inflow and soil porosity as the most influential factors papyrus simulator is suitable for studying nutrient retention and harvesting in wetlands and contributes to quantification of ecosystem services and sustainable wetland management keywords papyrus nitrogen retention phosphorus retention modelling ecosystem services wetlands 1 introduction in africa where so many people depend directly on wetlands for their livelihoods schuyt 2005 maclean et al 2014 population growth climate change the need for food security conceição et al 2016 the suitability of wetlands for food production and weak implementation of wetland conservation policies create an enormous pressure on wetlands rebelo et al 2010 davidson 2014 current per capita food production in africa is at the level of the 1960s pretty et al 2011 more than one in four africans are undernourished undp 2012 pretty et al 2011 defined sustainable agricultural intensification as producing more output from the same area of land while reducing negative environmental impacts and increasing contributions to natural capital and the value of environmental services and identified nutrient cycling as one of its attributes to apply this concept to agriculture in and around wetlands more knowledge is needed about the dynamics of nutrient retention both in relation to human activities e g agriculture and vegetation harvesting or extreme weather events due to climate change and to natural variation in wetland processes e g seasonal changes in hydrology this knowledge can be used to develop more effective management practices to increase food security while protecting important ecosystem services the still widespread papyrus cyperus papyrus wetlands in east africa van dam et al 2014 ajwang ondiek et al 2016 provide a wide range of ecosystem services these wetlands support the livelihoods of millions of people through provisioning services like seasonal agriculture papyrus harvesting drinking water clay and sand mining fishing and fuel production morrison et al 2012 2014 jones et al 2016 van dam and kipkemboi 2018 the regulating services in these wetlands reduce the runoff of nutrients sediments and trace elements into lakes and rivers kansiime et al 2007 and regulate water quantity flood protection and water storage and local climate e g by influencing rainfall patterns due to its c4 photosynthesis papyrus vegetation can sequester up to 0 48 kg c m 2 y 1 storing up to 8 8 kg c m 2 in biomass and 64 kg c m 2 in detrital and peat deposits saunders et al 2007 2014 after clearing the aboveground biomass a full stand of papyrus can grow back within 6 9 months muthuri et al 1989 terer et al 2012a 2012b this high productivity provides opportunity for seasonal crop production in the dry season with recovery of the papyrus vegetation through rhizomes and continuation of regulating services like nitrogen and phosphorus retention in the wet season van dam et al 2014 the processes underlying nitrogen n and phosphorus p retention accumulation of organic matter uptake nitrification denitrification adsorption depend on conditions resulting from the hydrology and the presence of vegetation in the wetland and their interaction powers et al 2012 permanently flooded areas favour peat formation and denitrification while seasonally flooded zones favour nitrification and are more prone to anthropogenic disturbance in papyrus between 45 and 105 g n m 2 is stored in living biomass and 105 457 g n m 2 in dead biomass and peat gaudet 1977 gaudet and muthuri 1981 boar et al 1999 boar 2006 potential denitrification in floating papyrus in lake naivasha ranged from 2 3 to 6 4 g n m 2 y 1 viner 1982 reported values for p storage in living biomass were 5 4 g p m 2 and in detritus and peat 3 57 g p m 2 gaudet 1977 gaudet and muthuri 1981 boar 2006 for p adsorption kelderman et al 2007 found a maximum adsorption of 4 mg p g 1 sediment in kirinya wetland uganda lakes are often downstream of papyrus wetlands and guildford and hecky 2000 found lake victoria to be n limited and therefore favouring n fixing cyanobacterial blooms at higher p concentrations lakes are more often n limited but not exclusively and limitation can shift from n to p with increased global n deposition rates elser et al 2009 it is therefore crucial to understand n and p cycling in papyrus wetlands and how wetlands influence the n p ratio in runoff water simulation models can help to improve understanding of the complex dynamics of nutrient retention as influenced by natural e g seasonal hydrology and anthropogenic e g agriculture and harvesting drivers of change earlier model studies analysed the retention capacity of lake victoria fringing wetlands with a focus on processes for n p and organic matter mwanuzi et al 2003 and n processes for floating papyrus wetlands van dam et al 2007 while some studies have concluded that papyrus wetlands are effective in removing or retaining n and p e g up to 35 of total n from the inflowing water kanyiginya et al 2010 and inorganic p up to 90 mwanuzi et al 2003 the understanding of underlying processes is still limited there is no evidence that these conclusions are valid over a longer period or merely a result of temporal n and p storage besides simulating changing conditions on a local scale e g vegetation harvesting process models can improve regional and global models to estimate the impact of human activities on biodiversity and ecosystem services sjögersten et al 2014 janse et al 2015 beusen et al 2016 costanza et al 2017 in a previous study we constructed a simulation model for n cycling in natural rooted papyrus wetlands hes et al 2014 the model now called papyrus simulator showed that n retention increased from 13 at low harvesting to a maximum of 50 per year at intermediate harvesting rates if harvesting was increased further n retention dropped dramatically 5 as harvesting exceeded regrowth the model did not incorporate p and was not able to compare permanently and seasonally flooded zones independently the overall objective of the current study is to understand the impact of changes in water regime and papyrus harvesting on n and p retention in rooted papyrus wetlands the specific objectives are 1 to further develop papyrus simulator by incorporating p processes and a hydrology section that enables independent comparison of different hydrological conditions and make it generally applicable to papyrus dominated wetlands 2 to conduct a sensitivity analysis to identify which processes and parameters are most important in retaining n and p and to which parameters the model is most sensitive and 3 to compare papyrus simulator outputs with published field data 2 methods 2 1 model description and development the original papyrus simulator described a rooted papyrus wetland hes et al 2014 and modelled the influence of papyrus growth and harvesting under different hydrological conditions on the cycling of n fig 1 to achieve this the model comprised three interacting sections hydrology carbon and nitrogen fig 2 in the hydrology section water level and soil moisture were calculated based on a simple water balance model soil moisture conditions determined the factor mode representing conditions from anaerobic to aerobic which influenced the nitrification and denitrification rates in the carbon section the assimilation by papyrus was estimated on the basis of maximum photosynthetic rate and irradiance and limited by the n concentration in the papyrus biomass other processes in the carbon section were mortality fragmentation leaching and harvesting biomass c and n content were linked through an optimum c n ratio so that changes in carbon for example when harvesting occurs led to proportional changes in the amount of n in the n section of the model in the n section mortality of papyrus biomass led to dead organic n which was further degraded to nh4 which could be converted to no3 and enter denitrification depending on moisture conditions in the current study papyrus simulator was developed further in two areas a phosphorus p section was added 1 and the model was generalized by merging the original seasonal and permanent wetland zones based on lake naivasha into one wetland zone that can be inundated based on the prevailing conditions of surface water inflow a combination of stream and overland flows in the model called river inflow lake inflow in the model this can be switched on and off depending on whether a seasonally inundated or permanently inundated wetland is simulated and precipitation 2 all state and rate variables processes of the new model are given in annex 1 and 2 respectively with their numbers between brackets in the text all other parameters and variables are given in annex 3 the key processes and conceptual diagrams of the four sections fig 2a are presented and described below 2 2 hydrology the hydrology section fig 2b calculates the water level as the volume of water per m2 of surface area based on inflow from river discharge outflow to and backflow from the lake precipitation evapotranspiration and groundwater recharge backflow from the lake occurs when the sum of water outputs is greater than the sum of water inputs keeping the surface water level constant at 0 5 m 72 and can be switched off to simulate wetlands that have an outflow but do not receive backflow allowing the surface water to drop in the dry season when the inputs are higher than the outputs the difference flows to the lake keeping the surface water level at a maxmimum of 0 5 m 100 lake naivasha conditions gaudet 1979 were used as a basis to describe an annual discharge precipitation and evapotranspiration regime groundwater recharge is modelled with first order equations resulting in a maximum recharge of 6 mm day 1 when soil is saturated 133 the factor mode based on van der peijl and verhoeven 1999 is calculated as the proportion of soil porosity that is filled with water annex 3 ranging from 0 soil completely saturated anaerobic to 1 soil at field capacity or less aerobic conditions mode influences nitrification and denitrification rates in the soil pore water conditions in the surface water are assumed to be aerobic the soil depth considered in the model was 0 2 m 2 3 carbon papyrus biomass is modelled as carbon c in aboveground biomass agb culms and umbels and belowground biomass bgb rhizome and roots fig 2 carbon in agb cagb results from net growth assimilation minus mortality minus respiration and translocation of c to and from the rhizome assimilation 42 was described using a logistic model depending on total aboveground biomass and limited by irradiance and the n and p concentrations in the plant both described by monod type equations a s s i m i l a t i o n max a s s i m i l a t i o n c o n s t a n t c a g b 1 c a g b c c o n c a g b k a s s i m r a d i a n c e r a d i a n c e k r a d i a n c e l i m i t n a s s l i m i t p a s s 0 81 in which max assimilation constant is the maximum relative assimilation rate day 1 cagb is carbon in agb g c m 2 c conc agb is carbon in agb g c g 1 dw k assim is papyrus biomass at which assimilation stops g dw m 2 radiance is irradiance mj m 2 day 1 k radiance is the half saturation constant of irradiance for assimilation mj m 2 day 1 limit n ass and limit p ass are limiting factors of n and p respectively for c assimilation annex 3 the factor 0 81 ensures that maximum growth limitation can be reached van der peijl and verhoeven 1999 at n or p concentrations in papyrus below the minimum needed for assimilation 1 6 10 3 g n g 1 dw and 8 10 5 g p g 1 dw no growth is assumed so a monod type function with a cut off is used van der peijl and verhoeven 1999 the dead agb is fragmented and hydrolysed according to first order kinetics 48 and 113 respiration in agb 45 is the sum of maintenance respiration proportional to biomass and growth respiration proportional to assimilation as c a g b r e s p i r a t i o n c a g b m a i n t c o e f f a g b a s s i m i l a t i o n g r o w t h c o e f f a g b in which maint coeff agb is the maintenance respiration coefficient for agb day 1 and growth coeff agb is the respiration coefficient for assimilation translocation of c between agb and bgb 41 is based on an assumed optimal ratio between aboveground and belowground c c agb to bgb optimal ratio annex 3 of 1 2 boar et al 1999 jones and humphries 2002 whenever c ratio differs from this optimal ratio c is translocated to restore the optimum i f c a g b t o b g b o p t i m a l r a t i o c a g b c b g b t h e n t r a n s l o c a t i o n t l d o w n m a x c c a g b c b g b c a g b c b g b k t l d o w n c i f c a g b t o b g b o p t i m a l r a t i o c a g b c b g b t h e n t r a n s l o c a t i o n t l u p m a x c c b g b c a g b c b g b c a g b k t l u p c in which cagb and cbgb are c in agb and bgb respectively g c m 2 c agb to bgb optimal ratio is the optimal ratio between c in agb and bgb tldownmax c is the maximum rate for translocation of c from agb to bgb g c day 1 tlupmax c is the maximum rate for translocation of carbon from bgb to agb g c day 1 ktldown c is the half saturation constant for downward translocation and ktlup c is the half saturation constant for upward translocation carbon in bgb cbgb is reduced by mortality following a first order equation depending on cbgb and a cbgb death constant 46 the dead bgb is fragmented and hydrolysed according to first order equations 50 and 112 respiration by bgb 47 is calculated as for agb with growth respiration proportional to downward translocation and maintenance respiration proportional to cbgb 2 4 nitrogen the n section fig 3 expresses the same aboveground and belowground alive and dead biomass compartments as the carbon model expressed in g m 2 of n added to these are the main components of the n cycle in the wetland particulate and dissolved organic n nh4 and no3 nitrate and ammonium are taken up by the bgb 84 and 94 and then passed on to the agb by translocation 74 n in dead biomass is re translocated 73 or passes through fragmentation 78 and 80 hydrolysis 116 and 119 mineralisation 56 and 59 nitrification 90 and 91 and denitrification 52 exchange of n between the aboveground and belowground layers takes place through diffusion 53 82 and 92 driven by concentration differences of soluble compounds no3 nh4 and dissolved organic n and settling of particulate organic n 115 the uptake of nh4 and no3 by papyrus 84 and 94 depends on the carrying capacity for papyrus and the concentration of n in the biomass and is limited by the concentration of nh4 and no3 respectively this limitation is modelled with a monod type equation only the equation for nh4 uptake is given here for illustration n h 4 u p t a k e m a x n h 4 u p t a k e n b i o m a s s 1 n b i o m a s s n max b i o m a s s n h 4 p c o n c n h 4 p c o n c k n h 4 in which max nh4 uptake is the maximum uptake rate of nh4 by papyrus day 1 n biomass is the amount of n in agb and bgb g n m 2 n max biomass the maximum amount of n stored in agb and bgb g n m 2 nh4p conc is the nh4 concentration in the pore water g n m 3 and k nh4 is a half saturation constant g n m 3 n max biomass n max agb n max bgb was calculated based on literature values for n content in both agb and bgb as 0 013 and 0 008 g n g 1 dw respectively annex 3 and the maximum papyrus density of agb and bgb found in literature 8118 g dw m 2 muthuri et al 1989 jones and muthuri 1997 2 5 phosphorus the p section is similar in structure to the n section fig 4 with the main exception being the adsorption and release of p to the sediment 31 and 32 these are modelled with a langmuir equation and a factor that decreases adsorption and increases the release rate when the amount of adsorbed p approaches the maximum of adsorbable p van der peijl and verhoeven 1999 i f o p a d s o p a d s e q t h e n a d s o r p t i o n 1 o p a d s o p a d s m a x o p a d s e q o p a d s i f o p a d s o p a d s e q t h e n r e l e a s e o p a d s o p a d s m a x o p a d s o p a d s e q in which opads is the amount of p adsorbed g p m 2 opads max is the maximum amount that can be adsorbed g p m 2 and opads eq is the amount adsorbed in equilibrium with the orthophosphate op concentration g p m 2 2 6 model assumptions and implementation it was assumed that the distribution of water in the wetland is uniform without preferential flow floating papyrus mats at the edge of the lake were not considered because harvesting is assumed to take place in rooted papyrus zones all elements needed for growth that are not included in the model were assumed not to be limiting uptake of no3 nh4 and op are not taking place when pore water is not available the model currently does not include ammonia volatilization n fixation and n deposition considerations were discussed in hes et al 2014 the model was implemented in stella 10 0 6 isee systems inc lebanon nh us and run for a period of 5 years with rectangular euler integration and a time step of 0 0625 days 1 5 h for 1 m2 of wetland a complete listing of the equation layer of the stella model is given in annex 4 2 7 parameterization and calibration the model was parameterized and calibrated with data from the literature for lake naivasha annex 1 and 3 when values from lake naivasha were not available data from other east african papyrus wetlands were used for parameters that were never studied or measured in papyrus wetlands literature values from other wetland types were used or estimated for details see annex 1 and 3 seasonal variability solar radiation and hydrological inputs in the lake naivasha wetland was described using monthly averages from the period 1970 1982 for irradiance muthuri et al 1989 and 1974 1976 for evaporation and precipitation gaudet 1979 and river inflow evaporation was multiplied with a factor 1 25 to estimate evapotranspiration for papyrus saunders et al 2007 monthly river inflow based on the flow regime of the malewa river gaudet 1979 was calibrated to achieve realistic flow rates and n and p concentrations with a main rainy season in the months march may and a short rainy season in december evapotranspiration 4 1 6 2 mm day 1 and precipitation 0 1 6 2 mm day 1 varied throughout the year fig 5 c two flooding conditions were simulated in the permanently flooded wetland zone it was assumed that there was a backflow from the lake of between 0 and 0 12 m3 m 2 day 1 whenever river flow and rainfall were low in the seasonally flooded zone it was assumed that no lake backflow occurred resulting in lower water levels during periods with low river flow and rainfall this hydrological regime was repeated annually for the 5 year simulation fig 5 2 8 retention and harvesting absolute n and p retention g n or p m 2 yr 1 were calculated as ninfow noutflow and pinflow poutflow both over the 5th year when the model had stabilised fig 6 a ninflow was the sum of n in river discharge 55 58 85 87 95 97 117 and 120 and n flowing in from the lake 54 83 93 and 114 and noutflow was the sum of n outflow to the lake 60 88 98 and 121 and groundwater recharge 57 86 96 and 118 pinflow 34 37 38 63 66 67 123 127 and 129 and poutflow 36 39 65 69 126 and 130 were calculated in the same way relative retention wt n or p m 2 yr 1 was also calculated over the 5th year as ninfow noutflow ninflow and pinflow poutflow pinflow two types of harvesting scenarios were applied 44 76 and 104 daily harvesting in g papyrus dw m 2 d 1 and annual harvesting in a percentage of standing biomass the annual harvest took place during the dry season on day 230 when the surface water levels go down fig 5a and seasonal agriculture is most likely 2 9 sensitivity analysis initially a one at a time oat local sensitivity analysis was done by running the model with calibrated initial values of state variables and model parameters and values of 10 and 10 of each of these the effect of these variations on the output variables papyrus biomass nutrient concentrations in surface water nutrient retention were observed and 28 out of 81 parameters to which the model was most sensitive were selected for a global sensitivity analysis based on the approach outlined in saltelli et al 2000 for each of the 28 parameters a range of possible values minimum maximum was determined based on what was assessed as being most realistic table 1 and a rectangular distribution was set within the sensitivity settings of stella the model was then run 500 times both for permanently and seasonally flooded conditions with parameter values drawn from the 28 distributions for each run as output variables the following ten were selected papyrus agb papyrus bgb both in g dw m 2 nh4 n no3 n and op p in the surface and pore water in g n or p and n and p retention for year 5 in g m 2 y 1 for the first eight variables the mean value for the inundated period days 32 322 of year 5 of the simulation run was computed for n and p retention the end values of year 5 were used the resulting output dataset of two times 500 model runs with combinations of random parameters and output variables was submitted to multiple regression analysis with the output variable as dependent variable for each regression model that had a sufficiently high coefficient of determination preferably r2 0 7 standardized regression coefficients or beta weights were calculated and compared to assess the contribution of each input variable in explaining the variation in the output variable as a measure of sensitivity of the output variables to the inputs only model parameters that had a significant regression coefficient t test p 0 05 were included all regression models were calculated using functions lm and lm beta in r version 3 5 0 r core team 2018 2 10 comparison with field data to compare the simulation results with field data papyrus wetland studies that reported the main output variables of the model agb and bgb n and p concentrations in biomass water quality net primary production of biomass were reviewed wetland characteristics location altitude and type floating or rooted were also included as the model was aimed at papyrus wetlands in general and not at one wetland site in particular model output was compared with the ranges of values found in the literature 3 results 3 1 water levels in and outflows and factor mode under permanently flooded conditions the surface water level was constant at 0 5 m above the substrate fig 5a resulting in a value of 0 for the controlling factor mode fig 5b implicating saturation and anaerobic conditions in the pore water under seasonally flooded conditions the surface water level was 0 5 m during the rainy season and dropped to zero in the dry season while the pore water was below saturation for part of the dry season fig 5a this resulted in peaks of the mode factor fig 5b in the dry season when the sum of river inflow and rainfall was smaller than the combined evapotranspiration and recharge the seasonal wetland did not receive backflow from the lake while the permanent wetland did fig 5d and e 3 2 above and belowground biomass and effects of harvesting the simulated papyrus biomass reached a maximum of 8127 g dw m 2 in year 5 when the model was assumed stable belowground biomass bgb remained higher than aboveground biomass agb fig 6a and b without harvesting and under permanently flooded conditions agb and bgb fluctuated annually between 3809 and 3824 g dw m 2 and 4287 and 4304 g dw m 2 respectively with seasonally flooded conditions fluctuation was slightly higher 3798 3823 g dw m 2 agb and 4276 4303 g dw m 2 bgb once per year there was a small drop in biomass coinciding with the dry season figs 5a and 6b the agb bgb ratio remained similar at 0 9 throughout the year in both zones when a harvesting rate of 25 g dw m 2 d 1 of agb was applied the negative impact of the dry season on agb and bgb increased and total biomass was reduced by 13 37 in the permanent zone and 14 39 in the seasonal zone fig 6c and d if harvesting was increased to 35 g dw m 2 d 1 the agb was reduced to zero after only 3 years in the seasonally flooded wetland and about 20 days later in the permanently flooded wetland with a one time annual harvest of 50 or 100 the biomass recovered fully in about 11 months in contrast with the daily harvesting scenarios the agb bgb ratio changed during the recovery period increasing from very low just after the harvest to the original ratio 0 9 after recovery for the 100 harvest the agb bgb ratio increased to more than unity before returning to the original value fig 6i and j this was a result of temporary slow recovery of bgb owing to nutrient limitation during the absence of river inflow fig 5d e 7i and 7 j 3 3 effects of harvesting on surface water concentrations of no3 nh4 and op under permanently flooded conditions the concentrations in year 5 of no3 n nh4 n and op p in the surface water reached a peak of 1 6 3 0 and 0 5 g m 3 respectively fig 7a these peaks coincided with the wet season fig 5 the lowest concentrations in year 5 of zero no3 n 0 2 nh4 n and zero op g m 3 occurred at the end of the year just before the start of the wet season and at the end of a period with low n and p inputs from river inflow fig 5d e and 7a the patterns in the permanently flooded and seasonally flooded systems were similar as were maximum concentrations recorded the lowest concentrations were close to zero due to the absence of input of n and p from the lake during the dry seasons fig 7b with a harvesting rate of 25 g dw m 2 d 1 under permanently flooded conditions the highest concentrations of no3 n 0 8 g m 3 nh4 n 1 2 g m 3 and op 0 2 g m 3 dropped and occurred earlier after the start of the dry season compared with no harvesting fig 7c for seasonally flooded conditions nutrient concentrations were similar fig 7d when harvesting was increased to 35 g dw m 2 d 1 under permanently flooded conditions the minimum concentration in year 5 for all compounds were higher than without harvesting no3 n 2 0 nh4 n 2 3 and op 0 4 all g m 3 maxima of no3 3 2 g n m 3 and op 0 5 g p m 3 occurred at the end of year 5 ammonium maxima 3 1 g n m 3 occurred early in the year under seasonally flooded conditions the effects on the lowest concentrations were the same as under permanently flooded conditions for no3 and op but higher for nh4 3 0 g n m 3 the highest concentrations for no3 and nh4 were around 5 g n m 3 just before and after the dry periods when surface water dropped fig 7e and f when 50 of the agb was harvested under both hydrological conditions all nutrient concentrations dropped from the moment of harvesting to the point of biomass recovery fig 6g h 7g and 7h with 100 harvesting the trend was similar under both seasonally and permanently flooded conditions just after the harvest there was an increase in all concentrations which was slightly lower under seasonally flooded conditions due to the absence of backflow with nutrients from the lake after this increase concentrations decreased and remained lower compared with the situation without harvesting until the biomass recovered fig 6i j 7i and 7j 3 4 effects of harvesting on n and p retention n and p retention increased with increasing harvesting rates in both permanently and seasonally flooded conditions until a dramatic drop when the papyrus was over harvested fig 6e and f and 8 for the permanently flooded wetland this point for both n and p was at a harvesting rate of about 34 g dw m 2 d 1 in seasonally flooded conditions this was similar at around 33 g dw m 2 d 1 n retention under both flooding conditions ranged from 10 g n m 2 yr 1 without harvesting to 67 g n m 2 yr 1 at a harvesting rate of 33 g dw m 2 d 1 dropping to a net maximum release of 3 9 g n m 2 yr 1 under permanently flooded conditions under seasonally flooded conditions n retention was 12 g n m 2 yr 1 without harvesting increasing to 66 g n m 2 yr 1 and then falling to a release of 3 4 g n m 2 yr 1 at higher harvesting rates p retention increased from 0 6 to 6 3 and from 0 6 to 6 2 g p m 2 yr 1 in permanently and seasonally flooded conditions respectively thereafter declining with faster harvesting to around zero for both conditions without harvesting fig 8 n and p were retained mainly in accumulating dead agb 7 1 g n m 2 yr 1 and 0 3 g p m 2 yr 1 and 8 7 g n m 2 yr 1 and 0 4 g p m 2 yr 1 for permanent and seasonal systems respectively to a lesser extent n and p were also retained in dead bgb and in particulate and dissolved organic matter with increased harvesting rates the accumulation in organic matter n and p gradually decreased while overall retention increased as a consequence of uptake by recovering papyrus fig 8 denitrification was marginal in both zones until papyrus was overharvested when it increased to 1 8 g n m 2 yr 1 permanent and 2 0 g n m 2 yr 1 seasonal and became the sole process responsible for retaining or more correctly removing n fig 8a and c the denitrification was slightly higher under seasonal conditions due to the higher no3 concentrations fig 7e and f compared with permanent flooding conditions and the short aerobic period under seasonally flooded conditions fig 5b at harvesting rates leading to the absence of papyrus fig 6e and f the main factor for p retention was adsorption at 1 0 g p m 2 yr 1 in both systems fig 8 b and d the values taken are from the 34 g dw m 2 d 1 harvesting scenario when harvesting was increased further adsorption rates were lower 3 5 sensitivity analysis table 2 shows the results of the twenty regression models and the ten output dependent variables for both permanently flooded and seasonally flooded systems all models were significant but the four models for n and p retention had adjusted r2 below 0 7 0 53 0 60 despite this all models where used for further analysis 3 5 1 sensitivity analysis for biomass outputs there was little difference between responses of biomass under permanently and seasonally flooded conditions and between agb and bgb fig 9 a and b biomass responded most positively to an increase in the maximum assimilation constant and the k assimilation with beta values between 0 46 and 0 55 for both constants responses where higher for permanent than for seasonally flooded conditions and for agb than for bgb positive responses beta between 0 10 and 0 15 were also observed with increased nh4 and no3 concentrations in the inflow for agb and bgb and for n translocation constant on bgb for both conditions biomass responded negatively to an increased aboveground death constant beta between 0 35 and 0 41 with the responses to bgb more negative than to agb and also slightly more negative for seasonal than for permanently flooded conditions for n fig 9c d e and f the differences between dry and wet conditions were modest the concentrations in the incoming river water had the highest impact on the concentrations in both wetland systems especially on surface water concentrations for surface water nh4 river concentrations explained the nh4 concentration in the surface water with a beta value of 0 85 and 0 79 for permanently flooded and seasonally flooded conditions respectively for the pore water concentrations the beta values were lower 0 69 and 0 65 the incoming no3 concentrations positively influenced no3 surface water concentrations in both systems beta 0 77 for permanent and beta 0 74 for seasonal and in the pore water beta values of 0 62 and 0 60 respectively after the river concentrations the nh4 and no3 concentrations were most sensitive to changes in the n re translocation constant beta values for the re translocation constant influencing no3 concentrations were higher than for nh4 and re translocation beta values in pore water for both no3 and nh4 concentrations were higher than those in surface water fig 9c d e and f there were also positive responses betas around 0 20 of the nh4 concentration in surface and pore water in both systems to an increased belowground leaching constant higher soil porosity dilution resulted in lower no3 and nh4 concentrations in the pore water with betas around 0 15 increased death constants for agb and more so for bgb had a negative effect on both n species but more on no3 than nh4 fig 9c d e and f for p there was also not much difference in sensitivity between the permanently and seasonally flooded systems fig 9g and h the op concentrations in the wetland were most sensitive to the river concentrations for surface water with beta values of 0 94 permanent and 0 92 seasonal and pore water 0 84 permanent and 0 83 seasonal the p re translocation constant explained op in pore water with beta of 0 28 for both systems and in surface water with beta 0 20 permanent and beta 0 22 seasonal similar to the n concentrations also the op concentrations in the pore water were sensitive to changes in porosity beta 0 20 for both systems like nh4 the op concentrations in both surface and pore water increased with increasing belowground leaching constant beta values around 0 15 similar to nh4 and no3 the concentrations of op decreased with higher death constants beta values between 0 11 and 0 16 3 5 2 sensitivity analysis for nitrogen and phosphorus retention n retention was largely sensitive to the same parameters in both systems the most influential factor was the n re translocation constant beta values 0 48 and 0 52 for the permanently and seasonally flooded systems respectively fig 9i and j higher aboveground fragmentation and leaching constants led to lower n retention beta values 0 26 and 0 23 respectively permanent and 0 25 and 0 20 seasonal n retention increased with a higher aboveground death constant 0 28 permanent and 0 29 seasonal increasing inflow nh4 and no3 concentrations increased retention in both systems beta values 0 14 0 16 p retention fig 9i and j was mainly sensitive to the op concentration in the influent beta 0 25 permanent 0 33 seasonal and similar to n the p re translocation constant beta 0 39 permanent and 0 38 seasonal and the aboveground death constant beta 0 28 permanent and 0 29 seasonal p retention was also sensitive to increase in fragmentation and leaching constants beta 0 28 and 0 21 permanent and 0 26 and 0 18 seasonal however in contrast to n retention of mainly bgb and much less of agb fig 9i and j this was likely a consequence of the higher re translocation constant for p annex 3 3 6 n p ratio in the surface water the n p ratio coming into the wetland was 9 9 for permanent and 10 0 for seasonally flooded conditions table 3 without harvesting and with sustainable harvesting rates d25 a50 and a100 the tn tp ratio was reduced to values between 8 2 a100 and 9 6 no harvesting for both flooding conditions indicating that relatively more n than p was retained with overharvesting d35 the tn tp ratio increased to 10 3 the main change occurred with dissolved inorganic n and p din dip ratios dropped between 6 4 and 8 9 table 3 the particulate n to p ratio increased with no harvesting and annual harvesting a50 and a100 and did not change much with daily harvesting d25 and over harvesting d35 while the ratio of dissolved organic compounds was hardly affected by the wetland table 3 3 7 comparison with field data since the 1970s frequent measurements have been made across a range of wetlands on biomass water quality and productivity table 4 a wide range of agb and bgb values were reported in studies ranging from south africa to egypt and from sea level in the nile delta to 1883 m altitude at lake naivasha maximum values were higher in rooted systems table 4 water quality values vary from low concentrations for e g lake naivasha and the shire river in malawi gaudet 1975 in the 1970s to high concentrations in wetlands with wastewater input like namiiro uganda kipkemboi et al 2002 and nakivibo uganda kansiime er al 2007 unfortunately productivity values were reported from studies that did not include water quality data and could not be related to nutrient inputs productivity values above 20 g m 2 d 1 were reported at altitudes ranging from 700 upemba dr congo to 1883 at lake naivasha table 4 values simulated with the model for agb and bgb n and p concentrations in biomass water quality and net primary production of biomass were all well within the ranges found in the literature table 4 4 discussion and conclusion 4 1 model performance the current model focussed on n and p processes related to papyrus vegetation growth mortality nutrient uptake and release but also included microbiological and physico chemical processes the model studied the impact of different hydrological permanent and seasonal flooding and harvesting regimes on n and p retention the hydrology section enabled studying the effect of hydrological regimes independently for example by including or excluding backflow from the lake the development of the p model enabled studying retention at process level in comparison with n and simulating changes in n p ratios in the water from inflow to outflow comparison of model outcomes with results of field studies table 4 confirmed that the model produced reasonable estimates of biomass n and p in biomass productivity and the concentrations of nutrients in the water the simulated time of 11 months for re growth of agb fig 6i and j was realistic compared with literature values of 6 12 months muthuri et al 1989 kansiime and nalubega 1999 terer et al 2012b 4 2 n and p retention n and p retention were mainly a result of accumulation in dead biomass fig 8 both relative and absolute p retention 4 wt m 2 yr 1 and 0 6 g p m 2 yr 1 were lower than n retention 7 wt m 2 yr 1 and 11 g n m 2 yr 1 the absolute retention was lower due to lower p content in dead biomass and not compensated by p adsorption due to the low simulated p concentrations in the pore water gaudet 1977 also identified peat accumulation as the main mechanism for retention of n and p in floating papyrus but found higher values 65 g n m 2 and 0 7 g p m 2 yr 1 indicating either a higher mortality or lower decomposition rate than used for model simulation relative n retention also exceeded p retention this was caused by a relatively high amount of n n p 20 in dead agb compared with living agb n p 15 and to a lesser extent denitrification the relatively low amount of p in agb was caused by a higher re translocation constant 0 77 for p vs 0 7 for n higher resorption of p under p limiting conditions is not exceptional for tropical wetland macrophytes as shown for eleocharis cellulosa and typha domingensis in belize rejmánková 2005 rejmánková and snyder 2008 but this remains to be confirmed for cyperus papyrus dominated wetlands under the environmental conditions at lake naivasha simulated papyrus growth was not p limited nevertheless due to n fixation not currently modelled in the root zone gaudet 1979 boar et al 1999 p may still be the limiting nutrient in reality justifying a higher p re translocation given the considerable impact of re translocation on retention of n and p asaeda et al 2008 as confirmed by the sensitivity analysis fig 9 empirical research into re translocation rates in c papyrus is recommended 4 3 effect of moisture on nutrient cycling differences in moisture conditions in the wetland permanent soil saturation and standing water under permanent flooding unsaturated soil and absence of water during part of the year under seasonal flooding fig 5 led to differences in papyrus biomass nutrient retention and water quality under seasonal flooding biomass was lower than under permanent flooding fig 6 the low water level led to both n and p limitation in the model less uptake belowground and less translocation of n and p to the aboveground parts of the plant the difference in impact on agb compared with bgb was amplified by re translocation of n and p resulting from agb mortality the dependence of re translocation on water depth is known from eleocharis sphacelata asaeda et al 2008 with harvesting the impact of reduced water levels on papyrus biomass increased fig 6 seasonally flooded conditions led to higher retention fig 8 and lower no3 nh4 and op concentrations in the water fig 7 than permanent flooding due to the higher mortality in the dry season and the fast recovery during the wet periods of the year this leads to a higher net n and p uptake lower concentrations and a larger accumulation of dead agb higher retention the effects are small with the two hydrological scenarios presented here but likely of greater importance with longer dry periods and lower concentrations of n and p in the inflow while the temporary nutrient limitation in the dry season leads to a higher simulated net annual retention in reality this may be less under water stress re translocation would likely increase resulting in lower concentrations of n and p in agb asaeda et al 2008 and decomposition and n and p release would increase because of more aerobic conditions denitrification would also be lower with a longer dry period under the current seasonal flooding regime there was little difference fig 8a and c and with over harvesting denitrification was even higher under seasonal conditions as a result of a higher no3 concentration fig 6e f 8a and 8c 4 4 effects of harvesting harvesting increased both n 6 times and p retention 10 times and decreased the tn tp mass ratio of the water from 10 to 8 this is in line with findings on n and p retention in a c papyrus dominated wetland receiving wastewater runoff kanyiginya et al 2010 as total n was retained more than total p fig 8 the tn tp ratio in the water decreased this effect was stronger with harvesting however with overharvesting retention decreased and n p ratio increased fig 8 and table 3 the ratio of dissolved organic n to dissolved organic p did not change much and the ratio for particulate n to particulate p in the water was even higher in the outflow reduction in tn tp in the water therefore could be attributed to dissolved inorganic n p ratio particulate n p in the water increased due to mortality because n content in biomass is higher than the p content and therefore more particulate n is released in the outflow harvesting reduced the absolute mortality rate because of a reduction in biomass fig 6 and consequently the particulate n p in the wetland outflow the reduction of the inorganic n p ratio in the water is caused by the uptake of papyrus plants to replace vegetation that has died off with harvesting this increased as the demand for inorganic n and p increases for re growth and fast translocation from rhizome to agb as the uptake of n is higher than p the n p ratio in the water is reduced a higher n than p retention was also found in field experiments gaudet 1977 kansiime et al 2007 kanyiginya et al 2010 with over harvesting adsorption of op increased as a result of higher op concentrations in the pore water fig 7e and f and 8b and d explaining the higher n p ratio in the outflow compared with inflow table 3 intact wetlands would therefore reduce both n and p and push the system to be more n limited the tn tp outflow ratio without harvesting was 9 6 g g equivalent to a 21 3 n p molar ratio and with harvesting this decreased to 18 n p molar ratio this is in between values found for lake victoria 13 6 n p molar and lake malawi 28 4 n p molar guildford and hecky 2000 a tn tp molar ratio in the water below 20 is considered n limiting and favours blooms of n fixing cyanobacteria at high p concentrations as they outcompete non n fixing algal species guildford and hecky 2000 wetlands are valued for their water purifying characteristics costanza et al 2017 this model suggests that this ecosystem service can be enhanced by harvesting however while n and p retention increase with harvesting the ratio at which n and p are retained pushes the system to be more n limited making it more sensitive to cyanobacterial blooms on the other hand elser et al 2009 showed a shift towards more p limited lake systems in europe and north america as a result of increased n deposition for africa n deposition has been lower but is now increasing faster compared with europe and north america dentener et al 2006 4 5 sensitivity analysis the global sensitivity analysis method that was used here saltelli et al 2000 is novel in ecological modelling an advantage over the more commonly used oat approach was the identification of a set of parameters that relative to each other explained the outputs fig 9 this is valuable in identifying which parameters need further attention and which can be left as they are cariboni et al 2007 the method also provides quality assurance by facilitating discussion on the biological or physico chemical explanation behind the dependence of the output on the input parameters saltelli et al 2000 the relatively low r2 values for the models describing n and p retention may indicate that the oat pre screening did not identify the most influential parameters for retention it is worth investigating this further by looking at other global sa methods in a future study e g makler pick et al 2011 the results of the sensitivity analysis fig 9 illustrated mostly logical biologically meaningful relationships and confirmed the importance of re translocation as expected higher assimilation led to higher biomass and higher mortality to lower biomass for water quality higher re translocation led to higher concentrations in the water by keeping more n and p in the vegetation when it senesced and reducing uptake from the water similarly retention was lower with higher re translocation because of less n and p in aboveground dead biomass based on a study of macrophytes in wetlands in various regions and with different nutrient status rejmánková 2005 suggested that re translocation in emergent macrophytes depends on the inorganic n and p concentrations in the water with lower concentrations increasing re translocation as a survival strategy higher mortality led to higher retention more dead biomass and higher fragmentation and leaching to lower retention less accumulating dead biomass all parameters that define papyrus growth mortality and decay processes are also related to system conditions e g temperature which were not all modelled because some of the parameter values were derived from other wetland types or plant species or calibrated or estimated annex 3 empirical studies on papyrus systems are needed to obtain more evidence based values for these parameters other influential parameters such as inflow concentrations and porosity can be measured easily and the values used in the model were realistic small increases or decreases of no3 nh4 and op in the inflow led to longer or shorter periods of nutrient limitation which highlights the impact of inflow concentrations on water quality this impact was higher under permanent flooding conditions than with seasonal flooding with seasonal flooding there was more nutrient limitation higher mortality more re growth a higher n and p uptake and lower concentrations in the water compared with permanent flooding similarly retention was more sensitive to inflow concentrations with seasonal flooding when longer periods of nutrient limitation led to more mortality and more accumulation of dead biomass 4 6 model evaluation and future development an evaluation of the quality and credibility of a model encompasses the whole process of model development calibration analysis and application augusiak et al 2014 based on the available data from field studies of papyrus the numerical model used and the results of the sensitivity analysis and comparison with literature data it can be concluded that papyrus simulator provides a good representation of the main nutrient cycles in rooted papyrus wetlands and allows a better understanding of the impact of water regime and harvesting on n and p retention if validation means that a model is acceptable for its intended use rykiel 1996 the model could be considered validated however validation in the sense of model output corroboration augusiak et al 2014 would require a more rigorous comparison of model output with independent time series data from different wetland sites wider application of papyrus simulator to specific wetland sites for decision making purposes would require coupling of the model to a spatially defined hydrology model while currently good datasets from sufficiently long time periods are not available we hope that increasing availability of data from monitoring programmes and remote sensing will provide opportunities for further model development and a full validation the model may also be developed further in other directions in the current version of the model oxygen only influences nitrification and denitrification other processes e g fragmentation leaching and assimilation are also dependent on environmental factors like temperature oxygen ph and irradiance for application of the model to all papyrus wetlands from the middle east to southern africa knowledge of how these factors vary with altitude and longitude and how they influence biomass retention and water quality is crucial for example under the mediterranean climate serag 2003 found a low agb in winter low temperatures and radiation and high agb in summer high temperatures and radiation and vice versa for bgb the relationship between altitude and biomass is also complex temperature is lower at higher altitude naivasha in kenya at 1900 m on average below 20 c compared with the sudd wetland in south sudan at 400 m average 30 c but radiation may be higher which favours c4 photosynthesis higher temperature stimulates growth but also leads to higher respiratory losses jones and muthuri 1985 higher temperature may increase decomposition and therefore reduce retention but assimilation may also be higher if this would result in higher mortality this could increase retention the empirical data from literature did not reveal a clear relationship between altitude biomass and productivity table 4 which emphasizes the need for experiments targeting the processes related to growth and mortality 4 7 potential for application the potential application of papyrus simulator for scientists and decision makers can be local regional and global for a specific papyrus wetland the model can be linked to a site specific hydrological model and developed into a spatially explicit tool for spatial planning or economic valuation of n and p retention the model could also be applied to design constructed wetlands for wastewater treatment to predict n and p removal rates and evaluate harvesting regimes at a regional level the model can improve our understanding of n and p cycling and retention and how they are affected by changes in climate or land use identify knowledge gaps and help focus empirical research with a more elaborated carbon section the model can address questions on the role of papyrus wetlands in the carbon cycle and quantify their expected role as net carbon sink moomaw et al 2018 a comparison with growth models of other emergent plants e g phragmites australis and typha sp asaeda and karunaratne 2000 asaeda et al 2008 tanaka et al 2004 could identify strength and weaknesses of these respective modelling approaches and lead to mutual benefits combined with existing vegetation models and global hydrological and climate models papyrus simulator could contribute to a global model which quantifies ecosystem services that contribute to the sustainable development goals janse et al 2019 such a global model can quantify the loss of economic value from conversion of wetlands for food security conceição et al 2016 and provide evidence of impact on human well being 5 conclusion in conclusion papyrus simulator is suitable for studying processes related to n and p retention and the effect of harvesting in papyrus wetlands absolute and relative retention of p was lower than n retention and the main mechanism for both was peat accumulation this led to a predominantly n limited environment by reducing the n p ratio in the water absence of surface water during part of the year resulted in a reduction of biomass mainly aboveground harvesting increased retention from 7 to more than 40 for n and for p from 4 to 40 the global sensitivity analysis was successful in identifying the relative contribution of the model inputs to explaining model outputs the most influential parameters were related to assimilation mortality decay re translocation inflow concentrations and soil porosity the main mechanism for retention was peat formation which would be relatively unaffected when combined with seasonal agriculture and sustainable harvesting of papyrus but would be completely lost when wetlands are converted to agriculture more permanently papyrus simulator can contribute to a global modelling effort to quantify ecosystem services and contribute to achieving the sdgs related to food water climate and biodiversity declaration of competing interest none acknowledgments the authors would like to thank ihe delft institute for water education for time allocated to this research prof kenneth irvine ihe delft for his support in general and his valuable reviews of various versions of the manuscript and two anonymous reviewers for their constructive feedback that helped to improve the quality of the manuscript considerably appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104531 
26124,the concept models of everywhere was first introduced in the mid 2000s as a means of reasoning about the environmental science of a place changing the nature of the underlying modelling process from one in which general model structures are used to one in which modelling becomes a learning process about specific places in particular capturing the idiosyncrasies of that place at one level this is a straightforward concept but at another it is a rich multi dimensional conceptual framework involving the following key dimensions models of everywhere models of everything and models at all times being constantly re evaluated against the most current evidence this is a compelling approach with the potential to deal with epistemic uncertainties and non linearities however the approach has as yet not been fully utilised or explored this paper examines the concept of models of everywhere in the light of recent advances in technology the paper argues that when first proposed technology was a limiting factor but now with advances in areas such as internet of things cloud computing and data analytics many of the barriers have been alleviated consequently it is timely to look again at the concept of models of everywhere in practical conditions as part of a trans disciplinary effort to tackle the remaining research questions the paper concludes by identifying the key elements of a research agenda that should underpin such experimentation and deployment keywords environmental modelling models of everywhere grid computing cloud computing data science 1 introduction the concept of models of everywhere was introduced by beven in 2007 beven 2007 and revised in a follow up paper beven and alcock 2012 the concept is fundamentally about having a stronger association between a given environmental model and the place that it represents in the 2012 paper they argue that it is useful and even necessary to think in terms of models of everywhere and this will change the nature of the modelling process from one in which general model structures are used in particular catchment applications to one in which modelling becomes a learning process about places the necessity stems from the need to constrain uncertainty in the modelling process in order to support policy setting and decision making particularly around water management e g flooding and water quality although the principles can also potentially apply to other areas of environmental modelling and management in the rest of the paper we will tend to use examples and illustrations from hydrology and flood modelling although we stress this potential generality of the approach this is a compelling argument and a reaction against the view that there can be generic environmental models capable of representing processes and behaviours across multiple places and indeed across multiple scales such general models are expensive to develop difficult to maintain and to apply because of their data demands and need for parameter estimation or calibration beven 2007 they also have problems in dealing with local epistemic uncertainties and non stationarities for example caused by change in local characteristics and climate drivers e g prudhomme et al 2010 beven 2009 2016 some examples of models of everywhere have been deployed but for relatively constrained applications and at specific scales however the concept has not been developed to the extent that the authors envisaged where everywhere is represented across all scales in a coherent way this paper re examines the concept of models of everywhere from a technological perspective arguing that at the time the underlying technology was not sufficiently advanced to support the concept now however this has changed with significant developments in areas such as data acquisition techniques data storage and processing technologies and data analytics capabilities alongside a move towards a more open science supported by these developments note that in developing ideas of models of everywhere we mean something quite different to the hyperresolution models that are starting to be used in earth systems science e g wood et al 2011 beven and cloke 2012 bierkens et al 2015 gilbert and maxwell 2017 with resolutions of the order of 1 km the latter do not as yet provide simulations and visualisations at scales that local stakeholders can relate to directly see the discussion of beven et al 2015 this is a critical aspect of how the models of everywhere concept has the potential to change the way that modelling is done both approaches do however focus attention on a requirement for scale dependent parameterisations that has proven difficult to resolve e g mcdonnell and beven 2014 the overall aim of the paper is to determine the current feasibility of models of everywhere particularly in the area of hydrological modelling given the state of the art in underlying technology this breaks down into the following objectives 1 to carry out a detailed examination of the concept of models of everywhere to determine key underlying technological requirements 2 to compare the state of the art in technology in the period 2007 2012 and the present day to evaluate whether the time is now right for a widespread deployment of models of everywhere 3 to provide a research roadmap to support such deployment in terms of outstanding research questions and challenges note that there are other issues related to models of everywhere that also should be addressed most notably human and societal issues such issues include the need to move towards open science and open data and the role of communities in improving models in representing local places these are alluded to in the paper but a full treatment of this important dimension is beyond the scope of the paper we elect instead to focus on technological readiness the work is being carried out in the context of a significant re evaluation of approaches to flood modelling and associated risk management for example the uk government s national flood resilience review published in september 2016 1 1 https www gov uk government publications national flood resilience review included important recommendations around improvements to long term modelling capabilities the review also encouraged the use of natural flood mitigation methods or working with natural processes this concept involves the use of many distributed in channel and off channel storage features coupled with changes of land use to try to retain more flood runoff in catchment headwaters or at least slow down its arrival to areas at risk of flooding see dadson et al 2017 there are many current projects in the uk that are implementing natural flood management measures very few however have been associated with detailed monitoring of changes to the flow or the operation of individual mitigation measures additionally there are issues about whether the strategy will be effective under extreme flood events which in the uk are often preceded by a period of prior catchment wetting see metcalfe et al 2017 hankin et al 2017 in fact by slowing the flow in some parts of the catchment it is possible that the peak flow might increase elsewhere this is called the synchronicity problem the impact of which will vary from event to event because of the different patterns of rainfall intensities and with changes in the scale of catchment being considered the distributed nature of this problem and the potential for such mitigation effects to have impacts on other environmental factors requires an integrated catchment modelling approach to evaluate possible implementation scenarios however the outputs from such models will be associated with significant uncertainty even after calibration on historical data thus this is a prime example where the concept of models of everywhere and the local constraint of uncertainty using local information would be useful especially when assessing the uncertainty in potential outcomes might make a difference to the decision that might be made a re evaluation of the concept of models of everywhere is therefore very timely the paper is structured as follows section 2 examines the concept of models of everywhere in more depth highlighting the different dimensions behind this initial vision and culminating in a set of technological requirements to support models of everywhere section 3 looks at the technological landscape as it existed in the period 2007 2012 systematically reviewing the different technological requirements and concluding with an overall assessment of technology readiness at that time section 4 repeats this analysis but looking at the state of the art now the paper then presents ongoing research in this area including the identification of a research roadmap for the implementation of the concept of models of everywhere section 5 section 6 documents related work including existing deployments of the concept of models of everywhere finally section 7 concludes the paper with some final reflections on models of everywhere from a technological perspective 2 models of everywhere unpicked while models of everywhere at one level is quite a straightforward concept representing as association of models with particular places at another level it is a rich multi dimensional conceptual framework in particular we discuss three mutually supportive dimensions with the goal of highlighting the technological requirements to support the overall vision models of everywhere models of everything models at all times these are discussed in turn below 2 1 models of everywhere 2 1 1 key characteristics the starting point of models of everywhere is to move from generic models that can then be customised to particular locations for example through appropriate parameterisation to models that are specific to particular places as such they can be tailored to represent the behaviour at a specific place without the need to represent any other place in particular observations and inputs from local stakeholders can be used to constrain the uncertainty that is associated with environmental modelling e g beven 2009 note that models of everywhere is often interpreted as models representing specific localised areas but the concept does not imply any particular scale rather models of everywhere can represent local regional national and global scale with these models often co existing while there is an expectation that model parameterisations should be resolution dependent e g mcdonnell and beven 2014 beven 2019 in the absence of any adequate scaling theory for many environmental processes particular models may need to be tailored for the scale at which they operate in terms of both process representations and effective parameter values the approach is illustrated in fig 1 a generic model with a specific set of parameter values cannot represent flooding at all areas of the catchment and hence five localised models need to be developed a core motivation of models of everywhere is to constrain uncertainty exploiting as much knowledge as is available about a particular place this is developed further when we look at models of everything and models at all times in sections 2 2 and 2 3 below 2 1 2 technological requirements the main requirement of models of everywhere is very large scale computational capacity for example if this approach were to be adopted for future flood prediction there would be a need for the deployment of very large numbers of models all over the country at different scales for illustration consider models applied to support flood and coastal risk management fcrm in england for any given community city town village or street there may be an array of models developed paid for and used by different organisations for different purposes and using different data resources or very often common data sets exploited in different ways most localities are included within national scale models sometimes referred to as strategic with the outputs of the national flood risk assessment nafra 2 2 https www gov uk government publications flooding in england national assessment of flood risk published as risk of flooding from rivers and sea 3 3 https data gov uk dataset risk of flooding from rivers and sea1 being typically the most generic separate models have been applied to provide mapping of the flooding from surface water 4 4 https www gov uk government publications flood maps for surface water how they were produced ubiquitous in coverage representing potential flooding from overland flows and ponding rather than water overflowing from rivers or the sea the risk of flooding from reservoirs predicting places at risk in the event of dams and impoundments being breached and groundwater flooding more detailed models also exist in many places to support activities such as the economic appraisal of proposed flood defence schemes flood risk assessments for proposed floodplain developments or the detailed design construction and maintenance of drainage systems these models usually capture further information about infrastructure e g bridges culverts weirs sluice gates and river channel surveys organisations commissioning and owning such models may include the environment agency which leads on fcrm in england for local government water companies supplying drainage services private developers or other landowners it is usual for all of the above models to evolve over time incorporating both new data and technical improvements e g better numerical solution schemes it is also common for multiple instances of each model to be executed i e many individual runs or simulations to support scenario or uncertainty analysis the environment agency has over 1500 such detailed local models and reported in its 2010 2015 modelling strategy 5 5 https www gov uk government uploads system uploads attachment data file 292949 geho0310bsbt e e pdf an investment of approximately 17 million per year in modelling and mapping and an additional 15 million in gathering and processing data to support fcrm this has significant resource requirements in terms of the number of processors or virtual machines to run these models and data storage as well as the human costs in terms of developing and tailoring the models for given places and analysing and understanding the outputs for example production 6 6 https www whatdotheyknow com request 208078 response 520170 attach 3 national 20modelling 20and 20mapping 20method 20statement 20may 202013 pdf of the risk of flooding from surface water maps cited earlier involved more than 70 000 individual simulations of flood inundation on a mosaic of approximately 7100 36 km2 tiles covering all of england run on a 2m 2m resolution digital height map that included over 91 000 manually determined corrections this process needed around two months for data preparation and one month of computer processing time fully utilising a grid of over 100 gpu accelerated pcs where possible technological and operational support would need to be provided for such development this also asks important questions over the underlying distributed systems architecture to support such massive deployment e g centralised distributed or decentralized or indeed combinations of different approaches the approach also asks fundamental questions over the relationship and consistency of models at different scales and how to support reasoning across scales in terms of supporting a deeper understanding of the science and all its complexities and inter dependencies 2 2 models of everything 2 2 1 key characteristics the second dimension is concerned with exploiting information about a place in particular coupling a model of that place with as much local data as can be collected thus embracing the heterogeneity of available data sources the availability of such data is increasing significantly and now includes blair et al 2019 remote sensing data collected by satellites or aircraft borne instruments including drones other monitoring technologies that consist of a range of sensor technologies typically in close proximity with the observed phenomena including the use of internet of things iot technologies to provide real time streaming and multi faceted data about the natural environment historical records held in a variety of locations and scales the increasing amount of data available from national local government and other open data portals often increasingly offering apis e g data gov uk data mining provides additional information from the web or social media data collected from citizen science with the potential to direct citizen science to areas of data scarcity together this adds up to the potential for having environmental data at an unprecedented scale blair et al 2019 for example if focusing on flood prediction it is possible to use a variety of data sources such as historical parish records and flood marks satellite imagery local sensors photographs from social media and citizen science to help steer process based hydrological models indeed many researchers are advocating such approaches in hydrology e g di baldassarre et al 2009 smith et al 2009 2015 and more generally in disaster risk reduction mccallum et al 2016 the concept also naturally extends to other aspects of environmental science for example collecting and analysing data around water quality issues biodiversity or soils and indeed the inter dependencies between them the additional dimension of models of everything is shown in fig 2 the concept of models of everything has the potential to further reduce the uncertainty around predictions for a variety of different variables of environmental interest in a coherent way this is particularly important where the relevant processes are intrinsically coupled for example the water flows that drive the transport of nutrients from farmland and households into rivers and lakes this constraint of uncertainty is important because many of the sources of uncertainty are epistemic in nature epistemic uncertainties are those are those that arise from lack of knowledge in contrast to aleatory uncertainties that represent random variability that derives from irreducible natural variability see beven 2009 2016 rougier et al 2013 beven and hall 2014 di baldassarre and beven 2016 by definition it is not possible to deal with epistemic uncertainties in process models without breakthroughs or deepening of knowledge about a given place and its states and behaviours beven et al 2015 also talk about the role of models of everything in overcoming what they refer to as hyperresolution ignorance in modelling that is evaluating the hyperresolution information produced by simulation to overcome the local lack of data and unknowns in scientific understanding e g the understanding of subsurface structures in hydrology 2 2 2 technological requirements models of everything adds significantly to the requirements imposed on the computational infrastructure around five key areas 1 how to store the big data with models of everywhere there is a need to capture and store significant quantities of data about a given place and then repeat this across all places this therefore very quickly becomes a big data problem in many ways though this is more demanding than many areas of big data given the high level of heterogeneity in the data sets with some of the data being structured and other elements being unstructured and inevitably captured in a wide variety of formats blair et al 2019 2 how to represent and manage the collected data given the variety and heterogeneity discussed above there is a need to represent evaluate and manage the overall collection of data and this must include support for interoperability data discovery and also the association of appropriate meta data and ontologies including provenance information 3 how to ensure open access to data the concept of models of everything implies a move towards open data where data is openly available for use and stored in a way that allows such open access also important to support a more collaborative and cross disciplinary science as required to interpret this data as mentioned in the introduction while this is technically straightforward to achieve this requirement is more concerned with cultural issues for example around the perceived value of data note that ideally this open philosophy would also extend to models with models available as open source 4 how to make sense of the heterogeneous data elements it is one thing to have access to this rich underlying data but it is another thing to be able to make sense of this data and therefore data analysis techniques are also required to build this higher level of understanding from the underlying data cf environmental data science blair et al 2019 this will inevitably imply the construction of data models using a rich array of statistical and machine learning techniques 5 how to combine process models with data models once the data models are constructed there is a need to couple the data model or models with process models to build a complete understanding of a given place techniques are therefore required to support model coupling between process and data models in hydrology it has been shown that even hydrological observations may not always be informative in model calibration and validation e g beven and smith 2015 there are also important human and societal issues around privacy and security but as mentioned in the introduction this area is not considered in this paper but is an important area of future investigation note that the execution of additional data models and the need to allow for observational uncertainties also increases the computational requirements of the system 2 3 models at all times 2 3 1 key characteristics the final dimension is that models should be active at all times constantly re evaluating what is known about particular places and adapting accordingly this does not necessarily mean that models are always executing as this would consume significant computational resources without any real gain rather models should execute periodically and frequently for example when new data becomes available to understand the idiosyncrasies of particular places beven 2007 and how this might change over time at other times the model would be in a quiescent state but otherwise ready to re execute at any time this contrasts significantly with existing practice when distinct model runs are carried out infrequently under the auspices of a scientific experiment with perhaps more runs carried out an at a pre deployment phase to understand the sensitivities and uncertainties of a given generic hydrological model this iterative approach is nicely aligned with the work of box 1980 on the iterative relationship between practice and theory box s loop recently extended by blei 2014 in the context of latent variable models applied to complex data sets this may also introduce more consistency between models used for short term forecasting often applied in an adaptive framework where on line data can be used for updating and simulation models that are rarely updated this leads to a new perspective of modelling as a learning process as discussed in depth in the 2007 paper beven 2007 the 2012 paper beven and alcock 2012 develops this further talking about models as hypotheses to be tested against current and historical observations with some models being rejected in favour of others and indeed this changing over time so the current chosen model structure and associated assumptions best reflect the full idiosyncrasies of a given place as represented by numerous additional data observations this in turn leads to an adaptive approach to modelling the final aspect to consider is what can be adapted about the model there are various possibilities here increasing in level of sophistication and ambition 1 the outcomes from a model can be adapted for the purposes of real time forecasting when data can be made available for assimilation and post event analysis can then be used to inform local improvements to the model including adaptation of parameter values to best represent behaviour at the current time 2 a number of models can co exist in an ensemble approach with model selection applied to identify the best models for that given place time 3 the internal structure and behaviour of a given model can be adapted for example by changing fine grained elements of the underlying hydrology to best reflect the current place time 4 the representation of residual uncertainty can be adapted as more information is obtained locally clearly these approaches can also be combined in different ways indeed a combination of all four offers a new and radical approach to models of everywhere the concept of models at all times is illustrated in fig 3 the key motivation of models at all times is to offer a modelling framework that supports explicit reasoning about uncertainty with the explicit goal of reducing uncertainty for a given place more specifically the approach also has the potential to deal with epistemic uncertainties as argued in beven and alcock 2012 in this paper following beven 2006 the authors argue for an approach based on limits of acceptability whereby models that perform well according to such limits are acceptable and perhaps reinforced while others are rejected with this driven by the collected set of observational data cf models of everything finally the approach has significant potential to deal with non linearities and fundamental changes over time for example related to climate change with its emphasis on ongoing adaptation to the current context 2 3 2 technological requirements models at all times is crucial to the overall vision of models of everywhere but adds a whole new level of complexity in terms of the underlying technology requirements in particular the approach amplifies the underlying resource requirements and also the underlying distributed systems architecture as discussed in section 2 1 for example the approach requires the frequent execution of potentially ensemble models at large numbers of places and different scales the approach also introduces significant additional mutually supportive requirements 1 how to support adaptive reasoning as discussed above models at all times is fundamentally a learning process and implies that models are constantly adapted in response to new knowledge extracted from available data sets there is therefore the need to support this adaptive reasoning and ideally this should involve a strong element of automation as provided for example by autonomic computing supporting self adaptive systems kephart and chess 2003 mckinley et al 2004 2 how to incorporate reasoning about uncertainty building on the above it is important that adaptation decisions incorporate reasoning about uncertainty and this implies making uncertainty explicit in the modelling process and also incorporating approaches to deal with epistemic uncertainties and non linearities as inevitably encountered in such complex systems 3 how to support adaptation a truly adaptive system requires ready access to a range of elements that can be changed supporting more coarse grained adaptation is relatively straightforward and implemented in terms of selecting from different models in model ensembles or changing model parameters supporting fine grained strategies is however more challenging as this requires intimate access to the structure and behaviour of individual models in terms of for example alternative hydrological equations at the heart of the model most existing models will not provide such access i e black box implementations to fully realise the vision however we need to go further than this and provide more white box access to internal software architectures of environmental models as provided by for example reflective architectures maes 1987 kon et al 2002 models at all times also places additional emphasis on the need to integrate process and data models discussed in section 2 2 to support adaptive reasoning there is also an over arching requirement emanating from this analysis and that is the ability to support deployment at scale and this implies the ready deployment of individual models of everywhere and also of large numbers of models at different scales there are many dimensions to this scalability involving for example making it easier to deploy models in underlying computational infrastructure whether provided by hpc or cloud facilities offering software frameworks that can support the deployment of models or ensembles of models ready to be tailored for the idiosyncrasies of places and automating the subsequent adaptation learning process hence the importance of self adaptive approaches 2 4 overall analysis models of everywhere is an important and potentially crucial approach to environmental modelling particularly in terms of managing uncertainty a full implementation of the concept however imposes very significant requirements in terms of the technological infrastructure alongside other fundamentals most notably cultural elements around a move to open science incorporating more open approaches to data and modelling the approach is best understood as a combination of models of everywhere everything and at all times with this trichotomy used to analyse the overall requirements in more depth in the discussions above the resultant requirements are shown in table 1 below these requirements can usefully be clustered as follows 1 the capacity and level of sophistication of the underlying technological infrastructure in terms of both computation and data r1 r2 r4 r5 2 the availability of rich data analytics capability to make sense of complex and highly heterogeneous data sets r3 r6 r7 r8 3 the ability to support modelling as a learning process including reasoning about uncertainties r9 r10 r11 4 practical issues around deployment at scale including availability of open data and approaches to support large scale deployment r3 r12 this clustering will be used in the assessment of the changing technological landscape as discussed in sections 3 and 4 below 3 technological landscape 2007 2012 3 1 overview of the landscape in the period 2007 to 2012 the landscape was dominated by grid computing the concept of grid computing was first introduced in in the 1990s and became prominent with the publication of the seminal paper by foster and kesselman 1998 introducing the grid as a blueprint for a new computing infrastructure the term was introduced as a metaphor for the electricity grid with the goal of making computational power as accessible and ubiquitous as electricity software platforms were developed to support the deployment of applications and services in the grid most notably the globus toolkit with various versions released starting in 1997 with the last major release globus toolkit version 5 in 2009 7 7 http toolkit globus org toolkit around this time the grid was being superseded by cloud computing for example the first version of amazon web services 8 8 https aws amazon com was introduced in 2006 with rapid growth since this growth in cloud computing is discussed further in section 4 1 below in parallel researchers were becoming interested in the use of such computational power to support a range of application domains including for example ecommerce most notably in the context of this paper there was also great interest in escience that was the use of technological infrastructure including the grid to support a new kind of computationally intensive and data rich science hey et al 2009 for example in the uk the national escience programme ran from 2001 to 2010 supporting a range of infrastructure projects and application projects in areas as diverse as bioinformatics neuroinformatics and medical informatics in the environmental area the most prominent project was climateprediction net 9 9 http www climateprediction net similar large scale initiatives were launched in other countries for example in the states the national science foundation nsf funded a series of cyberinfrastructure initiatives starting around 2003 including the open science grid 10 10 https www opensciencegrid org developed by the open science grid consortium osgc 3 2 addressing the requirements 3 2 1 underlying technological infrastructure the emergence of the grid and also the escience community that coalesced around the grid provided important expertise experience and also facilities to support the development of models of everywhere however in practice and this is clear in retrospect the grid did not meet the full set of requirements to support the broader vision of models of everywhere although the vision of the grid was to provide plentiful resources on demand the reality was somewhat different at that time the availability of resources varied greatly and depended on access to one of the experimental grid facilities that were introduced in different global centres the overall distributed systems architecture was therefore one of centres at given fixed locations offering by definition relatively centralised services with partial access and limited control of these services a number of researchers explored more decentralized architectures for example climateprediction net mentioned above and seti home 11 11 https setiathome berkeley edu utilising boinc 12 12 https boinc berkeley edu a more peer to peer volunteer computing platform but such initiatives were not mainstream and not integrated into other grid initiatives it is also important to emphasise that this was a research programme and hence the underlying platforms were not stable with frequent changes over time in terms of services and facilities on offer as will be seen this contrasts significantly with what is available now in terms of both capacity and stability of services see section 4 2 more fundamentally the services on offer did not have the level of sophistication to meet the technology infrastructure requirements as identified in table 1 the main middleware technology used at the time was the globus toolkit with the overall architecture of the toolkit v5 shown in fig 4 this was a large and complex architecture with many dimensions but as can be seen the emphasis is on supporting resource sharing and at a fairly low level of abstraction as stated in the seminal paper on the anatomy of the grid foster et al 2001 argue that the grid was fundamentally about coordinated resource sharing and problem solving in dynamic multi institutional virtual organisations they go on to argue that this implies direct access to computers software data and other resources and this sharing should be highly controlled hence the emphasis was very much on meta level concerns such as standardised apis to ensure interoperability service discovery access control and resource management there was also more emphasis in practice on computational resources rather than data management for example gram grid resource allocation and management offered an architecture to submit and monitor batch jobs in the grid see fig 5 this is quite different though from the execution style required for models of everywhere as mentioned above the data side was quite primitive with an emphasis on low level facilities for access to data remotely gridftp 13 13 http toolkit globus org toolkit docs latest stable gridftp and to assist in replication of data while the grid was used successfully for a number escience experiments involving elements of big data the level of sophistication of data management was insufficient for the rich and heterogeneous data required for models of everywhere and associated needs in terms of discovery and navigation and indeed for environmental data more generally as will be seen below this is one area that has advanced significantly in the last few years there was also a general lack of experience of using grid computing for the earth and environmental sciences other scientific communities were more advanced in terms of their use of grid computing and embracing escience this led to a lack of services specific to this field e g to support environmental modelling in the grid although parallel developments such as the openmi open modelling interface standard 14 14 http www openmi org as adopted by the open geospatial consortium provided important building blocks to support model deployment and most crucially interoperability across models in summary grid computing was important in terms of establishing a community working together on distributed architectures and infrastructure and in particular for building a strong dialogue with the science community in terms of a new open computational and data rich style of science however there are a number of limitations that impacted on the feasibility of models of everywhere most notably the difficulties of access to computational and data resources the lack of sophistication of the distributed infrastructure particularly in terms of data management and the primitive nature of many of the services on offer also revisited below under deployment at scale 3 2 2 data analytics the ability to represent and access very large scale and highly heterogeneous data is important equally it is crucial to have a range of techniques to make sense of this data as can be seen above this requires a move towards open data as a prerequisite for open science the availability of a rich set of techniques to analyse data the ability to extend this reasoning across scales and an integration of process modelling with data models produced to analyse the complex data over and above the baseline requirement for storing accessing and managing large and complex data sets open data was in its infancy in the period 2007 2012 with data often regarded as core intellectual property with many institutions seeking ways to commercialise their rich data sets there was however a growing recognition with the complexity of modern science that a new more open approach to data was necessary for example the royal society published science as an open enterprise in 2012 15 15 https royalsociety org media royal society content policy projects sape 2012 06 20 saoe pdf with a core recommendation scientists should communicate the data they collect and the models they create to allow free and open access and in ways that are intelligible assessable and usable for other specialists in the same or linked fields wherever they are in the world this built on the emergence of science 2 0 waldrop 2008 seeking an open approach to science based on emerging web standards particularly web 2 0 technologies offering user generated content and a move towards a more social web in practice however at that time there were many cultural and technological barriers to a world where data sets were available for open access in common repositories in terms of making sense of data the environmental sciences including hydrology make extensive use of process models to understand fundamental processes of nature and then use these models to make future predictions a wide range of process models have been developed for example in hydrology where there have been recent attempts to incorporate multiple process components into a common framework e g fenicia et al 2011 clark et al 2015 in applications for flood risk assessment there have been many codes routinely used both by industry and researchers to model the flow of water through the landscape including interactions with physical infrastructure systems these codes can be categorised reasonably precisely in terms of the approximations made to a set of physical governing equations in the case of flood models this means simplifications of the fundamental navier stokes equations for fluid dynamics even so there remain differences in the interpretation of the prototypical physical equations the numerical schemes that are used to solve them the discretisations involved in applying those schemes to real data sets and in the very many edge cases for which special solutions are required benchmark comparisons 16 16 https www gov uk government publications benchmarking the latest generation of 2d hydraulic flood modelling packages have shown how important these differences can be in controlling the results of flood simulations in various situations there is also a strong body of research on training models based on historical data and current observations can be used to steer future states of the model data assimilation lahoz et al 2010 park and xu 2017 more generally in the time period under consideration there was a deep concern that process models alone are not sufficient and that fundamental issues remain for example reasoning about uncertainty and dealing with epistemic uncertainties and non linearities in complex systems indeed this is the prime driver for models of everything this reflects a sense that it is necessary to integrate the process model view of science with one that recognises the importance of data and associated data analytic techniques effectively data models this is a significant cross disciplinary challenge requiring input from environmental computer and mathematical scientists at that time this dialogue was not happening discussed further in section 4 1 scientists also tended to focus on specific experiments and studies to understand phenomena at a given scale so reasoning across scales was in its infancy overall even by 2012 there were major barriers around data analytics that made it very hard to support the realisation of models of everywhere 3 2 3 modelling as a learning process as discussed above the perspective of models as a learning process is the most important but also most demanding aspect of implementing models of everywhere requiring a new adaptive approach to learning from our analysis this breaks down into support for adaptive reasoning explicitly representing consideration of uncertainty in this reasoning and also being able to carry out both coarse grained and importantly fine grained adaptations in the field of computer science in the time period under consideration a deep understanding of adaptive computing developed for example ibm launched a new initiative examining autonomic systems in 2001 that is systems that can self manage mirroring the autonomic functioning of nervous system in the human body in terms of a range of self properties e g self awareness self configuration self healing and self optimisation kephart and chess 2003 more generally there was a large literature around software architectures to support self adaptation including reflective architectures the use of control loops in decision making and the use of more advanced machine learning techniques to support higher levels of autonomic self management for example dealing with unknowns oreizy et al 1999 mckinley et al 2004 however there has been little or no consideration about how such techniques can be used in terms of adaptive environmental modelling existing environmental models are also often written in older programming languages most notably fortran and tend to be monolithic black box implementations hence do not lend themselves to the implementation of fine grained adaptation strategies there is also the important requirement to represent and reason about uncertainty explicitly as part of the adaptation process at that time there was growing recognition of the need to represent uncertainty in modelling and reasoning about uncertainty across scientific experiments for example the uncertweb project introduced techniques to capture uncertainties as meta data in web based environments with details of the resultant uncertweb framework published in early 2013 bastin et al 2013 researchers had also developed a number of frameworks to reason about uncertainty in scientific experiments including seminal work by beven and binley 1992 2014 and others see renard et al 2010 vrugt and sadegh 2013 nearing et al 2016 as discussed in the 2012 models of everywhere paper beven and alcock 2012 were just starting to think about reasoning about uncertainties in model selection or rejection as a key part of models as a learning process in summary most of the building blocks were there by 2012 but the work was fragmented and split across many communities and key issues remained over how to support more advanced reasoning of uncertainties including dealing with epistemic uncertainties 3 2 4 deployment at scale finally and importantly there is the key question of whether there was sufficient advancement at that time to support deployment of the kind of scale that makes models of everywhere a reality as discussed above there are several key dimensions to support such large scale deployment including how easy it is to deploy individual models what support there is to then repeat this across many places at different scales and also whether the learning and hence tailoring process can be automated the latter issue is intrinsically inked to the support for self adaptive modelling and hence we focus more on the first two issues one of the key problems with deploying in the grid environments or indeed to other hpc facilities is the low level of abstraction offered by software platforms this was discussed in the consideration of the globus toolkit above given this the development and deployment of even an individual model is a tedious expensive and error prone process and this in itself is a barrier simm et al 2018 to the deployment of models of everywhere this is a barrier to more general deployment across a range of places where the individual models need to be specific to this place both initially and also with the model or models refined over time to reflect the particular idiosyncrasies of this place this implies some form of software framework coupled with models as a learning process and at that time this was significantly beyond the state of the art for model development it is interesting to note that the initial models of everywhere paper beven 2007 discusses an object oriented approach to programming models of everywhere mapping individual active spatial objects to places and also explicitly representing the relationship between places mainly in terms of fluxes this is an attempt to seek a higher level of abstraction to support the deployment of models of everywhere at the time of writing object oriented computing and indeed distributed objects were an important area of research reflected in the importance of technologies such as corba common object request broker architecture 17 17 http www omg org spec corba this approach is now largely superseded by alternative programming models reflecting most principally difficulties in realising distributed objects in internet scale developments 3 3 overall assessment and technological readiness it is clear from the assessment above that even by the end of this period 2012 there were major technological barriers in terms of the deployment of models of everywhere our overall assessment is summarised in table 2 which shows an overall rating against each of the requirements together with the identification of the most important barriers as can be seen from table 2 the overall readiness level is generally low to medium with important barriers remaining across all categories it is interesting to note that quite a number of the barriers are due to a silo ed approach to research and can be addressed by more cross disciplinary collaboration in this area overall we would argue that in the period 2007 2012 the vision of models of everywhere was right but the technology was not ready we continue our discussion by considering how things have advanced to date noting important developments that make an implementation of the concept more realistic 4 technological landscape current 4 1 overview of the landscape the technological landscape has changed enormously since 2012 and indeed this is one of the key drivers to revisit the concept of models of everywhere in terms of technological readiness in particular there have been three mutually supportive areas of significant innovation namely cloud computing data science and iot we look at each in turn below the concept of cloud computing first came to prominence in the last decade for example amazon introduced amazon web services as an early cloud offering in 2006 it has really been in the last five years though that the area has exploded in terms of scale and sophistication of the underlying services on offer the cloud is defined as a set of internet based application storage and computing services sufficient to support most users needs thus enabling them to largely or totally dispense with local data storage and application software coulouris et al 2011 cloud computing further promotes the view of everything as a service from low level services such as data storage or virtualised machines through intermediary middleware services supporting parallel distributed computing or database facilities through to a plethora of applications referred to as infrastructure as a service iaas platform as a service paas and software as a service saas cloud computing may be offered by companies and made available to others as services i e public clouds such as those offered by amazon google ibm microsoft and yahoo or private clouds that can be established within an organisation or associated community e g using open source software such as openstack or cloudstack hybrid solutions are also possible where an organisation may have their own private cloud but extended with extra capacity from public clouds there is also a move in cloud computing from owning resources to a more elastic use where resources can be requested and paid for in the case of public clouds only when required the growth of cloud computing over the last five years in particular has been phenomenal for example a report by cisco indicates that in 2015 total data storage capacity in data centres is 382 eb with this projected to grow to 1 8 zb by 2020 18 18 https www cisco com c dam en us solutions collateral service provider global cloud index gci white paper c11 738085 pdf there has also a corresponding growth in processing capabilities and innovation around cloud services most notably for the purposes of this paper in the area paas with a wide range of new services introduced to storage and process massive data sets e g bigtable cassandra and hbase in terms of big data storage and mapreduce and apache spark in terms of distributed computation we return to this innovation in section 4 2 below the developments in cloud computing have also stimulated interest in big data or more generally data science that is the science of analysing and making sense of very large and or highly complex data sets this is a fundamentally cross disciplinary area of study involving for example mathematical sciences computational sciences and areas of application to support this a number of such cross disciplinary institutes have been set up worldwide including the alan turing institute in the uk and data science institutes at berkeley and columbia in the united states and imperial college london ucl warwick and lancaster also in the uk amongst many others with the huge investments in data science there is a growing body of literature on techniques to extract meaning for large and complex data sets including techniques that embrace unstructured data more importantly there is a dialogue across disciplines to understand how different techniques can work together to resolve major challenges around big data a lot of the research in data science is targeted at underlying algorithms and their scalability and efficiency there is also an emphasis on more applied research most notably in the areas of ecommerce and marketing smart cities logistics and transport and also health and wellbeing blair et al 2019 there is also huge potential in data science for the natural environment although perhaps surprisingly this is an area that is relatively under developed it is though one of the major themes of the data science institute 19 19 http www lancaster ac uk dsi at lancaster university uk finally there have been significant developments in the area of iot with the internet evolving from being an internet of computers to one that is an internet of things with the things being everyday objects with embedded intelligence atzori et al 2010 experts predict that iot will embrace over 50 billion devices by 2020 see fig 6 as with data science the main growth areas are expected to be around smart cities logistics and transport and health and wellbeing there is also significant potential for iot deployments in the natural environment for example nundloll et al 2019 describe an experiment in deploying an environmental iot in a catchment in wales it is clear however that this is an area in its infancy the real significance of iot technology in this area is when data can be combined with other sources including remote sensing earth monitoring technologies historical records and other data mined from the web in support of models of everything as discussed in section 2 2 these technologies are mutually supportive in that cloud computing provides the underlying very large scale and elastic pool of resources and associated services to store process and present very large data sets data science provides a range of methods to make sense of complex data and extract meaning from this data and iot technology provides access to real time observations on a very large scale this symbiotic relationship is illustrated in fig 7 4 2 addressing the requirements 4 2 1 underlying technological infrastructure the underlying technological infrastructure has changed significantly in terms of both the availability of large scale computational resources and also the stability of the associated platforms there has also been significant innovation in this area with an explosion of new services now available the developments in cloud computing as documented above are particularly significant in this regard whereas grid computing was a rather niche and immature technology cloud computing provides access to an abundance of underlying resources in terms of both computation and storage and also an ever increasing set of associated services the services most relevant for models of everywhere include a rich underlying set of programming constructs to support distributed programming including service oriented architecture containers and microservices serverless computing e g docker 20 20 https www docker com and rocket 21 21 https coreos com blog rocket html for containers and openwhisk 22 22 https openwhisk incubator apache org and aws lambda 23 23 https aws amazon com lambda for microservices serverless approaches services to support the subsequent deployment and execution of complex distributed executions e g kubernetes 24 24 https kubernetes io and zookeeper 25 25 https zookeeper apache org a range of underlying storage architectures that cater for very large scale and highly heterogeneous data sets including unstructured data e g cassandra 26 26 http cassandra apache org hbase 27 27 https hbase apache org and mongodb 28 28 https www mongodb com parallel and distributed programming paradigms to process and manipulate such data sets including historical and streaming data sets e g the hadoop framework 29 29 http hadoop apache org mapreduce dean and ghemawat 2008 spark 30 30 https spark apache org and pig 31 31 https pig apache org techniques to semantically enrich and subsequently navigate very large scale and highly heterogeneous data sets e g building on technologies such as owl sparql and rdf 32 32 https www w3 org standards semanticweb and also graph databases such as graphdb 33 33 http graphdb ontotext com allegrograph 34 34 https allegrograph com or neo4j 35 35 https neo4j com software frameworks and associated libraries to support data analytics e g mahout 36 36 http mahout apache org or rstudio 37 37 https www rstudio com services to support scientific workflow in the cloud e g taverna 38 38 http www taverna org uk and kepler 39 39 https kepler project org there is also strong interest in achieving integration between cloud computing and iot technology although this work is at early stages of development most significantly there is a rapidly growing body of research around edge computing sometimes also referred to as fog computing to provide intermediary storage and processing capabilities closer to end devices lopez et al 2015 for example edge devices can be used to carry out initial analyses of real time streaming data from iot devices with only aggregate or significant data then sent to the cloud environment edge computing can also support the integration of mobile devices ahmed and ahmed 2016 the technological landscape has therefore changed dramatically with many of the technologies now in place to support models of everywhere a number of significant barriers though still remain most notably the lack of standardisation in cloud computing with different providers offering quite distinct programming paradigms and apis this leads to problems of vendor lock in and also difficulties in managing computations that span multiple providers including hybrid cloud environments embracing public and private providers there are also difficulties in programming and managing the underlying technological infrastructure especially when combining cloud computing with iot technology the result being a rather sophisticated but highly complex system in itself more accurately described as a system of systems jamshidi 2011 we return to this point below under deployment at scale 4 2 2 data analytics there have been similar advances in terms of data analytics there is now much more awareness of the need to move to open science including the need for open data policies governments and research funding bodies are also moving towards the need for open data and there is a similar move towards more open reproducible or repeatable science 40 40 http royalsociety org uploadedfiles royal society content policy projects sape 2012 06 20 saoe pdf it is fair to say though that important barriers remain and these tend to be cultural rather than technological 41 41 http crc nottingham ac uk projects rcs openscience report sarah currier pdf in terms of making sense of data the emergence of data science as a discipline is strongly encouraging albeit with a need to attract more data scientists to work on environmental challenges and problems blair et al 2019 the most important development has been the cross disciplinary dialogue that is now happening within the data science community involving statisticians computer scientists and domain experts amongst others this is very significant and is leading to breakthroughs in terms of efficient algorithms and their application in important societal problems in the data science institute at lancaster for example we are interested in how contemporary techniques such as extreme value theory changepoint analysis time series analyses and statistical machine learning can be applied to complex environmental data we are also particularly interested in how resultant data models can co exist and inform process models combining stochastic and deterministic understanding of complex environmental phenomena while there is increasing awareness of the potential of such approaches this is a relatively immature area solutions tend to be ad hoc and a more principled understanding of how such techniques can work together has yet to emerge there is also a similar narrative around reasoning across scales while there is more experience of this in the earth and environmental sciences the solutions are also quite ad hoc and often not shared across different areas of study in conclusion there have been significant developments since 2012 particularly in terms of the required cross disciplinary dialogue around data science for the natural environment nevertheless this work is still at a relatively early stage of maturity with important and fairly unique challenges of this area still to be addressed blair et al 2019 4 2 3 modelling as a learning process as discussed above many of the building blocks for models as a learning process were already in place by 2012 albeit fragmented across different communities the state of the art now is quite similar and there remains a need for stronger cross disciplinary dialogue between researchers working on environmental modelling data science and adaptive autonomic computing the most significant changes in this time have been i advances in areas such as statistical and machine learning that directly supports meta reasoning about model selection and rejection and ii the computational capacity offered by the cloud which supports both the execution of complex environmental models in the cloud and the execution of associated reasoning algorithms there has been little progress on the crucial area of uncertainty in terms of representing uncertainty explicitly in computations and also reasoning about uncertainty as part of the decision making process more generally one of the most significant developments over this time period in the environmental sciences has been the recognition of the unavoidable uncertainties associated with predictive models whether used for simulation or forecasting purposes e g beven 2009 as noted earlier a primary driver for the models of everywhere concepts was the potential for using local information to constrain local uncertainties in predicting local variables this is not just a problem of assessing the statistics of model residuals though many studies have approached the problem in this way this is because many sources of uncertainty are the result of lack of knowledge about processes variables or forcings particularly into the future that are not necessarily easily represented in simple statistical forms in particular input uncertainties will be processed through the nonlinear dynamics of a model to produce complex nonstationary residual structures that will then interact with uncertainties in the observational data used in model evaluation which might also have associated epistemic uncertainties e g in hydrology arising from the rating curves used in the estimation of river flows see westerberg et al 2011 westerberg and mcmillan 2015 coxon et al 2015 these issues underlay the development of the generalised likelihood uncertainty estimation glue methodology e g beven and binley 1992 2014 beven 2006 2016 which includes some statistical methods as special cases as new data become available it should be possible to learn more about the characteristics of the uncertainties associated with different predictands at least where the new data are informative that this may not always be the case has been shown by beven et al 2011 and beven and smith 2015 in doing so it will be possible to combine prior information with the new information to update the estimates this leads naturally to a form of bayesian reasoning where uncertainties can be represented as probabilities but much more research is needed in environmental models to understand how best to define the likelihoods used in the bayesian methodology simple statistical likelihood functions used with multiplicative bayesian updating appear to lead to overconditioning of model parameters because they do not take any account of the epistemic nature of sources of uncertainty e g beven et al 2011 beven 2016 2019 there are also issues of whether even the best models might be fit for purpose for the type of decisions that they might be used for see the discussion of beven and lane 2019 a critical aspect of the models of everywhere concept is the potential for using local knowledge within this learning process to improve the representations of places this is where information from local stakeholders and the internet of things might be used in local model evaluations to reject potential model structures and constrain uncertainties in parameterisations and outcomes this can be considered as an extension of the collaborative and participatory learning that has already been used in a number of local flood risk assessments and water resource management projects e g lane et al 2011 landström et al 2011 evers et al 2012 maskrey et al 2016 ferré 2017 basco carrera et al 2017 see also voinov et al 2016 an important component of this learning process is the potential to visualise model outcomes at scales that allows consideration of local detail by local stakeholders so that different scenarios and their uncertainties can be explored in collaborative ways hankin et al 2017 see below 4 2 4 deployment at scale there have been several important developments in terms of deploying at scale with containers in particular making it far easier to deploy and subsequently manage executing models in the cloud in a platform independent manner the availability of cloud based workflow engines is also significant although there are questions over whether workflow offers the right abstraction for all elements of environmental modelling blair et al 2019 more generally there is still a problem implementation gap france and rumpe 2007 between what scientists would like to do in the cloud and the level of support offered by existing technologies and services with a prior knowledge of the underlying technical details required this makes it very time consuming and also error prone to execute environmental models or ensemble models in the cloud and also requires access to computing expertise which may be a scarce resource in many environmental research labs in the context of models of everywhere the models may themselves be quite complex involving different ensembles of process models or the integration of process and data models for example this makes the cost quite prohibitive especially when this would entail the deployment of many instances of these models at many different places and scales software frameworks offer a promising technology to support the more rapid deployment of recurrent software architectures johnson 1997 software frameworks are tailored towards particular domains of application abstracting over the lower level details and capturing the commonalities within that domain while allowing some degree of specialisation they are heavily used in cloud computing for example mapreduce abstracts over the complexities of managing a large and complex underlying cloud infrastructure and supports the execution of distributed algorithms in the cloud allowing the user to plug in and specialise the computation through providing application specific map and reduce operations dean and ghemawat 2008 at present though such frameworks tend to be relatively generic e g dealing with distributed computation and are not specific enough to support something as domain dependent as environmental models in terms of programming models distributed objects have now been replaced by alternative paradigms supported in the cloud around service oriented architecture enhanced by concepts such as deployment in containers and optionally support for microservices this approach overcomes the problems associated with distributed object technology being much better suited to large scale internet wide deployment some research is required though in terms of how to map models of everywhere on to such programming concepts 4 3 overall assessment and technological readiness it is apparent from the discussion above that there have been significant advances in the underlying technology to support the vision of models of everywhere equally a number of barriers remain our overall assessment is summarised in table 3 repeating the style of analysis carried out for the period 2007 2012 in table 2 from this analysis we can see that there have been significant shifts in readiness around the underlying technological infrastructure and in data analytics and also partially around supporting deployment at scale support for modelling as a learning process has not changed much although the developments in cloud computing and data analytics does offer the potential as yet unrealised of significant advances in this area the need for cross disciplinary dialogue is a common theme across all these areas and is crucial in terms of addressing the remaining barriers overall we conclude that in terms of technological readiness the time is right to carry out large scale experiments of the concept of models of everywhere the next section explores ongoing research in this area 5 initial experiments and research roadmap ongoing research at lancaster is looking at an experimental deployment of the concept of modes of everywhere in the area of hydrology supported by recent developments in cloud computing data science and new sources of data including but not limited to iot technology the initial deployment is targeting a specific place with the intention of having a modelling framework that is able to capture and indeed learn the idiosyncrasies of that place the overarching goal of this work is to identify software architectural principles for implementing models of everywhere in the cloud with a view to supporting more widespread deployment of models of everywhere at different places and at different scales discussed further below we are also strongly interested in supporting decision making at different scales for example over the potential effectiveness of different natural flood management strategies and also over how to use constrained national or regional budgets most effectively the high level systems architecture is as shown in fig 8 below this recognises the existence of multiple sources of data and the importance of integrating this data and in turn looking at model integration on top of this which includes both data and process models coupled together the top layer then supports interrogation and querying of the information about that particular place this maps on to a more detailed cloud based systems architecture exploiting the range of services supported by the cloud in each of these areas this is shown in fig 9 through this work we intend to overcome the remaining technological barriers around models of everywhere and therefore open the door to the desired more widespread deployment of the concept in hydrology and beyond further details of this research can be found in towe et al 2019 with edwards et al 2017 also discussing human and societal dimensions of the research around decision making our overall research roadmap is summarised in table 4 which shows the key research questions and associated areas of investigation having deployed the concept of models of everywhere at a given place we then hope to consider how to generalise the approach to model other environmental facets at that place to model other places including places at other scales and to support coherent reasoning across scales this also involves key questions over discretisation especially given the fact that data may exist at different scales for a given place in the longer term we will also be interested in how the concept can be applied to other areas of environmental science including biodiversity and soil management and also how models of everywhere can help us in understanding the inter dependencies across such areas a key motivation of models of everything as discussed above this quickly becomes a large research agenda that goes beyond the scope of our research study and we hope to stimulate other research to address these key issues 6 related work there are of course already models of everywhere and to some sense everything in the sense of global earth system science models that have been developed from global atmosphere and ocean dynamic circulation models examples are the japanese earth simulator habata et al 2003 ec earth hazeleger et al 2010 and the community earth system model hurrell et al 2013 while these are still limited to grid resolutions of kilometres for global applications these systems commonly include the possibility of nesting finer grid domains with boundary conditions provided by global simulations the philosophy of such approaches however has been quite different from that presented here the model structure and parameterisations are generally fixed so that application everywhere has been a matter of finding appropriate effective parameter values for different grid locations using whatever data might be available there have also been some attempts to produce distributed modelling systems that could be applied widely at finer grid scales allowing for a more flexible choice of structures in hydrology for example there was the inter agency object modelling system oms of leavesley et al 2002 that developed into a more general modelling system lloyd et al 2011 david et al 2013 more recently clark et al 2015 have proposed the structure for unifying multiple modeling alternatives summa framework in both cases several different model representations were provided for the user to choose from in producing a models structure for a particular catchment area within these systems the expertise of users can be elicited to define appropriate model structures although identification of appropriate model parameters and hypothesis testing of competing model structures are still major issues e g weiler and beven 2015 the type of approaches presented here could be used with such systems in flood risk management there are inherent motivations to view modelling as a process of learning about places stemming from two distinctive features of the problem firstly the likelihood and impacts of flooding although driven ultimately by weather and climate are strongly influenced by local features of landscapes land use and human activities in some cases even very small topographic features or infrastructure assets can have a significant control on flood risk for example by directing the flow of flood waters towards or away from buildings this information is not always captured well if at all in generic model structures and data sets secondly the assessment of flood risk involves gathering information about extreme events which tends to place an emphasis on historical knowledge often reliant upon detailed knowledge of the locality for interpretation and on the updating of risk assessments as new observations become available for these reasons some flood risk management applications already implement frameworks for iterative co production of modelling based on the incorporation of knowledge about specific localities from multiple stakeholders one such system 42 42 https www youtube com watch v lewteldwtsu feature youtu be t 74 has been developed for the flanders environment agency vlaamse milieumaatschappij vmm for mapping areas at risk of flooding from surface runoff here a web based interface creates a shared collaboration space enabling local partners such as town councils or local water and sewer managers to engage in a dialogue about model improvements fig 10 important features that have been incorporated within the modelling through this process include areas where flood water can be held back by embankments or drained by pumps and control structures e g gates sluices weirs culverts that are known to local staff and may not be represented adequately without that detailed local knowledge such as the flood retention storage and flow control structures represented in fig 10 the co production website is shared with professional partners and within its first three months of operation enabled more than 9000 detailed improvements to data and modelling to be implemented together with nearly 300 re simulations for 103 sub models involving 150 organisations and resulting in positive evaluations of model improvements at 500 locations across the whole of flanders the vmm example discussed here explicitly exposes modelling as part of a process of learning about place through knowledge sharing supported by digital technology a more constrained example is the flood hazard mapping of the fema national flood insurance program 43 43 https www fema gov national flood insurance program flood hazard mapping where model based flood risk assessments are published and can be challenged by individuals on the basis of detailed local knowledge via a web based facility stakeholder interaction with the outputs of models of everywhere can also made more direct and local to demonstrate and explain assumptions and to alter inputs or outputs as a way of iterating to a co produced model of a place in this way local models will be better constrained by the local stakeholder information and more trusted if done well as an example of this way of working a series of workshops sponsored by natural england were used to engage local farming communities on the potential benefits of working with natural processes wwnp to mitigate flooding often called natural flood management see hankin et al 2017 for modelling concepts two engagement devices were used an augmented reality sandbox was used to provide real time feedback on the response of flow pathways to a user sculpting channels in sand virtual inputs to the sandbox are controlled by waving a hand over the sandbox water flow pathways and storages are then shown by projection of blue onto the sand this was used as a precursor to the demonstration of more quantitative modelling results with visualisations being projected onto a large interactive itable and shown in fig 11 engagement using the itable followed 4 key steps 1 the model and modelling assumptions is explained following a general discussion of nfm a baseline run of the model under flood conditions is discussed for acceptability in terms of local knowledge of patterns of flooding 2 a gis package is used to show different layers for the local catchment and bring in layers of potential opportunities that might be based on national strategic layers 44 44 http naturalprocesses jbahosting com these are discussed with the participants and options and be switched on and off according to where the catchment partners identify where they would be happy to try different sorts of nfm 3 the measures are then plugged into the model and the model is run to predict the outcome of the changed configuration 4 the distributed changes to the hydrological responses are explored with the partners to understand model behaviour and effectiveness fig 12 shows the outputs following one of the workshops interestingly in some discussions of significant measures in front of their peers landowners came up with interventions that were very significant for example sacrificing some summer irrigation storage to act as flood storage areas in the winter season this process of standing around the tables and discussing the catchment with peers appeared to make people more forthcoming and the process more effective the process does however require a different approach to modelling since some of the feedback from local stakeholders might not be positive it is therefore important that the modellers involved should not be too protective about their model but should recognise and explain the assumptions and uncertainties inherent in the modelling process and be prepared to incorporate new knowledge as far as possible finding ways of conveying and if necessary recalculating prediction uncertainties within this context is the subject of on going work 7 conclusions this paper has carried out a systematic analysis of the technological readiness for the concept of models of everywhere in particular the paper has examined the various dimensions associated with models of everywhere and determined a set of technological requirements that must be met for the successful large scale deployment of the concept this set of requirements was then used to compare technological readiness when models of everywhere was first proposed against the readiness levels now showing that the time is right for widespread experimentation and deployment of the concept although many of the technological barriers have been removed key research issues remain and the paper has highlighted a set of open research questions that must be addressed before progress can be made importantly this research agenda represents a shift in environmental modelling from an approach centred on process understanding through deterministic models to one that embraces a more data centric perspective whereby the two approaches can work in tandem to achieve a deeper understanding of specific places and hence to support more nuanced decision making about a given place there are also key limitations of the models of everywhere approach including the computational requirements if rolled out on a large scale and also the advances needed in environmental science to develop scale dependent parameterisations and embrace an underlying science of everything everywhere and at all times in conclusion the concept of models of everywhere is even more important than when it was first proposed given the environmental challenges we face and this paper has demonstrated that the time is right for more large scale experimentation with the concept further research though is clearly needed to deliver against this vision and this research has to be fundamentally trans disciplinary in nature bringing together environmental scientists data scientists and computer scientists to reach a common understanding of representing complex environmental data making sense of the resultant highly heterogeneous data integrating knowledge from process and data models and rolling out the concept of scale equally importantly there is a need to work closely with social scientists to understand the human and societal issues related to models of everywhere including the necessary cultural shift to open data treatments of security and privacy and the role of communities in ensuring models represent the peculiarity of places acknowledgements this work is supported by epsrc grants ep p002285 1 and ep n027736 1 blair s senior fellowship on the role of digital technology in understanding mitigating and adapting to environmental change and models in the cloud generative software frameworks to support the execution of environmental models in the cloud respectively we would also like to thank our colleagues in the data science institute and the centre for ecology and hydrology ceh for input and for providing such an inspiring environment for cross disciplinary dialogue around data science for the natural environment finally we thank the terrific inputs from a wide range of researchers and practitioners in partner organisations including from jba trust jba consulting the environment agency united utilities the european centre for medium range weather forecasts the oasis loss modelling framework and the environmental change institute oxford university 
26124,the concept models of everywhere was first introduced in the mid 2000s as a means of reasoning about the environmental science of a place changing the nature of the underlying modelling process from one in which general model structures are used to one in which modelling becomes a learning process about specific places in particular capturing the idiosyncrasies of that place at one level this is a straightforward concept but at another it is a rich multi dimensional conceptual framework involving the following key dimensions models of everywhere models of everything and models at all times being constantly re evaluated against the most current evidence this is a compelling approach with the potential to deal with epistemic uncertainties and non linearities however the approach has as yet not been fully utilised or explored this paper examines the concept of models of everywhere in the light of recent advances in technology the paper argues that when first proposed technology was a limiting factor but now with advances in areas such as internet of things cloud computing and data analytics many of the barriers have been alleviated consequently it is timely to look again at the concept of models of everywhere in practical conditions as part of a trans disciplinary effort to tackle the remaining research questions the paper concludes by identifying the key elements of a research agenda that should underpin such experimentation and deployment keywords environmental modelling models of everywhere grid computing cloud computing data science 1 introduction the concept of models of everywhere was introduced by beven in 2007 beven 2007 and revised in a follow up paper beven and alcock 2012 the concept is fundamentally about having a stronger association between a given environmental model and the place that it represents in the 2012 paper they argue that it is useful and even necessary to think in terms of models of everywhere and this will change the nature of the modelling process from one in which general model structures are used in particular catchment applications to one in which modelling becomes a learning process about places the necessity stems from the need to constrain uncertainty in the modelling process in order to support policy setting and decision making particularly around water management e g flooding and water quality although the principles can also potentially apply to other areas of environmental modelling and management in the rest of the paper we will tend to use examples and illustrations from hydrology and flood modelling although we stress this potential generality of the approach this is a compelling argument and a reaction against the view that there can be generic environmental models capable of representing processes and behaviours across multiple places and indeed across multiple scales such general models are expensive to develop difficult to maintain and to apply because of their data demands and need for parameter estimation or calibration beven 2007 they also have problems in dealing with local epistemic uncertainties and non stationarities for example caused by change in local characteristics and climate drivers e g prudhomme et al 2010 beven 2009 2016 some examples of models of everywhere have been deployed but for relatively constrained applications and at specific scales however the concept has not been developed to the extent that the authors envisaged where everywhere is represented across all scales in a coherent way this paper re examines the concept of models of everywhere from a technological perspective arguing that at the time the underlying technology was not sufficiently advanced to support the concept now however this has changed with significant developments in areas such as data acquisition techniques data storage and processing technologies and data analytics capabilities alongside a move towards a more open science supported by these developments note that in developing ideas of models of everywhere we mean something quite different to the hyperresolution models that are starting to be used in earth systems science e g wood et al 2011 beven and cloke 2012 bierkens et al 2015 gilbert and maxwell 2017 with resolutions of the order of 1 km the latter do not as yet provide simulations and visualisations at scales that local stakeholders can relate to directly see the discussion of beven et al 2015 this is a critical aspect of how the models of everywhere concept has the potential to change the way that modelling is done both approaches do however focus attention on a requirement for scale dependent parameterisations that has proven difficult to resolve e g mcdonnell and beven 2014 the overall aim of the paper is to determine the current feasibility of models of everywhere particularly in the area of hydrological modelling given the state of the art in underlying technology this breaks down into the following objectives 1 to carry out a detailed examination of the concept of models of everywhere to determine key underlying technological requirements 2 to compare the state of the art in technology in the period 2007 2012 and the present day to evaluate whether the time is now right for a widespread deployment of models of everywhere 3 to provide a research roadmap to support such deployment in terms of outstanding research questions and challenges note that there are other issues related to models of everywhere that also should be addressed most notably human and societal issues such issues include the need to move towards open science and open data and the role of communities in improving models in representing local places these are alluded to in the paper but a full treatment of this important dimension is beyond the scope of the paper we elect instead to focus on technological readiness the work is being carried out in the context of a significant re evaluation of approaches to flood modelling and associated risk management for example the uk government s national flood resilience review published in september 2016 1 1 https www gov uk government publications national flood resilience review included important recommendations around improvements to long term modelling capabilities the review also encouraged the use of natural flood mitigation methods or working with natural processes this concept involves the use of many distributed in channel and off channel storage features coupled with changes of land use to try to retain more flood runoff in catchment headwaters or at least slow down its arrival to areas at risk of flooding see dadson et al 2017 there are many current projects in the uk that are implementing natural flood management measures very few however have been associated with detailed monitoring of changes to the flow or the operation of individual mitigation measures additionally there are issues about whether the strategy will be effective under extreme flood events which in the uk are often preceded by a period of prior catchment wetting see metcalfe et al 2017 hankin et al 2017 in fact by slowing the flow in some parts of the catchment it is possible that the peak flow might increase elsewhere this is called the synchronicity problem the impact of which will vary from event to event because of the different patterns of rainfall intensities and with changes in the scale of catchment being considered the distributed nature of this problem and the potential for such mitigation effects to have impacts on other environmental factors requires an integrated catchment modelling approach to evaluate possible implementation scenarios however the outputs from such models will be associated with significant uncertainty even after calibration on historical data thus this is a prime example where the concept of models of everywhere and the local constraint of uncertainty using local information would be useful especially when assessing the uncertainty in potential outcomes might make a difference to the decision that might be made a re evaluation of the concept of models of everywhere is therefore very timely the paper is structured as follows section 2 examines the concept of models of everywhere in more depth highlighting the different dimensions behind this initial vision and culminating in a set of technological requirements to support models of everywhere section 3 looks at the technological landscape as it existed in the period 2007 2012 systematically reviewing the different technological requirements and concluding with an overall assessment of technology readiness at that time section 4 repeats this analysis but looking at the state of the art now the paper then presents ongoing research in this area including the identification of a research roadmap for the implementation of the concept of models of everywhere section 5 section 6 documents related work including existing deployments of the concept of models of everywhere finally section 7 concludes the paper with some final reflections on models of everywhere from a technological perspective 2 models of everywhere unpicked while models of everywhere at one level is quite a straightforward concept representing as association of models with particular places at another level it is a rich multi dimensional conceptual framework in particular we discuss three mutually supportive dimensions with the goal of highlighting the technological requirements to support the overall vision models of everywhere models of everything models at all times these are discussed in turn below 2 1 models of everywhere 2 1 1 key characteristics the starting point of models of everywhere is to move from generic models that can then be customised to particular locations for example through appropriate parameterisation to models that are specific to particular places as such they can be tailored to represent the behaviour at a specific place without the need to represent any other place in particular observations and inputs from local stakeholders can be used to constrain the uncertainty that is associated with environmental modelling e g beven 2009 note that models of everywhere is often interpreted as models representing specific localised areas but the concept does not imply any particular scale rather models of everywhere can represent local regional national and global scale with these models often co existing while there is an expectation that model parameterisations should be resolution dependent e g mcdonnell and beven 2014 beven 2019 in the absence of any adequate scaling theory for many environmental processes particular models may need to be tailored for the scale at which they operate in terms of both process representations and effective parameter values the approach is illustrated in fig 1 a generic model with a specific set of parameter values cannot represent flooding at all areas of the catchment and hence five localised models need to be developed a core motivation of models of everywhere is to constrain uncertainty exploiting as much knowledge as is available about a particular place this is developed further when we look at models of everything and models at all times in sections 2 2 and 2 3 below 2 1 2 technological requirements the main requirement of models of everywhere is very large scale computational capacity for example if this approach were to be adopted for future flood prediction there would be a need for the deployment of very large numbers of models all over the country at different scales for illustration consider models applied to support flood and coastal risk management fcrm in england for any given community city town village or street there may be an array of models developed paid for and used by different organisations for different purposes and using different data resources or very often common data sets exploited in different ways most localities are included within national scale models sometimes referred to as strategic with the outputs of the national flood risk assessment nafra 2 2 https www gov uk government publications flooding in england national assessment of flood risk published as risk of flooding from rivers and sea 3 3 https data gov uk dataset risk of flooding from rivers and sea1 being typically the most generic separate models have been applied to provide mapping of the flooding from surface water 4 4 https www gov uk government publications flood maps for surface water how they were produced ubiquitous in coverage representing potential flooding from overland flows and ponding rather than water overflowing from rivers or the sea the risk of flooding from reservoirs predicting places at risk in the event of dams and impoundments being breached and groundwater flooding more detailed models also exist in many places to support activities such as the economic appraisal of proposed flood defence schemes flood risk assessments for proposed floodplain developments or the detailed design construction and maintenance of drainage systems these models usually capture further information about infrastructure e g bridges culverts weirs sluice gates and river channel surveys organisations commissioning and owning such models may include the environment agency which leads on fcrm in england for local government water companies supplying drainage services private developers or other landowners it is usual for all of the above models to evolve over time incorporating both new data and technical improvements e g better numerical solution schemes it is also common for multiple instances of each model to be executed i e many individual runs or simulations to support scenario or uncertainty analysis the environment agency has over 1500 such detailed local models and reported in its 2010 2015 modelling strategy 5 5 https www gov uk government uploads system uploads attachment data file 292949 geho0310bsbt e e pdf an investment of approximately 17 million per year in modelling and mapping and an additional 15 million in gathering and processing data to support fcrm this has significant resource requirements in terms of the number of processors or virtual machines to run these models and data storage as well as the human costs in terms of developing and tailoring the models for given places and analysing and understanding the outputs for example production 6 6 https www whatdotheyknow com request 208078 response 520170 attach 3 national 20modelling 20and 20mapping 20method 20statement 20may 202013 pdf of the risk of flooding from surface water maps cited earlier involved more than 70 000 individual simulations of flood inundation on a mosaic of approximately 7100 36 km2 tiles covering all of england run on a 2m 2m resolution digital height map that included over 91 000 manually determined corrections this process needed around two months for data preparation and one month of computer processing time fully utilising a grid of over 100 gpu accelerated pcs where possible technological and operational support would need to be provided for such development this also asks important questions over the underlying distributed systems architecture to support such massive deployment e g centralised distributed or decentralized or indeed combinations of different approaches the approach also asks fundamental questions over the relationship and consistency of models at different scales and how to support reasoning across scales in terms of supporting a deeper understanding of the science and all its complexities and inter dependencies 2 2 models of everything 2 2 1 key characteristics the second dimension is concerned with exploiting information about a place in particular coupling a model of that place with as much local data as can be collected thus embracing the heterogeneity of available data sources the availability of such data is increasing significantly and now includes blair et al 2019 remote sensing data collected by satellites or aircraft borne instruments including drones other monitoring technologies that consist of a range of sensor technologies typically in close proximity with the observed phenomena including the use of internet of things iot technologies to provide real time streaming and multi faceted data about the natural environment historical records held in a variety of locations and scales the increasing amount of data available from national local government and other open data portals often increasingly offering apis e g data gov uk data mining provides additional information from the web or social media data collected from citizen science with the potential to direct citizen science to areas of data scarcity together this adds up to the potential for having environmental data at an unprecedented scale blair et al 2019 for example if focusing on flood prediction it is possible to use a variety of data sources such as historical parish records and flood marks satellite imagery local sensors photographs from social media and citizen science to help steer process based hydrological models indeed many researchers are advocating such approaches in hydrology e g di baldassarre et al 2009 smith et al 2009 2015 and more generally in disaster risk reduction mccallum et al 2016 the concept also naturally extends to other aspects of environmental science for example collecting and analysing data around water quality issues biodiversity or soils and indeed the inter dependencies between them the additional dimension of models of everything is shown in fig 2 the concept of models of everything has the potential to further reduce the uncertainty around predictions for a variety of different variables of environmental interest in a coherent way this is particularly important where the relevant processes are intrinsically coupled for example the water flows that drive the transport of nutrients from farmland and households into rivers and lakes this constraint of uncertainty is important because many of the sources of uncertainty are epistemic in nature epistemic uncertainties are those are those that arise from lack of knowledge in contrast to aleatory uncertainties that represent random variability that derives from irreducible natural variability see beven 2009 2016 rougier et al 2013 beven and hall 2014 di baldassarre and beven 2016 by definition it is not possible to deal with epistemic uncertainties in process models without breakthroughs or deepening of knowledge about a given place and its states and behaviours beven et al 2015 also talk about the role of models of everything in overcoming what they refer to as hyperresolution ignorance in modelling that is evaluating the hyperresolution information produced by simulation to overcome the local lack of data and unknowns in scientific understanding e g the understanding of subsurface structures in hydrology 2 2 2 technological requirements models of everything adds significantly to the requirements imposed on the computational infrastructure around five key areas 1 how to store the big data with models of everywhere there is a need to capture and store significant quantities of data about a given place and then repeat this across all places this therefore very quickly becomes a big data problem in many ways though this is more demanding than many areas of big data given the high level of heterogeneity in the data sets with some of the data being structured and other elements being unstructured and inevitably captured in a wide variety of formats blair et al 2019 2 how to represent and manage the collected data given the variety and heterogeneity discussed above there is a need to represent evaluate and manage the overall collection of data and this must include support for interoperability data discovery and also the association of appropriate meta data and ontologies including provenance information 3 how to ensure open access to data the concept of models of everything implies a move towards open data where data is openly available for use and stored in a way that allows such open access also important to support a more collaborative and cross disciplinary science as required to interpret this data as mentioned in the introduction while this is technically straightforward to achieve this requirement is more concerned with cultural issues for example around the perceived value of data note that ideally this open philosophy would also extend to models with models available as open source 4 how to make sense of the heterogeneous data elements it is one thing to have access to this rich underlying data but it is another thing to be able to make sense of this data and therefore data analysis techniques are also required to build this higher level of understanding from the underlying data cf environmental data science blair et al 2019 this will inevitably imply the construction of data models using a rich array of statistical and machine learning techniques 5 how to combine process models with data models once the data models are constructed there is a need to couple the data model or models with process models to build a complete understanding of a given place techniques are therefore required to support model coupling between process and data models in hydrology it has been shown that even hydrological observations may not always be informative in model calibration and validation e g beven and smith 2015 there are also important human and societal issues around privacy and security but as mentioned in the introduction this area is not considered in this paper but is an important area of future investigation note that the execution of additional data models and the need to allow for observational uncertainties also increases the computational requirements of the system 2 3 models at all times 2 3 1 key characteristics the final dimension is that models should be active at all times constantly re evaluating what is known about particular places and adapting accordingly this does not necessarily mean that models are always executing as this would consume significant computational resources without any real gain rather models should execute periodically and frequently for example when new data becomes available to understand the idiosyncrasies of particular places beven 2007 and how this might change over time at other times the model would be in a quiescent state but otherwise ready to re execute at any time this contrasts significantly with existing practice when distinct model runs are carried out infrequently under the auspices of a scientific experiment with perhaps more runs carried out an at a pre deployment phase to understand the sensitivities and uncertainties of a given generic hydrological model this iterative approach is nicely aligned with the work of box 1980 on the iterative relationship between practice and theory box s loop recently extended by blei 2014 in the context of latent variable models applied to complex data sets this may also introduce more consistency between models used for short term forecasting often applied in an adaptive framework where on line data can be used for updating and simulation models that are rarely updated this leads to a new perspective of modelling as a learning process as discussed in depth in the 2007 paper beven 2007 the 2012 paper beven and alcock 2012 develops this further talking about models as hypotheses to be tested against current and historical observations with some models being rejected in favour of others and indeed this changing over time so the current chosen model structure and associated assumptions best reflect the full idiosyncrasies of a given place as represented by numerous additional data observations this in turn leads to an adaptive approach to modelling the final aspect to consider is what can be adapted about the model there are various possibilities here increasing in level of sophistication and ambition 1 the outcomes from a model can be adapted for the purposes of real time forecasting when data can be made available for assimilation and post event analysis can then be used to inform local improvements to the model including adaptation of parameter values to best represent behaviour at the current time 2 a number of models can co exist in an ensemble approach with model selection applied to identify the best models for that given place time 3 the internal structure and behaviour of a given model can be adapted for example by changing fine grained elements of the underlying hydrology to best reflect the current place time 4 the representation of residual uncertainty can be adapted as more information is obtained locally clearly these approaches can also be combined in different ways indeed a combination of all four offers a new and radical approach to models of everywhere the concept of models at all times is illustrated in fig 3 the key motivation of models at all times is to offer a modelling framework that supports explicit reasoning about uncertainty with the explicit goal of reducing uncertainty for a given place more specifically the approach also has the potential to deal with epistemic uncertainties as argued in beven and alcock 2012 in this paper following beven 2006 the authors argue for an approach based on limits of acceptability whereby models that perform well according to such limits are acceptable and perhaps reinforced while others are rejected with this driven by the collected set of observational data cf models of everything finally the approach has significant potential to deal with non linearities and fundamental changes over time for example related to climate change with its emphasis on ongoing adaptation to the current context 2 3 2 technological requirements models at all times is crucial to the overall vision of models of everywhere but adds a whole new level of complexity in terms of the underlying technology requirements in particular the approach amplifies the underlying resource requirements and also the underlying distributed systems architecture as discussed in section 2 1 for example the approach requires the frequent execution of potentially ensemble models at large numbers of places and different scales the approach also introduces significant additional mutually supportive requirements 1 how to support adaptive reasoning as discussed above models at all times is fundamentally a learning process and implies that models are constantly adapted in response to new knowledge extracted from available data sets there is therefore the need to support this adaptive reasoning and ideally this should involve a strong element of automation as provided for example by autonomic computing supporting self adaptive systems kephart and chess 2003 mckinley et al 2004 2 how to incorporate reasoning about uncertainty building on the above it is important that adaptation decisions incorporate reasoning about uncertainty and this implies making uncertainty explicit in the modelling process and also incorporating approaches to deal with epistemic uncertainties and non linearities as inevitably encountered in such complex systems 3 how to support adaptation a truly adaptive system requires ready access to a range of elements that can be changed supporting more coarse grained adaptation is relatively straightforward and implemented in terms of selecting from different models in model ensembles or changing model parameters supporting fine grained strategies is however more challenging as this requires intimate access to the structure and behaviour of individual models in terms of for example alternative hydrological equations at the heart of the model most existing models will not provide such access i e black box implementations to fully realise the vision however we need to go further than this and provide more white box access to internal software architectures of environmental models as provided by for example reflective architectures maes 1987 kon et al 2002 models at all times also places additional emphasis on the need to integrate process and data models discussed in section 2 2 to support adaptive reasoning there is also an over arching requirement emanating from this analysis and that is the ability to support deployment at scale and this implies the ready deployment of individual models of everywhere and also of large numbers of models at different scales there are many dimensions to this scalability involving for example making it easier to deploy models in underlying computational infrastructure whether provided by hpc or cloud facilities offering software frameworks that can support the deployment of models or ensembles of models ready to be tailored for the idiosyncrasies of places and automating the subsequent adaptation learning process hence the importance of self adaptive approaches 2 4 overall analysis models of everywhere is an important and potentially crucial approach to environmental modelling particularly in terms of managing uncertainty a full implementation of the concept however imposes very significant requirements in terms of the technological infrastructure alongside other fundamentals most notably cultural elements around a move to open science incorporating more open approaches to data and modelling the approach is best understood as a combination of models of everywhere everything and at all times with this trichotomy used to analyse the overall requirements in more depth in the discussions above the resultant requirements are shown in table 1 below these requirements can usefully be clustered as follows 1 the capacity and level of sophistication of the underlying technological infrastructure in terms of both computation and data r1 r2 r4 r5 2 the availability of rich data analytics capability to make sense of complex and highly heterogeneous data sets r3 r6 r7 r8 3 the ability to support modelling as a learning process including reasoning about uncertainties r9 r10 r11 4 practical issues around deployment at scale including availability of open data and approaches to support large scale deployment r3 r12 this clustering will be used in the assessment of the changing technological landscape as discussed in sections 3 and 4 below 3 technological landscape 2007 2012 3 1 overview of the landscape in the period 2007 to 2012 the landscape was dominated by grid computing the concept of grid computing was first introduced in in the 1990s and became prominent with the publication of the seminal paper by foster and kesselman 1998 introducing the grid as a blueprint for a new computing infrastructure the term was introduced as a metaphor for the electricity grid with the goal of making computational power as accessible and ubiquitous as electricity software platforms were developed to support the deployment of applications and services in the grid most notably the globus toolkit with various versions released starting in 1997 with the last major release globus toolkit version 5 in 2009 7 7 http toolkit globus org toolkit around this time the grid was being superseded by cloud computing for example the first version of amazon web services 8 8 https aws amazon com was introduced in 2006 with rapid growth since this growth in cloud computing is discussed further in section 4 1 below in parallel researchers were becoming interested in the use of such computational power to support a range of application domains including for example ecommerce most notably in the context of this paper there was also great interest in escience that was the use of technological infrastructure including the grid to support a new kind of computationally intensive and data rich science hey et al 2009 for example in the uk the national escience programme ran from 2001 to 2010 supporting a range of infrastructure projects and application projects in areas as diverse as bioinformatics neuroinformatics and medical informatics in the environmental area the most prominent project was climateprediction net 9 9 http www climateprediction net similar large scale initiatives were launched in other countries for example in the states the national science foundation nsf funded a series of cyberinfrastructure initiatives starting around 2003 including the open science grid 10 10 https www opensciencegrid org developed by the open science grid consortium osgc 3 2 addressing the requirements 3 2 1 underlying technological infrastructure the emergence of the grid and also the escience community that coalesced around the grid provided important expertise experience and also facilities to support the development of models of everywhere however in practice and this is clear in retrospect the grid did not meet the full set of requirements to support the broader vision of models of everywhere although the vision of the grid was to provide plentiful resources on demand the reality was somewhat different at that time the availability of resources varied greatly and depended on access to one of the experimental grid facilities that were introduced in different global centres the overall distributed systems architecture was therefore one of centres at given fixed locations offering by definition relatively centralised services with partial access and limited control of these services a number of researchers explored more decentralized architectures for example climateprediction net mentioned above and seti home 11 11 https setiathome berkeley edu utilising boinc 12 12 https boinc berkeley edu a more peer to peer volunteer computing platform but such initiatives were not mainstream and not integrated into other grid initiatives it is also important to emphasise that this was a research programme and hence the underlying platforms were not stable with frequent changes over time in terms of services and facilities on offer as will be seen this contrasts significantly with what is available now in terms of both capacity and stability of services see section 4 2 more fundamentally the services on offer did not have the level of sophistication to meet the technology infrastructure requirements as identified in table 1 the main middleware technology used at the time was the globus toolkit with the overall architecture of the toolkit v5 shown in fig 4 this was a large and complex architecture with many dimensions but as can be seen the emphasis is on supporting resource sharing and at a fairly low level of abstraction as stated in the seminal paper on the anatomy of the grid foster et al 2001 argue that the grid was fundamentally about coordinated resource sharing and problem solving in dynamic multi institutional virtual organisations they go on to argue that this implies direct access to computers software data and other resources and this sharing should be highly controlled hence the emphasis was very much on meta level concerns such as standardised apis to ensure interoperability service discovery access control and resource management there was also more emphasis in practice on computational resources rather than data management for example gram grid resource allocation and management offered an architecture to submit and monitor batch jobs in the grid see fig 5 this is quite different though from the execution style required for models of everywhere as mentioned above the data side was quite primitive with an emphasis on low level facilities for access to data remotely gridftp 13 13 http toolkit globus org toolkit docs latest stable gridftp and to assist in replication of data while the grid was used successfully for a number escience experiments involving elements of big data the level of sophistication of data management was insufficient for the rich and heterogeneous data required for models of everywhere and associated needs in terms of discovery and navigation and indeed for environmental data more generally as will be seen below this is one area that has advanced significantly in the last few years there was also a general lack of experience of using grid computing for the earth and environmental sciences other scientific communities were more advanced in terms of their use of grid computing and embracing escience this led to a lack of services specific to this field e g to support environmental modelling in the grid although parallel developments such as the openmi open modelling interface standard 14 14 http www openmi org as adopted by the open geospatial consortium provided important building blocks to support model deployment and most crucially interoperability across models in summary grid computing was important in terms of establishing a community working together on distributed architectures and infrastructure and in particular for building a strong dialogue with the science community in terms of a new open computational and data rich style of science however there are a number of limitations that impacted on the feasibility of models of everywhere most notably the difficulties of access to computational and data resources the lack of sophistication of the distributed infrastructure particularly in terms of data management and the primitive nature of many of the services on offer also revisited below under deployment at scale 3 2 2 data analytics the ability to represent and access very large scale and highly heterogeneous data is important equally it is crucial to have a range of techniques to make sense of this data as can be seen above this requires a move towards open data as a prerequisite for open science the availability of a rich set of techniques to analyse data the ability to extend this reasoning across scales and an integration of process modelling with data models produced to analyse the complex data over and above the baseline requirement for storing accessing and managing large and complex data sets open data was in its infancy in the period 2007 2012 with data often regarded as core intellectual property with many institutions seeking ways to commercialise their rich data sets there was however a growing recognition with the complexity of modern science that a new more open approach to data was necessary for example the royal society published science as an open enterprise in 2012 15 15 https royalsociety org media royal society content policy projects sape 2012 06 20 saoe pdf with a core recommendation scientists should communicate the data they collect and the models they create to allow free and open access and in ways that are intelligible assessable and usable for other specialists in the same or linked fields wherever they are in the world this built on the emergence of science 2 0 waldrop 2008 seeking an open approach to science based on emerging web standards particularly web 2 0 technologies offering user generated content and a move towards a more social web in practice however at that time there were many cultural and technological barriers to a world where data sets were available for open access in common repositories in terms of making sense of data the environmental sciences including hydrology make extensive use of process models to understand fundamental processes of nature and then use these models to make future predictions a wide range of process models have been developed for example in hydrology where there have been recent attempts to incorporate multiple process components into a common framework e g fenicia et al 2011 clark et al 2015 in applications for flood risk assessment there have been many codes routinely used both by industry and researchers to model the flow of water through the landscape including interactions with physical infrastructure systems these codes can be categorised reasonably precisely in terms of the approximations made to a set of physical governing equations in the case of flood models this means simplifications of the fundamental navier stokes equations for fluid dynamics even so there remain differences in the interpretation of the prototypical physical equations the numerical schemes that are used to solve them the discretisations involved in applying those schemes to real data sets and in the very many edge cases for which special solutions are required benchmark comparisons 16 16 https www gov uk government publications benchmarking the latest generation of 2d hydraulic flood modelling packages have shown how important these differences can be in controlling the results of flood simulations in various situations there is also a strong body of research on training models based on historical data and current observations can be used to steer future states of the model data assimilation lahoz et al 2010 park and xu 2017 more generally in the time period under consideration there was a deep concern that process models alone are not sufficient and that fundamental issues remain for example reasoning about uncertainty and dealing with epistemic uncertainties and non linearities in complex systems indeed this is the prime driver for models of everything this reflects a sense that it is necessary to integrate the process model view of science with one that recognises the importance of data and associated data analytic techniques effectively data models this is a significant cross disciplinary challenge requiring input from environmental computer and mathematical scientists at that time this dialogue was not happening discussed further in section 4 1 scientists also tended to focus on specific experiments and studies to understand phenomena at a given scale so reasoning across scales was in its infancy overall even by 2012 there were major barriers around data analytics that made it very hard to support the realisation of models of everywhere 3 2 3 modelling as a learning process as discussed above the perspective of models as a learning process is the most important but also most demanding aspect of implementing models of everywhere requiring a new adaptive approach to learning from our analysis this breaks down into support for adaptive reasoning explicitly representing consideration of uncertainty in this reasoning and also being able to carry out both coarse grained and importantly fine grained adaptations in the field of computer science in the time period under consideration a deep understanding of adaptive computing developed for example ibm launched a new initiative examining autonomic systems in 2001 that is systems that can self manage mirroring the autonomic functioning of nervous system in the human body in terms of a range of self properties e g self awareness self configuration self healing and self optimisation kephart and chess 2003 more generally there was a large literature around software architectures to support self adaptation including reflective architectures the use of control loops in decision making and the use of more advanced machine learning techniques to support higher levels of autonomic self management for example dealing with unknowns oreizy et al 1999 mckinley et al 2004 however there has been little or no consideration about how such techniques can be used in terms of adaptive environmental modelling existing environmental models are also often written in older programming languages most notably fortran and tend to be monolithic black box implementations hence do not lend themselves to the implementation of fine grained adaptation strategies there is also the important requirement to represent and reason about uncertainty explicitly as part of the adaptation process at that time there was growing recognition of the need to represent uncertainty in modelling and reasoning about uncertainty across scientific experiments for example the uncertweb project introduced techniques to capture uncertainties as meta data in web based environments with details of the resultant uncertweb framework published in early 2013 bastin et al 2013 researchers had also developed a number of frameworks to reason about uncertainty in scientific experiments including seminal work by beven and binley 1992 2014 and others see renard et al 2010 vrugt and sadegh 2013 nearing et al 2016 as discussed in the 2012 models of everywhere paper beven and alcock 2012 were just starting to think about reasoning about uncertainties in model selection or rejection as a key part of models as a learning process in summary most of the building blocks were there by 2012 but the work was fragmented and split across many communities and key issues remained over how to support more advanced reasoning of uncertainties including dealing with epistemic uncertainties 3 2 4 deployment at scale finally and importantly there is the key question of whether there was sufficient advancement at that time to support deployment of the kind of scale that makes models of everywhere a reality as discussed above there are several key dimensions to support such large scale deployment including how easy it is to deploy individual models what support there is to then repeat this across many places at different scales and also whether the learning and hence tailoring process can be automated the latter issue is intrinsically inked to the support for self adaptive modelling and hence we focus more on the first two issues one of the key problems with deploying in the grid environments or indeed to other hpc facilities is the low level of abstraction offered by software platforms this was discussed in the consideration of the globus toolkit above given this the development and deployment of even an individual model is a tedious expensive and error prone process and this in itself is a barrier simm et al 2018 to the deployment of models of everywhere this is a barrier to more general deployment across a range of places where the individual models need to be specific to this place both initially and also with the model or models refined over time to reflect the particular idiosyncrasies of this place this implies some form of software framework coupled with models as a learning process and at that time this was significantly beyond the state of the art for model development it is interesting to note that the initial models of everywhere paper beven 2007 discusses an object oriented approach to programming models of everywhere mapping individual active spatial objects to places and also explicitly representing the relationship between places mainly in terms of fluxes this is an attempt to seek a higher level of abstraction to support the deployment of models of everywhere at the time of writing object oriented computing and indeed distributed objects were an important area of research reflected in the importance of technologies such as corba common object request broker architecture 17 17 http www omg org spec corba this approach is now largely superseded by alternative programming models reflecting most principally difficulties in realising distributed objects in internet scale developments 3 3 overall assessment and technological readiness it is clear from the assessment above that even by the end of this period 2012 there were major technological barriers in terms of the deployment of models of everywhere our overall assessment is summarised in table 2 which shows an overall rating against each of the requirements together with the identification of the most important barriers as can be seen from table 2 the overall readiness level is generally low to medium with important barriers remaining across all categories it is interesting to note that quite a number of the barriers are due to a silo ed approach to research and can be addressed by more cross disciplinary collaboration in this area overall we would argue that in the period 2007 2012 the vision of models of everywhere was right but the technology was not ready we continue our discussion by considering how things have advanced to date noting important developments that make an implementation of the concept more realistic 4 technological landscape current 4 1 overview of the landscape the technological landscape has changed enormously since 2012 and indeed this is one of the key drivers to revisit the concept of models of everywhere in terms of technological readiness in particular there have been three mutually supportive areas of significant innovation namely cloud computing data science and iot we look at each in turn below the concept of cloud computing first came to prominence in the last decade for example amazon introduced amazon web services as an early cloud offering in 2006 it has really been in the last five years though that the area has exploded in terms of scale and sophistication of the underlying services on offer the cloud is defined as a set of internet based application storage and computing services sufficient to support most users needs thus enabling them to largely or totally dispense with local data storage and application software coulouris et al 2011 cloud computing further promotes the view of everything as a service from low level services such as data storage or virtualised machines through intermediary middleware services supporting parallel distributed computing or database facilities through to a plethora of applications referred to as infrastructure as a service iaas platform as a service paas and software as a service saas cloud computing may be offered by companies and made available to others as services i e public clouds such as those offered by amazon google ibm microsoft and yahoo or private clouds that can be established within an organisation or associated community e g using open source software such as openstack or cloudstack hybrid solutions are also possible where an organisation may have their own private cloud but extended with extra capacity from public clouds there is also a move in cloud computing from owning resources to a more elastic use where resources can be requested and paid for in the case of public clouds only when required the growth of cloud computing over the last five years in particular has been phenomenal for example a report by cisco indicates that in 2015 total data storage capacity in data centres is 382 eb with this projected to grow to 1 8 zb by 2020 18 18 https www cisco com c dam en us solutions collateral service provider global cloud index gci white paper c11 738085 pdf there has also a corresponding growth in processing capabilities and innovation around cloud services most notably for the purposes of this paper in the area paas with a wide range of new services introduced to storage and process massive data sets e g bigtable cassandra and hbase in terms of big data storage and mapreduce and apache spark in terms of distributed computation we return to this innovation in section 4 2 below the developments in cloud computing have also stimulated interest in big data or more generally data science that is the science of analysing and making sense of very large and or highly complex data sets this is a fundamentally cross disciplinary area of study involving for example mathematical sciences computational sciences and areas of application to support this a number of such cross disciplinary institutes have been set up worldwide including the alan turing institute in the uk and data science institutes at berkeley and columbia in the united states and imperial college london ucl warwick and lancaster also in the uk amongst many others with the huge investments in data science there is a growing body of literature on techniques to extract meaning for large and complex data sets including techniques that embrace unstructured data more importantly there is a dialogue across disciplines to understand how different techniques can work together to resolve major challenges around big data a lot of the research in data science is targeted at underlying algorithms and their scalability and efficiency there is also an emphasis on more applied research most notably in the areas of ecommerce and marketing smart cities logistics and transport and also health and wellbeing blair et al 2019 there is also huge potential in data science for the natural environment although perhaps surprisingly this is an area that is relatively under developed it is though one of the major themes of the data science institute 19 19 http www lancaster ac uk dsi at lancaster university uk finally there have been significant developments in the area of iot with the internet evolving from being an internet of computers to one that is an internet of things with the things being everyday objects with embedded intelligence atzori et al 2010 experts predict that iot will embrace over 50 billion devices by 2020 see fig 6 as with data science the main growth areas are expected to be around smart cities logistics and transport and health and wellbeing there is also significant potential for iot deployments in the natural environment for example nundloll et al 2019 describe an experiment in deploying an environmental iot in a catchment in wales it is clear however that this is an area in its infancy the real significance of iot technology in this area is when data can be combined with other sources including remote sensing earth monitoring technologies historical records and other data mined from the web in support of models of everything as discussed in section 2 2 these technologies are mutually supportive in that cloud computing provides the underlying very large scale and elastic pool of resources and associated services to store process and present very large data sets data science provides a range of methods to make sense of complex data and extract meaning from this data and iot technology provides access to real time observations on a very large scale this symbiotic relationship is illustrated in fig 7 4 2 addressing the requirements 4 2 1 underlying technological infrastructure the underlying technological infrastructure has changed significantly in terms of both the availability of large scale computational resources and also the stability of the associated platforms there has also been significant innovation in this area with an explosion of new services now available the developments in cloud computing as documented above are particularly significant in this regard whereas grid computing was a rather niche and immature technology cloud computing provides access to an abundance of underlying resources in terms of both computation and storage and also an ever increasing set of associated services the services most relevant for models of everywhere include a rich underlying set of programming constructs to support distributed programming including service oriented architecture containers and microservices serverless computing e g docker 20 20 https www docker com and rocket 21 21 https coreos com blog rocket html for containers and openwhisk 22 22 https openwhisk incubator apache org and aws lambda 23 23 https aws amazon com lambda for microservices serverless approaches services to support the subsequent deployment and execution of complex distributed executions e g kubernetes 24 24 https kubernetes io and zookeeper 25 25 https zookeeper apache org a range of underlying storage architectures that cater for very large scale and highly heterogeneous data sets including unstructured data e g cassandra 26 26 http cassandra apache org hbase 27 27 https hbase apache org and mongodb 28 28 https www mongodb com parallel and distributed programming paradigms to process and manipulate such data sets including historical and streaming data sets e g the hadoop framework 29 29 http hadoop apache org mapreduce dean and ghemawat 2008 spark 30 30 https spark apache org and pig 31 31 https pig apache org techniques to semantically enrich and subsequently navigate very large scale and highly heterogeneous data sets e g building on technologies such as owl sparql and rdf 32 32 https www w3 org standards semanticweb and also graph databases such as graphdb 33 33 http graphdb ontotext com allegrograph 34 34 https allegrograph com or neo4j 35 35 https neo4j com software frameworks and associated libraries to support data analytics e g mahout 36 36 http mahout apache org or rstudio 37 37 https www rstudio com services to support scientific workflow in the cloud e g taverna 38 38 http www taverna org uk and kepler 39 39 https kepler project org there is also strong interest in achieving integration between cloud computing and iot technology although this work is at early stages of development most significantly there is a rapidly growing body of research around edge computing sometimes also referred to as fog computing to provide intermediary storage and processing capabilities closer to end devices lopez et al 2015 for example edge devices can be used to carry out initial analyses of real time streaming data from iot devices with only aggregate or significant data then sent to the cloud environment edge computing can also support the integration of mobile devices ahmed and ahmed 2016 the technological landscape has therefore changed dramatically with many of the technologies now in place to support models of everywhere a number of significant barriers though still remain most notably the lack of standardisation in cloud computing with different providers offering quite distinct programming paradigms and apis this leads to problems of vendor lock in and also difficulties in managing computations that span multiple providers including hybrid cloud environments embracing public and private providers there are also difficulties in programming and managing the underlying technological infrastructure especially when combining cloud computing with iot technology the result being a rather sophisticated but highly complex system in itself more accurately described as a system of systems jamshidi 2011 we return to this point below under deployment at scale 4 2 2 data analytics there have been similar advances in terms of data analytics there is now much more awareness of the need to move to open science including the need for open data policies governments and research funding bodies are also moving towards the need for open data and there is a similar move towards more open reproducible or repeatable science 40 40 http royalsociety org uploadedfiles royal society content policy projects sape 2012 06 20 saoe pdf it is fair to say though that important barriers remain and these tend to be cultural rather than technological 41 41 http crc nottingham ac uk projects rcs openscience report sarah currier pdf in terms of making sense of data the emergence of data science as a discipline is strongly encouraging albeit with a need to attract more data scientists to work on environmental challenges and problems blair et al 2019 the most important development has been the cross disciplinary dialogue that is now happening within the data science community involving statisticians computer scientists and domain experts amongst others this is very significant and is leading to breakthroughs in terms of efficient algorithms and their application in important societal problems in the data science institute at lancaster for example we are interested in how contemporary techniques such as extreme value theory changepoint analysis time series analyses and statistical machine learning can be applied to complex environmental data we are also particularly interested in how resultant data models can co exist and inform process models combining stochastic and deterministic understanding of complex environmental phenomena while there is increasing awareness of the potential of such approaches this is a relatively immature area solutions tend to be ad hoc and a more principled understanding of how such techniques can work together has yet to emerge there is also a similar narrative around reasoning across scales while there is more experience of this in the earth and environmental sciences the solutions are also quite ad hoc and often not shared across different areas of study in conclusion there have been significant developments since 2012 particularly in terms of the required cross disciplinary dialogue around data science for the natural environment nevertheless this work is still at a relatively early stage of maturity with important and fairly unique challenges of this area still to be addressed blair et al 2019 4 2 3 modelling as a learning process as discussed above many of the building blocks for models as a learning process were already in place by 2012 albeit fragmented across different communities the state of the art now is quite similar and there remains a need for stronger cross disciplinary dialogue between researchers working on environmental modelling data science and adaptive autonomic computing the most significant changes in this time have been i advances in areas such as statistical and machine learning that directly supports meta reasoning about model selection and rejection and ii the computational capacity offered by the cloud which supports both the execution of complex environmental models in the cloud and the execution of associated reasoning algorithms there has been little progress on the crucial area of uncertainty in terms of representing uncertainty explicitly in computations and also reasoning about uncertainty as part of the decision making process more generally one of the most significant developments over this time period in the environmental sciences has been the recognition of the unavoidable uncertainties associated with predictive models whether used for simulation or forecasting purposes e g beven 2009 as noted earlier a primary driver for the models of everywhere concepts was the potential for using local information to constrain local uncertainties in predicting local variables this is not just a problem of assessing the statistics of model residuals though many studies have approached the problem in this way this is because many sources of uncertainty are the result of lack of knowledge about processes variables or forcings particularly into the future that are not necessarily easily represented in simple statistical forms in particular input uncertainties will be processed through the nonlinear dynamics of a model to produce complex nonstationary residual structures that will then interact with uncertainties in the observational data used in model evaluation which might also have associated epistemic uncertainties e g in hydrology arising from the rating curves used in the estimation of river flows see westerberg et al 2011 westerberg and mcmillan 2015 coxon et al 2015 these issues underlay the development of the generalised likelihood uncertainty estimation glue methodology e g beven and binley 1992 2014 beven 2006 2016 which includes some statistical methods as special cases as new data become available it should be possible to learn more about the characteristics of the uncertainties associated with different predictands at least where the new data are informative that this may not always be the case has been shown by beven et al 2011 and beven and smith 2015 in doing so it will be possible to combine prior information with the new information to update the estimates this leads naturally to a form of bayesian reasoning where uncertainties can be represented as probabilities but much more research is needed in environmental models to understand how best to define the likelihoods used in the bayesian methodology simple statistical likelihood functions used with multiplicative bayesian updating appear to lead to overconditioning of model parameters because they do not take any account of the epistemic nature of sources of uncertainty e g beven et al 2011 beven 2016 2019 there are also issues of whether even the best models might be fit for purpose for the type of decisions that they might be used for see the discussion of beven and lane 2019 a critical aspect of the models of everywhere concept is the potential for using local knowledge within this learning process to improve the representations of places this is where information from local stakeholders and the internet of things might be used in local model evaluations to reject potential model structures and constrain uncertainties in parameterisations and outcomes this can be considered as an extension of the collaborative and participatory learning that has already been used in a number of local flood risk assessments and water resource management projects e g lane et al 2011 landström et al 2011 evers et al 2012 maskrey et al 2016 ferré 2017 basco carrera et al 2017 see also voinov et al 2016 an important component of this learning process is the potential to visualise model outcomes at scales that allows consideration of local detail by local stakeholders so that different scenarios and their uncertainties can be explored in collaborative ways hankin et al 2017 see below 4 2 4 deployment at scale there have been several important developments in terms of deploying at scale with containers in particular making it far easier to deploy and subsequently manage executing models in the cloud in a platform independent manner the availability of cloud based workflow engines is also significant although there are questions over whether workflow offers the right abstraction for all elements of environmental modelling blair et al 2019 more generally there is still a problem implementation gap france and rumpe 2007 between what scientists would like to do in the cloud and the level of support offered by existing technologies and services with a prior knowledge of the underlying technical details required this makes it very time consuming and also error prone to execute environmental models or ensemble models in the cloud and also requires access to computing expertise which may be a scarce resource in many environmental research labs in the context of models of everywhere the models may themselves be quite complex involving different ensembles of process models or the integration of process and data models for example this makes the cost quite prohibitive especially when this would entail the deployment of many instances of these models at many different places and scales software frameworks offer a promising technology to support the more rapid deployment of recurrent software architectures johnson 1997 software frameworks are tailored towards particular domains of application abstracting over the lower level details and capturing the commonalities within that domain while allowing some degree of specialisation they are heavily used in cloud computing for example mapreduce abstracts over the complexities of managing a large and complex underlying cloud infrastructure and supports the execution of distributed algorithms in the cloud allowing the user to plug in and specialise the computation through providing application specific map and reduce operations dean and ghemawat 2008 at present though such frameworks tend to be relatively generic e g dealing with distributed computation and are not specific enough to support something as domain dependent as environmental models in terms of programming models distributed objects have now been replaced by alternative paradigms supported in the cloud around service oriented architecture enhanced by concepts such as deployment in containers and optionally support for microservices this approach overcomes the problems associated with distributed object technology being much better suited to large scale internet wide deployment some research is required though in terms of how to map models of everywhere on to such programming concepts 4 3 overall assessment and technological readiness it is apparent from the discussion above that there have been significant advances in the underlying technology to support the vision of models of everywhere equally a number of barriers remain our overall assessment is summarised in table 3 repeating the style of analysis carried out for the period 2007 2012 in table 2 from this analysis we can see that there have been significant shifts in readiness around the underlying technological infrastructure and in data analytics and also partially around supporting deployment at scale support for modelling as a learning process has not changed much although the developments in cloud computing and data analytics does offer the potential as yet unrealised of significant advances in this area the need for cross disciplinary dialogue is a common theme across all these areas and is crucial in terms of addressing the remaining barriers overall we conclude that in terms of technological readiness the time is right to carry out large scale experiments of the concept of models of everywhere the next section explores ongoing research in this area 5 initial experiments and research roadmap ongoing research at lancaster is looking at an experimental deployment of the concept of modes of everywhere in the area of hydrology supported by recent developments in cloud computing data science and new sources of data including but not limited to iot technology the initial deployment is targeting a specific place with the intention of having a modelling framework that is able to capture and indeed learn the idiosyncrasies of that place the overarching goal of this work is to identify software architectural principles for implementing models of everywhere in the cloud with a view to supporting more widespread deployment of models of everywhere at different places and at different scales discussed further below we are also strongly interested in supporting decision making at different scales for example over the potential effectiveness of different natural flood management strategies and also over how to use constrained national or regional budgets most effectively the high level systems architecture is as shown in fig 8 below this recognises the existence of multiple sources of data and the importance of integrating this data and in turn looking at model integration on top of this which includes both data and process models coupled together the top layer then supports interrogation and querying of the information about that particular place this maps on to a more detailed cloud based systems architecture exploiting the range of services supported by the cloud in each of these areas this is shown in fig 9 through this work we intend to overcome the remaining technological barriers around models of everywhere and therefore open the door to the desired more widespread deployment of the concept in hydrology and beyond further details of this research can be found in towe et al 2019 with edwards et al 2017 also discussing human and societal dimensions of the research around decision making our overall research roadmap is summarised in table 4 which shows the key research questions and associated areas of investigation having deployed the concept of models of everywhere at a given place we then hope to consider how to generalise the approach to model other environmental facets at that place to model other places including places at other scales and to support coherent reasoning across scales this also involves key questions over discretisation especially given the fact that data may exist at different scales for a given place in the longer term we will also be interested in how the concept can be applied to other areas of environmental science including biodiversity and soil management and also how models of everywhere can help us in understanding the inter dependencies across such areas a key motivation of models of everything as discussed above this quickly becomes a large research agenda that goes beyond the scope of our research study and we hope to stimulate other research to address these key issues 6 related work there are of course already models of everywhere and to some sense everything in the sense of global earth system science models that have been developed from global atmosphere and ocean dynamic circulation models examples are the japanese earth simulator habata et al 2003 ec earth hazeleger et al 2010 and the community earth system model hurrell et al 2013 while these are still limited to grid resolutions of kilometres for global applications these systems commonly include the possibility of nesting finer grid domains with boundary conditions provided by global simulations the philosophy of such approaches however has been quite different from that presented here the model structure and parameterisations are generally fixed so that application everywhere has been a matter of finding appropriate effective parameter values for different grid locations using whatever data might be available there have also been some attempts to produce distributed modelling systems that could be applied widely at finer grid scales allowing for a more flexible choice of structures in hydrology for example there was the inter agency object modelling system oms of leavesley et al 2002 that developed into a more general modelling system lloyd et al 2011 david et al 2013 more recently clark et al 2015 have proposed the structure for unifying multiple modeling alternatives summa framework in both cases several different model representations were provided for the user to choose from in producing a models structure for a particular catchment area within these systems the expertise of users can be elicited to define appropriate model structures although identification of appropriate model parameters and hypothesis testing of competing model structures are still major issues e g weiler and beven 2015 the type of approaches presented here could be used with such systems in flood risk management there are inherent motivations to view modelling as a process of learning about places stemming from two distinctive features of the problem firstly the likelihood and impacts of flooding although driven ultimately by weather and climate are strongly influenced by local features of landscapes land use and human activities in some cases even very small topographic features or infrastructure assets can have a significant control on flood risk for example by directing the flow of flood waters towards or away from buildings this information is not always captured well if at all in generic model structures and data sets secondly the assessment of flood risk involves gathering information about extreme events which tends to place an emphasis on historical knowledge often reliant upon detailed knowledge of the locality for interpretation and on the updating of risk assessments as new observations become available for these reasons some flood risk management applications already implement frameworks for iterative co production of modelling based on the incorporation of knowledge about specific localities from multiple stakeholders one such system 42 42 https www youtube com watch v lewteldwtsu feature youtu be t 74 has been developed for the flanders environment agency vlaamse milieumaatschappij vmm for mapping areas at risk of flooding from surface runoff here a web based interface creates a shared collaboration space enabling local partners such as town councils or local water and sewer managers to engage in a dialogue about model improvements fig 10 important features that have been incorporated within the modelling through this process include areas where flood water can be held back by embankments or drained by pumps and control structures e g gates sluices weirs culverts that are known to local staff and may not be represented adequately without that detailed local knowledge such as the flood retention storage and flow control structures represented in fig 10 the co production website is shared with professional partners and within its first three months of operation enabled more than 9000 detailed improvements to data and modelling to be implemented together with nearly 300 re simulations for 103 sub models involving 150 organisations and resulting in positive evaluations of model improvements at 500 locations across the whole of flanders the vmm example discussed here explicitly exposes modelling as part of a process of learning about place through knowledge sharing supported by digital technology a more constrained example is the flood hazard mapping of the fema national flood insurance program 43 43 https www fema gov national flood insurance program flood hazard mapping where model based flood risk assessments are published and can be challenged by individuals on the basis of detailed local knowledge via a web based facility stakeholder interaction with the outputs of models of everywhere can also made more direct and local to demonstrate and explain assumptions and to alter inputs or outputs as a way of iterating to a co produced model of a place in this way local models will be better constrained by the local stakeholder information and more trusted if done well as an example of this way of working a series of workshops sponsored by natural england were used to engage local farming communities on the potential benefits of working with natural processes wwnp to mitigate flooding often called natural flood management see hankin et al 2017 for modelling concepts two engagement devices were used an augmented reality sandbox was used to provide real time feedback on the response of flow pathways to a user sculpting channels in sand virtual inputs to the sandbox are controlled by waving a hand over the sandbox water flow pathways and storages are then shown by projection of blue onto the sand this was used as a precursor to the demonstration of more quantitative modelling results with visualisations being projected onto a large interactive itable and shown in fig 11 engagement using the itable followed 4 key steps 1 the model and modelling assumptions is explained following a general discussion of nfm a baseline run of the model under flood conditions is discussed for acceptability in terms of local knowledge of patterns of flooding 2 a gis package is used to show different layers for the local catchment and bring in layers of potential opportunities that might be based on national strategic layers 44 44 http naturalprocesses jbahosting com these are discussed with the participants and options and be switched on and off according to where the catchment partners identify where they would be happy to try different sorts of nfm 3 the measures are then plugged into the model and the model is run to predict the outcome of the changed configuration 4 the distributed changes to the hydrological responses are explored with the partners to understand model behaviour and effectiveness fig 12 shows the outputs following one of the workshops interestingly in some discussions of significant measures in front of their peers landowners came up with interventions that were very significant for example sacrificing some summer irrigation storage to act as flood storage areas in the winter season this process of standing around the tables and discussing the catchment with peers appeared to make people more forthcoming and the process more effective the process does however require a different approach to modelling since some of the feedback from local stakeholders might not be positive it is therefore important that the modellers involved should not be too protective about their model but should recognise and explain the assumptions and uncertainties inherent in the modelling process and be prepared to incorporate new knowledge as far as possible finding ways of conveying and if necessary recalculating prediction uncertainties within this context is the subject of on going work 7 conclusions this paper has carried out a systematic analysis of the technological readiness for the concept of models of everywhere in particular the paper has examined the various dimensions associated with models of everywhere and determined a set of technological requirements that must be met for the successful large scale deployment of the concept this set of requirements was then used to compare technological readiness when models of everywhere was first proposed against the readiness levels now showing that the time is right for widespread experimentation and deployment of the concept although many of the technological barriers have been removed key research issues remain and the paper has highlighted a set of open research questions that must be addressed before progress can be made importantly this research agenda represents a shift in environmental modelling from an approach centred on process understanding through deterministic models to one that embraces a more data centric perspective whereby the two approaches can work in tandem to achieve a deeper understanding of specific places and hence to support more nuanced decision making about a given place there are also key limitations of the models of everywhere approach including the computational requirements if rolled out on a large scale and also the advances needed in environmental science to develop scale dependent parameterisations and embrace an underlying science of everything everywhere and at all times in conclusion the concept of models of everywhere is even more important than when it was first proposed given the environmental challenges we face and this paper has demonstrated that the time is right for more large scale experimentation with the concept further research though is clearly needed to deliver against this vision and this research has to be fundamentally trans disciplinary in nature bringing together environmental scientists data scientists and computer scientists to reach a common understanding of representing complex environmental data making sense of the resultant highly heterogeneous data integrating knowledge from process and data models and rolling out the concept of scale equally importantly there is a need to work closely with social scientists to understand the human and societal issues related to models of everywhere including the necessary cultural shift to open data treatments of security and privacy and the role of communities in ensuring models represent the peculiarity of places acknowledgements this work is supported by epsrc grants ep p002285 1 and ep n027736 1 blair s senior fellowship on the role of digital technology in understanding mitigating and adapting to environmental change and models in the cloud generative software frameworks to support the execution of environmental models in the cloud respectively we would also like to thank our colleagues in the data science institute and the centre for ecology and hydrology ceh for input and for providing such an inspiring environment for cross disciplinary dialogue around data science for the natural environment finally we thank the terrific inputs from a wide range of researchers and practitioners in partner organisations including from jba trust jba consulting the environment agency united utilities the european centre for medium range weather forecasts the oasis loss modelling framework and the environmental change institute oxford university 
