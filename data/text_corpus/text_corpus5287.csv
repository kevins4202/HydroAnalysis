index,text
26435,algal simulations in many water quality models perform poorly because of oversimplifications in the process descriptions of the algae growth mechanisms in this study algae simulations were improved by implementing variable chlorophyll a algal biomass ratios in the ce qual w2 model a sophisticated two dimensional laterally averaged water quality model originally a constant in the model the chlorophyll a algal biomass ratio was reprogrammed to vary according to the nutrient and light limiting conditions in the water column the modified model was tested on lake diefenbaker a prairie reservoir in saskatchewan canada where similar to many other lakes in the world field observations confirm variable spatiotemporal ratios between chlorophyll a and algal biomass the modified version yielded more accurate simulations compared to the standard version and provides a promising algorithm to improve results for many lakes and reservoirs globally keywords lake diefenbaker water quality modeling ce qual w2 algal stoichiometry chlorophyll a algal biomass ratio 1 introduction within a waterbody the physical chemical and biological variables can change in a short period of time e g in hours chapra 2008 examples are diurnal changes in dissolved oxygen carbon dioxide co2 and temperature yates et al 2007 at the boundaries e g inlets these variations are even more important for instance suspended solids concentration is heavily dependent on flow rates and wind speed and direction based on the flow and wind characteristics suspended solids can settle to the bottom sediments or remain suspended within the water current at high velocities erosion and sediment resuspension add to suspended solids concentrations hence when studying waterbodies with dynamic flow characteristics and changing water quality variables short term variations need to be considered to fully evaluate the system for some studies especially at larger scales it is difficult to carry out an accurate evaluation of the environment on which responsible agencies can base management decisions the reasons could be due to the course temporal sampling frequencies and spatial distributions of stations making water quality databases not encompassing enough or the environment is so complex that many external factors must be taken into consideration before their outcomes can be useful for management decisions these factors include effects of climate change gosling and arnell 2016 land use changes el khoury et al 2015 increasing populations vorosmarty et al 2000 industrial development aulakh et al 2009 mining activities schmidt et al 2012 wastewater treatment plant effluents bunzel et al 2013 agricultural tillage practices kachi et al 2016 and fish poultry and livestock farming herbst et al 2012 the biggest problem with these activities is their contribution to both point source and nonpoint source contamination e g pesticides and nutrients mainly phosphorus and nitrogen discharged into waterbodies chislock et al 2013 wu and chen 2013 water quality models are useful tools to compensate these limitations in water quality data and to consider effects of different sources of uncertainties water quality models are used to address characteristics of the waterscape including complex geomorphology missaghi and hondzo 2010 complicated boundary conditions jin et al 2007 and multidimensional internal processes kopmann and markofsky 2000 a typical water quality model might use hourly meteorological data daily flow data and monthly chemical and biological data james 2016 leon et al 2011 a water quality model is often expected to produce high resolution results e g hourly by using monthly input data hughes and slaughter 2016 model results may be improved by using more complex processes inclusion of fluxes and derived nutrient constituents but at the cost of higher uncertainty levels due to over parameterization freni et al 2011 as a result some modelers are tempted to move towards more complex models with higher dimensions e g two and three dimensional 2d and 3d while others believe that it is the quality of the observations and correctness of model theories that determine model accuracy not model complexity reckhow 1999 algal concentrations are a good representative of the health of aquatic systems hence they are a component of any water quality models algae have many interactions with the inorganic and organic pools of nutrients in water therefore the quality of the input data is critical for obtaining a correct algae simulation as a result of low quality input data most water quality models simulate algal behavior only moderately well and poorly in most cases arhonditsis et al 2006 in addition to the quality of input data the other primary reason leading to poor algal results is using fixed growth mortality respiration excretion and settling rates for algae in models in reality algae adjust these ratios based on nutrient and light availability and temperature conditions ji 2008 algal biomass measurements are difficult to make so limnologists prefer measuring chlorophyll a concentrations ji 2008 since water quality models simulate algae as biomass algae is converted to chlorophyll a by multiplying it with a chlorophyll a algal biomass ratio the chlorophyll a algal biomass ratios for different species are held constant during simulations while in reality they change dynamically through time and space based on light and nutrient availability chapra 2008 the dynamic behavior of algae in response to the availability of nutrients and light is well understood and documented e g dickman et al 2006 hessen et al 2002 urabe et al 2002 wetzel 2001 the use of variable chlorophyll a algal biomass ratios was successfully implemented in some simple water quality models such as the steady state one dimensional water quality model qual2kw pelletier et al 2006 however it is not been entirely implemented in more comprehensive time varying and multi dimensional models such as ce qual w2 delft3d wasp and efdc the flow and water temperature simulations of these comprehensive models such as ce qual w2 are not expected to be influenced by the chlorophyll a algal biomass ratio e g sapin et al 2017 however for the water quality and eutrophication components of a model using a constant value for the chlorophyll a algal biomass ratio for the entire simulation can cause the algae simulations to yield the poorest performance among all the water quality variables e g kuo et al 2006 this study implemented a water quality modeling exercise on a prairie reservoir in saskatchewan canada where the measurements confirmed that algae adjust their chlorophyll a algal biomass ratios based on phosphorus availability abirhire et al 2015 lake diefenbaker is a strategic reservoir formed by the construction of two earth filled dams the gardiner dam and the qu appelle river dam on the south saskatchewan river ssr fig 1 the reservoir is 181 km long with increasing depth from 8 m at its upstream boundary to 60 m at the gardiner dam sadeghian et al 2015 most of the inflowing water 98 is from the ssr which merged with the red deer river rdr near the alberta saskatchewan border 332 km upstream of the dams the data on inflow to and outflow from lake diefenbaker is highly uncertain and the quality of the water quality data is limited the closest hydrometric stations for water flow rates and sampling stations for water quality data are at medicine hat and bindloss stations 203 km and 47 km upstream of the ssr rdr confluence 374 km and 218 km to reservoir respectively table 1 the outflow from the reservoir was not measured directly at the gardiner dam but at the nearest station 120 km downstream of the dam in the city of saskatoon the closest meteorological stations are several kilometers away from the reservoir and there were no buoy stations on this large strategic waterbody in this study we endeavored to develop a water quality model for lake diefenbaker that could be used to support decision making based on the limited water quality field measurements and highly uncertain boundary conditions of the reservoir the general objective of this study was to present a comprehensive modeling exercise using chlorophyll a algal biomass ratios that vary both in time and space a novelty in improving in lake water quality simulations specific objectives for the lake diefenbaker system were to 1 understand the physical chemical and biological characteristics of the reservoir 2 estimate the rates of physical chemical and biological changes in different parts of the reservoir 3 provide a predictive framework for applying available climate change and land use change scenarios 2 methods 2 1 study site the south saskatchewan river is a long river 1392 km sheelanere et al 2013 flowing through the canadian provinces of alberta and saskatchewan merging with the ssr near the alberta saskatchewan border the red deer river rdr has turbid water due to the river s steep slope and prairie land setting the average ssr flow is about 4 5 times larger than that of the rdr flow based on 50 years of daily data approximately 171 kilometers downstream of the ssr rdr confluence is the inlet to lake diefenbaker at highway 4 fig 1 the lake diefenbaker bifurcates near the village of elbow due to the construction of the gardiner and qu appelle river dams in the 1960s most of the inflow 98 comes from the ssr and the main outflow 98 flows north through the gardiner dam details on the physical characteristics of lake diefenbaker are available in sadeghian et al 2015 the hydrometric station for water flow rates and water quality sampling for ssr is at medicine hat 374 km upstream of the reservoir and for rdr is at bindloss 218 km upstream of the reservoir 203 km and 47 km upstream of the ssr rdr confluence respectively table 1 the outflow from the reservoir is used from the hydrometric station at the city of saskatoon 120 km downstream of the gardiner dam there are some preliminary guidelines to estimate and interpolate the flow and climate data at the reservoir pomeroy and shook 2012 however the same routing methods could not be used to estimate the nutrient input data for lake diefenbaker because comparable guidelines for predicting the chemical and biological constituents had not been developed by biologists and engineers the reason is that many factors such as sedimentation erosion retention point source e g waste water treatment plants and nonpoint source loadings e g agriculture field surface runoff and groundwater infusion could significantly change nutrient concentrations in the inflow to lake diefenbaker via the ssr hence we built a pre model to transfer the river data to lake diefenbaker s inlet data but first improving the temporal resolution of measured data in ssr and rdr we found the best approach to estimate the state variable concentrations at the lake diefenbaker inlet was to first build the daily water quality database by correlating the water quality variables with the date julian day discharge and water temperature at the hydrometric stations on ssr and rdr then a model which was calibrated with measurements at the most upstream stations on lake diefenbaker was employed to transfer these values to the lake diefenbaker inlet for more details on lake diefenbaker see north et al 2015 2 2 model selection the current study required a model that can be implemented for both the river and reservoir systems the length of the whole system from the hydrometric station on the south saskatchewan river near medicine hat to the dams plus the distance from the bindloss station at red deer river to the confluence is 602 km because the reservoir has a maximum depth of 60 m a two dimensional model was required to adequately simulate the hydrodynamics as well as resolve the longitudinal and vertical water quality gradients lateral values are averaged a third lateral dimension was not necessary because of the narrow width of the river and reservoir we selected the ce qual w2 model which could be used to perform both hydrodynamic and water quality simulations in both the riverine ssr and rdr and the lacustrine lake diefenbaker parts of the system 2 3 model description the ce qual w2 model is a 2d laterally averaged water quality model over 40 years in development the us army corps of engineers launched the model in 1975 edinger and buchak 1975 developments continued by edinger and buchak for about ten years buchak and edinger 1984 edinger and buchak 1978 and in 1984 the model was handed over to a team led by tom cole during the next 20 years major refinements of both the hydrodynamic and water quality components were incorporated cole and buchak 1995 cole and wells 2003a b 2006 after 30 years of progress the us corps of engineers stopped model development and the model was handed over to portland state university model development is still continuing with an emphasis on computational efficiency easier application through a graphical user interface and more efficient and accurate numerical schemes cole and wells 2008 2013 2015a b the ce qual w2 model could be used to simulate all the major constituents and processes required for our research objectives including water temperature hydrodynamics dissolved particulate solids dissolved oxygen nutrients organic matter and algae in addition it had an up to date user manual and an active user forum also the source code was freely available with clear comments allowing extension and application of new formulations and algorithms finally the model has a complete set of output generation tools it can print the outputs for all component of the model at any longitudinal vertical and temporal point the model produces detailed output files useful for visual calibration 2 4 adaptation of the model to lake diefenbaker model development for lake diefenbaker was carried out in several stages we started by building a segmentation network of the ssr medicine hat to saskatchewan landing and lake diefenbaker saskatchewan landing to dams so the river was directly coupled with the reservoir the benefits of such a system were numerous based on the water level of the reservoir and river discharge flow at the upstream end of the reservoir can back up into the river because of these reverse flow effects in the reservoir particularly at its upper end the nutrients and flow characteristics were more accurately captured than with separate river and lake systems another benefit was that this combination reduced the need for data processing and transfer to the lake model the main drawback was the computational cost the calculation time for a combined river reservoir system increased dramatically the main reason was that the timestep requirement for the river is more stringent than for the reservoir much smaller timesteps were used for the reservoir than would be required if the model for the lake was ran independently of that for the river based on our access to the high performance cluster hpc system of university of saskatchewan we found that the computational burden was justifiable at the first stage for calculating water quality state variables at the reservoir inlet the ce qual w2 model is designed for windows platforms so in order to run the model on the linux hpc the code was adapted and compiled for the centos linux platform the instructions on how to compile and run the code on linux servers were added to the model user forum http w2forum cee pdx edu q node 500 although the model became more complex nine inter connected waterbodies as delineated by black lines in fig 1 were considered in the model the reason for having several waterbodies permitted the use of different climate data stations different water quality parameters and constants and different slopes for the river sections each waterbody had one main branch to which additional branches could be added the additional branches were required to define a different slope or to connect a stream to the main river stem one extra branch connected the rdr to the ssr the second branch changes the river slope to almost zero when the river enters the reservoir at saskatchewan landing see fig 1 and the last branch was the qu appelle arm at the downstream end the model has 827 segments ranging in length from 200 m to 1200 m and has 60 cartesian vertical layers each one meter in thickness we considered algae total dissolved solids tds inorganic suspended solids iss phosphate po4 ammonium nh4 nitrate no3 labile dissolved organic matter ldom refractory dissolved organic matter rdom labile particulate organic matter lpom refractory particulate organic matter rpom dissolved oxygen dissolved silica and total inorganic carbon tic in our simulations both the dissolved and particulate forms of organic matter were considered in the model setup each group was divided to two categories liable short decay time e g days and refractory longer decay time e g months to years we also used derived constituents in our outputs where we had measurements available including dissolved organic carbon doc particulate organic carbon poc dissolved organic nitrogen don particulate organic nitrogen pon total nitrogen tn dissolved organic phosphorus dop particulate organic phosphorus pop total phosphorus tp chlorophyll a and total suspended solids tss we calibrated the model based on measurements at the most upstream station 1 in fig 1 by using the same methodology employed in the hydrodynamic model of sadeghian et al 2015 we used the model results to prepare the input data at the lake diefenbaker inlet inorganic nutrients used directly as inputs to the model were po4 nh4 no3 and dissolved silica and the organic components were algae biomass and organic matter concentration there were also a few variables considered as derived variables the main reason for using the derived variables was the availability of measured data these variables included particulate organic carbon poc particulate organic nitrogen pon total nitrogen tn and total phosphorus tp table a2 contains equations for calculating the derived variables and their explanations tds was treated as a conservative variable inside the model tds influences the movement of water by changing density and also affects ph by influencing the ionic strength of carbon dioxide the measured tds was considered as 65 of recorded electrical conductivity compared with the model simulations since the tds is non conservative its concentration is mainly dependent on the correct inflow concentrations and accurate hydrodynamics interestingly tds simulations produced the lowest errors among all the variables used for calibration table 2 2 5 simulations to reduce computational expenditure we excluded the riverine ssr rdr sections and reduced the segmentation from 827 to 302 segments keeping the vertical layers intact fig 2 the computation time decreased from 18 h when the whole system 602 km was considered to only 3 h for each run on a desktop computer intel core i7 3770 s 8 mb cache 3 4 ghz 5 0 gt 12 gb ddr3 then we calibrated the model using the same methodology by combining some segments the number of segments in the lite model was reduced to 87 segments of length 800 m 4000 m fig 2 and the layers were reduced to 21 ascending from 1 m at the water surface to 4 5 m toward the reservoir bed fig 3 more details on the lite model preparation are provided in table a1 the computational time for the lite model was less than five minutes the motive for development of the lite model was that in many cases a fast and straightforward setup is more desirable for example during the 2013 calgary flooding a flood plume moved rapidly towards lake diefenbaker when using the model as a predictive tool reduced simulation times become invaluable another use of the lite model is to test and verify new model formulations for example in this study we have examined the effects of variable algal stoichiometry on the accuracy of chlorophyll a concentration calculations based on the formulations provided by chapra 2008 we still have small layers within the euphotic zone but the layering did not have any effect on the accuracy of the results 2 6 algae chlorophyll a ratio implementation in model biological components of the ce qual w2 model are phytoplankton algae epiphyte periphyton macrophytes and zooplankton animals epiphytes and macrophytes are more related to the riverine and shallow waterbodies and algae is more prevailed in lakes and reservoirs in this study data on zooplankton were not available and the reservoir environment of lake diefenbaker led us to use algae as the sole biological component the use of algae is challenging for the development of water quality models because it interacts with almost every single variable within the model fig 4 other problems associated with the use of algae are the limited number of observations and that the measurements are mainly in the form of chlorophyll a concentrations which need to be converted to algal biomass to make the comparisons either the chlorophyll a concentrations from measurements or the algal biomasses concentrations from simulations need to be converted to the other form algae biomass is multiplied by algae chlorophyll a ratio achla to obtain chlorophyll a concentrations achla is a fixed value in the model however in a natural setting this rate is not constant and changes based on light and nutrient availability therefore as a novel aspect of this study with some modifications to the source code the achla parameter was designed to change based on the limiting conditions of water eqs 1 chapra 2008 and 2 cole and wells 2015a 1 c h l a c 6 4 45 ϕ n 1 ϕ l 2 a c h l a 1 c h l a c where chla is chlorophyll a c is carbon algal biomass φ n is nutrient limiting factor 0 φ n 1 and φ l is light limiting factor 0 φ l 1 equation 1 is based on the formulation suggested by chapra 2008 and was successfully adopted in the 1d steady state water quality model qual2k the reason for implementing the equations in ce qual w2 was the limitations in the qual2k model for example the qual2k is a 1d model while ce qual w2 is 2d model qual2k is a steady state model while ce qual w2 uses dynamic inputs for flow nutrient and water quality state variables qual2k is written in visual basic vba in ms excel which can make only smaller computations compared with sophisticated ce qual w2 which is written in fortran and c the ce qual w2 model already calculates the limiting factor for algae production for nutrients eq 3 monod 1949 and light eq 4 steele 1962 therefore only the conversion from algal biomass to chlorophyll a needs to be adjusted 3 ϕ n n n h s n 4 ϕ l i i k e x p 1 i i k where n is nutrient concentration n hs is algal half saturation for nutrient limited growth i is light intensity and i k is light saturation intensity at maximum photosynthetic rate a minimum of either nitrogen or phosphorus limitation was applied as the nutrient limiting rate and then the light limiting effects were incorporated into the calculations of the chla c ratio 2 7 model calibration model calibration was done by comparing the simulated values for different water quality variables from the model with the measured values from the field works and laboratory analysis the averaged root mean squared error r m s e eq 5 was used as the objective function for measuring the model performance statistically 5 r m s e o s 2 n o where o is the observed and s is the simulated values n is the number of samples and o is the average of observed values the use of r m s e is preferred to the rmse to normalize the error associated with different water quality variables the optimum values of the parameters determined through the calibration are provided in table a1 water temperature and flow were the dominant factors influencing model calibration calibration of the thermal and hydrodynamic regimes are discussed in detail in sadeghian et al 2015 the next most influential variable was dissolved oxygen dissolved oxygen is an important water quality component which can provide insight about the quality of water in the absence of other variables ji 2008 oxygen is necessary for the most of chemical and biological processes in the water fig 4 due to the interdependence of nutrients we first calibrated the do po4 nh4 no3 and organic matter together to estimate the model coefficients then the remaining parameters tds iss doc poc pon tn and tp were calibrated 3 results and discussion based on our model setup atmospheric aeration algal photosynthesis and oxygen in inflow water were the sources of dissolved oxygen to the reservoir algal respiration excretion and mortality decay of allochthonous organic matter nitrification and sediment oxygen demand were the consumers of oxygen similar to water temperature do measurements were based on sonde probes yielding a higher number of measurements than those made in the laboratory therefore the calibration was easier for do and made with less uncertainty the r m s e for dissolved oxygen based on 5000 field measurements one meter vertical resolution at 14 stations on 79 different days from 2011 to 2013 was 0 19 fig 5 temperature light dissolved oxygen and nutrient availability both inorganic and organic determine the rates of chemical and biological reactions in water dissolved oxygen concentration is simple to calibrate but difficult to validate because many components interact with oxygen fig 4 with many parameters contributing to oxygen production and consumption additionally an incorrect model parameter set can produce accurate results for example according to the model manual cole and wells 2015a the decay of ammonium requires 4 57 g o2 g n eq 6 while the decay of organic matter requires 1 4 g o2 g organic matter eq 7 which is about four times less oxygen compared to ammonium decay although a decay rate of 0 1 day 1 for ammonium has the same mathematical effect on oxygen consumption as a decay rate of 0 4 day 1 for organic matter the results on the nutrient pool are significantly different to avoid an incorrect setup do organic matter po4 nh4 and no3 were calibrated simultaneously fig 5 due to a high number of observations for do the model results for do have small uncertainties correctly predicting dissolved oxygen is a crucial part of water quality modeling because oxygen plays a vital role in the lives of aquatic organisms 6 2 n h 4 3 o 2 2 h 2 o 4 h 2 n o 2 2 n o 2 o 2 2 n o 3 7 5 c o 2 2 h 2 o n h 3 c 5 h 7 n o 2 5 o 2 the model results showed that oxygen depletion occurred more in areas with higher ammonium concentrations the ammonium stems from both the inflow ssr and organic matter decay in lower dark layers of the lake where algal photosynthetic activities are minimal due to poor light conditions the oxygen depletion is higher at these depths nitrification could be the primary oxygen consumer because two molecules of oxygen are required to convert each molecule of ammonium to nitrite and then to nitrate eq 6 ammonium and nitrate are not taken up by algae equally some algae groups may prefer ammonium over nitrate cole and wells 2015a and the preference may change through the life of algae for example at early stages algae may prefer ammonium but resort to nitrate as they mature aquarium fertilizer 2012 the lake diefenbaker model is not sensitive to the coefficient for algal nitrogen uptake preference aneqn between ammonium and nitrate because only nitrate and phosphate are consumed by algae oxygen depletion is overestimated by the model at the water surface the decay of organic matter accompanied by the formation and release of nitrate and phosphate from the sediment is also an oxygen depleting process near the bed of the reservoir the main sources of organic matter into the system are the incoming concentrations by ssr inflow external source and algal production in lake process the organic matter mineralizes to form inorganic nutrients and some of which deposit in the sediment fig 4 algal mortality contributes to particulate organic matter and algal excretion to both dissolved and particulate organic matter an example of a simplified cycle of nitrogen as a calibration variable in this model is presented in fig 6 algae contribute to both the particulate and dissolved organic nitrogen through mortality excretion and respiration a portion of the particulate form goes into the sediment fig 4 and a portion mineralizes into dissolved form organic matter concentrations were included into the simulations but only those results are shown for which we had measurements for comparison sake figs 5 and 7 the poc values are the sum of all forms of liable and refractory particulate organic matter lpom and rpom respectively multiplied by the carbon to biomass ratio δc which had a value of δc 0 45 in our simulations the doc values which are the sum of liable and refractory dissolved organic matter ldom and rdom respectively multiplied by the carbon to biomass ratio δc 0 45 were not shown because the measurements were not available particulate organic nitrogen pon is the nitrogen concentrations in organic matter both liable and refractory which is produced by phytoplankton or enters the rivers via detritus in fig 5 the pon is the sum of nitrogen components of liable and refractory dissolved organic matter ldom and rdom respectively since algae are a good representative of the health of aquatic system they are a component of any water quality model algae have many interactions with inorganic and organic pools of nutrients in water looking into the example of the nitrogen cycle fig 6 algae interacts with five different forms of nitrogen therefore quality of input data is critical for obtaining a correct algae simulation as a result of low quality input data most water quality models simulate algae behavior only moderately well or even poorly in many cases arhonditsis et al 2006 in addition to the quality of input data the other primary reason for getting poor algal results is using fixed growth mortality respiration excretion and settling ratios for algae in models the measurements at lake diefenbaker confirmed that algae adjust their chla c ratios based on p availability abirhire et al 2015 the upstream part of the reservoir is turbid so the algae have lower achla values due to low light conditions see eq 2 chlorophyll a simulations were almost the poorest among all the variables before applying the variable chlorophyll a biomass ratio table 2 based on model results achla had a range of 20 130 mg algal carbon mg chlorophyll a for lake diefenbaker during 2011 2013 fig 7 the minimum achla was at the upstream end of the lake in winter when light is most limiting in the model used in this study we used a variable chla c ratio for converting the simulated algal biomass into chlorophyll a concentrations by doing so simulation errors r m s e decreased by about 50 from 2 28 to 1 15 table 2 fig 8 meaning better performance and predictive capability of the model the factors that influence the chlorophyll a algal biomass ratio are not limited to nutrient and light availability other factors include species type life stage and previous light conditions to name but a few the model results could be improve by including these factors into the simulations however lack of data on species partitioning and our research scope to build simpler models prevented us from doing that it is worth mentioning that in eutrophication studies usually the diatom and green algae are not considered a threat to the environment and only the cyanobacteria concentrations are important hence partitioning algae species are required to avoid uncertainties in those studies depending on the objectives of the study the grid discretization may be made coarser or finer for example for the variable stoichiometry the study required the correct nutrient temperature and light values at the euphotic zone the lite model has longer longitudinal segments and thicker vertical layers however the layers become thicker with depth fig 3 as a result the euphotic zone for algae production remains almost unchanged and the thicker layers at the bottom did not significantly affect the results by increasing the longitudinal and vertical spacing we were able to run this lite model about 40 times faster without sacrificing accuracy it is worth mentioning that the grid discretization should be made according to the study objectives for example for a study with the objective of locating the thermocline depth at the coteau creek arm recreational beach near the gardiner dam on the 5th of august a model with fine vertical grids would be required comparing model results table 2 for the standard and lite models with a fixed achla some variables have smaller r m s e values for the lite model the reason could be averaging over depth in the lite model the vertical layers reduced from 60 to 21 hence in r m s e calculations the observations over larger layers were averaged yielding smaller model errors 4 conclusion we developed a 2d laterally averaged water quality model of lake diefenbaker by using the ce qual w2 model we considered a top down approach by first developing a large complex model and gradually simplifying it we first began building a riverine plus reservoir model 827 segments and 60 vertical layers and then making the model smaller by eliminating the riverine sections and considering only lake diefenbaker 302 segments and further simplifying the model by making the segments longer and vertical layer heights thicker 87 segments and 21 vertical layers depending on the study objective and available input data these models can be used to launch future climate and land use change scenarios the main objective of this study was to improve algae simulations by the model hence the model setup considered a complete list of important variables including do po4 nh4 no3 ldom rdom lpom rpom algae tds and tss then we applied chapra s 2008 proposed method for calculating chlorophyll a concentrations based on nutrient and light availability by using a variable stoichiometry we were able to calculate chlorophyll a more accurately compared with using a fixed chlorophyll a algal biomass ratio in lake diefenbaker although we were able to calibrate these models successfully excessive computational resources and time were used for calculations and interpretations of the results the majority of these efforts could have been avoided if better field measurements had been available for this reservoir we found much smaller simulation errors up to ten times table 2 for those variables that had enough measurements including do tds and water temperature hence it is recommended to first install a fixed sampling station at the upstream end of the reservoir to measure daily hourly flow and weekly monthly water chemistry data second the frequency of in lake sampling especially for nutrients and chlorophyll a during summer should at least be doubled moreover there is a need for data collecting groups to move towards utilizing new and cost effective instrumentation such as fluoroprobe by bbe moldaenke which can be used for chlorophyll measurements with the ability of species distinction between diatoms green algae and cyanobacteria acknowledgements this work was financially supported by the canada excellence research chair in water security through the global institute for water security we thank environment canada the saskatchewan water security agency and alberta environment for providing the hydrometric and water quality data we are grateful to meteoblue for providing the meteorological data thanks to the limnology laboratory at the university of saskatchewan for providing the water turbidity and temperature data thanks also to the department of geography and planning at the university of saskatchewan for providing the bathymetry data appendix table a1 ce qual w2 initial and optimal values for different parameters in the lite model parameters definitions are from the ce qual w2 manual cole and wells 2015a table a1 parameter value description imx 87 number of segments in the computational grid kmx 23 number of layers in the computational grid latitude 50 71 latitude degrees longitude 107 28 longitude degrees tmstrt april 1 2011 model simulation starting time tempi 4 initial temperature c tds 250 tds g m3 or mg l iss 2 inorganic suspended solids mg l po4 0 01 po4 p mg l as p nh4 0 02 nh4 n mg l as n no3 0 01 no3 n no2 n mg l as n dsi 2 dissolved silica mg l as si ldom 0 1 labile dissolved organic matter mg l as organic matter rdom 0 1 refractory dissolved organic matter mg l as organic matter lpom 0 67 labile particulate organic matter mg l as organic matter rpom 0 1 refractory particulate organic matter mg l as organic matter alg 0 00001 algae mg l as dry weight organic matter do 8 dissolved oxygen mg l tic 15 total inorganic carbon mg l as c exh2o 0 25 extinction for pure water m 1 exss 0 1 extinction due to inorganic suspended solids m 1 g m3 exom 0 1 extinction due to organic suspended solids m 1 g m3 exa 0 2 algal light extinction m 1 g m3 β 0 45 fraction of incident solar radiation absorbed at the water surface sss 1 1 suspended solids settling rate m day 1 τcr 1 9 critical shear stress for sediment resuspension dynes cm2 ag 0 43 maximum algal growth rate day 1 ar 0 04 maximum algal respiration rate day 1 ae 0 02 maximum algal excretion rate day 1 am 0 05 maximum algal mortality rate day 1 as 0 13 algal settling rate m day 1 ahsp 0 003 algal half saturation for phosphorus limited growth g m3 ahsn 0 014 algal half saturation for nitrogen limited growth g m3 asat 114 light saturation intensity at maximum photosynthetic rate w m 2 at1 6 9 lower temperature for algal growth c at2 22 3 lower temperature for maximum algal growth c at3 35 upper temperature for maximum algal growth c at4 40 upper temperature for algal growth c algp 0 005 stoichiometric equivalent between algal biomass and phosphorus algn 0 08 stoichiometric equivalent between algal biomass and nitrogen algc 0 45 stoichiometric equivalent between algal biomass and carbon achla 0 05 ratio between algal biomass and chlorophyll a in terms of mg algae μg chlorophyll a apom 0 8 fraction of algal biomass that is converted to particulate organic matter when algae die aneqn 2 equation number for algal ammonium preference ldomdk 0 02 labile dom decay rate day 1 rdomdk 0 001 refractory dom decay rate day 1 lrddk 0 001 labile to refractory dom decay rate day 1 lpomdk 0 05 labile pom decay rate day 1 rpomdk 0 001 refractory pom decay rate day 1 lrpdk 0 011 labile to refractory pom decay rate day 1 poms 0 25 pom settling rate m day 1 po4r 0 001 sediment release rate of phosphorus fraction of sod nh4r 0 001 sediment release rate of ammonium fraction of sod nh4dk 0 15 ammonium decay rate day 1 no3dk 0 05 nitrate decay rate day 1 no3s 0 011 denitrification rate from sediments m day 1 sod 0 1 zero order sediment oxygen demand for each segment g o2 m 2 day 1 table a2 derived variables in ce qual w2 model cole and wells 2015a table a2 poc pom orgc 7 pon pom orgn algae an 8 tn don pon nh4 no3 9 tp dop pop po4 tpss 10 where algae algal biomass poc particulate organic carbon pon particulate organic nitrogen tn total nitrogen tp total phosphorus pom particulate organic matter orgc stoichiometric equivalent between organic matter and carbon orgc 0 45 orgn stoichiometric equivalent between organic matter and nitrogen orgn 0 08 an stoichiometric equivalent between algal biomass and nitrogen an 0 08 don dissolved organic nitrogen tpss ss partp ss suspended solids partp phosphorus partitioning coefficient for suspended solids 
26435,algal simulations in many water quality models perform poorly because of oversimplifications in the process descriptions of the algae growth mechanisms in this study algae simulations were improved by implementing variable chlorophyll a algal biomass ratios in the ce qual w2 model a sophisticated two dimensional laterally averaged water quality model originally a constant in the model the chlorophyll a algal biomass ratio was reprogrammed to vary according to the nutrient and light limiting conditions in the water column the modified model was tested on lake diefenbaker a prairie reservoir in saskatchewan canada where similar to many other lakes in the world field observations confirm variable spatiotemporal ratios between chlorophyll a and algal biomass the modified version yielded more accurate simulations compared to the standard version and provides a promising algorithm to improve results for many lakes and reservoirs globally keywords lake diefenbaker water quality modeling ce qual w2 algal stoichiometry chlorophyll a algal biomass ratio 1 introduction within a waterbody the physical chemical and biological variables can change in a short period of time e g in hours chapra 2008 examples are diurnal changes in dissolved oxygen carbon dioxide co2 and temperature yates et al 2007 at the boundaries e g inlets these variations are even more important for instance suspended solids concentration is heavily dependent on flow rates and wind speed and direction based on the flow and wind characteristics suspended solids can settle to the bottom sediments or remain suspended within the water current at high velocities erosion and sediment resuspension add to suspended solids concentrations hence when studying waterbodies with dynamic flow characteristics and changing water quality variables short term variations need to be considered to fully evaluate the system for some studies especially at larger scales it is difficult to carry out an accurate evaluation of the environment on which responsible agencies can base management decisions the reasons could be due to the course temporal sampling frequencies and spatial distributions of stations making water quality databases not encompassing enough or the environment is so complex that many external factors must be taken into consideration before their outcomes can be useful for management decisions these factors include effects of climate change gosling and arnell 2016 land use changes el khoury et al 2015 increasing populations vorosmarty et al 2000 industrial development aulakh et al 2009 mining activities schmidt et al 2012 wastewater treatment plant effluents bunzel et al 2013 agricultural tillage practices kachi et al 2016 and fish poultry and livestock farming herbst et al 2012 the biggest problem with these activities is their contribution to both point source and nonpoint source contamination e g pesticides and nutrients mainly phosphorus and nitrogen discharged into waterbodies chislock et al 2013 wu and chen 2013 water quality models are useful tools to compensate these limitations in water quality data and to consider effects of different sources of uncertainties water quality models are used to address characteristics of the waterscape including complex geomorphology missaghi and hondzo 2010 complicated boundary conditions jin et al 2007 and multidimensional internal processes kopmann and markofsky 2000 a typical water quality model might use hourly meteorological data daily flow data and monthly chemical and biological data james 2016 leon et al 2011 a water quality model is often expected to produce high resolution results e g hourly by using monthly input data hughes and slaughter 2016 model results may be improved by using more complex processes inclusion of fluxes and derived nutrient constituents but at the cost of higher uncertainty levels due to over parameterization freni et al 2011 as a result some modelers are tempted to move towards more complex models with higher dimensions e g two and three dimensional 2d and 3d while others believe that it is the quality of the observations and correctness of model theories that determine model accuracy not model complexity reckhow 1999 algal concentrations are a good representative of the health of aquatic systems hence they are a component of any water quality models algae have many interactions with the inorganic and organic pools of nutrients in water therefore the quality of the input data is critical for obtaining a correct algae simulation as a result of low quality input data most water quality models simulate algal behavior only moderately well and poorly in most cases arhonditsis et al 2006 in addition to the quality of input data the other primary reason leading to poor algal results is using fixed growth mortality respiration excretion and settling rates for algae in models in reality algae adjust these ratios based on nutrient and light availability and temperature conditions ji 2008 algal biomass measurements are difficult to make so limnologists prefer measuring chlorophyll a concentrations ji 2008 since water quality models simulate algae as biomass algae is converted to chlorophyll a by multiplying it with a chlorophyll a algal biomass ratio the chlorophyll a algal biomass ratios for different species are held constant during simulations while in reality they change dynamically through time and space based on light and nutrient availability chapra 2008 the dynamic behavior of algae in response to the availability of nutrients and light is well understood and documented e g dickman et al 2006 hessen et al 2002 urabe et al 2002 wetzel 2001 the use of variable chlorophyll a algal biomass ratios was successfully implemented in some simple water quality models such as the steady state one dimensional water quality model qual2kw pelletier et al 2006 however it is not been entirely implemented in more comprehensive time varying and multi dimensional models such as ce qual w2 delft3d wasp and efdc the flow and water temperature simulations of these comprehensive models such as ce qual w2 are not expected to be influenced by the chlorophyll a algal biomass ratio e g sapin et al 2017 however for the water quality and eutrophication components of a model using a constant value for the chlorophyll a algal biomass ratio for the entire simulation can cause the algae simulations to yield the poorest performance among all the water quality variables e g kuo et al 2006 this study implemented a water quality modeling exercise on a prairie reservoir in saskatchewan canada where the measurements confirmed that algae adjust their chlorophyll a algal biomass ratios based on phosphorus availability abirhire et al 2015 lake diefenbaker is a strategic reservoir formed by the construction of two earth filled dams the gardiner dam and the qu appelle river dam on the south saskatchewan river ssr fig 1 the reservoir is 181 km long with increasing depth from 8 m at its upstream boundary to 60 m at the gardiner dam sadeghian et al 2015 most of the inflowing water 98 is from the ssr which merged with the red deer river rdr near the alberta saskatchewan border 332 km upstream of the dams the data on inflow to and outflow from lake diefenbaker is highly uncertain and the quality of the water quality data is limited the closest hydrometric stations for water flow rates and sampling stations for water quality data are at medicine hat and bindloss stations 203 km and 47 km upstream of the ssr rdr confluence 374 km and 218 km to reservoir respectively table 1 the outflow from the reservoir was not measured directly at the gardiner dam but at the nearest station 120 km downstream of the dam in the city of saskatoon the closest meteorological stations are several kilometers away from the reservoir and there were no buoy stations on this large strategic waterbody in this study we endeavored to develop a water quality model for lake diefenbaker that could be used to support decision making based on the limited water quality field measurements and highly uncertain boundary conditions of the reservoir the general objective of this study was to present a comprehensive modeling exercise using chlorophyll a algal biomass ratios that vary both in time and space a novelty in improving in lake water quality simulations specific objectives for the lake diefenbaker system were to 1 understand the physical chemical and biological characteristics of the reservoir 2 estimate the rates of physical chemical and biological changes in different parts of the reservoir 3 provide a predictive framework for applying available climate change and land use change scenarios 2 methods 2 1 study site the south saskatchewan river is a long river 1392 km sheelanere et al 2013 flowing through the canadian provinces of alberta and saskatchewan merging with the ssr near the alberta saskatchewan border the red deer river rdr has turbid water due to the river s steep slope and prairie land setting the average ssr flow is about 4 5 times larger than that of the rdr flow based on 50 years of daily data approximately 171 kilometers downstream of the ssr rdr confluence is the inlet to lake diefenbaker at highway 4 fig 1 the lake diefenbaker bifurcates near the village of elbow due to the construction of the gardiner and qu appelle river dams in the 1960s most of the inflow 98 comes from the ssr and the main outflow 98 flows north through the gardiner dam details on the physical characteristics of lake diefenbaker are available in sadeghian et al 2015 the hydrometric station for water flow rates and water quality sampling for ssr is at medicine hat 374 km upstream of the reservoir and for rdr is at bindloss 218 km upstream of the reservoir 203 km and 47 km upstream of the ssr rdr confluence respectively table 1 the outflow from the reservoir is used from the hydrometric station at the city of saskatoon 120 km downstream of the gardiner dam there are some preliminary guidelines to estimate and interpolate the flow and climate data at the reservoir pomeroy and shook 2012 however the same routing methods could not be used to estimate the nutrient input data for lake diefenbaker because comparable guidelines for predicting the chemical and biological constituents had not been developed by biologists and engineers the reason is that many factors such as sedimentation erosion retention point source e g waste water treatment plants and nonpoint source loadings e g agriculture field surface runoff and groundwater infusion could significantly change nutrient concentrations in the inflow to lake diefenbaker via the ssr hence we built a pre model to transfer the river data to lake diefenbaker s inlet data but first improving the temporal resolution of measured data in ssr and rdr we found the best approach to estimate the state variable concentrations at the lake diefenbaker inlet was to first build the daily water quality database by correlating the water quality variables with the date julian day discharge and water temperature at the hydrometric stations on ssr and rdr then a model which was calibrated with measurements at the most upstream stations on lake diefenbaker was employed to transfer these values to the lake diefenbaker inlet for more details on lake diefenbaker see north et al 2015 2 2 model selection the current study required a model that can be implemented for both the river and reservoir systems the length of the whole system from the hydrometric station on the south saskatchewan river near medicine hat to the dams plus the distance from the bindloss station at red deer river to the confluence is 602 km because the reservoir has a maximum depth of 60 m a two dimensional model was required to adequately simulate the hydrodynamics as well as resolve the longitudinal and vertical water quality gradients lateral values are averaged a third lateral dimension was not necessary because of the narrow width of the river and reservoir we selected the ce qual w2 model which could be used to perform both hydrodynamic and water quality simulations in both the riverine ssr and rdr and the lacustrine lake diefenbaker parts of the system 2 3 model description the ce qual w2 model is a 2d laterally averaged water quality model over 40 years in development the us army corps of engineers launched the model in 1975 edinger and buchak 1975 developments continued by edinger and buchak for about ten years buchak and edinger 1984 edinger and buchak 1978 and in 1984 the model was handed over to a team led by tom cole during the next 20 years major refinements of both the hydrodynamic and water quality components were incorporated cole and buchak 1995 cole and wells 2003a b 2006 after 30 years of progress the us corps of engineers stopped model development and the model was handed over to portland state university model development is still continuing with an emphasis on computational efficiency easier application through a graphical user interface and more efficient and accurate numerical schemes cole and wells 2008 2013 2015a b the ce qual w2 model could be used to simulate all the major constituents and processes required for our research objectives including water temperature hydrodynamics dissolved particulate solids dissolved oxygen nutrients organic matter and algae in addition it had an up to date user manual and an active user forum also the source code was freely available with clear comments allowing extension and application of new formulations and algorithms finally the model has a complete set of output generation tools it can print the outputs for all component of the model at any longitudinal vertical and temporal point the model produces detailed output files useful for visual calibration 2 4 adaptation of the model to lake diefenbaker model development for lake diefenbaker was carried out in several stages we started by building a segmentation network of the ssr medicine hat to saskatchewan landing and lake diefenbaker saskatchewan landing to dams so the river was directly coupled with the reservoir the benefits of such a system were numerous based on the water level of the reservoir and river discharge flow at the upstream end of the reservoir can back up into the river because of these reverse flow effects in the reservoir particularly at its upper end the nutrients and flow characteristics were more accurately captured than with separate river and lake systems another benefit was that this combination reduced the need for data processing and transfer to the lake model the main drawback was the computational cost the calculation time for a combined river reservoir system increased dramatically the main reason was that the timestep requirement for the river is more stringent than for the reservoir much smaller timesteps were used for the reservoir than would be required if the model for the lake was ran independently of that for the river based on our access to the high performance cluster hpc system of university of saskatchewan we found that the computational burden was justifiable at the first stage for calculating water quality state variables at the reservoir inlet the ce qual w2 model is designed for windows platforms so in order to run the model on the linux hpc the code was adapted and compiled for the centos linux platform the instructions on how to compile and run the code on linux servers were added to the model user forum http w2forum cee pdx edu q node 500 although the model became more complex nine inter connected waterbodies as delineated by black lines in fig 1 were considered in the model the reason for having several waterbodies permitted the use of different climate data stations different water quality parameters and constants and different slopes for the river sections each waterbody had one main branch to which additional branches could be added the additional branches were required to define a different slope or to connect a stream to the main river stem one extra branch connected the rdr to the ssr the second branch changes the river slope to almost zero when the river enters the reservoir at saskatchewan landing see fig 1 and the last branch was the qu appelle arm at the downstream end the model has 827 segments ranging in length from 200 m to 1200 m and has 60 cartesian vertical layers each one meter in thickness we considered algae total dissolved solids tds inorganic suspended solids iss phosphate po4 ammonium nh4 nitrate no3 labile dissolved organic matter ldom refractory dissolved organic matter rdom labile particulate organic matter lpom refractory particulate organic matter rpom dissolved oxygen dissolved silica and total inorganic carbon tic in our simulations both the dissolved and particulate forms of organic matter were considered in the model setup each group was divided to two categories liable short decay time e g days and refractory longer decay time e g months to years we also used derived constituents in our outputs where we had measurements available including dissolved organic carbon doc particulate organic carbon poc dissolved organic nitrogen don particulate organic nitrogen pon total nitrogen tn dissolved organic phosphorus dop particulate organic phosphorus pop total phosphorus tp chlorophyll a and total suspended solids tss we calibrated the model based on measurements at the most upstream station 1 in fig 1 by using the same methodology employed in the hydrodynamic model of sadeghian et al 2015 we used the model results to prepare the input data at the lake diefenbaker inlet inorganic nutrients used directly as inputs to the model were po4 nh4 no3 and dissolved silica and the organic components were algae biomass and organic matter concentration there were also a few variables considered as derived variables the main reason for using the derived variables was the availability of measured data these variables included particulate organic carbon poc particulate organic nitrogen pon total nitrogen tn and total phosphorus tp table a2 contains equations for calculating the derived variables and their explanations tds was treated as a conservative variable inside the model tds influences the movement of water by changing density and also affects ph by influencing the ionic strength of carbon dioxide the measured tds was considered as 65 of recorded electrical conductivity compared with the model simulations since the tds is non conservative its concentration is mainly dependent on the correct inflow concentrations and accurate hydrodynamics interestingly tds simulations produced the lowest errors among all the variables used for calibration table 2 2 5 simulations to reduce computational expenditure we excluded the riverine ssr rdr sections and reduced the segmentation from 827 to 302 segments keeping the vertical layers intact fig 2 the computation time decreased from 18 h when the whole system 602 km was considered to only 3 h for each run on a desktop computer intel core i7 3770 s 8 mb cache 3 4 ghz 5 0 gt 12 gb ddr3 then we calibrated the model using the same methodology by combining some segments the number of segments in the lite model was reduced to 87 segments of length 800 m 4000 m fig 2 and the layers were reduced to 21 ascending from 1 m at the water surface to 4 5 m toward the reservoir bed fig 3 more details on the lite model preparation are provided in table a1 the computational time for the lite model was less than five minutes the motive for development of the lite model was that in many cases a fast and straightforward setup is more desirable for example during the 2013 calgary flooding a flood plume moved rapidly towards lake diefenbaker when using the model as a predictive tool reduced simulation times become invaluable another use of the lite model is to test and verify new model formulations for example in this study we have examined the effects of variable algal stoichiometry on the accuracy of chlorophyll a concentration calculations based on the formulations provided by chapra 2008 we still have small layers within the euphotic zone but the layering did not have any effect on the accuracy of the results 2 6 algae chlorophyll a ratio implementation in model biological components of the ce qual w2 model are phytoplankton algae epiphyte periphyton macrophytes and zooplankton animals epiphytes and macrophytes are more related to the riverine and shallow waterbodies and algae is more prevailed in lakes and reservoirs in this study data on zooplankton were not available and the reservoir environment of lake diefenbaker led us to use algae as the sole biological component the use of algae is challenging for the development of water quality models because it interacts with almost every single variable within the model fig 4 other problems associated with the use of algae are the limited number of observations and that the measurements are mainly in the form of chlorophyll a concentrations which need to be converted to algal biomass to make the comparisons either the chlorophyll a concentrations from measurements or the algal biomasses concentrations from simulations need to be converted to the other form algae biomass is multiplied by algae chlorophyll a ratio achla to obtain chlorophyll a concentrations achla is a fixed value in the model however in a natural setting this rate is not constant and changes based on light and nutrient availability therefore as a novel aspect of this study with some modifications to the source code the achla parameter was designed to change based on the limiting conditions of water eqs 1 chapra 2008 and 2 cole and wells 2015a 1 c h l a c 6 4 45 ϕ n 1 ϕ l 2 a c h l a 1 c h l a c where chla is chlorophyll a c is carbon algal biomass φ n is nutrient limiting factor 0 φ n 1 and φ l is light limiting factor 0 φ l 1 equation 1 is based on the formulation suggested by chapra 2008 and was successfully adopted in the 1d steady state water quality model qual2k the reason for implementing the equations in ce qual w2 was the limitations in the qual2k model for example the qual2k is a 1d model while ce qual w2 is 2d model qual2k is a steady state model while ce qual w2 uses dynamic inputs for flow nutrient and water quality state variables qual2k is written in visual basic vba in ms excel which can make only smaller computations compared with sophisticated ce qual w2 which is written in fortran and c the ce qual w2 model already calculates the limiting factor for algae production for nutrients eq 3 monod 1949 and light eq 4 steele 1962 therefore only the conversion from algal biomass to chlorophyll a needs to be adjusted 3 ϕ n n n h s n 4 ϕ l i i k e x p 1 i i k where n is nutrient concentration n hs is algal half saturation for nutrient limited growth i is light intensity and i k is light saturation intensity at maximum photosynthetic rate a minimum of either nitrogen or phosphorus limitation was applied as the nutrient limiting rate and then the light limiting effects were incorporated into the calculations of the chla c ratio 2 7 model calibration model calibration was done by comparing the simulated values for different water quality variables from the model with the measured values from the field works and laboratory analysis the averaged root mean squared error r m s e eq 5 was used as the objective function for measuring the model performance statistically 5 r m s e o s 2 n o where o is the observed and s is the simulated values n is the number of samples and o is the average of observed values the use of r m s e is preferred to the rmse to normalize the error associated with different water quality variables the optimum values of the parameters determined through the calibration are provided in table a1 water temperature and flow were the dominant factors influencing model calibration calibration of the thermal and hydrodynamic regimes are discussed in detail in sadeghian et al 2015 the next most influential variable was dissolved oxygen dissolved oxygen is an important water quality component which can provide insight about the quality of water in the absence of other variables ji 2008 oxygen is necessary for the most of chemical and biological processes in the water fig 4 due to the interdependence of nutrients we first calibrated the do po4 nh4 no3 and organic matter together to estimate the model coefficients then the remaining parameters tds iss doc poc pon tn and tp were calibrated 3 results and discussion based on our model setup atmospheric aeration algal photosynthesis and oxygen in inflow water were the sources of dissolved oxygen to the reservoir algal respiration excretion and mortality decay of allochthonous organic matter nitrification and sediment oxygen demand were the consumers of oxygen similar to water temperature do measurements were based on sonde probes yielding a higher number of measurements than those made in the laboratory therefore the calibration was easier for do and made with less uncertainty the r m s e for dissolved oxygen based on 5000 field measurements one meter vertical resolution at 14 stations on 79 different days from 2011 to 2013 was 0 19 fig 5 temperature light dissolved oxygen and nutrient availability both inorganic and organic determine the rates of chemical and biological reactions in water dissolved oxygen concentration is simple to calibrate but difficult to validate because many components interact with oxygen fig 4 with many parameters contributing to oxygen production and consumption additionally an incorrect model parameter set can produce accurate results for example according to the model manual cole and wells 2015a the decay of ammonium requires 4 57 g o2 g n eq 6 while the decay of organic matter requires 1 4 g o2 g organic matter eq 7 which is about four times less oxygen compared to ammonium decay although a decay rate of 0 1 day 1 for ammonium has the same mathematical effect on oxygen consumption as a decay rate of 0 4 day 1 for organic matter the results on the nutrient pool are significantly different to avoid an incorrect setup do organic matter po4 nh4 and no3 were calibrated simultaneously fig 5 due to a high number of observations for do the model results for do have small uncertainties correctly predicting dissolved oxygen is a crucial part of water quality modeling because oxygen plays a vital role in the lives of aquatic organisms 6 2 n h 4 3 o 2 2 h 2 o 4 h 2 n o 2 2 n o 2 o 2 2 n o 3 7 5 c o 2 2 h 2 o n h 3 c 5 h 7 n o 2 5 o 2 the model results showed that oxygen depletion occurred more in areas with higher ammonium concentrations the ammonium stems from both the inflow ssr and organic matter decay in lower dark layers of the lake where algal photosynthetic activities are minimal due to poor light conditions the oxygen depletion is higher at these depths nitrification could be the primary oxygen consumer because two molecules of oxygen are required to convert each molecule of ammonium to nitrite and then to nitrate eq 6 ammonium and nitrate are not taken up by algae equally some algae groups may prefer ammonium over nitrate cole and wells 2015a and the preference may change through the life of algae for example at early stages algae may prefer ammonium but resort to nitrate as they mature aquarium fertilizer 2012 the lake diefenbaker model is not sensitive to the coefficient for algal nitrogen uptake preference aneqn between ammonium and nitrate because only nitrate and phosphate are consumed by algae oxygen depletion is overestimated by the model at the water surface the decay of organic matter accompanied by the formation and release of nitrate and phosphate from the sediment is also an oxygen depleting process near the bed of the reservoir the main sources of organic matter into the system are the incoming concentrations by ssr inflow external source and algal production in lake process the organic matter mineralizes to form inorganic nutrients and some of which deposit in the sediment fig 4 algal mortality contributes to particulate organic matter and algal excretion to both dissolved and particulate organic matter an example of a simplified cycle of nitrogen as a calibration variable in this model is presented in fig 6 algae contribute to both the particulate and dissolved organic nitrogen through mortality excretion and respiration a portion of the particulate form goes into the sediment fig 4 and a portion mineralizes into dissolved form organic matter concentrations were included into the simulations but only those results are shown for which we had measurements for comparison sake figs 5 and 7 the poc values are the sum of all forms of liable and refractory particulate organic matter lpom and rpom respectively multiplied by the carbon to biomass ratio δc which had a value of δc 0 45 in our simulations the doc values which are the sum of liable and refractory dissolved organic matter ldom and rdom respectively multiplied by the carbon to biomass ratio δc 0 45 were not shown because the measurements were not available particulate organic nitrogen pon is the nitrogen concentrations in organic matter both liable and refractory which is produced by phytoplankton or enters the rivers via detritus in fig 5 the pon is the sum of nitrogen components of liable and refractory dissolved organic matter ldom and rdom respectively since algae are a good representative of the health of aquatic system they are a component of any water quality model algae have many interactions with inorganic and organic pools of nutrients in water looking into the example of the nitrogen cycle fig 6 algae interacts with five different forms of nitrogen therefore quality of input data is critical for obtaining a correct algae simulation as a result of low quality input data most water quality models simulate algae behavior only moderately well or even poorly in many cases arhonditsis et al 2006 in addition to the quality of input data the other primary reason for getting poor algal results is using fixed growth mortality respiration excretion and settling ratios for algae in models the measurements at lake diefenbaker confirmed that algae adjust their chla c ratios based on p availability abirhire et al 2015 the upstream part of the reservoir is turbid so the algae have lower achla values due to low light conditions see eq 2 chlorophyll a simulations were almost the poorest among all the variables before applying the variable chlorophyll a biomass ratio table 2 based on model results achla had a range of 20 130 mg algal carbon mg chlorophyll a for lake diefenbaker during 2011 2013 fig 7 the minimum achla was at the upstream end of the lake in winter when light is most limiting in the model used in this study we used a variable chla c ratio for converting the simulated algal biomass into chlorophyll a concentrations by doing so simulation errors r m s e decreased by about 50 from 2 28 to 1 15 table 2 fig 8 meaning better performance and predictive capability of the model the factors that influence the chlorophyll a algal biomass ratio are not limited to nutrient and light availability other factors include species type life stage and previous light conditions to name but a few the model results could be improve by including these factors into the simulations however lack of data on species partitioning and our research scope to build simpler models prevented us from doing that it is worth mentioning that in eutrophication studies usually the diatom and green algae are not considered a threat to the environment and only the cyanobacteria concentrations are important hence partitioning algae species are required to avoid uncertainties in those studies depending on the objectives of the study the grid discretization may be made coarser or finer for example for the variable stoichiometry the study required the correct nutrient temperature and light values at the euphotic zone the lite model has longer longitudinal segments and thicker vertical layers however the layers become thicker with depth fig 3 as a result the euphotic zone for algae production remains almost unchanged and the thicker layers at the bottom did not significantly affect the results by increasing the longitudinal and vertical spacing we were able to run this lite model about 40 times faster without sacrificing accuracy it is worth mentioning that the grid discretization should be made according to the study objectives for example for a study with the objective of locating the thermocline depth at the coteau creek arm recreational beach near the gardiner dam on the 5th of august a model with fine vertical grids would be required comparing model results table 2 for the standard and lite models with a fixed achla some variables have smaller r m s e values for the lite model the reason could be averaging over depth in the lite model the vertical layers reduced from 60 to 21 hence in r m s e calculations the observations over larger layers were averaged yielding smaller model errors 4 conclusion we developed a 2d laterally averaged water quality model of lake diefenbaker by using the ce qual w2 model we considered a top down approach by first developing a large complex model and gradually simplifying it we first began building a riverine plus reservoir model 827 segments and 60 vertical layers and then making the model smaller by eliminating the riverine sections and considering only lake diefenbaker 302 segments and further simplifying the model by making the segments longer and vertical layer heights thicker 87 segments and 21 vertical layers depending on the study objective and available input data these models can be used to launch future climate and land use change scenarios the main objective of this study was to improve algae simulations by the model hence the model setup considered a complete list of important variables including do po4 nh4 no3 ldom rdom lpom rpom algae tds and tss then we applied chapra s 2008 proposed method for calculating chlorophyll a concentrations based on nutrient and light availability by using a variable stoichiometry we were able to calculate chlorophyll a more accurately compared with using a fixed chlorophyll a algal biomass ratio in lake diefenbaker although we were able to calibrate these models successfully excessive computational resources and time were used for calculations and interpretations of the results the majority of these efforts could have been avoided if better field measurements had been available for this reservoir we found much smaller simulation errors up to ten times table 2 for those variables that had enough measurements including do tds and water temperature hence it is recommended to first install a fixed sampling station at the upstream end of the reservoir to measure daily hourly flow and weekly monthly water chemistry data second the frequency of in lake sampling especially for nutrients and chlorophyll a during summer should at least be doubled moreover there is a need for data collecting groups to move towards utilizing new and cost effective instrumentation such as fluoroprobe by bbe moldaenke which can be used for chlorophyll measurements with the ability of species distinction between diatoms green algae and cyanobacteria acknowledgements this work was financially supported by the canada excellence research chair in water security through the global institute for water security we thank environment canada the saskatchewan water security agency and alberta environment for providing the hydrometric and water quality data we are grateful to meteoblue for providing the meteorological data thanks to the limnology laboratory at the university of saskatchewan for providing the water turbidity and temperature data thanks also to the department of geography and planning at the university of saskatchewan for providing the bathymetry data appendix table a1 ce qual w2 initial and optimal values for different parameters in the lite model parameters definitions are from the ce qual w2 manual cole and wells 2015a table a1 parameter value description imx 87 number of segments in the computational grid kmx 23 number of layers in the computational grid latitude 50 71 latitude degrees longitude 107 28 longitude degrees tmstrt april 1 2011 model simulation starting time tempi 4 initial temperature c tds 250 tds g m3 or mg l iss 2 inorganic suspended solids mg l po4 0 01 po4 p mg l as p nh4 0 02 nh4 n mg l as n no3 0 01 no3 n no2 n mg l as n dsi 2 dissolved silica mg l as si ldom 0 1 labile dissolved organic matter mg l as organic matter rdom 0 1 refractory dissolved organic matter mg l as organic matter lpom 0 67 labile particulate organic matter mg l as organic matter rpom 0 1 refractory particulate organic matter mg l as organic matter alg 0 00001 algae mg l as dry weight organic matter do 8 dissolved oxygen mg l tic 15 total inorganic carbon mg l as c exh2o 0 25 extinction for pure water m 1 exss 0 1 extinction due to inorganic suspended solids m 1 g m3 exom 0 1 extinction due to organic suspended solids m 1 g m3 exa 0 2 algal light extinction m 1 g m3 β 0 45 fraction of incident solar radiation absorbed at the water surface sss 1 1 suspended solids settling rate m day 1 τcr 1 9 critical shear stress for sediment resuspension dynes cm2 ag 0 43 maximum algal growth rate day 1 ar 0 04 maximum algal respiration rate day 1 ae 0 02 maximum algal excretion rate day 1 am 0 05 maximum algal mortality rate day 1 as 0 13 algal settling rate m day 1 ahsp 0 003 algal half saturation for phosphorus limited growth g m3 ahsn 0 014 algal half saturation for nitrogen limited growth g m3 asat 114 light saturation intensity at maximum photosynthetic rate w m 2 at1 6 9 lower temperature for algal growth c at2 22 3 lower temperature for maximum algal growth c at3 35 upper temperature for maximum algal growth c at4 40 upper temperature for algal growth c algp 0 005 stoichiometric equivalent between algal biomass and phosphorus algn 0 08 stoichiometric equivalent between algal biomass and nitrogen algc 0 45 stoichiometric equivalent between algal biomass and carbon achla 0 05 ratio between algal biomass and chlorophyll a in terms of mg algae μg chlorophyll a apom 0 8 fraction of algal biomass that is converted to particulate organic matter when algae die aneqn 2 equation number for algal ammonium preference ldomdk 0 02 labile dom decay rate day 1 rdomdk 0 001 refractory dom decay rate day 1 lrddk 0 001 labile to refractory dom decay rate day 1 lpomdk 0 05 labile pom decay rate day 1 rpomdk 0 001 refractory pom decay rate day 1 lrpdk 0 011 labile to refractory pom decay rate day 1 poms 0 25 pom settling rate m day 1 po4r 0 001 sediment release rate of phosphorus fraction of sod nh4r 0 001 sediment release rate of ammonium fraction of sod nh4dk 0 15 ammonium decay rate day 1 no3dk 0 05 nitrate decay rate day 1 no3s 0 011 denitrification rate from sediments m day 1 sod 0 1 zero order sediment oxygen demand for each segment g o2 m 2 day 1 table a2 derived variables in ce qual w2 model cole and wells 2015a table a2 poc pom orgc 7 pon pom orgn algae an 8 tn don pon nh4 no3 9 tp dop pop po4 tpss 10 where algae algal biomass poc particulate organic carbon pon particulate organic nitrogen tn total nitrogen tp total phosphorus pom particulate organic matter orgc stoichiometric equivalent between organic matter and carbon orgc 0 45 orgn stoichiometric equivalent between organic matter and nitrogen orgn 0 08 an stoichiometric equivalent between algal biomass and nitrogen an 0 08 don dissolved organic nitrogen tpss ss partp ss suspended solids partp phosphorus partitioning coefficient for suspended solids 
26436,the purpose of this paper is to investigate cost effective climate policy instruments for bioenergy and timber adapted to the impacts on interdependent forest carbon pools and applied in the eu climate policy to 2050 we develop a discrete time dynamic model including forest carbon pools in biomass soil and products as well as fossil fuel consumption the analytical results show that the optimal taxes on forest products depend on the growth in the respective carbon pool the application to the eu 2050 climate policy for emission trading shows that total costs for target achievement can be reduced by 33 percent if all carbon pools are included and the carbon tax on fossil fuel can be reduced by 50 percent optimal taxes on forest products differ among countries and over time depending on the potential for increased carbon sequestration over the planning period keywords eu climate policy carbon sequestration bioenergy timber policy instruments jel classification q23 q28 q48 q54 1 introduction terrestrial carbon pools 1 1 we use the ipcc 2003 definitions as presented by fao 2014 where carbon pool refers to carbon reservoirs with the capacity to accumulate or release carbon carbon stock to the amount of carbon in the pools at a specific point of time sequestration as the process of increasing the carbon content in the pools and carbon sink as a process for removing carbon content from the atmosphere have received attention for their climate change mitigation potential and the comparatively low associated costs increased carbon pools in natural ecosystems could thus be an alternative and complement to other measures such as reduced fossil fuel use and increases in renewable energy bosetti et al 2009 murray et al 2009 sohngen 2009 it can be costly to ignore forest carbon flows and stocks when developing strategies against climate change in europe the sequestration of carbon in forest biomass and soils corresponds to 8 10 percent of the total emissions kuikman et al 2011 lal 2005 and sequestration tends to increase over time kauppi et al 1992 liski et al 2002 consideration of the risk for future carbon losses and the potential for targeted increases in carbon sequestration could thus be of importance for economic and environmental reasons within the european union eu crediting of increases in natural carbon pools against the co2 burden allocation is not allowed in spite of the substantial cost savings it could entail gren et al 2012 michetti and rosa 2012 münnich vass and elofsson 2016 arguments against the introduction of policies to enhance carbon sinks in the eu include the complexity and mutual interdependence of forest carbon pools and the difficulties of designing appropriate incentive structures kuikman et al 2011 forest carbon consists of two main natural pools above ground carbon in the biomass and below ground carbon in the soil lal 2005 forest harvesting decisions affect the stock of carbon in growing biomass but also indirectly influence the stock of soil carbon jandl et al 2007 kuikman et al 2011 lal 2005 neglect of this dependency will lead to false conclusions about the impact of forest management on total forest carbon sequestration the dependency between forest carbon pools further aggravates policy design for carbon sink enhancement even if only a single carbon pool is considered there are challenges concerning monitoring of carbon stock changes and verification of the additionality and permanence of such changes cf bento et al 2015 engel et al 2015 mason and plantinga 2013 much of the literature on policy instruments for carbon sequestration deals with instruments directed towards individual forest owners which require measurement and monitoring of changes in each forest owner s carbon pool guthrie and kumareswaran 2009 latta et al 2011 lecocq et al 2011 updegraff et al 2010 van kooten et al 1995 policies targeting forest products could then have an advantage because of the comparatively lower costs to measure and monitor these products hoel and sletten 2016 additionality can still be a concern but the issue would be reduced to evaluation of the aggregate additionality rather than the additionality of sequestration achieved by each forest owner the purpose of this paper is to analyze the design of policies on forest products to enhance carbon sequestration in interdependent carbon pools as a complement to reductions in emissions from fossil fuels the analysis is applied to carbon sequestration in forest biomass and soils and carbon storage in forest products in the eu climate policy from 2010 to 2050 within the eu inclusion of a single carbon pool can be seen as a feasible alternative if for example there is disagreement about the advantages of including several carbon pools we therefore compare separate and complete inclusion of biomass soil and forest product pools in the policy decision in order to assess whether separate inclusion is a step in the right direction or even counterproductive in addition we investigate the cost efficient economic incentives for achieving increased carbon sequestration this is done with an aim to evaluate the potential for common policy instruments at the eu level to promote carbon sinks for these purposes we construct a discrete dynamic model for cost efficient attainment of future targets on carbon emission as suggested by the eu 2050 climate policy eucom 2012 by means of reduced combustion of fossil fuels and forest products and enhanced carbon sequestration the interlinked carbon pools are managed by taxes targeting timber and bioenergy which differ with respect to the displacement of fossil fuel it is shown analytically that the cost efficient carbon taxes on timber and bioenergy can either increase or decrease when both biomass and soil pools are considered instead of only one of these pools the direction of impact depends on the effect of harvesting on the growth rate in the respective pool it is also shown that the tax on timber decreases for a delayed combustion of wood products because of the larger discounting of future costs of carbon emissions the empirical results show that inclusion of carbon sequestration reduces overall costs for reaching eu 2050 climate targets by 33 percent and the optimal carbon tax on fossil fuel by up to 50 percent if only a single carbon pool is included the choice of pool to include matters not only for the cost savings achieved but also for the net impact on carbon emissions the optimal tax on fossil fuels is increasing over time for all countries but the carbon tax on wood products can either increase or decrease depending on the forest growth rate and the time path of reduction targets our study belongs to two main strands of the literature economics of carbon regulation by forest management and design of policy instruments for carbon sink enhancement several earlier economic studies on forest management include more than one forest carbon pool in the analysis such as lubowski et al 2006 newell and stavins 2000 sohngen and mendelsohn 2003 van kooten et al 1999 and wise and cacho 2005 however we have not found any study which compares a second best policy including only a single carbon pool with the first best policy where several interlinked pools are included a number of studies analyze policy instruments applied to a single biomass carbon pool using a national forest sector model caurla et al 2013 and lecocq et al 2011 compare the impact of alternative combinations of climate policy instruments on the forest sector and resources van kooten et al 1995 show that a combination of carbon taxes and subsidies can be used to achieve socially optimal forest rotation and latta et al 2011 investigate the consequences of a tax subsidy scheme voluntary or mandatory in a forest sector model with respect to the literature on policies for carbon sink enhancement mason and plantinga 2013 conclude that a uniform carbon subsidy scheme implies higher costs for achieving sequestration than a contract design system bento et al 2015 analyze the role of the additionality problem and monitoring costs for the design of carbon offset contracts using a real options model with uncertain future timber prices guthrie and kumareswaran 2009 compare subsidies paid in proportion to the actual amount of carbon sequestered to credits that are allocated according to the long run potential to sequester carbon showing that the former generates more sequestration using a globally aggregated model hoel and sletten 2016 analyze optimal taxes on energy consumption differentiated between fossil fuel energy and bioenergy to account for the impact on forest sequestration compared to those our study contributes through analysis and empirical calculation of cost efficient nationally differentiated taxes on timber bioenergy and fossil fuels for reaching politically determined targets on carbon dioxide emissions while accounting for the role of carbon pool interdependence the paper is organized as follows first the numerical model is described followed by the derivation of the cost efficient policy instruments then data are described and results are presented the paper ends with a discussion and conclusions 2 numerical model consider the eu with i 1 27 different countries together the countries have agreed on a co2 emissions reduction path until 2050 which they wish to implement at least cost the emission reductions can be achieved by either reduced consumption of fossil fuels within the eu emission trading scheme or by implementing changes in forest management the potential to use forests for different purposes is ultimately determined by the existing forest biomass and its development over time the development of the growing stock of trees 2 2 the growing stock is typically defined as the volume of all living trees in a certain area of forest with a minimum diameter at breast height and includes the stem from ground level or stump height up to a given top diameter and may also include branches above a certain diameter here the growing stock is assumed equal the merchantable tree volume on an average hectare of land is defined by 1 v t 1 i v t i g t i v t i h t 1 i v 0 i v 0 i where variables are measured in cubic meters h t 1 i is the harvest in country i which is assumed to take place in the beginning of the year 3 3 the choice of timing of the harvest in the beginning rather than in the end of each time period is made because this later facilitates the interpretation equations 15 and 16 v t i is the growing stock measured directly after the harvest and g t i v t i is the annual growth total stem wood volume in a country is a i v t i where a i is the area of forest land measured in hectares it is assumed that g t i v t i is positive differentiable and increasing up to a given intermediate volume level thereafter it successively declines towards zero as the forest approaches carrying capacity the use of a representative hectare of forest land is a simplification compared to large scale age class forestry models such as sohngen and sedjo 2006 which has global coverage and moiseyev et al 2011 which includes the european countries this simplification is motivated by our aim to describe the optimal time dynamics of carbon sequestration both analytically and numerically in relation to reductions in co2 emissions from fossil fuel the use of a representative hectare is reasonable if the distribution of forest across different age classes is relatively even it works less well if for example there is a disproportionally large area of forest near economic maturity which typically implies high growth when such a forest is harvested and replaced by young forest the average growth could fall more than is accounted for by equation 1 the approach is therefore more reasonable on a higher level of aggregation such as the national level as storm felling and fires could lead to considerable local variations in the age class distribution while having little impact on the aggregate national level forest carbon sequestration occurs in growing trees and in forest soil net annual carbon sequestration in trees w t i is assumed to be defined by 2 w t 1 i η β i a i v t 1 i v t i where β i is the ratio of total wood volume to stem volume and η is a parameter for conversion of tree volume to ton co2 equivalents removed from the atmosphere the development of the soil carbon stock on a representative hectare of land p t i is mainly determined by the wood volume which adds to the soil carbon stock as litter falls to the ground by forest harvest and natural carbon release following liski et al 2002 we assume that litter from the growing stock adds to the soil carbon stock and that the decomposed litter added is a constant share κ i of the total wood volume in a given year harvesting can cause a release of soil carbon on the harvested area due to disturbances in the soil structure shifts in abundance of woody and herbaceous vegetation and altered soil water and temperature conditions which increase decomposition jandl et al 2007 kuikman et al 2011 it is here assumed that the release of soil carbon depends on the area of final felling and carbon content in the soil the harvested share of the forest land area is then described by γ i h t 1 i v t i g t i v t i where γ i is a constant converting the share of volume harvested h t 1 i v t i g t i v t i into the share of harvested area 4 4 recall that h t 1 i occurs in the beginning of period t 1 whereas v t i g t i v t i is the volume measured in the end of period t we have γ i 1 since forest is harvested at an old age when the growing stock per hectare is relatively large the carbon release from a representative hectare of forest land due to final felling is then assumed to equal the harvested share of the area times a constant fraction ν i of the carbon soil pool p t i this gives release 5 5 we make a simplification by assuming that all of the loss occurs within a single time period albeit a net decline in soil carbon might continue over 10 15 years covington 1981 federer 1984 this simplification is only motivated by convenience of modelling in period t 1 as a share 0 ν i γ i h t 1 i v t i g t i v t i 1 of p t i in addition soil carbon is assumed to be continuously released due to natural processes we follow liski et al 2002 by assuming that the carbon pool p t i decays at constant rate ϑ i in each time period 6 6 detailed soil carbon models such as that in liski et al 2002 divide soil carbon into several interdependent sub pools while assuming a constant rate of decay for each of these sub pools the development of the soil carbon stock 7 7 measured at the end of the time period on an average hectare of forest land p t i is then defined by 3 p t 1 i p t i ν p t i γ h t 1 i v t i g t i v t i κ i η β i v t i ϑ i p t i the second term on the r h s expresses soil carbon losses due to final felling in the beginning of time t 1 8 8 we include the possibility for such losses here but given the mixed empirical evidence on such losses as the losses typically depend on harvesting technology jandl et al 2007 we also investigate a case with zero carbon losses from final felling in the sensitivity analysis the third term shows the added carbon from litter and the fourth the natural decay of soil carbon accounting for the release of carbon to the atmosphere total annual carbon sequestration in forest soil m t i can then be expressed as the incremental change in the soil carbon stock 4 m t 1 i a t 1 i p t 1 i p t i total carbon sequestration in trees and soils s t i is then 5 s t i w t i m t i the harvested forest volume is used for two different purposes bioenergy and timber 6 a i h t i b t i t t i where b t i and t t i are the total volumes of bioenergy and timber respectively bioenergy and timber both affect co2 emissions following hoel and sletten 2016 we assume that the co2 content of bioenergy η b t i is released to the atmosphere in the same time period as it is harvested the released co2 is however partly offset by displacement of fossil fuels displacement depends on the relative efficiency of bioenergy and replaced fossil systems schlamadinger and marland 1996 the parameter τ with τ 0 1 expresses net co2 emissions per unit of co2 in bioenergy after taking fossil fuel displacement into account implying that net co2 emissions from bioenergy are equal to τ η b t i when used as timber carbon is stored in wood products which are assumed to have a life span of k i years cf eggers 2002 after which they are combusted for energy purposes and the co2 content is released like bioenergy timber that is combusted is assumed to replace fossil fuels hence the emissions are partially offset implying that the net release of co2 after k i years is τ η t t k i i the contribution of bioenergy and timber to co2 emissions in a given year l t i can then be summarized as 7 l t i τ η t t k i i b t i where the first term is the release of carbon from wood products combusted at the end of their lifetime and the second is the net contribution of bioenergy to co2 emissions given the displacement of fossil fuels the above formulation implies that we abstract from carbon emissions that arise during harvesting transporting and processing of bioenergy and timber this simplification is justified by the fact that these emissions only correspond to about 2 per cent of the total carbon emissions from wood products petersen raymer 2006 the net reduction of co2 in the atmosphere r t i due to forest carbon sequestration and the different uses of forest products can then be summarized as 8 r t i s t i l t i the combustion of fossil fuels in each country contributes to co2 emissions emissions of co2 from fossil fuels are determined by the quantities of fossil fuels consumed x t i j with j 1 6 different types of fuel 9 9 hard coal lignite natural gas light fuel and heating oil heavy fuel oil and jet fuel and emission coefficients for each fuel type α j total emissions in all countries from fossil fuels and forest management e t are then 9 e t i j α j x t i j r t i there are costs associated with reduced fossil fuel consumption and a changed supply of forest products the cost for reducing the consumption of a certain type of fossil fuel is defined by c t x i j x b a u i j x t i j where x b a u i j is the business as usual bau consumption of the fossil fuel in question it is assumed that the cost function is twice differentiable decreasing and convex and that the consumption cannot fall below a given minimum level x t i j i e x t i j x t i j x b a u i j the use of timber and bioenergy can in principle be either reduced or increased in order to abate carbon emissions the cost of changing bioenergy production is defined as c t b i b b a u i b t i where b b a u i is the bau production of forest bioenergy it is assumed that b t i is subject to lower and upper bounds such that b i b t i b i in a corresponding manner changes in the production of timber give rise to a cost c t t i t b a u i t t i where t b a u i is the bau production level and lower and upper bound apply i e t i t t i t i the cost functions for bioenergy and timber are assumed to be continuous convex and decreasing increasing in b t i and t t i below above the bau level costs are assumed to be separable in bioenergy timber and fossil fuels studies applied at the regional scale often assume that a fixed share of the forest harvest is used for bioenergy see e g carlsson 2012 and trømborg and sjølie 2011 in contrast the global model in eriksson 2015 assumes bioenergy and timber are separable in production notably there is large variation in the share of forest harvest used for bioenergy in different european countries see table a1 in the appendix which implies that fixed shares is not an appropriate assumption at this scale 10 10 one possible reason for variations in forest use is differences in the use of forest residues for energy purposes data on the use of forest residues would be necessary to empirically assess the degree of cost separability between timber and bioenergy but such data are not available for the eu countries the assumption about cost separability in fossil fuels and forest products is motivated by the comparatively small role of bioenergy and timber combustion for total energy consumption 11 11 using data for 2010 on fossil fuel consumption and bioenergy production see appendix and a factor 0 18 for conversion of biomass in m3 to toe the bioenergy produced in european forests corresponds to about 1 per cent of the total energy from fossil fuels a more elaborate analysis of the substitution in production and consumption and hence cost interdependence between bioenergy and fossil fuels would require detailed analysis of supply and demand in different industries which is beyond the scope of this paper given its focus on sequestration time dynamics and carbon pool interdependencies instead the substitution is accounted for in a simplified manner through the use of a displacement factor see equation 7 it is assumed that eu policy makers want to meet a sequence of annual emissions targets e t m a x which are based on eu s roadmap for moving to a low carbon economy by 2050 eucom 2012 the sequence of emission targets can be met by reductions of the consumption of fossil fuels and changes in forest management which affect bioenergy and timber production as well as carbon sequestration in growing forests and soils the emission targets are expressed as 10 e t e t m a x for the targets years t 1 40 it is assumed that policy makers wants to meet 10 at a minimum cost the decision problem is then to 11 m i n x t i j b t i t t i t c t i ρ t c t b i b b a u i b t i c t t i t b a u i t t i j c t x i j x b a u i j x t i j s t 1 10 and the upper and lower bounds on the decision variables the dynamic discrete time lagrangian for this problem and the associated necessary conditions for an interior solution are presented in the appendix in the following section we present the cost efficient policy instruments which are derived from the necessary conditions 3 cost efficient policy instruments for fossil fuels the cost efficient tax is defined by 12 c t i j x b a u i j x t i j x t i j λ t α j i e each fuel is taxed in proportion to the carbon emissions per unit of fuel thus we get the well known result that all fuels can be taxed in proportion to the carbon emissions using a tax per ton of carbon equal to λ t i e the shadow cost of the emission constraint the shadow cost increases over time due to increased target stringency and depends jointly on the costs for fossil fuel reductions and changed forest management at different points in time taxes on timber and bioenergy will affect the decisions by forest product suppliers assuming that there are well functioning market for forest products in all countries the forest supply sectors decision problem will be to minimize the sum of the costs for producing below or above the bau levels i e the levels that would be privately optimal in the absence of taxes plus costs for taxation 13 m i n b t i t t i t c t i ρ t c t b i b b a u i b t i c t t i t b a u i t t i ψ t b i b t i ψ t t i t t i where ψ t b i and ψ t t i are the unit taxes on bioenergy and timber respectively assuming an interior solution the first order conditions for the problem in 13 require that 14 c t t i t b a u i t t i t t i ψ t t i and 15 c t b i b b a u i b t i b t i ψ t b i i e the marginal cost for adjusting timber and bioenergy supply equals the tax on the respective products comparing the expressions in 14 and 15 with the first order conditions for the policy makers decision problem in 11 see appendix we find that the efficient level of adjustment of timber supply will be induced by setting the tax on timber such that the indirect effects on sequestration in trees and soils are taken into account 16 ψ t t i c t t i t b a u i t t i t t i μ t v i 1 a t i μ t p i ν γ p t i v t 1 i g t 1 i v t 1 i a t i ρ k i 1 λ t k i τ η thus the optimal tax on timber is set such that the marginal cost i e the foregone current return due to a change in timber production equals the marginal benefit to society of that change the marginal benefit equals the value of the associated impact on growing stock and soil carbon stock and the discounted value of the impact on the emission target k i periods later the two first terms on the r h s reflect the country specific impact of changed timber production on the costs of future sequestration due to the impact on tree and soil carbon stocks it should be noted that the marginal user cost of the growing stock and soil carbon μ t v i and μ t p i can be positive or negative depending on whether forest growth is positively or negatively affected by the changed forest volume and harvests increase or decrease thus taxes may differ across countries in both sign and magnitude the corresponding efficient tax on bioenergy is defined by 17 ψ t b i c t b i b b a u i b t i b t i μ t v i 1 a t i μ t p i ν γ p t i v t 1 i g t 1 i v t 1 i a t i λ t τ η where the interpretation of the two first terms on the r h s is similar to that in equation 16 the last term expresses the marginal value of the impact on emission constraint in current year comparing equations 16 and 17 it can be seen that the efficient taxes on bioenergy and timber differ only due to the timing of the release of the carbon content the difference is then determined by the development of the discounted shadow cost over time to further understand how the efficient taxes on bioenergy and timber are determined we can look closer at the determinants of the marginal user costs using the necessary condition for the growing stock we have that 18 μ t v i ρ μ t 1 v i 1 g t i v t i ρ μ t 1 p i ν p t i γ h t 1 i 1 g t i v t i v t i g t i v t i 2 κ β i ρ λ t 1 λ t η β i a i equation 18 shows first that the marginal user cost of the growing stock μ t v i depends on the future value of the stock given forest growth if an increase in stock volume leads to increased forest growth μ t v i is larger there is then an additional value of leaving the wood in the forest because of the higher future sequestration that will be achieved second there is a value of the positive impact of increased forest volume on the soil carbon stock this impact is high if litter production κ i and soil carbon stocks are large as increased forest volume reduces the area subject to final felling ceteris paribus the last term expresses the impact of a change in forest volume on the emission targets at time t and t 1 as the marginal user cost of forest volume μ t v i is determined by the increase in the discounted shadow cost λ t we can conclude that rapidly increasing target stringency in combination with a low discount rate will imply a higher marginal user cost in such a case it is cost efficient to allocate harvests over time such that high sequestration can be obtained in the latter part of the policy period thereby reducing the need for costly fossil fuel reductions turning to the marginal user cost of soil carbon stock it can be written as 19 μ t p i ρ μ t 1 p i 1 ϑ i ν γ h t 1 i v t i g t i v t i ρ λ t 1 λ t a i the marginal user cost of soil carbon μ t p i reflects the future value of the soil carbon stock for meeting annual climate targets it is affected by stock development captured in the first term on the r h s of equation 19 a high decay rate ϑ i reduces the marginal user cost as a soil carbon stock increase in the current time period will to a larger extent be lost to the atmosphere in the following time period similarly a high share of harvested volume i e a high h t i v t i g t i v t i implies that an increase in the carbon soil stock will to larger extent be lost in the following time periods due to final felling both a high decay rate and a high harvest share will therefore increase user costs of soil carbon in the current time period the last term in 19 carries a similar interpretation as in equation 18 common to all countries is the increase in the carbon tax over time from increased target stringency as expressed by λ t depending on the marginal user costs of the growing stock and soil carbon pool the taxes on timber and bioenergy products can increase or decrease over time the tax is relatively high when the marginal user costs are high i e when the accumulation of carbon in trees and soil is large which occurs for a relatively high marginal growth in tree volume and a low rate of decay of soil carbon 4 data data and method for calculation of fossil fuels reduction costs within the eu emissions trading system follow gren et al 2009 where the costs are calculated as the decrease in consumer surplus when fossil fuel consumption is reduced emission coefficients for each type of fossil fuel have been obtained from the same source cost functions for decreases and increases in bioenergy and timber are calculated as changes in producer surplus i e the cost to producers in terms of profits foregone in the case of a reduction and costs above the market price payment in the case of an increase inverted linear supply functions for forest products were calculated based on estimates of price elasticities price data and input use data these supply functions were used to calculate quadratic cost functions consumer side welfare effects are not included for forest products because for bioenergy demand is highly politically determined and because trade in forest products is not easily incorporated in a partial model for both fossil fuels and forest product it is assumed that bau prices and quantities are constant over the studied time period this is a simplification as technological development or changes in demand could alter these prices and quantities the simplification is motivated by the use of a partial equilibrium model and the focus on the role of different carbon pools for climate policy cost functions for fossil fuels and forest products and data used for the calculations can be found in the appendix aggregate forest growth functions on national level suitable for our purpose are not available some earlier studies such as kallio et al 2004 use aggregate biomass and forest growth functions but assume a constant forest growth rate other studies such as schulp et al 2008 assume a constant forest growth until a given forest age is reached and growth shifts to zero none of these approaches are suitable when the focus is on the dynamics of sequestration over a longer time period as the successive decline in growth and hence sequestration is not accounted for following amacher et al 2009 chapter 4 we have therefore estimated concave forest growth functions using data from eurostat forestry statistics which report growing stock and increment for commercial forests for each eu country and four years 1990 2000 2005 and 2010 this gives a panel data set with 28 countries and 4 years a quadratic function describing the relationship between growing stock per hectare and the associated gross increment is estimated for commercial forests we take into account differences in growth rates across regions with different climate using dummy variables for the colder boreal and drier mediterranean regions where growth can be expected to be lower than in central europe we test for random effects with a breusch and pagan lagrange multiplier test which shows that this hypothesis cannot be rejected however existence of contemporaneous correlation may exist among countries pesaran 2004 cross sectional dependence can occur in countries which are subjected to the same type of regulations such as the eu directives if our independent variables do not reflect these cross sectional dependencies the estimated standard errors will be affected a pesaran test gave a value of p 60 which indicates non existence of cross section dependence a test was also made for heteroscedasticity which showed no correlation in the errors the quadratic functions where therefore estimated with ordinary least square estimator data and results of estimations can be found in the appendix the estimated growth functions imply that forest growth in different countries varies depending on climatic region and initial growing stock the maximum forest growth occurs when average growing stock per hectare is 266 m3 countries in the dataset with a growing stock equal to 266 30 m3 have an average age of 55 years 12 12 calculated from data in the appendix and vilén et al 2012 note that with our growth functions this volume is reached earlier in central europe and later in the boreal and mediterranean regions and other studies suggest that the age of maximum sequestration should occur close to this age newell and stavins 2000 schulp et al 2008 already in the initial time period eight of the countries have a growing stock larger than 266 m3 hence for several countries increased average forest age could lead to reduced sequestration over time corresponding data are not available for non commercial forests which might in the future be used as commercial ones we therefore apply the estimated growth function to all forest land in each country the ratio of total forest volume and volume of the growing stock is calculated using the average biomass expansion factors for conifers and broadleaves ipcc 2003 to obtain the above ground tree volume thereby we get β i equal to 1 125 and 1 175 for boreal and temperate countries respectively based on data in trømborg and sjølie 2011 the co2 content per cubic meter of wood is assumed to be 0 8 tons 13 13 trømborg and sjølie 2011 report co2 content to be 0 7 0 92 depending on tree species to obtain parameter values for the soil carbon equation we make use of estimates of soil carbon stock and sequestration reported in liski et al 2002 their soil carbon stock estimates apply to the tree originating carbon in the organic soil plus the topmost 20 cm mineral soil layer national estimates for 1990 are available for 14 countries of those included in this study we adjust these estimates for the average stock change 1990 2010 in the region to which the country belongs north northwest central or south europe for the 13 remaining countries in our study we use the average stock for the corresponding region further we use annual sequestration estimates in liski et al 2002 for 1990 assuming they apply also in 2010 while for countries not included in their study we use the average for the region to which the country belongs we assume that 50 percent of the soil carbon is lost on forest land subject to final felling as suggested in early studies on the subject covington 1981 federer 1984 yanai et al 2003 later studies have shown that the magnitude of soil carbon loss can sometimes be much smaller even zero covington 1981 federer 1984 johnson and curtis 2001 yanai et al 2003 and that the harvesting method and the extent of site preparation are important for the magnitude of the losses jandl et al 2007 therefore a case with zero soil carbon losses due to final felling is investigated in the sensitivity analysis building on swedish data for 2010 swedish forest agency 2013 γ is calculated to be 0 6 which we assume to apply for all countries decay rates are calculated from the functions for decomposition rates for slow and fast humus presented in liski et al 2002 where decomposition is modeled as functions of annual mean temperature we use the average of the decomposition rates for fast and slow humus the rate of litter fall κ i is used to calibrate the functions such that the above mentioned sequestration is achieved in the initial year calibrated values then range from 0 0011 to 0 0334 which can be compared with liski et al 2002 where e g the rate of litter to growing stock is reported to be 0 0043 for coniferous forests and 0 0087 for deciduous trees the variation in obtained litter coefficients seems reasonable given that the impact of tree growth on soil carbon accumulation differs between tree species jandl et al 2007 production of bioenergy requires fossil fuel in the refinement process and this process is typically less energy efficient than for refining fossil fuels the carbon displacement is therefore typically less than one schlamadinger and marland 1996 judge that 0 6 is a reasonable estimate of the displacement for bioenergy given current technology and sathre and o connor 2010 argue based on several studies that the bioenergy displacement factor can range from less than 0 5 up to 1 0 depending on the type of fossil fuel replaced and their relative combustion efficiencies cannell 2003 estimates that biomass used to generate electricity displaces coal by a factor 1 0 oil by a factor 0 88 and natural gas by a factor 0 56 we here assume that displacement equal 0 75 implying that τ 0 25 the average lifetime of timber products i e k i is obtained from eggers 2002 the bau consumption and production levels are assumed equal to the levels in 2010 it is assumed that fossil fuel consumption and bioenergy and timber production can at most be reduced by 95 55 and 20 percent respectively compared to bau also it assumed that bioenergy and timber production can at the most be increased by 75 percent which is reasonable compared to short and long term increases in renewables discussed by the eu commission eucom 2012 2013 the eu emissions target is interpreted as a successive reduction of co2 emissions by 80 percent until 2050 this target is assumed to be tightened by the same percentage each year from 2010 to 2050 taking into account that 2010 emissions are eleven percent below those in the reference year 1990 eucom 2012 in principle a successive reduction of emissions is mainly motivated when capital investments are necessary to achieve carbon reductions such as is relevant for fossil fuel reductions however we do not explicitly model capital vintages for the fossil fuel sector instead we use the successive tightening of targets as a simple way of achieving a similar overall carbon reduction path this makes it possible to investigate the role that carbon sequestration can play for reducing abatement costs along that path a discount rate of 3 percent is applied as suggested by boardman et al 2011 to be an appropriate level for public undertakings 5 results we calculate results for five different policy scenarios shown in table 1 the first scenario all includes all abatement alternatives and their effects on carbon release and uptake the all scenario provides a benchmark and corresponds to a cost effective policy the second scenario fossil is one where emission targets have to be achieved by only reductions in fossil fuel consumption forest harvests are held constant over time and equal to bau levels implying that carbon pools in trees soil and forest products change the third scenario fpro adds the possibility of changing the use of forest products and accounts for their direct impact on emissions to the decision problem as defined by equation 7 but the consequential impact on sequestration is ignored in the fourth scenario bio impacts on sequestration and carbon pools in trees are further added as defined by equation 2 the fifth scenario soil is similar to the third but instead of sequestration and pools in trees soil sequestration and soil carbon pools are included the scenarios thus permit comparison of costs and emission impacts under different assumptions about the number of carbon pools that are taken into account in the policy decision thereby it becomes possible to investigate whether a simplified policy where only a single carbon pool is included is an improvement compared to not including sequestration at all when carrying out the optimization only sequestration in addition to that with bau harvests is considered to contribute to the climate targets for tractability the model is aggregated into 5 year time periods the model is run for 20 years beyond 2050 requiring that emissions then remain constant and equal to those in 2050 in order to avoid end of period effects 5 1 the minimum cost the discounted minimum cost of meeting the eu roadmap targets is shown in fig 1 for each scenario as seen inclusion of more abatement options reduces the cost of meeting the targets the cost in the fossil scenario corresponds to approximately 0 4 percent of aggregate gdp which is within the range of costs estimated in capros et al 2014 14 14 capros et al 2014 compare three large scale energy economy models with regard to the least cost strategy for meeting 2050 targets and conclude such a strategy can reduce gdp by 0 0 0 5 different to this study their calculations include also the non trading sector which is larger than the trading sector and abatement costs are higher see e g böhringer et al 2009 the total cost in the all scenario is 33 percent lower than in the fossil scenario this can be compared to michetti and rosa 2012 and gren et al 2012 where cost savings from carbon sequestration are estimated to be 30 percent and 65 percent respectively comparing approaches our analysis is applied to the more demanding 2050 target compared to the 2020 target analyzed in michetti and rosa 2012 and gren et al 2012 this could imply a tendency towards smaller cost savings if the potential for increased sequestration is limited moreover the two mentioned studies allow for afforestation and gren et al 2012 include the non trading sector 15 15 the non trading sector has higher abatement costs so crediting carbon sequestration against targets for the non trading sector implies large cost savings which works in the same direction on the other hand our inclusion of soil sequestration should increase the cost savings compared to these studies 5 2 emission reductions achieved in the all scenario the optimization problem captures all carbon effects from the measures being undertaken in other scenarios only some of the effects are taken into account if we wish to evaluate second best policy scenarios all impacts on carbon emissions must be added to identify the total impact on carbon emissions in fig 2 we therefore compare the total emission reductions in the different scenarios in the fossil scenario a considerable amount of sequestration occurs as a consequence of bau forest management the total reduction in the figure is then the sum of reductions due to reduced fossil fuel consumption and bau sequestration in all other scenarios bioenergy and timber are both reduced in the cost efficient solutions in the fpro scenario bioenergy and timber are reduced in order to avoid emissions from combustion but the associated impact on sequestration is not taken into account however the reduction of forest products implies that sequestration is increased as a consequence the total emission reduction is larger than in the fossil scenario in the bio scenario the policy maker accounts for the effects of both reduced forest products and increased sequestration therefore fossil fuel reductions are smaller compared to the fpro scenario this saves costs but also implies that the total emission reductions are smaller than under fpro the total emission reduction is the highest in the soil scenario because increased growing stocks are required to increase soil sequestration the scenario is more expensive than the bio scenario because sequestration in trees is cheaper than soil sequestration as carbon sequestered in soils is partially lost through decay and at the time of harvesting the all scenario exactly achieves the emission targets implying the lowest cost as well as the lowest emission reductions compared to the fossil scenario the fpro and soil scenarios imply a small or modest reduction in costs but increases the total emission reductions the soil scenario outperforms the fpro scenario with respect to both emissions and costs the bio and all scenarios significantly reduce costs but have a minor positive and a negative impact respectively on emission reductions achieved hence the preferred policy depends on the value that policy makers attach to emission reductions in excess of the targets defined in the roadmap 5 3 the fossil fuel carbon tax carbon taxes on fossil fuels bioenergy and timber can be applied to meet the climate targets for the eu as shown in the model section fig 3 displays the cost efficient co2 tax on fossil fuels in the fossil and all scenarios the co2 tax increases over time when the emission target becomes more stringent and is higher in the fossil scenario than in the all scenario in the all scenario carbon taxes increase from 20 per tco2 in 2010 to 55 in 2050 the 2010 tax level can be compared with the actual carbon price which ranged between 8 and 29 per tco2 between 2008 and 2010 chen et al 2013 our estimated carbon tax levels can also be compared with permit prices 2020 calculated by böhringer et al 2009 from three different cge models they estimate that the permit price for the eu ets will be 50 75 tco2 in the absence of additional efforts to promote renewables our comparable estimate in the fossil scenario 42 per tco2 in 2020 is below their estimated price interval which can be explained by the fact that we do not take into account economy wide dispersal effects furthermore capros et al 2014 estimate that carbon prices will reach 243 565 in 2050 our result in the fossil scenario is only 108 which is likely to be explained by their inclusion of the non trading sector where abatement costs are higher than in the trading sector michetti and rosa 2012 estimate that the carbon price is reduced by 30 percent in 2020 when forest sequestration is included for that time period we obtain almost exactly the same price reduction 5 4 taxes on timber timber production is reduced in all countries seven of the 27 countries reduce timber production down to the lower bound t i during the whole time period 16 16 spain finland greece sweden ireland portugal and cyprus the high reductions in these countries are explained by a high potential for increased forest growth and hence increased sequestration in forests and soils and comparatively low costs for reducing timber production for 18 countries the lower bound is not binding at any point in time during the policy period considered 17 17 two countries estonia and latvia have a binding lower bound in the two or three last time periods we illustrate the results on efficient timber taxes by comparing three countries where the lower bound is not binding during the time period germany italy and the uk these countries differ with respect to initial growing stock and forest growth and soil carbon pools and sequestration see table 2 the growing stock per hectare is the largest in germany almost three times higher than the european average whereas that in italy and uk is close to the european average forest growth is high in germany due to the favorable climate and large growing stock compared to italy and the uk the initial soil carbon stock is large in germany moderate in the uk and small in italy and due to the higher rate of litter fall soil sequestration is higher in germany and the uk compared to italy sequestration in trees increases in italy and the uk until the fifth and seventh time period respectively and then falls in germany sequestration in trees falls over the whole policy period in spite of this the efficient tax on timber increases over time for all three countries in both scenarios except for a small decline in the last period in the uk in the all scenario see fig 4 the simultaneous increase in the tax and decrease in sequestration is explained by relatively low costs for reducing timber production and hence sequestering additional carbon in the same year in order to meet that year s environmental target compared to the disadvantage of having the future potential for sequestration reduced in both the uk and germany the timber taxes are higher in the all scenario than in the bio scenario during the whole policy period this is explained by the large positive impact of increased forest volume on soil carbon sequestration due to high litter fall in these countries an effect which is accounted for in the all scenario but not in the bio scenario the highest rate of litter fall is found in the uk and hence also the highest difference in the tax between the two scenarios in contrast the timber tax in italy is similar in both scenarios due to the small rate of litter fall and hence small soil sequestration the development of the tax over time in the three countries is more similar in the all scenario the tax increases between 1 3 and 2 6 times over the whole time period the rate of increase varies more in the bio scenario between 1 9 and 8 8 times in both scenarios the highest increase occurs in germany and the lowest in the uk the high increase in the german tax in the bio scenario is a consequence of the low cost efficiency of timber reductions in early time periods as the large growing stock implies that there is no potential for increased forest growth while in later time periods reductions become cost efficient in order to meet the stringent carbon targets in the shorter run in both scenarios the timber tax is the highest in the uk this is explained by a relatively high potential for increased forest growth in combination with high litter fall and soil sequestration in the all scenario germany has the lowest timber tax over the first half of the policy period and italy over the latter after half of the policy period forest growth in italy has become small and the rate of litter fall is not large enough to motivate further increases in the growing stock in germany forest growth falls over the whole time period but the successive growth of forest volume implies increased amounts of litter and hence increased soil sequestration which explains this outcome as seen in fig 4 cost efficient timber taxes differ substantially between countries suggesting that uniform timber taxes at the eu level would not be optimal in particular in the shorter run towards the end of the policy period taxes tend to converge as the growing stock and soil carbon stocks saturate and become more similar across countries corresponding taxes on bioenergy follow a similar increasing pattern over time as those on timber the level of the tax is lower because of the earlier release of the carbon as predicted in the theoretical section 5 5 sensitivity analysis in the sensitivity analysis we first investigate the impact of assumptions about forest growth as changed growth is a possible consequence of climate change increased growth is a likely consequence of higher temperatures in north and east europe whereas decreased growth due to drier climate can be expected in the mediterranean area lindner et al 2010 here we assume that forest growth increases by 10 percent in all countries and for all time periods except in the mediterranean countries where instead it decreases by the same rate second we analyze the role of assumptions made about soil carbon sequestration given differing conclusions in the literature regarding the magnitude of soil carbon losses at the time of harvesting here we therefore recalibrate the soil carbon stock function assuming that losses from harvesting are zero the recalibration implies that the litter coefficient which is the calibrated parameter is reduced to be compatible with the same sequestration given the smaller soil carbon losses results from the sensitivity analysis calculated for the all scenario are shown in table 3 with changed forest growth aggregate sequestration increases and hence costs fall the relative impact on tree and soil sequestration is similar larger forest growth on the aggregate level implies that there is less need for policy efforts towards fossil fuels and sequestration in order to meet targets hence timber taxes fall in most countries such as is the case for germany and the uk however for a few countries italy and france taxes increase because the additional growth increases the value of leaving the wood in the forest as this implies that significantly more sequestration can be achieved in the country during the policy period at a given cost it can be noted that the impact on timber taxes in a given country is not proportional to the size of the assumed change in forest growth but is determined by the country s forest growth function and costs of timber reductions when the soil stock function is calibrated for zero losses from final felling the net present cost increases and soil sequestration falls while sequestration in trees is almost unaffected timber taxes are increased in all countries because further efforts are made to increase biomass sequestration to compensate for the lower soil sequestration in this scenario fossil fuel taxes are increased even further in the first period by about 60 per cent 6 discussion and conclusions the aim of this paper is to evaluate policy instruments applied to forest products as a means to achieve carbon sequestration in a cost efficient manner we also compare the economic and environmental consequences of separate inclusion of one carbon pool in the eu s climate policy to that of including three interdependent pools the theoretical analysis shows that biomass soil and forest product pools could be included in a policy by means of differentiated taxes on bioenergy and timber the optimal taxes account for the impact of direct emissions from bioenergy and timber displacement of fossil fuels and the consequences for current and future sequestration in trees and in soils a numerical model is developed which includes cost functions for fossil fuels and forest products and functions which describe the development of forest biomass and soil carbon stocks our analysis suggests that the cost efficient taxes on forest products could be relatively high in countries with large potential for increased sequestration over the policy period given the wide variation in tree and soil carbon stocks and forest growth the efficient level and time path of subsidies and taxes varies substantially across countries therefore a system with uniform eu wide taxes on forest products is not a cost efficient policy over the next decades instead a coordinated approach with differentiated tax levels could work better uniform eu wide policy instruments for forest products might be justified on cost efficiency grounds closer to the target year 2050 provided that national policies to increase sequestration are introduced within the near future the model is used to analyze separate and combined inclusion of different carbon pools compared to a policy where fossil fuel reductions are the only means to meet the eu roadmap targets different to most earlier studies on dynamic forest sequestration applied at small updegraff et al 2010 van kooten et al 1995 wise and cacho 2005 and large e g latta et al 2011 lecocq et al 2011 sohngen and sedjo 2006 moiseyev et al 2011 spatial scale we treat the carbon price as endogenous and show empirically that inclusion of forest carbon sequestration at eu scale can lead to a considerable reduction in carbon price when climate targets are ambitious further lop sided consideration of fossil fuel displacement resulting from the use of bioenergy and timber can be detrimental as increased forest harvests over time reduces the potential for sequestration our results show that successively increased reductions in harvest can be a cost efficient ways to meet carbon targets in this regard our results support conclusions drawn in eriksson 2015 and münnich vass and elofsson 2016 which suggest that sequestration in forest biomass is a cheaper abatement method than bioenergy at global and eu scales respectively similar conclusions are drawn by schulze et al 2012 when evaluating the consequences for greenhouse gases emissions from an increase of the share of forest biomass in global primary energy supply it can be noted that the eu s policy against greenhouse gas emissions is focused on co2 emission trading for fossil fuel use in combination with a target to have 20 percent renewable energy by 2020 our results suggest that the latter can have a negative effect on overall carbon sequestration if bioenergy and used timber make up a large share of the renewable energy and the consequences for sequestration are ignored the results further suggest that a separate inclusion of either biomass or soil carbon pools to the eu s climate policy will lead to improvements compared to both a strictly fossil fuel based policy and a policy which combines fossil fuel reductions with efforts to increase carbon displacement by bioenergy and timber however the improvement will be different in nature inclusion of biomass sequestration will reduce costs more while inclusion of soil sequestration lead to further emissions reductions thus the choice of which single pool to include in the policy is determined by the policy makers valuation of emission reductions in excess of the target the above analysis has limitations including the partial approach and the exclusion of land use change the non trading sector uncertainty and heterogeneity in carbon sequestration within each country forest growth functions are aggregated at national level which is a simplification if they were replaced by growth functions that were disaggregated over space and age classes this would affect the level of taxes but not conclusions regarding the potential for uniform taxation of bioenergy and timber or the consequences of separate inclusion of a single carbon pool also important to the interpretation of results is the exclusion of transaction costs for alternative policy instruments for sequestration nevertheless our analyses and results can contribute to the ongoing debate on carbon pools in the eu which mainly concerns the rules for reporting of carbon pools kuikman et al 2011 the eu commission has recently introduced harmonized rules for carbon accounting implying that national reports should capture all relevant effects from land use land management and harvested wood products eu 2013 this can be a step towards policies promoting forest carbon sequestration our results can then serve as an input in the discussion since they point out principles for determining cost efficient policy instruments targeting forest products under different scenarios it seems likely that additionality and permanence in carbon enhancement in these products is easier and hence cheaper to measure monitor and verify compared to policies targeting carbon pools at stand level also compared to a system with a combination of subsidies and taxes on carbon pools at stand level as suggested by e g van kooten et al 1995 and latta et al 2011 our system implies that fewer and in total smaller financial transactions take place which could facilitate implementation by reducing transaction costs on the other hand policies directed towards products only regulate carbon pools indirectly which is a relative disadvantage as sequestration heterogeneity within each country is ignored 7 software availability the gams code used for the paper is available as supplementary material on the journal homepage running of the code requires installation of the software gams 23 9 5 or higher and the conopt2 solver available at https www gams com download the code developed for the study is not copy protected and can be distributed freely acknowledgements we are very grateful for the helpful comments provided by the anonymous referee of this journal this work was supported by the swedish energy agency grant number 35444 1 appendix the lagrangian and the necessary first order conditions the dynamic discrete time lagrangian is a1 l t i ρ t c t b i b b a u i b t i c t t i t b a u i t t i j c t x i j x b a u i j x t i j ρ μ t 1 v i v t i g t i v t i b t 1 i t t 1 i a i v t 1 i ρ μ t 1 p i p t i ν p t i γ b t 1 i t t 1 i v t i g t i v t i a i κ i β i v t i ϑ i p t i p t 1 i λ t e t m a x e t where ρ 1 1 r is the discount factor and r is the discount rate λ t 0 is the shadow cost for the emission constraint at time t μ t 1 v i 0 and μ t 1 p i 0 are the shadow costs of forest biomass and soil carbon stock the necessary conditions for an interior solution are a2 ρ t l x t i j c t i j x b a u i j x t i j x t i j λ t α j 0 a3 ρ t l b t i c t b i b b a u i b t i b t i μ t v i 1 a t i μ t p i ν γ p t 1 i v t 1 i g t 1 i v t 1 i a t i λ t τ η 0 a4 ρ t l t t i c t t i t b a u i t t i t t i μ t v i 1 a t i μ t p i ν γ p t 1 i v t 1 i g t 1 i v t 1 i a t i ρ k i 1 λ t k i τ η 0 a5 ρ t l v t i ρ μ t 1 v i 1 g t i v t i μ t v i ρ μ t 1 p i ν p t i γ h t 1 i 1 g t i v t i v t i g t i v t i 2 κ β i λ t η β i a i ρ λ t 1 η β i a i 0 a6 ρ t l p t i ρ μ t 1 p i 1 ϑ i ν γ h t 1 i v t i g t i v t i μ t p i λ t a i ρ λ t 1 a i 0 table a1 forest area growth fellings forest products and prices table a1 total forest and other wooded land area a growing stock a gross increment a fellings a use of domestic forest a prices b price elasticity c bio energy oth forest prod bio energy otherforestprod bio energy oth forest prod 1000 ha m3 ha m3 ha m3 ha meur 1000 m3 meur 1000 m3 eu 27 177003 137 5 8 3 2 21 79 at 3991 286 7 5 5 3 26 74 0 0227 0 0697 0 55 0 21 be 706 238 7 9 7 2 15 85 0 0227 0 0728 0 55 0 21 bg 3927 167 5 1 2 0 47 53 0 0227 0 0742 0 55 0 21 cy 387 27 0 9 0 2 41 59 0 0227 0 0768 0 55 0 21 cz 2657 290 9 9 7 2 12 88 0 0227 0 0708 0 55 0 21 de 11076 315 10 1 5 1 18 82 0 0227 0 0723 0 55 0 21 dk 635 180 10 0 4 6 40 60 0 0227 0 0767 0 55 0 21 ee 2337 191 5 6 3 6 27 73 0 016 0 0473 0 55 0 21 es 28214 32 3 1 1 1 32 68 0 0227 0 0665 0 55 0 21 fi 23116 96 4 6 2 6 10 90 0 0235 0 0503 0 55 0 21 fr 17572 148 6 2 3 7 47 53 0 0227 0 0733 0 55 0 21 gr 6539 31 1 3 0 3 68 32 0 0227 0 0768 0 55 0 21 hu 2039 174 6 4 3 3 52 48 0 0227 0 0768 0 55 0 21 ie 788 95 9 8 5 7 7 93 0 0227 0 0768 0 55 0 21 it 10916 133 4 0 1 0 66 34 0 0227 0 0743 0 55 0 21 lt 2249 214 5 7 3 8 27 73 0 0188 0 0453 0 55 0 21 lu 88 295 7 5 3 2 6 94 0 0227 0 0768 0 55 0 21 lv 3467 183 5 8 4 0 18 82 0 016 0 0505 0 55 0 21 mt 0 0 0 0 0 0 0 0227 0 0768 0 55 0 21 nl 365 192 7 6 3 7 27 73 0 0227 0 0723 0 55 0 21 pl 9319 247 8 0 4 2 12 88 0 016 0 0487 0 55 0 21 pt 3611 52 10 5 5 3 6 94 0 0227 0 0594 0 55 0 21 ro 6733 207 6 5 2 5 20 80 0 0227 0 0768 0 55 0 21 se 30625 106 4 7 3 5 8 92 0 0235 0 0518 0 55 0 21 sk 1938 265 7 4 5 4 5 95 0 0227 0 0687 0 55 0 21 si 1274 327 7 8 2 5 37 63 0 0227 0 0751 0 55 0 21 uk 2901 131 8 6 4 0 14 86 0 0227 0 0746 0 55 0 21 a all forest data are for 2010 and have been obtained from eurostat 2012 b the price of other forest products is the weighted average price of logs and pulp in 2010 in finnish forest research institute 2011 where prices are available for austria estonia lithuania and sweden those were extrapolated to the other countries as shown in the table no official price statistics for bioenergy are available here the price of bioenergy is assumed to be 2 3 of the pulp price c price elasticities are obtained from geijer et al 2011 due to lack of elasticity estimated across the eu countries the same elasticity is assumed for all countries table a2 soil carbon stock and sequestration table a2 soil c stock a soil csequestration a conversion factor b γ litter coeff c κ i decomposition rate a ϑ i harvest to volume ratio d harvest impact coeff υ ton co2 ha ton co2 ha volume to area harvested at 220 0 183 0 6 0 0053 4 94e 13 0 019 0 5 be 139 0 213 0 6 0 0066 6 07e 13 0 03 0 5 bg 220 0 183 0 6 0 0065 4 94e 13 0 012 0 5 cy 48 0 029 0 6 0 0054 7 80e 13 0 007 0 5 cz 220 0 183 0 6 0 0067 4 94e 13 0 025 0 5 de 220 0 183 0 6 0 0043 5 43e 13 0 016 0 5 dk 180 0 084 0 6 0 0087 4 98e 13 0 026 0 5 ee 180 0 084 0 6 0 0061 4 98e 13 0 019 0 5 es 48 0 029 0 6 0 0174 9 24e 13 0 034 0 5 fi 180 0 084 0 6 0 0179 2 46e 13 0 027 0 5 fr 220 0 183 0 6 0 0135 6 53e 13 0 025 0 5 gr 48 0 029 0 6 0 0058 9 16e 13 0 01 0 5 hu 220 0 183 0 6 0 0093 4 94e 13 0 019 0 5 ie 139 0 213 0 6 0 0334 5 80e 13 0 06 0 5 it 48 0 029 0 6 0 0011 7 80e 13 0 008 0 5 lt 180 0 084 0 6 0 0053 4 98e 13 0 018 0 5 lu 139 0 213 0 6 0 0024 6 07e 13 0 011 0 5 lv 180 0 084 0 6 0 0080 4 98e 13 0 022 0 5 mt 48 0 029 0 6 na 7 80e 13 na 0 5 nl 139 0 213 0 6 0 0056 5 90e 13 0 019 0 5 pl 220 0 183 0 6 0 0057 5 43e 13 0 017 0 5 pt 48 0 029 0 6 0 0305 9 39e 13 0 102 0 5 ro 220 0 183 0 6 0 0050 4 94e 13 0 012 0 5 se 180 0 084 0 6 0 0196 2 95e 13 0 033 0 5 sk 220 0 183 0 6 0 0060 4 94e 13 0 02 0 5 si 220 0 183 0 6 0 0023 4 94e 13 0 008 0 5 uk 139 0 213 0 6 0 0123 5 43e 13 0 031 0 5 a own calculation based on liski et al 2002 b own calculation based on data from swedish forest agency 2013 c from calibration of model d calculated from data in table a1 table a3 coefficients of cost functions for fossil fuel consumption in the eu ets sector cost functions are quadratic c t x i j a b x t i j c x t i j 2 na zero consumption the fuel type in the country is not included in the calculations sources method and data for calculation of cost function is obtained from gren et al 2009 table a3 hard coal derivatives lignite derivatives natural derived gases a b c a b c a b c at 540 92 0 407 7 67e 05 30 75 0 407 1 35e 03 2434 40 0 978 9 83e 05 be 588 68 0 418 7 40e 05 31 73 0 418 1 37e 03 4160 74 0 946 5 38e 05 bg 310 65 0 283 6 45e 05 591 72 0 283 3 38e 05 346 55 0 345 8 58e 05 cy na na na na na na na na na cz 665 10 0 341 4 37e 05 2240 70 0 341 1 30e 05 1530 38 0 769 9 67e 05 de 5264 33 0 274 3 56e 06 5128 95 0 274 3 65e 06 19623 20 1 086 1 50e 05 dk 671 32 0 246 2 25e 05 na na na 1150 33 0 933 1 89e 04 ee 3 68 0 283 5 44e 03 312 21 0 283 6 42e 05 63 09 0 224 1 99e 04 es 2170 72 0 278 8 92e 06 188 85 0 278 1 03e 04 7472 50 0 648 1 41e 05 fi 646 98 0 288 3 20e 05 318 31 0 288 6 51e 05 891 63 0 422 4 99e 05 fr 2227 42 0 451 2 29e 05 na na na 7775 83 1 167 4 38e 05 gr 38 38 0 269 4 73e 04 1088 60 0 269 1 67e 05 415 61 0 382 8 77e 05 hu 102 04 0 283 1 96e 04 241 58 0 283 8 29e 05 1285 97 0 534 5 55e 05 ie 171 99 0 249 9 03e 05 54 34 0 249 2 86e 04 1071 00 0 792 1 47e 04 it 2844 17 0 398 1 39e 05 0 40 0 398 9 94e 02 17744 78 0 919 1 19e 05 lt 19 25 0 283 1 04e 03 0 42 0 283 4 72e 02 228 82 0 328 1 17e 04 lu 22 34 0 418 1 95e 03 0 63 0 418 6 96e 02 322 16 0 865 5 80e 04 lv 5 09 0 283 3 93e 03 0 14 0 283 1 42e 01 141 93 0 276 1 34e 04 mt na na na na na na na na na nl 1075 96 0 341 2 70e 05 na na na 9892 73 1 212 3 71e 05 pl 4558 81 0 293 4 71e 06 1842 71 0 293 1 16e 05 2132 00 0 684 5 49e 05 pt 483 71 0 293 4 44e 05 na na na 1034 28 0 720 1 25e 04 ro 226 02 0 283 8 86e 05 983 32 0 283 2 04e 05 1611 72 0 414 2 66e 05 se 348 31 0 477 1 63e 04 57 97 0 477 9 82e 04 587 75 1 078 4 95e 04 sk 39 91 0 283 5 02e 04 175 35 0 283 1 14e 04 137 97 0 569 5 87e 04 si 336 12 0 283 5 96e 05 105 29 0 283 1 90e 04 624 05 0 517 1 07e 04 uk 5255 75 0 288 3 94e 06 na na na 20854 64 1 219 1 78e 05 light fuel oil heating oil heavy fuel oil jet fuel a b c a b c a b c at 442 04 1 713 1 66e 03 219 33 0 778 6 90e 04 3172 50 9 000 6 38e 03 be 299 68 0 937 7 32e 04 260 00 0 444 1 90e 04 5305 50 9 000 3 82e 03 bg 113 10 0 650 9 34e 04 100 36 0 388 3 74e 04 918 00 9 000 2 21e 02 cy 5 49 1 569 1 12e 01 38 90 0 608 2 37e 03 1386 00 9 000 1 46e 02 cz 96 25 1 100 3 14e 03 61 60 0 467 8 84e 04 1575 00 9 000 1 29e 02 de 3412 41 1 103 8 92e 05 1060 86 0 517 6 30e 05 39343 50 9 000 5 15e 04 dk 512 23 2 277 2 53e 03 192 43 1 447 2 72e 03 4135 50 9 000 4 90e 03 ee 4 71 0 725 2 79e 02 na na na 144 00 9 000 1 41e 01 es 3422 58 1 422 1 48e 04 2218 77 0 822 7 62e 05 25105 50 9 000 8 07e 04 fi 506 79 1 190 6 98e 04 406 39 0 810 4 04e 04 2767 50 9 000 7 32e 03 fr 2151 73 1 288 1 93e 04 1815 95 0 635 5 54e 05 31837 50 9 000 6 36e 04 gr 1148 17 1 627 5 77e 04 532 59 0 706 2 34e 04 5827 50 9 000 3 47e 03 hu 158 49 1 338 2 82e 03 69 80 0 400 5 73e 04 1224 00 9 000 1 65e 02 ie 152 29 1 259 2 60e 03 238 97 0 724 5 49e 04 3915 00 9 000 5 17e 03 it 4883 86 2 636 3 56e 04 2586 16 0 864 7 21e 05 17914 50 9 000 1 13e 03 lt 110 95 0 700 1 10e 03 48 05 0 388 7 81e 04 238 50 9 000 8 49e 02 lu 3 93 0 873 4 85e 02 na na na 1822 50 9 000 1 11e 02 lv 2 44 0 813 6 77e 02 7 94 0 388 4 73e 03 301 50 9 000 6 72e 02 mt 9 01 1 386 5 33e 02 na na na 346 50 9 000 5 84e 02 nl 5459 76 2 190 2 20e 04 244 17 0 833 7 11e 04 16663 50 9 000 1 22e 03 pl 857 09 1 512 6 67e 04 443 77 0 674 2 56e 04 1930 50 9 000 1 05e 02 pt 94 14 1 207 3 87e 03 429 66 0 690 2 77e 04 4158 00 9 000 4 87e 03 ro 318 83 0 650 3 31e 04 156 74 0 388 2 39e 04 625 50 9 000 3 24e 02 se 1126 37 1 911 8 10e 04 563 57 1 429 9 05e 04 3915 00 9 000 5 17e 03 sk 14 80 0 800 1 08e 02 16 86 0 475 3 35e 03 193 50 9 000 1 05e 01 si 202 30 0 850 8 93e 04 49 28 0 338 5 78e 04 117 00 9 000 1 73e 01 uk 2577 09 1 513 2 22e 04 1387 03 1 103 2 19e 04 58464 00 9 000 3 46e 04 table a4 coefficients of cost functions for forest products cost functions are quadratic timber c t t i a b t t i c t t i 2 bioenergy c t b i a b b t i c b t i 2 na zero production and the product in the country is not included in the calculations sources data on production volume prices and price elasticities are found in table a1 table a4 timber bioenergy a b c a b c at 2595 78 0 3317 3 75e 06 113 33 0 0412 1 06e 05 be 748 88 0 3466 2 70e 05 15 71 0 0412 4 01e 05 bg 735 28 0 3533 5 58e 06 76 06 0 0412 4 24e 05 cy 8 35 0 3657 6 49e 04 0 65 0 0412 4 00e 03 cz 2837 55 0 3371 8 98e 06 47 3 0 0412 1 00e 05 de 7970 52 0 3442 2 03e 06 209 52 0 0412 3 71e 06 dk 319 96 0 3651 1 76e 05 24 08 0 0412 1 04e 04 ee 692 29 0 2254 6 40e 06 33 04 0 0291 1 84e 05 es 3339 49 0 3165 2 07e 06 204 65 0 0412 7 50e 06 fi 6476 8 0 2395 3 56e 06 128 45 0 0427 2 21e 06 fr 6012 17 0 3489 6 74e 07 629 67 0 0412 5 06e 06 gr 114 79 0 3657 1 54e 05 27 49 0 0412 2 91e 04 hu 590 59 0 3657 5 89e 06 72 1 0 0412 5 66e 05 ie 725 26 0 3655 6 90e 05 6 15 0 0412 4 61e 05 it 656 94 0 3540 2 86e 06 148 46 0 0412 4 77e 05 lt 672 89 0 2157 7 41e 06 39 44 0 0342 1 73e 05 lu 48 4 0 3657 1 22e 03 0 35 0 0412 6 91e 04 lv 1367 32 0 2405 5 83e 06 36 31 0 0291 1 06e 05 mt na na na na na na nl 169 65 0 3442 5 65e 05 7 51 0 0412 1 75e 04 pl 3995 99 0 2320 3 10e 06 68 32 0 0291 3 37e 06 pt 1271 28 0 2827 3 59e 05 11 83 0 0412 1 57e 05 ro 2462 35 0 3657 6 12e 06 69 37 0 0412 1 36e 05 se 12163 56 0 2467 2 49e 06 183 27 0 0427 1 25e 06 sk 328 37 0 3273 1 75e 05 24 28 0 0412 8 16e 05 si 1777 56 0 3576 3 94e 05 10 78 0 0412 1 80e 05 uk 1772 73 0 3553 1 27e 05 33 48 0 0412 1 78e 05 table a5 summary statistics table a5 mean min max stdev obs g i forest growth m3 ha 6 6 0 9 12 2 0 2 109 v t i forest volume m3 ha 211 1 47 6 459 2 7 9 109 v t i 2 51373 2 2265 0 210846 8 3542 5 109 s c i dummy for se fi 0 1 0 1 0 025 109 m e i dummy for pt es fr it gr si 0 2 0 1 0 040 109 table a6 ols estimation dependent variable g t i forest growth in m3 ha table a6 variable parameterestimate standarderror t value pr t v t i 0 05881 0 00460 12 80 0001 v t i 2 0 00011044 0 00001591 6 94 0001 s c i 1 32664 0 93064 1 43 0 1570 m e i 0 96340 0 57831 1 67 0 0987 prob f 0 000 appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 006 
26436,the purpose of this paper is to investigate cost effective climate policy instruments for bioenergy and timber adapted to the impacts on interdependent forest carbon pools and applied in the eu climate policy to 2050 we develop a discrete time dynamic model including forest carbon pools in biomass soil and products as well as fossil fuel consumption the analytical results show that the optimal taxes on forest products depend on the growth in the respective carbon pool the application to the eu 2050 climate policy for emission trading shows that total costs for target achievement can be reduced by 33 percent if all carbon pools are included and the carbon tax on fossil fuel can be reduced by 50 percent optimal taxes on forest products differ among countries and over time depending on the potential for increased carbon sequestration over the planning period keywords eu climate policy carbon sequestration bioenergy timber policy instruments jel classification q23 q28 q48 q54 1 introduction terrestrial carbon pools 1 1 we use the ipcc 2003 definitions as presented by fao 2014 where carbon pool refers to carbon reservoirs with the capacity to accumulate or release carbon carbon stock to the amount of carbon in the pools at a specific point of time sequestration as the process of increasing the carbon content in the pools and carbon sink as a process for removing carbon content from the atmosphere have received attention for their climate change mitigation potential and the comparatively low associated costs increased carbon pools in natural ecosystems could thus be an alternative and complement to other measures such as reduced fossil fuel use and increases in renewable energy bosetti et al 2009 murray et al 2009 sohngen 2009 it can be costly to ignore forest carbon flows and stocks when developing strategies against climate change in europe the sequestration of carbon in forest biomass and soils corresponds to 8 10 percent of the total emissions kuikman et al 2011 lal 2005 and sequestration tends to increase over time kauppi et al 1992 liski et al 2002 consideration of the risk for future carbon losses and the potential for targeted increases in carbon sequestration could thus be of importance for economic and environmental reasons within the european union eu crediting of increases in natural carbon pools against the co2 burden allocation is not allowed in spite of the substantial cost savings it could entail gren et al 2012 michetti and rosa 2012 münnich vass and elofsson 2016 arguments against the introduction of policies to enhance carbon sinks in the eu include the complexity and mutual interdependence of forest carbon pools and the difficulties of designing appropriate incentive structures kuikman et al 2011 forest carbon consists of two main natural pools above ground carbon in the biomass and below ground carbon in the soil lal 2005 forest harvesting decisions affect the stock of carbon in growing biomass but also indirectly influence the stock of soil carbon jandl et al 2007 kuikman et al 2011 lal 2005 neglect of this dependency will lead to false conclusions about the impact of forest management on total forest carbon sequestration the dependency between forest carbon pools further aggravates policy design for carbon sink enhancement even if only a single carbon pool is considered there are challenges concerning monitoring of carbon stock changes and verification of the additionality and permanence of such changes cf bento et al 2015 engel et al 2015 mason and plantinga 2013 much of the literature on policy instruments for carbon sequestration deals with instruments directed towards individual forest owners which require measurement and monitoring of changes in each forest owner s carbon pool guthrie and kumareswaran 2009 latta et al 2011 lecocq et al 2011 updegraff et al 2010 van kooten et al 1995 policies targeting forest products could then have an advantage because of the comparatively lower costs to measure and monitor these products hoel and sletten 2016 additionality can still be a concern but the issue would be reduced to evaluation of the aggregate additionality rather than the additionality of sequestration achieved by each forest owner the purpose of this paper is to analyze the design of policies on forest products to enhance carbon sequestration in interdependent carbon pools as a complement to reductions in emissions from fossil fuels the analysis is applied to carbon sequestration in forest biomass and soils and carbon storage in forest products in the eu climate policy from 2010 to 2050 within the eu inclusion of a single carbon pool can be seen as a feasible alternative if for example there is disagreement about the advantages of including several carbon pools we therefore compare separate and complete inclusion of biomass soil and forest product pools in the policy decision in order to assess whether separate inclusion is a step in the right direction or even counterproductive in addition we investigate the cost efficient economic incentives for achieving increased carbon sequestration this is done with an aim to evaluate the potential for common policy instruments at the eu level to promote carbon sinks for these purposes we construct a discrete dynamic model for cost efficient attainment of future targets on carbon emission as suggested by the eu 2050 climate policy eucom 2012 by means of reduced combustion of fossil fuels and forest products and enhanced carbon sequestration the interlinked carbon pools are managed by taxes targeting timber and bioenergy which differ with respect to the displacement of fossil fuel it is shown analytically that the cost efficient carbon taxes on timber and bioenergy can either increase or decrease when both biomass and soil pools are considered instead of only one of these pools the direction of impact depends on the effect of harvesting on the growth rate in the respective pool it is also shown that the tax on timber decreases for a delayed combustion of wood products because of the larger discounting of future costs of carbon emissions the empirical results show that inclusion of carbon sequestration reduces overall costs for reaching eu 2050 climate targets by 33 percent and the optimal carbon tax on fossil fuel by up to 50 percent if only a single carbon pool is included the choice of pool to include matters not only for the cost savings achieved but also for the net impact on carbon emissions the optimal tax on fossil fuels is increasing over time for all countries but the carbon tax on wood products can either increase or decrease depending on the forest growth rate and the time path of reduction targets our study belongs to two main strands of the literature economics of carbon regulation by forest management and design of policy instruments for carbon sink enhancement several earlier economic studies on forest management include more than one forest carbon pool in the analysis such as lubowski et al 2006 newell and stavins 2000 sohngen and mendelsohn 2003 van kooten et al 1999 and wise and cacho 2005 however we have not found any study which compares a second best policy including only a single carbon pool with the first best policy where several interlinked pools are included a number of studies analyze policy instruments applied to a single biomass carbon pool using a national forest sector model caurla et al 2013 and lecocq et al 2011 compare the impact of alternative combinations of climate policy instruments on the forest sector and resources van kooten et al 1995 show that a combination of carbon taxes and subsidies can be used to achieve socially optimal forest rotation and latta et al 2011 investigate the consequences of a tax subsidy scheme voluntary or mandatory in a forest sector model with respect to the literature on policies for carbon sink enhancement mason and plantinga 2013 conclude that a uniform carbon subsidy scheme implies higher costs for achieving sequestration than a contract design system bento et al 2015 analyze the role of the additionality problem and monitoring costs for the design of carbon offset contracts using a real options model with uncertain future timber prices guthrie and kumareswaran 2009 compare subsidies paid in proportion to the actual amount of carbon sequestered to credits that are allocated according to the long run potential to sequester carbon showing that the former generates more sequestration using a globally aggregated model hoel and sletten 2016 analyze optimal taxes on energy consumption differentiated between fossil fuel energy and bioenergy to account for the impact on forest sequestration compared to those our study contributes through analysis and empirical calculation of cost efficient nationally differentiated taxes on timber bioenergy and fossil fuels for reaching politically determined targets on carbon dioxide emissions while accounting for the role of carbon pool interdependence the paper is organized as follows first the numerical model is described followed by the derivation of the cost efficient policy instruments then data are described and results are presented the paper ends with a discussion and conclusions 2 numerical model consider the eu with i 1 27 different countries together the countries have agreed on a co2 emissions reduction path until 2050 which they wish to implement at least cost the emission reductions can be achieved by either reduced consumption of fossil fuels within the eu emission trading scheme or by implementing changes in forest management the potential to use forests for different purposes is ultimately determined by the existing forest biomass and its development over time the development of the growing stock of trees 2 2 the growing stock is typically defined as the volume of all living trees in a certain area of forest with a minimum diameter at breast height and includes the stem from ground level or stump height up to a given top diameter and may also include branches above a certain diameter here the growing stock is assumed equal the merchantable tree volume on an average hectare of land is defined by 1 v t 1 i v t i g t i v t i h t 1 i v 0 i v 0 i where variables are measured in cubic meters h t 1 i is the harvest in country i which is assumed to take place in the beginning of the year 3 3 the choice of timing of the harvest in the beginning rather than in the end of each time period is made because this later facilitates the interpretation equations 15 and 16 v t i is the growing stock measured directly after the harvest and g t i v t i is the annual growth total stem wood volume in a country is a i v t i where a i is the area of forest land measured in hectares it is assumed that g t i v t i is positive differentiable and increasing up to a given intermediate volume level thereafter it successively declines towards zero as the forest approaches carrying capacity the use of a representative hectare of forest land is a simplification compared to large scale age class forestry models such as sohngen and sedjo 2006 which has global coverage and moiseyev et al 2011 which includes the european countries this simplification is motivated by our aim to describe the optimal time dynamics of carbon sequestration both analytically and numerically in relation to reductions in co2 emissions from fossil fuel the use of a representative hectare is reasonable if the distribution of forest across different age classes is relatively even it works less well if for example there is a disproportionally large area of forest near economic maturity which typically implies high growth when such a forest is harvested and replaced by young forest the average growth could fall more than is accounted for by equation 1 the approach is therefore more reasonable on a higher level of aggregation such as the national level as storm felling and fires could lead to considerable local variations in the age class distribution while having little impact on the aggregate national level forest carbon sequestration occurs in growing trees and in forest soil net annual carbon sequestration in trees w t i is assumed to be defined by 2 w t 1 i η β i a i v t 1 i v t i where β i is the ratio of total wood volume to stem volume and η is a parameter for conversion of tree volume to ton co2 equivalents removed from the atmosphere the development of the soil carbon stock on a representative hectare of land p t i is mainly determined by the wood volume which adds to the soil carbon stock as litter falls to the ground by forest harvest and natural carbon release following liski et al 2002 we assume that litter from the growing stock adds to the soil carbon stock and that the decomposed litter added is a constant share κ i of the total wood volume in a given year harvesting can cause a release of soil carbon on the harvested area due to disturbances in the soil structure shifts in abundance of woody and herbaceous vegetation and altered soil water and temperature conditions which increase decomposition jandl et al 2007 kuikman et al 2011 it is here assumed that the release of soil carbon depends on the area of final felling and carbon content in the soil the harvested share of the forest land area is then described by γ i h t 1 i v t i g t i v t i where γ i is a constant converting the share of volume harvested h t 1 i v t i g t i v t i into the share of harvested area 4 4 recall that h t 1 i occurs in the beginning of period t 1 whereas v t i g t i v t i is the volume measured in the end of period t we have γ i 1 since forest is harvested at an old age when the growing stock per hectare is relatively large the carbon release from a representative hectare of forest land due to final felling is then assumed to equal the harvested share of the area times a constant fraction ν i of the carbon soil pool p t i this gives release 5 5 we make a simplification by assuming that all of the loss occurs within a single time period albeit a net decline in soil carbon might continue over 10 15 years covington 1981 federer 1984 this simplification is only motivated by convenience of modelling in period t 1 as a share 0 ν i γ i h t 1 i v t i g t i v t i 1 of p t i in addition soil carbon is assumed to be continuously released due to natural processes we follow liski et al 2002 by assuming that the carbon pool p t i decays at constant rate ϑ i in each time period 6 6 detailed soil carbon models such as that in liski et al 2002 divide soil carbon into several interdependent sub pools while assuming a constant rate of decay for each of these sub pools the development of the soil carbon stock 7 7 measured at the end of the time period on an average hectare of forest land p t i is then defined by 3 p t 1 i p t i ν p t i γ h t 1 i v t i g t i v t i κ i η β i v t i ϑ i p t i the second term on the r h s expresses soil carbon losses due to final felling in the beginning of time t 1 8 8 we include the possibility for such losses here but given the mixed empirical evidence on such losses as the losses typically depend on harvesting technology jandl et al 2007 we also investigate a case with zero carbon losses from final felling in the sensitivity analysis the third term shows the added carbon from litter and the fourth the natural decay of soil carbon accounting for the release of carbon to the atmosphere total annual carbon sequestration in forest soil m t i can then be expressed as the incremental change in the soil carbon stock 4 m t 1 i a t 1 i p t 1 i p t i total carbon sequestration in trees and soils s t i is then 5 s t i w t i m t i the harvested forest volume is used for two different purposes bioenergy and timber 6 a i h t i b t i t t i where b t i and t t i are the total volumes of bioenergy and timber respectively bioenergy and timber both affect co2 emissions following hoel and sletten 2016 we assume that the co2 content of bioenergy η b t i is released to the atmosphere in the same time period as it is harvested the released co2 is however partly offset by displacement of fossil fuels displacement depends on the relative efficiency of bioenergy and replaced fossil systems schlamadinger and marland 1996 the parameter τ with τ 0 1 expresses net co2 emissions per unit of co2 in bioenergy after taking fossil fuel displacement into account implying that net co2 emissions from bioenergy are equal to τ η b t i when used as timber carbon is stored in wood products which are assumed to have a life span of k i years cf eggers 2002 after which they are combusted for energy purposes and the co2 content is released like bioenergy timber that is combusted is assumed to replace fossil fuels hence the emissions are partially offset implying that the net release of co2 after k i years is τ η t t k i i the contribution of bioenergy and timber to co2 emissions in a given year l t i can then be summarized as 7 l t i τ η t t k i i b t i where the first term is the release of carbon from wood products combusted at the end of their lifetime and the second is the net contribution of bioenergy to co2 emissions given the displacement of fossil fuels the above formulation implies that we abstract from carbon emissions that arise during harvesting transporting and processing of bioenergy and timber this simplification is justified by the fact that these emissions only correspond to about 2 per cent of the total carbon emissions from wood products petersen raymer 2006 the net reduction of co2 in the atmosphere r t i due to forest carbon sequestration and the different uses of forest products can then be summarized as 8 r t i s t i l t i the combustion of fossil fuels in each country contributes to co2 emissions emissions of co2 from fossil fuels are determined by the quantities of fossil fuels consumed x t i j with j 1 6 different types of fuel 9 9 hard coal lignite natural gas light fuel and heating oil heavy fuel oil and jet fuel and emission coefficients for each fuel type α j total emissions in all countries from fossil fuels and forest management e t are then 9 e t i j α j x t i j r t i there are costs associated with reduced fossil fuel consumption and a changed supply of forest products the cost for reducing the consumption of a certain type of fossil fuel is defined by c t x i j x b a u i j x t i j where x b a u i j is the business as usual bau consumption of the fossil fuel in question it is assumed that the cost function is twice differentiable decreasing and convex and that the consumption cannot fall below a given minimum level x t i j i e x t i j x t i j x b a u i j the use of timber and bioenergy can in principle be either reduced or increased in order to abate carbon emissions the cost of changing bioenergy production is defined as c t b i b b a u i b t i where b b a u i is the bau production of forest bioenergy it is assumed that b t i is subject to lower and upper bounds such that b i b t i b i in a corresponding manner changes in the production of timber give rise to a cost c t t i t b a u i t t i where t b a u i is the bau production level and lower and upper bound apply i e t i t t i t i the cost functions for bioenergy and timber are assumed to be continuous convex and decreasing increasing in b t i and t t i below above the bau level costs are assumed to be separable in bioenergy timber and fossil fuels studies applied at the regional scale often assume that a fixed share of the forest harvest is used for bioenergy see e g carlsson 2012 and trømborg and sjølie 2011 in contrast the global model in eriksson 2015 assumes bioenergy and timber are separable in production notably there is large variation in the share of forest harvest used for bioenergy in different european countries see table a1 in the appendix which implies that fixed shares is not an appropriate assumption at this scale 10 10 one possible reason for variations in forest use is differences in the use of forest residues for energy purposes data on the use of forest residues would be necessary to empirically assess the degree of cost separability between timber and bioenergy but such data are not available for the eu countries the assumption about cost separability in fossil fuels and forest products is motivated by the comparatively small role of bioenergy and timber combustion for total energy consumption 11 11 using data for 2010 on fossil fuel consumption and bioenergy production see appendix and a factor 0 18 for conversion of biomass in m3 to toe the bioenergy produced in european forests corresponds to about 1 per cent of the total energy from fossil fuels a more elaborate analysis of the substitution in production and consumption and hence cost interdependence between bioenergy and fossil fuels would require detailed analysis of supply and demand in different industries which is beyond the scope of this paper given its focus on sequestration time dynamics and carbon pool interdependencies instead the substitution is accounted for in a simplified manner through the use of a displacement factor see equation 7 it is assumed that eu policy makers want to meet a sequence of annual emissions targets e t m a x which are based on eu s roadmap for moving to a low carbon economy by 2050 eucom 2012 the sequence of emission targets can be met by reductions of the consumption of fossil fuels and changes in forest management which affect bioenergy and timber production as well as carbon sequestration in growing forests and soils the emission targets are expressed as 10 e t e t m a x for the targets years t 1 40 it is assumed that policy makers wants to meet 10 at a minimum cost the decision problem is then to 11 m i n x t i j b t i t t i t c t i ρ t c t b i b b a u i b t i c t t i t b a u i t t i j c t x i j x b a u i j x t i j s t 1 10 and the upper and lower bounds on the decision variables the dynamic discrete time lagrangian for this problem and the associated necessary conditions for an interior solution are presented in the appendix in the following section we present the cost efficient policy instruments which are derived from the necessary conditions 3 cost efficient policy instruments for fossil fuels the cost efficient tax is defined by 12 c t i j x b a u i j x t i j x t i j λ t α j i e each fuel is taxed in proportion to the carbon emissions per unit of fuel thus we get the well known result that all fuels can be taxed in proportion to the carbon emissions using a tax per ton of carbon equal to λ t i e the shadow cost of the emission constraint the shadow cost increases over time due to increased target stringency and depends jointly on the costs for fossil fuel reductions and changed forest management at different points in time taxes on timber and bioenergy will affect the decisions by forest product suppliers assuming that there are well functioning market for forest products in all countries the forest supply sectors decision problem will be to minimize the sum of the costs for producing below or above the bau levels i e the levels that would be privately optimal in the absence of taxes plus costs for taxation 13 m i n b t i t t i t c t i ρ t c t b i b b a u i b t i c t t i t b a u i t t i ψ t b i b t i ψ t t i t t i where ψ t b i and ψ t t i are the unit taxes on bioenergy and timber respectively assuming an interior solution the first order conditions for the problem in 13 require that 14 c t t i t b a u i t t i t t i ψ t t i and 15 c t b i b b a u i b t i b t i ψ t b i i e the marginal cost for adjusting timber and bioenergy supply equals the tax on the respective products comparing the expressions in 14 and 15 with the first order conditions for the policy makers decision problem in 11 see appendix we find that the efficient level of adjustment of timber supply will be induced by setting the tax on timber such that the indirect effects on sequestration in trees and soils are taken into account 16 ψ t t i c t t i t b a u i t t i t t i μ t v i 1 a t i μ t p i ν γ p t i v t 1 i g t 1 i v t 1 i a t i ρ k i 1 λ t k i τ η thus the optimal tax on timber is set such that the marginal cost i e the foregone current return due to a change in timber production equals the marginal benefit to society of that change the marginal benefit equals the value of the associated impact on growing stock and soil carbon stock and the discounted value of the impact on the emission target k i periods later the two first terms on the r h s reflect the country specific impact of changed timber production on the costs of future sequestration due to the impact on tree and soil carbon stocks it should be noted that the marginal user cost of the growing stock and soil carbon μ t v i and μ t p i can be positive or negative depending on whether forest growth is positively or negatively affected by the changed forest volume and harvests increase or decrease thus taxes may differ across countries in both sign and magnitude the corresponding efficient tax on bioenergy is defined by 17 ψ t b i c t b i b b a u i b t i b t i μ t v i 1 a t i μ t p i ν γ p t i v t 1 i g t 1 i v t 1 i a t i λ t τ η where the interpretation of the two first terms on the r h s is similar to that in equation 16 the last term expresses the marginal value of the impact on emission constraint in current year comparing equations 16 and 17 it can be seen that the efficient taxes on bioenergy and timber differ only due to the timing of the release of the carbon content the difference is then determined by the development of the discounted shadow cost over time to further understand how the efficient taxes on bioenergy and timber are determined we can look closer at the determinants of the marginal user costs using the necessary condition for the growing stock we have that 18 μ t v i ρ μ t 1 v i 1 g t i v t i ρ μ t 1 p i ν p t i γ h t 1 i 1 g t i v t i v t i g t i v t i 2 κ β i ρ λ t 1 λ t η β i a i equation 18 shows first that the marginal user cost of the growing stock μ t v i depends on the future value of the stock given forest growth if an increase in stock volume leads to increased forest growth μ t v i is larger there is then an additional value of leaving the wood in the forest because of the higher future sequestration that will be achieved second there is a value of the positive impact of increased forest volume on the soil carbon stock this impact is high if litter production κ i and soil carbon stocks are large as increased forest volume reduces the area subject to final felling ceteris paribus the last term expresses the impact of a change in forest volume on the emission targets at time t and t 1 as the marginal user cost of forest volume μ t v i is determined by the increase in the discounted shadow cost λ t we can conclude that rapidly increasing target stringency in combination with a low discount rate will imply a higher marginal user cost in such a case it is cost efficient to allocate harvests over time such that high sequestration can be obtained in the latter part of the policy period thereby reducing the need for costly fossil fuel reductions turning to the marginal user cost of soil carbon stock it can be written as 19 μ t p i ρ μ t 1 p i 1 ϑ i ν γ h t 1 i v t i g t i v t i ρ λ t 1 λ t a i the marginal user cost of soil carbon μ t p i reflects the future value of the soil carbon stock for meeting annual climate targets it is affected by stock development captured in the first term on the r h s of equation 19 a high decay rate ϑ i reduces the marginal user cost as a soil carbon stock increase in the current time period will to a larger extent be lost to the atmosphere in the following time period similarly a high share of harvested volume i e a high h t i v t i g t i v t i implies that an increase in the carbon soil stock will to larger extent be lost in the following time periods due to final felling both a high decay rate and a high harvest share will therefore increase user costs of soil carbon in the current time period the last term in 19 carries a similar interpretation as in equation 18 common to all countries is the increase in the carbon tax over time from increased target stringency as expressed by λ t depending on the marginal user costs of the growing stock and soil carbon pool the taxes on timber and bioenergy products can increase or decrease over time the tax is relatively high when the marginal user costs are high i e when the accumulation of carbon in trees and soil is large which occurs for a relatively high marginal growth in tree volume and a low rate of decay of soil carbon 4 data data and method for calculation of fossil fuels reduction costs within the eu emissions trading system follow gren et al 2009 where the costs are calculated as the decrease in consumer surplus when fossil fuel consumption is reduced emission coefficients for each type of fossil fuel have been obtained from the same source cost functions for decreases and increases in bioenergy and timber are calculated as changes in producer surplus i e the cost to producers in terms of profits foregone in the case of a reduction and costs above the market price payment in the case of an increase inverted linear supply functions for forest products were calculated based on estimates of price elasticities price data and input use data these supply functions were used to calculate quadratic cost functions consumer side welfare effects are not included for forest products because for bioenergy demand is highly politically determined and because trade in forest products is not easily incorporated in a partial model for both fossil fuels and forest product it is assumed that bau prices and quantities are constant over the studied time period this is a simplification as technological development or changes in demand could alter these prices and quantities the simplification is motivated by the use of a partial equilibrium model and the focus on the role of different carbon pools for climate policy cost functions for fossil fuels and forest products and data used for the calculations can be found in the appendix aggregate forest growth functions on national level suitable for our purpose are not available some earlier studies such as kallio et al 2004 use aggregate biomass and forest growth functions but assume a constant forest growth rate other studies such as schulp et al 2008 assume a constant forest growth until a given forest age is reached and growth shifts to zero none of these approaches are suitable when the focus is on the dynamics of sequestration over a longer time period as the successive decline in growth and hence sequestration is not accounted for following amacher et al 2009 chapter 4 we have therefore estimated concave forest growth functions using data from eurostat forestry statistics which report growing stock and increment for commercial forests for each eu country and four years 1990 2000 2005 and 2010 this gives a panel data set with 28 countries and 4 years a quadratic function describing the relationship between growing stock per hectare and the associated gross increment is estimated for commercial forests we take into account differences in growth rates across regions with different climate using dummy variables for the colder boreal and drier mediterranean regions where growth can be expected to be lower than in central europe we test for random effects with a breusch and pagan lagrange multiplier test which shows that this hypothesis cannot be rejected however existence of contemporaneous correlation may exist among countries pesaran 2004 cross sectional dependence can occur in countries which are subjected to the same type of regulations such as the eu directives if our independent variables do not reflect these cross sectional dependencies the estimated standard errors will be affected a pesaran test gave a value of p 60 which indicates non existence of cross section dependence a test was also made for heteroscedasticity which showed no correlation in the errors the quadratic functions where therefore estimated with ordinary least square estimator data and results of estimations can be found in the appendix the estimated growth functions imply that forest growth in different countries varies depending on climatic region and initial growing stock the maximum forest growth occurs when average growing stock per hectare is 266 m3 countries in the dataset with a growing stock equal to 266 30 m3 have an average age of 55 years 12 12 calculated from data in the appendix and vilén et al 2012 note that with our growth functions this volume is reached earlier in central europe and later in the boreal and mediterranean regions and other studies suggest that the age of maximum sequestration should occur close to this age newell and stavins 2000 schulp et al 2008 already in the initial time period eight of the countries have a growing stock larger than 266 m3 hence for several countries increased average forest age could lead to reduced sequestration over time corresponding data are not available for non commercial forests which might in the future be used as commercial ones we therefore apply the estimated growth function to all forest land in each country the ratio of total forest volume and volume of the growing stock is calculated using the average biomass expansion factors for conifers and broadleaves ipcc 2003 to obtain the above ground tree volume thereby we get β i equal to 1 125 and 1 175 for boreal and temperate countries respectively based on data in trømborg and sjølie 2011 the co2 content per cubic meter of wood is assumed to be 0 8 tons 13 13 trømborg and sjølie 2011 report co2 content to be 0 7 0 92 depending on tree species to obtain parameter values for the soil carbon equation we make use of estimates of soil carbon stock and sequestration reported in liski et al 2002 their soil carbon stock estimates apply to the tree originating carbon in the organic soil plus the topmost 20 cm mineral soil layer national estimates for 1990 are available for 14 countries of those included in this study we adjust these estimates for the average stock change 1990 2010 in the region to which the country belongs north northwest central or south europe for the 13 remaining countries in our study we use the average stock for the corresponding region further we use annual sequestration estimates in liski et al 2002 for 1990 assuming they apply also in 2010 while for countries not included in their study we use the average for the region to which the country belongs we assume that 50 percent of the soil carbon is lost on forest land subject to final felling as suggested in early studies on the subject covington 1981 federer 1984 yanai et al 2003 later studies have shown that the magnitude of soil carbon loss can sometimes be much smaller even zero covington 1981 federer 1984 johnson and curtis 2001 yanai et al 2003 and that the harvesting method and the extent of site preparation are important for the magnitude of the losses jandl et al 2007 therefore a case with zero soil carbon losses due to final felling is investigated in the sensitivity analysis building on swedish data for 2010 swedish forest agency 2013 γ is calculated to be 0 6 which we assume to apply for all countries decay rates are calculated from the functions for decomposition rates for slow and fast humus presented in liski et al 2002 where decomposition is modeled as functions of annual mean temperature we use the average of the decomposition rates for fast and slow humus the rate of litter fall κ i is used to calibrate the functions such that the above mentioned sequestration is achieved in the initial year calibrated values then range from 0 0011 to 0 0334 which can be compared with liski et al 2002 where e g the rate of litter to growing stock is reported to be 0 0043 for coniferous forests and 0 0087 for deciduous trees the variation in obtained litter coefficients seems reasonable given that the impact of tree growth on soil carbon accumulation differs between tree species jandl et al 2007 production of bioenergy requires fossil fuel in the refinement process and this process is typically less energy efficient than for refining fossil fuels the carbon displacement is therefore typically less than one schlamadinger and marland 1996 judge that 0 6 is a reasonable estimate of the displacement for bioenergy given current technology and sathre and o connor 2010 argue based on several studies that the bioenergy displacement factor can range from less than 0 5 up to 1 0 depending on the type of fossil fuel replaced and their relative combustion efficiencies cannell 2003 estimates that biomass used to generate electricity displaces coal by a factor 1 0 oil by a factor 0 88 and natural gas by a factor 0 56 we here assume that displacement equal 0 75 implying that τ 0 25 the average lifetime of timber products i e k i is obtained from eggers 2002 the bau consumption and production levels are assumed equal to the levels in 2010 it is assumed that fossil fuel consumption and bioenergy and timber production can at most be reduced by 95 55 and 20 percent respectively compared to bau also it assumed that bioenergy and timber production can at the most be increased by 75 percent which is reasonable compared to short and long term increases in renewables discussed by the eu commission eucom 2012 2013 the eu emissions target is interpreted as a successive reduction of co2 emissions by 80 percent until 2050 this target is assumed to be tightened by the same percentage each year from 2010 to 2050 taking into account that 2010 emissions are eleven percent below those in the reference year 1990 eucom 2012 in principle a successive reduction of emissions is mainly motivated when capital investments are necessary to achieve carbon reductions such as is relevant for fossil fuel reductions however we do not explicitly model capital vintages for the fossil fuel sector instead we use the successive tightening of targets as a simple way of achieving a similar overall carbon reduction path this makes it possible to investigate the role that carbon sequestration can play for reducing abatement costs along that path a discount rate of 3 percent is applied as suggested by boardman et al 2011 to be an appropriate level for public undertakings 5 results we calculate results for five different policy scenarios shown in table 1 the first scenario all includes all abatement alternatives and their effects on carbon release and uptake the all scenario provides a benchmark and corresponds to a cost effective policy the second scenario fossil is one where emission targets have to be achieved by only reductions in fossil fuel consumption forest harvests are held constant over time and equal to bau levels implying that carbon pools in trees soil and forest products change the third scenario fpro adds the possibility of changing the use of forest products and accounts for their direct impact on emissions to the decision problem as defined by equation 7 but the consequential impact on sequestration is ignored in the fourth scenario bio impacts on sequestration and carbon pools in trees are further added as defined by equation 2 the fifth scenario soil is similar to the third but instead of sequestration and pools in trees soil sequestration and soil carbon pools are included the scenarios thus permit comparison of costs and emission impacts under different assumptions about the number of carbon pools that are taken into account in the policy decision thereby it becomes possible to investigate whether a simplified policy where only a single carbon pool is included is an improvement compared to not including sequestration at all when carrying out the optimization only sequestration in addition to that with bau harvests is considered to contribute to the climate targets for tractability the model is aggregated into 5 year time periods the model is run for 20 years beyond 2050 requiring that emissions then remain constant and equal to those in 2050 in order to avoid end of period effects 5 1 the minimum cost the discounted minimum cost of meeting the eu roadmap targets is shown in fig 1 for each scenario as seen inclusion of more abatement options reduces the cost of meeting the targets the cost in the fossil scenario corresponds to approximately 0 4 percent of aggregate gdp which is within the range of costs estimated in capros et al 2014 14 14 capros et al 2014 compare three large scale energy economy models with regard to the least cost strategy for meeting 2050 targets and conclude such a strategy can reduce gdp by 0 0 0 5 different to this study their calculations include also the non trading sector which is larger than the trading sector and abatement costs are higher see e g böhringer et al 2009 the total cost in the all scenario is 33 percent lower than in the fossil scenario this can be compared to michetti and rosa 2012 and gren et al 2012 where cost savings from carbon sequestration are estimated to be 30 percent and 65 percent respectively comparing approaches our analysis is applied to the more demanding 2050 target compared to the 2020 target analyzed in michetti and rosa 2012 and gren et al 2012 this could imply a tendency towards smaller cost savings if the potential for increased sequestration is limited moreover the two mentioned studies allow for afforestation and gren et al 2012 include the non trading sector 15 15 the non trading sector has higher abatement costs so crediting carbon sequestration against targets for the non trading sector implies large cost savings which works in the same direction on the other hand our inclusion of soil sequestration should increase the cost savings compared to these studies 5 2 emission reductions achieved in the all scenario the optimization problem captures all carbon effects from the measures being undertaken in other scenarios only some of the effects are taken into account if we wish to evaluate second best policy scenarios all impacts on carbon emissions must be added to identify the total impact on carbon emissions in fig 2 we therefore compare the total emission reductions in the different scenarios in the fossil scenario a considerable amount of sequestration occurs as a consequence of bau forest management the total reduction in the figure is then the sum of reductions due to reduced fossil fuel consumption and bau sequestration in all other scenarios bioenergy and timber are both reduced in the cost efficient solutions in the fpro scenario bioenergy and timber are reduced in order to avoid emissions from combustion but the associated impact on sequestration is not taken into account however the reduction of forest products implies that sequestration is increased as a consequence the total emission reduction is larger than in the fossil scenario in the bio scenario the policy maker accounts for the effects of both reduced forest products and increased sequestration therefore fossil fuel reductions are smaller compared to the fpro scenario this saves costs but also implies that the total emission reductions are smaller than under fpro the total emission reduction is the highest in the soil scenario because increased growing stocks are required to increase soil sequestration the scenario is more expensive than the bio scenario because sequestration in trees is cheaper than soil sequestration as carbon sequestered in soils is partially lost through decay and at the time of harvesting the all scenario exactly achieves the emission targets implying the lowest cost as well as the lowest emission reductions compared to the fossil scenario the fpro and soil scenarios imply a small or modest reduction in costs but increases the total emission reductions the soil scenario outperforms the fpro scenario with respect to both emissions and costs the bio and all scenarios significantly reduce costs but have a minor positive and a negative impact respectively on emission reductions achieved hence the preferred policy depends on the value that policy makers attach to emission reductions in excess of the targets defined in the roadmap 5 3 the fossil fuel carbon tax carbon taxes on fossil fuels bioenergy and timber can be applied to meet the climate targets for the eu as shown in the model section fig 3 displays the cost efficient co2 tax on fossil fuels in the fossil and all scenarios the co2 tax increases over time when the emission target becomes more stringent and is higher in the fossil scenario than in the all scenario in the all scenario carbon taxes increase from 20 per tco2 in 2010 to 55 in 2050 the 2010 tax level can be compared with the actual carbon price which ranged between 8 and 29 per tco2 between 2008 and 2010 chen et al 2013 our estimated carbon tax levels can also be compared with permit prices 2020 calculated by böhringer et al 2009 from three different cge models they estimate that the permit price for the eu ets will be 50 75 tco2 in the absence of additional efforts to promote renewables our comparable estimate in the fossil scenario 42 per tco2 in 2020 is below their estimated price interval which can be explained by the fact that we do not take into account economy wide dispersal effects furthermore capros et al 2014 estimate that carbon prices will reach 243 565 in 2050 our result in the fossil scenario is only 108 which is likely to be explained by their inclusion of the non trading sector where abatement costs are higher than in the trading sector michetti and rosa 2012 estimate that the carbon price is reduced by 30 percent in 2020 when forest sequestration is included for that time period we obtain almost exactly the same price reduction 5 4 taxes on timber timber production is reduced in all countries seven of the 27 countries reduce timber production down to the lower bound t i during the whole time period 16 16 spain finland greece sweden ireland portugal and cyprus the high reductions in these countries are explained by a high potential for increased forest growth and hence increased sequestration in forests and soils and comparatively low costs for reducing timber production for 18 countries the lower bound is not binding at any point in time during the policy period considered 17 17 two countries estonia and latvia have a binding lower bound in the two or three last time periods we illustrate the results on efficient timber taxes by comparing three countries where the lower bound is not binding during the time period germany italy and the uk these countries differ with respect to initial growing stock and forest growth and soil carbon pools and sequestration see table 2 the growing stock per hectare is the largest in germany almost three times higher than the european average whereas that in italy and uk is close to the european average forest growth is high in germany due to the favorable climate and large growing stock compared to italy and the uk the initial soil carbon stock is large in germany moderate in the uk and small in italy and due to the higher rate of litter fall soil sequestration is higher in germany and the uk compared to italy sequestration in trees increases in italy and the uk until the fifth and seventh time period respectively and then falls in germany sequestration in trees falls over the whole policy period in spite of this the efficient tax on timber increases over time for all three countries in both scenarios except for a small decline in the last period in the uk in the all scenario see fig 4 the simultaneous increase in the tax and decrease in sequestration is explained by relatively low costs for reducing timber production and hence sequestering additional carbon in the same year in order to meet that year s environmental target compared to the disadvantage of having the future potential for sequestration reduced in both the uk and germany the timber taxes are higher in the all scenario than in the bio scenario during the whole policy period this is explained by the large positive impact of increased forest volume on soil carbon sequestration due to high litter fall in these countries an effect which is accounted for in the all scenario but not in the bio scenario the highest rate of litter fall is found in the uk and hence also the highest difference in the tax between the two scenarios in contrast the timber tax in italy is similar in both scenarios due to the small rate of litter fall and hence small soil sequestration the development of the tax over time in the three countries is more similar in the all scenario the tax increases between 1 3 and 2 6 times over the whole time period the rate of increase varies more in the bio scenario between 1 9 and 8 8 times in both scenarios the highest increase occurs in germany and the lowest in the uk the high increase in the german tax in the bio scenario is a consequence of the low cost efficiency of timber reductions in early time periods as the large growing stock implies that there is no potential for increased forest growth while in later time periods reductions become cost efficient in order to meet the stringent carbon targets in the shorter run in both scenarios the timber tax is the highest in the uk this is explained by a relatively high potential for increased forest growth in combination with high litter fall and soil sequestration in the all scenario germany has the lowest timber tax over the first half of the policy period and italy over the latter after half of the policy period forest growth in italy has become small and the rate of litter fall is not large enough to motivate further increases in the growing stock in germany forest growth falls over the whole time period but the successive growth of forest volume implies increased amounts of litter and hence increased soil sequestration which explains this outcome as seen in fig 4 cost efficient timber taxes differ substantially between countries suggesting that uniform timber taxes at the eu level would not be optimal in particular in the shorter run towards the end of the policy period taxes tend to converge as the growing stock and soil carbon stocks saturate and become more similar across countries corresponding taxes on bioenergy follow a similar increasing pattern over time as those on timber the level of the tax is lower because of the earlier release of the carbon as predicted in the theoretical section 5 5 sensitivity analysis in the sensitivity analysis we first investigate the impact of assumptions about forest growth as changed growth is a possible consequence of climate change increased growth is a likely consequence of higher temperatures in north and east europe whereas decreased growth due to drier climate can be expected in the mediterranean area lindner et al 2010 here we assume that forest growth increases by 10 percent in all countries and for all time periods except in the mediterranean countries where instead it decreases by the same rate second we analyze the role of assumptions made about soil carbon sequestration given differing conclusions in the literature regarding the magnitude of soil carbon losses at the time of harvesting here we therefore recalibrate the soil carbon stock function assuming that losses from harvesting are zero the recalibration implies that the litter coefficient which is the calibrated parameter is reduced to be compatible with the same sequestration given the smaller soil carbon losses results from the sensitivity analysis calculated for the all scenario are shown in table 3 with changed forest growth aggregate sequestration increases and hence costs fall the relative impact on tree and soil sequestration is similar larger forest growth on the aggregate level implies that there is less need for policy efforts towards fossil fuels and sequestration in order to meet targets hence timber taxes fall in most countries such as is the case for germany and the uk however for a few countries italy and france taxes increase because the additional growth increases the value of leaving the wood in the forest as this implies that significantly more sequestration can be achieved in the country during the policy period at a given cost it can be noted that the impact on timber taxes in a given country is not proportional to the size of the assumed change in forest growth but is determined by the country s forest growth function and costs of timber reductions when the soil stock function is calibrated for zero losses from final felling the net present cost increases and soil sequestration falls while sequestration in trees is almost unaffected timber taxes are increased in all countries because further efforts are made to increase biomass sequestration to compensate for the lower soil sequestration in this scenario fossil fuel taxes are increased even further in the first period by about 60 per cent 6 discussion and conclusions the aim of this paper is to evaluate policy instruments applied to forest products as a means to achieve carbon sequestration in a cost efficient manner we also compare the economic and environmental consequences of separate inclusion of one carbon pool in the eu s climate policy to that of including three interdependent pools the theoretical analysis shows that biomass soil and forest product pools could be included in a policy by means of differentiated taxes on bioenergy and timber the optimal taxes account for the impact of direct emissions from bioenergy and timber displacement of fossil fuels and the consequences for current and future sequestration in trees and in soils a numerical model is developed which includes cost functions for fossil fuels and forest products and functions which describe the development of forest biomass and soil carbon stocks our analysis suggests that the cost efficient taxes on forest products could be relatively high in countries with large potential for increased sequestration over the policy period given the wide variation in tree and soil carbon stocks and forest growth the efficient level and time path of subsidies and taxes varies substantially across countries therefore a system with uniform eu wide taxes on forest products is not a cost efficient policy over the next decades instead a coordinated approach with differentiated tax levels could work better uniform eu wide policy instruments for forest products might be justified on cost efficiency grounds closer to the target year 2050 provided that national policies to increase sequestration are introduced within the near future the model is used to analyze separate and combined inclusion of different carbon pools compared to a policy where fossil fuel reductions are the only means to meet the eu roadmap targets different to most earlier studies on dynamic forest sequestration applied at small updegraff et al 2010 van kooten et al 1995 wise and cacho 2005 and large e g latta et al 2011 lecocq et al 2011 sohngen and sedjo 2006 moiseyev et al 2011 spatial scale we treat the carbon price as endogenous and show empirically that inclusion of forest carbon sequestration at eu scale can lead to a considerable reduction in carbon price when climate targets are ambitious further lop sided consideration of fossil fuel displacement resulting from the use of bioenergy and timber can be detrimental as increased forest harvests over time reduces the potential for sequestration our results show that successively increased reductions in harvest can be a cost efficient ways to meet carbon targets in this regard our results support conclusions drawn in eriksson 2015 and münnich vass and elofsson 2016 which suggest that sequestration in forest biomass is a cheaper abatement method than bioenergy at global and eu scales respectively similar conclusions are drawn by schulze et al 2012 when evaluating the consequences for greenhouse gases emissions from an increase of the share of forest biomass in global primary energy supply it can be noted that the eu s policy against greenhouse gas emissions is focused on co2 emission trading for fossil fuel use in combination with a target to have 20 percent renewable energy by 2020 our results suggest that the latter can have a negative effect on overall carbon sequestration if bioenergy and used timber make up a large share of the renewable energy and the consequences for sequestration are ignored the results further suggest that a separate inclusion of either biomass or soil carbon pools to the eu s climate policy will lead to improvements compared to both a strictly fossil fuel based policy and a policy which combines fossil fuel reductions with efforts to increase carbon displacement by bioenergy and timber however the improvement will be different in nature inclusion of biomass sequestration will reduce costs more while inclusion of soil sequestration lead to further emissions reductions thus the choice of which single pool to include in the policy is determined by the policy makers valuation of emission reductions in excess of the target the above analysis has limitations including the partial approach and the exclusion of land use change the non trading sector uncertainty and heterogeneity in carbon sequestration within each country forest growth functions are aggregated at national level which is a simplification if they were replaced by growth functions that were disaggregated over space and age classes this would affect the level of taxes but not conclusions regarding the potential for uniform taxation of bioenergy and timber or the consequences of separate inclusion of a single carbon pool also important to the interpretation of results is the exclusion of transaction costs for alternative policy instruments for sequestration nevertheless our analyses and results can contribute to the ongoing debate on carbon pools in the eu which mainly concerns the rules for reporting of carbon pools kuikman et al 2011 the eu commission has recently introduced harmonized rules for carbon accounting implying that national reports should capture all relevant effects from land use land management and harvested wood products eu 2013 this can be a step towards policies promoting forest carbon sequestration our results can then serve as an input in the discussion since they point out principles for determining cost efficient policy instruments targeting forest products under different scenarios it seems likely that additionality and permanence in carbon enhancement in these products is easier and hence cheaper to measure monitor and verify compared to policies targeting carbon pools at stand level also compared to a system with a combination of subsidies and taxes on carbon pools at stand level as suggested by e g van kooten et al 1995 and latta et al 2011 our system implies that fewer and in total smaller financial transactions take place which could facilitate implementation by reducing transaction costs on the other hand policies directed towards products only regulate carbon pools indirectly which is a relative disadvantage as sequestration heterogeneity within each country is ignored 7 software availability the gams code used for the paper is available as supplementary material on the journal homepage running of the code requires installation of the software gams 23 9 5 or higher and the conopt2 solver available at https www gams com download the code developed for the study is not copy protected and can be distributed freely acknowledgements we are very grateful for the helpful comments provided by the anonymous referee of this journal this work was supported by the swedish energy agency grant number 35444 1 appendix the lagrangian and the necessary first order conditions the dynamic discrete time lagrangian is a1 l t i ρ t c t b i b b a u i b t i c t t i t b a u i t t i j c t x i j x b a u i j x t i j ρ μ t 1 v i v t i g t i v t i b t 1 i t t 1 i a i v t 1 i ρ μ t 1 p i p t i ν p t i γ b t 1 i t t 1 i v t i g t i v t i a i κ i β i v t i ϑ i p t i p t 1 i λ t e t m a x e t where ρ 1 1 r is the discount factor and r is the discount rate λ t 0 is the shadow cost for the emission constraint at time t μ t 1 v i 0 and μ t 1 p i 0 are the shadow costs of forest biomass and soil carbon stock the necessary conditions for an interior solution are a2 ρ t l x t i j c t i j x b a u i j x t i j x t i j λ t α j 0 a3 ρ t l b t i c t b i b b a u i b t i b t i μ t v i 1 a t i μ t p i ν γ p t 1 i v t 1 i g t 1 i v t 1 i a t i λ t τ η 0 a4 ρ t l t t i c t t i t b a u i t t i t t i μ t v i 1 a t i μ t p i ν γ p t 1 i v t 1 i g t 1 i v t 1 i a t i ρ k i 1 λ t k i τ η 0 a5 ρ t l v t i ρ μ t 1 v i 1 g t i v t i μ t v i ρ μ t 1 p i ν p t i γ h t 1 i 1 g t i v t i v t i g t i v t i 2 κ β i λ t η β i a i ρ λ t 1 η β i a i 0 a6 ρ t l p t i ρ μ t 1 p i 1 ϑ i ν γ h t 1 i v t i g t i v t i μ t p i λ t a i ρ λ t 1 a i 0 table a1 forest area growth fellings forest products and prices table a1 total forest and other wooded land area a growing stock a gross increment a fellings a use of domestic forest a prices b price elasticity c bio energy oth forest prod bio energy otherforestprod bio energy oth forest prod 1000 ha m3 ha m3 ha m3 ha meur 1000 m3 meur 1000 m3 eu 27 177003 137 5 8 3 2 21 79 at 3991 286 7 5 5 3 26 74 0 0227 0 0697 0 55 0 21 be 706 238 7 9 7 2 15 85 0 0227 0 0728 0 55 0 21 bg 3927 167 5 1 2 0 47 53 0 0227 0 0742 0 55 0 21 cy 387 27 0 9 0 2 41 59 0 0227 0 0768 0 55 0 21 cz 2657 290 9 9 7 2 12 88 0 0227 0 0708 0 55 0 21 de 11076 315 10 1 5 1 18 82 0 0227 0 0723 0 55 0 21 dk 635 180 10 0 4 6 40 60 0 0227 0 0767 0 55 0 21 ee 2337 191 5 6 3 6 27 73 0 016 0 0473 0 55 0 21 es 28214 32 3 1 1 1 32 68 0 0227 0 0665 0 55 0 21 fi 23116 96 4 6 2 6 10 90 0 0235 0 0503 0 55 0 21 fr 17572 148 6 2 3 7 47 53 0 0227 0 0733 0 55 0 21 gr 6539 31 1 3 0 3 68 32 0 0227 0 0768 0 55 0 21 hu 2039 174 6 4 3 3 52 48 0 0227 0 0768 0 55 0 21 ie 788 95 9 8 5 7 7 93 0 0227 0 0768 0 55 0 21 it 10916 133 4 0 1 0 66 34 0 0227 0 0743 0 55 0 21 lt 2249 214 5 7 3 8 27 73 0 0188 0 0453 0 55 0 21 lu 88 295 7 5 3 2 6 94 0 0227 0 0768 0 55 0 21 lv 3467 183 5 8 4 0 18 82 0 016 0 0505 0 55 0 21 mt 0 0 0 0 0 0 0 0227 0 0768 0 55 0 21 nl 365 192 7 6 3 7 27 73 0 0227 0 0723 0 55 0 21 pl 9319 247 8 0 4 2 12 88 0 016 0 0487 0 55 0 21 pt 3611 52 10 5 5 3 6 94 0 0227 0 0594 0 55 0 21 ro 6733 207 6 5 2 5 20 80 0 0227 0 0768 0 55 0 21 se 30625 106 4 7 3 5 8 92 0 0235 0 0518 0 55 0 21 sk 1938 265 7 4 5 4 5 95 0 0227 0 0687 0 55 0 21 si 1274 327 7 8 2 5 37 63 0 0227 0 0751 0 55 0 21 uk 2901 131 8 6 4 0 14 86 0 0227 0 0746 0 55 0 21 a all forest data are for 2010 and have been obtained from eurostat 2012 b the price of other forest products is the weighted average price of logs and pulp in 2010 in finnish forest research institute 2011 where prices are available for austria estonia lithuania and sweden those were extrapolated to the other countries as shown in the table no official price statistics for bioenergy are available here the price of bioenergy is assumed to be 2 3 of the pulp price c price elasticities are obtained from geijer et al 2011 due to lack of elasticity estimated across the eu countries the same elasticity is assumed for all countries table a2 soil carbon stock and sequestration table a2 soil c stock a soil csequestration a conversion factor b γ litter coeff c κ i decomposition rate a ϑ i harvest to volume ratio d harvest impact coeff υ ton co2 ha ton co2 ha volume to area harvested at 220 0 183 0 6 0 0053 4 94e 13 0 019 0 5 be 139 0 213 0 6 0 0066 6 07e 13 0 03 0 5 bg 220 0 183 0 6 0 0065 4 94e 13 0 012 0 5 cy 48 0 029 0 6 0 0054 7 80e 13 0 007 0 5 cz 220 0 183 0 6 0 0067 4 94e 13 0 025 0 5 de 220 0 183 0 6 0 0043 5 43e 13 0 016 0 5 dk 180 0 084 0 6 0 0087 4 98e 13 0 026 0 5 ee 180 0 084 0 6 0 0061 4 98e 13 0 019 0 5 es 48 0 029 0 6 0 0174 9 24e 13 0 034 0 5 fi 180 0 084 0 6 0 0179 2 46e 13 0 027 0 5 fr 220 0 183 0 6 0 0135 6 53e 13 0 025 0 5 gr 48 0 029 0 6 0 0058 9 16e 13 0 01 0 5 hu 220 0 183 0 6 0 0093 4 94e 13 0 019 0 5 ie 139 0 213 0 6 0 0334 5 80e 13 0 06 0 5 it 48 0 029 0 6 0 0011 7 80e 13 0 008 0 5 lt 180 0 084 0 6 0 0053 4 98e 13 0 018 0 5 lu 139 0 213 0 6 0 0024 6 07e 13 0 011 0 5 lv 180 0 084 0 6 0 0080 4 98e 13 0 022 0 5 mt 48 0 029 0 6 na 7 80e 13 na 0 5 nl 139 0 213 0 6 0 0056 5 90e 13 0 019 0 5 pl 220 0 183 0 6 0 0057 5 43e 13 0 017 0 5 pt 48 0 029 0 6 0 0305 9 39e 13 0 102 0 5 ro 220 0 183 0 6 0 0050 4 94e 13 0 012 0 5 se 180 0 084 0 6 0 0196 2 95e 13 0 033 0 5 sk 220 0 183 0 6 0 0060 4 94e 13 0 02 0 5 si 220 0 183 0 6 0 0023 4 94e 13 0 008 0 5 uk 139 0 213 0 6 0 0123 5 43e 13 0 031 0 5 a own calculation based on liski et al 2002 b own calculation based on data from swedish forest agency 2013 c from calibration of model d calculated from data in table a1 table a3 coefficients of cost functions for fossil fuel consumption in the eu ets sector cost functions are quadratic c t x i j a b x t i j c x t i j 2 na zero consumption the fuel type in the country is not included in the calculations sources method and data for calculation of cost function is obtained from gren et al 2009 table a3 hard coal derivatives lignite derivatives natural derived gases a b c a b c a b c at 540 92 0 407 7 67e 05 30 75 0 407 1 35e 03 2434 40 0 978 9 83e 05 be 588 68 0 418 7 40e 05 31 73 0 418 1 37e 03 4160 74 0 946 5 38e 05 bg 310 65 0 283 6 45e 05 591 72 0 283 3 38e 05 346 55 0 345 8 58e 05 cy na na na na na na na na na cz 665 10 0 341 4 37e 05 2240 70 0 341 1 30e 05 1530 38 0 769 9 67e 05 de 5264 33 0 274 3 56e 06 5128 95 0 274 3 65e 06 19623 20 1 086 1 50e 05 dk 671 32 0 246 2 25e 05 na na na 1150 33 0 933 1 89e 04 ee 3 68 0 283 5 44e 03 312 21 0 283 6 42e 05 63 09 0 224 1 99e 04 es 2170 72 0 278 8 92e 06 188 85 0 278 1 03e 04 7472 50 0 648 1 41e 05 fi 646 98 0 288 3 20e 05 318 31 0 288 6 51e 05 891 63 0 422 4 99e 05 fr 2227 42 0 451 2 29e 05 na na na 7775 83 1 167 4 38e 05 gr 38 38 0 269 4 73e 04 1088 60 0 269 1 67e 05 415 61 0 382 8 77e 05 hu 102 04 0 283 1 96e 04 241 58 0 283 8 29e 05 1285 97 0 534 5 55e 05 ie 171 99 0 249 9 03e 05 54 34 0 249 2 86e 04 1071 00 0 792 1 47e 04 it 2844 17 0 398 1 39e 05 0 40 0 398 9 94e 02 17744 78 0 919 1 19e 05 lt 19 25 0 283 1 04e 03 0 42 0 283 4 72e 02 228 82 0 328 1 17e 04 lu 22 34 0 418 1 95e 03 0 63 0 418 6 96e 02 322 16 0 865 5 80e 04 lv 5 09 0 283 3 93e 03 0 14 0 283 1 42e 01 141 93 0 276 1 34e 04 mt na na na na na na na na na nl 1075 96 0 341 2 70e 05 na na na 9892 73 1 212 3 71e 05 pl 4558 81 0 293 4 71e 06 1842 71 0 293 1 16e 05 2132 00 0 684 5 49e 05 pt 483 71 0 293 4 44e 05 na na na 1034 28 0 720 1 25e 04 ro 226 02 0 283 8 86e 05 983 32 0 283 2 04e 05 1611 72 0 414 2 66e 05 se 348 31 0 477 1 63e 04 57 97 0 477 9 82e 04 587 75 1 078 4 95e 04 sk 39 91 0 283 5 02e 04 175 35 0 283 1 14e 04 137 97 0 569 5 87e 04 si 336 12 0 283 5 96e 05 105 29 0 283 1 90e 04 624 05 0 517 1 07e 04 uk 5255 75 0 288 3 94e 06 na na na 20854 64 1 219 1 78e 05 light fuel oil heating oil heavy fuel oil jet fuel a b c a b c a b c at 442 04 1 713 1 66e 03 219 33 0 778 6 90e 04 3172 50 9 000 6 38e 03 be 299 68 0 937 7 32e 04 260 00 0 444 1 90e 04 5305 50 9 000 3 82e 03 bg 113 10 0 650 9 34e 04 100 36 0 388 3 74e 04 918 00 9 000 2 21e 02 cy 5 49 1 569 1 12e 01 38 90 0 608 2 37e 03 1386 00 9 000 1 46e 02 cz 96 25 1 100 3 14e 03 61 60 0 467 8 84e 04 1575 00 9 000 1 29e 02 de 3412 41 1 103 8 92e 05 1060 86 0 517 6 30e 05 39343 50 9 000 5 15e 04 dk 512 23 2 277 2 53e 03 192 43 1 447 2 72e 03 4135 50 9 000 4 90e 03 ee 4 71 0 725 2 79e 02 na na na 144 00 9 000 1 41e 01 es 3422 58 1 422 1 48e 04 2218 77 0 822 7 62e 05 25105 50 9 000 8 07e 04 fi 506 79 1 190 6 98e 04 406 39 0 810 4 04e 04 2767 50 9 000 7 32e 03 fr 2151 73 1 288 1 93e 04 1815 95 0 635 5 54e 05 31837 50 9 000 6 36e 04 gr 1148 17 1 627 5 77e 04 532 59 0 706 2 34e 04 5827 50 9 000 3 47e 03 hu 158 49 1 338 2 82e 03 69 80 0 400 5 73e 04 1224 00 9 000 1 65e 02 ie 152 29 1 259 2 60e 03 238 97 0 724 5 49e 04 3915 00 9 000 5 17e 03 it 4883 86 2 636 3 56e 04 2586 16 0 864 7 21e 05 17914 50 9 000 1 13e 03 lt 110 95 0 700 1 10e 03 48 05 0 388 7 81e 04 238 50 9 000 8 49e 02 lu 3 93 0 873 4 85e 02 na na na 1822 50 9 000 1 11e 02 lv 2 44 0 813 6 77e 02 7 94 0 388 4 73e 03 301 50 9 000 6 72e 02 mt 9 01 1 386 5 33e 02 na na na 346 50 9 000 5 84e 02 nl 5459 76 2 190 2 20e 04 244 17 0 833 7 11e 04 16663 50 9 000 1 22e 03 pl 857 09 1 512 6 67e 04 443 77 0 674 2 56e 04 1930 50 9 000 1 05e 02 pt 94 14 1 207 3 87e 03 429 66 0 690 2 77e 04 4158 00 9 000 4 87e 03 ro 318 83 0 650 3 31e 04 156 74 0 388 2 39e 04 625 50 9 000 3 24e 02 se 1126 37 1 911 8 10e 04 563 57 1 429 9 05e 04 3915 00 9 000 5 17e 03 sk 14 80 0 800 1 08e 02 16 86 0 475 3 35e 03 193 50 9 000 1 05e 01 si 202 30 0 850 8 93e 04 49 28 0 338 5 78e 04 117 00 9 000 1 73e 01 uk 2577 09 1 513 2 22e 04 1387 03 1 103 2 19e 04 58464 00 9 000 3 46e 04 table a4 coefficients of cost functions for forest products cost functions are quadratic timber c t t i a b t t i c t t i 2 bioenergy c t b i a b b t i c b t i 2 na zero production and the product in the country is not included in the calculations sources data on production volume prices and price elasticities are found in table a1 table a4 timber bioenergy a b c a b c at 2595 78 0 3317 3 75e 06 113 33 0 0412 1 06e 05 be 748 88 0 3466 2 70e 05 15 71 0 0412 4 01e 05 bg 735 28 0 3533 5 58e 06 76 06 0 0412 4 24e 05 cy 8 35 0 3657 6 49e 04 0 65 0 0412 4 00e 03 cz 2837 55 0 3371 8 98e 06 47 3 0 0412 1 00e 05 de 7970 52 0 3442 2 03e 06 209 52 0 0412 3 71e 06 dk 319 96 0 3651 1 76e 05 24 08 0 0412 1 04e 04 ee 692 29 0 2254 6 40e 06 33 04 0 0291 1 84e 05 es 3339 49 0 3165 2 07e 06 204 65 0 0412 7 50e 06 fi 6476 8 0 2395 3 56e 06 128 45 0 0427 2 21e 06 fr 6012 17 0 3489 6 74e 07 629 67 0 0412 5 06e 06 gr 114 79 0 3657 1 54e 05 27 49 0 0412 2 91e 04 hu 590 59 0 3657 5 89e 06 72 1 0 0412 5 66e 05 ie 725 26 0 3655 6 90e 05 6 15 0 0412 4 61e 05 it 656 94 0 3540 2 86e 06 148 46 0 0412 4 77e 05 lt 672 89 0 2157 7 41e 06 39 44 0 0342 1 73e 05 lu 48 4 0 3657 1 22e 03 0 35 0 0412 6 91e 04 lv 1367 32 0 2405 5 83e 06 36 31 0 0291 1 06e 05 mt na na na na na na nl 169 65 0 3442 5 65e 05 7 51 0 0412 1 75e 04 pl 3995 99 0 2320 3 10e 06 68 32 0 0291 3 37e 06 pt 1271 28 0 2827 3 59e 05 11 83 0 0412 1 57e 05 ro 2462 35 0 3657 6 12e 06 69 37 0 0412 1 36e 05 se 12163 56 0 2467 2 49e 06 183 27 0 0427 1 25e 06 sk 328 37 0 3273 1 75e 05 24 28 0 0412 8 16e 05 si 1777 56 0 3576 3 94e 05 10 78 0 0412 1 80e 05 uk 1772 73 0 3553 1 27e 05 33 48 0 0412 1 78e 05 table a5 summary statistics table a5 mean min max stdev obs g i forest growth m3 ha 6 6 0 9 12 2 0 2 109 v t i forest volume m3 ha 211 1 47 6 459 2 7 9 109 v t i 2 51373 2 2265 0 210846 8 3542 5 109 s c i dummy for se fi 0 1 0 1 0 025 109 m e i dummy for pt es fr it gr si 0 2 0 1 0 040 109 table a6 ols estimation dependent variable g t i forest growth in m3 ha table a6 variable parameterestimate standarderror t value pr t v t i 0 05881 0 00460 12 80 0001 v t i 2 0 00011044 0 00001591 6 94 0001 s c i 1 32664 0 93064 1 43 0 1570 m e i 0 96340 0 57831 1 67 0 0987 prob f 0 000 appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 006 
26437,river managers of alluvial rivers often need to reconcile conflicting objectives but stakeholder processes are prone to subjectivity time consuming and therefore limited in scope here we present riverscape a modeling tool for numerical creation positioning and implementation of seven common flood hazard reduction measures at any intensity in a 2d hydrodynamic model for a river with embanked floodplains it evaluates the measures for 1 hydrodynamic effects with the 2d flow model delft3d flexible mesh and 2 the required landscaping work expressed as the displaced volume of material the most effective flood hazard reduction in terms of transported material is vegetation roughness smoothing followed by main embankment raising groyne lowering minor embankment lowering side channel construction floodplain lowering and relocating the main embankment implementation of this tool may speed up decision making considerably applications elsewhere could weigh in adverse downstream effects degradation of the ecology and overly expensive choices graphical abstract image 1 keywords intervention planning hydrodynamic modeling disaster risk reduction river management riverscape river waal 1 introduction flood risk reduction ranked high on the political agenda over the last two decades which is warranted given the high and increasing societal cost of flooding the anticipated ongoing climate change and economic developments in fluvial and deltaic areas hirabayashi et al 2013 here flood risk is defined as the inundation probability times the inundation effect the european flood directive european commission 2007 states that it is feasible and desirable to reduce the risk of adverse consequences associated with floods and obliges member states to create flood hazard and risk maps and a flood risk management plan for the implementation flood risk management can be summarized by 1 strategy i e protection against floods living with floods and retreat to flood safe areas and 2 timing of the action relative to the flood event i e pre flood preparedness operational flood management and post flood response kundzewicz and takeuchi 1999 consequently river managers are confronted with large challenges in the planning of measures in and around floodplains of embanked alluvial rivers not only due to the number of stakeholders involved but also due to the long lasting effect on the landscape economic development and riparian ecosystems pinter 2005 flood hazard management at the river basin scale consists of storing water in the headwater of the basin retaining water instream in the middle parts and discharging the water in the downstream reaches hooijer et al 2004 this is because the propagation of a flood wave or flood wave celerity increases with the flow velocity of the water and with the fraction of the discharge conveyed by the main channel jansen et al 1979 for example the narrowing of the floodplains by embankments and decreasing the flow resistance of the floodplain vegetation increases the flood wave celerity which adversely affects the flood hazard downstream clilverd et al 2016 here we present a flexible tool for quantifying effects and effectiveness of common measures to lower the flood risk with the aim to support stakeholder discussions with evidence based facts and figures we develop and apply the tool to a specific case of a lowland deltaic floodplain at the downstream end of the river rhine which is a medium sized river draining part of north west europe typical measures at the scale of a floodplain section fig 1 have in common that they increase the water storage and the conveyance capacity during floods two types of measures are considered here to lower the flood hazard more specifically the probability of flooding the embanked areas the first type lowers the flood stage during peak discharges measure type 1 to 6 fig 1 by creating more space for the river within the embankments the second type comprises raising the main embankment which enables higher water levels the flood hazard reductions of these measures have been reported previously baptist et al 2004 remo et al 2012 and are routinely evaluated in operational river management the typical workflow comprises a geodatabase with spatial information that is converted to input data for a hydrodynamic model experts together with stakeholders choose what measure will be implemented and manual adjustments are made to the geodatabase and the derived hydrodynamic model expert judgment drives this process which is limited by the amount of manual work required to update the hydrodynamic model with a realistic bathymetry and land cover at the spatial extent of the measure these processes can take years for simple measures and more than a decade for complicated projects due to the complex and iterative nature of joint decision making decision support systems dss for these long term planning projects in the preparedness phase are scarce contrary to dsss for operational flood management the options for flood hazard management for the lower reaches of the river rhine in the netherlands silva et al 2004 were modelled for individual measures and the water level lowering at the river axis were made available in a graphical user interface wl delft hydraulics 2008 interactive planning of some measures was possible using geospatial software van der werff ten bosch 2009 application at the river reach scale with realistic measures however is tedious and impractical showing a need for automated procedures to generate these measures in larger areas measures can be applied with different gradations and spatial extents to which we will refer to as intensities of application the units of this intensity vary e g small and large side channels or relocation of embankments over short or large distances nonetheless each measure lowers the flood hazard and their implementation requires material displacement our main objectives were to 1 develop a tool to automatically position and parameterize seven flood hazard reduction measures and 2 evaluate these measures on hydrodynamic effects plus the required volume of displaced material these aims are limited to the physical domain evaluation on costs was outside the scope of this study even though it is closely related to transported material we developed the riverscape package in python and applied it to the main distributary of the river rhine the results are followed by discussion of the applicability to other alluvial rivers and future perspectives to incorporate values other than material displacement 2 materials and methods we developed riverscape a python package which uses map algebra functions from pcraster schmitz et al 2013 riverscape can position and parameterize landscaping measures and update the input data for the two dimensional 2d flow model delft3d flexible mesh dfm which is also open source it requires input on hydrodynamic boundary conditions a geodatabase with layers of river attributes and settings to determine the intensity of application for each measure fig 2 once the measures are known we updated the 2d flow model s input in order to determine the flood hazard reduction and the flow velocities here we present the methods implemented 2 1 study area and available data the case study area is located in the rhine delta which consists of three distributaries the rivers waal nederrijn and ijssel we selected the river waal which is the main distributary of the river rhine in the netherlands fig 3 the three main concerns here are flood risk in view of global change navigability and ecosystem functioning the study area spans an 94 km long river reach with an average water surface gradient of 0 10 m km the total area of the embanked floodplains amounts to 132 km2 the main channel is around 250 m wide and fixed by groynes the cross sectional width between the primary embankments varies between 0 5 and 2 6 km meadows dominate the land cover but recent nature rehabilitation programs led to increased areas with herbaceous vegetation shrubs and forest the design discharge for the river waal is now set to 10 165 m3s 1 which has an average return period of 1250 years such a discharge is expected to give a 3 99 m water level above ordnance datum od at the downstream end of the study area the main channel functions as the primary shipping route between the port of rotterdam and major industrial areas in germany the main channel position is fixed in place by groynes which were partly lowered during the room for the river project van stokkom et al 2005 in the future the design discharge will be combined with a risk based approach that takes the potential damage and casualties within the protected areas into account van alphen 2016 the spatial data describing the major rivers in the netherlands are stored in an arcgis file geodatabase according to the baseline data protocol version 5 scholten and stout 2013 this protocol specific for the netherlands describes the layers in the geodatabase and specifies the required attributes for each of the layers in terms of names and properties baseline schematizations include layers with 1 land cover as a polygon layer of ecotopes landscape ecological units 2 hydrodynamic roughness as point line and polygon layers 3 minor embankments groynes and main embankments as 3d lines consisting of routes and events and 4 river geometry describing the extent of main channel groyne fields floodplains as polygons fig 4 a we adhered to the baseline schematization in order to allow comparison between our results and existing projects but in principle any other method of input of the aforementioned data can be used in combination with riverscape all spatial river attributes are represented as vector layers except bathymetry which is represented as a triangular irregular network tin for the main channel and the floodplain the tin represents the ground level and does not include the groynes and minor embankments we used the rijn beno14 5 v2 schematization of the rhine branches which describes the layout after the finalization of the measures of the room for the river program in december 2015 in the areas protected from flooding by the embankments additional data sources were required as baseline only covers the embanked floodplains and the main channel the national lidar based digital terrain model dtm provided terrain elevation data this gridded dtm has a 0 5 m resolution and a vertical error less than 5 cm bias and random error in open terrain van der zon 2013 building locations were derived from the national database of addresses and buildings bag 2016 the ministry of infrastructure and environment provided the computational mesh of the flow model baart and scholten unpublished data it consisted of 24 small quadrilateral cells across the main channel and groyne field sized around 40 by 20 m fig 5 these are connected by triangular cells to large quadrilateral cells in the floodplains sized around 80 by 80 m no mesh refinement was implemented around the individual groynes to limit the computation time we extended this mesh with triangular cells for the areas protected from flooding by embankments discharge and water level time series between 1989 and 2014 were obtained from the gauging station at tiel fig 1 www live waterbase nl 2 2 hydrodynamic modeling riverscape was coupled to a 2d hydrodynamic model and required a calibrated model as a starting point in this study we used dfm the open source hydrodynamic model that is being developed and maintained by deltares 2016 the computational core of dfm solves the shallow water equations based on the finite volume methods on an unstructured grid kernkamp et al 2011 dfm s computational mesh and output are stored in netcdf files that follow the ugrid conventions for specifying the topology of unstructured and flexible triangular quadrilateral etc grids ugrid 2016 the computational mesh of the study area consisted of 120 000 cells of which 71 000 were active with the current location of the major embankments the spatial dfm input consists of five components in either netcdf or ascii format firstly the ground level of the bathymetry is derived from the tin in the baseline geodatabase which is converted to netcdf format using a predefined computational mesh secondly linear terrain features such as groynes minor embankments and steep terrain jumps are defined as ascii formatted line elements these linear features cause additional energy loss when submerged they are excluded from the bathymetry to limit the number of computational cells and the associated long computation time these linear elements are called fixed weirs and contain the coordinates the height difference on the left and right side and the width and slope of the linear feature thirdly the hydrodynamic roughness is based on trachytopes spatially distributed and stage dependent roughness values trachytopes are based on points for single trees on lines for hedge rows and on polygons for land cover derived from the ecotope map trachytopes in the main channel are adjusted in the model calibration for each flow cell in the computational mesh the fractions of each trachytope is given e g 0 7 for trachytope x and 0 3 for trachytope y chézy roughness is computed at runtime within dfm using the water depth dependent roughness equations developed by klopstra et al 1997 fourthly obstacles that can not be submerged such as bridge pillars or houses are implemented as so called thin dams these represent infinitely high obstacles to flow that may consist of lines or polygons finally dry areas are only represented as polygons that render the contained computational cells inactive whereas for thin dams the cell remains active and water can flow around the obstacle we generated the dfm spatial input from the baseline geodatabase using the baseline plugin for arcgis scholten and stout 2014 further we extended the trachytope definitions with missing codes defined the discharge time series on the upstream boundary and compiled a rating curve at the downstream boundary this completed the dfm model setup 2 3 compilation of river attributes riverscape works on a gridded representation of the river to increase computational speed the basic data consists of a terrain model land use measured water levels a rating curve and a functioning hydrodynamic model this makes application in areas that are more data scarce than the netherlands feasible for the study area vector based data were available in baseline which were rasterized to a 25 m raster resolution to ensure that the cell area of the rasters 625 m2 was smaller than the cell area of the computational mesh of the 2d flow model 800 m2 some subgrid roughness information is lost in this way because a single flow cell may contain multiple trachytopes in dfm this information is maintained as each flow cell may store fractional trachytope areas thirteen relevant baseline layers table 1 were rasterized to a common map extent and resolution in pcraster format using the geospatial data abstraction library gdal warmerdam 2008 the second set of attributes needed for the automatic positioning of measures gave additional information on the river geometry such as channel curvature curve direction and separate floodplains sections table 1 for example a good location for embankment relocation is an area with a sharp right turn in the river axis a narrow floodplain on river right and a low total value in real estate floodplain width calculation was challenging as it posed a one to many problem many points on the main embankment in the outer bend could be connected to a limited set of points on the channel bank and in the inner bend many channel points could be connected to a single cell on the main embankment here we pragmatically calculated the distance from each embankment cell to the channel bank on a line perpendicular to the channel center line fig 4e crossing lines were redirected towards the nearest embankment while maintaining the highest width value the radius and turning direction of the river were derived from fitting a circle to the river axis at each axis cell the location of the center point of the fitted circle in river left or right determines the turning direction the third set of river attributes represented hydrodynamic characteristics derived from a reference run with the 2d flow model table 1 the discharge was increased in a stepwise manner between low flow and design discharge and each step was maintained for 4 days to create a stationary flow we used discharges of 698 1481 1713 2157 2935 4966 m3 s 1 which are exceeded 363 150 100 50 20 and 2 days per year respectively based on the time series of the tiel gauging station the discharge values were derived from percentiles derived from the exceedance percentage in days the choice for these exceedance values was based on their ecological significance as implemented in the classification method for the dutch ecotope map of the large water bodies klijn and de haes 1994 van der molen et al 2000 this map is periodically made with a standardized method for management purposes for example in areas that are inundated less than two days per year the vegetation is considered unaffected by inundation the low exceedance values are ecologically relevant for vegetated floodplains and the high exceedance values are related to the water bodies and side channels in addition we ran the model with the design discharge for the river waal of 10 165 m3 s 1 which provided data on water depth flow velocity and hydrodynamic roughness fig 4bcd amongst others cell ids were required for fast nearest neighbor interpolation of the data stored in an unstructured mesh to regular rasters 2 4 positioning and parameterization of seven flood hazard reduction measures we developed automated procedures to determine the flood hazard reduction potential of seven landscaping measures by adjusting the input of the 2d flow model table 2 six flood stage lowering measures in the floodplain and groyne field were determined plus embankment raising measure positioning was required to determine suitable locations for roughness smoothing side channel construction floodplain lowering and embankment relocation table 3 gives a summary of these methods and settings groyne lowering minor embankment lowering and main embankment raising do not need positioning as their location is predefined in baseline each of the measures was applied with six intensities of application we omitted deepening of the main channel as a flood hazard adaptation option because the river rhine is already deepening due to reduced sediment input from the basin and the narrowing of the main channel with groynes frings et al 2014a bed degradation between tiel and the downstream model boundary was approximately 5 mm y based on frings et al 2009 fig 5 bed erosion negatively affects shipping at non erodable outcrops infrastructure and ecology due to lower water levels frings et al 2014b and exposes entrenched telecom cables and pipelines technically the implementation would be similar to floodplain lowering each of the measures requires the transport of material soil stones vegetation or high quality clay for the main embankments when implemented in the field to create more space for the river the volume of moved material does not necessarily equal the added volume available for water for floodplain smoothing the volume of moved material is larger than the water volume as the emergent vegetation also needs to be removed contrarily in the case of raising the main embankments the volume of water is much larger than the volume of soil required for raising to compare the flood stage reduction effect of the different measures we calculated material volume that needs to be moved and water volume created for each of the measures separate volumes were calculated for 1 vegetation based on stem densities and stem diameters per roughness class van velzen et al 2003 and mean volume per vegetation class schelhaas et al 2014 2 material in groynes and minor embankments derived from the 3d attributes and 3 soil transport based on the differences between the current and new dtm we limited the evaluation to the physical domain i e the conveyance capacity using the lowered flood levels and the storage capacity by calculating the increased water volume the logical extension of the evaluation on transported material would be the cost of the measures but this was outside of the scope for this study flood wave celerity was not considered since the study area is close to the river mouth and flood waves are long relative to the study reach length 2 4 1 side channels side channels were created in a two step approach which comprised positioning of the channel center line followed by the parameterization of the cross section shape and hydrodynamic roughness firstly we positioned side channels only in wide floodplain sections fig 4e without side channels currently present over each section the start and end point of a side channel were positioned on the river axis alongside the upstream and downstream end of the section the centerline of the side channel was determined by the path of the least resistance between start and end point a high resistance value was assigned to the main channel and groyne field to force the centerline into the floodplain a low resistance was given to existing floodplain backwaters and a resistance based on distance to the main channel and main embankment was given to the remaining areas fig 4f the upstream end of the side channel was disconnected from the main channel to prevent large morphological changes in the main channel secondly we parameterized the side channels with a trapezoidal shape of which width depth and cross sectional slope can be set with user specified values the new side channel is only defined where its depth is below the current bathymetry leaving existing lakes largely untouched for the largest side channel we set the depth as an offset of 2 5 m below the water level at the river axis exceeded 363 days per year the width was set to 75 m and the bank slope to 1 3 fig 4g in this study we implemented a series of six side channels in each of the suitable floodplain sections the six intensities of application were defined by the depth and width values scaled to 10 20 40 60 80 and 100 percent of the value used for the largest side channel 2 4 2 vegetation roughness smoothing a low vegetation roughness increases the conveyance capacity of the floodplain area lowering the overall water levels baptist et al 2007 aberle and järvelä 2013 there is no standard procedure for choosing where to lower the roughness and in practice it is based on the judgement of the river manager and contested by nature developers we developed a method that optimizes roughness smoothing by selecting the areas where lowering the floodplain roughness is most effective in terms of water level lowering this is the case where a high specific discharge q m2s 1 coincides with a high vegetation roughness expressed as the nikuradse equivalent roughness length k m fig 4d for example a dense forest at the outflow point of a floodplain section would be a big obstruction to flow we calculated α the product of two fields q and k and determined its cumulative frequency distribution cfd α the score of cfd α at a specific percentile of the distribution was used as a threshold for positioning the roughness smoothing areas where α exceeded the percentile score were selected for roughness smoothing the percentile was calculated as 100 minus a user specified percentage of the terrestrial floodplain area p sm for example floodplain smoothing over 10 of the floodplain area is positioned where the score at the 90th percentile of cfd α is exceeded the vegetation type at the selected areas was changed into production meadow the vegetation with the lowest roughness intensities of application were set to floodplain smoothing over 1 5 10 25 50 and 99 of the terrestrial floodplain area fig 4h the increasing increment was chosen because of the decreasing effectiveness of this measure as the current land cover also includes production meadows 2 4 3 floodplain lowering floodplain lowering was positioned using a similar method as for roughness lowering it is most effective where a high flow velocity coincides with a low water depth under peak discharge for example in case of flow over a natural levee deposit at the upstream end of a floodplain section we calculated the product of two fields denoted as β 1 the water depth and 2 flow velocity field subtracted from the maximum flow velocity at design discharge the inverse of the flow velocity was chosen to prevent equifinality in the selection floodplain lowering was positioned where β exceeded the score at percentile of cfd β where the percentile equals 100 minus a user specified percentage the new terrain elevation was set to the height corresponding to the 50 days per year d y flood duration we chose the roughness code for production meadows as the new roughness this smooth land cover adds to the flood level lowering but riverscape is flexible in assigning new codes like floodplain smoothing we increased the intensity of application by applying floodplain lowering over 1 5 10 25 50 and 99 percent of the terrestrial parts of the floodplain fig 4i within the schematization of the flow model bathymetry roughness and fixed weirs were updated table 1 2 4 4 embankment relocation embankment relocation can be implemented as a buffer around the current main embankment but it is more efficient when the embankment is straightened locally especially with a tortuous embankment shape in top view therefore we relocated the embankment using an alpha shape derived from the embanked area an alpha shape edelsbrunner and muecke 1994 also known as the concave hull is based on a delauney triangulation of a point set where long edges are removed based on the alpha value the lower the alpha value the more it follows the current embankments while an infinitely high alpha value gives the convex hull we increased the intensity by using alpha values of 500 1000 2000 3000 5000 and 7000 m fig 4j we took current built up areas into account by creating ring dikes around areas with high building costs relocation was implemented by adjusting the dry areas the vegetation type of the new floodplain area was set to production meadow 2 4 5 lowering of groynes and minor embankments minor embankments have been constructed to prevent the inundation of agricultural fields during minor floods and their crest level varies likewise groynes have varying crest levels as some have been lowered within the room for the river project to lower flood water levels lowering of groynes and minor embankments was implemented using their current locations as stored in the flow model s input groynes and minor embankments are both stored as fixed weirs in dfm fixed weirs are three dimensional lines that describe location and crest height xyz for each vertex along the line in addition to the crest height each vertex contains information on the cross sectional shape of the fixed weir the terrain height on the left and right of the crest the toe of the minor embankment or groyne the cross sectional slope and the crest width lowering can be applied as a percentage of the current height an absolute change in height or by using an external height level such as a water level that is exceeded a fixed number of days per year due to the current differences in crest height we chose to homogenize the differences by applying an external height the new height consisted of the minimum of the current crest height and the external level but the crest height should not be lower than the terrain height left or right of the crest the intensity of application was increased by lowering to flood durations of 50 100 150 200 250 and 366 d y for groynes and to 2 20 50 100 150 and 366 d y for minor embankments the 366 d y flood duration involved the complete removal of the groynes and minor embankments from the fixed weir input of the 2d flow model 3 results we aimed at the evaluation of flood stage reduction effects of seven landscaping measures with increasing intensity and its relation to displaced material calculation time for rasterizing the input data calculating the derived information positioning and parameterization of measures and updating the flow model output required 0 5 1 2 and 1 75 h respectively on a i7 6700 3 4 ghz processor using a single thread the calculation time for updating the flow model input includes conversion of the updated data to gis compatible raster and vector layers the initial hydrodynamic calculation with the stepwise increase in discharge and the ensemble of realizations both took 24 h we first describe the measures that resulted from the automated methods and then describe their effects on flood water level and channel flow velocity changes finally the results are expressed as a function of the required material displacement volume for the measures and the additional space for flood water due to the measures 3 1 positioning and parameterization of measures the spatial layout of the six flood stage lowering measures is given in fig 6 for the downstream section and in fig s1 in the supporting information on an a3 sized figure for the whole study area roughness lowering locations fig 6b and fig s1b coincide with forested areas when applied to 1 and 5 of the terrestrial part of the floodplain area examples include the forest on river left at river kilometer rkm 872 5 on river right at rkm 878 5 and on river left at rkm 889 these forest patches are often located in areas with low specific discharge at higher intensities of applications 50 or 99 also herbaceous vegetation single trees and hedgerows are removed and converted to meadows groyne lowering was applied to the 797 individual groynes along the main channel the current height of the groynes is not the same over the river reach between rkm 876 and 886 914 and 922 and 952 and 960 the groynes are around a meter above the water level exceeded 100 d y whereas in the remaining stretches the groynes are slightly below this level fig 6c and fig s1c the section between rkm 911 and 928 does not contain groynes in the inner curves as they were converted to longitudinal training dams which are treated as minor embankments in the hydrodynamic input fig s1d not all groynes were equally affected by groyne lowering due to the spatial differentiation of the current groyne height when groynes are lowered to the water level exceeded 250 days per year the median height difference at the endpoint of the groynes between the crest and the highest groyne toe is still 2 93 m which is marginally higher than the minimal guaranteed depth of 2 8 m van vuren et al 2015 all of the 223 km of minor embankments were lowered to increasingly long flood durations the maximum lowering of the crest fig 6d and fig s1d was limited by the maximum toe height which represents the ground level of the floodplain heights of the minor embankments above ground level vary strongly and they showed a 1 25 m interquartile range 0 31 1 56 m the highest minor embankments are found upstream from rkm 883 especially on the river right new side channels were planned in 16 out of the 29 wide floodplain sections fig s1e positioning of side channels was comparatively demanding computationally 20 min computation time as all floodplain sections were addressed sequentially the centerlines follow the midpoints between the main embankments and the main channel in sections without water bodies present e g at rkm 895 5 on river left in curved sections the center line is drawn towards the inner part of the curve within the floodplain section i e rkm 875 on river right and when water is present the centerline is drawn towards existing water i e rkm 880 on river right we positioned one new side channel in the floodplain section on river left around rkm 930 fig 6e in reality a side channel was created here as well inset fig 9 at a similar position in contrast to our modeling choices the side channel was connected to the main channel at the upstream end as well floodplain lowering at 1 and 5 of the terrestrial floodplain area mainly affected artificially raised industrial areas fig 6f and fig s1f such as the shipyard at rkm 897 5 on river left at 10 25 natural levee deposits are removed whereas at 50 99 also the low lying sections are lowered to the water level that is exceeded 50 d y it should be noted that the difference between floodplain lowering and groyne lowering is defined in baseline which states that the maximum width of minor embankments is 10 m wider areas are contained in the bathymetry this is visible in the lowering pattern as elongated lines e g at rkm 922 on river right fig 6f embankment relocation was carried out with six increasing alpha shapes values while existing real estate was taken into account fig 6h and fig s1h the larger alpha shapes almost doubled the surface area of the embanked floodplains in the tortuous upstream part and around rkm 923 to 934 where existing villages become islands in the floodplain the straight sections led to elongated new floodplain areas such as on both sides of the river between rkm 888 and 898 3 2 hydrodynamic effects of measures with increasing application intensities we derived water levels at the river axis and depth averaged flow velocities from the 2d flow model the simulation period was set to three days with a 10165 ms 1 discharge which ensured that the flow became fully stationary wall clock time of a single simulation on a single core was around 4 h differences in water level between the reference run and runs with measures gave insight in the flood stage reduction of each measure and each intensity fig 7 the downstream boundary condition a fixed water level creating a backwater effect led to zero change at rkm 961 changes in flow velocities from the measures e g fig 8 for side channels indicate possible adverse morphological effects in the main channel that could hamper navigation flow velocity differences at 25 and 75 of the main channel width fig 9 summarize the potential morphological effects 3 2 1 flood level reductions roughness smoothing increasingly lowered the water levels with larger extents of application fig 7a the effectiveness reduces at larger percentages with the water level reduction between 1 and 5 almost equal to the reduction from 25 to 50 the 99 intensity showed a negligible effect compared to 50 the lowering was equally distributed over the area with a maximum lowering of 0 2 m groyne lowering showed three distinct steps in the lowered profiles at rkm 883 920 and 958 for all lowering intensities except for the lowering 366 d y fig 7b the three steps coincide with the sections where the groyne height exceeds the 100 d y exceedance level the maximum reduction is 0 1 m the 366 d y groyne lowering involves the complete removal of the groynes from the flow model the resulting 0 25 m reduction serves as a reference for a more natural river with active meandering which is not feasible for the waal minor embankment lowering reduced water levels when lowered to the 2 or 20 d y flood duration with a 0 12 m maximum fig 7c lowering to flood durations of 50 100 and 150 days did not lead to additional water level reduction which indicates that the lowest ground levels around the minor embankment have an inundation duration of approximately 20 d y similar to groyne lowering we completely removed the minor embankments from the flow model input which was labeled as 366 d y for consistency this led to an additional 0 1 m reduction in predicted flood levels at 150 d y all minor embankments are effectively reduced to a terrain jump because the 150 d y level is lower than the terrain height the additional 0 1 m reduction is due to the neglect of the energy loss from the terrain jumps that are in the current bathymetry but that are lost in the relatively coarse bathymetry model the real world implementation would be to alter the terrain in such a way that the downstream slope of the jumps is less than 1 to 7 to avoid flow separation side channel construction led to flood water level reductions that increased with increasing cross sectional area fig 7d area indicated as percentage each increase in intensity did not lead to equal steps in the lowering for example the side channel around rkm 933 on river right fig 6e showed a 0 06 m lowering from 40 to 60 intensity which is larger than for the other steps in intensity this nonlinearity resulted from the upstream end of the disconnected side channel at 40 intensity it does not affect the minor embankment here whereas at 60 the extent is larger and the minor embankment was removed increasing the discharge capacity of the floodplain maximum lowering was 0 38 m small differences in lowering were present between the 10 and 20 intensities floodplain lowering and embankment relocation resulted in flood level reductions that differed an order of magnitude with the other measures fig 7e and f maximum reductions are 1 6 and 2 1 m for lowering and relocation respectively floodplain lowering increased the flood level reduction in upstream direction whereas relocation led to strong local reductions with their own backwater effects 3 2 2 flow velocity differences in the main channel while the main focus of our work is flood risk here we also studied changes in flow velocity in the channel this is important because of the morphodynamic response a spatial gradient in flow velocity leads to a gradient in sediment transport and the latter gradient causes erosion and sedimentation in the shipping fairway which would require dredging the flow velocity along the channel shows patterns as a result of channel convergence and divergence and of exchange with the floodplain fig 9a c shows minor changes in flow velocity but still large gradients in width averaged velocities here we averaged flow velocities over the main channel width and compared the velocity against the reference scenario we assume that the flow velocity in the reference run does not cause erosion and sedimentation that require dredging for fairway maintenance but this is not strictly true because maintenance dredging is conducted frequently the key result is that construction of side channels floodplain lowering and relocation of embankments have significant effects fig 9def zones of reduced flow velocity appear adjacent to the modified floodplain sections on the other hand floodplain smoothing groyne lowering and removal of minor embankments in the floodplain hardly cause changes in flow velocity the local velocity reduction is up to 0 5 ms 1 for side channels and floodplain lowering upstream from rkm 880 which is significant given that typical flow velocity in the channel is 1 75 ms 1 so considerable sedimentation is expected to result this trend is even much stronger for embankment relocation which implies dramatic morphological change in the river channel the relatively modest reduction of sediment transport in the side channel and floodplain lowering measures should also be evaluated against the sediment balance of the river waal over the past decades possibly more than a century the river bed eroded in response to the installation of the groynes due to dredging and due to reduced upstream sediment supply frings et al 2009 our modelled reduction of sediment transport capacity in the channel counteracts this trend meaning that floodplain modification potentially has a positive effect these results point at the need to adapt measures along the river such that changes in the gradients of sediment transport in the channel are minimized this is not the same as the present strategy to minimize changes in sediment transport magnitude 3 3 water volume increase and flood hazard reduction from seven landscaping measures each of the seven measures involves the transport of one or more types of materials 1 vegetation from roughness lowering 2 stones and soil from the adjustments of groynes minor embankments and main embankments and 3 soil from the ground level fig 10 the material volume varied strongly per measure type and intensity the vegetation volume for roughness smoothing was 15 larger than the water volume fig 10a with a maximum vegetation volume of 1 3 105 m3 for the lowering of groynes and minor embankments material volume equals the water volume fig 10b and c the material volume for main embankment raising was exceeded by a nine times larger water volume fig 10d side channel construction and floodplain lowering involved all three material types as the vegetation and minor embankments are removed as well at the measure extents vegetation volume is at least an order of magnitude smaller than the volume for groynes and embankments which is again an order of magnitude smaller than soil from the ground level interestingly the vegetation volume triples when floodplain lowering is doubled from 50 to 99 due to the low lying vegetated areas for embankment relocation the material volume is larger than the water volume at an alpha shape of 500 m this confirms that relocation over small areas is inefficient but the material volume barely increases with larger alpha shapes relocation with the alpha shape at 7000 m provided the largest water volume 2 8 108 m3 which is 23 of the total water volume in the study area during current design water levels the relation between the flood hazard reduction and the volumetric changes per measure fig 11 provided a concise overview of the effectiveness of the different river management options solid and dashed lines indicate the material and water volume respectively a small target of 0 05 m flood stage reduction could be achieved by all different measures but the required intensity of application differed as well as the volumes roughness smoothing required least material volume displaced followed by main embankment raising groyne and minor embankment lowering embankment relocation using alpha shapes required the largest material volume at a 0 05 m flood level reduction conversely an ambitious target of 0 5 m flood stage reduction could only be reached using floodplain lowering main embankment raising and relocating the main embankment note that the difference between the displaced material volume of main embankment raising and the increased water volume is a factor 10 for our study area the factor depends on the mean width of the cross sectional area but this clearly shows that embankment raising is an effective method surprisingly the lines of material and water volume for embankment relocation cross each other at a flood level reduction of around 0 1 m and the material volume increases by 2 5 for larger alpha shapes many small embankment relocations require a large material displacement and are ineffective for flood hazard reduction 4 discussion with riverscape primary geospatial data can be used to 1 quickly update a hydrodynamic model and 2 determine the two dimensional hydrodynamic effect of landscaping measures this modeling pipeline provides a transparent data stack for a systematically modelled set of specific measures at a range of intensities the results can be seen as endmembers of river management options as each of the measures is assessed in isolation the ranking of the measures in terms of their potential in mean flood level lowering over the whole study area fig 11 is as follows minor embankment lowering 0 04 or 0 11 m depending on full removal of fixed weirs floodplain smoothing 0 13 m groyne lowering 0 04 m or 0 20 m for full removal side channels 0 18 m floodplain lowering 0 92 m and embankment relocation 1 23 m the riverscape routines provide flexibility in the area of application per river section per floodplain section or over the whole reach as presented in this paper also the positioning and parameterization settings table 3 can quickly be adjusted in intuitive ways to create new measures our tool can be applied to all alluvial rivers provided the input data are available results depend on the initial land cover the bathymetry and the position of the main embankments relative to the main channel however the major alluvial rivers in densely populated areas share many characteristics with our study area floodplains of the mississippi yangtze elbe danube and san joaquin rivers all have floodplains that are 1 around five times the width of the main channel 2 largely comprise land cover with a low vegetation roughness and 3 are fixed in position with groynes or riprap therefore we believe that the ranking between flood hazard reduction and volume of displaced material fig 11 can be upscaled or downscaled with main channel width and discharge the precise relationship in other areas would require additional modeling our results are expected to scale less well with freely meandering rivers such as the red river usa paraná river brazil or the guaviare river in colombia as the natural land cover differs strongly from the river waal the applicability of the tool depends also on the availability of data and a hydrodynamic model schematization the minimum requirement is a 2d hydrodynamic model a land cover map and a digital terrain model however for most countries where the problems of reconciling river functions are urgent these data are likely to be available in some form in addition global hydrodynamic models are getting more detailed by using remote sensing data and open data e g openstreetmap schellekens et al 2014 riverscape now uses 14 layers from a ready made geodatabase but most layers could be derived from a hydrodynamic model for example river axis river kilometer left and right shoreline table 1 could be derived from the flood extent at low flow roughness codes could be extracted from a global land cover dataset chen et al 2015 and floodplain lakes and channels can be derived from global scale permanent water body products pekel et al 2016 the preprocessing for riverscape would need to be tailored to these inputs similarly we used delft3d flexible mesh as our 2d flow model but interoperability with other models could be created due to the modular setup of riverscape this would require a suitable conversion script for each 2d flow model e g lisflood fp telemac mike21 tuflow with python as the scripting language the data preprocessing and updating the 2d flow model input is likely to be possible within the framework of decision support we focused on two physical criteria for the evaluation of all measures transported material and the flood level lowering the results should be interpreted as the exploration of the parameter space more detailed measures should be designed by landscape architects in combination with engineers and stakeholders in reality measure selection includes a number of additional parameters which were outside the scope of this study these aspects are both limitations of the current study and possibilities for other applications and model extensions in the future as follows firstly in the middle and upper reaches of the river the measures should not increase the flood wave celerity to avoid adverse downstream effects hooijer et al 2004 to determine the effects on the propagation of the flood wave the stationary design discharge in this study should be replaced with a standardized flood wave with a peak discharge equal to the design discharge this would particularly be useful in steeper and longer river reaches while our study reach is situated at the downstream end of a large river meaning that flood waves are relatively low and long measures that increase the flow velocity and the fractional discharge through the main channel increase the flood wave celerity jansen et al 1979 such as roughness lowering and groyne lowering in contrast floodplain lowering embankment relocation and side channel recreation will slow down the flood wave flood wave celerity and attenuation could provide additional parameters for measure evaluation outside of the downstream river reaches this is useful to quantify in a longer reach situated more upstream in a river system a major flood wave for the river waal takes around two weeks which is much longer than the travel time through the area of around 12 h in the upper parts of the catchment the flood wave length and travel time differ less and the effects on the flood wave propagation are therefore stronger secondly the set of measures could be extended with deepening of the main channel in addition to measures in the floodplain and in the groyne field main channel deepening is technically similar to floodplain lowering create a mask for the area to be lowered and apply a change in bathymetry over the masked area either as a fixed value or as spatially distributed values contrary to floodplain lowering channel deepening would require a relative change in bathymetry rather lowering to an absolute external level based on exceedance levels for the waal also the permanent layers should be taken into account that were created to reduce deep scour in sharp bends thirdly the improvement of the fluvial ecology provides a secondary objective of many flood hazard measures buijse et al 2002 bernhardt et al 2005 which is also required by the european water framework directive and the american clean water act hering et al 2010 the changes in ecotope composition can be evaluated beforehand on the potential biodiversity for different taxonomic groups lenders et al 2001 de nooij et al 2004 straatsma et al 2017 for example in the parameterization of floodplain lowering we assigned production meadow as the new ecotope but its biodiversity potential is rather low including potential biodiversity scores would provide a more complete evaluation fifthly investment costs eijgenraam et al 2017 depreciation costs from higher flood frequencies for agriculture in the floodplains and maintenance costs would provide insight in the financial feasibility the investment costs include cost for earthwork treatment or storage of polluted soil dike raising groyne lowering and acquisition and or demolition cost of buildings and land parcels sixtly under natural land management vegetation succession leads to a shift in vegetation from meadows or agricultural fields to herbaceous vegetation shrubs and forest over a period of decades the associated increase in hydrodynamic vegetation roughness lowers the conveyance capacity of the floodplains and increases the water levels makaske et al 2011 the model could be extended with a vegetation succession model the resulting time series of vegetation distributions require the conversion to trachytopes or roughness value to serve as input for a two dimensional hydrodynamic model lastly the durability of the measure can also affect the selection embankment relocation has a long lasting effect on the flood levels in contrast roughness lowering can be reversed in years due to vegetation succession and floodplain lowering can be undone in decades due to increased sediment deposition baptist 2005 makaske and maas 2007 the seven extensions would add to the completeness of the decision support but can not replace stakeholder interaction with experts to make the final decision in the netherlands flood risk management has been based on a design flood with an average return period 1250 years 10 165 m3 s 1 for the river waal for the future we should consider the new risk based approach which takes the economic value of the protected hinterland and the number of lives at risk into account in designing the type size and location of flood protection measures broekx et al 2011 this should include a cost benefit assessment of the measures that addresses the flood risk of the protected area as well brouwer and van ek 2004 in addition it might be required to increase the conveyance capacity of the river waal from 10 165 to 11 436 m3 s te linde et al 2010 which can be combined with ecological restoration which measures should be carried out and in what order a large scale set back of the main embankment clearly had the strongest effect on flood levels should we continue with small scale measures in the floodplain or would it be more cost effective and ecologically superior to set back the embankments and rebuild houses on raised mounds making these trade offs visible could revitalize the public debate on flood proofing river and delta systems much of the time during the planning of flood alleviation measures is spend on negotiations between stakeholders decision making processes and exploration of alternatives which is typically done in a sequential manner rather than in parallel there is no guarantee that the outcome of the policy arena will be the same if the process is repeated evolving decisions depend on timing the individuals involved interdependencies between people and organizations involved and the larger context we are operating in a systematic inventory of intervention options and their costs and benefits hydrodynamic financial costs biodiversity ecosystem services would provide high dimensional feature space that makes the choices transparent and numerically underpinned pareto optimal solutions in this feature space represent the rational numerically optimized cost benefit solution which can be compared against solutions driven by desires of specific stakeholders or political optimization given a fully operational and interactive tool we believe the decision process can be shortened by years this approach turns the usual planning process around by starting at the effects and working backwards towards the spatial implementation thereby transferring more significant information on flood cost and ecology while fundamental information on geometry is still available nonetheless the modeling results should always be interpreted by an expert panel 5 conclusions in this study we aimed to 1 develop a tool to automatically position and parameterize seven flood hazard reduction measures and 2 evaluate these measures on hydrodynamic effects plus the required volume of displaced material the riverscape toolbox automatically positions and parameterizes typical landscaping measures and updates a hydrodynamic model accordingly the ranking of flood hazard reduction in terms of transported material from high to low effectiveness is vegetation roughness smoothing main embankment raising groyne lowering minor embankment lowering side channel construction floodplain lowering and relocating the main embankment this provides an integrated assessment at river reach scale rather than many disconnected measures for individual floodplains as is the current practice for the study area water level reductions of more than 0 5 m could only be achieved with floodplain lowering or embankment relocation for the waal river the modelled reduction in flow velocities in the main channel served as a proxy for morphological tendencies which suggested that the trend of ongoing bed degradation could be slowed down due to lower flow velocities in the main channel we applied all measures in isolation to determine the endmembers of river management options however the routines are flexible in their application spatial subsets could be used for local planning or a combination of measures could be tested to optimize specific solutions with respect to biodiversity or long term flood safety given a fully operational and interactive tool and an expert panel to interpret the results we believe the decision process can be shortened by years acknowledgements this research is part of the research programme rivercare supported by the domain applied and engineering sciences aes which is part of the netherlands organization for scientific research nwo and which is partly funded by the ministry of economic affairs under grant number p12 14 perspective programme we are grateful for the thorough and constructive reviews by bart makaske and one anonymous reviewer that helped to substantially improve the final manuscript johan kabout arcadis is gratefully acknowledged for commenting on a draft version of the manuscript and sharing his experiences in river management and stakeholder processes we are grateful to toine smits ru nijmegen for his initiative that led to implementation of this research in the rivercare research programme the rasterized input data the landscape realizations and the hydrodynamic model input and output are available upon request m w straatsma uu nl data are archived at the faculty of geosciences utrecht university and available in part upon request appendix a supplementary data the following is the supplementary data related to this article supporting information supporting information appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 010 
26437,river managers of alluvial rivers often need to reconcile conflicting objectives but stakeholder processes are prone to subjectivity time consuming and therefore limited in scope here we present riverscape a modeling tool for numerical creation positioning and implementation of seven common flood hazard reduction measures at any intensity in a 2d hydrodynamic model for a river with embanked floodplains it evaluates the measures for 1 hydrodynamic effects with the 2d flow model delft3d flexible mesh and 2 the required landscaping work expressed as the displaced volume of material the most effective flood hazard reduction in terms of transported material is vegetation roughness smoothing followed by main embankment raising groyne lowering minor embankment lowering side channel construction floodplain lowering and relocating the main embankment implementation of this tool may speed up decision making considerably applications elsewhere could weigh in adverse downstream effects degradation of the ecology and overly expensive choices graphical abstract image 1 keywords intervention planning hydrodynamic modeling disaster risk reduction river management riverscape river waal 1 introduction flood risk reduction ranked high on the political agenda over the last two decades which is warranted given the high and increasing societal cost of flooding the anticipated ongoing climate change and economic developments in fluvial and deltaic areas hirabayashi et al 2013 here flood risk is defined as the inundation probability times the inundation effect the european flood directive european commission 2007 states that it is feasible and desirable to reduce the risk of adverse consequences associated with floods and obliges member states to create flood hazard and risk maps and a flood risk management plan for the implementation flood risk management can be summarized by 1 strategy i e protection against floods living with floods and retreat to flood safe areas and 2 timing of the action relative to the flood event i e pre flood preparedness operational flood management and post flood response kundzewicz and takeuchi 1999 consequently river managers are confronted with large challenges in the planning of measures in and around floodplains of embanked alluvial rivers not only due to the number of stakeholders involved but also due to the long lasting effect on the landscape economic development and riparian ecosystems pinter 2005 flood hazard management at the river basin scale consists of storing water in the headwater of the basin retaining water instream in the middle parts and discharging the water in the downstream reaches hooijer et al 2004 this is because the propagation of a flood wave or flood wave celerity increases with the flow velocity of the water and with the fraction of the discharge conveyed by the main channel jansen et al 1979 for example the narrowing of the floodplains by embankments and decreasing the flow resistance of the floodplain vegetation increases the flood wave celerity which adversely affects the flood hazard downstream clilverd et al 2016 here we present a flexible tool for quantifying effects and effectiveness of common measures to lower the flood risk with the aim to support stakeholder discussions with evidence based facts and figures we develop and apply the tool to a specific case of a lowland deltaic floodplain at the downstream end of the river rhine which is a medium sized river draining part of north west europe typical measures at the scale of a floodplain section fig 1 have in common that they increase the water storage and the conveyance capacity during floods two types of measures are considered here to lower the flood hazard more specifically the probability of flooding the embanked areas the first type lowers the flood stage during peak discharges measure type 1 to 6 fig 1 by creating more space for the river within the embankments the second type comprises raising the main embankment which enables higher water levels the flood hazard reductions of these measures have been reported previously baptist et al 2004 remo et al 2012 and are routinely evaluated in operational river management the typical workflow comprises a geodatabase with spatial information that is converted to input data for a hydrodynamic model experts together with stakeholders choose what measure will be implemented and manual adjustments are made to the geodatabase and the derived hydrodynamic model expert judgment drives this process which is limited by the amount of manual work required to update the hydrodynamic model with a realistic bathymetry and land cover at the spatial extent of the measure these processes can take years for simple measures and more than a decade for complicated projects due to the complex and iterative nature of joint decision making decision support systems dss for these long term planning projects in the preparedness phase are scarce contrary to dsss for operational flood management the options for flood hazard management for the lower reaches of the river rhine in the netherlands silva et al 2004 were modelled for individual measures and the water level lowering at the river axis were made available in a graphical user interface wl delft hydraulics 2008 interactive planning of some measures was possible using geospatial software van der werff ten bosch 2009 application at the river reach scale with realistic measures however is tedious and impractical showing a need for automated procedures to generate these measures in larger areas measures can be applied with different gradations and spatial extents to which we will refer to as intensities of application the units of this intensity vary e g small and large side channels or relocation of embankments over short or large distances nonetheless each measure lowers the flood hazard and their implementation requires material displacement our main objectives were to 1 develop a tool to automatically position and parameterize seven flood hazard reduction measures and 2 evaluate these measures on hydrodynamic effects plus the required volume of displaced material these aims are limited to the physical domain evaluation on costs was outside the scope of this study even though it is closely related to transported material we developed the riverscape package in python and applied it to the main distributary of the river rhine the results are followed by discussion of the applicability to other alluvial rivers and future perspectives to incorporate values other than material displacement 2 materials and methods we developed riverscape a python package which uses map algebra functions from pcraster schmitz et al 2013 riverscape can position and parameterize landscaping measures and update the input data for the two dimensional 2d flow model delft3d flexible mesh dfm which is also open source it requires input on hydrodynamic boundary conditions a geodatabase with layers of river attributes and settings to determine the intensity of application for each measure fig 2 once the measures are known we updated the 2d flow model s input in order to determine the flood hazard reduction and the flow velocities here we present the methods implemented 2 1 study area and available data the case study area is located in the rhine delta which consists of three distributaries the rivers waal nederrijn and ijssel we selected the river waal which is the main distributary of the river rhine in the netherlands fig 3 the three main concerns here are flood risk in view of global change navigability and ecosystem functioning the study area spans an 94 km long river reach with an average water surface gradient of 0 10 m km the total area of the embanked floodplains amounts to 132 km2 the main channel is around 250 m wide and fixed by groynes the cross sectional width between the primary embankments varies between 0 5 and 2 6 km meadows dominate the land cover but recent nature rehabilitation programs led to increased areas with herbaceous vegetation shrubs and forest the design discharge for the river waal is now set to 10 165 m3s 1 which has an average return period of 1250 years such a discharge is expected to give a 3 99 m water level above ordnance datum od at the downstream end of the study area the main channel functions as the primary shipping route between the port of rotterdam and major industrial areas in germany the main channel position is fixed in place by groynes which were partly lowered during the room for the river project van stokkom et al 2005 in the future the design discharge will be combined with a risk based approach that takes the potential damage and casualties within the protected areas into account van alphen 2016 the spatial data describing the major rivers in the netherlands are stored in an arcgis file geodatabase according to the baseline data protocol version 5 scholten and stout 2013 this protocol specific for the netherlands describes the layers in the geodatabase and specifies the required attributes for each of the layers in terms of names and properties baseline schematizations include layers with 1 land cover as a polygon layer of ecotopes landscape ecological units 2 hydrodynamic roughness as point line and polygon layers 3 minor embankments groynes and main embankments as 3d lines consisting of routes and events and 4 river geometry describing the extent of main channel groyne fields floodplains as polygons fig 4 a we adhered to the baseline schematization in order to allow comparison between our results and existing projects but in principle any other method of input of the aforementioned data can be used in combination with riverscape all spatial river attributes are represented as vector layers except bathymetry which is represented as a triangular irregular network tin for the main channel and the floodplain the tin represents the ground level and does not include the groynes and minor embankments we used the rijn beno14 5 v2 schematization of the rhine branches which describes the layout after the finalization of the measures of the room for the river program in december 2015 in the areas protected from flooding by the embankments additional data sources were required as baseline only covers the embanked floodplains and the main channel the national lidar based digital terrain model dtm provided terrain elevation data this gridded dtm has a 0 5 m resolution and a vertical error less than 5 cm bias and random error in open terrain van der zon 2013 building locations were derived from the national database of addresses and buildings bag 2016 the ministry of infrastructure and environment provided the computational mesh of the flow model baart and scholten unpublished data it consisted of 24 small quadrilateral cells across the main channel and groyne field sized around 40 by 20 m fig 5 these are connected by triangular cells to large quadrilateral cells in the floodplains sized around 80 by 80 m no mesh refinement was implemented around the individual groynes to limit the computation time we extended this mesh with triangular cells for the areas protected from flooding by embankments discharge and water level time series between 1989 and 2014 were obtained from the gauging station at tiel fig 1 www live waterbase nl 2 2 hydrodynamic modeling riverscape was coupled to a 2d hydrodynamic model and required a calibrated model as a starting point in this study we used dfm the open source hydrodynamic model that is being developed and maintained by deltares 2016 the computational core of dfm solves the shallow water equations based on the finite volume methods on an unstructured grid kernkamp et al 2011 dfm s computational mesh and output are stored in netcdf files that follow the ugrid conventions for specifying the topology of unstructured and flexible triangular quadrilateral etc grids ugrid 2016 the computational mesh of the study area consisted of 120 000 cells of which 71 000 were active with the current location of the major embankments the spatial dfm input consists of five components in either netcdf or ascii format firstly the ground level of the bathymetry is derived from the tin in the baseline geodatabase which is converted to netcdf format using a predefined computational mesh secondly linear terrain features such as groynes minor embankments and steep terrain jumps are defined as ascii formatted line elements these linear features cause additional energy loss when submerged they are excluded from the bathymetry to limit the number of computational cells and the associated long computation time these linear elements are called fixed weirs and contain the coordinates the height difference on the left and right side and the width and slope of the linear feature thirdly the hydrodynamic roughness is based on trachytopes spatially distributed and stage dependent roughness values trachytopes are based on points for single trees on lines for hedge rows and on polygons for land cover derived from the ecotope map trachytopes in the main channel are adjusted in the model calibration for each flow cell in the computational mesh the fractions of each trachytope is given e g 0 7 for trachytope x and 0 3 for trachytope y chézy roughness is computed at runtime within dfm using the water depth dependent roughness equations developed by klopstra et al 1997 fourthly obstacles that can not be submerged such as bridge pillars or houses are implemented as so called thin dams these represent infinitely high obstacles to flow that may consist of lines or polygons finally dry areas are only represented as polygons that render the contained computational cells inactive whereas for thin dams the cell remains active and water can flow around the obstacle we generated the dfm spatial input from the baseline geodatabase using the baseline plugin for arcgis scholten and stout 2014 further we extended the trachytope definitions with missing codes defined the discharge time series on the upstream boundary and compiled a rating curve at the downstream boundary this completed the dfm model setup 2 3 compilation of river attributes riverscape works on a gridded representation of the river to increase computational speed the basic data consists of a terrain model land use measured water levels a rating curve and a functioning hydrodynamic model this makes application in areas that are more data scarce than the netherlands feasible for the study area vector based data were available in baseline which were rasterized to a 25 m raster resolution to ensure that the cell area of the rasters 625 m2 was smaller than the cell area of the computational mesh of the 2d flow model 800 m2 some subgrid roughness information is lost in this way because a single flow cell may contain multiple trachytopes in dfm this information is maintained as each flow cell may store fractional trachytope areas thirteen relevant baseline layers table 1 were rasterized to a common map extent and resolution in pcraster format using the geospatial data abstraction library gdal warmerdam 2008 the second set of attributes needed for the automatic positioning of measures gave additional information on the river geometry such as channel curvature curve direction and separate floodplains sections table 1 for example a good location for embankment relocation is an area with a sharp right turn in the river axis a narrow floodplain on river right and a low total value in real estate floodplain width calculation was challenging as it posed a one to many problem many points on the main embankment in the outer bend could be connected to a limited set of points on the channel bank and in the inner bend many channel points could be connected to a single cell on the main embankment here we pragmatically calculated the distance from each embankment cell to the channel bank on a line perpendicular to the channel center line fig 4e crossing lines were redirected towards the nearest embankment while maintaining the highest width value the radius and turning direction of the river were derived from fitting a circle to the river axis at each axis cell the location of the center point of the fitted circle in river left or right determines the turning direction the third set of river attributes represented hydrodynamic characteristics derived from a reference run with the 2d flow model table 1 the discharge was increased in a stepwise manner between low flow and design discharge and each step was maintained for 4 days to create a stationary flow we used discharges of 698 1481 1713 2157 2935 4966 m3 s 1 which are exceeded 363 150 100 50 20 and 2 days per year respectively based on the time series of the tiel gauging station the discharge values were derived from percentiles derived from the exceedance percentage in days the choice for these exceedance values was based on their ecological significance as implemented in the classification method for the dutch ecotope map of the large water bodies klijn and de haes 1994 van der molen et al 2000 this map is periodically made with a standardized method for management purposes for example in areas that are inundated less than two days per year the vegetation is considered unaffected by inundation the low exceedance values are ecologically relevant for vegetated floodplains and the high exceedance values are related to the water bodies and side channels in addition we ran the model with the design discharge for the river waal of 10 165 m3 s 1 which provided data on water depth flow velocity and hydrodynamic roughness fig 4bcd amongst others cell ids were required for fast nearest neighbor interpolation of the data stored in an unstructured mesh to regular rasters 2 4 positioning and parameterization of seven flood hazard reduction measures we developed automated procedures to determine the flood hazard reduction potential of seven landscaping measures by adjusting the input of the 2d flow model table 2 six flood stage lowering measures in the floodplain and groyne field were determined plus embankment raising measure positioning was required to determine suitable locations for roughness smoothing side channel construction floodplain lowering and embankment relocation table 3 gives a summary of these methods and settings groyne lowering minor embankment lowering and main embankment raising do not need positioning as their location is predefined in baseline each of the measures was applied with six intensities of application we omitted deepening of the main channel as a flood hazard adaptation option because the river rhine is already deepening due to reduced sediment input from the basin and the narrowing of the main channel with groynes frings et al 2014a bed degradation between tiel and the downstream model boundary was approximately 5 mm y based on frings et al 2009 fig 5 bed erosion negatively affects shipping at non erodable outcrops infrastructure and ecology due to lower water levels frings et al 2014b and exposes entrenched telecom cables and pipelines technically the implementation would be similar to floodplain lowering each of the measures requires the transport of material soil stones vegetation or high quality clay for the main embankments when implemented in the field to create more space for the river the volume of moved material does not necessarily equal the added volume available for water for floodplain smoothing the volume of moved material is larger than the water volume as the emergent vegetation also needs to be removed contrarily in the case of raising the main embankments the volume of water is much larger than the volume of soil required for raising to compare the flood stage reduction effect of the different measures we calculated material volume that needs to be moved and water volume created for each of the measures separate volumes were calculated for 1 vegetation based on stem densities and stem diameters per roughness class van velzen et al 2003 and mean volume per vegetation class schelhaas et al 2014 2 material in groynes and minor embankments derived from the 3d attributes and 3 soil transport based on the differences between the current and new dtm we limited the evaluation to the physical domain i e the conveyance capacity using the lowered flood levels and the storage capacity by calculating the increased water volume the logical extension of the evaluation on transported material would be the cost of the measures but this was outside of the scope for this study flood wave celerity was not considered since the study area is close to the river mouth and flood waves are long relative to the study reach length 2 4 1 side channels side channels were created in a two step approach which comprised positioning of the channel center line followed by the parameterization of the cross section shape and hydrodynamic roughness firstly we positioned side channels only in wide floodplain sections fig 4e without side channels currently present over each section the start and end point of a side channel were positioned on the river axis alongside the upstream and downstream end of the section the centerline of the side channel was determined by the path of the least resistance between start and end point a high resistance value was assigned to the main channel and groyne field to force the centerline into the floodplain a low resistance was given to existing floodplain backwaters and a resistance based on distance to the main channel and main embankment was given to the remaining areas fig 4f the upstream end of the side channel was disconnected from the main channel to prevent large morphological changes in the main channel secondly we parameterized the side channels with a trapezoidal shape of which width depth and cross sectional slope can be set with user specified values the new side channel is only defined where its depth is below the current bathymetry leaving existing lakes largely untouched for the largest side channel we set the depth as an offset of 2 5 m below the water level at the river axis exceeded 363 days per year the width was set to 75 m and the bank slope to 1 3 fig 4g in this study we implemented a series of six side channels in each of the suitable floodplain sections the six intensities of application were defined by the depth and width values scaled to 10 20 40 60 80 and 100 percent of the value used for the largest side channel 2 4 2 vegetation roughness smoothing a low vegetation roughness increases the conveyance capacity of the floodplain area lowering the overall water levels baptist et al 2007 aberle and järvelä 2013 there is no standard procedure for choosing where to lower the roughness and in practice it is based on the judgement of the river manager and contested by nature developers we developed a method that optimizes roughness smoothing by selecting the areas where lowering the floodplain roughness is most effective in terms of water level lowering this is the case where a high specific discharge q m2s 1 coincides with a high vegetation roughness expressed as the nikuradse equivalent roughness length k m fig 4d for example a dense forest at the outflow point of a floodplain section would be a big obstruction to flow we calculated α the product of two fields q and k and determined its cumulative frequency distribution cfd α the score of cfd α at a specific percentile of the distribution was used as a threshold for positioning the roughness smoothing areas where α exceeded the percentile score were selected for roughness smoothing the percentile was calculated as 100 minus a user specified percentage of the terrestrial floodplain area p sm for example floodplain smoothing over 10 of the floodplain area is positioned where the score at the 90th percentile of cfd α is exceeded the vegetation type at the selected areas was changed into production meadow the vegetation with the lowest roughness intensities of application were set to floodplain smoothing over 1 5 10 25 50 and 99 of the terrestrial floodplain area fig 4h the increasing increment was chosen because of the decreasing effectiveness of this measure as the current land cover also includes production meadows 2 4 3 floodplain lowering floodplain lowering was positioned using a similar method as for roughness lowering it is most effective where a high flow velocity coincides with a low water depth under peak discharge for example in case of flow over a natural levee deposit at the upstream end of a floodplain section we calculated the product of two fields denoted as β 1 the water depth and 2 flow velocity field subtracted from the maximum flow velocity at design discharge the inverse of the flow velocity was chosen to prevent equifinality in the selection floodplain lowering was positioned where β exceeded the score at percentile of cfd β where the percentile equals 100 minus a user specified percentage the new terrain elevation was set to the height corresponding to the 50 days per year d y flood duration we chose the roughness code for production meadows as the new roughness this smooth land cover adds to the flood level lowering but riverscape is flexible in assigning new codes like floodplain smoothing we increased the intensity of application by applying floodplain lowering over 1 5 10 25 50 and 99 percent of the terrestrial parts of the floodplain fig 4i within the schematization of the flow model bathymetry roughness and fixed weirs were updated table 1 2 4 4 embankment relocation embankment relocation can be implemented as a buffer around the current main embankment but it is more efficient when the embankment is straightened locally especially with a tortuous embankment shape in top view therefore we relocated the embankment using an alpha shape derived from the embanked area an alpha shape edelsbrunner and muecke 1994 also known as the concave hull is based on a delauney triangulation of a point set where long edges are removed based on the alpha value the lower the alpha value the more it follows the current embankments while an infinitely high alpha value gives the convex hull we increased the intensity by using alpha values of 500 1000 2000 3000 5000 and 7000 m fig 4j we took current built up areas into account by creating ring dikes around areas with high building costs relocation was implemented by adjusting the dry areas the vegetation type of the new floodplain area was set to production meadow 2 4 5 lowering of groynes and minor embankments minor embankments have been constructed to prevent the inundation of agricultural fields during minor floods and their crest level varies likewise groynes have varying crest levels as some have been lowered within the room for the river project to lower flood water levels lowering of groynes and minor embankments was implemented using their current locations as stored in the flow model s input groynes and minor embankments are both stored as fixed weirs in dfm fixed weirs are three dimensional lines that describe location and crest height xyz for each vertex along the line in addition to the crest height each vertex contains information on the cross sectional shape of the fixed weir the terrain height on the left and right of the crest the toe of the minor embankment or groyne the cross sectional slope and the crest width lowering can be applied as a percentage of the current height an absolute change in height or by using an external height level such as a water level that is exceeded a fixed number of days per year due to the current differences in crest height we chose to homogenize the differences by applying an external height the new height consisted of the minimum of the current crest height and the external level but the crest height should not be lower than the terrain height left or right of the crest the intensity of application was increased by lowering to flood durations of 50 100 150 200 250 and 366 d y for groynes and to 2 20 50 100 150 and 366 d y for minor embankments the 366 d y flood duration involved the complete removal of the groynes and minor embankments from the fixed weir input of the 2d flow model 3 results we aimed at the evaluation of flood stage reduction effects of seven landscaping measures with increasing intensity and its relation to displaced material calculation time for rasterizing the input data calculating the derived information positioning and parameterization of measures and updating the flow model output required 0 5 1 2 and 1 75 h respectively on a i7 6700 3 4 ghz processor using a single thread the calculation time for updating the flow model input includes conversion of the updated data to gis compatible raster and vector layers the initial hydrodynamic calculation with the stepwise increase in discharge and the ensemble of realizations both took 24 h we first describe the measures that resulted from the automated methods and then describe their effects on flood water level and channel flow velocity changes finally the results are expressed as a function of the required material displacement volume for the measures and the additional space for flood water due to the measures 3 1 positioning and parameterization of measures the spatial layout of the six flood stage lowering measures is given in fig 6 for the downstream section and in fig s1 in the supporting information on an a3 sized figure for the whole study area roughness lowering locations fig 6b and fig s1b coincide with forested areas when applied to 1 and 5 of the terrestrial part of the floodplain area examples include the forest on river left at river kilometer rkm 872 5 on river right at rkm 878 5 and on river left at rkm 889 these forest patches are often located in areas with low specific discharge at higher intensities of applications 50 or 99 also herbaceous vegetation single trees and hedgerows are removed and converted to meadows groyne lowering was applied to the 797 individual groynes along the main channel the current height of the groynes is not the same over the river reach between rkm 876 and 886 914 and 922 and 952 and 960 the groynes are around a meter above the water level exceeded 100 d y whereas in the remaining stretches the groynes are slightly below this level fig 6c and fig s1c the section between rkm 911 and 928 does not contain groynes in the inner curves as they were converted to longitudinal training dams which are treated as minor embankments in the hydrodynamic input fig s1d not all groynes were equally affected by groyne lowering due to the spatial differentiation of the current groyne height when groynes are lowered to the water level exceeded 250 days per year the median height difference at the endpoint of the groynes between the crest and the highest groyne toe is still 2 93 m which is marginally higher than the minimal guaranteed depth of 2 8 m van vuren et al 2015 all of the 223 km of minor embankments were lowered to increasingly long flood durations the maximum lowering of the crest fig 6d and fig s1d was limited by the maximum toe height which represents the ground level of the floodplain heights of the minor embankments above ground level vary strongly and they showed a 1 25 m interquartile range 0 31 1 56 m the highest minor embankments are found upstream from rkm 883 especially on the river right new side channels were planned in 16 out of the 29 wide floodplain sections fig s1e positioning of side channels was comparatively demanding computationally 20 min computation time as all floodplain sections were addressed sequentially the centerlines follow the midpoints between the main embankments and the main channel in sections without water bodies present e g at rkm 895 5 on river left in curved sections the center line is drawn towards the inner part of the curve within the floodplain section i e rkm 875 on river right and when water is present the centerline is drawn towards existing water i e rkm 880 on river right we positioned one new side channel in the floodplain section on river left around rkm 930 fig 6e in reality a side channel was created here as well inset fig 9 at a similar position in contrast to our modeling choices the side channel was connected to the main channel at the upstream end as well floodplain lowering at 1 and 5 of the terrestrial floodplain area mainly affected artificially raised industrial areas fig 6f and fig s1f such as the shipyard at rkm 897 5 on river left at 10 25 natural levee deposits are removed whereas at 50 99 also the low lying sections are lowered to the water level that is exceeded 50 d y it should be noted that the difference between floodplain lowering and groyne lowering is defined in baseline which states that the maximum width of minor embankments is 10 m wider areas are contained in the bathymetry this is visible in the lowering pattern as elongated lines e g at rkm 922 on river right fig 6f embankment relocation was carried out with six increasing alpha shapes values while existing real estate was taken into account fig 6h and fig s1h the larger alpha shapes almost doubled the surface area of the embanked floodplains in the tortuous upstream part and around rkm 923 to 934 where existing villages become islands in the floodplain the straight sections led to elongated new floodplain areas such as on both sides of the river between rkm 888 and 898 3 2 hydrodynamic effects of measures with increasing application intensities we derived water levels at the river axis and depth averaged flow velocities from the 2d flow model the simulation period was set to three days with a 10165 ms 1 discharge which ensured that the flow became fully stationary wall clock time of a single simulation on a single core was around 4 h differences in water level between the reference run and runs with measures gave insight in the flood stage reduction of each measure and each intensity fig 7 the downstream boundary condition a fixed water level creating a backwater effect led to zero change at rkm 961 changes in flow velocities from the measures e g fig 8 for side channels indicate possible adverse morphological effects in the main channel that could hamper navigation flow velocity differences at 25 and 75 of the main channel width fig 9 summarize the potential morphological effects 3 2 1 flood level reductions roughness smoothing increasingly lowered the water levels with larger extents of application fig 7a the effectiveness reduces at larger percentages with the water level reduction between 1 and 5 almost equal to the reduction from 25 to 50 the 99 intensity showed a negligible effect compared to 50 the lowering was equally distributed over the area with a maximum lowering of 0 2 m groyne lowering showed three distinct steps in the lowered profiles at rkm 883 920 and 958 for all lowering intensities except for the lowering 366 d y fig 7b the three steps coincide with the sections where the groyne height exceeds the 100 d y exceedance level the maximum reduction is 0 1 m the 366 d y groyne lowering involves the complete removal of the groynes from the flow model the resulting 0 25 m reduction serves as a reference for a more natural river with active meandering which is not feasible for the waal minor embankment lowering reduced water levels when lowered to the 2 or 20 d y flood duration with a 0 12 m maximum fig 7c lowering to flood durations of 50 100 and 150 days did not lead to additional water level reduction which indicates that the lowest ground levels around the minor embankment have an inundation duration of approximately 20 d y similar to groyne lowering we completely removed the minor embankments from the flow model input which was labeled as 366 d y for consistency this led to an additional 0 1 m reduction in predicted flood levels at 150 d y all minor embankments are effectively reduced to a terrain jump because the 150 d y level is lower than the terrain height the additional 0 1 m reduction is due to the neglect of the energy loss from the terrain jumps that are in the current bathymetry but that are lost in the relatively coarse bathymetry model the real world implementation would be to alter the terrain in such a way that the downstream slope of the jumps is less than 1 to 7 to avoid flow separation side channel construction led to flood water level reductions that increased with increasing cross sectional area fig 7d area indicated as percentage each increase in intensity did not lead to equal steps in the lowering for example the side channel around rkm 933 on river right fig 6e showed a 0 06 m lowering from 40 to 60 intensity which is larger than for the other steps in intensity this nonlinearity resulted from the upstream end of the disconnected side channel at 40 intensity it does not affect the minor embankment here whereas at 60 the extent is larger and the minor embankment was removed increasing the discharge capacity of the floodplain maximum lowering was 0 38 m small differences in lowering were present between the 10 and 20 intensities floodplain lowering and embankment relocation resulted in flood level reductions that differed an order of magnitude with the other measures fig 7e and f maximum reductions are 1 6 and 2 1 m for lowering and relocation respectively floodplain lowering increased the flood level reduction in upstream direction whereas relocation led to strong local reductions with their own backwater effects 3 2 2 flow velocity differences in the main channel while the main focus of our work is flood risk here we also studied changes in flow velocity in the channel this is important because of the morphodynamic response a spatial gradient in flow velocity leads to a gradient in sediment transport and the latter gradient causes erosion and sedimentation in the shipping fairway which would require dredging the flow velocity along the channel shows patterns as a result of channel convergence and divergence and of exchange with the floodplain fig 9a c shows minor changes in flow velocity but still large gradients in width averaged velocities here we averaged flow velocities over the main channel width and compared the velocity against the reference scenario we assume that the flow velocity in the reference run does not cause erosion and sedimentation that require dredging for fairway maintenance but this is not strictly true because maintenance dredging is conducted frequently the key result is that construction of side channels floodplain lowering and relocation of embankments have significant effects fig 9def zones of reduced flow velocity appear adjacent to the modified floodplain sections on the other hand floodplain smoothing groyne lowering and removal of minor embankments in the floodplain hardly cause changes in flow velocity the local velocity reduction is up to 0 5 ms 1 for side channels and floodplain lowering upstream from rkm 880 which is significant given that typical flow velocity in the channel is 1 75 ms 1 so considerable sedimentation is expected to result this trend is even much stronger for embankment relocation which implies dramatic morphological change in the river channel the relatively modest reduction of sediment transport in the side channel and floodplain lowering measures should also be evaluated against the sediment balance of the river waal over the past decades possibly more than a century the river bed eroded in response to the installation of the groynes due to dredging and due to reduced upstream sediment supply frings et al 2009 our modelled reduction of sediment transport capacity in the channel counteracts this trend meaning that floodplain modification potentially has a positive effect these results point at the need to adapt measures along the river such that changes in the gradients of sediment transport in the channel are minimized this is not the same as the present strategy to minimize changes in sediment transport magnitude 3 3 water volume increase and flood hazard reduction from seven landscaping measures each of the seven measures involves the transport of one or more types of materials 1 vegetation from roughness lowering 2 stones and soil from the adjustments of groynes minor embankments and main embankments and 3 soil from the ground level fig 10 the material volume varied strongly per measure type and intensity the vegetation volume for roughness smoothing was 15 larger than the water volume fig 10a with a maximum vegetation volume of 1 3 105 m3 for the lowering of groynes and minor embankments material volume equals the water volume fig 10b and c the material volume for main embankment raising was exceeded by a nine times larger water volume fig 10d side channel construction and floodplain lowering involved all three material types as the vegetation and minor embankments are removed as well at the measure extents vegetation volume is at least an order of magnitude smaller than the volume for groynes and embankments which is again an order of magnitude smaller than soil from the ground level interestingly the vegetation volume triples when floodplain lowering is doubled from 50 to 99 due to the low lying vegetated areas for embankment relocation the material volume is larger than the water volume at an alpha shape of 500 m this confirms that relocation over small areas is inefficient but the material volume barely increases with larger alpha shapes relocation with the alpha shape at 7000 m provided the largest water volume 2 8 108 m3 which is 23 of the total water volume in the study area during current design water levels the relation between the flood hazard reduction and the volumetric changes per measure fig 11 provided a concise overview of the effectiveness of the different river management options solid and dashed lines indicate the material and water volume respectively a small target of 0 05 m flood stage reduction could be achieved by all different measures but the required intensity of application differed as well as the volumes roughness smoothing required least material volume displaced followed by main embankment raising groyne and minor embankment lowering embankment relocation using alpha shapes required the largest material volume at a 0 05 m flood level reduction conversely an ambitious target of 0 5 m flood stage reduction could only be reached using floodplain lowering main embankment raising and relocating the main embankment note that the difference between the displaced material volume of main embankment raising and the increased water volume is a factor 10 for our study area the factor depends on the mean width of the cross sectional area but this clearly shows that embankment raising is an effective method surprisingly the lines of material and water volume for embankment relocation cross each other at a flood level reduction of around 0 1 m and the material volume increases by 2 5 for larger alpha shapes many small embankment relocations require a large material displacement and are ineffective for flood hazard reduction 4 discussion with riverscape primary geospatial data can be used to 1 quickly update a hydrodynamic model and 2 determine the two dimensional hydrodynamic effect of landscaping measures this modeling pipeline provides a transparent data stack for a systematically modelled set of specific measures at a range of intensities the results can be seen as endmembers of river management options as each of the measures is assessed in isolation the ranking of the measures in terms of their potential in mean flood level lowering over the whole study area fig 11 is as follows minor embankment lowering 0 04 or 0 11 m depending on full removal of fixed weirs floodplain smoothing 0 13 m groyne lowering 0 04 m or 0 20 m for full removal side channels 0 18 m floodplain lowering 0 92 m and embankment relocation 1 23 m the riverscape routines provide flexibility in the area of application per river section per floodplain section or over the whole reach as presented in this paper also the positioning and parameterization settings table 3 can quickly be adjusted in intuitive ways to create new measures our tool can be applied to all alluvial rivers provided the input data are available results depend on the initial land cover the bathymetry and the position of the main embankments relative to the main channel however the major alluvial rivers in densely populated areas share many characteristics with our study area floodplains of the mississippi yangtze elbe danube and san joaquin rivers all have floodplains that are 1 around five times the width of the main channel 2 largely comprise land cover with a low vegetation roughness and 3 are fixed in position with groynes or riprap therefore we believe that the ranking between flood hazard reduction and volume of displaced material fig 11 can be upscaled or downscaled with main channel width and discharge the precise relationship in other areas would require additional modeling our results are expected to scale less well with freely meandering rivers such as the red river usa paraná river brazil or the guaviare river in colombia as the natural land cover differs strongly from the river waal the applicability of the tool depends also on the availability of data and a hydrodynamic model schematization the minimum requirement is a 2d hydrodynamic model a land cover map and a digital terrain model however for most countries where the problems of reconciling river functions are urgent these data are likely to be available in some form in addition global hydrodynamic models are getting more detailed by using remote sensing data and open data e g openstreetmap schellekens et al 2014 riverscape now uses 14 layers from a ready made geodatabase but most layers could be derived from a hydrodynamic model for example river axis river kilometer left and right shoreline table 1 could be derived from the flood extent at low flow roughness codes could be extracted from a global land cover dataset chen et al 2015 and floodplain lakes and channels can be derived from global scale permanent water body products pekel et al 2016 the preprocessing for riverscape would need to be tailored to these inputs similarly we used delft3d flexible mesh as our 2d flow model but interoperability with other models could be created due to the modular setup of riverscape this would require a suitable conversion script for each 2d flow model e g lisflood fp telemac mike21 tuflow with python as the scripting language the data preprocessing and updating the 2d flow model input is likely to be possible within the framework of decision support we focused on two physical criteria for the evaluation of all measures transported material and the flood level lowering the results should be interpreted as the exploration of the parameter space more detailed measures should be designed by landscape architects in combination with engineers and stakeholders in reality measure selection includes a number of additional parameters which were outside the scope of this study these aspects are both limitations of the current study and possibilities for other applications and model extensions in the future as follows firstly in the middle and upper reaches of the river the measures should not increase the flood wave celerity to avoid adverse downstream effects hooijer et al 2004 to determine the effects on the propagation of the flood wave the stationary design discharge in this study should be replaced with a standardized flood wave with a peak discharge equal to the design discharge this would particularly be useful in steeper and longer river reaches while our study reach is situated at the downstream end of a large river meaning that flood waves are relatively low and long measures that increase the flow velocity and the fractional discharge through the main channel increase the flood wave celerity jansen et al 1979 such as roughness lowering and groyne lowering in contrast floodplain lowering embankment relocation and side channel recreation will slow down the flood wave flood wave celerity and attenuation could provide additional parameters for measure evaluation outside of the downstream river reaches this is useful to quantify in a longer reach situated more upstream in a river system a major flood wave for the river waal takes around two weeks which is much longer than the travel time through the area of around 12 h in the upper parts of the catchment the flood wave length and travel time differ less and the effects on the flood wave propagation are therefore stronger secondly the set of measures could be extended with deepening of the main channel in addition to measures in the floodplain and in the groyne field main channel deepening is technically similar to floodplain lowering create a mask for the area to be lowered and apply a change in bathymetry over the masked area either as a fixed value or as spatially distributed values contrary to floodplain lowering channel deepening would require a relative change in bathymetry rather lowering to an absolute external level based on exceedance levels for the waal also the permanent layers should be taken into account that were created to reduce deep scour in sharp bends thirdly the improvement of the fluvial ecology provides a secondary objective of many flood hazard measures buijse et al 2002 bernhardt et al 2005 which is also required by the european water framework directive and the american clean water act hering et al 2010 the changes in ecotope composition can be evaluated beforehand on the potential biodiversity for different taxonomic groups lenders et al 2001 de nooij et al 2004 straatsma et al 2017 for example in the parameterization of floodplain lowering we assigned production meadow as the new ecotope but its biodiversity potential is rather low including potential biodiversity scores would provide a more complete evaluation fifthly investment costs eijgenraam et al 2017 depreciation costs from higher flood frequencies for agriculture in the floodplains and maintenance costs would provide insight in the financial feasibility the investment costs include cost for earthwork treatment or storage of polluted soil dike raising groyne lowering and acquisition and or demolition cost of buildings and land parcels sixtly under natural land management vegetation succession leads to a shift in vegetation from meadows or agricultural fields to herbaceous vegetation shrubs and forest over a period of decades the associated increase in hydrodynamic vegetation roughness lowers the conveyance capacity of the floodplains and increases the water levels makaske et al 2011 the model could be extended with a vegetation succession model the resulting time series of vegetation distributions require the conversion to trachytopes or roughness value to serve as input for a two dimensional hydrodynamic model lastly the durability of the measure can also affect the selection embankment relocation has a long lasting effect on the flood levels in contrast roughness lowering can be reversed in years due to vegetation succession and floodplain lowering can be undone in decades due to increased sediment deposition baptist 2005 makaske and maas 2007 the seven extensions would add to the completeness of the decision support but can not replace stakeholder interaction with experts to make the final decision in the netherlands flood risk management has been based on a design flood with an average return period 1250 years 10 165 m3 s 1 for the river waal for the future we should consider the new risk based approach which takes the economic value of the protected hinterland and the number of lives at risk into account in designing the type size and location of flood protection measures broekx et al 2011 this should include a cost benefit assessment of the measures that addresses the flood risk of the protected area as well brouwer and van ek 2004 in addition it might be required to increase the conveyance capacity of the river waal from 10 165 to 11 436 m3 s te linde et al 2010 which can be combined with ecological restoration which measures should be carried out and in what order a large scale set back of the main embankment clearly had the strongest effect on flood levels should we continue with small scale measures in the floodplain or would it be more cost effective and ecologically superior to set back the embankments and rebuild houses on raised mounds making these trade offs visible could revitalize the public debate on flood proofing river and delta systems much of the time during the planning of flood alleviation measures is spend on negotiations between stakeholders decision making processes and exploration of alternatives which is typically done in a sequential manner rather than in parallel there is no guarantee that the outcome of the policy arena will be the same if the process is repeated evolving decisions depend on timing the individuals involved interdependencies between people and organizations involved and the larger context we are operating in a systematic inventory of intervention options and their costs and benefits hydrodynamic financial costs biodiversity ecosystem services would provide high dimensional feature space that makes the choices transparent and numerically underpinned pareto optimal solutions in this feature space represent the rational numerically optimized cost benefit solution which can be compared against solutions driven by desires of specific stakeholders or political optimization given a fully operational and interactive tool we believe the decision process can be shortened by years this approach turns the usual planning process around by starting at the effects and working backwards towards the spatial implementation thereby transferring more significant information on flood cost and ecology while fundamental information on geometry is still available nonetheless the modeling results should always be interpreted by an expert panel 5 conclusions in this study we aimed to 1 develop a tool to automatically position and parameterize seven flood hazard reduction measures and 2 evaluate these measures on hydrodynamic effects plus the required volume of displaced material the riverscape toolbox automatically positions and parameterizes typical landscaping measures and updates a hydrodynamic model accordingly the ranking of flood hazard reduction in terms of transported material from high to low effectiveness is vegetation roughness smoothing main embankment raising groyne lowering minor embankment lowering side channel construction floodplain lowering and relocating the main embankment this provides an integrated assessment at river reach scale rather than many disconnected measures for individual floodplains as is the current practice for the study area water level reductions of more than 0 5 m could only be achieved with floodplain lowering or embankment relocation for the waal river the modelled reduction in flow velocities in the main channel served as a proxy for morphological tendencies which suggested that the trend of ongoing bed degradation could be slowed down due to lower flow velocities in the main channel we applied all measures in isolation to determine the endmembers of river management options however the routines are flexible in their application spatial subsets could be used for local planning or a combination of measures could be tested to optimize specific solutions with respect to biodiversity or long term flood safety given a fully operational and interactive tool and an expert panel to interpret the results we believe the decision process can be shortened by years acknowledgements this research is part of the research programme rivercare supported by the domain applied and engineering sciences aes which is part of the netherlands organization for scientific research nwo and which is partly funded by the ministry of economic affairs under grant number p12 14 perspective programme we are grateful for the thorough and constructive reviews by bart makaske and one anonymous reviewer that helped to substantially improve the final manuscript johan kabout arcadis is gratefully acknowledged for commenting on a draft version of the manuscript and sharing his experiences in river management and stakeholder processes we are grateful to toine smits ru nijmegen for his initiative that led to implementation of this research in the rivercare research programme the rasterized input data the landscape realizations and the hydrodynamic model input and output are available upon request m w straatsma uu nl data are archived at the faculty of geosciences utrecht university and available in part upon request appendix a supplementary data the following is the supplementary data related to this article supporting information supporting information appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 010 
26438,uncertainty analyses show how variability inherent in model parameters affects model outcomes while conducting uncertainty analyses is considered best practice technical and conceptual challenges limit applications for network models this work adapts linear inverse modeling lim techniques to conduct uncertainty analysis on ecosystem flow networks which represent the movement of energy matter through ecosystems we present a new r function for the enar package to perform the analysis and use two case studies of previously published networks to demonstrate the power of this approach the first case study examines a system with available flow uncertainty data to show how lim uncertainty analysis can support stronger statistical inference the second case study examines a system without available uncertainty data to illustrate how these techniques can determine the relative strength of model conclusions even without quantitative data the tools presented here represent an important step in the maturation of ecological network analysis keywords ecological network analysis linear inverse modeling uncertainty analysis network ecology estuaries food web software and data availability section name of software enar developer systems ecology and ecoinformatics laboratory contact address department of biology and marine biology university of north carolina wilmington 601 s college rd wilmington nc 28303 telephone 1 910 962 2411 e mail enar maintainer gmail com year first available 2012 hardware required pc mac linux software required r availability public open source freely available from www r project org language r 1 introduction researchers analysts and managers use models to test hypotheses hilborn and mangel 1997 jørgensen and bendoricchio 2001 quinn and bates 2011 jones and lennon 2014 inform recommendations costanza and ruth 1998 miller 2012 and draw inferences about ecological relationships from data johnson and omland 2004 lafferty et al 2015 while a variety of model types can be used to evaluate the state and function of systems weisberg 2012 the application of network models to accomplish this goal has rapidly increased over the past several decades borrett et al 2014 ecosystem networks have been used to investigate a variety of topics ranging from the effects of predators in trophic cascades wallach et al 2017 to identifying species interactions at multiple spatial scales ovaskainen et al 2015 to evaluating the overall sustainability of ecosystems ulanowicz et al 2009 one common class of ecosystem network model flow networks characterizes budgets for the movement of energy matter through ecosystems using nodes that represent resource pools and edges that represent the transfer of energy matter between resource pools these models can describe a diverse range of systems and interactions including food webs niquil et al 1999 dunne et al 2013 biogeochemical cycles christian and thomas 2003 borrett et al 2016 and systems containing mostly non living components such as urban metabolism networks samaniego and moses 2008 zhang 2013 zhang et al 2016 for example a food web network typically consists of nodes that are species or groups of species and edges that represent the transfer of matter through physical consumption pascual and dunne 2005 borrett et al 2016 ecosystem flow networks let researchers conduct ecological network analysis ena which quantifies and tracks the organization and movement of energy matter in a system of interest hannon 1973 patten et al 1976 ulanowicz 1986 these models and analyses can be used to evaluate specific components and interactions within a system as well as the state of whole ecosystems ena evaluation of network models can reveal hidden relationships that result from compound and indirect interactions bondavalli and ulanowicz 1999 borrett et al 2010 jordán and scheuring 2002 schückel et al 2015 for example christian and luczkovich 1999 used a trophic network of the relationships among species in st marks bay florida to calculate the effective trophic levels of the ecosystem components considering all of the interactions in the system in a separate example schramski et al 2006 used ena of a nitrogen n cycling network model to quantify the role each n pool played in regulating the movement of n through an estuary these types of analyses grant researchers insight into some of the complex interactions that occur within ecosystems and can facilitate monitoring of ecosystem indicators coll and steenbeek 2017 but their usefulness is dependent on the accuracy and precision of the parameters used to build the networks although understanding how imprecisions and uncertainties in parameterization affect network models is essential for appropriately interpreting ena results procedures to directly address this question are underdeveloped and often overlooked in the assessment of ecosystem flow networks ulanowicz 2004 dame and christian 2006 fath et al 2007 uncertainty analyses which quantify how the combined error in all model parameters propagates through model calculations to generate uncertainty in outputs crosetto and tarantola 2001 can be useful tools for this task but can be difficult to apply to network flow models the scarcity of uncertainty analyses in network ecology literature may be a result of the phenomenological approach that is often applied to network model construction ulanowicz 1992 2012 as opposed to the more mechanistic approaches of other model types such as building ordinary differential equations ecosystem flow networks are often parameterized by synthesizing multiple experimentally observed flow and biomass estimations together leading to ambiguity about the way that the errors inherent in these measurements interact to affect the final model outputs despite these difficulties forms of uncertainty analyses have been successfully applied to network flow models for example dame and christian 2006 advocate varying network inputs and structure to identify uncertainty in results but point out that adequate methodologies for thorough uncertainty analyses are lacking borrett and osidele 2007 used monte carlo simulations to evaluate the robustness of network properties in 122 plausible parameterizations of a phosphorous transport network for lake sidney lanier ga usa and kaufman and borrett 2010 analyzed these same 122 phosphorous transport networks to quantify the variability of 18 network analysis metrics given the model uncertainty others have applied uncertainty analyses at fixed levels of error to assess the robustness of ena results borrett and salas 2010 for example salas and borrett 2011 applied an uncertainty analysis that examined the range of ena outputs after perturbing all network edges by 5 and re balancing the models by altering boundary output flows other approaches such as the sensitivity based analysis used by ayers and scharler 2011 to evaluate uncertainty in network models of the kwazulu natal bight south africa or the perturbation approach used by mukherjee et al 2015 to evaluate network indicators under three different scenarios representing biomass changes have also been applied to address uncertainty in ecosystem flow networks linear inverse modeling lim has emerged as a useful tool for evaluating uncertainty in network models and constructing plausible network parameterizations vézina and platt 1988 vézina and pace 1994 kones et al 2009 for example taffi et al 2015 used lim to fill data gaps in a food web of the adriatic sea before conducting network analyses recently guesnet et al 2015 released software for matlab that uses lim along with latin hypercube and monte carlo sampling techniques to generate uncertainty estimations for the ena routines included in the ecopath with ecosim software christensen et al 2005 although these advances represent important steps towards incorporating uncertainty analyses into ena these techniques remain under recognized and underutilized furthermore there are several approaches to model construction for ena including ecopath with ecosim christensen et al 2005 lim vézina and platt 1988 and phenomenological parameterization ulanowicz 1986 and the best modeling approach for each problem may vary based on data availability and system type as the use of network flow models continues to expand standardizing approaches for uncertainty analysis in these models is necessary to ensure that ena results are interpreted consistently and appropriately further recent work has called for increased applications of ena to inform ecosystem assessment and management de jonge et al 2012 longo et al 2015 and quantifying the uncertainty in network flow model results is essential if these applications are to be useful in this work we present a generalized methodology and software function to conduct uncertainty analysis on ecosystem flow networks and to make this analysis more accessible to researchers we first adapt lim techniques to introduce an uncertainty analysis step into the ena workflow fig 1 and provide a new r function to accomplish this task we then demonstrate the power of these analyses using two case studies of previously published models that have different availability of uncertainty data to highlight the diverse applicability of our approach in the first case study we introduce a methodology for uncertainty analysis that can be applied to models with specified data on the uncertainty of edge parameters this data guided uncertainty analysis enables hypothesis testing to detect statistically significant differences in the ena metrics of models given the uncertainty in network parameterization in the second case study we demonstrate how this uncertainty analysis can be modified to accommodate networks where uncertainty data are not readily available we applied an increasing amount of uniform uncertainty across the network flows to quantify how much variability was required to eliminate observed differences between networks these examples highlight the power of the lim uncertainty analyses presented in this work to both enable stronger inferences when comparing network flow models and provide insight into the robustness of network modeling results 2 materials and methods 2 1 workflow we present a software function and modified workflow for ena to make lim uncertainty analysis for network models accessible to researchers fig 1 this workflow introduces an uncertainty step between the initial construction of network flow models and the application of ena while fath et al 2007 outline guidelines for the construction of ecosystem flow networks several approaches to this task are available each of these approaches requires substantial input data and which method researchers use may depend on data availability for example if biomass measurements and consumption data are available ecopath with ecosim may be the best tool for network construction christensen et al 2005 while if empirical measurements of nutrient flows are available phenomenological parameterization may be the preferred approach ulanowicz 1986 further the trophic modeling approach used in ecopath would not make sense for some applications such as urban metabolisms the uncertainty analysis techniques described in this work differ from other approaches such a those in guesnet et al 2015 in that they are applicable regardless of model construction technique and are conducted after model construction once a network model is constructed uncertainty data for each edge weight can be combined with the original model through a lim uncertainty analysis this uncertainty analysis can be performed by the enauncertainty function which is available in the supplemental material to this work and is included in the enar v 3 0 1 package for r borrett and lau 2014 lau et al 2017 this software function first uses the original network model along with uncertainty data tables to generate the inputs needed for lim it then applies a lim analysis to generate plausible model sets using the limsolve package for r soetaert et al 2009 van den meersche et al 2009 next lim results are converted back into a format readable by enar and are returned to the user as a set of plausible model parameterizations for use in ena fig 1 the position of this uncertainty analysis prior to ena in the workflow is important because it implies that multiple different types of ena can be applied to the same set of plausible model parameterizations for example ena analyses including structure flow storage impact control environ and ascendency analyses fath and patten 1999 schramski et al 2006 ulanowicz 2004 ulanowicz et al 2009 could all be conducted to evaluate different network metrics in the same parameterization set fig 1 applying ena across a set of plausible network parameterizations results in distributions of ena results which can than be evaluated and statistically compared example code for a start to finish workflow that loads network models incorporates uncertainty data conducts an lim uncertainty analysis applies ena to a set of plausible models and evaluates results using the enar software is available in the supplementary material for this work 2 2 case studies each of the two case studies used in this work compared pairs of previously published networks representing sites within different estuarine ecosystems the first case study applied a data guided uncertainty analysis to compare two n cycling networks in the cape fear river estuary cfre nc usa hines et al 2012 2015 the second case study modified the uncertainty analysis to inform a second comparative analysis of two food web networks for the sundarban mangrove ecosystem india ray et al 2000 ray 2008 each pair of models had similar network topology structure to facilitate their comparison but each model was parameterized with different edge weights based on site specific data to quantify differences in ecosystem functioning between the sites all of the models used for case studies in this work are available as part of the enar package for r borrett and lau 2014 2 2 1 cape fear river estuary models the two networks for the cfre case study were constructed at sites with different salinity regimes to investigate the impact of seawater intrusion on the estuarine n cycle one at a more freshwater oligohaline site and one at a more saline polyhaline site each of these models tracks the movement of n through the ecosystem in units of nmol n cm 3d 1 these networks consist of eight n pools nodes connected by 38 transformations edges these edges represented the movement of n species between the standing stock pools as well as boundary fluxes into and out of the system the n pools included ammonium nitrate nitrite microbial biomass and organic n in both the water column w nh4 w no x w m w on and sediment s nh4 s no x s m s on respectively fig 2 the edge weights were parameterized using a combination of direct measurements and literature values for each site along with mass balance estimation when necessary hines et al 2012 2015 the cfre network models were compared in previous work to quantify coupling of microbial n cycling processes and to estimate the effects of seawater intrusion on an estuarine n cycle hines et al 2012 2015 2016 2 2 2 sundarban mangrove models the food web networks used for the second case study represent trophic interactions on mudflats for two islands in the sundarban mangrove ecosystem one virgin undisturbed and one reclaimed anthropogenically altered each of these networks consist of 14 nodes representing aggregated groups of species such as detritus bacteria phytoplankton and top predators fig 3 these nodes are connected by 43 internal edges and 38 boundary edges that represent the transfer of energy matter by trophic interactions and physical transport through five trophic levels in units of kcal m 2 y 1 ray et al 2000 previously published comparisons of these networks evaluated the potential impacts of human activity on the flow of energy through this mangrove ecosystem ray 2008 2 3 ecological network analysis to demonstrate the versatility and limitations of applications for the uncertainty techniques presented in this work we applied three forms of ena to our case studies flow analysis ascendency analysis and structural analysis flow analysis quantifies and partitions the movement of energy matter in networks into different kinds of pathways fath and patten 1999 schramski et al 2011 while ascendency analysis assesses flow diversity and provides a measure of the overall state of a system by quantifying the activity and organization of the network flows patrıcio et al 2004 ulanowicz 2000 morris et al 2005 structural analysis evaluates the topology of network models to reveal pathways and cycles fath and patten 1999 and was included to demonstrate that the uncertainty analysis methods presented in this work do not impact structural metrics here we provide a brief description of eight network metrics selected from among these three analyses to provide example applications of the lim uncertainty analysis all ena metrics were calculated using the enar v 3 0 1 package for r borrett and lau 2014 lau et al 2017 2 3 1 flow analysis we examined three commonly calculated network metrics from flow analysis total system through flow tst finn s cycling index fci and indirect flow intensity ifi tst is a measure of the amount of material moving through a system and is calculated as the sum of all of the internal flows and boundary inputs eq 1 1 tst internal flows inputs tst has the same units as the network edges and is sometimes interpreted as the size of a system finn 1976 fath and patten 1999 this metric is central to the calculation of the other flow metrics reported in this manuscript as these analyses partition tst to reveal system behavior fci is the fraction of tst generated by recycling finn 1976 1980 it is calculated as 2 fci cycled flow tst where cycled flow is defined as material that exits a node then passes through the same node again before exiting the network finn 1980 eq 2 the fci statistic provides a measure of how leaky a network is to the surrounding environment this metric has implications for the retention time of material in a system baird and ulanowicz 1993 and has also been interpreted as an indication of system health wulff and ulanowicz 1989 the ifi metric reports the proportion of tst driven by fluxes over a path length greater than one crossing two or more edges indirect flow as in eq 3 borrett et al 2006 salas and borrett 2011 3 ifi indirect flow tst the proportion of flow in ecosystem networks resulting from indirect flow has been used to quantify the strength of competition in trophic networks and emphasize the importance of systems based analyses fath and grant 2007 2 3 2 ascendency analysis ascendency analyses quantify ecosystem organization and development by applying information theoretic metrics to network flow models ulanowicz 1986 we examined four ascendency analysis metrics development capacity c the ascendency to capacity ratio a c network robustness and effective link density eld c describes the flow diversity that is possible in a the system and is calculated as in eq 4 by multiplying the sum of all flows t by the diversity of flows in a network defined by shannon s information measure h macarthur 1955 ulanowicz 2004 4 c t h h has informational units of bits therefore c has units of edge flow times bits c is the theoretical maximum flow diversity that a system can attain ulanowicz et al 2009 the a c ratio quantifies how diverse the flows are in a network ascendency a relative to maximum possible capacity c ulanowicz 1980 patrıcio et al 2004 ascendency is calculated as the product of t and average mutual information a m i which is a measure of shared information eq 5 ulanowicz et al 2009 5 a t a m i a m i has units of bits thus a has the same units as c and a c is unitless optimization of the a c ratio has been hypothesized as a goal function for ecosystem flow networks christensen 1994 ulanowicz 1986 mageau et al 1998 network robustness is calculated by multiplying the fitness potential f of an ecosystem by t where f is derived from the a c ratio ulanowicz et al 2009 this metric has been proposed as a measure of system sustainability and empirically based flow networks are often seen to produce robustness statistics that fall within a window of vitality zorach and ulanowicz 2003 goerner et al 2009 mukherjee et al 2015 eld is a weighted metric that describes the average number of nodes that enter and leave each compartment in a network model this metric is derived from a and c as in ulanowicz et al 2009 detailed descriptions of the ascendency metric calculations have already been published ulanowicz 2004 patrıcio et al 2004 ulanowicz et al 2009 2 3 3 structural analysis we report one metric from structural analysis link density ld which is defined as the ratio of network edges links l to network nodes n such that ld l n ld is an unweighted metric that summarizes the number of connections per node in a network and we contrast ld with eld from ascendency analysis to highlight that structural analyses are not affected by the uncertainty analysis presented in this work 2 4 uncertainty analysis 2 4 1 calculations to determine the effect of varied uncertainty among flows on the ena results we used lim vézina and pace 1994 kones et al 2009 to constrain plausible models of the examined ecosystems based on investigator defined uncertainty ranges for each parameter models are considered to be plausible if they contain parameter values that all fall within the defined range for each edge and also meet mass balancing criteria to identify plausible models the scaling vector x in an under determined set of matrix equalities and inequalities that represent a network model eqs 6 and 7 can be constrained by edge data to define a plausible solution space 6 e x f 7 g x h here e is an n k matrix where n is the number of nodes and k is the number of network edges parameters including boundary flows this matrix contains the left side of the network mass balance equations that includes all of the model edge parameters while f is an n 1 vector containing the right side of these equations for example the hypothetical network in fig 4 can be represented in the form of this equality as image 1 note that the solution space for x can be further constrained to meet steady state requirements of some ena analyses by defining f 0 eq 8 similarly g is a 2 k k matrix containing a row for the upper and lower limit of each network parameter and h is a 2 k 1 vector containing the right sides the inequalities that constrain the solution space for x an example of eq 7 inequality for fig 4 is presented in the supplementary material to conduct the uncertainty analysis the inequality in eq 7 can be used to define the minimum and maximum possible values for each parameter the solution space for x in eqs 6 and 7 then defines plausible network models which have parameter values that all fall within the possible ranges for each edge and also meet mass balancing criteria a monte carlo sampling technique can then be applied to sample the solution space for x in this work a uniform sampling distribution was used for all network edges and sequential testing indicated that a sample size of 10 000 was sufficient to ensure adequate representation of the solution space for the models analyzed in this paper supplementary material the resultant network model sets can be evaluated using ena and the distributions of ena results can be compared statistically to aid in interpretation example code for this analysis can be found in the supplementary material for this work when uncertainty data are unavailable for a network flow model the analysis presented in this work can be modified to address the strength of results in models by using uniform levels of uncertainty across all parameters for example all edges in a network could be allowed to vary by up to 50 using a uniform distribution to determine if results from network analyses are robust to this level of uncertainty borrett et al 2016 sequential analyses with increasing degrees of uncertainty can be used to determine what amount of uncertainty is necessary to obscure ena results and thus can inform researchers about the relative strength of conclusions example code that includes uniform uncertainty analysis can be found in the supplementary material for this work 2 4 2 uncertainty analysis application we applied a data guided uncertainty analysis to the cfre case study to address the question from the perspective of whole network indicators are the two ecosystem models significantly different to accomplish this we restricted the model edge values based on the level of variation for each edge hines et al 2015 parameter values obtained through direct empirical measurements were restricted to fall within one standard deviation of the mean parameters that were obtained from the literature were restricted to be at least as variable as the most variable measured parameter 47 and parameters that were estimated through mass balance were allowed to vary by up to 100 this technique generated sets of plausible networks for each of the oligohaline and polyhaline models and we then applied ena to calculate distributions of tst fci ifi c a c robustness eld and ld at each site these distributions of results were compared visually as well as by calculating the 95 ranges confidence intervals of each data set to assess variability in the network metric results and to evaluate differences between the two models we applied a sequential uniform uncertainty analysis to the sundarban mangrove case study to address the questions how robust are the observed differences in model output to parameterization variability and at what level of parameter uncertainty are the differences no longer present we used a sequential increase in uniform uncertainty from 0 to 90 using 10 increments to evaluate the robustness of the network comparison for the same response variables selected in the cfre case study we used this approach because these models were originally presented without uncertainty data although the original models were nearly balanced we allowed the lim uncertainty algorithm to completely balance the models by constraining the solution space to include only balanced networks f 0 we then calculated the 95 confidence intervals of the distributions of results for the different uncertainty levels and compared these ranges to determine how much parameter uncertainty was necessary to obscure the differences between these two sites thus we were able to describe how robust the model conclusions are to parameterization uncertainty 3 results 3 1 cape fear river estuary case study the data guided uncertainty analysis for the cfre networks produced sets of 10 000 plausible network parameterizations for each site which were used to produce distributions of ena metrics for comparative assessment the set of values for plausible models returned from the analysis spanned over 99 of the allowable range for 94 of the network edges thus confirming that the sampling procedure covered the range of possible values for most model parameters sets of values for larger parameters were more likely to have constrained coverage of the allowable range than sets of values for smaller parameters fig 5 the 95 ranges of network metric distributions overlapped for tst fci ifi c a c robustness and eld table 1 therefore no significant differences were observed between these sites for example the distribution of plausible results for fci in the polyhaline network was completely contained within the distribution of results for the oligohaline network fig 6 no variation was observed in ld values across the set of plausible models confirming that this analysis does not affect structural metrics table 1 3 2 sundarban mangrove case study the sequence of uncertainty analyses applied to the sundarban mangrove networks generated an increase in the 95 confidence interval for each of the metrics examined as uniform uncertainty increased this observed increase was approximately linear resulting in an increase in the standard deviation of the distribution of results for the network response variables with only minor changes in the mean values fig 7 the lim sampling algorithm van den meersche et al 2009 was unable to find acceptable models below 19 and 7 uncertainty at the virgin and reclaimed sites respectively fig 7 unshaded areas the analyses indicated that the difference between four of the network metrics fci a c robustness and eld were obscured by less than 90 uniform uncertainty in edge weights while the remaining metrics tst ifi and c were not table 2 for example the 95 confidence interval for fci began to overlap between the two sites at 86 uniform uncertainty while no overlap was observed for distributions of c fig 7 shaded areas as with the cfre case study no variation was observed in ld values across the set of plausible models parameterizations table 2 4 discussion the methods presented in this work adapt and build upon existing computational techniques to create a tool set for assessing the impact of model uncertainty in ecosystem flow networks regardless of the model construction approach used fig 1 this is an important development not just for ecosystem flow networks but for flow networks in general because these techniques allow researchers to rigorously evaluate model results based on the available data and to draw stronger inferences this capability is essential if network modeling and ena are going to substantively inform environmental decision making de jonge et al 2012 longo et al 2015 here we discuss the results of the cfre and sundarban mangrove case studies to provide an example of the interpretation of these techniques highlight their limitations and recommend best practices for future network flow studies 4 1 cape fear river estuary uncertainty analysis the distributions of network metrics calculated from the sets of plausible model parameterizations generated ranges of results for each of the cfre sites comparison of the 95 confidence intervals of these distributions revealed that the differences observed in the network metrics between these sites were not statistically significant table 1 for example the original cfre models indicated that a difference of 0 03 18 was observed in fci at the oligohaline and polyhaline sites hines and borrett 2014 however the 95 range of the fci distributions overlapped between the two sites indicating that no statistically significant difference in fci was observed at the α 0 05 level fig 6 thus the lim uncertainty analysis presented in this work enables stronger statistical inference for comparative network modeling and represents an important step in the maturation of ena the finding that no statistically significant differences were observed between the oligohaline and polyhaline sites for the eight network metrics examined in this study is consistent with previous work on these models hines et al 2015 2016 did not detect differences in whole network metrics at these sites suggesting that the n cycle at these locations in the cfre functions in similar ways at the highest level of organization however examination of less aggregated network properties in these models with potential ecological importance such as the coupling of nitrogen cycling processes revealed statistically significant differences when compared using the same sets of plausible networks hines et al 2015 thus the uncertainty analysis presented in this work facilitates comparison of network models at multiple hierarchical levels of analysis which are frequently used in ena and can be useful for understanding complex ecosystems wu and david 2002 hines and borrett 2014 when data are available for the degree of uncertainty in each network edge as is the case with the cfre networks hines et al 2015 a data guided uncertainty analysis that accounts for varying degrees of uncertainty in specific parameters is preferable for examining distributions of plausible networks all of the network models produced by this technique are considered equally plausible because each network contains parameters that fall within the range of plausibility for each edge and the model meets thermodynamic balancing constraints kones et al 2009 small et al 2014 however it is relevant to report the original network models as the edge parameters of the original networks reflect the best estimations of the modelers fath et al 2007 and are subject to more scrutiny than the networks generated by the lim technique therefore we recommend that original model results be reported along side of plausible ranges as in fig 6 and table 1 4 2 sundarban mangrove uncertainty analysis the sequential uniform uncertainty analysis conducted on the sundarban mangrove networks highlights the utility of this tool for assessing the strength of ena conclusions in models without readily available uncertainty data the analysis generated an approximately linear increase in the 95 distribution of results with increasing uniform uncertainty likely due to the linear constraints of the lim technique the results of this analysis provid evidence that the differences in tst fci ifi and c reported for these networks by ray 2008 are robust findings by demonstrating that the parameter uncertainty necessary to obscure these differences was relatively high 86 table 2 these are noteworthy findings because these network metrics can be associated with healthy ecosystem functioning finn 1980 wulff and ulanowicz 1989 fath and patten 1999 ulanowicz 2004 ulanowicz et al 2009 and the uncertainty analysis strengthens the evidence provided by these models that anthropogenic influences may alter these ecosystem properties differences in a c robustness and eld were not as resistant to uniform uncertainty and were obscured at 37 45 and 40 uncertainty respectively while the amount of uniform uncertainty considered to be significant is somewhat arbitrary the analysis is able to indicate that differences in these metrics are less robust than for tst fci ifi and c in the sundarban mangrove networks this type of uncertainty analysis technique has a broad range of applications across network analysis for example zhang et al 2014 used a sequential uncertainty analysis to examine parameter uncertainty in a hydrological model and borrett et al 2016 applied a uniform uncertainty of 50 to a database of 73 published trophic and biogeochemical cycling networks to evaluate six hypothesized ecosystem properties in a separate example chaalali et al 2015 used a uniform uncertainty analysis to show the plausible variation in network metrics for a model of the bay of biscay food web and compared these to previous estimates from an ecopath based model while the inclusion of uncertainty data is preferable when presenting network models many previously published networks do not include uncertainty data this tool allows researchers to assess the degree of certainty they have in network results even if data on parameterization variability are not available and can be used to generate a range of potential model outcomes or to compare results from multiple networks 4 3 limitations and future directions there are several limitations to the uncertainty analysis techniques described in this manuscript that require further development first these analyses apply only to the edge weights in network models while many ena algorithms rely solely on these edge weights some such as storage analysis matis and patten 1981 ma and kazanci 2014 fath 2012 rely on the magnitude of energy or matter stored in each node the lim analyses presented here cannot address results based on node storage biomass and different techniques are required to evaluate uncertainty in these forms of ena second the methodologies presented in this work do not currently include covariance constraints and are restricted to linear interactions for example in the lim technique plausible edge values are selected based only on the mass balance constraints of the model structure no mechanism is in place yet to link the values of edges that have been shown empirically to co vary third this analysis currently uses a uniform sampling distribution while an unbiased uniform distribution can provide a conservative estimate of the plausible range of results other sampling distributions may be more appropriate and future work is needed to incorporate these distributions into the analysis finally these techniques do not allow for assessment of structural changes in network models although the structure and aggregation of models is known to be influential on model results dame and christian 2006 abarca arenas and ulanowicz 2002 allesina et al 2005 patonai and jordán 2017 developing uncertainty approaches for structural analysis such as those presented in du and ligmann zielinska 2015 or based on probabilistic approaches such as in poisot et al 2015 is an important next step as ena matures despite these limitations the software function and uncertainty analysis presented in this work provide investigators with readily accessible tools to begin to standardize the evaluation of uncertainty in network flow models 4 4 best practices for ecosystem network modeling uncertainty analyses for ecosystem flow networks are uncommon in the literature despite advances in technology over the past several decades that make these analyses more accessible here we suggest three criteria for best practices when presenting and analyzing ecosystem flow networks to help standardize these analyses in the future 1 we recommend that information quantifying the uncertainty and error in all network fluxes and storages be presented as a standard part of ecosystem network models data guided uncertainty analyses are preferable to uniform uncertainty analyses because they provide more informed insight into the strength of model results 2 we recommend that flow networks based on phenomenological observations be evaluated using lim to generate sets of plausible models for analysis advances in technical capabilities over the past several decades have enabled lim to address many of the challenges of uncertainty analysis for these networks including simultaneous perturbation of multiple network edges and non uniform distribution of uncertainty throughout the models finally 3 we recommend that the 95 confidence intervals from plausible network models be used to make statistical comparisons between network models as these types of comparisons provide more useful information about the model results than traditional comparisons of static networks funding this work was supported by the us national science foundation deb1020944 and oce0851435 acknowledgements we acknowledge and thank dr oksana buzhdygan dr ursula scharler and dr camille de la vega for the help they provided with beta testing of the enauncertainty software function we also thank the anonymous reviewers who helped to improve this paper appendix a supplementary data the following are the supplementary data related to this article emands supplementary material data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 011 
26438,uncertainty analyses show how variability inherent in model parameters affects model outcomes while conducting uncertainty analyses is considered best practice technical and conceptual challenges limit applications for network models this work adapts linear inverse modeling lim techniques to conduct uncertainty analysis on ecosystem flow networks which represent the movement of energy matter through ecosystems we present a new r function for the enar package to perform the analysis and use two case studies of previously published networks to demonstrate the power of this approach the first case study examines a system with available flow uncertainty data to show how lim uncertainty analysis can support stronger statistical inference the second case study examines a system without available uncertainty data to illustrate how these techniques can determine the relative strength of model conclusions even without quantitative data the tools presented here represent an important step in the maturation of ecological network analysis keywords ecological network analysis linear inverse modeling uncertainty analysis network ecology estuaries food web software and data availability section name of software enar developer systems ecology and ecoinformatics laboratory contact address department of biology and marine biology university of north carolina wilmington 601 s college rd wilmington nc 28303 telephone 1 910 962 2411 e mail enar maintainer gmail com year first available 2012 hardware required pc mac linux software required r availability public open source freely available from www r project org language r 1 introduction researchers analysts and managers use models to test hypotheses hilborn and mangel 1997 jørgensen and bendoricchio 2001 quinn and bates 2011 jones and lennon 2014 inform recommendations costanza and ruth 1998 miller 2012 and draw inferences about ecological relationships from data johnson and omland 2004 lafferty et al 2015 while a variety of model types can be used to evaluate the state and function of systems weisberg 2012 the application of network models to accomplish this goal has rapidly increased over the past several decades borrett et al 2014 ecosystem networks have been used to investigate a variety of topics ranging from the effects of predators in trophic cascades wallach et al 2017 to identifying species interactions at multiple spatial scales ovaskainen et al 2015 to evaluating the overall sustainability of ecosystems ulanowicz et al 2009 one common class of ecosystem network model flow networks characterizes budgets for the movement of energy matter through ecosystems using nodes that represent resource pools and edges that represent the transfer of energy matter between resource pools these models can describe a diverse range of systems and interactions including food webs niquil et al 1999 dunne et al 2013 biogeochemical cycles christian and thomas 2003 borrett et al 2016 and systems containing mostly non living components such as urban metabolism networks samaniego and moses 2008 zhang 2013 zhang et al 2016 for example a food web network typically consists of nodes that are species or groups of species and edges that represent the transfer of matter through physical consumption pascual and dunne 2005 borrett et al 2016 ecosystem flow networks let researchers conduct ecological network analysis ena which quantifies and tracks the organization and movement of energy matter in a system of interest hannon 1973 patten et al 1976 ulanowicz 1986 these models and analyses can be used to evaluate specific components and interactions within a system as well as the state of whole ecosystems ena evaluation of network models can reveal hidden relationships that result from compound and indirect interactions bondavalli and ulanowicz 1999 borrett et al 2010 jordán and scheuring 2002 schückel et al 2015 for example christian and luczkovich 1999 used a trophic network of the relationships among species in st marks bay florida to calculate the effective trophic levels of the ecosystem components considering all of the interactions in the system in a separate example schramski et al 2006 used ena of a nitrogen n cycling network model to quantify the role each n pool played in regulating the movement of n through an estuary these types of analyses grant researchers insight into some of the complex interactions that occur within ecosystems and can facilitate monitoring of ecosystem indicators coll and steenbeek 2017 but their usefulness is dependent on the accuracy and precision of the parameters used to build the networks although understanding how imprecisions and uncertainties in parameterization affect network models is essential for appropriately interpreting ena results procedures to directly address this question are underdeveloped and often overlooked in the assessment of ecosystem flow networks ulanowicz 2004 dame and christian 2006 fath et al 2007 uncertainty analyses which quantify how the combined error in all model parameters propagates through model calculations to generate uncertainty in outputs crosetto and tarantola 2001 can be useful tools for this task but can be difficult to apply to network flow models the scarcity of uncertainty analyses in network ecology literature may be a result of the phenomenological approach that is often applied to network model construction ulanowicz 1992 2012 as opposed to the more mechanistic approaches of other model types such as building ordinary differential equations ecosystem flow networks are often parameterized by synthesizing multiple experimentally observed flow and biomass estimations together leading to ambiguity about the way that the errors inherent in these measurements interact to affect the final model outputs despite these difficulties forms of uncertainty analyses have been successfully applied to network flow models for example dame and christian 2006 advocate varying network inputs and structure to identify uncertainty in results but point out that adequate methodologies for thorough uncertainty analyses are lacking borrett and osidele 2007 used monte carlo simulations to evaluate the robustness of network properties in 122 plausible parameterizations of a phosphorous transport network for lake sidney lanier ga usa and kaufman and borrett 2010 analyzed these same 122 phosphorous transport networks to quantify the variability of 18 network analysis metrics given the model uncertainty others have applied uncertainty analyses at fixed levels of error to assess the robustness of ena results borrett and salas 2010 for example salas and borrett 2011 applied an uncertainty analysis that examined the range of ena outputs after perturbing all network edges by 5 and re balancing the models by altering boundary output flows other approaches such as the sensitivity based analysis used by ayers and scharler 2011 to evaluate uncertainty in network models of the kwazulu natal bight south africa or the perturbation approach used by mukherjee et al 2015 to evaluate network indicators under three different scenarios representing biomass changes have also been applied to address uncertainty in ecosystem flow networks linear inverse modeling lim has emerged as a useful tool for evaluating uncertainty in network models and constructing plausible network parameterizations vézina and platt 1988 vézina and pace 1994 kones et al 2009 for example taffi et al 2015 used lim to fill data gaps in a food web of the adriatic sea before conducting network analyses recently guesnet et al 2015 released software for matlab that uses lim along with latin hypercube and monte carlo sampling techniques to generate uncertainty estimations for the ena routines included in the ecopath with ecosim software christensen et al 2005 although these advances represent important steps towards incorporating uncertainty analyses into ena these techniques remain under recognized and underutilized furthermore there are several approaches to model construction for ena including ecopath with ecosim christensen et al 2005 lim vézina and platt 1988 and phenomenological parameterization ulanowicz 1986 and the best modeling approach for each problem may vary based on data availability and system type as the use of network flow models continues to expand standardizing approaches for uncertainty analysis in these models is necessary to ensure that ena results are interpreted consistently and appropriately further recent work has called for increased applications of ena to inform ecosystem assessment and management de jonge et al 2012 longo et al 2015 and quantifying the uncertainty in network flow model results is essential if these applications are to be useful in this work we present a generalized methodology and software function to conduct uncertainty analysis on ecosystem flow networks and to make this analysis more accessible to researchers we first adapt lim techniques to introduce an uncertainty analysis step into the ena workflow fig 1 and provide a new r function to accomplish this task we then demonstrate the power of these analyses using two case studies of previously published models that have different availability of uncertainty data to highlight the diverse applicability of our approach in the first case study we introduce a methodology for uncertainty analysis that can be applied to models with specified data on the uncertainty of edge parameters this data guided uncertainty analysis enables hypothesis testing to detect statistically significant differences in the ena metrics of models given the uncertainty in network parameterization in the second case study we demonstrate how this uncertainty analysis can be modified to accommodate networks where uncertainty data are not readily available we applied an increasing amount of uniform uncertainty across the network flows to quantify how much variability was required to eliminate observed differences between networks these examples highlight the power of the lim uncertainty analyses presented in this work to both enable stronger inferences when comparing network flow models and provide insight into the robustness of network modeling results 2 materials and methods 2 1 workflow we present a software function and modified workflow for ena to make lim uncertainty analysis for network models accessible to researchers fig 1 this workflow introduces an uncertainty step between the initial construction of network flow models and the application of ena while fath et al 2007 outline guidelines for the construction of ecosystem flow networks several approaches to this task are available each of these approaches requires substantial input data and which method researchers use may depend on data availability for example if biomass measurements and consumption data are available ecopath with ecosim may be the best tool for network construction christensen et al 2005 while if empirical measurements of nutrient flows are available phenomenological parameterization may be the preferred approach ulanowicz 1986 further the trophic modeling approach used in ecopath would not make sense for some applications such as urban metabolisms the uncertainty analysis techniques described in this work differ from other approaches such a those in guesnet et al 2015 in that they are applicable regardless of model construction technique and are conducted after model construction once a network model is constructed uncertainty data for each edge weight can be combined with the original model through a lim uncertainty analysis this uncertainty analysis can be performed by the enauncertainty function which is available in the supplemental material to this work and is included in the enar v 3 0 1 package for r borrett and lau 2014 lau et al 2017 this software function first uses the original network model along with uncertainty data tables to generate the inputs needed for lim it then applies a lim analysis to generate plausible model sets using the limsolve package for r soetaert et al 2009 van den meersche et al 2009 next lim results are converted back into a format readable by enar and are returned to the user as a set of plausible model parameterizations for use in ena fig 1 the position of this uncertainty analysis prior to ena in the workflow is important because it implies that multiple different types of ena can be applied to the same set of plausible model parameterizations for example ena analyses including structure flow storage impact control environ and ascendency analyses fath and patten 1999 schramski et al 2006 ulanowicz 2004 ulanowicz et al 2009 could all be conducted to evaluate different network metrics in the same parameterization set fig 1 applying ena across a set of plausible network parameterizations results in distributions of ena results which can than be evaluated and statistically compared example code for a start to finish workflow that loads network models incorporates uncertainty data conducts an lim uncertainty analysis applies ena to a set of plausible models and evaluates results using the enar software is available in the supplementary material for this work 2 2 case studies each of the two case studies used in this work compared pairs of previously published networks representing sites within different estuarine ecosystems the first case study applied a data guided uncertainty analysis to compare two n cycling networks in the cape fear river estuary cfre nc usa hines et al 2012 2015 the second case study modified the uncertainty analysis to inform a second comparative analysis of two food web networks for the sundarban mangrove ecosystem india ray et al 2000 ray 2008 each pair of models had similar network topology structure to facilitate their comparison but each model was parameterized with different edge weights based on site specific data to quantify differences in ecosystem functioning between the sites all of the models used for case studies in this work are available as part of the enar package for r borrett and lau 2014 2 2 1 cape fear river estuary models the two networks for the cfre case study were constructed at sites with different salinity regimes to investigate the impact of seawater intrusion on the estuarine n cycle one at a more freshwater oligohaline site and one at a more saline polyhaline site each of these models tracks the movement of n through the ecosystem in units of nmol n cm 3d 1 these networks consist of eight n pools nodes connected by 38 transformations edges these edges represented the movement of n species between the standing stock pools as well as boundary fluxes into and out of the system the n pools included ammonium nitrate nitrite microbial biomass and organic n in both the water column w nh4 w no x w m w on and sediment s nh4 s no x s m s on respectively fig 2 the edge weights were parameterized using a combination of direct measurements and literature values for each site along with mass balance estimation when necessary hines et al 2012 2015 the cfre network models were compared in previous work to quantify coupling of microbial n cycling processes and to estimate the effects of seawater intrusion on an estuarine n cycle hines et al 2012 2015 2016 2 2 2 sundarban mangrove models the food web networks used for the second case study represent trophic interactions on mudflats for two islands in the sundarban mangrove ecosystem one virgin undisturbed and one reclaimed anthropogenically altered each of these networks consist of 14 nodes representing aggregated groups of species such as detritus bacteria phytoplankton and top predators fig 3 these nodes are connected by 43 internal edges and 38 boundary edges that represent the transfer of energy matter by trophic interactions and physical transport through five trophic levels in units of kcal m 2 y 1 ray et al 2000 previously published comparisons of these networks evaluated the potential impacts of human activity on the flow of energy through this mangrove ecosystem ray 2008 2 3 ecological network analysis to demonstrate the versatility and limitations of applications for the uncertainty techniques presented in this work we applied three forms of ena to our case studies flow analysis ascendency analysis and structural analysis flow analysis quantifies and partitions the movement of energy matter in networks into different kinds of pathways fath and patten 1999 schramski et al 2011 while ascendency analysis assesses flow diversity and provides a measure of the overall state of a system by quantifying the activity and organization of the network flows patrıcio et al 2004 ulanowicz 2000 morris et al 2005 structural analysis evaluates the topology of network models to reveal pathways and cycles fath and patten 1999 and was included to demonstrate that the uncertainty analysis methods presented in this work do not impact structural metrics here we provide a brief description of eight network metrics selected from among these three analyses to provide example applications of the lim uncertainty analysis all ena metrics were calculated using the enar v 3 0 1 package for r borrett and lau 2014 lau et al 2017 2 3 1 flow analysis we examined three commonly calculated network metrics from flow analysis total system through flow tst finn s cycling index fci and indirect flow intensity ifi tst is a measure of the amount of material moving through a system and is calculated as the sum of all of the internal flows and boundary inputs eq 1 1 tst internal flows inputs tst has the same units as the network edges and is sometimes interpreted as the size of a system finn 1976 fath and patten 1999 this metric is central to the calculation of the other flow metrics reported in this manuscript as these analyses partition tst to reveal system behavior fci is the fraction of tst generated by recycling finn 1976 1980 it is calculated as 2 fci cycled flow tst where cycled flow is defined as material that exits a node then passes through the same node again before exiting the network finn 1980 eq 2 the fci statistic provides a measure of how leaky a network is to the surrounding environment this metric has implications for the retention time of material in a system baird and ulanowicz 1993 and has also been interpreted as an indication of system health wulff and ulanowicz 1989 the ifi metric reports the proportion of tst driven by fluxes over a path length greater than one crossing two or more edges indirect flow as in eq 3 borrett et al 2006 salas and borrett 2011 3 ifi indirect flow tst the proportion of flow in ecosystem networks resulting from indirect flow has been used to quantify the strength of competition in trophic networks and emphasize the importance of systems based analyses fath and grant 2007 2 3 2 ascendency analysis ascendency analyses quantify ecosystem organization and development by applying information theoretic metrics to network flow models ulanowicz 1986 we examined four ascendency analysis metrics development capacity c the ascendency to capacity ratio a c network robustness and effective link density eld c describes the flow diversity that is possible in a the system and is calculated as in eq 4 by multiplying the sum of all flows t by the diversity of flows in a network defined by shannon s information measure h macarthur 1955 ulanowicz 2004 4 c t h h has informational units of bits therefore c has units of edge flow times bits c is the theoretical maximum flow diversity that a system can attain ulanowicz et al 2009 the a c ratio quantifies how diverse the flows are in a network ascendency a relative to maximum possible capacity c ulanowicz 1980 patrıcio et al 2004 ascendency is calculated as the product of t and average mutual information a m i which is a measure of shared information eq 5 ulanowicz et al 2009 5 a t a m i a m i has units of bits thus a has the same units as c and a c is unitless optimization of the a c ratio has been hypothesized as a goal function for ecosystem flow networks christensen 1994 ulanowicz 1986 mageau et al 1998 network robustness is calculated by multiplying the fitness potential f of an ecosystem by t where f is derived from the a c ratio ulanowicz et al 2009 this metric has been proposed as a measure of system sustainability and empirically based flow networks are often seen to produce robustness statistics that fall within a window of vitality zorach and ulanowicz 2003 goerner et al 2009 mukherjee et al 2015 eld is a weighted metric that describes the average number of nodes that enter and leave each compartment in a network model this metric is derived from a and c as in ulanowicz et al 2009 detailed descriptions of the ascendency metric calculations have already been published ulanowicz 2004 patrıcio et al 2004 ulanowicz et al 2009 2 3 3 structural analysis we report one metric from structural analysis link density ld which is defined as the ratio of network edges links l to network nodes n such that ld l n ld is an unweighted metric that summarizes the number of connections per node in a network and we contrast ld with eld from ascendency analysis to highlight that structural analyses are not affected by the uncertainty analysis presented in this work 2 4 uncertainty analysis 2 4 1 calculations to determine the effect of varied uncertainty among flows on the ena results we used lim vézina and pace 1994 kones et al 2009 to constrain plausible models of the examined ecosystems based on investigator defined uncertainty ranges for each parameter models are considered to be plausible if they contain parameter values that all fall within the defined range for each edge and also meet mass balancing criteria to identify plausible models the scaling vector x in an under determined set of matrix equalities and inequalities that represent a network model eqs 6 and 7 can be constrained by edge data to define a plausible solution space 6 e x f 7 g x h here e is an n k matrix where n is the number of nodes and k is the number of network edges parameters including boundary flows this matrix contains the left side of the network mass balance equations that includes all of the model edge parameters while f is an n 1 vector containing the right side of these equations for example the hypothetical network in fig 4 can be represented in the form of this equality as image 1 note that the solution space for x can be further constrained to meet steady state requirements of some ena analyses by defining f 0 eq 8 similarly g is a 2 k k matrix containing a row for the upper and lower limit of each network parameter and h is a 2 k 1 vector containing the right sides the inequalities that constrain the solution space for x an example of eq 7 inequality for fig 4 is presented in the supplementary material to conduct the uncertainty analysis the inequality in eq 7 can be used to define the minimum and maximum possible values for each parameter the solution space for x in eqs 6 and 7 then defines plausible network models which have parameter values that all fall within the possible ranges for each edge and also meet mass balancing criteria a monte carlo sampling technique can then be applied to sample the solution space for x in this work a uniform sampling distribution was used for all network edges and sequential testing indicated that a sample size of 10 000 was sufficient to ensure adequate representation of the solution space for the models analyzed in this paper supplementary material the resultant network model sets can be evaluated using ena and the distributions of ena results can be compared statistically to aid in interpretation example code for this analysis can be found in the supplementary material for this work when uncertainty data are unavailable for a network flow model the analysis presented in this work can be modified to address the strength of results in models by using uniform levels of uncertainty across all parameters for example all edges in a network could be allowed to vary by up to 50 using a uniform distribution to determine if results from network analyses are robust to this level of uncertainty borrett et al 2016 sequential analyses with increasing degrees of uncertainty can be used to determine what amount of uncertainty is necessary to obscure ena results and thus can inform researchers about the relative strength of conclusions example code that includes uniform uncertainty analysis can be found in the supplementary material for this work 2 4 2 uncertainty analysis application we applied a data guided uncertainty analysis to the cfre case study to address the question from the perspective of whole network indicators are the two ecosystem models significantly different to accomplish this we restricted the model edge values based on the level of variation for each edge hines et al 2015 parameter values obtained through direct empirical measurements were restricted to fall within one standard deviation of the mean parameters that were obtained from the literature were restricted to be at least as variable as the most variable measured parameter 47 and parameters that were estimated through mass balance were allowed to vary by up to 100 this technique generated sets of plausible networks for each of the oligohaline and polyhaline models and we then applied ena to calculate distributions of tst fci ifi c a c robustness eld and ld at each site these distributions of results were compared visually as well as by calculating the 95 ranges confidence intervals of each data set to assess variability in the network metric results and to evaluate differences between the two models we applied a sequential uniform uncertainty analysis to the sundarban mangrove case study to address the questions how robust are the observed differences in model output to parameterization variability and at what level of parameter uncertainty are the differences no longer present we used a sequential increase in uniform uncertainty from 0 to 90 using 10 increments to evaluate the robustness of the network comparison for the same response variables selected in the cfre case study we used this approach because these models were originally presented without uncertainty data although the original models were nearly balanced we allowed the lim uncertainty algorithm to completely balance the models by constraining the solution space to include only balanced networks f 0 we then calculated the 95 confidence intervals of the distributions of results for the different uncertainty levels and compared these ranges to determine how much parameter uncertainty was necessary to obscure the differences between these two sites thus we were able to describe how robust the model conclusions are to parameterization uncertainty 3 results 3 1 cape fear river estuary case study the data guided uncertainty analysis for the cfre networks produced sets of 10 000 plausible network parameterizations for each site which were used to produce distributions of ena metrics for comparative assessment the set of values for plausible models returned from the analysis spanned over 99 of the allowable range for 94 of the network edges thus confirming that the sampling procedure covered the range of possible values for most model parameters sets of values for larger parameters were more likely to have constrained coverage of the allowable range than sets of values for smaller parameters fig 5 the 95 ranges of network metric distributions overlapped for tst fci ifi c a c robustness and eld table 1 therefore no significant differences were observed between these sites for example the distribution of plausible results for fci in the polyhaline network was completely contained within the distribution of results for the oligohaline network fig 6 no variation was observed in ld values across the set of plausible models confirming that this analysis does not affect structural metrics table 1 3 2 sundarban mangrove case study the sequence of uncertainty analyses applied to the sundarban mangrove networks generated an increase in the 95 confidence interval for each of the metrics examined as uniform uncertainty increased this observed increase was approximately linear resulting in an increase in the standard deviation of the distribution of results for the network response variables with only minor changes in the mean values fig 7 the lim sampling algorithm van den meersche et al 2009 was unable to find acceptable models below 19 and 7 uncertainty at the virgin and reclaimed sites respectively fig 7 unshaded areas the analyses indicated that the difference between four of the network metrics fci a c robustness and eld were obscured by less than 90 uniform uncertainty in edge weights while the remaining metrics tst ifi and c were not table 2 for example the 95 confidence interval for fci began to overlap between the two sites at 86 uniform uncertainty while no overlap was observed for distributions of c fig 7 shaded areas as with the cfre case study no variation was observed in ld values across the set of plausible models parameterizations table 2 4 discussion the methods presented in this work adapt and build upon existing computational techniques to create a tool set for assessing the impact of model uncertainty in ecosystem flow networks regardless of the model construction approach used fig 1 this is an important development not just for ecosystem flow networks but for flow networks in general because these techniques allow researchers to rigorously evaluate model results based on the available data and to draw stronger inferences this capability is essential if network modeling and ena are going to substantively inform environmental decision making de jonge et al 2012 longo et al 2015 here we discuss the results of the cfre and sundarban mangrove case studies to provide an example of the interpretation of these techniques highlight their limitations and recommend best practices for future network flow studies 4 1 cape fear river estuary uncertainty analysis the distributions of network metrics calculated from the sets of plausible model parameterizations generated ranges of results for each of the cfre sites comparison of the 95 confidence intervals of these distributions revealed that the differences observed in the network metrics between these sites were not statistically significant table 1 for example the original cfre models indicated that a difference of 0 03 18 was observed in fci at the oligohaline and polyhaline sites hines and borrett 2014 however the 95 range of the fci distributions overlapped between the two sites indicating that no statistically significant difference in fci was observed at the α 0 05 level fig 6 thus the lim uncertainty analysis presented in this work enables stronger statistical inference for comparative network modeling and represents an important step in the maturation of ena the finding that no statistically significant differences were observed between the oligohaline and polyhaline sites for the eight network metrics examined in this study is consistent with previous work on these models hines et al 2015 2016 did not detect differences in whole network metrics at these sites suggesting that the n cycle at these locations in the cfre functions in similar ways at the highest level of organization however examination of less aggregated network properties in these models with potential ecological importance such as the coupling of nitrogen cycling processes revealed statistically significant differences when compared using the same sets of plausible networks hines et al 2015 thus the uncertainty analysis presented in this work facilitates comparison of network models at multiple hierarchical levels of analysis which are frequently used in ena and can be useful for understanding complex ecosystems wu and david 2002 hines and borrett 2014 when data are available for the degree of uncertainty in each network edge as is the case with the cfre networks hines et al 2015 a data guided uncertainty analysis that accounts for varying degrees of uncertainty in specific parameters is preferable for examining distributions of plausible networks all of the network models produced by this technique are considered equally plausible because each network contains parameters that fall within the range of plausibility for each edge and the model meets thermodynamic balancing constraints kones et al 2009 small et al 2014 however it is relevant to report the original network models as the edge parameters of the original networks reflect the best estimations of the modelers fath et al 2007 and are subject to more scrutiny than the networks generated by the lim technique therefore we recommend that original model results be reported along side of plausible ranges as in fig 6 and table 1 4 2 sundarban mangrove uncertainty analysis the sequential uniform uncertainty analysis conducted on the sundarban mangrove networks highlights the utility of this tool for assessing the strength of ena conclusions in models without readily available uncertainty data the analysis generated an approximately linear increase in the 95 distribution of results with increasing uniform uncertainty likely due to the linear constraints of the lim technique the results of this analysis provid evidence that the differences in tst fci ifi and c reported for these networks by ray 2008 are robust findings by demonstrating that the parameter uncertainty necessary to obscure these differences was relatively high 86 table 2 these are noteworthy findings because these network metrics can be associated with healthy ecosystem functioning finn 1980 wulff and ulanowicz 1989 fath and patten 1999 ulanowicz 2004 ulanowicz et al 2009 and the uncertainty analysis strengthens the evidence provided by these models that anthropogenic influences may alter these ecosystem properties differences in a c robustness and eld were not as resistant to uniform uncertainty and were obscured at 37 45 and 40 uncertainty respectively while the amount of uniform uncertainty considered to be significant is somewhat arbitrary the analysis is able to indicate that differences in these metrics are less robust than for tst fci ifi and c in the sundarban mangrove networks this type of uncertainty analysis technique has a broad range of applications across network analysis for example zhang et al 2014 used a sequential uncertainty analysis to examine parameter uncertainty in a hydrological model and borrett et al 2016 applied a uniform uncertainty of 50 to a database of 73 published trophic and biogeochemical cycling networks to evaluate six hypothesized ecosystem properties in a separate example chaalali et al 2015 used a uniform uncertainty analysis to show the plausible variation in network metrics for a model of the bay of biscay food web and compared these to previous estimates from an ecopath based model while the inclusion of uncertainty data is preferable when presenting network models many previously published networks do not include uncertainty data this tool allows researchers to assess the degree of certainty they have in network results even if data on parameterization variability are not available and can be used to generate a range of potential model outcomes or to compare results from multiple networks 4 3 limitations and future directions there are several limitations to the uncertainty analysis techniques described in this manuscript that require further development first these analyses apply only to the edge weights in network models while many ena algorithms rely solely on these edge weights some such as storage analysis matis and patten 1981 ma and kazanci 2014 fath 2012 rely on the magnitude of energy or matter stored in each node the lim analyses presented here cannot address results based on node storage biomass and different techniques are required to evaluate uncertainty in these forms of ena second the methodologies presented in this work do not currently include covariance constraints and are restricted to linear interactions for example in the lim technique plausible edge values are selected based only on the mass balance constraints of the model structure no mechanism is in place yet to link the values of edges that have been shown empirically to co vary third this analysis currently uses a uniform sampling distribution while an unbiased uniform distribution can provide a conservative estimate of the plausible range of results other sampling distributions may be more appropriate and future work is needed to incorporate these distributions into the analysis finally these techniques do not allow for assessment of structural changes in network models although the structure and aggregation of models is known to be influential on model results dame and christian 2006 abarca arenas and ulanowicz 2002 allesina et al 2005 patonai and jordán 2017 developing uncertainty approaches for structural analysis such as those presented in du and ligmann zielinska 2015 or based on probabilistic approaches such as in poisot et al 2015 is an important next step as ena matures despite these limitations the software function and uncertainty analysis presented in this work provide investigators with readily accessible tools to begin to standardize the evaluation of uncertainty in network flow models 4 4 best practices for ecosystem network modeling uncertainty analyses for ecosystem flow networks are uncommon in the literature despite advances in technology over the past several decades that make these analyses more accessible here we suggest three criteria for best practices when presenting and analyzing ecosystem flow networks to help standardize these analyses in the future 1 we recommend that information quantifying the uncertainty and error in all network fluxes and storages be presented as a standard part of ecosystem network models data guided uncertainty analyses are preferable to uniform uncertainty analyses because they provide more informed insight into the strength of model results 2 we recommend that flow networks based on phenomenological observations be evaluated using lim to generate sets of plausible models for analysis advances in technical capabilities over the past several decades have enabled lim to address many of the challenges of uncertainty analysis for these networks including simultaneous perturbation of multiple network edges and non uniform distribution of uncertainty throughout the models finally 3 we recommend that the 95 confidence intervals from plausible network models be used to make statistical comparisons between network models as these types of comparisons provide more useful information about the model results than traditional comparisons of static networks funding this work was supported by the us national science foundation deb1020944 and oce0851435 acknowledgements we acknowledge and thank dr oksana buzhdygan dr ursula scharler and dr camille de la vega for the help they provided with beta testing of the enauncertainty software function we also thank the anonymous reviewers who helped to improve this paper appendix a supplementary data the following are the supplementary data related to this article emands supplementary material data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 011 
26439,we compare the ability of eight machine learning models elastic net gradient boosting kernel k nearest neighbors two variants of support vector machines m5 cubist random forest and a meta learning ensemble m5 cubist model and four baseline models ordinary kriging a unit area discharge model and two variants of censored regression to generate estimates of the annual minimum 7 day mean streamflow with an annual exceedance probability of 90 7q10 at 224 unregulated sites in south carolina georgia and alabama usa the machine learning models produced substantially lower cross validation errors compared to the baseline models the meta learning m5 cubist model had the lowest root mean squared error of 26 72 cubic feet per second partial dependence plots show that 7q10s are likely moderated by late summer and early fall precipitation and the infiltration capacity of basin soils keywords low streamflow ungaged basins 7q10 machine learning censored regression variable importance 1 introduction water managers rely on streamflow data to allocate water resources define the dilution potential of catchments set ecological streamflow limits and ensure sustainable watershed planning razavi and coulibaly 2012 knight et al 2014 kapo et al 2015 however many streams do not have observed streamflow data and water managers must depend on the streamflow estimates from various prediction models mishra and coulibaly 2009 razavi and coulibaly 2012 luce 2014 improving the predictions of streamflow in ungaged basins has been a primary objective for hydrologists for decades and international initiatives have resulted in rapid advances in this field sivapalan et al 2003 hrachowitz et al 2013 blöschl 2016 the two primary modeling strategies for predicting streamflow response in ungaged basins are 1 deterministic physically based models i e calculating streamflow based on distributed hydrologic parameters and 2 statistical regionalization i e using regression models to transfer hydrologic information from gaged to ungaged basins razavi and coulibaly 2012 farmer and vogel 2016 this current paper focuses on the statistical regionalization of a low streamflow statistic the annual minimum 7 day mean streamflow with an annual exceedance probability of 90 7q10 a stream s low flow refers to the amount of water flowing in a stream during prolonged periods of little to no rainfall during an average non drought year the low flow regime for a particular stream is controlled by the physical characteristics of its basin and the local climate smakhtin 2001 the 7q10 statistic describes a basin s expected low flow and provides a way to compare directly the low flow regimes of different basins this statistic is commonly used to determine permitted point source pollutant levels in streams ames 2006 there are a number of other important low flow metrics not discussed in this paper several examples are the 7q10 for a particular season or month the annual minimum 7 day mean streamflow with an annual exceedance probability of 50 7q2 mean annual minimum median september streamflow and ecologically derived values knight et al 2014 kormos et al 2016 murphy et al 2013 raines and asquith 1997 the contribution of this research is the comparison of statistical estimation techniques the choice of the specific response variable would not change the structure of the analysis but we cannot conjecture how specific models would perform for a different target variable low flow regionalization methods attempt to predict low flow metrics in ungaged basins by leveraging the correlation between basin characteristics and streamflow at gaged basins razavi and coulibaly 2012 the primary goal of 7q10 regionalization is accurate predictions and not mechanistic explanations of what controls the 7q10 and this distinction between prediction and explanation should guide the statistical analysis shmueli 2010 regardless of outcome goal or the type of model used all hydrologic models require assumptions deterministic models for example assume that the physical relationships between parts of a hydrologic system are adequately captured by a set of static functions and decision rules while stochastic models may depend on assumptions about the probabilistic constraints on parameters i e priors the choice of the likelihood and cost functions the numerical methods used for parameter estimation e g gradient descent maximum likelihood numerical integration etc and choices about data preprocessing and transformation furthermore hydrologic models often assume some level of stationarity lins and cohn 2011 these assumptions can have significant effects on the applicability of model results and researchers must acknowledge how their model design choices propagate into conclusions drawn from the model this paper evaluates the predictive performance of various association based models e g linear regression models that leverage the covariance structure between variables to make inferences and predictions association based models have proved to be a useful engineering tool for predicting 7q10s and have become increasingly sophisticated in the last 30 years hrachowitz et al 2013 regression methods have evolved from simple ordinary least squares riggs 1973 thomas and benson 1970 hardison 1971 to time series weighted least squares tasker 1980 generalized least squares gls stedinger and tasker 1985 censored regression kroll and stedinger 1999 two step gls logistic regression funkhouser et al 2008 truncated models and catchment clustering methods law et al 2009 there has also been an increased application of geostatistical low flow regionalization methods primarily ordinary kriging top kriging and physiographical spaced based interpolation castiglioni et al 2009 2011 despite the recent methodological advances mentioned above few studies have explored machine learning methods to predict low flow metrics in ungaged basins ouarda and shu 2009 used an ensemble of artificial neural networks for predicting various low flow metrics in canada laaha and blöschl 2006 used regression trees to predict q95s in austria schnier and cai 2014 used model tree ensembles to predict a complete flow duration curve fdc for streams in illinois and texas and booker and woods 2014 used random forest models to predict several components of a fdc in new zealand these studies contributed valuable baseline assessments of the applicability of machine learning to streamflow statistic estimation yet however they compare only 2 3 estimation techniques each using a unique data set a practice that confounds direct comparison of model performance between individual studies in this paper twelve different modeling methods were applied to a publicly available data set falcone 2011 and the multi model comparison approach presented by elshorbagy et al 2010a 2010b and shortridge et al 2016 was used to determine the predictive performance of the models using multiple assessment criteria several machine learning techniques were introduced gradient boosting machines kernel k nearest neighbors and elastic net that to our knowledge have not yet been used to predict low flow statistics a meta learning m5 cubist model was also introduced that minimizes the overall generalization error by combining the cross validated predictions of each machine learning model finally hydrologic insights to the physical controls of low streamflow were explored through a discussion of the relative importance of predictor variables and their corresponding partial dependence functions for each model the novelty of this contribution is the use multiple machine learning models the introduction of meta modeling approaches for the regionalization of low streamflow statistics the comparison with models historically used to estimate 7q10s and the large gains in predictive accuracy over historical methods 1 1 research objectives and major findings this paper provides the 7q10 prediction performance estimates of twelve statistical estimation techniques four baseline methods type i tobit regression region of influence type i tobit regression ordinary kriging and an average unit area discharge null model and eight machine learning models 1 m5 cubist regression trees 2 gradient boosting machines 3 kernel k nearest neighbors 4 random forests 5 elastic net support vector machines with a 6 polynomial kernel and a 7 radial basis function kernel and an 8 ensemble meta learning m5 cubist model is also explored the specific research objectives are 1 use leave one out cross validation loo cv to simulate the prediction of 7q10s at ungaged sites in three states in the southeast u s using eleven estimation techniques 2 compare the predictive accuracy of each model using root mean squared error rmse unit area root mean squared area ua rmse median percentage error mpe and the nash sutcliffe efficiency coefficient nse and decompose the rmse to examine what is controlling the error for each model 3 discuss the relative importance and partial dependence functions of predictor variables for each model we found that machine learning methods can produce more accurate predictions of 7q10s in ungaged basins than baseline models variable importance measures and partial dependence plots suggest that 7q10s are partially driven by landcover late summer and early fall precipitation the infiltration rate of soils and the variability of minimum and maximum monthly temperatures 1 2 background of machine learning in hydrology machine learning also referred to as statistical learning data driven modeling and computational intelligence refers to a set of statistical methods that are optimized for predictive performance through a cross validated parameter tuning process hastie et al 2013 kuhn and johnson 2013 these methods have been called black box approaches and criticized for having little connection to the underlying physical processes being modeled see references in elshorbagy et al 2010a and see et al 2007 for examples of these critiques in hydrology regardless machine learning techniques have become prevalent in the hydrology literature artificial neural networks have been used for predictions in hundreds of water resource studies maier et al 2010 kasiviswanathan et al 2016 humphrey et al 2016 daliakopoulos and tsanis 2016 random forest models have been used to predict natural and altered streamflow regimes in ungaged basins carlisle et al 2010 eng et al 2013 li et al 2016 support vector machines have been used to forecast monthly streamflow kalteh 2016 guo et al 2011 and to downscale low flow indices joshi et al 2013 genetic algorithms have been used to calibrate rainfall runoff models goswami and o connor 2007 instance based methods e g k nearest neighbors have been used to forecast daily streamflow solomatine et al 2008 and m5 cubist models have been used for low flow forecasting štravs and brilly 2007 flood forecasting solomatine and xue 2004 and monthly streamflow forecasting shortridge et al 2016 yaseen et al 2016 zia et al 2015 various ensemble methods have also been used for prediction in hydrology two examples are artificial neural network ensembles applied in flood frequency analysis shu and burn 2004 and the prediction of monthly streamflow using ensemble methods for support vector machine and regression trees erdal and karakurt 2013 historically machine learning models have been best suited for modeling tasks that are concerned with accurate predictions and not physical interpretability breimanet al 2001 however applied researchers are exploring new methods to extract knowledge and gain domain specific insights from data driven models vellido et al 2012 2 material and methods 2 1 study site and data models were developed using 7q10 values from a total of 224 basins 45 basins in south carolina feaster and guimaraes 2009 2012 2014 2016 guimaraes and feaster 2010 68 basins in georgia gotvald 2016 and 111 basins in alabama feaster and lee 2017 fig 1 table 1 predictions in this paper were based on 7q10s calculated as of 2015 and may not reflect updated or forecasted 7q10 values the basins were selected from studies that estimated 7q10 values for near reference conditions i e basins unregulated and unlikely to be altered given associated measures of development over 230 independent variables from the gages ii data set falcone 2011 were originally considered as predictor variables the full gages ii data set consists of basin characteristics for 9322 streams within the u s that have at least 20 years of complete streamflow record since 1950 or streams that have been active since water year 2009 the u s geological survey usgs defines a water year as the 12 month period october 1 for any given year through september 30 of the following year several of the variables represented measures of regulation in the upstream basins the focus in this current analysis was unregulated streamflow so these variables were removed the method to explore and eliminate variables in this study closely follows that of farmer et al 2015 which left 125 variables for our analysis the selected variables are listed in worland et al worland et al 2017 and are defined in the supplemental material each of the independent variables were standardized by subtracting the mean of the variable and dividing by the standard deviation of the variable prior to model development values of 7q10s can span several orders of magnitude and so the response variable was also transformed 1 y i l n 7 q 10 i 0 001 d a i where y i is the transformed response variable for site i 7 q 10 i is the 7q10 for site i and d a i is the drainage area for site i because 13 of the sites had 7q10 values of zero 0 001 was added to each 7q10 value to avoid having infinite values the predicted 7q10 values were converted back to natural space cfs cubic feet per second by simple algebraic manipulation of equation 1 the response variable transformation for the tobit models was slightly different than what is presented in equation 1 for reasons particular to censored regression and details are presented in section 2 4 2 2 software data availability all of the analysis was done in the r language and environment for statistical computing core team 2017 and the required packages for each model are listed within the individual model description section the input data and r model archive can be accessed in worland et al 2017 2 3 tuning parameter selection for machine learning models a tuning parameter is any free parameter in a model that is provided by the user and tuning parameters are indicated by italics in the model descriptions below model tuning also referred to as hyperparameter optimization or model training is the process of searching for values of model parameters that optimize a predefined loss function e g the rmse the cross validated rmse was used for all tuning parameters for each model and is an arbitrary design choice that ensured consistency between models we took a two step model tuning approach for the machine learning models 1 30 initial points were generated in hyperparameter and rmse space using a simple random search across possible values of hyperparameters and 2 a bayesian optimization of the hyperparameters was conducted using a gaussian process prior and initial points from the random search bayesian optimization attempts to select optimal hyperparameters by treating the relationship between hyperparameter values and the rmse as an unknown function to be minimized i e the negative rmse is used for maximization a gaussian process model describes this function by constructing a posterior distribution of functions the posterior distribution improves as the number of samples from the hyperparameter space grows and the algorithm becomes more certain of the regions in hyperparameter space that are worth further exploration snoek et al 2012 in this study the gaussian process model was updated for 15 steps and the final model was selected based on the combination of parameters that produced the smallest leave one out cross validation loo cv rmse value table 2 the hyperparameters were tuned using the rbayesianoptimization r package yan 2016 the optimization function was parameterized with the matérn 5 2 kernel and the expected improvement or upper confidence bound acquisition functions based on the recommendations in snoek et al 2012 the only exception to this tuning process for the machine learning models was for the kernel k nearest neighbor kknn model where grid search was used instead of bayesian optimization because the latter required numeric hyperparameters and the kernel hyperparameter of the kknn model is a text string grid search involves an exhaustive search through a user defined subset of hyperparameter space hastie et al 2013 the number of predictor variables used to build the models can also be considered a free parameter we did not explicitly tune the number of predictor variables but allowed the machine learning models to potentially use all 125 variables the hyper parameter tuning scheme will naturally avoid overfitting i e the final model architecture is chosen by selecting the combinations of hyperparameters that produce the lowest cross validated prediction error thus rejecting model architectures that overfit to the training data tuning of the baseline models is described separately in each section 2 4 baseline models we classified multivariate regression and geostatistical techniques as baseline models this classification scheme was used as a way to compare groups of models and does not reflect the complexity accuracy or robustness of the method four baseline models and the tuning associated with each of those models is described below 2 4 1 type i tobit model a left censored tobit regression model full tobit was used as the baseline 7q10 prediction tobin 1958 kroll and stedinger 1999 left censored regression is useful for situations where the response variable cannot be observed below a certain value possibly due to measurement sensitivity but the predictor variables are known for every observation tobit models are frequently used to develop regionalization equations for low flow statistics when the streamflow statistic can be equal to zero esralew and smith 2009 risley et al 2008 kroll and luz 2003 eash and barnes 2012 the model can be written as 2 y ˆ i x i t β ε i i f y i y y i f y i y where y i is the response value for observation i x i are the values of predictor variables for observation i β is a vector of regression parameters ε i is the unexplained variance for an observation i and y is the censoring value basins with 7q10 values equal to zero n 13 were set to 0 001 the response was transformed using the natural log and ln 0 001 was used as the censoring value the natural log of the drainage area was then included as a candidate predictor we use this response transformation rather than the unit area 7q10 equation 1 because it 1 provides a unique censoring value of ln 0 001 and 2 produces better predictions for the tobit models than equation 1 a tobit model has the potential to overfit when a large number of predictor variables are included in the model to mitigate this forward stepwise selection was used to select explanatory variables within loo cv the final model with the lowest rmse included 8 predictor variables 2 4 2 region of influence tobit roi tobit the region of influence method builds a regression model for a particular site using only a subset of the full data set eng et al 2005 altman 1992 burn 1990 law et al 2009 eash and barnes 2012 in this study the sites included in the subset were selected based on their similarity to the site of interest where similarity was measured by euclidean distances in predictor space the optimal number of sites and predictor number designated as the region of influence was found using loo cv fig 2 2 4 3 ordinary kriging ordinary kriging is a geostatistical tool that uses the distance between two points to predict the semivariance of a dependent variable isaaks and srivastava 1989 the inter site semivariances of data from a measured network can be used to create a system of linear equations predicting the semivariance at an unmeasured site to be a weighted linear sum of the semivariance between all observed sites for an unmonitored site these same weights can be used to estimate the unknown quantity on which the semivariances were based if all the assumptions of ordinary kriging are valid this tool provides the best linear unbiased estimate in this study a spherical model was used to represent the semivariance between 7q10s other hydrologic applications archfield and vogel 2010 farmer 2016 have found success with spherical models and we used cross validation to confirm that the choice of a model form did not have a substantial impact on the prediction which is consistent with previous research farmer 2016 2 4 4 null model a null prediction model was created where the 7q10 prediction for a site was calculated as the left out mean of value of the unit 7q10s multiplied by the drainage area for the site 3 y i ˆ 1 n j 1 n 7 q 10 j a j a i where y i ˆ is the prediction for site i 7 q 10 j a j is the unit 7q10 value for every site but the site of interest and a i is the drainage area for the site of interest equation 3 can be rewritten as a one parameter single variate regression model where drainage area is the only predictor 4 y i ˆ 0 β a i 2 5 machine learning models it is rarely possible to make meaningful a priori distinctions between learning algorithms for a given data set wolpert 1996 therefore it is desirable to select a range of initial models with distinct functional differences ie models from a range of model families to increase the likelihood of discovering a well performing model each model in this study was fit to the data and the most promising models were further fine tuned to achieve optimal performance we include a brief section describing each model below we begin each section with a general description of how the model relates to the hydrologic task of predicting 7q10 values in ungaged basins we then provide further details describing the specific mechanics of each model 2 5 1 elastic net general overview elastic net models address overfitting by preventing parameters from inflating in response to a basin with an anomalously large 7q10 from a hydrologic perspective this results in out of sample predictions with reduced variance but potentially higher bias than a non regularized regression model this can lead to better predictions for sites with large 7q10 values 2 5 1 1 further details elastic net models are produced by a regularized regression method that combines the two penalties from least absolute shrinkage and selection operator lasso regression and ridge regression zou and hastie 2005 regularized regression methods also referred to as shrinkage or penalized regression provide a less complex model with better fit by including a penalization parameter in the loss function of least squares that shrinks the slope coefficients towards zero hastie et al 2013 ridge regression uses a squared penalty in the loss function which shrinks the parameter estimates towards zero whereas lasso regression uses an absolute value penalty in the loss function which results in some coefficients being set to exactly zero from a bayesian perspective ridge regression is equivalent to assigning a zero mean normally distributed prior distribution on the parameter vector and lasso regression is equivalent to assigning a zero mean laplace prior distribution on the parameter vector elastic net is a blend between the two the loss function for an elastic net model can be written as 5 β ˆ a r g m i n β y x β 2 λ 1 α β 2 2 α β where β ˆ is the vector of regression coefficients y is the response vector x is the predictor matrix α is a hyperparameter that serves to bridge the gap between lasso regression and ridge regression where α 1 results in lasso and α 0 results in ridge regression and λ controls the overall strength of the penalty hastie et al 2013 the elastic net model was fit using the glmnet r package friedman et al 2010 2 5 2 gradient boosting machine gbm 2 5 2 1 general overview the gradient boosting algorithm implemented here uses a regression tree as a base learner a regression tree is a simple rule based method basically a flow chart generated analytically to locate sites with similar basin characteristics the model then generates predictions by taking the average 7q10 values of sites that fall within the same nodes of a tree gradient boosting takes this a step further by using the model error from a single regression tree to iteratively build models that make better predictions for example if a site that falls within a node receives a poor prediction the algorithm generates a secondary model that tries to predict the residual of the base regression tree from a physical perspective this is a way to capture non linear relationships between 7q10s and basin characteristics 2 5 2 2 further details a gradient boosting algorithm uses the residuals i e the gradient from a base model to subsequently fit new models that are then added to the base model friedman 2002 a regression tree is often used as the base model for example a tree with a specified number of terminal nodes interaction depth is fit to data its residuals are calculated and a second tree is built using the residuals from the first tree as the response variable and the predictions from the second tree are added to the predictions from the first tree resulting in a new model this process is repeated a certain number of times specified by the user the number of trees a shrinkage parameter ranging between zero and one can be used to control the fraction of the new prediction added to the previous model for regression trees there is also an additional parameter that restricts the minimum number of observations that must be within each node min obs in node which can reduce the overall variance in the model in this study the gbm model was fit using the gbm r package hickey et al 2016 2 5 3 k nearest neighbors knn 2 5 3 1 general overview knn models leverage the proximity of basins in predictor space to predict 7q10 values that is basins with more similar basin characteristics predictor variables are considered to be near each other to predict a 7q10 for a new basin the algorithm determines the k most similar basins to the one of interest and assigns the mean 7q10 for the k most similar basins as the prediction at the site of interest altman 1992 a variant of knn was used in this study that involved transforming the predictor variables via a kernel function to allow the discovery of non linear relationships 2 5 3 2 further details the distance between samples can be measured using the minkowski distances which is calculated by 6 j 1 p x a j x b j q 1 q where p is the number of predictors x a j and x b j are observations in predictor space and q is passed to the model as a distance parameter when q 2 the mikowski distance is simply the euclidean distance the predicted value is the average value of the response for a given number of nearest neighbors in predictor space a kernel referred to here as kernel k nearest neighbors kknn can be used to transform the predictors prior to calculating the distances and has been shown to increase the prediction accuracy of the model yu et al 2002 the kknn model was fit using the kknn r package schliep and hechenbichler 2016 2 5 4 m5 cubist 2 5 4 1 general overview similar to gbm knn and region of influence regression models cubist models subset groups of basins via a regression tree method that have similar basin characteristics and makes predictions based off the subset cubist models however have two features that improve predictions 1 they weight the out of sample predicted 7q10 value for a particular basin by the in sample model performance on basins that are close to the basin of interest i e close in a nearest neighbor sense and 2 they use a linear regression in the terminal nodes of a tree rather than the mean to make predictions 2 5 4 2 further details a m5 cubist model is a type of regression tree kuhn et al 2012 loh 2008 the predictor space is partitioned through a set of recursive binary splits and prediction of the target variable is based on values of the features contained within the partitions the individual splits are chosen based on a greedy algorithm that seeks to minimize prediction error of possible subsets using only the branch of the subtree where the model is making a split the major difference between a simple regression tree and a m5 cubist model is how the models make predictions within the nodes a regression tree produces a single value prediction for each node whereas a m5 cubist model produces a prediction using a linear regression model quinlan 1993 the regression model in each node is built using only the predictor from the split directly above the node the final prediction from a single tree is based on the regression model in the terminal node but can be smoothed using a weighting scheme based on predictions from an arbitrary number of nodes within the subtree the number of nodes used in the smoothing process is referred to as neighbors the predictions from single tree m5 cubist model can be improved using a boosting like ensemble method where subsequent trees are built using the residuals from the single tree the number of trees used in the ensemble is referred to as committees in this study the m5 cubist models were fit using the cubist r package kuhn et al 2014 2 5 5 random forest rf 2 5 5 1 general overview rf models combine the results of multiple regression trees to predict 7q10 values rf models differ from other regression tree based methods because they rely on random sampling to describe persistent relationships between 7q10s and basin characteristics for example if a particular site s 7q10 is highly correlated with a certain basin characteristic but most of the other sites do not show the same level of correlation then an rf model will sacrifice a good prediction for that particular site to avoid overfitting this is accomplished by the repeated random sampling of 7q10s and basin characteristics 2 5 5 2 further details rf models aggregate individual regression trees to reduce variance and improve prediction accuracy breiman 2001 observations are randomly sampled from the training set an individual regression tree is built using the random sample predictions are made for the remaining observations i e out of bag samples and this process is repeated a certain number of times ntree randomness is further added by forcing each tree to consider different randomly selected sets of predictor variables mtry at each split in order to reduce overall variance by lessening the strength of correlation between trees this results in a bootstrapped aggregation of models referred to as bagging that is almost always more accurate than its constituent models hastie et al 2013 in this study the random forest models were fit using the random forest r package liaw and wiener 2002 2 5 6 support vector machines svm 2 5 6 1 general overview the physical interpretation of an svms is similar to the interpretation for the elastic net model however using a kernel function for the predictor variables allows the svm to discover non linear relationships between 7q10s and basin characteristics the support vectors are observations that have the most influence on the regression and are given weight over other observations 2 5 6 2 further details support vector machine regression models fit a regression line using only the data points i e the support vectors that fall outside of a user defined threshold denoted as ε the residuals outside of the threshold contribute a linear scaled amount to the model fit and residuals within the threshold do not contribute to the model fit hence svm regression is considered an ε insensitive regression the effect that the large residuals have on the regression is controlled by a cost parameter which can be shown to have a regularizing effect much like ridge regression a kernel function is often used to extend an svm to nonlinear regression smola and schölkopf 2004 different kernels have different effects on the model predictions in this study two svms were built one with a gaussian kernel and one with a polynomial kernel for the polynomial kernel the degree of the polynomial degree was provided an additional scale parameter can be provided that controls how close observations are in kernel space in this study the support vector machine models were fit using the kernlab r package karatzoglou et al 2004 2 6 stack generalization model meta m5 cubist 2 6 11 general overview meta models also referred to as ensemble models generate predictions by combining the output of multiple first order models wolpert 1992 breiman 1996 first order models are referred to as level 0 and the meta model is referred to as level 1 the models do not need to have a similar structure for example imagine a linear regression model a regression tree and a nearest neighbor model were used to independently predict the value of some observation a simple meta model prediction could be calculated by taking the mean prediction of the three level 0 models we expand this idea by using a stacked regression model that uses regression to combine the predictions from the level 0 models 2 6 22 further details the stacked regression model employed here combines the loo cv predictions from all the level 0 models using a level 1 m5 cubist model fig 3 a technique similar to bagging used in rf models and boosting used in gbms combining the loo cv predictions from a suite of different models can reduce variance and increase prediction accuracy wolpert 1992 breiman 1996 the level 0 models are the 7 machine learning and 4 baseline models described above and the level 1 model is the m5 cubist that uses only the predictions from the level 0 models as predictor variables prior to building the level 1 m5 cubist model the mean and median predictions for each model were added as predictor variables for the level 1 model the stacked model used only the loo cv predictions from each level 0 model and the unweighted mean and median predictions across each all models i e it does not use the basin characteristics 2 7 error metrics and error decomposition the models were all evaluated using loo cv an observation was removed from the data set i e left out the model was built on n 1 observations and the left out observation was predicted for each model this was iteratively done for each observation the loo cv predictions from each model were evaluated using four error metrics the root mean squared area rmse 7 r m s e 1 n y y ˆ 2 the unit area rmse ua rmse 8 u a r m s e 1 n y d a y ˆ d a 2 the median percent error 9 m p e m e d i a n y ˆ y y 100 and the nash sutcliffe efficiency coefficient nse 10 n s e 1 y ˆ y 2 y y 2 where y are the observed values y ˆ are the estimated values n is the total number of sites d a is the drainage area and y is the grand mean for the mpe y and y ˆ are only for sites where y 0 the mpe unit area rmse and nse provide a measures of model performance relative to the size of the observed value mpe the drainage area unit area rmse or the mean of the observed data nse the rmse can be decomposed following derivations presented in gupta et al 2009 and murphy 1988 if p is a subscript for the predicted 7q10 values and o is a subscript for the observed 7q10 values then the rmse can be decomposed into three parts 11 r m s e 2 m s e μ p μ o 2 σ p σ o 2 2 σ p σ o 1 r where r is the linear correlation coefficient between x 0 and x p μ p and σ p are the mean and standard deviation of the predicted values and μ o and σ o are the mean and standard deviation of the observed values if we designate a μ p μ o 2 b σ p σ o 2 and c 2 σ p σ o 1 r from equation 11 then we can visualize the absolute contribution of each component a b and c the first term a is a measure of model bias i e the difference in the means of the observed and predicted 7q10s the second term b is a measure of how well the model matches the variance of the observed values and the third term c is the remaining error and is largely controlled by the covariance or correlation of the predicted and observed 7q10s components a and b represent how well the model is able to recreate the location and shape of target distribution while component c accounts for the pairwise relationship between the predicted and observed values 2 8 variable importance and partial dependence plots the relative importance values from the top three predictor variables for each machine learning model were combined and rescaled the svmg model was omitted but the svmp model was retained to keep a representative from each model family the relative variable importance for each model was calculated using the varimp function from the r caret package kuhn 2012 for the rf model the mean squared error was computed on the out of bag sample for each tree that was built using a random subset of the predictor variables the differences in mean squared errors with and without certain predictor variables was then used to determine the relative importance of each predictor a similar approach was taken for gbm but the relative importance was the sum of the importances from each boosting iteration the m5 cubist model returned the percentage of times a variable was used for a condition or was used in a linear regression model the elastic net and svm models returned the absolute value of the non zero coefficients from the regression for the kknn model a technique without an obvious way to calculate the variable importance a filter method was used to compare r 2 values for models fit with a certain predictor variable compared to the r 2 from a null model the predictor variable that showed the largest improvement in r 2 values over the null was considered the most important predictor the top three most important predictor variables from each model were then selected and the importance values were rescaled by dividing the sum of the importance value of each predictor variable by six the maximum possible sum this provided a combined measure of variable importance from all of the machine learning models partial dependence plots pdp were created for the most important predictor variables from the rf gbm svmp and m5 cubist models these four models were chosen to show the effect of varying the most important covariates for models from several families a pdp showed the effect of varying a predictor variable of interest while accounting for the average effects of all other variables hastie et al 2013 providing mechanistic insights from black box algorithms the pdps were created using the icebox r package goldstein et al 2015 3 results 3 1 prediction errors and error decomposition the machine learning models outperformed the baseline models across almost all error metrics fig 4 table 3 the exception was the ordinary kriging and the null models both had a lower ua rmse than the elastic net model the full tobit model had a lower rmse than the roi tobit model but both had similar median percent errors the level 1 meta m5 cubist model resulted in the lowest rmse highest nse lowest mpe and lowest unit area rmse the machine learning models generated predictions that more closely matched the observed values than the baseline models fig 5 the error decomposition revealed that although the tobit models have the smallest bias errors they had relatively large variance and covariance errors fig 6 the top performing i e lowers rmse machine learning models generally showed smaller overall covariance errors 3 2 variable importance the overall variable importance value for percent wetlands was equal to one which indicated that percent wetlands was the most important predictor variable for each individual machine learning model fig 7 unit 7q10 values generally increase with an increase of percentage wetlands in a basin the second most important predictor variable was the percentage of soils in hydrologic soil group b hgb see wolock 1997 for more details about the soil classes which has moderate infiltration rates and is moderately coarse in texture and was in the top three most important predictor variables for all of the models except elastic net and m5 cubist unit 7q10 values tended to increase with an increase of moderately well drained soils other important predictor variables were the standard deviations of the minimum and maximum temperatures for the basins depth to the seasonally high water table mean august and november precipitation and the percentage of well drained soils with a high gravel and sand content unit 7q10s increased with a greater amount of august and november precipitation a higher amount of well drained soils and greater variability in the minimum and maximum temperatures for a basin the results were mixed for the average depth to the water table fig 7 further descriptions of the variables can be found in the supplementary material the most important variables for the machine learning models can be compared to the variables identified by forward stepwise selection used for the tobit models loo cv resulted in 8 predictors for both models the 8 predictors in order of the absolute value of their coefficients are 1 drainage area of the basin 2 percent well drained soils 3 mean estimated march runoff from 1951 to 2000 4 percent forest in the basin 5 percent wetlands in the basin 6 percent pasture in the basin 7 percent moderately well drained soils and the 8 standard deviation of the minimum temperature 4 discussion 4 1 predictive performance and applications our results showed that machine learning models can produce more accurate 7q10 estimates for ungaged basins than traditional statistical methods the range of rmses also indicated that exploring multiple methods is paramount for discovering well performing models the relative accuracy of different models can provide additional insights into the nature of the data for example the three most accurate level 0 models m5 cubist kknn and svmp learn from data using different approaches svmp fits a robust regression line in a kernel feature space kknn takes the average response value of sites that are close together in kernel space and m5 cubist extends tree based methods although these models belong to different families they all performed well on the largest 7q10 values and did not exhibit any systematic bias fig 5 in contrast the elastic net model accurately estimated the larger 7q10 values but under predicted a majority of sites with 7q10 values between 5 and 500 cfs this suggests that the best performing models were able to handle high leverage observations in the training set the meta m5 cubist model learned from the loo cv errors produced by the level 0 models fig 3 and further increased the accuracy of the predictions however the meta m5 cubist model was unable to reduce greatly the ua rmse which suggests that the error relative to the size of the basin may be near a threshold and is insensitive to small changes to absolute error as represented by e g rmse in practice watershed management goals should guide the choice of the error metric and the best model for a given task shortridge et al 2016 for example if an aquatic ecological habitat depends on a specific streamflow characteristic then the model that results in the lowest rmse for that characteristic may provide the best information for managing the ecological streamflow conversely if the management goal is to determine basins with anomalous flows relative to the size of the basin e g predict where a large basin might have a small 7q10 then the ua rmse performance of models is more relevant to that task than the rmse many states use the 7q10 for design flows while other states have selected other low flow statistics that are more suited for a given objective our study design is dependent variable agnostic i e the data processing and choice of models are not specific to 7q10s the same analysis could be rerun for a different statistic and a water manager could select the model that performs best in their region for their statistic and their purpose the model could then be used to make predictions in ungaged locations the results presented in this paper demonstrate that machine learning methods including meta modeling are viable approaches for future low streamflow statistic regionalization studies comparable results in truly ungaged basins can only be expected if the same set of explanatory variables are available for ungaged locations and to the authors knowledge this type of dataset does not yet exist however the 10 variables in fig 7 and drainage area provide a tractable starting place additionally the models in this paper are built using 7q10 data from 2015 and future models would benefit from updated 7q10 estimates prior to regionalization 4 2 error decomposition the machine learning models are optimized to minimize the overall prediction error rmse whereas tobit and ordinary kriging are optimized to produce an unbiased estimate with the smallest variance decomposing the overall error can lead to insights about model behavior and allows us to examine what is driving the error for each model fig 6 for example the tobit models were unbiased with low variance but had a large amount of unsystematic error whereas the elastic net model accepted higher bias to reduce variance and decrease the overall error which is a property of regularized regression hastie et al 2013 the ordinary kriging and null models had the same mse but the ordinary kriging error was almost entirely composed of variance which is likely the result of transformation bias on a linearly weighted summation helsel and hirsch 1992 the meta m5 cubist model was unbiased with minimal variance this indicated that the mean and variance of meta m5 cubist predictions was equal to the mean and variance of the actual 7q10s the remaining error results are from imperfect linear dependence between the predicted and the observed 7q10 values although we recognize the value of incorporating uncertainty into the model predictions we do not explicitly include uncertainty intervals here because real world applications of 7q10 predictions require a single 7q10 prediction for an ungaged basin we partially explore uncertainty by a decomposition of the error terms for each model section 4 2 but a full exploration of the implications of predictive uncertainty are beyond the scope of this work 4 3 physical controls of 7q10s the primary goal of estimating 7q10 values in ungaged basins is predictive accuracy and simple interpretable functions rarely produce the most accurate predictions breimanet al 2001 however even when the objective is prediction valuable mechanistic insights can be gained by examining the effect of specific predictor variables goldstein et al 2015 this is often accomplished by calculating the importance of each covariate used to fit a particular model which although useful only accounts for the magnitude of the effect and not the direction fig 7 left panel in addition to variable importance we explored the effect of each predictor in more detail using partial dependence plots fig 7 right panel in a review of low flow hydrology smakhtin 2001 lists several factors that influence the low flow regime of a basin the distribution and infiltration characteristics of soils the hydraulic characteristics and extent of the aquifers the rate frequency and amount of recharge the evapotranspiration rates from the basin distribution of vegetation types topography and climate five of the seven factors mentioned by smakhtin 2001 including soils aquifer characteristics recharge vegetation type and climate are reflected in the most important predictor variables identified here for the machine learning models fig 7 the percentage of emergent wetlands was an important covariate for the machine learning models and wetlands have been shown to modulate streamflow johnston et al 1990 as the percent of emergent wetlands increases the 7q10 drainage area decreases 7 the distribution of emergent wetlands is heavily left skewed where only a few basins have large percentages of wetlands and almost all of those sites are clustered in the coastal plain of georgia and south carolina percent wetlands is negatively correlated with a shallow depth to the water table ρ 0 82 november precipitation ρ 0 53 and moderately well drained soils ρ 0 45 this correlation suggests that wetlands depth to water table and 7q10s are controlled by precipitation and local surface geology and that the importance of percent wetlands for predicting unit 7q10s may simply reflect that they are both influenced by similar processes the pdp suggested that basins with very low standard deviations of maximum and minimum monthly air temperature min max temp also tend to have low unit 7q10s however this trend quickly disappears after an increase in unit 7q10 one possible explanation is that most basins with high standard deviations of min max temps are located in the higher elevations of the piedmont and blue ridge physiographic provinces which consist of a different geology than the coastal plain as with the percent wetlands predictor it is difficult to infer whether there exists a mechanistic relationship between the standard deviation of min max temps and unit 7q10s or if the trend is simply an artifact of this particular data set 4 4 conclusions machine learning methods can produce more accurate predictions of 7q10s in ungaged basins than historically relevant baseline models m5 cubist models kernel k nearest neighbor models and polynomial kernel support vector machines show the greatest improvements in prediction accuracy over type i tobit models and ordinary kriging the improved prediction accuracy of the machine learning models can be explained by how each model treats the bias variance tradeoff multivariate regression and ordinary kriging are both optimized to produce the best linear unbiased estimator whereas machine learning models minimize the overall prediction error by tuning hyperparameters using bayesian optimization or grid search this tuning process commonly accepts bias to reduce variance but generates optimal predictions variable importance measures and partial dependence plots show that percent emergent wetlands in the basin is the most important predictor variable for the machine learning models we interpret this correlation as simply an indication that 7q10s and percent wetlands are likely controlled by similar factors late summer and early fall precipitation the infiltration rate of soils and the variability of minimum and maximum monthly temperatures which also emerge as some of the most important variables for predicting 7q10s machine learning approaches show much promise for improving predictions of low streamflow in ungaged catchments additionally combining the predictions of multiple first order machine learning models via a global meta model is a novel yet practical advancement for hydrologic regionalization studies acknowledgments this work was funded in part by the u s environmental protection agency and the u s geological survey usgs water availability and use science project we thank the three anonymous journal reviewers for their helpful comments that greatly improved the manuscript we also thank daren carlisle ecological studies coordinator for the usgs national water quality assessment program for his internal review of the manuscript any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government the input data and r model archive associated with this paper are available digitally at worland et al 2017 appendix a supplementary data the following are the supplementary data related to this article sup material sup material data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 021 
26439,we compare the ability of eight machine learning models elastic net gradient boosting kernel k nearest neighbors two variants of support vector machines m5 cubist random forest and a meta learning ensemble m5 cubist model and four baseline models ordinary kriging a unit area discharge model and two variants of censored regression to generate estimates of the annual minimum 7 day mean streamflow with an annual exceedance probability of 90 7q10 at 224 unregulated sites in south carolina georgia and alabama usa the machine learning models produced substantially lower cross validation errors compared to the baseline models the meta learning m5 cubist model had the lowest root mean squared error of 26 72 cubic feet per second partial dependence plots show that 7q10s are likely moderated by late summer and early fall precipitation and the infiltration capacity of basin soils keywords low streamflow ungaged basins 7q10 machine learning censored regression variable importance 1 introduction water managers rely on streamflow data to allocate water resources define the dilution potential of catchments set ecological streamflow limits and ensure sustainable watershed planning razavi and coulibaly 2012 knight et al 2014 kapo et al 2015 however many streams do not have observed streamflow data and water managers must depend on the streamflow estimates from various prediction models mishra and coulibaly 2009 razavi and coulibaly 2012 luce 2014 improving the predictions of streamflow in ungaged basins has been a primary objective for hydrologists for decades and international initiatives have resulted in rapid advances in this field sivapalan et al 2003 hrachowitz et al 2013 blöschl 2016 the two primary modeling strategies for predicting streamflow response in ungaged basins are 1 deterministic physically based models i e calculating streamflow based on distributed hydrologic parameters and 2 statistical regionalization i e using regression models to transfer hydrologic information from gaged to ungaged basins razavi and coulibaly 2012 farmer and vogel 2016 this current paper focuses on the statistical regionalization of a low streamflow statistic the annual minimum 7 day mean streamflow with an annual exceedance probability of 90 7q10 a stream s low flow refers to the amount of water flowing in a stream during prolonged periods of little to no rainfall during an average non drought year the low flow regime for a particular stream is controlled by the physical characteristics of its basin and the local climate smakhtin 2001 the 7q10 statistic describes a basin s expected low flow and provides a way to compare directly the low flow regimes of different basins this statistic is commonly used to determine permitted point source pollutant levels in streams ames 2006 there are a number of other important low flow metrics not discussed in this paper several examples are the 7q10 for a particular season or month the annual minimum 7 day mean streamflow with an annual exceedance probability of 50 7q2 mean annual minimum median september streamflow and ecologically derived values knight et al 2014 kormos et al 2016 murphy et al 2013 raines and asquith 1997 the contribution of this research is the comparison of statistical estimation techniques the choice of the specific response variable would not change the structure of the analysis but we cannot conjecture how specific models would perform for a different target variable low flow regionalization methods attempt to predict low flow metrics in ungaged basins by leveraging the correlation between basin characteristics and streamflow at gaged basins razavi and coulibaly 2012 the primary goal of 7q10 regionalization is accurate predictions and not mechanistic explanations of what controls the 7q10 and this distinction between prediction and explanation should guide the statistical analysis shmueli 2010 regardless of outcome goal or the type of model used all hydrologic models require assumptions deterministic models for example assume that the physical relationships between parts of a hydrologic system are adequately captured by a set of static functions and decision rules while stochastic models may depend on assumptions about the probabilistic constraints on parameters i e priors the choice of the likelihood and cost functions the numerical methods used for parameter estimation e g gradient descent maximum likelihood numerical integration etc and choices about data preprocessing and transformation furthermore hydrologic models often assume some level of stationarity lins and cohn 2011 these assumptions can have significant effects on the applicability of model results and researchers must acknowledge how their model design choices propagate into conclusions drawn from the model this paper evaluates the predictive performance of various association based models e g linear regression models that leverage the covariance structure between variables to make inferences and predictions association based models have proved to be a useful engineering tool for predicting 7q10s and have become increasingly sophisticated in the last 30 years hrachowitz et al 2013 regression methods have evolved from simple ordinary least squares riggs 1973 thomas and benson 1970 hardison 1971 to time series weighted least squares tasker 1980 generalized least squares gls stedinger and tasker 1985 censored regression kroll and stedinger 1999 two step gls logistic regression funkhouser et al 2008 truncated models and catchment clustering methods law et al 2009 there has also been an increased application of geostatistical low flow regionalization methods primarily ordinary kriging top kriging and physiographical spaced based interpolation castiglioni et al 2009 2011 despite the recent methodological advances mentioned above few studies have explored machine learning methods to predict low flow metrics in ungaged basins ouarda and shu 2009 used an ensemble of artificial neural networks for predicting various low flow metrics in canada laaha and blöschl 2006 used regression trees to predict q95s in austria schnier and cai 2014 used model tree ensembles to predict a complete flow duration curve fdc for streams in illinois and texas and booker and woods 2014 used random forest models to predict several components of a fdc in new zealand these studies contributed valuable baseline assessments of the applicability of machine learning to streamflow statistic estimation yet however they compare only 2 3 estimation techniques each using a unique data set a practice that confounds direct comparison of model performance between individual studies in this paper twelve different modeling methods were applied to a publicly available data set falcone 2011 and the multi model comparison approach presented by elshorbagy et al 2010a 2010b and shortridge et al 2016 was used to determine the predictive performance of the models using multiple assessment criteria several machine learning techniques were introduced gradient boosting machines kernel k nearest neighbors and elastic net that to our knowledge have not yet been used to predict low flow statistics a meta learning m5 cubist model was also introduced that minimizes the overall generalization error by combining the cross validated predictions of each machine learning model finally hydrologic insights to the physical controls of low streamflow were explored through a discussion of the relative importance of predictor variables and their corresponding partial dependence functions for each model the novelty of this contribution is the use multiple machine learning models the introduction of meta modeling approaches for the regionalization of low streamflow statistics the comparison with models historically used to estimate 7q10s and the large gains in predictive accuracy over historical methods 1 1 research objectives and major findings this paper provides the 7q10 prediction performance estimates of twelve statistical estimation techniques four baseline methods type i tobit regression region of influence type i tobit regression ordinary kriging and an average unit area discharge null model and eight machine learning models 1 m5 cubist regression trees 2 gradient boosting machines 3 kernel k nearest neighbors 4 random forests 5 elastic net support vector machines with a 6 polynomial kernel and a 7 radial basis function kernel and an 8 ensemble meta learning m5 cubist model is also explored the specific research objectives are 1 use leave one out cross validation loo cv to simulate the prediction of 7q10s at ungaged sites in three states in the southeast u s using eleven estimation techniques 2 compare the predictive accuracy of each model using root mean squared error rmse unit area root mean squared area ua rmse median percentage error mpe and the nash sutcliffe efficiency coefficient nse and decompose the rmse to examine what is controlling the error for each model 3 discuss the relative importance and partial dependence functions of predictor variables for each model we found that machine learning methods can produce more accurate predictions of 7q10s in ungaged basins than baseline models variable importance measures and partial dependence plots suggest that 7q10s are partially driven by landcover late summer and early fall precipitation the infiltration rate of soils and the variability of minimum and maximum monthly temperatures 1 2 background of machine learning in hydrology machine learning also referred to as statistical learning data driven modeling and computational intelligence refers to a set of statistical methods that are optimized for predictive performance through a cross validated parameter tuning process hastie et al 2013 kuhn and johnson 2013 these methods have been called black box approaches and criticized for having little connection to the underlying physical processes being modeled see references in elshorbagy et al 2010a and see et al 2007 for examples of these critiques in hydrology regardless machine learning techniques have become prevalent in the hydrology literature artificial neural networks have been used for predictions in hundreds of water resource studies maier et al 2010 kasiviswanathan et al 2016 humphrey et al 2016 daliakopoulos and tsanis 2016 random forest models have been used to predict natural and altered streamflow regimes in ungaged basins carlisle et al 2010 eng et al 2013 li et al 2016 support vector machines have been used to forecast monthly streamflow kalteh 2016 guo et al 2011 and to downscale low flow indices joshi et al 2013 genetic algorithms have been used to calibrate rainfall runoff models goswami and o connor 2007 instance based methods e g k nearest neighbors have been used to forecast daily streamflow solomatine et al 2008 and m5 cubist models have been used for low flow forecasting štravs and brilly 2007 flood forecasting solomatine and xue 2004 and monthly streamflow forecasting shortridge et al 2016 yaseen et al 2016 zia et al 2015 various ensemble methods have also been used for prediction in hydrology two examples are artificial neural network ensembles applied in flood frequency analysis shu and burn 2004 and the prediction of monthly streamflow using ensemble methods for support vector machine and regression trees erdal and karakurt 2013 historically machine learning models have been best suited for modeling tasks that are concerned with accurate predictions and not physical interpretability breimanet al 2001 however applied researchers are exploring new methods to extract knowledge and gain domain specific insights from data driven models vellido et al 2012 2 material and methods 2 1 study site and data models were developed using 7q10 values from a total of 224 basins 45 basins in south carolina feaster and guimaraes 2009 2012 2014 2016 guimaraes and feaster 2010 68 basins in georgia gotvald 2016 and 111 basins in alabama feaster and lee 2017 fig 1 table 1 predictions in this paper were based on 7q10s calculated as of 2015 and may not reflect updated or forecasted 7q10 values the basins were selected from studies that estimated 7q10 values for near reference conditions i e basins unregulated and unlikely to be altered given associated measures of development over 230 independent variables from the gages ii data set falcone 2011 were originally considered as predictor variables the full gages ii data set consists of basin characteristics for 9322 streams within the u s that have at least 20 years of complete streamflow record since 1950 or streams that have been active since water year 2009 the u s geological survey usgs defines a water year as the 12 month period october 1 for any given year through september 30 of the following year several of the variables represented measures of regulation in the upstream basins the focus in this current analysis was unregulated streamflow so these variables were removed the method to explore and eliminate variables in this study closely follows that of farmer et al 2015 which left 125 variables for our analysis the selected variables are listed in worland et al worland et al 2017 and are defined in the supplemental material each of the independent variables were standardized by subtracting the mean of the variable and dividing by the standard deviation of the variable prior to model development values of 7q10s can span several orders of magnitude and so the response variable was also transformed 1 y i l n 7 q 10 i 0 001 d a i where y i is the transformed response variable for site i 7 q 10 i is the 7q10 for site i and d a i is the drainage area for site i because 13 of the sites had 7q10 values of zero 0 001 was added to each 7q10 value to avoid having infinite values the predicted 7q10 values were converted back to natural space cfs cubic feet per second by simple algebraic manipulation of equation 1 the response variable transformation for the tobit models was slightly different than what is presented in equation 1 for reasons particular to censored regression and details are presented in section 2 4 2 2 software data availability all of the analysis was done in the r language and environment for statistical computing core team 2017 and the required packages for each model are listed within the individual model description section the input data and r model archive can be accessed in worland et al 2017 2 3 tuning parameter selection for machine learning models a tuning parameter is any free parameter in a model that is provided by the user and tuning parameters are indicated by italics in the model descriptions below model tuning also referred to as hyperparameter optimization or model training is the process of searching for values of model parameters that optimize a predefined loss function e g the rmse the cross validated rmse was used for all tuning parameters for each model and is an arbitrary design choice that ensured consistency between models we took a two step model tuning approach for the machine learning models 1 30 initial points were generated in hyperparameter and rmse space using a simple random search across possible values of hyperparameters and 2 a bayesian optimization of the hyperparameters was conducted using a gaussian process prior and initial points from the random search bayesian optimization attempts to select optimal hyperparameters by treating the relationship between hyperparameter values and the rmse as an unknown function to be minimized i e the negative rmse is used for maximization a gaussian process model describes this function by constructing a posterior distribution of functions the posterior distribution improves as the number of samples from the hyperparameter space grows and the algorithm becomes more certain of the regions in hyperparameter space that are worth further exploration snoek et al 2012 in this study the gaussian process model was updated for 15 steps and the final model was selected based on the combination of parameters that produced the smallest leave one out cross validation loo cv rmse value table 2 the hyperparameters were tuned using the rbayesianoptimization r package yan 2016 the optimization function was parameterized with the matérn 5 2 kernel and the expected improvement or upper confidence bound acquisition functions based on the recommendations in snoek et al 2012 the only exception to this tuning process for the machine learning models was for the kernel k nearest neighbor kknn model where grid search was used instead of bayesian optimization because the latter required numeric hyperparameters and the kernel hyperparameter of the kknn model is a text string grid search involves an exhaustive search through a user defined subset of hyperparameter space hastie et al 2013 the number of predictor variables used to build the models can also be considered a free parameter we did not explicitly tune the number of predictor variables but allowed the machine learning models to potentially use all 125 variables the hyper parameter tuning scheme will naturally avoid overfitting i e the final model architecture is chosen by selecting the combinations of hyperparameters that produce the lowest cross validated prediction error thus rejecting model architectures that overfit to the training data tuning of the baseline models is described separately in each section 2 4 baseline models we classified multivariate regression and geostatistical techniques as baseline models this classification scheme was used as a way to compare groups of models and does not reflect the complexity accuracy or robustness of the method four baseline models and the tuning associated with each of those models is described below 2 4 1 type i tobit model a left censored tobit regression model full tobit was used as the baseline 7q10 prediction tobin 1958 kroll and stedinger 1999 left censored regression is useful for situations where the response variable cannot be observed below a certain value possibly due to measurement sensitivity but the predictor variables are known for every observation tobit models are frequently used to develop regionalization equations for low flow statistics when the streamflow statistic can be equal to zero esralew and smith 2009 risley et al 2008 kroll and luz 2003 eash and barnes 2012 the model can be written as 2 y ˆ i x i t β ε i i f y i y y i f y i y where y i is the response value for observation i x i are the values of predictor variables for observation i β is a vector of regression parameters ε i is the unexplained variance for an observation i and y is the censoring value basins with 7q10 values equal to zero n 13 were set to 0 001 the response was transformed using the natural log and ln 0 001 was used as the censoring value the natural log of the drainage area was then included as a candidate predictor we use this response transformation rather than the unit area 7q10 equation 1 because it 1 provides a unique censoring value of ln 0 001 and 2 produces better predictions for the tobit models than equation 1 a tobit model has the potential to overfit when a large number of predictor variables are included in the model to mitigate this forward stepwise selection was used to select explanatory variables within loo cv the final model with the lowest rmse included 8 predictor variables 2 4 2 region of influence tobit roi tobit the region of influence method builds a regression model for a particular site using only a subset of the full data set eng et al 2005 altman 1992 burn 1990 law et al 2009 eash and barnes 2012 in this study the sites included in the subset were selected based on their similarity to the site of interest where similarity was measured by euclidean distances in predictor space the optimal number of sites and predictor number designated as the region of influence was found using loo cv fig 2 2 4 3 ordinary kriging ordinary kriging is a geostatistical tool that uses the distance between two points to predict the semivariance of a dependent variable isaaks and srivastava 1989 the inter site semivariances of data from a measured network can be used to create a system of linear equations predicting the semivariance at an unmeasured site to be a weighted linear sum of the semivariance between all observed sites for an unmonitored site these same weights can be used to estimate the unknown quantity on which the semivariances were based if all the assumptions of ordinary kriging are valid this tool provides the best linear unbiased estimate in this study a spherical model was used to represent the semivariance between 7q10s other hydrologic applications archfield and vogel 2010 farmer 2016 have found success with spherical models and we used cross validation to confirm that the choice of a model form did not have a substantial impact on the prediction which is consistent with previous research farmer 2016 2 4 4 null model a null prediction model was created where the 7q10 prediction for a site was calculated as the left out mean of value of the unit 7q10s multiplied by the drainage area for the site 3 y i ˆ 1 n j 1 n 7 q 10 j a j a i where y i ˆ is the prediction for site i 7 q 10 j a j is the unit 7q10 value for every site but the site of interest and a i is the drainage area for the site of interest equation 3 can be rewritten as a one parameter single variate regression model where drainage area is the only predictor 4 y i ˆ 0 β a i 2 5 machine learning models it is rarely possible to make meaningful a priori distinctions between learning algorithms for a given data set wolpert 1996 therefore it is desirable to select a range of initial models with distinct functional differences ie models from a range of model families to increase the likelihood of discovering a well performing model each model in this study was fit to the data and the most promising models were further fine tuned to achieve optimal performance we include a brief section describing each model below we begin each section with a general description of how the model relates to the hydrologic task of predicting 7q10 values in ungaged basins we then provide further details describing the specific mechanics of each model 2 5 1 elastic net general overview elastic net models address overfitting by preventing parameters from inflating in response to a basin with an anomalously large 7q10 from a hydrologic perspective this results in out of sample predictions with reduced variance but potentially higher bias than a non regularized regression model this can lead to better predictions for sites with large 7q10 values 2 5 1 1 further details elastic net models are produced by a regularized regression method that combines the two penalties from least absolute shrinkage and selection operator lasso regression and ridge regression zou and hastie 2005 regularized regression methods also referred to as shrinkage or penalized regression provide a less complex model with better fit by including a penalization parameter in the loss function of least squares that shrinks the slope coefficients towards zero hastie et al 2013 ridge regression uses a squared penalty in the loss function which shrinks the parameter estimates towards zero whereas lasso regression uses an absolute value penalty in the loss function which results in some coefficients being set to exactly zero from a bayesian perspective ridge regression is equivalent to assigning a zero mean normally distributed prior distribution on the parameter vector and lasso regression is equivalent to assigning a zero mean laplace prior distribution on the parameter vector elastic net is a blend between the two the loss function for an elastic net model can be written as 5 β ˆ a r g m i n β y x β 2 λ 1 α β 2 2 α β where β ˆ is the vector of regression coefficients y is the response vector x is the predictor matrix α is a hyperparameter that serves to bridge the gap between lasso regression and ridge regression where α 1 results in lasso and α 0 results in ridge regression and λ controls the overall strength of the penalty hastie et al 2013 the elastic net model was fit using the glmnet r package friedman et al 2010 2 5 2 gradient boosting machine gbm 2 5 2 1 general overview the gradient boosting algorithm implemented here uses a regression tree as a base learner a regression tree is a simple rule based method basically a flow chart generated analytically to locate sites with similar basin characteristics the model then generates predictions by taking the average 7q10 values of sites that fall within the same nodes of a tree gradient boosting takes this a step further by using the model error from a single regression tree to iteratively build models that make better predictions for example if a site that falls within a node receives a poor prediction the algorithm generates a secondary model that tries to predict the residual of the base regression tree from a physical perspective this is a way to capture non linear relationships between 7q10s and basin characteristics 2 5 2 2 further details a gradient boosting algorithm uses the residuals i e the gradient from a base model to subsequently fit new models that are then added to the base model friedman 2002 a regression tree is often used as the base model for example a tree with a specified number of terminal nodes interaction depth is fit to data its residuals are calculated and a second tree is built using the residuals from the first tree as the response variable and the predictions from the second tree are added to the predictions from the first tree resulting in a new model this process is repeated a certain number of times specified by the user the number of trees a shrinkage parameter ranging between zero and one can be used to control the fraction of the new prediction added to the previous model for regression trees there is also an additional parameter that restricts the minimum number of observations that must be within each node min obs in node which can reduce the overall variance in the model in this study the gbm model was fit using the gbm r package hickey et al 2016 2 5 3 k nearest neighbors knn 2 5 3 1 general overview knn models leverage the proximity of basins in predictor space to predict 7q10 values that is basins with more similar basin characteristics predictor variables are considered to be near each other to predict a 7q10 for a new basin the algorithm determines the k most similar basins to the one of interest and assigns the mean 7q10 for the k most similar basins as the prediction at the site of interest altman 1992 a variant of knn was used in this study that involved transforming the predictor variables via a kernel function to allow the discovery of non linear relationships 2 5 3 2 further details the distance between samples can be measured using the minkowski distances which is calculated by 6 j 1 p x a j x b j q 1 q where p is the number of predictors x a j and x b j are observations in predictor space and q is passed to the model as a distance parameter when q 2 the mikowski distance is simply the euclidean distance the predicted value is the average value of the response for a given number of nearest neighbors in predictor space a kernel referred to here as kernel k nearest neighbors kknn can be used to transform the predictors prior to calculating the distances and has been shown to increase the prediction accuracy of the model yu et al 2002 the kknn model was fit using the kknn r package schliep and hechenbichler 2016 2 5 4 m5 cubist 2 5 4 1 general overview similar to gbm knn and region of influence regression models cubist models subset groups of basins via a regression tree method that have similar basin characteristics and makes predictions based off the subset cubist models however have two features that improve predictions 1 they weight the out of sample predicted 7q10 value for a particular basin by the in sample model performance on basins that are close to the basin of interest i e close in a nearest neighbor sense and 2 they use a linear regression in the terminal nodes of a tree rather than the mean to make predictions 2 5 4 2 further details a m5 cubist model is a type of regression tree kuhn et al 2012 loh 2008 the predictor space is partitioned through a set of recursive binary splits and prediction of the target variable is based on values of the features contained within the partitions the individual splits are chosen based on a greedy algorithm that seeks to minimize prediction error of possible subsets using only the branch of the subtree where the model is making a split the major difference between a simple regression tree and a m5 cubist model is how the models make predictions within the nodes a regression tree produces a single value prediction for each node whereas a m5 cubist model produces a prediction using a linear regression model quinlan 1993 the regression model in each node is built using only the predictor from the split directly above the node the final prediction from a single tree is based on the regression model in the terminal node but can be smoothed using a weighting scheme based on predictions from an arbitrary number of nodes within the subtree the number of nodes used in the smoothing process is referred to as neighbors the predictions from single tree m5 cubist model can be improved using a boosting like ensemble method where subsequent trees are built using the residuals from the single tree the number of trees used in the ensemble is referred to as committees in this study the m5 cubist models were fit using the cubist r package kuhn et al 2014 2 5 5 random forest rf 2 5 5 1 general overview rf models combine the results of multiple regression trees to predict 7q10 values rf models differ from other regression tree based methods because they rely on random sampling to describe persistent relationships between 7q10s and basin characteristics for example if a particular site s 7q10 is highly correlated with a certain basin characteristic but most of the other sites do not show the same level of correlation then an rf model will sacrifice a good prediction for that particular site to avoid overfitting this is accomplished by the repeated random sampling of 7q10s and basin characteristics 2 5 5 2 further details rf models aggregate individual regression trees to reduce variance and improve prediction accuracy breiman 2001 observations are randomly sampled from the training set an individual regression tree is built using the random sample predictions are made for the remaining observations i e out of bag samples and this process is repeated a certain number of times ntree randomness is further added by forcing each tree to consider different randomly selected sets of predictor variables mtry at each split in order to reduce overall variance by lessening the strength of correlation between trees this results in a bootstrapped aggregation of models referred to as bagging that is almost always more accurate than its constituent models hastie et al 2013 in this study the random forest models were fit using the random forest r package liaw and wiener 2002 2 5 6 support vector machines svm 2 5 6 1 general overview the physical interpretation of an svms is similar to the interpretation for the elastic net model however using a kernel function for the predictor variables allows the svm to discover non linear relationships between 7q10s and basin characteristics the support vectors are observations that have the most influence on the regression and are given weight over other observations 2 5 6 2 further details support vector machine regression models fit a regression line using only the data points i e the support vectors that fall outside of a user defined threshold denoted as ε the residuals outside of the threshold contribute a linear scaled amount to the model fit and residuals within the threshold do not contribute to the model fit hence svm regression is considered an ε insensitive regression the effect that the large residuals have on the regression is controlled by a cost parameter which can be shown to have a regularizing effect much like ridge regression a kernel function is often used to extend an svm to nonlinear regression smola and schölkopf 2004 different kernels have different effects on the model predictions in this study two svms were built one with a gaussian kernel and one with a polynomial kernel for the polynomial kernel the degree of the polynomial degree was provided an additional scale parameter can be provided that controls how close observations are in kernel space in this study the support vector machine models were fit using the kernlab r package karatzoglou et al 2004 2 6 stack generalization model meta m5 cubist 2 6 11 general overview meta models also referred to as ensemble models generate predictions by combining the output of multiple first order models wolpert 1992 breiman 1996 first order models are referred to as level 0 and the meta model is referred to as level 1 the models do not need to have a similar structure for example imagine a linear regression model a regression tree and a nearest neighbor model were used to independently predict the value of some observation a simple meta model prediction could be calculated by taking the mean prediction of the three level 0 models we expand this idea by using a stacked regression model that uses regression to combine the predictions from the level 0 models 2 6 22 further details the stacked regression model employed here combines the loo cv predictions from all the level 0 models using a level 1 m5 cubist model fig 3 a technique similar to bagging used in rf models and boosting used in gbms combining the loo cv predictions from a suite of different models can reduce variance and increase prediction accuracy wolpert 1992 breiman 1996 the level 0 models are the 7 machine learning and 4 baseline models described above and the level 1 model is the m5 cubist that uses only the predictions from the level 0 models as predictor variables prior to building the level 1 m5 cubist model the mean and median predictions for each model were added as predictor variables for the level 1 model the stacked model used only the loo cv predictions from each level 0 model and the unweighted mean and median predictions across each all models i e it does not use the basin characteristics 2 7 error metrics and error decomposition the models were all evaluated using loo cv an observation was removed from the data set i e left out the model was built on n 1 observations and the left out observation was predicted for each model this was iteratively done for each observation the loo cv predictions from each model were evaluated using four error metrics the root mean squared area rmse 7 r m s e 1 n y y ˆ 2 the unit area rmse ua rmse 8 u a r m s e 1 n y d a y ˆ d a 2 the median percent error 9 m p e m e d i a n y ˆ y y 100 and the nash sutcliffe efficiency coefficient nse 10 n s e 1 y ˆ y 2 y y 2 where y are the observed values y ˆ are the estimated values n is the total number of sites d a is the drainage area and y is the grand mean for the mpe y and y ˆ are only for sites where y 0 the mpe unit area rmse and nse provide a measures of model performance relative to the size of the observed value mpe the drainage area unit area rmse or the mean of the observed data nse the rmse can be decomposed following derivations presented in gupta et al 2009 and murphy 1988 if p is a subscript for the predicted 7q10 values and o is a subscript for the observed 7q10 values then the rmse can be decomposed into three parts 11 r m s e 2 m s e μ p μ o 2 σ p σ o 2 2 σ p σ o 1 r where r is the linear correlation coefficient between x 0 and x p μ p and σ p are the mean and standard deviation of the predicted values and μ o and σ o are the mean and standard deviation of the observed values if we designate a μ p μ o 2 b σ p σ o 2 and c 2 σ p σ o 1 r from equation 11 then we can visualize the absolute contribution of each component a b and c the first term a is a measure of model bias i e the difference in the means of the observed and predicted 7q10s the second term b is a measure of how well the model matches the variance of the observed values and the third term c is the remaining error and is largely controlled by the covariance or correlation of the predicted and observed 7q10s components a and b represent how well the model is able to recreate the location and shape of target distribution while component c accounts for the pairwise relationship between the predicted and observed values 2 8 variable importance and partial dependence plots the relative importance values from the top three predictor variables for each machine learning model were combined and rescaled the svmg model was omitted but the svmp model was retained to keep a representative from each model family the relative variable importance for each model was calculated using the varimp function from the r caret package kuhn 2012 for the rf model the mean squared error was computed on the out of bag sample for each tree that was built using a random subset of the predictor variables the differences in mean squared errors with and without certain predictor variables was then used to determine the relative importance of each predictor a similar approach was taken for gbm but the relative importance was the sum of the importances from each boosting iteration the m5 cubist model returned the percentage of times a variable was used for a condition or was used in a linear regression model the elastic net and svm models returned the absolute value of the non zero coefficients from the regression for the kknn model a technique without an obvious way to calculate the variable importance a filter method was used to compare r 2 values for models fit with a certain predictor variable compared to the r 2 from a null model the predictor variable that showed the largest improvement in r 2 values over the null was considered the most important predictor the top three most important predictor variables from each model were then selected and the importance values were rescaled by dividing the sum of the importance value of each predictor variable by six the maximum possible sum this provided a combined measure of variable importance from all of the machine learning models partial dependence plots pdp were created for the most important predictor variables from the rf gbm svmp and m5 cubist models these four models were chosen to show the effect of varying the most important covariates for models from several families a pdp showed the effect of varying a predictor variable of interest while accounting for the average effects of all other variables hastie et al 2013 providing mechanistic insights from black box algorithms the pdps were created using the icebox r package goldstein et al 2015 3 results 3 1 prediction errors and error decomposition the machine learning models outperformed the baseline models across almost all error metrics fig 4 table 3 the exception was the ordinary kriging and the null models both had a lower ua rmse than the elastic net model the full tobit model had a lower rmse than the roi tobit model but both had similar median percent errors the level 1 meta m5 cubist model resulted in the lowest rmse highest nse lowest mpe and lowest unit area rmse the machine learning models generated predictions that more closely matched the observed values than the baseline models fig 5 the error decomposition revealed that although the tobit models have the smallest bias errors they had relatively large variance and covariance errors fig 6 the top performing i e lowers rmse machine learning models generally showed smaller overall covariance errors 3 2 variable importance the overall variable importance value for percent wetlands was equal to one which indicated that percent wetlands was the most important predictor variable for each individual machine learning model fig 7 unit 7q10 values generally increase with an increase of percentage wetlands in a basin the second most important predictor variable was the percentage of soils in hydrologic soil group b hgb see wolock 1997 for more details about the soil classes which has moderate infiltration rates and is moderately coarse in texture and was in the top three most important predictor variables for all of the models except elastic net and m5 cubist unit 7q10 values tended to increase with an increase of moderately well drained soils other important predictor variables were the standard deviations of the minimum and maximum temperatures for the basins depth to the seasonally high water table mean august and november precipitation and the percentage of well drained soils with a high gravel and sand content unit 7q10s increased with a greater amount of august and november precipitation a higher amount of well drained soils and greater variability in the minimum and maximum temperatures for a basin the results were mixed for the average depth to the water table fig 7 further descriptions of the variables can be found in the supplementary material the most important variables for the machine learning models can be compared to the variables identified by forward stepwise selection used for the tobit models loo cv resulted in 8 predictors for both models the 8 predictors in order of the absolute value of their coefficients are 1 drainage area of the basin 2 percent well drained soils 3 mean estimated march runoff from 1951 to 2000 4 percent forest in the basin 5 percent wetlands in the basin 6 percent pasture in the basin 7 percent moderately well drained soils and the 8 standard deviation of the minimum temperature 4 discussion 4 1 predictive performance and applications our results showed that machine learning models can produce more accurate 7q10 estimates for ungaged basins than traditional statistical methods the range of rmses also indicated that exploring multiple methods is paramount for discovering well performing models the relative accuracy of different models can provide additional insights into the nature of the data for example the three most accurate level 0 models m5 cubist kknn and svmp learn from data using different approaches svmp fits a robust regression line in a kernel feature space kknn takes the average response value of sites that are close together in kernel space and m5 cubist extends tree based methods although these models belong to different families they all performed well on the largest 7q10 values and did not exhibit any systematic bias fig 5 in contrast the elastic net model accurately estimated the larger 7q10 values but under predicted a majority of sites with 7q10 values between 5 and 500 cfs this suggests that the best performing models were able to handle high leverage observations in the training set the meta m5 cubist model learned from the loo cv errors produced by the level 0 models fig 3 and further increased the accuracy of the predictions however the meta m5 cubist model was unable to reduce greatly the ua rmse which suggests that the error relative to the size of the basin may be near a threshold and is insensitive to small changes to absolute error as represented by e g rmse in practice watershed management goals should guide the choice of the error metric and the best model for a given task shortridge et al 2016 for example if an aquatic ecological habitat depends on a specific streamflow characteristic then the model that results in the lowest rmse for that characteristic may provide the best information for managing the ecological streamflow conversely if the management goal is to determine basins with anomalous flows relative to the size of the basin e g predict where a large basin might have a small 7q10 then the ua rmse performance of models is more relevant to that task than the rmse many states use the 7q10 for design flows while other states have selected other low flow statistics that are more suited for a given objective our study design is dependent variable agnostic i e the data processing and choice of models are not specific to 7q10s the same analysis could be rerun for a different statistic and a water manager could select the model that performs best in their region for their statistic and their purpose the model could then be used to make predictions in ungaged locations the results presented in this paper demonstrate that machine learning methods including meta modeling are viable approaches for future low streamflow statistic regionalization studies comparable results in truly ungaged basins can only be expected if the same set of explanatory variables are available for ungaged locations and to the authors knowledge this type of dataset does not yet exist however the 10 variables in fig 7 and drainage area provide a tractable starting place additionally the models in this paper are built using 7q10 data from 2015 and future models would benefit from updated 7q10 estimates prior to regionalization 4 2 error decomposition the machine learning models are optimized to minimize the overall prediction error rmse whereas tobit and ordinary kriging are optimized to produce an unbiased estimate with the smallest variance decomposing the overall error can lead to insights about model behavior and allows us to examine what is driving the error for each model fig 6 for example the tobit models were unbiased with low variance but had a large amount of unsystematic error whereas the elastic net model accepted higher bias to reduce variance and decrease the overall error which is a property of regularized regression hastie et al 2013 the ordinary kriging and null models had the same mse but the ordinary kriging error was almost entirely composed of variance which is likely the result of transformation bias on a linearly weighted summation helsel and hirsch 1992 the meta m5 cubist model was unbiased with minimal variance this indicated that the mean and variance of meta m5 cubist predictions was equal to the mean and variance of the actual 7q10s the remaining error results are from imperfect linear dependence between the predicted and the observed 7q10 values although we recognize the value of incorporating uncertainty into the model predictions we do not explicitly include uncertainty intervals here because real world applications of 7q10 predictions require a single 7q10 prediction for an ungaged basin we partially explore uncertainty by a decomposition of the error terms for each model section 4 2 but a full exploration of the implications of predictive uncertainty are beyond the scope of this work 4 3 physical controls of 7q10s the primary goal of estimating 7q10 values in ungaged basins is predictive accuracy and simple interpretable functions rarely produce the most accurate predictions breimanet al 2001 however even when the objective is prediction valuable mechanistic insights can be gained by examining the effect of specific predictor variables goldstein et al 2015 this is often accomplished by calculating the importance of each covariate used to fit a particular model which although useful only accounts for the magnitude of the effect and not the direction fig 7 left panel in addition to variable importance we explored the effect of each predictor in more detail using partial dependence plots fig 7 right panel in a review of low flow hydrology smakhtin 2001 lists several factors that influence the low flow regime of a basin the distribution and infiltration characteristics of soils the hydraulic characteristics and extent of the aquifers the rate frequency and amount of recharge the evapotranspiration rates from the basin distribution of vegetation types topography and climate five of the seven factors mentioned by smakhtin 2001 including soils aquifer characteristics recharge vegetation type and climate are reflected in the most important predictor variables identified here for the machine learning models fig 7 the percentage of emergent wetlands was an important covariate for the machine learning models and wetlands have been shown to modulate streamflow johnston et al 1990 as the percent of emergent wetlands increases the 7q10 drainage area decreases 7 the distribution of emergent wetlands is heavily left skewed where only a few basins have large percentages of wetlands and almost all of those sites are clustered in the coastal plain of georgia and south carolina percent wetlands is negatively correlated with a shallow depth to the water table ρ 0 82 november precipitation ρ 0 53 and moderately well drained soils ρ 0 45 this correlation suggests that wetlands depth to water table and 7q10s are controlled by precipitation and local surface geology and that the importance of percent wetlands for predicting unit 7q10s may simply reflect that they are both influenced by similar processes the pdp suggested that basins with very low standard deviations of maximum and minimum monthly air temperature min max temp also tend to have low unit 7q10s however this trend quickly disappears after an increase in unit 7q10 one possible explanation is that most basins with high standard deviations of min max temps are located in the higher elevations of the piedmont and blue ridge physiographic provinces which consist of a different geology than the coastal plain as with the percent wetlands predictor it is difficult to infer whether there exists a mechanistic relationship between the standard deviation of min max temps and unit 7q10s or if the trend is simply an artifact of this particular data set 4 4 conclusions machine learning methods can produce more accurate predictions of 7q10s in ungaged basins than historically relevant baseline models m5 cubist models kernel k nearest neighbor models and polynomial kernel support vector machines show the greatest improvements in prediction accuracy over type i tobit models and ordinary kriging the improved prediction accuracy of the machine learning models can be explained by how each model treats the bias variance tradeoff multivariate regression and ordinary kriging are both optimized to produce the best linear unbiased estimator whereas machine learning models minimize the overall prediction error by tuning hyperparameters using bayesian optimization or grid search this tuning process commonly accepts bias to reduce variance but generates optimal predictions variable importance measures and partial dependence plots show that percent emergent wetlands in the basin is the most important predictor variable for the machine learning models we interpret this correlation as simply an indication that 7q10s and percent wetlands are likely controlled by similar factors late summer and early fall precipitation the infiltration rate of soils and the variability of minimum and maximum monthly temperatures which also emerge as some of the most important variables for predicting 7q10s machine learning approaches show much promise for improving predictions of low streamflow in ungaged catchments additionally combining the predictions of multiple first order machine learning models via a global meta model is a novel yet practical advancement for hydrologic regionalization studies acknowledgments this work was funded in part by the u s environmental protection agency and the u s geological survey usgs water availability and use science project we thank the three anonymous journal reviewers for their helpful comments that greatly improved the manuscript we also thank daren carlisle ecological studies coordinator for the usgs national water quality assessment program for his internal review of the manuscript any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government the input data and r model archive associated with this paper are available digitally at worland et al 2017 appendix a supplementary data the following are the supplementary data related to this article sup material sup material data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 021 
