index,text
26225,the effectiveness of integrated water resource management iwrm modeling hinges on the quality of practices employed through the process starting from early problem definition all the way through to using the model in a way that serves its intended purpose the adoption and implementation of effective modeling practices need to be guided by a practical understanding of the variety of decisions that modelers make and the information considered in making these choices there is still limited documented knowledge on the modeling workflow and the role of contextual factors in determining this workflow and which practices to employ this paper attempts to contribute to this knowledge gap by providing systematic guidance of the modeling practices through the phases planning development application and perpetuation and steps that comprise the modeling process positing questions that should be addressed practice focused guidance helps explain the detailed process of conducting iwrm modeling including the role of contextual factors in shaping practices we draw on findings from literature and the authors collective experience to articulate what and how contextual factors play out in employing those practices in order to accelerate our learning about how to improve iwrm modeling the paper concludes with five key areas for future practice related research knowledge sharing overcoming data limitations informed stakeholder involvement social equity and uncertainty management keywords decision making social learning stakeholders uncertainty calibration integrated modeling iwrm 1 introduction integrated water resources management iwrm is above all a process e g molle 2008 ibisch et al 2016 one that promotes the coordinated development and management of water land and related resources in order to maximize the resultant economic and social welfare in an equitable manner without compromising the sustainability of vital ecosystems gwp 2000 iwrm principles also known as the dublin principles have functioned to serve public participation in water resource decision processes and increasingly this encompasses participatory or the related collaborative modeling basco carrera et al 2017 indeed an iwrm project typically includes substantial stakeholder engagement and is supported by at least one jointly developed model such participatory modeling has recently been defined by voinov et al 2018 as a purposeful learning exercise for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representations of reality modeling for iwrm represents interactions between humans and their environment and therefore has much in common with social ecological systems modeling and the more recent concept of socio hydrology social ecological systems modeling often focuses on sustainability ostrom 2009 or resilience folke 2006 while socio hydrology focuses on how coupled human water systems interact and co evolve sivapalan et al 2012 iwrm is distinguished by a practical focus on supporting water management or policy decisions in a context that typically involves stakeholders from multiple sectors iwrm is underpinned by water balance and hydrological modeling adapted to a specific policy or planning setting this requires connecting hydrology with other socio environmental knowledge and component models see e g croke et al 2014 for a case study in the murray darling basin the term model however has many connotations here we define a model as an explicit representation of features and relationships of a target system badham 2015 within this definition there are many types of models including process physically and or behaviorally based gaming actor based conceptual and flowcharts each can have a range of characteristics such as static dynamic statistical empirical probabilistic and distributed parameter lumped parameter an iwrm process can utilise any of these here we emphasize quantitative models because they provide an appropriate framework for cost benefit and other trade off analyses as well as playing a key role in understanding managing and negotiating solutions for integrated socio environmental issues quantitative models 1 can serve as boundary objects to encourage and facilitate focus in communication among participants 2 help in problem framing and explicit boundary setting which bridge gaps in understanding that can divide participants and bring together multiple perspectives and 3 determine which actions out of all those possible are in fact being evaluated i e which hypotheses are tested falconi and palmer 2017 although these benefits also arise with other types of models the rigorous process of converting qualitative statements and broad understandings i e mental models into quantitative or at least categorical relations allows for testing the veracity of perspectives successful iwrm modeling can be effective in many ways and to varying degrees merritt et al 2017 in this paper we view success in a holistic way as the ability to embed the modeling process in a social process that connects scientists decision makers and stakeholders and achieves impact in accordance to its purpose which may vary from a shared understanding of a problem to policy analysis hamilton et al in review our starting point is that the degree of iwrm modeling success can be enhanced by the use of effective practices throughout the model development application lifecycle from framing key questions and defining objectives all way through to using the model to satisfy its intended purpose s adoption and implementation of effective iwrm modeling practices need to be informed by practical and fit for purpose driven guidance on how to mobilize the iwrm concepts and techniques towards successful outcomes for decision making and stakeholders although scholars have made strides in fleshing out iwrm e g tortajada 2016 conceptualizing modeling methodologies and reporting case studies there is still limited documented knowledge on the modeling workflow and the role of contextual factors in determining this workflow and the practices to employ practice focused guidance may help explain at a micro level some of the nuances experienced by those involved in iwrm modeling we aim to contribute to bridging this knowledge gap by describing the actions needed to develop a successful iwrm modeling project this focus on detailed steps and activities motivates the need to develop an in depth understanding of the iwrm process in particular seeking to understand how integration will be implemented how stakeholders are effectively involved and linking the modeling decisions to the variety of factors that make up the problem and system context we draw on findings from literature and the authors collective experience to articulate what and how contextual factors play out in employing those practices the guidance synthesizes findings about the modeling process drawn from literature reported in overlapping areas including environmental and hydrologic modeling jakeman et al 2006 anderson et al 2015 harmel et al 2018 engaging stakeholders vennix et al 1990 voinov and bousquet 2010 voinov et al 2018 and general modeling banks 1999 in the next section we summarize some key literature on the notion of iwrm including main criticisms arguing that these are concerned with the challenge of its implementation in a practical context and that operationalizing iwrm is assisted by a good modeling process the phases and steps that constitute an effective iwrm modeling process are later described in sections 3 to 7 this is followed by an outline of key areas for future development in iwrm modeling section 8 2 background the concept the criticisms and the operational role of modeling in iwrm it is a well accepted premise that modeling has a crucial role to play in the implementation of iwrm soncini sessa et al 2007 yet the effective use of modeling for operationalization of iwrm still faces important challenges for the future in terms of integration and implementation of knowledge these challenges are inherent to the nature of the iwrm concept itself and how it can be implemented in practice in this section we give an overview of those challenges and how they motivate this paper the first challenge is the difficulty of managing wicked problems in water policy when there are multiple pressures conflicting stakeholder values competing goals multiple decision makers limited resources and deep uncertainty in the context of integrated groundwater management jakeman et al 2016 bring together some 74 contributors to elucidate the governance biophysical socioeconomic and decision support aspects of iwrm the three iwrm goals of economic efficiency equity and environmental sustainability often conflict and so require trade offs molle 2008 criticisms of iwrm often focus on the practical challenges of implementation perhaps the most strident critic has been biswas 2004 who considers the definition of iwrm itself amorphous and questions whether integration across so many aspects is achievable tortajada 2016 responds to these criticisms by clarifying iwrm as a concept as a goal in itself and as a strategy to achieve development goals practical high level guidance on what to integrate is given by hamilton et al 2015 who flesh out ten dimensions of integration e g issues of concerns stakeholders spatial and temporal scales uncertainties etc as an extension of this challenge there has also been discussion in the literature about the water energy food nexus as either a competing paradigm to iwrm or one that could inspire improvements in the concept and operationalization of iwrm e g keskinen et al 2016 m√ºller 2015 but while representing a set of issues to be considered in many iwrm problem settings the water energy food nexus concept may focus attention on the food and energy sectors and draw attention away from the natural environment and possibly other social and economic water values climate change environmental change and adaptation also are being increasingly recognized as a component to be integrated with iwrm especially in regard to the planetary effort towards sustainable development and its goals including a specific target sdg 6 5 on the implementation of iwrm at all levels giupponi and gain 2017 the second challenge relates to handling the human element in iwrm and reconciling the conflicting agendas involved with water management grid 2016 for example iwrm has been criticized in that stakeholder engagement has been perfunctory the concept has not been accepted and practised by local water managers funke et al 2007 and technical integration in analyses has not coincided with a balanced institutional integration fischhendler 2008 or the modeling has not been convincing middlemis 2000 on the other hand grid 2016 argues that while iwrm is challenging because of the human element no other process can reconcile the conflicting agendas involved with water management the primary challenge therefore is in building guidance on how one implements iwrm or tackles the integration dimensions in the implementation of iwrm for a given problem whereas the literature includes some prescriptive guidance on how to implement an iwrm modeling process there is still a gap in linking this advice to practice for example hamilton et al 2015 take a step towards operationalizing integration by offering a framework for mapping research methods that can be used to implement each of the integration dimensions throughout the modeling process however the framework remains mainly prescriptive and work is still needed to document and reflect on the lessons from the actual execution of the modeling processes in summary we are well aware of the difficulties of operationalizing iwrm whilst ibisch et al 2016 have recently examined 14 iwrm projects and given a synthesis of lessons learnt from them we agree with giordano and shah 2014 that the lessons from attempts to implement iwrm are otherwise not well documented and have impeded its development beyond a concept for dealing with wicked water problems if we are to add substance to carrying out successful iwrm we need to begin to document systematically how iwrm projects are being carried out and evaluate the lessons being systematic will require a template that includes a set of categories defining the context of a problem so that patterns and anti patterns of iwrm operations in specific contexts can emerge for future guidance on what and how to integrate there is no alternative to us as a community than learning by doing iwrm while reporting on it and explicitly accruing the lessons in recognition of this need we have been associated with a pursuits project at the national socio environmental synthesis center sesync https www sesync org for you educator research themes pursuits funded by the united states national science foundation we come from a range of backgrounds spanning science and social science of water resource management as well as public health modeling and computer science the pursuit project aims to address the overarching research question of identifying the core practices that should be employed in developing and using models to support iwrm thus we argue for effective iwrm through a modeling lens but it recognizes that such modeling needs to be a process that is well grounded in the needs of the iwrm problem at hand hence we also focus on guidance for applied problems following good practice modeling can bring many benefits to the iwrm process including elicitation systemization and sharing of otherwise fragmented knowledge management of uncertainty clarification of tradeoffs facilitation of capacity development transparency and participation and inclusiveness the paper does not delve into the necessary institutional and governance reforms advocated by many for iwrm moreover we feel that there is still much progress to be made by working within whatever current governance constraints exist but also using modeling to infer and communicate how new arrangements and approaches might improve the effectiveness of water resource management at its core our paper s main aim is to address the how of iwrm modeling by guiding modelers and commissioners of projects on what questions to ask and what issues to address thereby prompting decisions through a model grounded process for an iwrm problem 3 introducing phases and steps for effective iwrm modeling iwrm modeling actions can be broadly grouped into four phases cf hamilton et al 2015 the planning phase identifies what is to be achieved how this is to be accomplished and what resources can be brought to bear the model is built and tested during the development phase and then used in the application phase finally models that become part of the ongoing policy or decision making process or are for routine operational use require a perpetuation phase these phases are sufficiently general that they typically apply to the widest range of iwrm activities and model types within these general phases are more specific steps see left side of table 1 or activities associated with the iwrm modeling process which may be implemented in different ways we provide a modeling centric view of an iwrm decision support process a policy centric or stakeholder centric view might identify and emphsize phases somewhat differently connecting to these to varying extents depending on the level of policy maker or other stakeholder involvement in the broader iwrm arena different authors have separated the modeling process into distinct actions for example black et al 2014 focus on the use of models to analyze water resource planning scenarios and their guidance is principally intended for modelers we step back and focus on what fundamentally is to be considered and achieved in each of the four phases and accompanying steps for the context in which the iwrm process operates the framework described here emphasizes flexibility and directs project attention to key points and questions as a way to facilitate better access to the iwrm concepts and tools available here we define context as the social and project characteristics that influence which steps are brought to bear and how they are implemented in any application specifics of what happens within the steps may differ important contexts for the model development include the general role of the modeling the funding levels for the effort level of stakeholder conflict type of governance setting relevant scales of the problem and tolerance for uncertainty description of each step within a phase is elucidated in the following sections using brief examples drawn from a range of iwrm settings typically based on quantitative water management modeling with substantial stakeholder involvement given the broad nature of iwrm problems examples alluded to here are necessarily illustrative rather than comprehensive although the phases are broadly chronological ordering is not intended to imply that the steps are necessarily sequential the steps presented in this paper are not intended to form a rigid checklist but capture key practices that should be considered and performed as needed a great deal of overlap and iteration between phases and steps can be expected some steps may be conducted concurrently whilst in particular projects others may not be needed at all for example tasks undertaken in the first planning phase might be revisited during the development phase if data are found to be of poorer quality than expected or the desired model outputs change similarly there is likely to be overlapping parallel progress between data collection model construction and testing as testing reveals weaknesses in the model and identifies additional data that would be required to reduce uncertainty in important model outputs such concurrent cyclic iteration is widely understood to be a part of hydrological and other models where the development and application phases of modeling provoke revisiting of previous efforts each step of the model can be seen as a path which has forks where alternative choices can be made the choices made by the modeling team can affect the quality of the modeling results for example choosing one stakeholder group over another can affect the relevance of the information included in the model lahtinen et al 2017 have developed a checklist to help modelers evaluate alternative paths and recognize and act on situations where changing the path may be desirable while not discussed further here it is a useful complementary tool 4 phase 1 planning the planning phase defines what the modeling is intended to achieve within the context of the larger iwrm project who should be involved and what resources are required it converts an initial need that a model or set of models is required into a description of the model s and a plan for how to meet that need efficiently and effectively models may need developing or if suitable can be adapted from already developed models this phase comprises four steps problem definition and scoping stakeholder planning project management planning and preliminary conceptual model 4 1 problem definition and scoping the first step is one of clarification and agreement about what are the objectives and especially which water centric issues and cross sectoral issues e g climate food energy environmental cultural are essential to be addressed this essentially determines what is and is not to be modeled a key aspect is to identify the questions that the modeling should answer however the form and scope of these questions and their answers are shaped by the intended users of the model s and identified stakeholders as well as the function or role that the model s and modeling process is to have within the broader iwrm project clearly defining the problem and how the model is to be used to address the problem determines many of the decisions made during subsequent modeling steps some of these decisions are whose views and knowledge are to be included which functions and processes are required of the model s expected level of model accuracy and complexity and how model utility will be tested 4 1 1 model function fundamentally a model is a representation of knowledge about a system i e past present or future and that representation can be used in different ways badham 2015 kelly et al 2013 oxley et al 2004 an iwrm modeling process is policy oriented because the defining characteristic of iwrm as outlined earlier is its focus on the human and decision making dimensions of issues such as ways to equitably manage sources of water or how to cost effectively allocate volumes of water in a river system to different uses and the environment commonly policy oriented modeling is supported by quantitative models of a particular water resource or other aspects of the socio environmental system which form the foundation for exploration of options and constraints a major component of these models tends to be physically based but sometimes data based models that advance scientific knowledge of or improve scientific tools applied to that resource typical examples include hydrology models beven 2012 which aim at understanding surface water behaviours hydrogeological models anderson et al 2015 to advance the knowledge of groundwater systems and water quality models fu et al 2019 chapra 2008 merritt et al 2003 to comprehend the fate of nutrients and or contaminants in water but increasingly these are supplemented or integrated with modeling of the human dimensions noel and cai 2017 kelly et al 2013 especially their behaviour and decision making at various levels such as individual e g farmer household industry irrigation community rural urban and policy jurisdiction local regional national policy oriented models can be further subdivided into those models used for policy support or decision making and those for conceptual thinking and social learning models used for policy support and decision making extrapolate from knowledge and behaviour to estimate potential consequences or a range of consequences of potential actions while everyday use of water models can include forecasting for operational objectives e g reservoir real time operational decisions for flood control hsu and wei 2007 iwrm modeling is typically for longer term strategic and or multiple objectives such as assessing spatiotemporal tradeoffs of impacts resulting from alternative management options e g for licensing abstraction volumes and locations under a varying climate these models are used either directly by the decision maker or their advisors or the modeling team may summarize the model outputs for different scenarios as advice for decision makers models for thinking and learning are used to communicate visualize and explore the system to inform the diverse groups concerned with a project such models can also integrate knowledge elicited from those groups e g see elsawah et al 2015 for cognitive mapping of stakeholder system understanding in this process the model acts as a translator or boundary object between groups with different understanding and languages about the problem the emphasis is on restricting all imaginable system behaviours to those that are reasonably likely while at the same time providing insights improving mental models and fostering interaction and communication among stakeholders the modeling process itself is often an important part of this role with possible cognitive or social outcomes including systems thinking consensus building and conflict reduction belt and blake 2015 for example pahl wostl and hare 2004 describe benefits of a learning activity that included gaining insights into complexities of the system and greater understanding of the actors own perspectives and the role of other actors in addition the openness of the process fosters the discussion of innovative ideas although presented as somewhat distinct in this discussion the purpose of a modeling exercise may be manifold a model that promotes social learning may also be taken further and used for policy support likewise scientific models may be harnessed by a modeling system in order to provide policy support furthermore the role may change during an iwrm process with different aspects of the model emphasized at different points for example social learning when generating options but then policy support when the options are being compared although developing a model or models may be the main deliverable of an iwrm project some complex projects may involve drawing on several sources of evidence and methods perhaps with the model playing a relatively minor part bots and van daalen 2008 for example there may be formal legal hearings and submission processes to obtain the views of members of the public as well as key stakeholder organizations in such a situation the modeling process needs to complement and interact with the non modeling activities and sources of information consider for example the objectives of the murray darling basin plan in australia where the objective was to determine a sustainable limit of water extraction in the basin csiro 2008 largely existing hydrological and ecological models were adapted to address this objective supplemented by socioeconomic analyses croke et al 2014 4 1 2 model question and scope clarification of the modeling purpose role and use triggers the first model design decision what are the questions the modeling is answering questions can be framed around a set of inputs and outputs iwrm inputs typically include drivers that impact water systems e g climate change water use needs for energy urban or agriculture demands stressors e g over exploitation of water resources and management and policy options e g riparian buffer management water allocation policy where financial and socioeconomic impacts are associated with each driver outputs of interest feed iwrm outcomes that need to be improved or maintained to address the identified issue s an example question is which management interventions acting in which locations are most likely to lead to desired outcomes a typical output would be the nature and magnitude of change in the physical water resource examples include how much water reaches the end of the river given a rainfall population and abstraction scenario or how much is some measure of pollution reduced and perhaps at what cost given a specific change in policy the modeling may generate several outputs that each answer a different question or multiple outputs might be used to answer a single question as iwrm projects typically involve researchers and stakeholders from different disciplinary backgrounds and experiences there can be different perspectives on what key questions need to be investigated brandt et al 2013 coherent framing of questions can be challenging and time consuming but the process of re framing the modeling questions is an essential exercise for building a shared understanding about the modeling inquiry and its implications for the system the process also sets relationships between stakeholders that will be active during the remainder of the iwrm process problem definition facilitates subsequent boundary setting and scoping of the problem domain identifying what aspects of the system are to be included excluded from the model in terms of its relevance or contribution to addressing the question of interest of course some elements of the setting may change over time challenges to problem definition are inevitable as iwrm is a wicked problem characterized by multiple and often conflicting viewpoints about the problem possible solutions who will be affected by the problem and who should take part in solving the problem based on the plausible interpretations of these issues and questions there are multiple problems to be defined and boundaries to be set walker et al 2003 dewulf and bouwen 2008 offer an example case study where three actors i e water power plant water management authority and sand miners have divergent problem frames about water management problems in their region the water plant which provides electricity to the region defines the goal as reducing soil erosion and resulting sediment flow to the reservoirs on the other hand miners do not frame sediment as a threat but as an economic opportunity problem definition is not a straightforward exercise but an organic process through which stakeholders reflect on their existing problem frames negotiate differences and ideally re construct their views to reach a shared representation of the problem although many iwrm applications are typically confined to the physical boundary settings of targeted water systems emerging concepts such as virtual water which involves the role of food trade in compensating water deficit can stretch beyond the biophysical boundary yang and zehnder 2007 social and economic considerations can also stretch the physical boundary for example income used to sustain a regional community may be derived not just from activities within the iwrm zone but may be supplemented from additional work outside a catchment nexus thinking is another driver for expanding system boundaries by promoting the importance of attending to the food water energy environment linkages and cross sector impacts hussey and pittock 2012 framing water problems using a nexus lens extends many of the aspects that need to be attended to including stakeholder groups to be involved goals and risks to be considered governance and regulatory framework and the interdependencies among the modeled systems however different problem interpretations are not only attributed to different stakeholder views and systems of interest the disciplinary background of the research team can be another challenge whereas diverse disciplinary teams are often cited as beneficial for problem definition and even critical for the success of an iwrm modeling process the assumption that any interdisciplinary team will effectively work together to develop an integrated problem cannot be taken at face value nicolson et al 2002 interdisciplinary teams do have their challenges and can stumble over their differences bark et al 2016 such as different views about what constitutes data different research agendas etc there is still a need for more guidance into how interdisciplinary teams can work together in iwrm modeling projects to negotiate their differences and reach an integrated understanding of the problem 4 2 stakeholder planning iwrm inevitably involves multiple stakeholders and interest groups that include domain experts effective stakeholder engagement leverages diverse knowledge and perspectives in order to inform what water related issues are addressed the subsequent construction of realistic input data scenarios and potential management solutions elsawah et al 2015 the extent of stakeholder involvement however depends on both the project needs and the interests of the particular stakeholders the purpose timing ways and mode of engagement may also differ for different stakeholder groups stakeholder planning steps must consider several questions such as which stakeholders to engage separately or together and when disciplines domain knowledge relevant interests what role does the main client or funding agency play relative to other stakeholders are there cultural or other sensitivities to consider how much authority should the stakeholders have in making decisions about the model at what points in the modeling process is their input most important will stakeholders be involved as individuals or part of a group how will information flow from to stakeholders and the model how does the project provide value to the stakeholders what is required to enable the stakeholders to engage effectively what commitment is required from the stakeholders and are they sufficiently interested and available to make that commitment what is required to build trust plans for stakeholder engagement should be developed monitored and adjusted in concert with the stakeholders who are to be engaged barreteau et al 2010 four forms of engaging iwrm stakeholders in modeling have been identified hare 2011 1 front and back end modeling where stakeholders are involved at the early and last stages of the project 2 co construction where stakeholders are involved throughout the model development process 3 front end modeling where participation is limited to the early stage and 4 back end modeling where stakeholders are brought in at the end of the modeling process in general it is best to engage representative stakeholders as early and as often as possible langsdale et al 2013 checking in with regular status discussions at all stages of the modeling effort extensive engagement is particularly important in projects oriented towards promoting social learning and collaboration and requiring high levels of trust however the early stages of a project typically involve more abstract discussions hence there is a tension between involving stakeholders early and consuming their time with little progress or later and running the risk of them feeling that key decisions have already been made developing and maintaining stakeholder interest is key to resolving this tension 4 3 project management planning like all substantial projects the modeling process requires resources such as time funding data see data collection step below and skilled personnel in some cases there can be externally imposed deadlines or funding constraints that must be met which in turn can drive problem definition choice of model used and the modeling process even without fixed constraints government or regulatory authorities are likely to contribute much of the funding and some technical skills and may have a different role in supporting planning from other stakeholders stakeholder engagement discussions should cover what support is desired and what is available and then be revised as more is learned as the iwrm process progresses key decision points in the modeling process should be coordinated with stakeholders to ensure agreement on what is broadly being done when and with whom this typically is a negotiated process between some stakeholders and the modelers and can be aided by developing a workplan of activities that is revisited throughout the project to allow response to changing priorities resource constraints or leveraging opportunities in the context of iwrm more resources are typically required for larger projects in order to facilitate the communication and collaboration that typically include a greater variety of stakeholders with different perspectives disciplinary backgrounds and experiences van asselt and rijkens klomp 2002 4 4 preliminary conceptual model the preliminary conceptual model forms the basis for transitioning between the planning and development phases it is a broad qualitative outline of the model design and identifies the main features that must be included in the model s that is are to be constructed conceptual models typically include desired outputs of the model which are those that represent the effects of controllable and uncontrollable drivers on the societally relevant resources and indicators key entities or system components that drive the desired outputs of the model such as land and water use behaviour policy that influences how water is extracted ecosystem impacts river flow aquifer responses of interest and water quantity and quality broad relationships and connecting processes between these entities and uncertain elements that are to be explored during the decision making process such as policy options and those exogenous factors outside of policy control e g climate other sectoral influences such as from energy or agricultural policy the objective of the preliminary conceptualization is to describe aspects of the iwrm issues and the influences that contribute to these issues these elements are then refined to those that are relevant to the output for iwrm outcomes and scenarios and to the comparison of those scenarios for any negotiation or recommendation a conceptual model defines connections between important components of the iwrm issue and includes model inputs and model outputs and intermediate states and processes that are relevant some entities and processes do not influence the relevant output but may be important to stakeholders to improve relevance and make the model more accessible to end users elsawah et al 2017a b therefore the development of the preliminary conceptual model can also be a useful activity for building a shared understanding among the project stakeholders about the system drivers and how they will be addressed in the model bertone et al 2016 throughout the conceptualization process substantial discussions are indispensable regarding how the model will be used what limitations might need to be overcome what alternatives should be considered and how multiple models will work together the conceptual model design step focuses on higher level views of the iwrm problem for example determining boundaries of the problem e g what is in what is out temporal and spatial scales and extent and general level of complexity required for the intended decision level e g whether short term operational medium term management or long term planning determining the appropriate scales can be challenging because the components of socio environmental models in the iwrm domain can have different spatial boundaries and characteristic process time scales and the stakeholders may be interested in issues that occur at different scales for example farmers are likely to be interested in impacts at the spatial scale of an individual farm and the temporal scales of both a single season and a span of several years while water resource planners are more likely to be interested in sustainable watershed spatial scales over a decadal planning horizon there may also be a mismatch between stakeholders scales of interest and the scales of relevant biophysical and socioeconomic processes hamilton et al 2015 conceptual models are typically constructed with some sort of diagram using arrows to mark influential relationships and flows argent et al 2016 robinson 2008 jakeman et al 2006 narratives or other visual and textual descriptions of what happens if may also be useful e g rich pictures data collection and knowledge elicitation methods are commonly used to develop the preliminary conceptual model such as desktop literature review expert and stakeholder interviews and workshops vennix et al 1990 the components and processes should be broadly described in terms suitable for stakeholders who are not modeling experts qualitative descriptions regarding how a change of one component or process can change the state of another can be valuable and can provide insight regarding the direction of change whether it is a relatively small or large effect and what other factors may influence that relationship liu et al 2008 quantitative measures and the detailed rules and equations specifying relationships are established in the development phase conceptual models are also useful when adapting and or integrating pre existing models which may well be required by the water management sector because of their legacy of and commitment to their previous investments in hydrological model types in particular here revisiting the conceptual model relates the existing model s to what may be a new problem as well as identifying and resolving any conceptual differences between the adopted model and the problem as perceived by the modeler and stakeholders belete et al 2017 this is an essential step before further resource investments are made 4 4 1 modeling technique as the conceptual model is clarified the type of modeling techniques to be used must also be decided there are usually many different possible techniques that could be selected each with different strengths kelly et al 2013 fulton et al 2015 hamilton et al 2015 selection is influenced by factors including the level of knowledge about the system of interest the level of spatiotemporal detail to be represented the type of processes to be represented and the nature of the data to drive and calibrate the model see badham 2015 kelly et al 2013 anderson et al 2015 for guidance broadly however conceptual or toy models or serious games are typically more suitable for social learning and communication oriented purposes argent et al 2016 van der wal et al 2016 decision support on the other hand typically requires mathematical or computer models which require more quantitative data and different skills to develop method selection may also involve trading off among the best method modeling effort and participation in the model development and possibly also model application sophisticated physically based mathematical models for example can be difficult for non specialists to understand the model s structure and relationships between model input and output and thus explore adapting an existing computer model for a new iwrm objective can reduce the need for skills and time and the model may be already familiar to some participants but this may be at the expense of overall engagement the experience of working with some form of a model can assist non specialists to contribute to iwrm more effectively than discussing the specification of the expert knowledge to be embedded in the model for example monks et al 2016 describe a modeling effort where those involved in conceptualizing and developing the model had less time for exploring with the model but explored more adventurously than those who were provided with pre prepared models and given more time for exploration 5 phase 2 development the development phase converts the preliminary conceptual model into a complete model ready for its intended purpose within the project this phase comprises five steps data collection model construction model calibration uncertainty analysis and testing in this phase the steps are particularly interwoven and some elements must be undertaken in parallel or iteratively for example testing which is a challenge in multi component iwrm models especially those with system feedbacks is an ongoing process during model construction as it may be necessary to test and re test each model element its linkages and the full model itself to ensure that the new or modified element has not interacted incorrectly with existing elements bertolino 2003 vale et al 2016 even steps that appear sequential may be revisited for example an unsuccessful attempt to calibrate the model might lead to amendments in the model implementation revisiting construction which could further require additional data to be collected nevertheless it is useful to identify separate steps to ensure all key activities are given due consideration in the modeling process for mathematical or computer models much of this phase is technical with the modelers working from literature expert opinion data as well as with stakeholders to formalize the important processes and system properties previously outlined in the conceptual model although not discussed in depth here similar steps are performed in the modeling process for qualitative models for exploratory and communication purposes malekpour et al 2016 these methods typically use knowledge elicitation processes and diagrams to describe relations between stresses and system responses and validate the final model with stakeholders to ensure it reflects the shared understanding of the system 5 1 data collection data are required to implement the conceptual model and to test that implementation in iwrm models these data may be a mixture of numerical values as is commonly the case for hydrological components and categorical information where measurements and knowledge are less precise or more qualitative as could occur with some social or ecological components as well as measurements relevant data encompass information from established theory existing models published studies assumptions stakeholders and expert opinion data collection involves compiling cataloguing obtaining and evaluating all of these forms of knowledge this step also identifies the data and knowledge that are not available and considers potential proxies for those gaps some of the specific questions to be considered in this step include what data are needed to represent theories included in the model for each relationship and overall how accurate do the data need to be what quantitative data are available what is the quality of the quantitative data what are the temporal and spatial scopes and resolutions of available quantitative data what qualitative data are available particularly with respect to the social and ecological elements of the system how can gaps between available and needed data be addressed are there any privacy intellectual property ip or other constraints on the use of relevant data do any datasets contain sensitive information once compiled some of the data may need to be cleaned transformed into another format or rescaled to be compatible with the other data or the model requirements note that more data may become available throughout the modeling process as stakeholders gain understanding and interest 5 2 model construction the model construction step converts the conceptual model design into a functioning quantitative model or models this involves rigorously developing the details of all the processes to be represented in the model s each process must be formalized as a set of equations or rules that specifically describe what happens in the model s in situations likely to be encountered and then implemented within the model s for example deterministic differential equations could be used to simulate the way in which water availability changes over time due to extraction evapotranspiration or other processes alternatively relations could be represented probabilistically for example if the population increases by more than 20 then water requirements will increase by 20 with 50 probability or 10 with 50 probability development of the detailed equations or rules may need to consider alternate models such as those including alternative ideas about system geometry or processes which arise from discussion between stakeholders and the modelers considering alternative model concepts is often an extremely useful communication approach especially if they are perceived to be rival or advocacy models ferr√© 2017 these contentious alternative models can help identify what future data collection might discriminate between the rival ideas and allow all stakeholders to have their ideas considered and vetted within the framework provided by the modeling process elements within the overall model s may require translation for example they may be at different temporal or spatial scales which then require upscaling and downscaling to communicate hamilton et al 2015 moreover there are aspects of the system that may be conceptualized with different paradigms or units so inputs and outputs need to be transformed from one type to another for example water quantity may be required for some equations and flows for others one approach is to construct the quantitative model with several sub models that address components of the conceptual model and overall system which facilitates the use of different types of equations or rules where different considerations are important the sub models typically pass relevant information to each other and detailed description of the communication between sub models is developed prior to model construction different approaches to variables such as scales must be reconciled for interoperability of the sub models brandmeyer and karimi 2000 and evaluated for conflicting assumptions between sub models mackay and robinson 2000 such considerations are especially important for iwrm models which include quantification of hydrologic environmental decision making and other social processes technical implementation of the model may be achieved by adopting pre existing modeling platforms libraries or else developing the model from scratch these options indicate from least to most the required amount of software development knowledge that has to be on hand platforms refer to software packages that allow a user to construct a model without any coding knowledge often through the use of a graphical user interface libraries provide a pre developed model or set of models along with utilities and toolsets interaction with which often requires code to be written research is often conducted in specialized niche contexts not covered by generalized platforms and so it is not uncommon for additional code development to serve the modeling s specific purpose ahalt et al 2014 development of an integrated model requires collaboration between domain specialists e g surface and groundwater hydrologists various types of social scientists including economists social and policy domains ecologists etc and leadership from integrated modeling experts regardless of the development approach adopted implicit assumptions or misunderstandings through the model development process will likely induce unrecognised limitations in the model functionality tests to identify these mismatches and other general errors should be designed and developed preferably before any model construction begins but may also be created post hoc doing so communicates to others what the intended model limitations are and provides confidence that the model functions as intended these tests may be modified throughout the model construction process this subject is expanded further in section 5 5 as well as the technical process of building the model the model construction step includes consideration of broader model choices and constraints on the implementation such issues include the limitations or assumptions associated with the modeling techniques being used and how these are to be managed this includes the levels of abstractions and simplifications whether there are licensing or other constraints on the tools used to develop or apply the model accessibility and usability of the eventual model including issues of interface design for computer based models potential reusability of parts of the model and the intellectual property rights that should be asserted for the developed model at the conclusion of this step the quantitative model s is are run and transforms model inputs into outputs of interest however how well these outputs represent the socio environmental system is not yet evaluated which is the focus of the remaining steps in this phase 5 3 model calibration calibration adjusts model parameters and other model factors such as boundary conditions to best reproduce observational data often done formally using a metric and possibly constraints in an objective function though visual and other qualitative approaches may have value bennett et al 2013 qualitative approaches to parameter settings can be especially useful in non hydrological components e g ecological social processes of iwrm models where data on observations are more scant and expert knowledge is required some model components in iwrm particularly of a hydrological nature are capable of accurately representing major relations and predictions that reflect broad properties of the system typically other predictions will have less certainty such as those requiring knowledge of small scale properties of the system e g preferential flowpaths needed to accurately simulate groundwater travel times or those related to the social components of the system after initial model construction model parameters and other model factors are varied to better fit what was measured or expected from a given system for a set of conditions a process called history matching the model output is compared to observed or expected equivalents in the data to assess how well the model is able to represent the system of interest when the observed data themselves are inadequate to constrain all model parameters theory and expert soft knowledge can provide further constraints if the modeling contradicts well grounded theory this may point to an error in the data or the model structure model parameter reasonableness is usually not judged using exact values as they are not typically available distributions or plausible ranges are used instead observational data are subject to a range of uncertainties and biases including measurement or sampling errors and spatial and temporal heterogeneity history matching does not require or even expect an exact match between observed data and model output indeed too close a match can suggest overfitting where the model is fitting errors within the data which commonly degrade the model s ability to make predictions in other conditions beyond those operating in the calibration period some level of tolerance or an acceptability threshold is therefore required when a model both fits the observations or conforms to expectations and has reasonable values for model parameters and other factors it is deemed calibrated anderson et al 2015 ch 9 it is important to apply history matching with multiple criteria these criteria should include many different observation types in order to break correlations between model variables different observation types in the hydrological component of iwrm models could for example include measurements of water balance terms evapotranspiration groundwater levels streamflow rates etc in addition decisions related to how to weigh the importance of the different observation expectation types and constraints on them directly influences the trade offs in fit anderson et al 2015 diverse comparisons can also reveal different areas where the model is strong or weak haasnoot et al 2014 parker 2009 and many weak signals that each assess different aspects of reality can be more informative than strong performance against a single aspect of behaviour grimm 2005 however even with several criteria judging calibration may not be straightforward communication with stakeholders during calibration commonly includes discussing data or processes important to the model that may not have been sufficiently considered it may also include showing how different calibration approaches yield different results and if these differences may have different practical implications moreover the calibration step should also clarify that many alternative models may fit or explain the same observational dataset s typically a model is only useful for decision support if it is perceived to be credible by stakeholders often model credibility is primarily influenced by its ability to simulate what has been observed in the socio environmental system this can be challenging as stakeholders often obtain an understanding of inherent uncertainty in the modeling process from their everyday experience of the weather report however rather than immediate evaluation being available as it is for a weather forecast the timeframe of iwrm processes represented may be combinations of months years decades and even longer thus continual evaluation of model outputs such as is done with weather forecasting is generally difficult to conduct rather the iwrm model calibration functions best when it provides tests that are meaningful and intuitive to stakeholders which in turn facilitates building the stakeholder trust required for adopting the modeling results 5 4 uncertainty analysis once a model is calibrated to what is known about the system it can become a generator for predictions in other situations where data are typically not available model predictions portray the estimated effect of changes to model inputs that represent current stresses such as land use management or stream or aquifer pumping rates and or future conditions of which there could be many in iwrm cross sectoral issues such as changes in climate energy use and demographic patterns or policy changes like managed aquifer recharge opportunities and water trading rules however a model s ability to create representative predictions is expected to be worse than its ability to simulate calibration conditions there is uncertainty in the model that was constructed not only in the ability of field social survey and other observations to constrain the model inputs and the necessarily simplified structure of the model that represents the iwrm system being modeled but also in how specified future conditions represent what actually occurs an uncertainty analysis describes the uncertainty in the model output under stipulated conditions input data used to calibrate the model model structure and other assumptions parameterization output data used for calibration and future scenarios used for simulation of the model if model outputs used for calibration are very sensitive to one or more model factors that is model outputs vary substantially the plausible values of the model factors will be constrained in calibration and their uncertainty is reduced when those model factors are also important to model predictions the prediction is expected to have less uncertainty when calibration data do not appreciably constrain the model factors important for a prediction that prediction is constrained primarily by soft knowledge and may have a high degree of uncertainty there are many ways to assess and describe uncertainty in the predictions guzman et al 2015 the best representation will depend on the context in which the iwrm problem operates and the requirements and understanding of the stakeholders hunt 2017 in iwrm where cost benefit trade offs are often assessed a more quantitative approach to characterizing uncertainty can be valuable guillaume et al 2016 most straightforward is a simple reporting of model predictions under a few specific conditions e g streamflow depletion land use practices water allocations or environmental flow releases under drought and wet conditions however most approaches involve a large number of simulations under different conditions so that the uncertainty can be described over the full range of modeled behaviour basic investigation of uncertainty might include a sensitivity analysis e g hill and tiedeman 2006 norton 2015 which adjusts different model factors by some known amounts and examines the effects on the key model outputs higher level uncertainty investigation might include monte carlo methods that report uncertainty in terms of a probability distribution of predictions simulations which are generated by running the model many times using samples pulled from a probabilistic representation of the model factors referred to as realizations monte carlo is computationally expensive because there are many combinations possible hence many model runs are required to characterize the behaviour of the model across the scenario and parameter space anderson et al 2015 pp 471 476 this is especially the case for iwrm modeling where potentially a large number of input and output variables are of interest as the investigations expand to multiple systems that incorporate hydrological ecological and socioeconomic systems it may therefore be more appropriate to sample the model inputs more efficiently latin hypercube quasi monte carlo and constrained walking methods campolongo et al 2007 are examples of pseudo monte carlo sampling methods emulation methods are increasingly being used to build faster running surrogate models of those models whose runtimes preclude monte carlo type sampling asher et al 2015 razavi et al 2012 machac et al 2018 in this way sensitivity and uncertainty analysis can be undertaken using the response surface of the surrogate model e g yang et al 2018 there are two additional uncertainty approaches that are well suited for iwrm problems one is scenario modeling especially of plausible but extreme case scenarios anderson et al 2015 pp 458 460 kwakkel 2017 guivarch et al 2017 mills et al 2018 anderson et al 2018 in this case a set of model inputs is developed that represent factors that when combined simulate the conceivable extreme but also intermediate outcomes that might be reasonably expected in this case the analysis represents outer edges and intermediate points of the envelope of possible model forecasts which can inform the iwrm decision making beyond simple reporting of expected outcomes this approach is well suited to simulating future conditions such as climate scenarios and cross sectoral influences and can be undertaken in a flexibly prescribed robust decision making framework a second more qualitative approach is quality assurance of the modeling process itself refsgaard et al 2007 where checks are undertaken to ensure that a model has been properly applied and decisions and choices are monitored and recorded to enable transparency and reproducibility of the process 5 5 testing of model robustness before it can be used in the application phase the model should be tested in several ways as the model is constructed the individual elements must also be tested to ensure the construction correctly implements the rules as intended standard model testing includes debugging and unit consistency checking balci 1994 jakeman et al 2006 as new components are being added to the model additional tests are run to ensure the new component has not generated errors in the already tested parts of the model tests of sufficient scale and scope increase confidence that model behaviour is as intended in practice this approach is similar to bayesian inference rarely can it be determined that model behaviour is correct across all possible scenarios but a large number of successful tests increases confidence davidson pilon 2015 the ability to invoke these tests in an automated manner can help ensure a sustainable pace of model development this is especially true for larger scale projects as automated running of tests which may be run after a change in a model for example aids in reducing the time and resource costs that may be incurred as model complexity increases having such tests and the requisite infrastructure in place assists in the model construction process maintenance see section 7 3 and may serve as further technical documentation of the underlying processes within the model see section 5 1 however effective testing is much broader than model verification including evaluation against a model s intended purpose and assessing the accuracy of model outputs bennett et al 2013 conditions simulated during calibration may not encompass the range of conditions that the model will use to generate predictions in the application phase tests of the model s robustness can build credibility in these forecasts including checks such as can the model successfully simulate extreme conditions e g drought or flooding do test cases generate output that is consistent with manual calculations or other existing models does altering a variable to an extreme value change model outputs in an expected way such checks may also reveal issues with data sets in use such as implausible positive negative values e g data sets were generated with interpolation mismatch in expected spatial and temporal resolution and range across model elements mismatch in assumed value types exchanged between models elements e g a categorical classification given as a textual representation one as opposed to its numerical equivalent a common form of qualitative assessment involves domain experts who review the model logic and simulated output details to assess the degree to which a model appears effective in terms of its stated aims i e face validity van der sluijs et al 2005 there may also be the opportunity to quantitatively assess predictions directly using a split sample or cross validation approach where some data are not used during calibration but instead provide test cases if insufficient data are available for this then a post audit might be performed after sufficient time has passed that a prediction can be evaluated or in an ongoing process as new observations are acquired such as those derived from adaptive management a final check involves revisiting appropriateness or a model s fitness for use this could include assessing whether the user interface is sufficiently easy to use by the intended users easy to understand from the user s perspective not the modeler s and sufficiently well explained in lay terms for models developed for participatory or educational purposes the criteria pertaining to its usefulness may be more important than achieving a high performance or accuracy elsawah et al 2017a b using the case study of the chesapeake bay coastal system allison et al 2018 argue further that for addressing wicked problems in socio ecological systems it is more important that one seeks holistic modeling of results yielding robust directions of system change than high numerical precision of outputs achieved through increasing model complexity jakeman and letcher 2003 argue similarly that in iwrm modeling the broad objective should not be about treating system outputs as accurate predictions it should be aimed at allowing differentiation between system outcomes under different conditions e g policy drivers at least with a qualitative confidence 6 phase 3 application application of the model ensues according to the modeling purpose determined during the planning phase for simulation games mathematical models and computer simulations using the model includes both running the model with various settings or options and analyzing the generated output for diagrammatic and other models used for thinking and learning application of the model is more qualitative and generally consists of one or more discussions and a report concerning the contents yet such discussions are also critical for mathematical models and computer simulations as this phase can enhance understanding of impacts of decisions made regarding model structure and input data from earlier steps this phase comprises three steps experimentation analysis and visualization and communication these steps are often iterative with discussion about the results from initial model runs stimulating additional questions that lead to further experimentation analysis and discussion the modeling project is most effective when the application phase provides stakeholders with an understanding of how the model works builds confidence in the model and supports substantive discussion about iwrm trade offs and issues 6 1 experimentation there are two key considerations for practical experimentation in order to obtain robust results which parameter and other factor combinations are to be used and how many runs are required each modeling effort will have different responses though some broad generalizations apply across most iwrm problems a mathematical or computational model will typically have two broad types of manipulation the first selects between specific scenarios of interest where scenarios reflect simulated system stressors or conditions different from calibration stresses and conditions scenario definition can encompass a range of possible changes such as different policy options e g water trading rules regulation of water quality varying operating rules or conditions for a dam and conditions e g high or low rainfall high or low farm input prices and product returns land use set at historical patterns or allowed to vary over time together with the parameter values or distributions that define the scenario such as the amount of rain each day during a high rainfall year the second manipulation goes beyond specific scenarios of interest by using computational methods to explore the plausible ranges of model parameters and other factors more broadly efficient methods of parameter space sampling may be used to limit the number of simulations required which is particularly useful when model runtimes are prohibitively long although the process is similar to the uncertainty analyses conducted during the development phase the purpose and hence selection of input values and the analysis are different here the model is manipulated to explore what ifs posed by stakeholders and to understand the uncertainty associated with specific scenarios rather than to characterize the robustness of the model 6 2 analysis and visualization the objective of the analysis and visualization step is to distil the insights gained from model development and experimentation in order to identify those meaningful to the stakeholders including decision makers however multiple scenarios and experimentation runs combined with a likely diversity of interests and expertise of the stakeholders can make the reporting lengthy and analysis complex simple analytical approaches such as regression methods are of limited use as there is typically no single equation for the overall system instead multidimensional visualization methods such as heat maps box plots and arrays of charts allow communication of important relations insights and system responses communication may be further aided by contextualizing the presented analysis analyses may be contextualized by overlaying data on a map providing spatial context comparisons against historical events temporal context and simulated results scenario context some key questions to consider during the analysis and visualization step how do outputs important for decision making respond to changes in inputs parameters and other factors including interventions are there thresholds or tipping points in the parameter space what are the likely outcomes to the questions addressed by the modeling are the results sufficiently accurate are some stakeholders more adversely affected by model shortcomings than others in addition to the most likely outcome predicted by the modeling what is the distribution of outcomes for individual stakeholders in addition if comparing iwrm options can any options be excluded because a better option exists that applies to all situations tested referred to as dominance are best options in some situations undesirable in others are there robust options that are good in all feasible or even probable situations even if there are no situations in which they are the best lempert and collins 2007 once all possible model results are culled to those that have utility and relevance for the iwrm decision making process they are finalized for distribution beyond the modeling team 6 3 communication the final aspect of model application is to communicate the insights from the modeling process in a way that most effectively contributes to model credibility and to policy or other decision making one of the key challenges in an iwrm process is how to communicate with stakeholders about model results in a way that enables the stakeholders to understand the system being modeled at a minimum the model s scope assumptions and capacity should be clearly specified in terms of what the model can and cannot do to help the audience understand the model limitations to achieve effective communication the modeler should consider the needs and background of the stakeholders and tailor the results accordingly even a simple model may be difficult to explain particularly when there is a large disparity with respect to technical knowledge between the modelers and the stakeholders one must balance an inclination to relate too much detail against appearances of hiding important model information skilful meeting facilitation will often be required to ensure equity among stakeholders which is more challenging where there are diverse or contentious stakeholder interests and with increased importance of decisions an audience analysis may be useful to help plan the communication this analysis includes questions about the motivations for the stakeholders participation their understanding of the system and their prior experience with models hall et al 2014 it may be desirable to segment the stakeholders based on the audience analysis for example by offering some groups tailored discussion beforehand and therefore providing a better opportunity for all to contribute meaningfully when they all meet together one approach to communication is to focus on the issue for which the modeling was developed presenting the key insights and connections as a narrative richels 1981 the objective is to be clear and concise and assist stakeholders in understanding the relevant issues with the modeling supporting the discussion once the essential story is clear more detailed discussion is required about the critical modeling assumptions limitations and their implications for results and recommendations another approach is to use iterative discovery where the modelers and stakeholders jointly investigate a series of scenarios particularly for complex problems that involve deep uncertainty fu et al 2015 rather than simply providing the results this method involves running the model alternating with discussion to raise new questions and propose further scenarios to be run the iteration fosters a knowledge partnership between modelers and stakeholders including decision makers enhancing the modelers understanding of stakeholder needs and their understanding of modelers knowledge thus leading to more useful presentation of results this contributes to stakeholder confidence in the modeling and modeler and therefore greater acceptance of the model results moreover both the stakeholders and the modelers attain a better understanding of the modeling assumptions and how they could potentially impact any decisions to address confusion over terms or concepts in both the model and analytical techniques used the verbal or written presentation of models should be carefully crafted so that key but unfamiliar terms are illustrated with examples and thought experiments and feedback should be sought from the audience to verify that they understand the presentation should utilise images visual aids and strategies kelleher and wagener 2011 as well as actual examples say from similar systems particularly when complex ideas are being discussed in addition to the potentially technical nature of the discussion stakeholders may have a different perspective of the system that can lead to difficulty in forming a mental picture of how the model represents the system and dissonance with their pre existing understanding of that system if the modeling provides a different system view than that held by the audience it is important to not dismiss their perspective but to use their own relevant observations to begin to see the alternative perspectives hall et al 2014 such perspectives can form the basis for additional scenarios 7 phase 4 perpetuation the perpetuation phase plans for the future this phase comprises three steps documentation process evaluation and monitoring and maintenance where relevant this phase integrates the model into ongoing policy processes and ensures it is maintained and updated when required and or when new observations become available so that the model can contribute to future decisions models for decision support often require a high degree of technical development and testing to provide appropriate estimates as meaningful input to decisions especially those involving high risks and intensive investments this development can incur large costs reuse of the model for future or similar applications can help to justify that cost for example an australian multi year basin scale modeling investment assessed future water availability in the murray darling basin and the impacts of development water extraction and climate csiro 2008 subsequent model outputs continued to provide scientific evidence to assist in later development of the murray darling basin plan in contrast some models are used only once for example models developed as thinking tools for a given stakeholder group and timeframe are required only during the participatory process to generate the intended learning pidd 2004 even in such cases however the steps in the perpetuation phase should be explicitly considered for example while a full process evaluation is unlikely to be appropriate reflection on the modeling process and discussion with the stakeholders about what they found useful can improve future similar modeling exercises 7 1 documentation the type of model documentation and the detail included depend primarily on who will use the model s in the immediate present and future there are three common types of model documentation user documentation technical documentation and the project report user documentation should assist users in running and interpreting the model outputs it should be focused on providing clear instructions for the main model objectives this documentation may include user interface description including screenshots interpretation of each input and output along with sources of data tutorials with incremental examples for training and credibility examples that reflect real scenarios guidance for running the model and description of potential common pitfalls technical documentation has two parts the first is a model archive that includes machine readable model input output executable and source code if available the second provides detailed descriptions of the model s and justification for the choices made in the model design it will typically include choice of modeling type and method including how limitations are managed assumptions and their justification or reasons rules and equations scales of representation variable definitions and codes data dictionary calibration method and results preferably with relevant uncertainty assessments data sources including references to theory such documentation serves two purposes scientific credibility and support for future development model results must be able to be reproduced and replicated if they are to be considered part of the scientific method morin et al 2012 peng 2011 technical detail about the modeling is therefore critical for credible model use in iwrm applications in addition to demonstrating scientific rigour such detail may be necessary to meet legal obligations associated with some decision support models if a model is intended to have an enduring presence then technical documentation that includes description of code or software is necessary to allow a future developer to modify or add to the existing code base its role is to describe each function method routine or class and why it was constructed in the way that it was modern code documentation is frequently done within the code of the model itself using a variety of tools available to auto generate or extract that documentation examples depicting how the code can be used may be included and doubly serve as executable test snippets the advantage of including documentation within the code itself is that it is more likely to be updated as the model is developed or modified well documented and tested code can be considered a necessary requirement to achieve full reproducibility and transparency and eases the maintenance burden often textual or narrative descriptions of the represented processes do not fully capture assumptions made during development both conceptual and technical which results in implementation differences subsequent implementations relying solely on descriptive documentation may have differing behaviour compromising model verifiability hutton et al 2016 if elements of the model s were developed in a sufficiently abstracted manner it is possible to reuse these beyond the original use case this is the underlying concept applied in component based modeling practices which require models to be coded to a standardized specification peckham et al 2013 adhering to such standards may also allow semantic information to be extracted enabling high level comparisons between different models within a model repository several guidelines exist that are intended to ensure adequate information is provided to enable replication and reproducibility these include reporting for deterministic models anderson et al 2015 reilly and harbaugh 2004 the odd protocol overview design concepts and details for agent based models grimm et al 2010 m√ºller et al 2013 and sd doc approaches for documenting system dynamics martinez moyano 2012 the project report is written for the client who commissioned the modeling it summarizes the analysis and provides a written form of communication focusing on the specific scenarios of interest key elements include scenarios and results selected graphical and other visualizations interactive where warranted to highlight messages and information about the limits of the results including assumptions uncertainty analysis and plausible parameter values that would generate different results regardless of model purpose model documentation should be continuously developed over the course of the project furthermore it is difficult to conceive of a situation in which a single person is able to contribute all the requisite domain knowledge model implementation details model interactions and data including sources types and pre and post processes applied documentation is therefore most likely to be developed collaboratively 7 2 process evaluation evaluation of the modeling process assesses whether the desired role of the modeling was effectively and efficiently achieved within the broader project a formal process evaluation informs future modeling projects and should consider issues such as did the model and modeling process meet the role originally envisaged and were changes to that role intended and appropriate how effective was the engagement of stakeholders who needs to be added to any future modeling process did the project meet time and budget objectives how much adjustment was required to the original plans what would have improved the modeling process and at which phase step depending on the modeling purpose several criteria have been developed for evaluating a modeling exercise for example haasnoot and middelkoop 2012 suggested three criteria predictive success has the future turned out as envisaged decision success robustness have good decisions subsequently been made and learning success have scenarios enabled participation and learning a different set of criteria was suggested by alcamo and henrichs 2008 relevance credibility legitimacy and creativity in a post audit merritt et al 2017 identified 33 factors for realizing success in an analysis of 15 water resource projects of the four factors considered to be the most necessary all of them largely non technical three related to aspects of stakeholder engagement in the modeling process the other to critical thinking around problem framing and the role of models 7 3 monitoring and maintenance the final step is to put into place the plans and mechanisms to ensure that if needed the model is updated as required and continues to be relevant for any policy process in which it is embedded this can be particularly problematic if the policy organization does not have technical expertise or funds to maintain the model s or if a model has been developed by an outside entity so that a transition process is required if the model is to be maintained beyond its final documented form the key elements of the maintenance plan are to establish a process for updating any input data documenting new data sources and deciding the timing and assigning responsibility for the updating a succession plan is also required to provide continuity and maintain the knowledge base about the purpose and results of the model this plan should include lists of materials including documentation and training materials and where they can be obtained many models are revised and adapted for different but related policy questions for example a similar decision but located in a different area of the problem domain maintenance questions to be considered include how easy is it to adapt the model conceptualization as well as implementation for example to another catchment or set of water resource issues what expertise data and other resources are required in order to make changes to the existing model what changes in model design might be needed in the future for example boundary conditions scales processes etc who can approve changes what are the implications for the policy process when important existing model outputs appreciably change as the model is changed which steps in the modeling process will most likely need to be reiterated or revisited and can any steps be excluded in specific situations 8 five key areas for future development in iwrm modeling although there is always room for improvement iwrm is now endowed with a rich set of techniques for modeling both social and biophysical aspects of a system as described in section 2 there are also many authors providing different prescriptions and recommendations for operationalizing iwrm we have argued here that the overarching challenge for the future is integration and implementation bammer 2013 of the knowledge gained from experience while the application of iwrm in regions with weak institutions is often seen to be challenging the need to better tailor analysis to policy making contexts is universal a key concern for instance in the field of policy analytics tsoukias et al 2013 daniell et al 2016 de marchi et al 2016 we focus now on five key areas for future development knowledge sharing overcoming data limitations informed stakeholder involvement social equity and uncertainty management 8 1 knowledge sharing the ability of the iwrm modeling community to share and exchange knowledge about the practices employed throughout the modeling process is essential for growing the field and expanding its applications knowledge about how to frame a particular iwrm problem and perform particular modeling activities i e knowledge in use is valuable for beginners learning the craft of modeling distilling information about learnt lessons and promoting ideas about new solutions knowledge sharing of practices is also valuable to inform comparison across multiple dimensions e g methodological design case studies issues of interest a prime challenge is to document systematically and with sufficient detail the approach taken in projects so that the knowledge of how to do iwrm modeling and lessons learnt accrue for future projects it should be possible to document a wide selection of iwrm projects to form the basis for on going knowledge sharing in the future systematic and consistent documentation practices and tools for this purpose still need to be developed to support transparency and learning documentation should not only be focused on describing the modeling process as what happened but cover the reasoning i e why did it happen behind the decisions and observed outcomes throughout the process and ideally it should entail critical reflection on what could have been done differently lahtinen et al 2017 one promising approach is the use of patterns to elicit capture and formalize this knowledge alexander et al 1997 a pattern is defined as a representation of a solution to a recurrent problem in a particular context a pattern could be with respect to any component of the iwrm process where modeling related decisions are taken patterns can be represented in different forms and presentation i e diagram template regardless of the presentation a pattern must have the following elements 1 a name to facilitate use and communication 2 problem description 3 solution and 4 context the premise of the pattern approach is that once the successful practice has been recognized as a pattern and expressed in some pattern form it may then be reapplied to similar problems contexts which will eventually lead to improved practices new observations and new or refined existing patterns discovering and capturing the relationships among patterns results in a pattern language which provides a shared lexicon for the community to communicate about the different contexts where patterns can be used 8 2 overcoming data limitations data can pose a challenge to iwrm especially in relation to the multiple and diverse types of data including quantitative and qualitative representing the various system components and their different scales and quality the type of data for a given project can determine whether or not a certain type of model can be used or how well it can be applied to address this there has been increased interest in more flexible semi quantitative modeling approaches such as bayesian networks e g ticehurst et al 2007 as well as different couplings of multiple methods e g nikolic and simonovic 2015 limited data can place a major constraint on iwrm modeling basic hydrologic and meteorological data can be patchy and unreliable in countries in developing regions and those where governments place a low priority on investment in monitoring networks new forms of observations such as remotely sensed data provide promising opportunities for iwrm particularly in data poor environments however these require more efficient ways of extracting useful information from the raw data montanari et al 2013 poor availability of socioeconomic data with the general exception of census data is widespread and presents a major barrier to understanding feedbacks between society and the environment undermining many water management endeavours mcdonnell 2008 social media present an opportunity for bridging at least some of these gaps in addition stakeholder involvement in the modeling process will continue to play an important role in capturing social variables and processes 8 3 informed stakeholder involvement a common criticism is that stakeholder involvement is inadequate in decision support system dss exercises generally not just in iwrm for example zasada et al 2017 in a survey of eu funded research programs on dss for agricultural and environmental issues this is despite recognition e g merritt et al 2017 for water resource projects and mcintosh et al 2011 that attention to associated non technical issues i e social and behavioral is vital to success in particular when the purpose is to support decision making and social learning for example as noted earlier merritt et al 2017 identified aiming for open and transparent communication good relationships and trust and sufficient interaction between the development team and users as three of four main factors crucial to success thus time spent in the design phase where stakeholder engagement usually begins is time well spent to avoid rushing into poor decisions that can lock the project onto the wrong path nicolson et al 2002 projects therefore require the will and resources for attending to these aspects this may also require more flexible funding arrangements that recognize the value in fostering an evolving understanding of the problem and imposing soft deliverables e g problem formulation documents rather than rushing into quantitative deliverables a more technical issue in this respect is integration of stakeholder interests preferences and attitudes to risk into the modeling process for example in terms of indicators or objective functions variations in all these elements can lead to completely different results and negotiated solutions model output can be converted into indicators by filtering the uncertainty in many ways e g using different robustness criteria e g mcphail et al 2018 leading to different rival problem framings e g quinn et al 2017 this is not as simple as just obtaining and providing data from to the end user more thought and effort needs to be paid to methods like interactive data visualization that meet stakeholders information needs and empower them to explore the problem and devise possible solutions bridging scales is a particular challenge in stakeholder engagement within iwrm in principle stakeholders are well placed to comment on their immediate problems and changes they wish to see however they may not be in a good position to account for their own future needs let alone consider how best to achieve them given others preferences and interactions within the system modeling plays an important role in supporting stakeholders in making sense of the iwrm situation at longer timescales and larger spatial scales but connecting abstract strategic ideas to concrete operational concerns remains difficult rather than trying to determine an ideal scale to use in iwrm models it will be important in the future to reflect on the range of strategies available for bridging scales in a way that brings the necessary stakeholders together to achieve meaningful progress when implementing iwrm 8 4 social equity social equity is often sidelined in iwrm models especially compared with standard cost benefit considerations of different trade offs the distribution of benefits from water and in particular whether the needs and rights of different groups are met pe√±a 2011 warrants greater consideration the need to better consider the totality of benefits and costs associated with water management including indirect outcomes non use values option values etc and how these vary between each person and group is a broader challenge for iwrm in general social equity in water management is not just about equity of outcomes benefits and costs but also equity in the decision making process especially having a voice or opportunity to influence the process i e procedural justice syme et al 1999 given that the equity dimension is typically poorly captured in models its consideration in the modeling process then relies heavily upon interpretation of model results and implications stojanovic et al 2016 adequately addressing issues of equity in modeling necessitates careful consideration in the stakeholder planning step to ensure fair representation of stakeholder groups in the process and consideration of social processes and alternate knowledge sources or potential solutions particularly in the planning phase and application phase steps budds 2009 argued that failure to do so in the la ligua river basin in chile and reliance upon a purely physical assessment in response to a situation that was predominantly socio political led to a positioning of the model based assessment and its commissioning agency as the only legitimate knowledge source which closed down a range of possible solutions decisions made using the assessment then disadvantaged poor farmers who had not created the groundwater scarcity problem budds 2009 in the context of iwrm the modeling process should facilitate procedural justice and at the very least not be a further barrier to equity 8 5 uncertainty management uncertainty pervades the treatment of iwrm problems users and managers of a water resource deal with uncertainty every day in a variety of ways the use of modeling itself is a powerful way of organizing information to understand and reduce uncertainties measures for managing uncertainty in management and modeling are still mostly considered separately rather than being integrated though some research areas are making progress on this front e g in the deep uncertainty literature kwakkel 2017 maier et al 2016 much of the attention to uncertainty in the water resources literature focuses on sensitivity and uncertainty analysis of the hydrological models unfortunately these are among the most certain of integrated model components and the human and ecological processes warrant much more attention hunt and welter 2010 here we see an opportunity to prioritize uncertainty sources and deal with those most crucial among other things this will involve attention to uncertainty assessment of model components but also the propagation between the component linkages where there are often feedbacks qualitative uncertainty assessment e g van der sluijs et al 2005 refsgaard et al 2007 will be a necessary perhaps even the main ingredient a key issue is understanding how uncertainty accumulates and diminishes within an iwrm process including the model itself and therefore where efforts can be targeted to constrain it combining uncertainty in future conditions system understanding and stakeholder preferences quickly leads to an explosion in possible system outcomes which can be overwhelming for stakeholders to consider in addition to obtaining additional information or facilitating consensus processes uncertainty can be made more manageable for example by using adaptive approaches and looking for robustness rather than eliminating uncertainty especially where uncertainty leads to disagreement these techniques can be important to achieve progress in implementing iwrm without needing perfect understanding of a system work on these techniques has not however been completely synthesized and needs further efforts to support their implementation 9 conclusion despite promising benefits and high expectations the potential of iwrm in translating integration aspirations to successful on ground results has not yet been fully realized it is well recognized in the literature that modeling has a crucial role to play in operationalizing and successfully implementing iwrm this paper attempts to contribute to improving the implementation of an iwrm modeling project by providing practical and fit for context guidance of the practices employed through the various modeling phases and steps these practices explain the details of employing iwrm modeling our general approach identifies four broad phases that occur during the modeling process planning development application and perpetuation these phases are common to diverse iwrm projects each phase comprises several steps or tasks while one or more steps may not be required for some projects it is valuable to consider the relevance of each step so that omission is an explicit decision and not simply an oversight in practice there will be many ways to implement each step appropriate methods and level of stakeholder engagement depend on the specific characteristics of the iwrm problem aspects of the system being modeled as well as other contextual factors e g funding institutional setting data available or level of stakeholder conflict we have therefore focused on the objectives of the step and guided how the step may be undertaken finally we identified some important gaps in iwrm modeling practice related research highlighting the need for advances in knowledge sharing overcoming data limitations informed stakeholder involvement social equity and uncertainty management as well as some potential methods for answering those questions in short effective iwrm modeling should involve a process and set of practices that are fit for the given context and well grounded in the iwrm problem at hand the implementation of effective modeling practice can play an important role in facilitating the what and how of iwrm by contributing to identifying and understanding of the elements that underpin the problem and guiding the questions to ask and the issues to address acknowledgements this work was supported by the sesync national socio environmental synthesis center core modeling practices in iwrm project under funding received by the national science foundation dbi 1052875 badham s contribution was partially supported under national institute for health research grant cdf 2014 07 020 
26225,the effectiveness of integrated water resource management iwrm modeling hinges on the quality of practices employed through the process starting from early problem definition all the way through to using the model in a way that serves its intended purpose the adoption and implementation of effective modeling practices need to be guided by a practical understanding of the variety of decisions that modelers make and the information considered in making these choices there is still limited documented knowledge on the modeling workflow and the role of contextual factors in determining this workflow and which practices to employ this paper attempts to contribute to this knowledge gap by providing systematic guidance of the modeling practices through the phases planning development application and perpetuation and steps that comprise the modeling process positing questions that should be addressed practice focused guidance helps explain the detailed process of conducting iwrm modeling including the role of contextual factors in shaping practices we draw on findings from literature and the authors collective experience to articulate what and how contextual factors play out in employing those practices in order to accelerate our learning about how to improve iwrm modeling the paper concludes with five key areas for future practice related research knowledge sharing overcoming data limitations informed stakeholder involvement social equity and uncertainty management keywords decision making social learning stakeholders uncertainty calibration integrated modeling iwrm 1 introduction integrated water resources management iwrm is above all a process e g molle 2008 ibisch et al 2016 one that promotes the coordinated development and management of water land and related resources in order to maximize the resultant economic and social welfare in an equitable manner without compromising the sustainability of vital ecosystems gwp 2000 iwrm principles also known as the dublin principles have functioned to serve public participation in water resource decision processes and increasingly this encompasses participatory or the related collaborative modeling basco carrera et al 2017 indeed an iwrm project typically includes substantial stakeholder engagement and is supported by at least one jointly developed model such participatory modeling has recently been defined by voinov et al 2018 as a purposeful learning exercise for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representations of reality modeling for iwrm represents interactions between humans and their environment and therefore has much in common with social ecological systems modeling and the more recent concept of socio hydrology social ecological systems modeling often focuses on sustainability ostrom 2009 or resilience folke 2006 while socio hydrology focuses on how coupled human water systems interact and co evolve sivapalan et al 2012 iwrm is distinguished by a practical focus on supporting water management or policy decisions in a context that typically involves stakeholders from multiple sectors iwrm is underpinned by water balance and hydrological modeling adapted to a specific policy or planning setting this requires connecting hydrology with other socio environmental knowledge and component models see e g croke et al 2014 for a case study in the murray darling basin the term model however has many connotations here we define a model as an explicit representation of features and relationships of a target system badham 2015 within this definition there are many types of models including process physically and or behaviorally based gaming actor based conceptual and flowcharts each can have a range of characteristics such as static dynamic statistical empirical probabilistic and distributed parameter lumped parameter an iwrm process can utilise any of these here we emphasize quantitative models because they provide an appropriate framework for cost benefit and other trade off analyses as well as playing a key role in understanding managing and negotiating solutions for integrated socio environmental issues quantitative models 1 can serve as boundary objects to encourage and facilitate focus in communication among participants 2 help in problem framing and explicit boundary setting which bridge gaps in understanding that can divide participants and bring together multiple perspectives and 3 determine which actions out of all those possible are in fact being evaluated i e which hypotheses are tested falconi and palmer 2017 although these benefits also arise with other types of models the rigorous process of converting qualitative statements and broad understandings i e mental models into quantitative or at least categorical relations allows for testing the veracity of perspectives successful iwrm modeling can be effective in many ways and to varying degrees merritt et al 2017 in this paper we view success in a holistic way as the ability to embed the modeling process in a social process that connects scientists decision makers and stakeholders and achieves impact in accordance to its purpose which may vary from a shared understanding of a problem to policy analysis hamilton et al in review our starting point is that the degree of iwrm modeling success can be enhanced by the use of effective practices throughout the model development application lifecycle from framing key questions and defining objectives all way through to using the model to satisfy its intended purpose s adoption and implementation of effective iwrm modeling practices need to be informed by practical and fit for purpose driven guidance on how to mobilize the iwrm concepts and techniques towards successful outcomes for decision making and stakeholders although scholars have made strides in fleshing out iwrm e g tortajada 2016 conceptualizing modeling methodologies and reporting case studies there is still limited documented knowledge on the modeling workflow and the role of contextual factors in determining this workflow and the practices to employ practice focused guidance may help explain at a micro level some of the nuances experienced by those involved in iwrm modeling we aim to contribute to bridging this knowledge gap by describing the actions needed to develop a successful iwrm modeling project this focus on detailed steps and activities motivates the need to develop an in depth understanding of the iwrm process in particular seeking to understand how integration will be implemented how stakeholders are effectively involved and linking the modeling decisions to the variety of factors that make up the problem and system context we draw on findings from literature and the authors collective experience to articulate what and how contextual factors play out in employing those practices the guidance synthesizes findings about the modeling process drawn from literature reported in overlapping areas including environmental and hydrologic modeling jakeman et al 2006 anderson et al 2015 harmel et al 2018 engaging stakeholders vennix et al 1990 voinov and bousquet 2010 voinov et al 2018 and general modeling banks 1999 in the next section we summarize some key literature on the notion of iwrm including main criticisms arguing that these are concerned with the challenge of its implementation in a practical context and that operationalizing iwrm is assisted by a good modeling process the phases and steps that constitute an effective iwrm modeling process are later described in sections 3 to 7 this is followed by an outline of key areas for future development in iwrm modeling section 8 2 background the concept the criticisms and the operational role of modeling in iwrm it is a well accepted premise that modeling has a crucial role to play in the implementation of iwrm soncini sessa et al 2007 yet the effective use of modeling for operationalization of iwrm still faces important challenges for the future in terms of integration and implementation of knowledge these challenges are inherent to the nature of the iwrm concept itself and how it can be implemented in practice in this section we give an overview of those challenges and how they motivate this paper the first challenge is the difficulty of managing wicked problems in water policy when there are multiple pressures conflicting stakeholder values competing goals multiple decision makers limited resources and deep uncertainty in the context of integrated groundwater management jakeman et al 2016 bring together some 74 contributors to elucidate the governance biophysical socioeconomic and decision support aspects of iwrm the three iwrm goals of economic efficiency equity and environmental sustainability often conflict and so require trade offs molle 2008 criticisms of iwrm often focus on the practical challenges of implementation perhaps the most strident critic has been biswas 2004 who considers the definition of iwrm itself amorphous and questions whether integration across so many aspects is achievable tortajada 2016 responds to these criticisms by clarifying iwrm as a concept as a goal in itself and as a strategy to achieve development goals practical high level guidance on what to integrate is given by hamilton et al 2015 who flesh out ten dimensions of integration e g issues of concerns stakeholders spatial and temporal scales uncertainties etc as an extension of this challenge there has also been discussion in the literature about the water energy food nexus as either a competing paradigm to iwrm or one that could inspire improvements in the concept and operationalization of iwrm e g keskinen et al 2016 m√ºller 2015 but while representing a set of issues to be considered in many iwrm problem settings the water energy food nexus concept may focus attention on the food and energy sectors and draw attention away from the natural environment and possibly other social and economic water values climate change environmental change and adaptation also are being increasingly recognized as a component to be integrated with iwrm especially in regard to the planetary effort towards sustainable development and its goals including a specific target sdg 6 5 on the implementation of iwrm at all levels giupponi and gain 2017 the second challenge relates to handling the human element in iwrm and reconciling the conflicting agendas involved with water management grid 2016 for example iwrm has been criticized in that stakeholder engagement has been perfunctory the concept has not been accepted and practised by local water managers funke et al 2007 and technical integration in analyses has not coincided with a balanced institutional integration fischhendler 2008 or the modeling has not been convincing middlemis 2000 on the other hand grid 2016 argues that while iwrm is challenging because of the human element no other process can reconcile the conflicting agendas involved with water management the primary challenge therefore is in building guidance on how one implements iwrm or tackles the integration dimensions in the implementation of iwrm for a given problem whereas the literature includes some prescriptive guidance on how to implement an iwrm modeling process there is still a gap in linking this advice to practice for example hamilton et al 2015 take a step towards operationalizing integration by offering a framework for mapping research methods that can be used to implement each of the integration dimensions throughout the modeling process however the framework remains mainly prescriptive and work is still needed to document and reflect on the lessons from the actual execution of the modeling processes in summary we are well aware of the difficulties of operationalizing iwrm whilst ibisch et al 2016 have recently examined 14 iwrm projects and given a synthesis of lessons learnt from them we agree with giordano and shah 2014 that the lessons from attempts to implement iwrm are otherwise not well documented and have impeded its development beyond a concept for dealing with wicked water problems if we are to add substance to carrying out successful iwrm we need to begin to document systematically how iwrm projects are being carried out and evaluate the lessons being systematic will require a template that includes a set of categories defining the context of a problem so that patterns and anti patterns of iwrm operations in specific contexts can emerge for future guidance on what and how to integrate there is no alternative to us as a community than learning by doing iwrm while reporting on it and explicitly accruing the lessons in recognition of this need we have been associated with a pursuits project at the national socio environmental synthesis center sesync https www sesync org for you educator research themes pursuits funded by the united states national science foundation we come from a range of backgrounds spanning science and social science of water resource management as well as public health modeling and computer science the pursuit project aims to address the overarching research question of identifying the core practices that should be employed in developing and using models to support iwrm thus we argue for effective iwrm through a modeling lens but it recognizes that such modeling needs to be a process that is well grounded in the needs of the iwrm problem at hand hence we also focus on guidance for applied problems following good practice modeling can bring many benefits to the iwrm process including elicitation systemization and sharing of otherwise fragmented knowledge management of uncertainty clarification of tradeoffs facilitation of capacity development transparency and participation and inclusiveness the paper does not delve into the necessary institutional and governance reforms advocated by many for iwrm moreover we feel that there is still much progress to be made by working within whatever current governance constraints exist but also using modeling to infer and communicate how new arrangements and approaches might improve the effectiveness of water resource management at its core our paper s main aim is to address the how of iwrm modeling by guiding modelers and commissioners of projects on what questions to ask and what issues to address thereby prompting decisions through a model grounded process for an iwrm problem 3 introducing phases and steps for effective iwrm modeling iwrm modeling actions can be broadly grouped into four phases cf hamilton et al 2015 the planning phase identifies what is to be achieved how this is to be accomplished and what resources can be brought to bear the model is built and tested during the development phase and then used in the application phase finally models that become part of the ongoing policy or decision making process or are for routine operational use require a perpetuation phase these phases are sufficiently general that they typically apply to the widest range of iwrm activities and model types within these general phases are more specific steps see left side of table 1 or activities associated with the iwrm modeling process which may be implemented in different ways we provide a modeling centric view of an iwrm decision support process a policy centric or stakeholder centric view might identify and emphsize phases somewhat differently connecting to these to varying extents depending on the level of policy maker or other stakeholder involvement in the broader iwrm arena different authors have separated the modeling process into distinct actions for example black et al 2014 focus on the use of models to analyze water resource planning scenarios and their guidance is principally intended for modelers we step back and focus on what fundamentally is to be considered and achieved in each of the four phases and accompanying steps for the context in which the iwrm process operates the framework described here emphasizes flexibility and directs project attention to key points and questions as a way to facilitate better access to the iwrm concepts and tools available here we define context as the social and project characteristics that influence which steps are brought to bear and how they are implemented in any application specifics of what happens within the steps may differ important contexts for the model development include the general role of the modeling the funding levels for the effort level of stakeholder conflict type of governance setting relevant scales of the problem and tolerance for uncertainty description of each step within a phase is elucidated in the following sections using brief examples drawn from a range of iwrm settings typically based on quantitative water management modeling with substantial stakeholder involvement given the broad nature of iwrm problems examples alluded to here are necessarily illustrative rather than comprehensive although the phases are broadly chronological ordering is not intended to imply that the steps are necessarily sequential the steps presented in this paper are not intended to form a rigid checklist but capture key practices that should be considered and performed as needed a great deal of overlap and iteration between phases and steps can be expected some steps may be conducted concurrently whilst in particular projects others may not be needed at all for example tasks undertaken in the first planning phase might be revisited during the development phase if data are found to be of poorer quality than expected or the desired model outputs change similarly there is likely to be overlapping parallel progress between data collection model construction and testing as testing reveals weaknesses in the model and identifies additional data that would be required to reduce uncertainty in important model outputs such concurrent cyclic iteration is widely understood to be a part of hydrological and other models where the development and application phases of modeling provoke revisiting of previous efforts each step of the model can be seen as a path which has forks where alternative choices can be made the choices made by the modeling team can affect the quality of the modeling results for example choosing one stakeholder group over another can affect the relevance of the information included in the model lahtinen et al 2017 have developed a checklist to help modelers evaluate alternative paths and recognize and act on situations where changing the path may be desirable while not discussed further here it is a useful complementary tool 4 phase 1 planning the planning phase defines what the modeling is intended to achieve within the context of the larger iwrm project who should be involved and what resources are required it converts an initial need that a model or set of models is required into a description of the model s and a plan for how to meet that need efficiently and effectively models may need developing or if suitable can be adapted from already developed models this phase comprises four steps problem definition and scoping stakeholder planning project management planning and preliminary conceptual model 4 1 problem definition and scoping the first step is one of clarification and agreement about what are the objectives and especially which water centric issues and cross sectoral issues e g climate food energy environmental cultural are essential to be addressed this essentially determines what is and is not to be modeled a key aspect is to identify the questions that the modeling should answer however the form and scope of these questions and their answers are shaped by the intended users of the model s and identified stakeholders as well as the function or role that the model s and modeling process is to have within the broader iwrm project clearly defining the problem and how the model is to be used to address the problem determines many of the decisions made during subsequent modeling steps some of these decisions are whose views and knowledge are to be included which functions and processes are required of the model s expected level of model accuracy and complexity and how model utility will be tested 4 1 1 model function fundamentally a model is a representation of knowledge about a system i e past present or future and that representation can be used in different ways badham 2015 kelly et al 2013 oxley et al 2004 an iwrm modeling process is policy oriented because the defining characteristic of iwrm as outlined earlier is its focus on the human and decision making dimensions of issues such as ways to equitably manage sources of water or how to cost effectively allocate volumes of water in a river system to different uses and the environment commonly policy oriented modeling is supported by quantitative models of a particular water resource or other aspects of the socio environmental system which form the foundation for exploration of options and constraints a major component of these models tends to be physically based but sometimes data based models that advance scientific knowledge of or improve scientific tools applied to that resource typical examples include hydrology models beven 2012 which aim at understanding surface water behaviours hydrogeological models anderson et al 2015 to advance the knowledge of groundwater systems and water quality models fu et al 2019 chapra 2008 merritt et al 2003 to comprehend the fate of nutrients and or contaminants in water but increasingly these are supplemented or integrated with modeling of the human dimensions noel and cai 2017 kelly et al 2013 especially their behaviour and decision making at various levels such as individual e g farmer household industry irrigation community rural urban and policy jurisdiction local regional national policy oriented models can be further subdivided into those models used for policy support or decision making and those for conceptual thinking and social learning models used for policy support and decision making extrapolate from knowledge and behaviour to estimate potential consequences or a range of consequences of potential actions while everyday use of water models can include forecasting for operational objectives e g reservoir real time operational decisions for flood control hsu and wei 2007 iwrm modeling is typically for longer term strategic and or multiple objectives such as assessing spatiotemporal tradeoffs of impacts resulting from alternative management options e g for licensing abstraction volumes and locations under a varying climate these models are used either directly by the decision maker or their advisors or the modeling team may summarize the model outputs for different scenarios as advice for decision makers models for thinking and learning are used to communicate visualize and explore the system to inform the diverse groups concerned with a project such models can also integrate knowledge elicited from those groups e g see elsawah et al 2015 for cognitive mapping of stakeholder system understanding in this process the model acts as a translator or boundary object between groups with different understanding and languages about the problem the emphasis is on restricting all imaginable system behaviours to those that are reasonably likely while at the same time providing insights improving mental models and fostering interaction and communication among stakeholders the modeling process itself is often an important part of this role with possible cognitive or social outcomes including systems thinking consensus building and conflict reduction belt and blake 2015 for example pahl wostl and hare 2004 describe benefits of a learning activity that included gaining insights into complexities of the system and greater understanding of the actors own perspectives and the role of other actors in addition the openness of the process fosters the discussion of innovative ideas although presented as somewhat distinct in this discussion the purpose of a modeling exercise may be manifold a model that promotes social learning may also be taken further and used for policy support likewise scientific models may be harnessed by a modeling system in order to provide policy support furthermore the role may change during an iwrm process with different aspects of the model emphasized at different points for example social learning when generating options but then policy support when the options are being compared although developing a model or models may be the main deliverable of an iwrm project some complex projects may involve drawing on several sources of evidence and methods perhaps with the model playing a relatively minor part bots and van daalen 2008 for example there may be formal legal hearings and submission processes to obtain the views of members of the public as well as key stakeholder organizations in such a situation the modeling process needs to complement and interact with the non modeling activities and sources of information consider for example the objectives of the murray darling basin plan in australia where the objective was to determine a sustainable limit of water extraction in the basin csiro 2008 largely existing hydrological and ecological models were adapted to address this objective supplemented by socioeconomic analyses croke et al 2014 4 1 2 model question and scope clarification of the modeling purpose role and use triggers the first model design decision what are the questions the modeling is answering questions can be framed around a set of inputs and outputs iwrm inputs typically include drivers that impact water systems e g climate change water use needs for energy urban or agriculture demands stressors e g over exploitation of water resources and management and policy options e g riparian buffer management water allocation policy where financial and socioeconomic impacts are associated with each driver outputs of interest feed iwrm outcomes that need to be improved or maintained to address the identified issue s an example question is which management interventions acting in which locations are most likely to lead to desired outcomes a typical output would be the nature and magnitude of change in the physical water resource examples include how much water reaches the end of the river given a rainfall population and abstraction scenario or how much is some measure of pollution reduced and perhaps at what cost given a specific change in policy the modeling may generate several outputs that each answer a different question or multiple outputs might be used to answer a single question as iwrm projects typically involve researchers and stakeholders from different disciplinary backgrounds and experiences there can be different perspectives on what key questions need to be investigated brandt et al 2013 coherent framing of questions can be challenging and time consuming but the process of re framing the modeling questions is an essential exercise for building a shared understanding about the modeling inquiry and its implications for the system the process also sets relationships between stakeholders that will be active during the remainder of the iwrm process problem definition facilitates subsequent boundary setting and scoping of the problem domain identifying what aspects of the system are to be included excluded from the model in terms of its relevance or contribution to addressing the question of interest of course some elements of the setting may change over time challenges to problem definition are inevitable as iwrm is a wicked problem characterized by multiple and often conflicting viewpoints about the problem possible solutions who will be affected by the problem and who should take part in solving the problem based on the plausible interpretations of these issues and questions there are multiple problems to be defined and boundaries to be set walker et al 2003 dewulf and bouwen 2008 offer an example case study where three actors i e water power plant water management authority and sand miners have divergent problem frames about water management problems in their region the water plant which provides electricity to the region defines the goal as reducing soil erosion and resulting sediment flow to the reservoirs on the other hand miners do not frame sediment as a threat but as an economic opportunity problem definition is not a straightforward exercise but an organic process through which stakeholders reflect on their existing problem frames negotiate differences and ideally re construct their views to reach a shared representation of the problem although many iwrm applications are typically confined to the physical boundary settings of targeted water systems emerging concepts such as virtual water which involves the role of food trade in compensating water deficit can stretch beyond the biophysical boundary yang and zehnder 2007 social and economic considerations can also stretch the physical boundary for example income used to sustain a regional community may be derived not just from activities within the iwrm zone but may be supplemented from additional work outside a catchment nexus thinking is another driver for expanding system boundaries by promoting the importance of attending to the food water energy environment linkages and cross sector impacts hussey and pittock 2012 framing water problems using a nexus lens extends many of the aspects that need to be attended to including stakeholder groups to be involved goals and risks to be considered governance and regulatory framework and the interdependencies among the modeled systems however different problem interpretations are not only attributed to different stakeholder views and systems of interest the disciplinary background of the research team can be another challenge whereas diverse disciplinary teams are often cited as beneficial for problem definition and even critical for the success of an iwrm modeling process the assumption that any interdisciplinary team will effectively work together to develop an integrated problem cannot be taken at face value nicolson et al 2002 interdisciplinary teams do have their challenges and can stumble over their differences bark et al 2016 such as different views about what constitutes data different research agendas etc there is still a need for more guidance into how interdisciplinary teams can work together in iwrm modeling projects to negotiate their differences and reach an integrated understanding of the problem 4 2 stakeholder planning iwrm inevitably involves multiple stakeholders and interest groups that include domain experts effective stakeholder engagement leverages diverse knowledge and perspectives in order to inform what water related issues are addressed the subsequent construction of realistic input data scenarios and potential management solutions elsawah et al 2015 the extent of stakeholder involvement however depends on both the project needs and the interests of the particular stakeholders the purpose timing ways and mode of engagement may also differ for different stakeholder groups stakeholder planning steps must consider several questions such as which stakeholders to engage separately or together and when disciplines domain knowledge relevant interests what role does the main client or funding agency play relative to other stakeholders are there cultural or other sensitivities to consider how much authority should the stakeholders have in making decisions about the model at what points in the modeling process is their input most important will stakeholders be involved as individuals or part of a group how will information flow from to stakeholders and the model how does the project provide value to the stakeholders what is required to enable the stakeholders to engage effectively what commitment is required from the stakeholders and are they sufficiently interested and available to make that commitment what is required to build trust plans for stakeholder engagement should be developed monitored and adjusted in concert with the stakeholders who are to be engaged barreteau et al 2010 four forms of engaging iwrm stakeholders in modeling have been identified hare 2011 1 front and back end modeling where stakeholders are involved at the early and last stages of the project 2 co construction where stakeholders are involved throughout the model development process 3 front end modeling where participation is limited to the early stage and 4 back end modeling where stakeholders are brought in at the end of the modeling process in general it is best to engage representative stakeholders as early and as often as possible langsdale et al 2013 checking in with regular status discussions at all stages of the modeling effort extensive engagement is particularly important in projects oriented towards promoting social learning and collaboration and requiring high levels of trust however the early stages of a project typically involve more abstract discussions hence there is a tension between involving stakeholders early and consuming their time with little progress or later and running the risk of them feeling that key decisions have already been made developing and maintaining stakeholder interest is key to resolving this tension 4 3 project management planning like all substantial projects the modeling process requires resources such as time funding data see data collection step below and skilled personnel in some cases there can be externally imposed deadlines or funding constraints that must be met which in turn can drive problem definition choice of model used and the modeling process even without fixed constraints government or regulatory authorities are likely to contribute much of the funding and some technical skills and may have a different role in supporting planning from other stakeholders stakeholder engagement discussions should cover what support is desired and what is available and then be revised as more is learned as the iwrm process progresses key decision points in the modeling process should be coordinated with stakeholders to ensure agreement on what is broadly being done when and with whom this typically is a negotiated process between some stakeholders and the modelers and can be aided by developing a workplan of activities that is revisited throughout the project to allow response to changing priorities resource constraints or leveraging opportunities in the context of iwrm more resources are typically required for larger projects in order to facilitate the communication and collaboration that typically include a greater variety of stakeholders with different perspectives disciplinary backgrounds and experiences van asselt and rijkens klomp 2002 4 4 preliminary conceptual model the preliminary conceptual model forms the basis for transitioning between the planning and development phases it is a broad qualitative outline of the model design and identifies the main features that must be included in the model s that is are to be constructed conceptual models typically include desired outputs of the model which are those that represent the effects of controllable and uncontrollable drivers on the societally relevant resources and indicators key entities or system components that drive the desired outputs of the model such as land and water use behaviour policy that influences how water is extracted ecosystem impacts river flow aquifer responses of interest and water quantity and quality broad relationships and connecting processes between these entities and uncertain elements that are to be explored during the decision making process such as policy options and those exogenous factors outside of policy control e g climate other sectoral influences such as from energy or agricultural policy the objective of the preliminary conceptualization is to describe aspects of the iwrm issues and the influences that contribute to these issues these elements are then refined to those that are relevant to the output for iwrm outcomes and scenarios and to the comparison of those scenarios for any negotiation or recommendation a conceptual model defines connections between important components of the iwrm issue and includes model inputs and model outputs and intermediate states and processes that are relevant some entities and processes do not influence the relevant output but may be important to stakeholders to improve relevance and make the model more accessible to end users elsawah et al 2017a b therefore the development of the preliminary conceptual model can also be a useful activity for building a shared understanding among the project stakeholders about the system drivers and how they will be addressed in the model bertone et al 2016 throughout the conceptualization process substantial discussions are indispensable regarding how the model will be used what limitations might need to be overcome what alternatives should be considered and how multiple models will work together the conceptual model design step focuses on higher level views of the iwrm problem for example determining boundaries of the problem e g what is in what is out temporal and spatial scales and extent and general level of complexity required for the intended decision level e g whether short term operational medium term management or long term planning determining the appropriate scales can be challenging because the components of socio environmental models in the iwrm domain can have different spatial boundaries and characteristic process time scales and the stakeholders may be interested in issues that occur at different scales for example farmers are likely to be interested in impacts at the spatial scale of an individual farm and the temporal scales of both a single season and a span of several years while water resource planners are more likely to be interested in sustainable watershed spatial scales over a decadal planning horizon there may also be a mismatch between stakeholders scales of interest and the scales of relevant biophysical and socioeconomic processes hamilton et al 2015 conceptual models are typically constructed with some sort of diagram using arrows to mark influential relationships and flows argent et al 2016 robinson 2008 jakeman et al 2006 narratives or other visual and textual descriptions of what happens if may also be useful e g rich pictures data collection and knowledge elicitation methods are commonly used to develop the preliminary conceptual model such as desktop literature review expert and stakeholder interviews and workshops vennix et al 1990 the components and processes should be broadly described in terms suitable for stakeholders who are not modeling experts qualitative descriptions regarding how a change of one component or process can change the state of another can be valuable and can provide insight regarding the direction of change whether it is a relatively small or large effect and what other factors may influence that relationship liu et al 2008 quantitative measures and the detailed rules and equations specifying relationships are established in the development phase conceptual models are also useful when adapting and or integrating pre existing models which may well be required by the water management sector because of their legacy of and commitment to their previous investments in hydrological model types in particular here revisiting the conceptual model relates the existing model s to what may be a new problem as well as identifying and resolving any conceptual differences between the adopted model and the problem as perceived by the modeler and stakeholders belete et al 2017 this is an essential step before further resource investments are made 4 4 1 modeling technique as the conceptual model is clarified the type of modeling techniques to be used must also be decided there are usually many different possible techniques that could be selected each with different strengths kelly et al 2013 fulton et al 2015 hamilton et al 2015 selection is influenced by factors including the level of knowledge about the system of interest the level of spatiotemporal detail to be represented the type of processes to be represented and the nature of the data to drive and calibrate the model see badham 2015 kelly et al 2013 anderson et al 2015 for guidance broadly however conceptual or toy models or serious games are typically more suitable for social learning and communication oriented purposes argent et al 2016 van der wal et al 2016 decision support on the other hand typically requires mathematical or computer models which require more quantitative data and different skills to develop method selection may also involve trading off among the best method modeling effort and participation in the model development and possibly also model application sophisticated physically based mathematical models for example can be difficult for non specialists to understand the model s structure and relationships between model input and output and thus explore adapting an existing computer model for a new iwrm objective can reduce the need for skills and time and the model may be already familiar to some participants but this may be at the expense of overall engagement the experience of working with some form of a model can assist non specialists to contribute to iwrm more effectively than discussing the specification of the expert knowledge to be embedded in the model for example monks et al 2016 describe a modeling effort where those involved in conceptualizing and developing the model had less time for exploring with the model but explored more adventurously than those who were provided with pre prepared models and given more time for exploration 5 phase 2 development the development phase converts the preliminary conceptual model into a complete model ready for its intended purpose within the project this phase comprises five steps data collection model construction model calibration uncertainty analysis and testing in this phase the steps are particularly interwoven and some elements must be undertaken in parallel or iteratively for example testing which is a challenge in multi component iwrm models especially those with system feedbacks is an ongoing process during model construction as it may be necessary to test and re test each model element its linkages and the full model itself to ensure that the new or modified element has not interacted incorrectly with existing elements bertolino 2003 vale et al 2016 even steps that appear sequential may be revisited for example an unsuccessful attempt to calibrate the model might lead to amendments in the model implementation revisiting construction which could further require additional data to be collected nevertheless it is useful to identify separate steps to ensure all key activities are given due consideration in the modeling process for mathematical or computer models much of this phase is technical with the modelers working from literature expert opinion data as well as with stakeholders to formalize the important processes and system properties previously outlined in the conceptual model although not discussed in depth here similar steps are performed in the modeling process for qualitative models for exploratory and communication purposes malekpour et al 2016 these methods typically use knowledge elicitation processes and diagrams to describe relations between stresses and system responses and validate the final model with stakeholders to ensure it reflects the shared understanding of the system 5 1 data collection data are required to implement the conceptual model and to test that implementation in iwrm models these data may be a mixture of numerical values as is commonly the case for hydrological components and categorical information where measurements and knowledge are less precise or more qualitative as could occur with some social or ecological components as well as measurements relevant data encompass information from established theory existing models published studies assumptions stakeholders and expert opinion data collection involves compiling cataloguing obtaining and evaluating all of these forms of knowledge this step also identifies the data and knowledge that are not available and considers potential proxies for those gaps some of the specific questions to be considered in this step include what data are needed to represent theories included in the model for each relationship and overall how accurate do the data need to be what quantitative data are available what is the quality of the quantitative data what are the temporal and spatial scopes and resolutions of available quantitative data what qualitative data are available particularly with respect to the social and ecological elements of the system how can gaps between available and needed data be addressed are there any privacy intellectual property ip or other constraints on the use of relevant data do any datasets contain sensitive information once compiled some of the data may need to be cleaned transformed into another format or rescaled to be compatible with the other data or the model requirements note that more data may become available throughout the modeling process as stakeholders gain understanding and interest 5 2 model construction the model construction step converts the conceptual model design into a functioning quantitative model or models this involves rigorously developing the details of all the processes to be represented in the model s each process must be formalized as a set of equations or rules that specifically describe what happens in the model s in situations likely to be encountered and then implemented within the model s for example deterministic differential equations could be used to simulate the way in which water availability changes over time due to extraction evapotranspiration or other processes alternatively relations could be represented probabilistically for example if the population increases by more than 20 then water requirements will increase by 20 with 50 probability or 10 with 50 probability development of the detailed equations or rules may need to consider alternate models such as those including alternative ideas about system geometry or processes which arise from discussion between stakeholders and the modelers considering alternative model concepts is often an extremely useful communication approach especially if they are perceived to be rival or advocacy models ferr√© 2017 these contentious alternative models can help identify what future data collection might discriminate between the rival ideas and allow all stakeholders to have their ideas considered and vetted within the framework provided by the modeling process elements within the overall model s may require translation for example they may be at different temporal or spatial scales which then require upscaling and downscaling to communicate hamilton et al 2015 moreover there are aspects of the system that may be conceptualized with different paradigms or units so inputs and outputs need to be transformed from one type to another for example water quantity may be required for some equations and flows for others one approach is to construct the quantitative model with several sub models that address components of the conceptual model and overall system which facilitates the use of different types of equations or rules where different considerations are important the sub models typically pass relevant information to each other and detailed description of the communication between sub models is developed prior to model construction different approaches to variables such as scales must be reconciled for interoperability of the sub models brandmeyer and karimi 2000 and evaluated for conflicting assumptions between sub models mackay and robinson 2000 such considerations are especially important for iwrm models which include quantification of hydrologic environmental decision making and other social processes technical implementation of the model may be achieved by adopting pre existing modeling platforms libraries or else developing the model from scratch these options indicate from least to most the required amount of software development knowledge that has to be on hand platforms refer to software packages that allow a user to construct a model without any coding knowledge often through the use of a graphical user interface libraries provide a pre developed model or set of models along with utilities and toolsets interaction with which often requires code to be written research is often conducted in specialized niche contexts not covered by generalized platforms and so it is not uncommon for additional code development to serve the modeling s specific purpose ahalt et al 2014 development of an integrated model requires collaboration between domain specialists e g surface and groundwater hydrologists various types of social scientists including economists social and policy domains ecologists etc and leadership from integrated modeling experts regardless of the development approach adopted implicit assumptions or misunderstandings through the model development process will likely induce unrecognised limitations in the model functionality tests to identify these mismatches and other general errors should be designed and developed preferably before any model construction begins but may also be created post hoc doing so communicates to others what the intended model limitations are and provides confidence that the model functions as intended these tests may be modified throughout the model construction process this subject is expanded further in section 5 5 as well as the technical process of building the model the model construction step includes consideration of broader model choices and constraints on the implementation such issues include the limitations or assumptions associated with the modeling techniques being used and how these are to be managed this includes the levels of abstractions and simplifications whether there are licensing or other constraints on the tools used to develop or apply the model accessibility and usability of the eventual model including issues of interface design for computer based models potential reusability of parts of the model and the intellectual property rights that should be asserted for the developed model at the conclusion of this step the quantitative model s is are run and transforms model inputs into outputs of interest however how well these outputs represent the socio environmental system is not yet evaluated which is the focus of the remaining steps in this phase 5 3 model calibration calibration adjusts model parameters and other model factors such as boundary conditions to best reproduce observational data often done formally using a metric and possibly constraints in an objective function though visual and other qualitative approaches may have value bennett et al 2013 qualitative approaches to parameter settings can be especially useful in non hydrological components e g ecological social processes of iwrm models where data on observations are more scant and expert knowledge is required some model components in iwrm particularly of a hydrological nature are capable of accurately representing major relations and predictions that reflect broad properties of the system typically other predictions will have less certainty such as those requiring knowledge of small scale properties of the system e g preferential flowpaths needed to accurately simulate groundwater travel times or those related to the social components of the system after initial model construction model parameters and other model factors are varied to better fit what was measured or expected from a given system for a set of conditions a process called history matching the model output is compared to observed or expected equivalents in the data to assess how well the model is able to represent the system of interest when the observed data themselves are inadequate to constrain all model parameters theory and expert soft knowledge can provide further constraints if the modeling contradicts well grounded theory this may point to an error in the data or the model structure model parameter reasonableness is usually not judged using exact values as they are not typically available distributions or plausible ranges are used instead observational data are subject to a range of uncertainties and biases including measurement or sampling errors and spatial and temporal heterogeneity history matching does not require or even expect an exact match between observed data and model output indeed too close a match can suggest overfitting where the model is fitting errors within the data which commonly degrade the model s ability to make predictions in other conditions beyond those operating in the calibration period some level of tolerance or an acceptability threshold is therefore required when a model both fits the observations or conforms to expectations and has reasonable values for model parameters and other factors it is deemed calibrated anderson et al 2015 ch 9 it is important to apply history matching with multiple criteria these criteria should include many different observation types in order to break correlations between model variables different observation types in the hydrological component of iwrm models could for example include measurements of water balance terms evapotranspiration groundwater levels streamflow rates etc in addition decisions related to how to weigh the importance of the different observation expectation types and constraints on them directly influences the trade offs in fit anderson et al 2015 diverse comparisons can also reveal different areas where the model is strong or weak haasnoot et al 2014 parker 2009 and many weak signals that each assess different aspects of reality can be more informative than strong performance against a single aspect of behaviour grimm 2005 however even with several criteria judging calibration may not be straightforward communication with stakeholders during calibration commonly includes discussing data or processes important to the model that may not have been sufficiently considered it may also include showing how different calibration approaches yield different results and if these differences may have different practical implications moreover the calibration step should also clarify that many alternative models may fit or explain the same observational dataset s typically a model is only useful for decision support if it is perceived to be credible by stakeholders often model credibility is primarily influenced by its ability to simulate what has been observed in the socio environmental system this can be challenging as stakeholders often obtain an understanding of inherent uncertainty in the modeling process from their everyday experience of the weather report however rather than immediate evaluation being available as it is for a weather forecast the timeframe of iwrm processes represented may be combinations of months years decades and even longer thus continual evaluation of model outputs such as is done with weather forecasting is generally difficult to conduct rather the iwrm model calibration functions best when it provides tests that are meaningful and intuitive to stakeholders which in turn facilitates building the stakeholder trust required for adopting the modeling results 5 4 uncertainty analysis once a model is calibrated to what is known about the system it can become a generator for predictions in other situations where data are typically not available model predictions portray the estimated effect of changes to model inputs that represent current stresses such as land use management or stream or aquifer pumping rates and or future conditions of which there could be many in iwrm cross sectoral issues such as changes in climate energy use and demographic patterns or policy changes like managed aquifer recharge opportunities and water trading rules however a model s ability to create representative predictions is expected to be worse than its ability to simulate calibration conditions there is uncertainty in the model that was constructed not only in the ability of field social survey and other observations to constrain the model inputs and the necessarily simplified structure of the model that represents the iwrm system being modeled but also in how specified future conditions represent what actually occurs an uncertainty analysis describes the uncertainty in the model output under stipulated conditions input data used to calibrate the model model structure and other assumptions parameterization output data used for calibration and future scenarios used for simulation of the model if model outputs used for calibration are very sensitive to one or more model factors that is model outputs vary substantially the plausible values of the model factors will be constrained in calibration and their uncertainty is reduced when those model factors are also important to model predictions the prediction is expected to have less uncertainty when calibration data do not appreciably constrain the model factors important for a prediction that prediction is constrained primarily by soft knowledge and may have a high degree of uncertainty there are many ways to assess and describe uncertainty in the predictions guzman et al 2015 the best representation will depend on the context in which the iwrm problem operates and the requirements and understanding of the stakeholders hunt 2017 in iwrm where cost benefit trade offs are often assessed a more quantitative approach to characterizing uncertainty can be valuable guillaume et al 2016 most straightforward is a simple reporting of model predictions under a few specific conditions e g streamflow depletion land use practices water allocations or environmental flow releases under drought and wet conditions however most approaches involve a large number of simulations under different conditions so that the uncertainty can be described over the full range of modeled behaviour basic investigation of uncertainty might include a sensitivity analysis e g hill and tiedeman 2006 norton 2015 which adjusts different model factors by some known amounts and examines the effects on the key model outputs higher level uncertainty investigation might include monte carlo methods that report uncertainty in terms of a probability distribution of predictions simulations which are generated by running the model many times using samples pulled from a probabilistic representation of the model factors referred to as realizations monte carlo is computationally expensive because there are many combinations possible hence many model runs are required to characterize the behaviour of the model across the scenario and parameter space anderson et al 2015 pp 471 476 this is especially the case for iwrm modeling where potentially a large number of input and output variables are of interest as the investigations expand to multiple systems that incorporate hydrological ecological and socioeconomic systems it may therefore be more appropriate to sample the model inputs more efficiently latin hypercube quasi monte carlo and constrained walking methods campolongo et al 2007 are examples of pseudo monte carlo sampling methods emulation methods are increasingly being used to build faster running surrogate models of those models whose runtimes preclude monte carlo type sampling asher et al 2015 razavi et al 2012 machac et al 2018 in this way sensitivity and uncertainty analysis can be undertaken using the response surface of the surrogate model e g yang et al 2018 there are two additional uncertainty approaches that are well suited for iwrm problems one is scenario modeling especially of plausible but extreme case scenarios anderson et al 2015 pp 458 460 kwakkel 2017 guivarch et al 2017 mills et al 2018 anderson et al 2018 in this case a set of model inputs is developed that represent factors that when combined simulate the conceivable extreme but also intermediate outcomes that might be reasonably expected in this case the analysis represents outer edges and intermediate points of the envelope of possible model forecasts which can inform the iwrm decision making beyond simple reporting of expected outcomes this approach is well suited to simulating future conditions such as climate scenarios and cross sectoral influences and can be undertaken in a flexibly prescribed robust decision making framework a second more qualitative approach is quality assurance of the modeling process itself refsgaard et al 2007 where checks are undertaken to ensure that a model has been properly applied and decisions and choices are monitored and recorded to enable transparency and reproducibility of the process 5 5 testing of model robustness before it can be used in the application phase the model should be tested in several ways as the model is constructed the individual elements must also be tested to ensure the construction correctly implements the rules as intended standard model testing includes debugging and unit consistency checking balci 1994 jakeman et al 2006 as new components are being added to the model additional tests are run to ensure the new component has not generated errors in the already tested parts of the model tests of sufficient scale and scope increase confidence that model behaviour is as intended in practice this approach is similar to bayesian inference rarely can it be determined that model behaviour is correct across all possible scenarios but a large number of successful tests increases confidence davidson pilon 2015 the ability to invoke these tests in an automated manner can help ensure a sustainable pace of model development this is especially true for larger scale projects as automated running of tests which may be run after a change in a model for example aids in reducing the time and resource costs that may be incurred as model complexity increases having such tests and the requisite infrastructure in place assists in the model construction process maintenance see section 7 3 and may serve as further technical documentation of the underlying processes within the model see section 5 1 however effective testing is much broader than model verification including evaluation against a model s intended purpose and assessing the accuracy of model outputs bennett et al 2013 conditions simulated during calibration may not encompass the range of conditions that the model will use to generate predictions in the application phase tests of the model s robustness can build credibility in these forecasts including checks such as can the model successfully simulate extreme conditions e g drought or flooding do test cases generate output that is consistent with manual calculations or other existing models does altering a variable to an extreme value change model outputs in an expected way such checks may also reveal issues with data sets in use such as implausible positive negative values e g data sets were generated with interpolation mismatch in expected spatial and temporal resolution and range across model elements mismatch in assumed value types exchanged between models elements e g a categorical classification given as a textual representation one as opposed to its numerical equivalent a common form of qualitative assessment involves domain experts who review the model logic and simulated output details to assess the degree to which a model appears effective in terms of its stated aims i e face validity van der sluijs et al 2005 there may also be the opportunity to quantitatively assess predictions directly using a split sample or cross validation approach where some data are not used during calibration but instead provide test cases if insufficient data are available for this then a post audit might be performed after sufficient time has passed that a prediction can be evaluated or in an ongoing process as new observations are acquired such as those derived from adaptive management a final check involves revisiting appropriateness or a model s fitness for use this could include assessing whether the user interface is sufficiently easy to use by the intended users easy to understand from the user s perspective not the modeler s and sufficiently well explained in lay terms for models developed for participatory or educational purposes the criteria pertaining to its usefulness may be more important than achieving a high performance or accuracy elsawah et al 2017a b using the case study of the chesapeake bay coastal system allison et al 2018 argue further that for addressing wicked problems in socio ecological systems it is more important that one seeks holistic modeling of results yielding robust directions of system change than high numerical precision of outputs achieved through increasing model complexity jakeman and letcher 2003 argue similarly that in iwrm modeling the broad objective should not be about treating system outputs as accurate predictions it should be aimed at allowing differentiation between system outcomes under different conditions e g policy drivers at least with a qualitative confidence 6 phase 3 application application of the model ensues according to the modeling purpose determined during the planning phase for simulation games mathematical models and computer simulations using the model includes both running the model with various settings or options and analyzing the generated output for diagrammatic and other models used for thinking and learning application of the model is more qualitative and generally consists of one or more discussions and a report concerning the contents yet such discussions are also critical for mathematical models and computer simulations as this phase can enhance understanding of impacts of decisions made regarding model structure and input data from earlier steps this phase comprises three steps experimentation analysis and visualization and communication these steps are often iterative with discussion about the results from initial model runs stimulating additional questions that lead to further experimentation analysis and discussion the modeling project is most effective when the application phase provides stakeholders with an understanding of how the model works builds confidence in the model and supports substantive discussion about iwrm trade offs and issues 6 1 experimentation there are two key considerations for practical experimentation in order to obtain robust results which parameter and other factor combinations are to be used and how many runs are required each modeling effort will have different responses though some broad generalizations apply across most iwrm problems a mathematical or computational model will typically have two broad types of manipulation the first selects between specific scenarios of interest where scenarios reflect simulated system stressors or conditions different from calibration stresses and conditions scenario definition can encompass a range of possible changes such as different policy options e g water trading rules regulation of water quality varying operating rules or conditions for a dam and conditions e g high or low rainfall high or low farm input prices and product returns land use set at historical patterns or allowed to vary over time together with the parameter values or distributions that define the scenario such as the amount of rain each day during a high rainfall year the second manipulation goes beyond specific scenarios of interest by using computational methods to explore the plausible ranges of model parameters and other factors more broadly efficient methods of parameter space sampling may be used to limit the number of simulations required which is particularly useful when model runtimes are prohibitively long although the process is similar to the uncertainty analyses conducted during the development phase the purpose and hence selection of input values and the analysis are different here the model is manipulated to explore what ifs posed by stakeholders and to understand the uncertainty associated with specific scenarios rather than to characterize the robustness of the model 6 2 analysis and visualization the objective of the analysis and visualization step is to distil the insights gained from model development and experimentation in order to identify those meaningful to the stakeholders including decision makers however multiple scenarios and experimentation runs combined with a likely diversity of interests and expertise of the stakeholders can make the reporting lengthy and analysis complex simple analytical approaches such as regression methods are of limited use as there is typically no single equation for the overall system instead multidimensional visualization methods such as heat maps box plots and arrays of charts allow communication of important relations insights and system responses communication may be further aided by contextualizing the presented analysis analyses may be contextualized by overlaying data on a map providing spatial context comparisons against historical events temporal context and simulated results scenario context some key questions to consider during the analysis and visualization step how do outputs important for decision making respond to changes in inputs parameters and other factors including interventions are there thresholds or tipping points in the parameter space what are the likely outcomes to the questions addressed by the modeling are the results sufficiently accurate are some stakeholders more adversely affected by model shortcomings than others in addition to the most likely outcome predicted by the modeling what is the distribution of outcomes for individual stakeholders in addition if comparing iwrm options can any options be excluded because a better option exists that applies to all situations tested referred to as dominance are best options in some situations undesirable in others are there robust options that are good in all feasible or even probable situations even if there are no situations in which they are the best lempert and collins 2007 once all possible model results are culled to those that have utility and relevance for the iwrm decision making process they are finalized for distribution beyond the modeling team 6 3 communication the final aspect of model application is to communicate the insights from the modeling process in a way that most effectively contributes to model credibility and to policy or other decision making one of the key challenges in an iwrm process is how to communicate with stakeholders about model results in a way that enables the stakeholders to understand the system being modeled at a minimum the model s scope assumptions and capacity should be clearly specified in terms of what the model can and cannot do to help the audience understand the model limitations to achieve effective communication the modeler should consider the needs and background of the stakeholders and tailor the results accordingly even a simple model may be difficult to explain particularly when there is a large disparity with respect to technical knowledge between the modelers and the stakeholders one must balance an inclination to relate too much detail against appearances of hiding important model information skilful meeting facilitation will often be required to ensure equity among stakeholders which is more challenging where there are diverse or contentious stakeholder interests and with increased importance of decisions an audience analysis may be useful to help plan the communication this analysis includes questions about the motivations for the stakeholders participation their understanding of the system and their prior experience with models hall et al 2014 it may be desirable to segment the stakeholders based on the audience analysis for example by offering some groups tailored discussion beforehand and therefore providing a better opportunity for all to contribute meaningfully when they all meet together one approach to communication is to focus on the issue for which the modeling was developed presenting the key insights and connections as a narrative richels 1981 the objective is to be clear and concise and assist stakeholders in understanding the relevant issues with the modeling supporting the discussion once the essential story is clear more detailed discussion is required about the critical modeling assumptions limitations and their implications for results and recommendations another approach is to use iterative discovery where the modelers and stakeholders jointly investigate a series of scenarios particularly for complex problems that involve deep uncertainty fu et al 2015 rather than simply providing the results this method involves running the model alternating with discussion to raise new questions and propose further scenarios to be run the iteration fosters a knowledge partnership between modelers and stakeholders including decision makers enhancing the modelers understanding of stakeholder needs and their understanding of modelers knowledge thus leading to more useful presentation of results this contributes to stakeholder confidence in the modeling and modeler and therefore greater acceptance of the model results moreover both the stakeholders and the modelers attain a better understanding of the modeling assumptions and how they could potentially impact any decisions to address confusion over terms or concepts in both the model and analytical techniques used the verbal or written presentation of models should be carefully crafted so that key but unfamiliar terms are illustrated with examples and thought experiments and feedback should be sought from the audience to verify that they understand the presentation should utilise images visual aids and strategies kelleher and wagener 2011 as well as actual examples say from similar systems particularly when complex ideas are being discussed in addition to the potentially technical nature of the discussion stakeholders may have a different perspective of the system that can lead to difficulty in forming a mental picture of how the model represents the system and dissonance with their pre existing understanding of that system if the modeling provides a different system view than that held by the audience it is important to not dismiss their perspective but to use their own relevant observations to begin to see the alternative perspectives hall et al 2014 such perspectives can form the basis for additional scenarios 7 phase 4 perpetuation the perpetuation phase plans for the future this phase comprises three steps documentation process evaluation and monitoring and maintenance where relevant this phase integrates the model into ongoing policy processes and ensures it is maintained and updated when required and or when new observations become available so that the model can contribute to future decisions models for decision support often require a high degree of technical development and testing to provide appropriate estimates as meaningful input to decisions especially those involving high risks and intensive investments this development can incur large costs reuse of the model for future or similar applications can help to justify that cost for example an australian multi year basin scale modeling investment assessed future water availability in the murray darling basin and the impacts of development water extraction and climate csiro 2008 subsequent model outputs continued to provide scientific evidence to assist in later development of the murray darling basin plan in contrast some models are used only once for example models developed as thinking tools for a given stakeholder group and timeframe are required only during the participatory process to generate the intended learning pidd 2004 even in such cases however the steps in the perpetuation phase should be explicitly considered for example while a full process evaluation is unlikely to be appropriate reflection on the modeling process and discussion with the stakeholders about what they found useful can improve future similar modeling exercises 7 1 documentation the type of model documentation and the detail included depend primarily on who will use the model s in the immediate present and future there are three common types of model documentation user documentation technical documentation and the project report user documentation should assist users in running and interpreting the model outputs it should be focused on providing clear instructions for the main model objectives this documentation may include user interface description including screenshots interpretation of each input and output along with sources of data tutorials with incremental examples for training and credibility examples that reflect real scenarios guidance for running the model and description of potential common pitfalls technical documentation has two parts the first is a model archive that includes machine readable model input output executable and source code if available the second provides detailed descriptions of the model s and justification for the choices made in the model design it will typically include choice of modeling type and method including how limitations are managed assumptions and their justification or reasons rules and equations scales of representation variable definitions and codes data dictionary calibration method and results preferably with relevant uncertainty assessments data sources including references to theory such documentation serves two purposes scientific credibility and support for future development model results must be able to be reproduced and replicated if they are to be considered part of the scientific method morin et al 2012 peng 2011 technical detail about the modeling is therefore critical for credible model use in iwrm applications in addition to demonstrating scientific rigour such detail may be necessary to meet legal obligations associated with some decision support models if a model is intended to have an enduring presence then technical documentation that includes description of code or software is necessary to allow a future developer to modify or add to the existing code base its role is to describe each function method routine or class and why it was constructed in the way that it was modern code documentation is frequently done within the code of the model itself using a variety of tools available to auto generate or extract that documentation examples depicting how the code can be used may be included and doubly serve as executable test snippets the advantage of including documentation within the code itself is that it is more likely to be updated as the model is developed or modified well documented and tested code can be considered a necessary requirement to achieve full reproducibility and transparency and eases the maintenance burden often textual or narrative descriptions of the represented processes do not fully capture assumptions made during development both conceptual and technical which results in implementation differences subsequent implementations relying solely on descriptive documentation may have differing behaviour compromising model verifiability hutton et al 2016 if elements of the model s were developed in a sufficiently abstracted manner it is possible to reuse these beyond the original use case this is the underlying concept applied in component based modeling practices which require models to be coded to a standardized specification peckham et al 2013 adhering to such standards may also allow semantic information to be extracted enabling high level comparisons between different models within a model repository several guidelines exist that are intended to ensure adequate information is provided to enable replication and reproducibility these include reporting for deterministic models anderson et al 2015 reilly and harbaugh 2004 the odd protocol overview design concepts and details for agent based models grimm et al 2010 m√ºller et al 2013 and sd doc approaches for documenting system dynamics martinez moyano 2012 the project report is written for the client who commissioned the modeling it summarizes the analysis and provides a written form of communication focusing on the specific scenarios of interest key elements include scenarios and results selected graphical and other visualizations interactive where warranted to highlight messages and information about the limits of the results including assumptions uncertainty analysis and plausible parameter values that would generate different results regardless of model purpose model documentation should be continuously developed over the course of the project furthermore it is difficult to conceive of a situation in which a single person is able to contribute all the requisite domain knowledge model implementation details model interactions and data including sources types and pre and post processes applied documentation is therefore most likely to be developed collaboratively 7 2 process evaluation evaluation of the modeling process assesses whether the desired role of the modeling was effectively and efficiently achieved within the broader project a formal process evaluation informs future modeling projects and should consider issues such as did the model and modeling process meet the role originally envisaged and were changes to that role intended and appropriate how effective was the engagement of stakeholders who needs to be added to any future modeling process did the project meet time and budget objectives how much adjustment was required to the original plans what would have improved the modeling process and at which phase step depending on the modeling purpose several criteria have been developed for evaluating a modeling exercise for example haasnoot and middelkoop 2012 suggested three criteria predictive success has the future turned out as envisaged decision success robustness have good decisions subsequently been made and learning success have scenarios enabled participation and learning a different set of criteria was suggested by alcamo and henrichs 2008 relevance credibility legitimacy and creativity in a post audit merritt et al 2017 identified 33 factors for realizing success in an analysis of 15 water resource projects of the four factors considered to be the most necessary all of them largely non technical three related to aspects of stakeholder engagement in the modeling process the other to critical thinking around problem framing and the role of models 7 3 monitoring and maintenance the final step is to put into place the plans and mechanisms to ensure that if needed the model is updated as required and continues to be relevant for any policy process in which it is embedded this can be particularly problematic if the policy organization does not have technical expertise or funds to maintain the model s or if a model has been developed by an outside entity so that a transition process is required if the model is to be maintained beyond its final documented form the key elements of the maintenance plan are to establish a process for updating any input data documenting new data sources and deciding the timing and assigning responsibility for the updating a succession plan is also required to provide continuity and maintain the knowledge base about the purpose and results of the model this plan should include lists of materials including documentation and training materials and where they can be obtained many models are revised and adapted for different but related policy questions for example a similar decision but located in a different area of the problem domain maintenance questions to be considered include how easy is it to adapt the model conceptualization as well as implementation for example to another catchment or set of water resource issues what expertise data and other resources are required in order to make changes to the existing model what changes in model design might be needed in the future for example boundary conditions scales processes etc who can approve changes what are the implications for the policy process when important existing model outputs appreciably change as the model is changed which steps in the modeling process will most likely need to be reiterated or revisited and can any steps be excluded in specific situations 8 five key areas for future development in iwrm modeling although there is always room for improvement iwrm is now endowed with a rich set of techniques for modeling both social and biophysical aspects of a system as described in section 2 there are also many authors providing different prescriptions and recommendations for operationalizing iwrm we have argued here that the overarching challenge for the future is integration and implementation bammer 2013 of the knowledge gained from experience while the application of iwrm in regions with weak institutions is often seen to be challenging the need to better tailor analysis to policy making contexts is universal a key concern for instance in the field of policy analytics tsoukias et al 2013 daniell et al 2016 de marchi et al 2016 we focus now on five key areas for future development knowledge sharing overcoming data limitations informed stakeholder involvement social equity and uncertainty management 8 1 knowledge sharing the ability of the iwrm modeling community to share and exchange knowledge about the practices employed throughout the modeling process is essential for growing the field and expanding its applications knowledge about how to frame a particular iwrm problem and perform particular modeling activities i e knowledge in use is valuable for beginners learning the craft of modeling distilling information about learnt lessons and promoting ideas about new solutions knowledge sharing of practices is also valuable to inform comparison across multiple dimensions e g methodological design case studies issues of interest a prime challenge is to document systematically and with sufficient detail the approach taken in projects so that the knowledge of how to do iwrm modeling and lessons learnt accrue for future projects it should be possible to document a wide selection of iwrm projects to form the basis for on going knowledge sharing in the future systematic and consistent documentation practices and tools for this purpose still need to be developed to support transparency and learning documentation should not only be focused on describing the modeling process as what happened but cover the reasoning i e why did it happen behind the decisions and observed outcomes throughout the process and ideally it should entail critical reflection on what could have been done differently lahtinen et al 2017 one promising approach is the use of patterns to elicit capture and formalize this knowledge alexander et al 1997 a pattern is defined as a representation of a solution to a recurrent problem in a particular context a pattern could be with respect to any component of the iwrm process where modeling related decisions are taken patterns can be represented in different forms and presentation i e diagram template regardless of the presentation a pattern must have the following elements 1 a name to facilitate use and communication 2 problem description 3 solution and 4 context the premise of the pattern approach is that once the successful practice has been recognized as a pattern and expressed in some pattern form it may then be reapplied to similar problems contexts which will eventually lead to improved practices new observations and new or refined existing patterns discovering and capturing the relationships among patterns results in a pattern language which provides a shared lexicon for the community to communicate about the different contexts where patterns can be used 8 2 overcoming data limitations data can pose a challenge to iwrm especially in relation to the multiple and diverse types of data including quantitative and qualitative representing the various system components and their different scales and quality the type of data for a given project can determine whether or not a certain type of model can be used or how well it can be applied to address this there has been increased interest in more flexible semi quantitative modeling approaches such as bayesian networks e g ticehurst et al 2007 as well as different couplings of multiple methods e g nikolic and simonovic 2015 limited data can place a major constraint on iwrm modeling basic hydrologic and meteorological data can be patchy and unreliable in countries in developing regions and those where governments place a low priority on investment in monitoring networks new forms of observations such as remotely sensed data provide promising opportunities for iwrm particularly in data poor environments however these require more efficient ways of extracting useful information from the raw data montanari et al 2013 poor availability of socioeconomic data with the general exception of census data is widespread and presents a major barrier to understanding feedbacks between society and the environment undermining many water management endeavours mcdonnell 2008 social media present an opportunity for bridging at least some of these gaps in addition stakeholder involvement in the modeling process will continue to play an important role in capturing social variables and processes 8 3 informed stakeholder involvement a common criticism is that stakeholder involvement is inadequate in decision support system dss exercises generally not just in iwrm for example zasada et al 2017 in a survey of eu funded research programs on dss for agricultural and environmental issues this is despite recognition e g merritt et al 2017 for water resource projects and mcintosh et al 2011 that attention to associated non technical issues i e social and behavioral is vital to success in particular when the purpose is to support decision making and social learning for example as noted earlier merritt et al 2017 identified aiming for open and transparent communication good relationships and trust and sufficient interaction between the development team and users as three of four main factors crucial to success thus time spent in the design phase where stakeholder engagement usually begins is time well spent to avoid rushing into poor decisions that can lock the project onto the wrong path nicolson et al 2002 projects therefore require the will and resources for attending to these aspects this may also require more flexible funding arrangements that recognize the value in fostering an evolving understanding of the problem and imposing soft deliverables e g problem formulation documents rather than rushing into quantitative deliverables a more technical issue in this respect is integration of stakeholder interests preferences and attitudes to risk into the modeling process for example in terms of indicators or objective functions variations in all these elements can lead to completely different results and negotiated solutions model output can be converted into indicators by filtering the uncertainty in many ways e g using different robustness criteria e g mcphail et al 2018 leading to different rival problem framings e g quinn et al 2017 this is not as simple as just obtaining and providing data from to the end user more thought and effort needs to be paid to methods like interactive data visualization that meet stakeholders information needs and empower them to explore the problem and devise possible solutions bridging scales is a particular challenge in stakeholder engagement within iwrm in principle stakeholders are well placed to comment on their immediate problems and changes they wish to see however they may not be in a good position to account for their own future needs let alone consider how best to achieve them given others preferences and interactions within the system modeling plays an important role in supporting stakeholders in making sense of the iwrm situation at longer timescales and larger spatial scales but connecting abstract strategic ideas to concrete operational concerns remains difficult rather than trying to determine an ideal scale to use in iwrm models it will be important in the future to reflect on the range of strategies available for bridging scales in a way that brings the necessary stakeholders together to achieve meaningful progress when implementing iwrm 8 4 social equity social equity is often sidelined in iwrm models especially compared with standard cost benefit considerations of different trade offs the distribution of benefits from water and in particular whether the needs and rights of different groups are met pe√±a 2011 warrants greater consideration the need to better consider the totality of benefits and costs associated with water management including indirect outcomes non use values option values etc and how these vary between each person and group is a broader challenge for iwrm in general social equity in water management is not just about equity of outcomes benefits and costs but also equity in the decision making process especially having a voice or opportunity to influence the process i e procedural justice syme et al 1999 given that the equity dimension is typically poorly captured in models its consideration in the modeling process then relies heavily upon interpretation of model results and implications stojanovic et al 2016 adequately addressing issues of equity in modeling necessitates careful consideration in the stakeholder planning step to ensure fair representation of stakeholder groups in the process and consideration of social processes and alternate knowledge sources or potential solutions particularly in the planning phase and application phase steps budds 2009 argued that failure to do so in the la ligua river basin in chile and reliance upon a purely physical assessment in response to a situation that was predominantly socio political led to a positioning of the model based assessment and its commissioning agency as the only legitimate knowledge source which closed down a range of possible solutions decisions made using the assessment then disadvantaged poor farmers who had not created the groundwater scarcity problem budds 2009 in the context of iwrm the modeling process should facilitate procedural justice and at the very least not be a further barrier to equity 8 5 uncertainty management uncertainty pervades the treatment of iwrm problems users and managers of a water resource deal with uncertainty every day in a variety of ways the use of modeling itself is a powerful way of organizing information to understand and reduce uncertainties measures for managing uncertainty in management and modeling are still mostly considered separately rather than being integrated though some research areas are making progress on this front e g in the deep uncertainty literature kwakkel 2017 maier et al 2016 much of the attention to uncertainty in the water resources literature focuses on sensitivity and uncertainty analysis of the hydrological models unfortunately these are among the most certain of integrated model components and the human and ecological processes warrant much more attention hunt and welter 2010 here we see an opportunity to prioritize uncertainty sources and deal with those most crucial among other things this will involve attention to uncertainty assessment of model components but also the propagation between the component linkages where there are often feedbacks qualitative uncertainty assessment e g van der sluijs et al 2005 refsgaard et al 2007 will be a necessary perhaps even the main ingredient a key issue is understanding how uncertainty accumulates and diminishes within an iwrm process including the model itself and therefore where efforts can be targeted to constrain it combining uncertainty in future conditions system understanding and stakeholder preferences quickly leads to an explosion in possible system outcomes which can be overwhelming for stakeholders to consider in addition to obtaining additional information or facilitating consensus processes uncertainty can be made more manageable for example by using adaptive approaches and looking for robustness rather than eliminating uncertainty especially where uncertainty leads to disagreement these techniques can be important to achieve progress in implementing iwrm without needing perfect understanding of a system work on these techniques has not however been completely synthesized and needs further efforts to support their implementation 9 conclusion despite promising benefits and high expectations the potential of iwrm in translating integration aspirations to successful on ground results has not yet been fully realized it is well recognized in the literature that modeling has a crucial role to play in operationalizing and successfully implementing iwrm this paper attempts to contribute to improving the implementation of an iwrm modeling project by providing practical and fit for context guidance of the practices employed through the various modeling phases and steps these practices explain the details of employing iwrm modeling our general approach identifies four broad phases that occur during the modeling process planning development application and perpetuation these phases are common to diverse iwrm projects each phase comprises several steps or tasks while one or more steps may not be required for some projects it is valuable to consider the relevance of each step so that omission is an explicit decision and not simply an oversight in practice there will be many ways to implement each step appropriate methods and level of stakeholder engagement depend on the specific characteristics of the iwrm problem aspects of the system being modeled as well as other contextual factors e g funding institutional setting data available or level of stakeholder conflict we have therefore focused on the objectives of the step and guided how the step may be undertaken finally we identified some important gaps in iwrm modeling practice related research highlighting the need for advances in knowledge sharing overcoming data limitations informed stakeholder involvement social equity and uncertainty management as well as some potential methods for answering those questions in short effective iwrm modeling should involve a process and set of practices that are fit for the given context and well grounded in the iwrm problem at hand the implementation of effective modeling practice can play an important role in facilitating the what and how of iwrm by contributing to identifying and understanding of the elements that underpin the problem and guiding the questions to ask and the issues to address acknowledgements this work was supported by the sesync national socio environmental synthesis center core modeling practices in iwrm project under funding received by the national science foundation dbi 1052875 badham s contribution was partially supported under national institute for health research grant cdf 2014 07 020 
26226,many recent modeling efforts have employed component based modeling frameworks to take advantage of the flexibility they provide in representing systems more holistically despite the benefits that are driving this adoption conducting model parameter estimation uncertainty analysis sensitivity assessment and other simulations of this nature within component based modeling frameworks has remained unexplored using a multi objective calibration of a component based river temperature model we illustrate how the component cloning and parallel model execution interfaces we have implemented in the hydrocouple framework support such simulations the river temperature model calibration application we present involves a heavily human mediated 13 6 km section of the logan river in utah usa with limited information regarding variable inflows due to the flexibility in the modeling and calibration framework results from the calibration effort were successful with root mean square errors of 0 4 0 7 c and provided insights on mechanisms controlling river temperatures keywords component based modeling model calibration high performance computing river temperature modeling openmi hydrocouple software availability name of software the software described in this paper includes seven hydrocouple model components these components include 1 a hydraulic model component developed from the united states environmental protection agency s stormwater management model swmm code 2 a channel solute and heat transport component cshcomponent 3 a hyporheic transient storage zone model component htscomponent 4 a radiative heat exchange model component rhecomponent 5 a calibration component calibrationcomponent 6 a time series objective function calculation component tsobjectivefunctioncomponent 7 a time series provider component timeseriesprovidercomponent developer caleb a buahin email caleb buahin gmail com required hardware and software any computer that runs the windows linux or macintosh operating system components can be compiled using openmp http www openmp org compilation requires the qt 5 7 framework at minimum http www qt io cost the hydrocouple interface definitions software stack and model components in this manuscript are freely available under the gnu lesser general public license version 3 lgpl and can be downloaded from the hydrocouple github repository https github com hydrocouple 1 introduction the component based modeling approach involves the definition of standard software interfaces that when implemented allow disparate models to exchange information at runtime use of the component based software development paradigm promises faster development times greater reusability and more maintainable code bases l√∂wy 2003 szyperski 2002 beyond these software development related benefits the use of component based models in the earth systems and environmental modeling field provides the ability to select only those components that are relevant to representing a sub system of interest the ability to evaluate feedbacks between different domains and processes and a platform for evaluating the appropriateness of different model structures for representing the same process in a system argent et al 1999 peckham et al 2013 to date several component based modeling frameworks and interface standards have been proposed and implemented including the earth system modeling framework esmf hill et al 2004 model coupling toolkit mct warner et al 2008 the community surface dynamics modeling system csdms peckham et al 2013 and the open modeling interface openmi moore and tindall 2005 to name a few these frameworks and standards have been applied to a myriad of studies e g ascough et al 2009 shrestha et al 2013 long et al 2016 zhu et al 2016 shrestha et al 2018 in reviewing these applications however only a few have attempted the types of simulations characterized by running a model many times while intelligently varying input variables and parameters to evaluate their effects e g parameter estimation uncertainty analysis optimization sensitivity analysis and others recently christelis and hughes 2018 conducted a sensitivity analysis exercise that utilized a surrogate kringing model that was developed using training data from a computationally expensive component based model comprised of a channel routing component and a grid based groundwater component however they performed their sensitivity analysis outside of the component based modeling framework using matlab additionally they did not utilize the parallelism increasingly available on multicore clusters rizzoli et al 2008 noted that this class of simulations could be considered as model components in their own right to which the principles of hierarchical decomposition and object orientation that underlie the component based development paradigm can be equally applied still examples and tools that follow this recommendation have so far been missing there may be several explanations for this general lack of examples that follow the recommendation by rizzoli et al 2008 first there is a general difficulty in conducting this class of simulations for complex coupled model compositions involving multiple components oreskes 2003 voinov and shugart 2013 for example in calibration simulations the interdependency between the outputs and inputs of components involved in a coupled model as well as the errors introduced in the model integration process may mean that instead of calibrating individual components independently calibrating component based models most likely requires re calibration of the integrated model as a whole voinov and shugart 2013 the absence of examples of this class of simulations in component based modeling frameworks may also be attributed to the relative infancy of the component based modeling approach more crucially the designs of many of the existing component based modeling frameworks have not provided interfaces tools and mechanisms to facilitate these simulations in an efficient manner peckham et al 2016 noted this absence of uncertainty quantification or data based model verification calibration tools and applications in particular within component based modeling frameworks they proposed the need to integrate general purpose toolkits for optimization parameter estimation and uncertainty quantification as well as tools to numerically compute a range of penalty and objective functions and their derivatives in an efficient manner to address the need for calibration sensitivity analysis uncertainty assessment and other related tools in the context of component based modeling we describe a complex multi component river temperature model and demonstrate the integration and application of a generalizable method for calibrating a coupled model composition within a component based modeling framework for our application we used the hydrocouple component based modeling framework which is a recently developed model coupling framework based on the openmi 2 0 interface definitions that provides new interfaces to facilitate more efficient simulations buahin and horsburgh 2018 hydrocouple provides interface definitions that when implemented allow components to make independent copies of themselves for more efficient parallel and distributed execution while hydrocouple defines the necessary interfaces for enabling parallel execution of a coupled model composition the details of the process used to execute the class of simulations being considered here are missing we provide those details in this paper using the river temperature model as a demonstrative case study the specific model components used in our application were developed to enable the creation of adaptable river temperature model compositions such that modelers can identify and implement the key heat fluxes at relevant spatial and temporal scales in the different parts of a system by simply adding or removing one or more of the components such adaptability of temperature models is important given the variability in the dominant heat fluxes in various physical systems e g johnson 2004 neilson et al 2009 cardenas et al 2014 king et al 2016 etc and the computational requirements for simulating these fluxes using the components we developed we assembled a coupled model composition that was used to simulate the water temperature dynamics in a section of the logan river utah 2 methods 2 1 study area the logan river which was the focus of our study is located on the eastern end of cache valley in northern utah usa it drains a 109 5 km2 area situated on the western end of the bear river range as the logan river exits logan canyon it flows through the city of logan and then westward to cutler reservoir and eventually flowing into the great salt lake it is diverted at various points along its length into canals that are used for irrigation in the valley these diversions lead to dewatering of sections of the logan river during the irrigation season some of the water from these irrigation diversions returns to the river channel via shallow groundwater while some is returned to the river at different locations downstream additionally urban stormwater outfalls groundwater drains and various tributaries enter the river through the study area and further complicate the longitudinal and temporal temperature dynamics in the river the model domain used in this study includes a 13 6 km section of the logan river with two major diversions two tributaries and many urban drains inflows six long term monitoring locations have been established for discharge and temperature in the main stem of the logan river and some of the major inflows into the river fig 1 while some of the inflows into the logan river have been identified and monitored many of them have either not been located or have been located but not monitored our modeling objective was to investigate the relative importance of different heat fluxes during ecologically important times of the year when flows are reduced by agricultural diversions and water temperatures become elevated because of the poorly monitored influences from irrigation returns groundwater inflows and other urban inflows calibration of the coupled model composition was necessary to account for these unmeasured flows this case study provides a specific example of how model calibration and other types of simulations that involve running a model many times while varying input variables and parameters can be performed using coupled modeling frameworks through the use of component cloning and parallel execution 2 2 core river temperature model components to simulate the temperature in the logan river we used a coupled model composition consisting of four core components that together represent the major processes and heat fluxes controlling water temperatures in the logan river fig 2 the hydraulic routing component swmmcomponent was developed from the epa swmm model to route time variable flow through the river channel and provide output variables including velocity flow depth flow cross sectional area and channel top width that are needed for other heat transfer and transport components the channel solute and heat transport component cshcomponent was developed to simulate the advection and dispersion of heat in the channel over time based on formulations within neilson et al 2010a and 2010b it also computes the latent heat of evaporation and condensation j e as well as sensible heat from convection and conduction j c the radiative heat exchange component rhecomponent computes radiant heat fluxes received j sn and j an and emitted j br from the surface of the water in the channel a two layer sediment conduction and hyporheic zone exchange component htscomponent was developed to simulate the advective j hts and conductive j sed heat fluxes between the water in the channel and the hyporheic zone and the hyporheic zone and ground j gr based on formulations presented in neilson et al 2010a 2 2 1 hydraulic routing component swmmcomponent the swmmcomponent was developed from the united states environmental protection agency s epa s stormwater management model swmm which calculates runoff from sub catchments and routes the resulting runoff through a network of open channels and closed conduits with regular and irregular cross sections connected together by nodes representing inlets junctions outfalls and other nodes types rossman 2010 the routing component of the swmm model can solve the full dynamic wave equations or the reduced physics diffusive and kinematic wave approximations it is therefore able to handle flows in non dendritic networks pressurized flows flow reversals and backwater effects in addition to these features swmm is open source and has been well validated over the years by an engaging community of users 2 2 2 channel heat and solute transport component cshcomponent the 1d advection dispersion heat transport equation solved in the cshcomponent is 1 œÅ w c p t t œÅ w c p v t x œÅ w c p x d t x j y j e j c y s where t is the water temperature c t is the time s v is the velocity of the water in the channel m s x is the distance along the channel m d is longitudinal dispersion m 2 s œÅ w is the water density kg m 3 c p is the specific heat capacity of water j kg c j are external radiant heat fluxes j m 2 s or w m 2 incident on the water surface j e is the evaporation and condensation from the water surface j m 2 s or w m 2 j c is the convection and conduction from the water surface j m 2 s or w m 2 s is the heat supplied by other external sources j m 3 s and y is the depth of water in the channel m equation 1 is approximated numerically using the finite volume method appendix a provides the formulations used for computing j e and j c as well as details of the solution techniques used to solve equation 1 all external heat fluxes provided by other components e g sediment conduction hyporheic exchange back radiation etc are applied through the j and s terms in equation 1 2 2 3 hyporheic transient storage zone htscomponent the htscomponent was developed based on formulations presented within neilson et al 2010a and 2010b which extend transient storage model formulations originally developed for solute transport e g bencala and walters 1983 to heat heat exchange between the hyporheic transient storage zone and the river channel is simulated using a first order heat transfer relation for each computational element equation 2 2 œÅ s e d c p s e d dt h t s dt v h t s œÅ s e d c p s e d Œ± s e d b h t s Œ¥ x t t h t s y h t s s e d i m e n t c o n d u c t i o n t e r m œÅ s e d c p s e d Œ± s e d b h t s Œ¥ x t g r t h t s y g r g r o u n d c o n d u c t i o n t e r m œÅ w c p q h t s t t h t s a d v e c t i v e h y p o r h e i c e x c h a n g e t e r m s v h t s where œÅ s e d is the density of the sediment in the hyporheic transient storage hts zone kg m 3 c p s e d is the specific heat capacity of sediment in the hts zone j kg c t h t s is the temperature of the hts zone c v h t s is the volume of the hts zone m 3 Œ± s e d is the coefficient of thermal diffusivity for the sediment in the hts computational element b h t s is the width of the hts zone m Œ¥ x is the length of the hts computational element m t is the water temperature in the main channel overlying the hts zone c y h t s and y g r represent the depths of the hts and ground conducting zones respectively m t g r is the ground temperature c q h t s is the coefficient of advective transport m 3 s and s are heat fluxes supplied by other external sources j m 3 s e g groundwater the coefficient of advective transport q hts represents the volumetric water flux that is exchanged between the main channel and the hyporheic transient storage zone equation 2 can be solved using various ordinary differential equation ode solvers provided within the htscomponent including the classical fourth order runge kutta method or the adaptive step size controlled fifth order runge kutta cash carp method cash and karp 1990 alternatively users can select variable multistep methods including the adams moulton formulas or the backward differentiation formulas that are provided through the cvode hindmarsh et al 2017 external ode solver library 2 2 4 radiative heat exchange component rhecomponent following maidment 1993 magnusson et al 2012 and glose et al 2017 the net shortwave solar radiation on the water surface jsn is estimated as follows 3 j s n 1 r s j i n 1 f s where r s is albedo the fraction of shortwave radiation that is reflected j i n is the incoming shortwave radiation w m 2 and f s is the fraction of the solar radiation that is removed through shading the long wave radiation calculations for back radiation j br from the surface of the water atmospheric j an and land cover j lc are calculated using modified forms of the stefan boltzmann law mccutcheon 1990 as follows 4 j b r Œµ w œÉ t 4 5 j a n Œµ a t m œÉ t a 4 1 r l 6 j l c Œµ l c 1 f s k y œÉ t a 4 where Œµ is the emissivity of the material under consideration i e Œµ w for emissivity of water Œµ a t m for emissivity of the atmosphere and Œµ l c for emissivity of land cover t and t a are temperatures of the water in the river channel and of air respectively k œÉ is the stefan boltzmann constant w m 2 k 4 rl is the longwave reflection coefficient and f s k y is the sky view factor the formulations for computing atmospheric emissivity Œµ a t m are provided in appendix b 2 3 model setup and coupling configurations the discretization of the spatial domain for the swmm component for the logan river temperature modeling was performed using a combination of lidar data measured bathymetric cross section profiles and aerial imagery for simplicity rectangular cross sectional channels were adopted for all of the computational elements in the model the resulting widths of the computational elements estimated from aerial imagery and lidar ranged between 5 and 25 m the lengths of these elements were selected to capture the curvature of the river and locations where channel widths changed noticeably they were also selected to capture monitoring inflow and diversion locations the lengths of these elements ranged between 16 and 116 m with an average element length of 40 m the full dynamic wave equation was adopted for the hydraulic routing to avoid the cost of resolving spatially misaligned computational elements across model components the spatial discretization used in the swmm component was adopted for all the other components involved in the coupled river temperature model while this approach was valid for our application in terms of the spatial and temporal scales of applicability of our formulations it may not be valid for some applications for example in scenarios where components have been developed for applications at disparate scales forcing all components to operate at a common spatial scale may render the assumptions underlying one or more of the components invalid leading to erroneous results the upstream boundary condition was prescribed based on flow observations from the logan river at the utah water research laboratory uwrl monitoring location fig 1 the major measured inflows and diversions were accounted for in our application fig 3 however due to the nature of the abstractions from the logan river and the many stormwater inflow points the locations of some of these diversions and inflows are either unknown or poorly monitored in fig 3 the widths of the lines within the logan river have been roughly scaled to the magnitude of the average discharge during our study period locations with good discharge records for our simulation period are marked with the letter d and those with good temperature records are marked with the letter t fig 3 after applying all monitored inflows and diversions to the routing component and comparing the resulting discharges with observations at monitoring locations along the river it was clear that the unaccounted contributions from groundwater and other urban inflows were significant and therefore had to be considered to ensure mass balance closure to close this mass balance error the routing component was executed and residuals between the observed and simulated values were computed between monitoring stations e g logan river at uwrl to logan river at main street these residuals were then manually added to the corresponding sub reaches as distributed inflows between discharge monitoring points the mass balance closure approach described was applied between three zones representing sections bracketed by instream discharge measurements and the major diversion at 1600 west the three zones included 1 temperature calibration zone 1 tcz1 between the uwrl and main street 2 temperature calibration zone 2 tcz2 between main street and the 1600 west diversion and 3 temperature calibration zone 3 tcz3 between the 1600 west diversion and mendon road fig 3 the mass balance closure approach used in tcz2 and tcz3 was slightly different because there were no instream flow measurements above or below the 1600 west diversion therefore the total amount of water entering the system between main street and mendon had to be weighted such that enough water was added in the tcz2 reach so that the diversion at 1600 west would not lead to negative discharge values in the channel we additionally added a criterion that the discharge remaining after the 1600 west diversion was roughly half of the discharge measured at mendon road based on detailed instantaneous discharge measurements made throughout this reach in 2015 this allowed for the rest of the contributions to tcz3 to come from shallow groundwater and contributions from the little logan river return figs 1 and 3 for these estimated inflows temperatures from nearby monitored external inflows were selected and used as their surrogates to compute their associated heat contributions for tcz1 and tcz2 the temperature of the river heights storm drain was used as a surrogate for the unaccounted inflows we suspected that the unaccounted flow in those zones was largely groundwater and the river heights stormwater drain which discharges year round and exhibits a temperature signature similar to groundwater i e relatively constant temperature of 14 8 c with a diurnal temperature swing of about 0 6 c which is a relatively muted diurnal variability when compared to that typically associated with surface water for the unaccounted flow computed for tcz3 and spring creek inflows the temperature from the blacksmith fork river inflow site was used as a surrogate we used blacksmith fork river temperatures because subsequent data collected in 2018 showed that blacksmith fork temperatures were most similar to spring creek and little logan river return flows in terms of average temperature the range of diurnal temperature swings and timing for the heat transport component cshcomponent advection was approximated numerically based on hydraulics information provided from the swmmcomponent and the temperature boundary condition information from the uwrl monitoring location evaporation condensation conduction and convection were estimated based on information provided from the swmm model temperature boundary conditions and weather information provided by a local weather station fig 1 for the radiative heat exchange component the heat fluxes computed were net solar radiation atmospheric longwave radiation and back longwave radiation from the water surface land cover longwave radiation was not simulated due to data limitations and the simplified approach for handling shading within the model calibration as discussed below for the sediment conduction and hyporheic exchange component all the heat fluxes including sediment and ground conduction as well as the advective heat exchange were simulated 2 3 1 coupling configurations the heat transport component cshcomponent is the central component that computes river temperatures and thus receives various hydraulic variables and heat fluxes from other components in order to compute temperatures for each computational element in the coupled river temperature model input data were provided by each of the core components at each timestep to the cshcomponent in the coupled model fig 4 in fig 4 the rectangles represent the component instances i e component initialized with input data where the leftmost rectangle with the red outline i e the calibration component indicates the component that controls the entire simulation the red dots represent the outputs that can be supplied from one component to another while the blue dots represent the inputs that can be retrieved from other components the lines connecting the inputs and outputs represent coupling linkages over which data is exchanged during a simulation the arrows on the lines indicate the direction of data flow the swmmcomponent merely acted as a source for hydraulic data to other components and was not set up to retrieve data from any of the other components input data retrieved by the htscomponent and the rhecomponent at each time step were passed to compute the heat fluxes needed by the cshcomponent temporal misalignment between the time stamp for the data requested by a component and time stamp of the data received by the component was resolved using a simple linear interpolation for all components in the coupled model while we could have used higher order interpolation methods we chose linear interpolation to avoid the memory data storage requirements computational costs and complexity of higher order interpolation methods additionally because of the short time steps we used in our components i e 5 s or less the accuracy of the linear interpolation method was acceptable 2 4 model calibration like other models in the earth systems and environmental modeling field river temperature models use parameters that cannot be easily measured directly and therefore must be calibrated using observations of the constituent or parameter of interest additionally the problem of non identifiability or equifinality arises because many different parameter sets within a chosen model structure can reproduce the observed behavior of that system harvey and wagner 2000 beven and freer 2001 bandaragoda and neilson 2011 several parameter estimation and uncertainty assessment approaches have been proposed including the generalized likelihood uncertainty estimation glue beven and binley 1992 and the multi algorithm genetically adaptive multiobjective amalgam vrugt and robinson 2007 for our study we adopted the nondominated sorting genetic algorithm ii nsga ii deb et al 2002 which has been successfully used in many water resources related optimization and model calibration studies reed et al 2000 tang et al 2006 bekele and nicklow 2007 confesor and whittaker 2007 the nsga ii algorithm is an evolutionary algorithm which is a class of metaheuristic algorithms that are inspired by the survival of the fittest principle from darwin s evolutionary theory coello and becerra 2009 like other multiobjective optimization algorithms the nsga ii algorithm is concerned with finding a set of decision variables that optimizes supplied objectives subject to certain constraints since there is rarely a single decision variable that simultaneously optimizes all conflicting objectives a set of variables with solutions that optimize the trade offs between objectives is sought each solution in this set of solutions referred to as the non dominated or pareto optimal set is one in which one objective cannot be improved further without causing a simultaneous deterioration in at least one objective the nsga ii algorithm specifically falls into the genetic algorithm class of evolutionary algorithms as described by nicklow et al 2010 genetic algorithms are characterized by the following steps 1 generation of an initial population of potential solutions each identified as an individual chromosome 2 computation of the objective function value or fitness metric of each solution and subsequent ranking of individuals according to this metric 3 some aspect of individual ranking and selection of candidate solutions to participate in a crossover operator where information from two or more parent solutions are combined to create offspring solutions and 4 mutation of each individual offspring to maintain diversity and prevent premature convergence to local optima these steps are repeated in sequential generations until a suitable solution is obtained the nsga ii algorithm was coded into an independent component that can be used as a general tool for other calibration applications beyond the specific temperature modeling application described here at the beginning of each generation the calibration component generates the decision variables parameters associated with the individuals to be evaluated and supplies them to the components involved in the calibration for their evaluations it then initiates the components involved to perform their evaluations after evaluations are completed it retrieves objective function values calculated from other components and performs its optimization computations to determine feasible solutions and the next set of individuals to evaluate this process is repeated until the calibration component satisfies one or more termination criteria or the maximum number of generations specified for our particular temperature modeling application the parameters calibrated the objective functions used and the calibration procedure are described in the following sections 2 4 1 calibration parameters through a simple sensitivity analysis and a review of temperature calibration literature we identified ranges of values for shading f s sediment coefficient of thermal diffusivity Œ± s e d hyporheic storage advective transport coefficient q h t s ground conduction zone depth y g r and hyporheic storage zone depth y h t s table 1 shading factors were first prescribed manually using aerial imagery and topographic data as a guide they were then adjusted through a multiplication factor during calibration shading sediment conduction and hyporheic exchange parameters were estimated separately for two different sub reaches cz1 and cz2 fig 4 based on the locations of the monitoring points in the river and a change in channel slope from a steep moderate gradient to a relatively flat gradient this change in slope has implications for sediment processes and thermal properties of these sediments additionally the two zones also roughly corresponded to a change in the spatial orientation of the river channel which has implications for the amount of shading and solar radiation influences a unique aspect of the calibration approach we implemented was applying calibration or scaling factors to the temperatures assigned to distributed inflows estimated from the mass balance closure process while shifting time series using a multiplier may not be appropriate for many calibration efforts in urban settings with limited information regarding the highly variable inflows over time and space it is the most justifiable approach given data limitations the time varying heat fluxes associated with these distributed flows and spring creek were provided through another independent component that we developed i e timeseriesprovidercomponent in fig 4 the timeseriesprovidercomponent was developed to read time varying data associated with geographic features in various formats and provide them to other components time varying data may be modified through a scaling factor and or adjusted using geometric attributes of their associated geographic features i e length area perimeter etc in total there were 16 parameters varied in the calibration process parameter ranges were selected using literature values bandaragoda and neilson 2011 neilson et al 2010a chapra 2008 and our understanding of the unique characteristics of the logan river system as a guide for our application the calibration was performed for 500 generations with 256 individuals for each generation 128 000 evaluations the calibration was conducted for an eight day period from august 11 to august 18 2016 the model was then validated using the period from august 19 to september 1 2016 2 4 2 calibration objective functions to compute the objective function values used to determine the degree to which temperature was predicted correctly for use by the calibration component we developed an independent objective function calculation component i e tsobjectivefunctioncomponent this component retrieves simulated time series data from other components at the locations of interest and compares them to observed data to compute various efficiency indexes these indexes are then provided to the calibrationcomponent for optimization for the application described in this paper we adopted the nash sutcliffe efficiency nse nash and sutcliffe 1970 index as our objective function for temperature optimization at main street and mendon road nse values range from 1 to where an nse value of 1 indicates a perfect match and a value of 0 implies that the model gives no more information than a simple mean for our calibration 1 nse was used instead of nse because the objective function is minimized in the nsga ii algorithm we used we selected the nse criteria because its sensitivity to peaks in time series records krause et al 2005 aligned with our goal of being able to accurately predict the maximum temperatures associated with the dewatering that occurs in the logan river 2 4 3 calibration procedure as detailed in buahin and horsburgh 2018 the hydrocouple component based modeling framework advances new interfaces that can be implemented to so that a component can be initialized to make independent clones of itself for execution on distributed memory architectures a cloned component could be developed as a proxy that serves as a pathway for communicating results from its associated component initialized on another processor however communication between processors on distributed memory architectures occurs over a network which introduces latency and overhead costs that may increase simulation times for this application we only implemented the cloning interface for a shared memory architecture for all components there is often a limit on the number of cores that are available on a single machine which restricts the number of coupled model compositions that can be executed in parallel for the approach we have implemented cloning of each component involved making a copy of the input file for that component on disk and rerouting outputs to new files whose names were qualified using universally unique identifiers uuids to make sure there were no conflicts in order to ensure that multiple swmm components could be executed concurrently in a thread safe manner the core swmm computational code was refactored to encapsulate all the global variables into a new structure the calibration component adopts a farmer worker parallelization scheme for more efficient simulations at the beginning of each generation of the calibration process the calibration component makes as many clones of the entire composition as needed i e all components and their respective linkages each representing an individual in the population the cloned compositions are then farmed out to the available computing cores for parallel execution upon completion the cloned calibration components collate results of objective function evaluations from their respective compositions and transmit them to their parent component to continue the calibration process for our calibration application we executed our coupled model composition on a single 32 core high performance computing hpc node to test the speedup in simulation times we executed the coupled temperature model composition several times while varying the number of computing cores available to measure the improvement in simulation time as a result of the parallelism implemented in the calibration component 3 results and discussion overall the automated calibration process performed through the calibration component progressed successfully to completion without any failures the component cloning method and subsequent farmer worker parallel model execution pattern employed also worked successfully using this parallel pattern improved our simulation times significantly as more computation cores were allocated profiling of the calibration simulations showed that increasing the number of computational cores from 1 to 32 decreased the simulation times by up to 14 times as shown in fig 5 also shown in fig 5 is the linear speedup one would expect for fully parallel code that does not have any serial sections for comparison the speedup observed corresponds to code where 4 of the time is spent in the serial sections and 96 in the parallel sections when fitted to the speedup function prescribed by amdahl s law amdahl 1967 the serial portions of our code included the computation of the nsga ii operators i e ranking sorting mutation crossover etc component cloning and wiring components together to form coupled model compositions all of which could be parallelized for additional computational efficiency it is also important to note that the number of individuals per generation used 256 outstrips the 32 cores we used hence only 32 individuals could be evaluated in parallel at a time leading to wait periods that further eroded performance therefore we anticipate that further reductions in simulation times may be achieved by allocating more cores and nodes through the use of both shared memory and distributed memory parallel computing architectures however developing components to handle the complexities of message passing over distributed memory architectures is not trivial and requires more programming expertise and development time for the components the benefit of being able to use more computing cores and nodes is that more individuals and evaluations over more generations can be performed leading to better solutions and an ability to solve more complex problems for our application we carefully selected time step settings and model parameters to ensure that there were no failures in our components however in practice components may fail due to any number of reasons e g wrong inputs model non convergence model instability etc failures must therefore be handled carefully in a component and communicated with other components in an appropriate manner the same cloning mechanism and parallel model execution pattern employed in this study can be adopted in other component based modeling frameworks for simulations that are characterized by independently executing multiple copies of the same model with varied parameters and collating the results for evaluation the results from our modeling effort showed that the mass balance closure method implemented to estimate the unaccounted inflows worked well when comparing simulated versus observed discharge before and after the mass balance closure process at main street and mendon road fig 6 however the procedure led to discharge predictions that did not capture some of the sharp changes in flow that were seen in the observed discharge this is because the missing discharge was applied as distributed inflows and therefore smoothed over some of these discontinuities it is important to note here that the discharge measurements themselves have uncertainties since they were developed from rating curves and measured water stage data however these data have undergone detailed quality assurance and quality control procedures and many of the sharp changes in flow at main street can be explained by upstream water management the 43 feasible solutions obtained at the end of the final generation of the calibration process had temperature nse values between 0 75 and 0 92 for main street and 0 78 and 0 79 for mendon road fig 7 these results are comparable or better than results reported in other river temperature modeling studies for applications in rural to urban areas e g norton and bradford 2009 bandaragoda and neilson 2011 glose et al 2017 this is true despite the complexity of the river system simulated and the various assumptions made regarding the temperatures of the various missing inflows are considered the nse values for the feasible solutions at main street were better than those at mendon road because of the errors surrounding the assumptions made for the temperatures for the unaccounted flows and spring creek in the downstream section of the model domain regardless these results highlight the utility of the calibration approach implemented within the component based framework the temperatures associated with the feasible solutions obtained from the calibration simulation for main street and mendon road fig 8 illustrate that the model calibrated parameters worked well predicting peak temperatures in the validation period with root mean square errors of 0 44 0 61 c for main street and 0 64 0 70 c for mendon road additionally the ranges of the parameters for the feasible solutions obtained table 2 provided useful insights into the dynamics affecting water temperature in the section of the logan river that was simulated the relative standard deviation of the feasible solutions indicated that parameters in the upstream section of the model i e cz1 were generally more uncertain than parameters in the downstream section i e cz2 of the model this observation may be explained by the assumptions we made regarding inflows and diversions in tcz2 tcz3 and cz2 which resulted in a narrow range of parameter values that could correctly account for observed temperatures at our calibration location at mendon road in general there was more solar radiation influence in cz2 fig 9 which coupled with the decreased flows and shallower depths resulting from diversions gave rise to increased temperatures and therefore more evaporation fig 9 while the advective transport and thermal diffusivity coefficients were comparable for cz1 and cz2 they were slightly greater in cz2 leading to slightly higher sediment conduction and hyporheic exchange heat fluxes in cz2 fig 9 these results highlight the utility of a component based temperature model that can be adapted as additional heat fluxes are identified as important for capturing observed temperature dynamics in each system in other prior efforts e g king and neilson 2019 cardenas et al 2014 knowledge of the underlying code structure was necessary so that it could be modified to account for additional heat fluxes the flexibility afforded by the component based modeling paradigm allows for the application to diverse systems additionally by allowing users to select and only apply the most important components it results in efficient models that allow for many iterations during model calibration the calibrated temperature multiplication factors for the surrogate temperatures used for the unaccounted inflows computed from the mass balance closure process revealed additional insights for tcz1 the temperature multiplication factor was greater than unity with an average value of 1 22 this indicates that our assumption that the bulk of inflow entering that section is groundwater was likely incorrect unaccounted inflows in tcz1 are most likely from a combination of groundwater and warmer surface water sources including residential irrigation water routed through the city via street gutters the tcz2 temperature multiplication factor was unity indicating that our assumption that inflows in that section were from cooler groundwater was reasonable for tcz3 the temperature multiplier was also unity indicating that our use of the blacksmith fork as a surrogate for temperature of inflows was reasonable for the spring creek inflow the temperature multiplication factor was greater than unity indicating that either temperatures are warmer than the blacksmith fork i e the station used as surrogate temperature or the inflow volumes from spring creek are larger at its confluence with the logan river than the values applied from the monitoring station located further upstream 4 conclusions in this paper we have presented an approach for efficiently conducting model calibration uncertainty assessment sensitivity analysis and other types of simulations in this class within component based modeling frameworks we illustrated this approach through the calibration of a component based river temperature model developed for the logan river which is a complex system with many inflows and diversions many of which are unmonitored the approach we employed involves converting these algorithms in this class of simulation into components in their own right that can be coupled to other models in coupled model compositions since this class of simulations is characterized by executing many instances of the same model multiple times and varying their parameters to evaluate their effect in an independent fashion they are amenable to parallelization for computational efficiency to achieve this efficiency within component based models we employed the component cloning infrastructure provided by the hydrocouple framework this infrastructure allows for components to make independent copies of themselves on demand either locally or on a remote computer for parallel execution for the river temperature modeling application presented in this paper we developed a calibration component that employed the farmer worker parallel pattern to achieve efficiency this process involved cloning the entire composition as many times as needed at each generation of the calibration and farming the clones out to available computing cores for parallel execution the cloning of the coupled model composition involved the calibration component requesting all the individual components in the composition to make clones of themselves and establishing identical connections between a clone and its parent this pattern led to a 14 times reduction in simulation times when we moved from a 1 core processor to a 32 core processor while we only used the shared memory parallelism architecture for our application the cloning mechanism can be extended to distributed memory architectures by implementing the cloning function such that clones are created on remote machines for more efficiency a clone on a remote machine would be presented to components on the local machine using a proxy component whose job it would be to communicate results from its corresponding component on the remote machine the component cloning interfaces as well as the parallel execution pattern described in this paper can be adopted and implemented within other existing component based modeling frameworks to enable similar functionality for more efficient simulations the results from our calibration efforts were successful with nse values between 0 78 and 0 92 for temperature results at our calibration locations additionally the increased level of granularity enabled by the component based modeling framework allowed us to implement the various optimization penalty functions and a time series provider as components in their own right this provided flexibility and allowed us to better understand some river temperature dynamics and feedbacks with the hyporheic zones stormwater and groundwater that occur in the logan river for example the time series provider allowed us to apply an adjusted time series by calibrating scaling factors for observed inflows which allowed for the identification dominant sources of inflow to various sections of the logan river more broadly in addition to providing modelers with flexibility to represent different parts of the water system with scale relevant process formulations and allowing modelers to evaluate feedbacks between sub systems the approach we have demonstrated here extends these capabilities further for the component based modeling community instead of single deterministic simulations modelers can embark on multi scenario evaluations efficiently using different parameters and model structures to understand how they affect the magnitudes of feedbacks and system wide behavior in turn this can help in deriving solutions or adaptation strategies to the myriad of water resources related challenges we face this way solutions are not myopically developed to address problems from the perspective of a single domain acknowledgements this research was supported by national science foundation epscor grant iia 1208732 awarded to utah state university as part of the state of utah epscor research infrastructure improvement award and nsf ear 1343861 any opinions findings and conclusions or recommendations expressed are those of the author s and do not necessarily reflect the views of the national science foundation the authors would also like to extend their gratitude to hyrum tennant who was invaluable in the data collection effort used to support the work presented in this manuscript appendix a cshcomponent formulations fig a1 1d control volume element fig a1 the integral version of the heat advection and dispersion equation over a time step from t to Œ¥ t over computational element i e cvi is a1 œÅ w c w t t Œ¥ t c v t t d v d t œÅ w c w t t Œ¥ t c v v t x d v d t œÅ w c w t t Œ¥ t c v x d t x d v d t t t Œ¥ t c v j y d v d t t t Œ¥ t c v j e j c y d v d t t t Œ¥ t c v s d v d t where v is the volume of the cv m 3 t represents the current time step s and t Œ¥ t represents the next time step where we seek a solution using gauss s divergence theorem and expanding the terms yields a2 t t Œ¥ t œÅ w c w t v t d t t t Œ¥ t œÅ w c w k 1 n b v t a d t t t Œ¥ t œÅ w c w k 1 n b d t x a d t t t Œ¥ t j y v d t t t Œ¥ t j e j c y v d t t t Œ¥ t s v d t where n b represents the number of inlet and outlet boundaries for the cv k 1 n b v t a represents summation of the advective heat fluxes across the inlet and outlet boundaries of the cv k 1 n b d t x a represents the sum of the dispersive heat fluxes across the inlet and outlet boundaries of the cv and a is the cross sectional area m 2 of flow using an explicit time marching scheme for the cv depicted in fig a1 yields equation a3 which expands to equation a4 a3 œÅ w c w t i t Œ¥ t t i t Œ¥ t v i t i t v i t Œ¥ t v i t Œ¥ t œÅ w c w k 1 n b q t i t œÅ w c w k 1 n b d t x a i t j y v i t j e j c y v i t s v i t a4 œÅ w c w t i t Œ¥ t t i t Œ¥ t v i t i t v i t Œ¥ t v i t Œ¥ t œÅ w c w q t i 1 2 t œÅ w c w q t i 1 2 t a d v e c t i o n œÅ w c w d t x a i 1 2 t œÅ w c w d t x a i 1 2 t d i s p e r s i o n j y v i t s v i t e x t e r n a l h e a t s o u r c e s j e j c y v i t s e n s i b l e a n d l a t e n t h e a t where fluxes out of the cv take on positive values fluxes into the cv take on negative values values with the superscripts t and t Œ¥ t represent values at the current time step and next time step respectively values with the subscripts i i 1 2 and i 1 2 represent values at the current cv its left boundary and right boundary respectively Œ¥ t is the time step s and q is the flow for the cv m 3 s advection discretization several methods are available for discretizing the advection terms in equation a4 these include the upwind central and hybrid differencing methods additionally several total variation diminishing tvd harten 1983 schemes are also available for problems that have sharp discontinuities in their solutions an exhaustive treatment of tvd schemes is provided by versteeg and malalasekera 2007 and are not described here for the first order accurate upwind differencing scheme the assumptions made for inlet and outlet advective heat fluxes for boundaries of the control volume are prescribed as follows a5 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i 1 a6 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i for the second order accurate central differencing scheme the inlet and outlet advective heat fluxes at the boundaries of the control volume are interpolated using the inverse distance weighting idw interpolation scheme as shown in equations a7 and a8 a7 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i 1 x i 1 2 x i 1 t i x i x i 1 2 x i x i 1 a8 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i x i 1 2 x i t i 1 x i 1 x i 1 2 x i 1 x i while the upwind differencing scheme is stable it is only first order accurate which gives rise to false diffusion this contrasts with the central differencing scheme which although second order accurate does not possess the transportiveness property i e ability to account for flow direction as well as the upwind scheme especially for highly advective flows versteeg and malalasekera 2007 the hybrid differencing scheme proposed by spalding 1972 attempts to split these tradeoffs be assessing whether advection or dispersion is the dominant transport mechanism the hybrid differencing scheme proceeds by first estimating the peclet number p e at the face of the control volume of interest as follows a9 p e i 1 2 v i 1 2 d i 1 2 x i x i 1 the flux through that face of the control volume is then estimated as follows a10 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 f i 1 t i 1 1 1 f i 1 p e i 1 2 f i t i 1 1 f i p e i 1 2 f o r 2 p e i 1 2 2 a11 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i 1 for p e i 1 2 2 a12 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i for p e i 1 2 2 where f i 1 and f i are inverse distance weighted interpolation factors for the current and left control volumes that surround the boundary under consideration respectively dispersion discretization the spatial gradients of temperature at inlet and outlet of the cv used for computing dispersion in equation a4 are discretized numerically as follows a13 t x i 1 2 t i t i 1 x i x i 1 a14 t x i 1 2 t i 1 t i x i 1 x i following the qual2k model pelletier and chapra 2008 the cshcomponent adopts the formulations by fischer 1979 to calculate longitudinal dispersion when it is not explicitly provided as follows a15 d i 0 11 v i 2 b i 2 y i u i where b i is the channel width m y i is the mean flow depth m and u i is shear velocity m s of the cv the shear velocity is calculated as a16 u i g y i s i where s i is the channel bottom slope the computed dispersion coefficient d i is compared with the numerical dispersion estimated using equation a17 a17 e i v i x i 1 x i 1 2 if the computed numerical dispersion is less than the computed dispersion in equation a17 d i e i is used as the dispersion coefficient used in equation a4 otherwise the dispersion coefficient is set to zero continuity and junctions following islam and chaudhry 1998 internal junctions where 3 or more elements meet are treated as internal boundary conditions where temperatures are estimated using the simple mass balance equation equation a18 this equation assumes complete mixing at the junction a18 t j i 1 m q i t i i 1 m q i where t j is the temperature at the junction of interest c kg m 3 m is the number of elements with flows entering the junction q i is the flow from incoming element i kg m 3 and t i is the temperature constituent concentration in the incoming element i c kg m 3 evaporation and condensation evaporation condensation is a function of the sensible heat carried with the evaporated water the latent heat of evaporation density of water and the evaporative rate as expressed in equation a19 webb and zhang 1997 evans et al 1998 boyd and kasper 2003 a19 j e œÅ w l e e where l e is the latent heat of vaporization j kg and e is the evaporative rate m s the latent heat of vaporization is estimated as a weak function of water temperature using equation a20 martin and mccutcheon 1998 a20 l e 1000 2499 2 36 t where t is the water temperature in the channel c several approaches are available for estimating the evaporative rate including mass transfer methods explicit energy balance methods and combination methods that combine both mass and energy balance methods in the cshcomponent a mass balance method was implemented following dingman 2008 the evaporative rate is estimated using equation a21 a21 e f w e s w e a where e s w is the saturation vapor pressure of the evaporating surface kpa e a is the actual vapor pressure kpa and f w is a wind function used to estimate the adiabatic portion of evaporation boyd and kasper 2003 e s w is computed using equation a22 raudkivi 1979 chapra 2008 a22 e s w 0 61275 e 17 27 t 237 3 t where t is the temperature of the water in the channel the actual vapor pressure e a is calculated as a function of relative humidity h and saturation vapor pressure e s using equation a23 a23 e a h 100 e s where e s is computed using equation a24 a24 e s 0 61275 e 17 27 t a 237 3 t a where t a is air temperature in c extensive observations have yielded equation a25 as the general form of the wind function shanahan et al 1984 a25 f w a b w where a and b are empirical coefficients with units kpa 1 ms 1 and kpa 1 respectively and w is the wind speed measured 2 m above the water surface m s several authors have proposed values for these coefficients including dunne and leopold 1978 who proposed the values 1 505 10 8 and 1 6 10 8 for the coefficients a and b respectively these values are used as the defaults in the cshcomponent but can be overridden by user specified coefficients convection and conduction estimating sensible heat lost or gained through conduction convection with air in the atmosphere is typically performed using the bowen ratio b r which relates latent heat to sensible heat equation a26 bowen 1926 webb and zhang 1997 evans et al 1998 westhoff et al 2007 glose et al 2017 a26 b r j c j e martin and mccutcheon 1998 prescribed equation a27 for estimating the bowen ratio a27 b r j c j e c b p a p t t a e s w e a where c b is a bowen s coefficient usually equal to 0 0651 kpa c p a is atmospheric pressure kpa and p is a reference pressure at sea level kpa while the ratio p a p is often assumed to be unity the pressure difference cannot be neglected in higher elevations martin and mccutcheon 1998 cshcomponent solvers the cshcomponent can solve equation a4 using several ordinary differential equation ode solvers provided including the classical fourth order runge kutta method i e rk4 or the adaptive step size controlled fifth order runge kutta cash carp method cash and karp 1990 alternatively users can select variable multistep methods including the adams moulton formulas or the backward differentiation formulas i e bdf that are provided through the cvode hindmarsh et al 2017 external ode solver library readers are referred to the cshcomponent code in the github repository https github com hydrocouple cshcomponent for implementation details appendix b rhecomponent formulations the emissivity of the atmosphere Œµ a t m used in computing the atmospheric long wave radiation is computed from brunt 1932 b1 Œµ a t m a a a b e a where a a is an empirical coefficient with typical values between 0 5 and 0 7 a b is an empirical coefficient with a typical value of 0 0027 and e a is the vapor pressure of air p a the actual vapor pressure of air e a that is used to compute atmospheric emissivity Œµ a t m used in estimating atmospheric longwave radiation j a n is computed using equation b2 raudkivi 1979 chapra et al 2008 b2 e a h 100 e s where h is the relative humidity and e s is the saturation vapor pressure pa computed using equation b3 b3 e s 0 61275 e 17 27 t a 237 3 t a where t a is air temperature in c 
26226,many recent modeling efforts have employed component based modeling frameworks to take advantage of the flexibility they provide in representing systems more holistically despite the benefits that are driving this adoption conducting model parameter estimation uncertainty analysis sensitivity assessment and other simulations of this nature within component based modeling frameworks has remained unexplored using a multi objective calibration of a component based river temperature model we illustrate how the component cloning and parallel model execution interfaces we have implemented in the hydrocouple framework support such simulations the river temperature model calibration application we present involves a heavily human mediated 13 6 km section of the logan river in utah usa with limited information regarding variable inflows due to the flexibility in the modeling and calibration framework results from the calibration effort were successful with root mean square errors of 0 4 0 7 c and provided insights on mechanisms controlling river temperatures keywords component based modeling model calibration high performance computing river temperature modeling openmi hydrocouple software availability name of software the software described in this paper includes seven hydrocouple model components these components include 1 a hydraulic model component developed from the united states environmental protection agency s stormwater management model swmm code 2 a channel solute and heat transport component cshcomponent 3 a hyporheic transient storage zone model component htscomponent 4 a radiative heat exchange model component rhecomponent 5 a calibration component calibrationcomponent 6 a time series objective function calculation component tsobjectivefunctioncomponent 7 a time series provider component timeseriesprovidercomponent developer caleb a buahin email caleb buahin gmail com required hardware and software any computer that runs the windows linux or macintosh operating system components can be compiled using openmp http www openmp org compilation requires the qt 5 7 framework at minimum http www qt io cost the hydrocouple interface definitions software stack and model components in this manuscript are freely available under the gnu lesser general public license version 3 lgpl and can be downloaded from the hydrocouple github repository https github com hydrocouple 1 introduction the component based modeling approach involves the definition of standard software interfaces that when implemented allow disparate models to exchange information at runtime use of the component based software development paradigm promises faster development times greater reusability and more maintainable code bases l√∂wy 2003 szyperski 2002 beyond these software development related benefits the use of component based models in the earth systems and environmental modeling field provides the ability to select only those components that are relevant to representing a sub system of interest the ability to evaluate feedbacks between different domains and processes and a platform for evaluating the appropriateness of different model structures for representing the same process in a system argent et al 1999 peckham et al 2013 to date several component based modeling frameworks and interface standards have been proposed and implemented including the earth system modeling framework esmf hill et al 2004 model coupling toolkit mct warner et al 2008 the community surface dynamics modeling system csdms peckham et al 2013 and the open modeling interface openmi moore and tindall 2005 to name a few these frameworks and standards have been applied to a myriad of studies e g ascough et al 2009 shrestha et al 2013 long et al 2016 zhu et al 2016 shrestha et al 2018 in reviewing these applications however only a few have attempted the types of simulations characterized by running a model many times while intelligently varying input variables and parameters to evaluate their effects e g parameter estimation uncertainty analysis optimization sensitivity analysis and others recently christelis and hughes 2018 conducted a sensitivity analysis exercise that utilized a surrogate kringing model that was developed using training data from a computationally expensive component based model comprised of a channel routing component and a grid based groundwater component however they performed their sensitivity analysis outside of the component based modeling framework using matlab additionally they did not utilize the parallelism increasingly available on multicore clusters rizzoli et al 2008 noted that this class of simulations could be considered as model components in their own right to which the principles of hierarchical decomposition and object orientation that underlie the component based development paradigm can be equally applied still examples and tools that follow this recommendation have so far been missing there may be several explanations for this general lack of examples that follow the recommendation by rizzoli et al 2008 first there is a general difficulty in conducting this class of simulations for complex coupled model compositions involving multiple components oreskes 2003 voinov and shugart 2013 for example in calibration simulations the interdependency between the outputs and inputs of components involved in a coupled model as well as the errors introduced in the model integration process may mean that instead of calibrating individual components independently calibrating component based models most likely requires re calibration of the integrated model as a whole voinov and shugart 2013 the absence of examples of this class of simulations in component based modeling frameworks may also be attributed to the relative infancy of the component based modeling approach more crucially the designs of many of the existing component based modeling frameworks have not provided interfaces tools and mechanisms to facilitate these simulations in an efficient manner peckham et al 2016 noted this absence of uncertainty quantification or data based model verification calibration tools and applications in particular within component based modeling frameworks they proposed the need to integrate general purpose toolkits for optimization parameter estimation and uncertainty quantification as well as tools to numerically compute a range of penalty and objective functions and their derivatives in an efficient manner to address the need for calibration sensitivity analysis uncertainty assessment and other related tools in the context of component based modeling we describe a complex multi component river temperature model and demonstrate the integration and application of a generalizable method for calibrating a coupled model composition within a component based modeling framework for our application we used the hydrocouple component based modeling framework which is a recently developed model coupling framework based on the openmi 2 0 interface definitions that provides new interfaces to facilitate more efficient simulations buahin and horsburgh 2018 hydrocouple provides interface definitions that when implemented allow components to make independent copies of themselves for more efficient parallel and distributed execution while hydrocouple defines the necessary interfaces for enabling parallel execution of a coupled model composition the details of the process used to execute the class of simulations being considered here are missing we provide those details in this paper using the river temperature model as a demonstrative case study the specific model components used in our application were developed to enable the creation of adaptable river temperature model compositions such that modelers can identify and implement the key heat fluxes at relevant spatial and temporal scales in the different parts of a system by simply adding or removing one or more of the components such adaptability of temperature models is important given the variability in the dominant heat fluxes in various physical systems e g johnson 2004 neilson et al 2009 cardenas et al 2014 king et al 2016 etc and the computational requirements for simulating these fluxes using the components we developed we assembled a coupled model composition that was used to simulate the water temperature dynamics in a section of the logan river utah 2 methods 2 1 study area the logan river which was the focus of our study is located on the eastern end of cache valley in northern utah usa it drains a 109 5 km2 area situated on the western end of the bear river range as the logan river exits logan canyon it flows through the city of logan and then westward to cutler reservoir and eventually flowing into the great salt lake it is diverted at various points along its length into canals that are used for irrigation in the valley these diversions lead to dewatering of sections of the logan river during the irrigation season some of the water from these irrigation diversions returns to the river channel via shallow groundwater while some is returned to the river at different locations downstream additionally urban stormwater outfalls groundwater drains and various tributaries enter the river through the study area and further complicate the longitudinal and temporal temperature dynamics in the river the model domain used in this study includes a 13 6 km section of the logan river with two major diversions two tributaries and many urban drains inflows six long term monitoring locations have been established for discharge and temperature in the main stem of the logan river and some of the major inflows into the river fig 1 while some of the inflows into the logan river have been identified and monitored many of them have either not been located or have been located but not monitored our modeling objective was to investigate the relative importance of different heat fluxes during ecologically important times of the year when flows are reduced by agricultural diversions and water temperatures become elevated because of the poorly monitored influences from irrigation returns groundwater inflows and other urban inflows calibration of the coupled model composition was necessary to account for these unmeasured flows this case study provides a specific example of how model calibration and other types of simulations that involve running a model many times while varying input variables and parameters can be performed using coupled modeling frameworks through the use of component cloning and parallel execution 2 2 core river temperature model components to simulate the temperature in the logan river we used a coupled model composition consisting of four core components that together represent the major processes and heat fluxes controlling water temperatures in the logan river fig 2 the hydraulic routing component swmmcomponent was developed from the epa swmm model to route time variable flow through the river channel and provide output variables including velocity flow depth flow cross sectional area and channel top width that are needed for other heat transfer and transport components the channel solute and heat transport component cshcomponent was developed to simulate the advection and dispersion of heat in the channel over time based on formulations within neilson et al 2010a and 2010b it also computes the latent heat of evaporation and condensation j e as well as sensible heat from convection and conduction j c the radiative heat exchange component rhecomponent computes radiant heat fluxes received j sn and j an and emitted j br from the surface of the water in the channel a two layer sediment conduction and hyporheic zone exchange component htscomponent was developed to simulate the advective j hts and conductive j sed heat fluxes between the water in the channel and the hyporheic zone and the hyporheic zone and ground j gr based on formulations presented in neilson et al 2010a 2 2 1 hydraulic routing component swmmcomponent the swmmcomponent was developed from the united states environmental protection agency s epa s stormwater management model swmm which calculates runoff from sub catchments and routes the resulting runoff through a network of open channels and closed conduits with regular and irregular cross sections connected together by nodes representing inlets junctions outfalls and other nodes types rossman 2010 the routing component of the swmm model can solve the full dynamic wave equations or the reduced physics diffusive and kinematic wave approximations it is therefore able to handle flows in non dendritic networks pressurized flows flow reversals and backwater effects in addition to these features swmm is open source and has been well validated over the years by an engaging community of users 2 2 2 channel heat and solute transport component cshcomponent the 1d advection dispersion heat transport equation solved in the cshcomponent is 1 œÅ w c p t t œÅ w c p v t x œÅ w c p x d t x j y j e j c y s where t is the water temperature c t is the time s v is the velocity of the water in the channel m s x is the distance along the channel m d is longitudinal dispersion m 2 s œÅ w is the water density kg m 3 c p is the specific heat capacity of water j kg c j are external radiant heat fluxes j m 2 s or w m 2 incident on the water surface j e is the evaporation and condensation from the water surface j m 2 s or w m 2 j c is the convection and conduction from the water surface j m 2 s or w m 2 s is the heat supplied by other external sources j m 3 s and y is the depth of water in the channel m equation 1 is approximated numerically using the finite volume method appendix a provides the formulations used for computing j e and j c as well as details of the solution techniques used to solve equation 1 all external heat fluxes provided by other components e g sediment conduction hyporheic exchange back radiation etc are applied through the j and s terms in equation 1 2 2 3 hyporheic transient storage zone htscomponent the htscomponent was developed based on formulations presented within neilson et al 2010a and 2010b which extend transient storage model formulations originally developed for solute transport e g bencala and walters 1983 to heat heat exchange between the hyporheic transient storage zone and the river channel is simulated using a first order heat transfer relation for each computational element equation 2 2 œÅ s e d c p s e d dt h t s dt v h t s œÅ s e d c p s e d Œ± s e d b h t s Œ¥ x t t h t s y h t s s e d i m e n t c o n d u c t i o n t e r m œÅ s e d c p s e d Œ± s e d b h t s Œ¥ x t g r t h t s y g r g r o u n d c o n d u c t i o n t e r m œÅ w c p q h t s t t h t s a d v e c t i v e h y p o r h e i c e x c h a n g e t e r m s v h t s where œÅ s e d is the density of the sediment in the hyporheic transient storage hts zone kg m 3 c p s e d is the specific heat capacity of sediment in the hts zone j kg c t h t s is the temperature of the hts zone c v h t s is the volume of the hts zone m 3 Œ± s e d is the coefficient of thermal diffusivity for the sediment in the hts computational element b h t s is the width of the hts zone m Œ¥ x is the length of the hts computational element m t is the water temperature in the main channel overlying the hts zone c y h t s and y g r represent the depths of the hts and ground conducting zones respectively m t g r is the ground temperature c q h t s is the coefficient of advective transport m 3 s and s are heat fluxes supplied by other external sources j m 3 s e g groundwater the coefficient of advective transport q hts represents the volumetric water flux that is exchanged between the main channel and the hyporheic transient storage zone equation 2 can be solved using various ordinary differential equation ode solvers provided within the htscomponent including the classical fourth order runge kutta method or the adaptive step size controlled fifth order runge kutta cash carp method cash and karp 1990 alternatively users can select variable multistep methods including the adams moulton formulas or the backward differentiation formulas that are provided through the cvode hindmarsh et al 2017 external ode solver library 2 2 4 radiative heat exchange component rhecomponent following maidment 1993 magnusson et al 2012 and glose et al 2017 the net shortwave solar radiation on the water surface jsn is estimated as follows 3 j s n 1 r s j i n 1 f s where r s is albedo the fraction of shortwave radiation that is reflected j i n is the incoming shortwave radiation w m 2 and f s is the fraction of the solar radiation that is removed through shading the long wave radiation calculations for back radiation j br from the surface of the water atmospheric j an and land cover j lc are calculated using modified forms of the stefan boltzmann law mccutcheon 1990 as follows 4 j b r Œµ w œÉ t 4 5 j a n Œµ a t m œÉ t a 4 1 r l 6 j l c Œµ l c 1 f s k y œÉ t a 4 where Œµ is the emissivity of the material under consideration i e Œµ w for emissivity of water Œµ a t m for emissivity of the atmosphere and Œµ l c for emissivity of land cover t and t a are temperatures of the water in the river channel and of air respectively k œÉ is the stefan boltzmann constant w m 2 k 4 rl is the longwave reflection coefficient and f s k y is the sky view factor the formulations for computing atmospheric emissivity Œµ a t m are provided in appendix b 2 3 model setup and coupling configurations the discretization of the spatial domain for the swmm component for the logan river temperature modeling was performed using a combination of lidar data measured bathymetric cross section profiles and aerial imagery for simplicity rectangular cross sectional channels were adopted for all of the computational elements in the model the resulting widths of the computational elements estimated from aerial imagery and lidar ranged between 5 and 25 m the lengths of these elements were selected to capture the curvature of the river and locations where channel widths changed noticeably they were also selected to capture monitoring inflow and diversion locations the lengths of these elements ranged between 16 and 116 m with an average element length of 40 m the full dynamic wave equation was adopted for the hydraulic routing to avoid the cost of resolving spatially misaligned computational elements across model components the spatial discretization used in the swmm component was adopted for all the other components involved in the coupled river temperature model while this approach was valid for our application in terms of the spatial and temporal scales of applicability of our formulations it may not be valid for some applications for example in scenarios where components have been developed for applications at disparate scales forcing all components to operate at a common spatial scale may render the assumptions underlying one or more of the components invalid leading to erroneous results the upstream boundary condition was prescribed based on flow observations from the logan river at the utah water research laboratory uwrl monitoring location fig 1 the major measured inflows and diversions were accounted for in our application fig 3 however due to the nature of the abstractions from the logan river and the many stormwater inflow points the locations of some of these diversions and inflows are either unknown or poorly monitored in fig 3 the widths of the lines within the logan river have been roughly scaled to the magnitude of the average discharge during our study period locations with good discharge records for our simulation period are marked with the letter d and those with good temperature records are marked with the letter t fig 3 after applying all monitored inflows and diversions to the routing component and comparing the resulting discharges with observations at monitoring locations along the river it was clear that the unaccounted contributions from groundwater and other urban inflows were significant and therefore had to be considered to ensure mass balance closure to close this mass balance error the routing component was executed and residuals between the observed and simulated values were computed between monitoring stations e g logan river at uwrl to logan river at main street these residuals were then manually added to the corresponding sub reaches as distributed inflows between discharge monitoring points the mass balance closure approach described was applied between three zones representing sections bracketed by instream discharge measurements and the major diversion at 1600 west the three zones included 1 temperature calibration zone 1 tcz1 between the uwrl and main street 2 temperature calibration zone 2 tcz2 between main street and the 1600 west diversion and 3 temperature calibration zone 3 tcz3 between the 1600 west diversion and mendon road fig 3 the mass balance closure approach used in tcz2 and tcz3 was slightly different because there were no instream flow measurements above or below the 1600 west diversion therefore the total amount of water entering the system between main street and mendon had to be weighted such that enough water was added in the tcz2 reach so that the diversion at 1600 west would not lead to negative discharge values in the channel we additionally added a criterion that the discharge remaining after the 1600 west diversion was roughly half of the discharge measured at mendon road based on detailed instantaneous discharge measurements made throughout this reach in 2015 this allowed for the rest of the contributions to tcz3 to come from shallow groundwater and contributions from the little logan river return figs 1 and 3 for these estimated inflows temperatures from nearby monitored external inflows were selected and used as their surrogates to compute their associated heat contributions for tcz1 and tcz2 the temperature of the river heights storm drain was used as a surrogate for the unaccounted inflows we suspected that the unaccounted flow in those zones was largely groundwater and the river heights stormwater drain which discharges year round and exhibits a temperature signature similar to groundwater i e relatively constant temperature of 14 8 c with a diurnal temperature swing of about 0 6 c which is a relatively muted diurnal variability when compared to that typically associated with surface water for the unaccounted flow computed for tcz3 and spring creek inflows the temperature from the blacksmith fork river inflow site was used as a surrogate we used blacksmith fork river temperatures because subsequent data collected in 2018 showed that blacksmith fork temperatures were most similar to spring creek and little logan river return flows in terms of average temperature the range of diurnal temperature swings and timing for the heat transport component cshcomponent advection was approximated numerically based on hydraulics information provided from the swmmcomponent and the temperature boundary condition information from the uwrl monitoring location evaporation condensation conduction and convection were estimated based on information provided from the swmm model temperature boundary conditions and weather information provided by a local weather station fig 1 for the radiative heat exchange component the heat fluxes computed were net solar radiation atmospheric longwave radiation and back longwave radiation from the water surface land cover longwave radiation was not simulated due to data limitations and the simplified approach for handling shading within the model calibration as discussed below for the sediment conduction and hyporheic exchange component all the heat fluxes including sediment and ground conduction as well as the advective heat exchange were simulated 2 3 1 coupling configurations the heat transport component cshcomponent is the central component that computes river temperatures and thus receives various hydraulic variables and heat fluxes from other components in order to compute temperatures for each computational element in the coupled river temperature model input data were provided by each of the core components at each timestep to the cshcomponent in the coupled model fig 4 in fig 4 the rectangles represent the component instances i e component initialized with input data where the leftmost rectangle with the red outline i e the calibration component indicates the component that controls the entire simulation the red dots represent the outputs that can be supplied from one component to another while the blue dots represent the inputs that can be retrieved from other components the lines connecting the inputs and outputs represent coupling linkages over which data is exchanged during a simulation the arrows on the lines indicate the direction of data flow the swmmcomponent merely acted as a source for hydraulic data to other components and was not set up to retrieve data from any of the other components input data retrieved by the htscomponent and the rhecomponent at each time step were passed to compute the heat fluxes needed by the cshcomponent temporal misalignment between the time stamp for the data requested by a component and time stamp of the data received by the component was resolved using a simple linear interpolation for all components in the coupled model while we could have used higher order interpolation methods we chose linear interpolation to avoid the memory data storage requirements computational costs and complexity of higher order interpolation methods additionally because of the short time steps we used in our components i e 5 s or less the accuracy of the linear interpolation method was acceptable 2 4 model calibration like other models in the earth systems and environmental modeling field river temperature models use parameters that cannot be easily measured directly and therefore must be calibrated using observations of the constituent or parameter of interest additionally the problem of non identifiability or equifinality arises because many different parameter sets within a chosen model structure can reproduce the observed behavior of that system harvey and wagner 2000 beven and freer 2001 bandaragoda and neilson 2011 several parameter estimation and uncertainty assessment approaches have been proposed including the generalized likelihood uncertainty estimation glue beven and binley 1992 and the multi algorithm genetically adaptive multiobjective amalgam vrugt and robinson 2007 for our study we adopted the nondominated sorting genetic algorithm ii nsga ii deb et al 2002 which has been successfully used in many water resources related optimization and model calibration studies reed et al 2000 tang et al 2006 bekele and nicklow 2007 confesor and whittaker 2007 the nsga ii algorithm is an evolutionary algorithm which is a class of metaheuristic algorithms that are inspired by the survival of the fittest principle from darwin s evolutionary theory coello and becerra 2009 like other multiobjective optimization algorithms the nsga ii algorithm is concerned with finding a set of decision variables that optimizes supplied objectives subject to certain constraints since there is rarely a single decision variable that simultaneously optimizes all conflicting objectives a set of variables with solutions that optimize the trade offs between objectives is sought each solution in this set of solutions referred to as the non dominated or pareto optimal set is one in which one objective cannot be improved further without causing a simultaneous deterioration in at least one objective the nsga ii algorithm specifically falls into the genetic algorithm class of evolutionary algorithms as described by nicklow et al 2010 genetic algorithms are characterized by the following steps 1 generation of an initial population of potential solutions each identified as an individual chromosome 2 computation of the objective function value or fitness metric of each solution and subsequent ranking of individuals according to this metric 3 some aspect of individual ranking and selection of candidate solutions to participate in a crossover operator where information from two or more parent solutions are combined to create offspring solutions and 4 mutation of each individual offspring to maintain diversity and prevent premature convergence to local optima these steps are repeated in sequential generations until a suitable solution is obtained the nsga ii algorithm was coded into an independent component that can be used as a general tool for other calibration applications beyond the specific temperature modeling application described here at the beginning of each generation the calibration component generates the decision variables parameters associated with the individuals to be evaluated and supplies them to the components involved in the calibration for their evaluations it then initiates the components involved to perform their evaluations after evaluations are completed it retrieves objective function values calculated from other components and performs its optimization computations to determine feasible solutions and the next set of individuals to evaluate this process is repeated until the calibration component satisfies one or more termination criteria or the maximum number of generations specified for our particular temperature modeling application the parameters calibrated the objective functions used and the calibration procedure are described in the following sections 2 4 1 calibration parameters through a simple sensitivity analysis and a review of temperature calibration literature we identified ranges of values for shading f s sediment coefficient of thermal diffusivity Œ± s e d hyporheic storage advective transport coefficient q h t s ground conduction zone depth y g r and hyporheic storage zone depth y h t s table 1 shading factors were first prescribed manually using aerial imagery and topographic data as a guide they were then adjusted through a multiplication factor during calibration shading sediment conduction and hyporheic exchange parameters were estimated separately for two different sub reaches cz1 and cz2 fig 4 based on the locations of the monitoring points in the river and a change in channel slope from a steep moderate gradient to a relatively flat gradient this change in slope has implications for sediment processes and thermal properties of these sediments additionally the two zones also roughly corresponded to a change in the spatial orientation of the river channel which has implications for the amount of shading and solar radiation influences a unique aspect of the calibration approach we implemented was applying calibration or scaling factors to the temperatures assigned to distributed inflows estimated from the mass balance closure process while shifting time series using a multiplier may not be appropriate for many calibration efforts in urban settings with limited information regarding the highly variable inflows over time and space it is the most justifiable approach given data limitations the time varying heat fluxes associated with these distributed flows and spring creek were provided through another independent component that we developed i e timeseriesprovidercomponent in fig 4 the timeseriesprovidercomponent was developed to read time varying data associated with geographic features in various formats and provide them to other components time varying data may be modified through a scaling factor and or adjusted using geometric attributes of their associated geographic features i e length area perimeter etc in total there were 16 parameters varied in the calibration process parameter ranges were selected using literature values bandaragoda and neilson 2011 neilson et al 2010a chapra 2008 and our understanding of the unique characteristics of the logan river system as a guide for our application the calibration was performed for 500 generations with 256 individuals for each generation 128 000 evaluations the calibration was conducted for an eight day period from august 11 to august 18 2016 the model was then validated using the period from august 19 to september 1 2016 2 4 2 calibration objective functions to compute the objective function values used to determine the degree to which temperature was predicted correctly for use by the calibration component we developed an independent objective function calculation component i e tsobjectivefunctioncomponent this component retrieves simulated time series data from other components at the locations of interest and compares them to observed data to compute various efficiency indexes these indexes are then provided to the calibrationcomponent for optimization for the application described in this paper we adopted the nash sutcliffe efficiency nse nash and sutcliffe 1970 index as our objective function for temperature optimization at main street and mendon road nse values range from 1 to where an nse value of 1 indicates a perfect match and a value of 0 implies that the model gives no more information than a simple mean for our calibration 1 nse was used instead of nse because the objective function is minimized in the nsga ii algorithm we used we selected the nse criteria because its sensitivity to peaks in time series records krause et al 2005 aligned with our goal of being able to accurately predict the maximum temperatures associated with the dewatering that occurs in the logan river 2 4 3 calibration procedure as detailed in buahin and horsburgh 2018 the hydrocouple component based modeling framework advances new interfaces that can be implemented to so that a component can be initialized to make independent clones of itself for execution on distributed memory architectures a cloned component could be developed as a proxy that serves as a pathway for communicating results from its associated component initialized on another processor however communication between processors on distributed memory architectures occurs over a network which introduces latency and overhead costs that may increase simulation times for this application we only implemented the cloning interface for a shared memory architecture for all components there is often a limit on the number of cores that are available on a single machine which restricts the number of coupled model compositions that can be executed in parallel for the approach we have implemented cloning of each component involved making a copy of the input file for that component on disk and rerouting outputs to new files whose names were qualified using universally unique identifiers uuids to make sure there were no conflicts in order to ensure that multiple swmm components could be executed concurrently in a thread safe manner the core swmm computational code was refactored to encapsulate all the global variables into a new structure the calibration component adopts a farmer worker parallelization scheme for more efficient simulations at the beginning of each generation of the calibration process the calibration component makes as many clones of the entire composition as needed i e all components and their respective linkages each representing an individual in the population the cloned compositions are then farmed out to the available computing cores for parallel execution upon completion the cloned calibration components collate results of objective function evaluations from their respective compositions and transmit them to their parent component to continue the calibration process for our calibration application we executed our coupled model composition on a single 32 core high performance computing hpc node to test the speedup in simulation times we executed the coupled temperature model composition several times while varying the number of computing cores available to measure the improvement in simulation time as a result of the parallelism implemented in the calibration component 3 results and discussion overall the automated calibration process performed through the calibration component progressed successfully to completion without any failures the component cloning method and subsequent farmer worker parallel model execution pattern employed also worked successfully using this parallel pattern improved our simulation times significantly as more computation cores were allocated profiling of the calibration simulations showed that increasing the number of computational cores from 1 to 32 decreased the simulation times by up to 14 times as shown in fig 5 also shown in fig 5 is the linear speedup one would expect for fully parallel code that does not have any serial sections for comparison the speedup observed corresponds to code where 4 of the time is spent in the serial sections and 96 in the parallel sections when fitted to the speedup function prescribed by amdahl s law amdahl 1967 the serial portions of our code included the computation of the nsga ii operators i e ranking sorting mutation crossover etc component cloning and wiring components together to form coupled model compositions all of which could be parallelized for additional computational efficiency it is also important to note that the number of individuals per generation used 256 outstrips the 32 cores we used hence only 32 individuals could be evaluated in parallel at a time leading to wait periods that further eroded performance therefore we anticipate that further reductions in simulation times may be achieved by allocating more cores and nodes through the use of both shared memory and distributed memory parallel computing architectures however developing components to handle the complexities of message passing over distributed memory architectures is not trivial and requires more programming expertise and development time for the components the benefit of being able to use more computing cores and nodes is that more individuals and evaluations over more generations can be performed leading to better solutions and an ability to solve more complex problems for our application we carefully selected time step settings and model parameters to ensure that there were no failures in our components however in practice components may fail due to any number of reasons e g wrong inputs model non convergence model instability etc failures must therefore be handled carefully in a component and communicated with other components in an appropriate manner the same cloning mechanism and parallel model execution pattern employed in this study can be adopted in other component based modeling frameworks for simulations that are characterized by independently executing multiple copies of the same model with varied parameters and collating the results for evaluation the results from our modeling effort showed that the mass balance closure method implemented to estimate the unaccounted inflows worked well when comparing simulated versus observed discharge before and after the mass balance closure process at main street and mendon road fig 6 however the procedure led to discharge predictions that did not capture some of the sharp changes in flow that were seen in the observed discharge this is because the missing discharge was applied as distributed inflows and therefore smoothed over some of these discontinuities it is important to note here that the discharge measurements themselves have uncertainties since they were developed from rating curves and measured water stage data however these data have undergone detailed quality assurance and quality control procedures and many of the sharp changes in flow at main street can be explained by upstream water management the 43 feasible solutions obtained at the end of the final generation of the calibration process had temperature nse values between 0 75 and 0 92 for main street and 0 78 and 0 79 for mendon road fig 7 these results are comparable or better than results reported in other river temperature modeling studies for applications in rural to urban areas e g norton and bradford 2009 bandaragoda and neilson 2011 glose et al 2017 this is true despite the complexity of the river system simulated and the various assumptions made regarding the temperatures of the various missing inflows are considered the nse values for the feasible solutions at main street were better than those at mendon road because of the errors surrounding the assumptions made for the temperatures for the unaccounted flows and spring creek in the downstream section of the model domain regardless these results highlight the utility of the calibration approach implemented within the component based framework the temperatures associated with the feasible solutions obtained from the calibration simulation for main street and mendon road fig 8 illustrate that the model calibrated parameters worked well predicting peak temperatures in the validation period with root mean square errors of 0 44 0 61 c for main street and 0 64 0 70 c for mendon road additionally the ranges of the parameters for the feasible solutions obtained table 2 provided useful insights into the dynamics affecting water temperature in the section of the logan river that was simulated the relative standard deviation of the feasible solutions indicated that parameters in the upstream section of the model i e cz1 were generally more uncertain than parameters in the downstream section i e cz2 of the model this observation may be explained by the assumptions we made regarding inflows and diversions in tcz2 tcz3 and cz2 which resulted in a narrow range of parameter values that could correctly account for observed temperatures at our calibration location at mendon road in general there was more solar radiation influence in cz2 fig 9 which coupled with the decreased flows and shallower depths resulting from diversions gave rise to increased temperatures and therefore more evaporation fig 9 while the advective transport and thermal diffusivity coefficients were comparable for cz1 and cz2 they were slightly greater in cz2 leading to slightly higher sediment conduction and hyporheic exchange heat fluxes in cz2 fig 9 these results highlight the utility of a component based temperature model that can be adapted as additional heat fluxes are identified as important for capturing observed temperature dynamics in each system in other prior efforts e g king and neilson 2019 cardenas et al 2014 knowledge of the underlying code structure was necessary so that it could be modified to account for additional heat fluxes the flexibility afforded by the component based modeling paradigm allows for the application to diverse systems additionally by allowing users to select and only apply the most important components it results in efficient models that allow for many iterations during model calibration the calibrated temperature multiplication factors for the surrogate temperatures used for the unaccounted inflows computed from the mass balance closure process revealed additional insights for tcz1 the temperature multiplication factor was greater than unity with an average value of 1 22 this indicates that our assumption that the bulk of inflow entering that section is groundwater was likely incorrect unaccounted inflows in tcz1 are most likely from a combination of groundwater and warmer surface water sources including residential irrigation water routed through the city via street gutters the tcz2 temperature multiplication factor was unity indicating that our assumption that inflows in that section were from cooler groundwater was reasonable for tcz3 the temperature multiplier was also unity indicating that our use of the blacksmith fork as a surrogate for temperature of inflows was reasonable for the spring creek inflow the temperature multiplication factor was greater than unity indicating that either temperatures are warmer than the blacksmith fork i e the station used as surrogate temperature or the inflow volumes from spring creek are larger at its confluence with the logan river than the values applied from the monitoring station located further upstream 4 conclusions in this paper we have presented an approach for efficiently conducting model calibration uncertainty assessment sensitivity analysis and other types of simulations in this class within component based modeling frameworks we illustrated this approach through the calibration of a component based river temperature model developed for the logan river which is a complex system with many inflows and diversions many of which are unmonitored the approach we employed involves converting these algorithms in this class of simulation into components in their own right that can be coupled to other models in coupled model compositions since this class of simulations is characterized by executing many instances of the same model multiple times and varying their parameters to evaluate their effect in an independent fashion they are amenable to parallelization for computational efficiency to achieve this efficiency within component based models we employed the component cloning infrastructure provided by the hydrocouple framework this infrastructure allows for components to make independent copies of themselves on demand either locally or on a remote computer for parallel execution for the river temperature modeling application presented in this paper we developed a calibration component that employed the farmer worker parallel pattern to achieve efficiency this process involved cloning the entire composition as many times as needed at each generation of the calibration and farming the clones out to available computing cores for parallel execution the cloning of the coupled model composition involved the calibration component requesting all the individual components in the composition to make clones of themselves and establishing identical connections between a clone and its parent this pattern led to a 14 times reduction in simulation times when we moved from a 1 core processor to a 32 core processor while we only used the shared memory parallelism architecture for our application the cloning mechanism can be extended to distributed memory architectures by implementing the cloning function such that clones are created on remote machines for more efficiency a clone on a remote machine would be presented to components on the local machine using a proxy component whose job it would be to communicate results from its corresponding component on the remote machine the component cloning interfaces as well as the parallel execution pattern described in this paper can be adopted and implemented within other existing component based modeling frameworks to enable similar functionality for more efficient simulations the results from our calibration efforts were successful with nse values between 0 78 and 0 92 for temperature results at our calibration locations additionally the increased level of granularity enabled by the component based modeling framework allowed us to implement the various optimization penalty functions and a time series provider as components in their own right this provided flexibility and allowed us to better understand some river temperature dynamics and feedbacks with the hyporheic zones stormwater and groundwater that occur in the logan river for example the time series provider allowed us to apply an adjusted time series by calibrating scaling factors for observed inflows which allowed for the identification dominant sources of inflow to various sections of the logan river more broadly in addition to providing modelers with flexibility to represent different parts of the water system with scale relevant process formulations and allowing modelers to evaluate feedbacks between sub systems the approach we have demonstrated here extends these capabilities further for the component based modeling community instead of single deterministic simulations modelers can embark on multi scenario evaluations efficiently using different parameters and model structures to understand how they affect the magnitudes of feedbacks and system wide behavior in turn this can help in deriving solutions or adaptation strategies to the myriad of water resources related challenges we face this way solutions are not myopically developed to address problems from the perspective of a single domain acknowledgements this research was supported by national science foundation epscor grant iia 1208732 awarded to utah state university as part of the state of utah epscor research infrastructure improvement award and nsf ear 1343861 any opinions findings and conclusions or recommendations expressed are those of the author s and do not necessarily reflect the views of the national science foundation the authors would also like to extend their gratitude to hyrum tennant who was invaluable in the data collection effort used to support the work presented in this manuscript appendix a cshcomponent formulations fig a1 1d control volume element fig a1 the integral version of the heat advection and dispersion equation over a time step from t to Œ¥ t over computational element i e cvi is a1 œÅ w c w t t Œ¥ t c v t t d v d t œÅ w c w t t Œ¥ t c v v t x d v d t œÅ w c w t t Œ¥ t c v x d t x d v d t t t Œ¥ t c v j y d v d t t t Œ¥ t c v j e j c y d v d t t t Œ¥ t c v s d v d t where v is the volume of the cv m 3 t represents the current time step s and t Œ¥ t represents the next time step where we seek a solution using gauss s divergence theorem and expanding the terms yields a2 t t Œ¥ t œÅ w c w t v t d t t t Œ¥ t œÅ w c w k 1 n b v t a d t t t Œ¥ t œÅ w c w k 1 n b d t x a d t t t Œ¥ t j y v d t t t Œ¥ t j e j c y v d t t t Œ¥ t s v d t where n b represents the number of inlet and outlet boundaries for the cv k 1 n b v t a represents summation of the advective heat fluxes across the inlet and outlet boundaries of the cv k 1 n b d t x a represents the sum of the dispersive heat fluxes across the inlet and outlet boundaries of the cv and a is the cross sectional area m 2 of flow using an explicit time marching scheme for the cv depicted in fig a1 yields equation a3 which expands to equation a4 a3 œÅ w c w t i t Œ¥ t t i t Œ¥ t v i t i t v i t Œ¥ t v i t Œ¥ t œÅ w c w k 1 n b q t i t œÅ w c w k 1 n b d t x a i t j y v i t j e j c y v i t s v i t a4 œÅ w c w t i t Œ¥ t t i t Œ¥ t v i t i t v i t Œ¥ t v i t Œ¥ t œÅ w c w q t i 1 2 t œÅ w c w q t i 1 2 t a d v e c t i o n œÅ w c w d t x a i 1 2 t œÅ w c w d t x a i 1 2 t d i s p e r s i o n j y v i t s v i t e x t e r n a l h e a t s o u r c e s j e j c y v i t s e n s i b l e a n d l a t e n t h e a t where fluxes out of the cv take on positive values fluxes into the cv take on negative values values with the superscripts t and t Œ¥ t represent values at the current time step and next time step respectively values with the subscripts i i 1 2 and i 1 2 represent values at the current cv its left boundary and right boundary respectively Œ¥ t is the time step s and q is the flow for the cv m 3 s advection discretization several methods are available for discretizing the advection terms in equation a4 these include the upwind central and hybrid differencing methods additionally several total variation diminishing tvd harten 1983 schemes are also available for problems that have sharp discontinuities in their solutions an exhaustive treatment of tvd schemes is provided by versteeg and malalasekera 2007 and are not described here for the first order accurate upwind differencing scheme the assumptions made for inlet and outlet advective heat fluxes for boundaries of the control volume are prescribed as follows a5 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i 1 a6 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i for the second order accurate central differencing scheme the inlet and outlet advective heat fluxes at the boundaries of the control volume are interpolated using the inverse distance weighting idw interpolation scheme as shown in equations a7 and a8 a7 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i 1 x i 1 2 x i 1 t i x i x i 1 2 x i x i 1 a8 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i x i 1 2 x i t i 1 x i 1 x i 1 2 x i 1 x i while the upwind differencing scheme is stable it is only first order accurate which gives rise to false diffusion this contrasts with the central differencing scheme which although second order accurate does not possess the transportiveness property i e ability to account for flow direction as well as the upwind scheme especially for highly advective flows versteeg and malalasekera 2007 the hybrid differencing scheme proposed by spalding 1972 attempts to split these tradeoffs be assessing whether advection or dispersion is the dominant transport mechanism the hybrid differencing scheme proceeds by first estimating the peclet number p e at the face of the control volume of interest as follows a9 p e i 1 2 v i 1 2 d i 1 2 x i x i 1 the flux through that face of the control volume is then estimated as follows a10 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 f i 1 t i 1 1 1 f i 1 p e i 1 2 f i t i 1 1 f i p e i 1 2 f o r 2 p e i 1 2 2 a11 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i 1 for p e i 1 2 2 a12 œÅ w c w q t i 1 2 œÅ w c w q i 1 2 t i for p e i 1 2 2 where f i 1 and f i are inverse distance weighted interpolation factors for the current and left control volumes that surround the boundary under consideration respectively dispersion discretization the spatial gradients of temperature at inlet and outlet of the cv used for computing dispersion in equation a4 are discretized numerically as follows a13 t x i 1 2 t i t i 1 x i x i 1 a14 t x i 1 2 t i 1 t i x i 1 x i following the qual2k model pelletier and chapra 2008 the cshcomponent adopts the formulations by fischer 1979 to calculate longitudinal dispersion when it is not explicitly provided as follows a15 d i 0 11 v i 2 b i 2 y i u i where b i is the channel width m y i is the mean flow depth m and u i is shear velocity m s of the cv the shear velocity is calculated as a16 u i g y i s i where s i is the channel bottom slope the computed dispersion coefficient d i is compared with the numerical dispersion estimated using equation a17 a17 e i v i x i 1 x i 1 2 if the computed numerical dispersion is less than the computed dispersion in equation a17 d i e i is used as the dispersion coefficient used in equation a4 otherwise the dispersion coefficient is set to zero continuity and junctions following islam and chaudhry 1998 internal junctions where 3 or more elements meet are treated as internal boundary conditions where temperatures are estimated using the simple mass balance equation equation a18 this equation assumes complete mixing at the junction a18 t j i 1 m q i t i i 1 m q i where t j is the temperature at the junction of interest c kg m 3 m is the number of elements with flows entering the junction q i is the flow from incoming element i kg m 3 and t i is the temperature constituent concentration in the incoming element i c kg m 3 evaporation and condensation evaporation condensation is a function of the sensible heat carried with the evaporated water the latent heat of evaporation density of water and the evaporative rate as expressed in equation a19 webb and zhang 1997 evans et al 1998 boyd and kasper 2003 a19 j e œÅ w l e e where l e is the latent heat of vaporization j kg and e is the evaporative rate m s the latent heat of vaporization is estimated as a weak function of water temperature using equation a20 martin and mccutcheon 1998 a20 l e 1000 2499 2 36 t where t is the water temperature in the channel c several approaches are available for estimating the evaporative rate including mass transfer methods explicit energy balance methods and combination methods that combine both mass and energy balance methods in the cshcomponent a mass balance method was implemented following dingman 2008 the evaporative rate is estimated using equation a21 a21 e f w e s w e a where e s w is the saturation vapor pressure of the evaporating surface kpa e a is the actual vapor pressure kpa and f w is a wind function used to estimate the adiabatic portion of evaporation boyd and kasper 2003 e s w is computed using equation a22 raudkivi 1979 chapra 2008 a22 e s w 0 61275 e 17 27 t 237 3 t where t is the temperature of the water in the channel the actual vapor pressure e a is calculated as a function of relative humidity h and saturation vapor pressure e s using equation a23 a23 e a h 100 e s where e s is computed using equation a24 a24 e s 0 61275 e 17 27 t a 237 3 t a where t a is air temperature in c extensive observations have yielded equation a25 as the general form of the wind function shanahan et al 1984 a25 f w a b w where a and b are empirical coefficients with units kpa 1 ms 1 and kpa 1 respectively and w is the wind speed measured 2 m above the water surface m s several authors have proposed values for these coefficients including dunne and leopold 1978 who proposed the values 1 505 10 8 and 1 6 10 8 for the coefficients a and b respectively these values are used as the defaults in the cshcomponent but can be overridden by user specified coefficients convection and conduction estimating sensible heat lost or gained through conduction convection with air in the atmosphere is typically performed using the bowen ratio b r which relates latent heat to sensible heat equation a26 bowen 1926 webb and zhang 1997 evans et al 1998 westhoff et al 2007 glose et al 2017 a26 b r j c j e martin and mccutcheon 1998 prescribed equation a27 for estimating the bowen ratio a27 b r j c j e c b p a p t t a e s w e a where c b is a bowen s coefficient usually equal to 0 0651 kpa c p a is atmospheric pressure kpa and p is a reference pressure at sea level kpa while the ratio p a p is often assumed to be unity the pressure difference cannot be neglected in higher elevations martin and mccutcheon 1998 cshcomponent solvers the cshcomponent can solve equation a4 using several ordinary differential equation ode solvers provided including the classical fourth order runge kutta method i e rk4 or the adaptive step size controlled fifth order runge kutta cash carp method cash and karp 1990 alternatively users can select variable multistep methods including the adams moulton formulas or the backward differentiation formulas i e bdf that are provided through the cvode hindmarsh et al 2017 external ode solver library readers are referred to the cshcomponent code in the github repository https github com hydrocouple cshcomponent for implementation details appendix b rhecomponent formulations the emissivity of the atmosphere Œµ a t m used in computing the atmospheric long wave radiation is computed from brunt 1932 b1 Œµ a t m a a a b e a where a a is an empirical coefficient with typical values between 0 5 and 0 7 a b is an empirical coefficient with a typical value of 0 0027 and e a is the vapor pressure of air p a the actual vapor pressure of air e a that is used to compute atmospheric emissivity Œµ a t m used in estimating atmospheric longwave radiation j a n is computed using equation b2 raudkivi 1979 chapra et al 2008 b2 e a h 100 e s where h is the relative humidity and e s is the saturation vapor pressure pa computed using equation b3 b3 e s 0 61275 e 17 27 t a 237 3 t a where t a is air temperature in c 
26227,hydro flattening is an operation required to generate deliverable terrain products after lidar survey collections that usually entails extensive manual intervention in this paper we develop new modules of geonet a computational tool for the automatic extraction of geomorphic channel features from high resolution topography for detecting channel banks and produce a hydro flattened digital terrain model dtm we first review the original geonet workflow and then describe how it has been modified to extract channels as the least cost paths from given channel heads to outlets a new code component enables hydro flattening on a given reference network based on curvature and connectivity a raster based routine for extracting the geomorphic channel zone based on a statistical analysis of slope has also been added we test our new components using three different test cases compared to manually delineated hydro flattened zones and satellite imagery our results show high consistency and the capability of our method to automatically hydro flatten high resolution topographic data keywords lidar high resolution topography hydro flattening river bank hydrological terrain analysis software name geonet developer geonet team contact paola austin utexas edu software required the python version of geonet requires grass gis and python libraries numpy scikitfmm and scipy availability https github com passah2o geonet 1 introduction over the last two decades the availability of high resolution terrain data obtained using remote sensing techniques has provided an unprecedented opportunity for better charaterizing earth surface features and understanding processes tarolli 2014 passalacqua et al 2015 among these data acquisition platforms airborne lidar light detection and ranging has been a game changer tarolli and dalla fontana 2009 roering et al 2013 both the raw data and information derived from point clouds have been widely used in geomorphology hilldale and raff 2008 notebaert et al 2009 hohenthal et al 2011 hydrology lyon et al 2015 flood risk evaluation casas et al 2006 bates 2012 chen et al 2017 especially inundation mapping in urban environments neal et al 2018 noh et al 2018 wang et al 2018 and natural resources management hudak et al 2009 in all these applications the extraction of land surface features such as coastlines stockdonf et al 2002 liu et al 2007 chust et al 2008 roads ferraz et al 2016 terrain depressions doctor and young 2013 waterbodies toscano 2015 and river networks passalacqua et al 2010 pelletier 2013 sangireddy et al 2016 has been challenging among features of interest on the earth surface the boundary between a river and its floodplain is critical for improving the performance of channel routing schemes and inundation mapping as distinct geometry and roughness characterize these two portions of a landscape being able to distinguish these two features has become even more important with the implementation of continental scale hydrologic simulations maidment 2017 a task similar to the bankline extraction and commonly conducted in cartography is hydro flattening hydro flattening is the process to generate a lidar derived dem where water surfaces appear and behave as those in traditional topographic dems generated with photogrammetry heidemann 2012 traditionally topographic dems are generated from mass points and breaklines collectively referred to as a dtm compiled through photogrammetric compilation heidemann 2012 photogrammetric dtms inherently contain breaklines that clearly define the edges of water surface which force flat and continuous water surfaces in the derived dems heidemann 2012 a triangulated irregular network tin surface is generated as an intermediate product from the dtm to the dem heidemann 2012 since the primary source of data for dems has been shifted to lidar which only records point clouds the lack of breaklines in the intermediate tin results in triangulation across waterbodies which refers to irregular unnatural and visually unappealing triangulation artifacts across the water surface heidemann 2012 therefore the usgs national geospatial program ngp requires an hydro flattening process to be conducted on the lidar derived dems before they are integrated into the standard national dem heidemann 2012 breaklines delineated with other techniques are used to modify a dem previously generated without them heidemann 2012 the hydro flattening process generates a bank to bank flat and level water surface and a gradient downhill centerline heidemann 2012 the centerline gradient can be either smooth or stair stepped heidemann 2012 due to the uncertainty in water surface elevation measurement using topographic lidar the usgs ngp does not require absolute accuracy of water surface elevations in lidar and dem deliveries heidemann 2012 the difference between cartographic breakline delineation and geomorphic bankline is that a breakline represents the instantaneous water land boundary during the survey and varies with flow conditions fig 1 while a bankline is overall static unless significant geomorphic change happens through time thus breaklines represent the land water boundary in regular flow conditions while banklines represent the land water boundary in flooding conditions in the past the delineation of banklines and breaklines has been conducted manually which was expensive time consuming and labor intensive automatic approaches have been developed and tested some of them rely on the raw point cloud for classification while others extract information from the elevation signal intensity and point density raster h√∂fle et al 2009 for example proposed a method that first applies an intensity correction and a dropout signal absent zone simulation and then uses an object based classification to identify the land water surface boundary from point cloud derived segments another study deshpande and yilmaz 2017 used the nhd flowline as reference to trace cross sections perpendicular to the reference line and identify the lowest elevation point along the cross section as water surface elevation then a constant level was added to that elevation to generate a virtual water surface and the intersection between this virtual surface and the raw surface identified the bank location the main limitation of this method is that it is semi automated as it requires the users inspection and experience in deciding the parameter values other methods extract breaklines by combining elevation and intensity data toscano et al 2013 2014 acharjee et al 2017 or point density information johansen et al 2011 2013 smeeckaert et al 2013 worstell et al 2014 using raw point clouds has the advantage of preserving information without smoothing the signal a challenge associated with these operations is that the size of the raw data is much greater than the processed raster products requiring more computational resources significant variation in return intensity can be present within the waterbodies acharjee et al 2017 chen et al 2017 making the low intensity assumption used in water classification problematic furthermore density based approaches may misclassify roads as water as the density difference between land and water may not be detectable if the water depth in a channel is shallow or the channel is located in survey strip overlapping zones here we propose a method to hydro flatten high resolution topographic data by using the elevation raster which is the most commonly available lidar data product with a reference channel network to define the expected network density and nodes our approach uses only few adjustable parameters whose values are applicable to a variety of areas the proposed method builds upon geonet passalacqua et al 2010 sangireddy et al 2016 an open source automatic tool for geomorphic feature extraction from high resolution topographic data in geonet a nonlinear filter is first applied to the elevation data to remove local terrain variability and enhance features of interest terrain attributes including curvature and flow accumulation are computed from the filtered dem based on a properly defined cost function channel centerlines are extracted as lines of minimum cost from each channel head to the watershed outlet the first version of geonet only extracted channel heads and centerlines passalacqua et al 2010 later on we added tools for extracting channel cross sections bank locations and bankfull water surface elevation passalacqua et al 2012 recently the channel head and centerline extraction portion has been rewritten in python from the original matlab and c version sangireddy et al 2016 however the cross section and bank extraction portion has not been added and its capability of detecting geomorphic banklines has not been tested the goal of this paper is to propose an improved solution for extracting geomorphic banks over large scale river networks and estimating reach average bankfull width using geonet we first describe the three study areas used to test our tool section 2 we then review the original geonet workflow and discuss its limitations in delineating channel banklines section 3 1 next we present our proposed approach to solve these limitations section 3 2 and implement it in our study sites we compare the results to observations to demonstrate the effectiveness of our method section 4 and identify its limitations section 5 finally we summarize the conclusions of this work section 6 2 study sites 2 1 natural basin east branch sturgeon river subwatershed mi the east branch sturgeon river subwatershed fig 2 is located in dickinson county michigan it has a total drainage area of 55 32 km2 and the total length of the medium resolution nhd river network mckay et al 2012 within it is 9 84 km the maximum annual mean flow of a given stream segment in this basin is less than 0 6 m3 s a 1 m resolution lidar derived bare earth dem was provided by the national geospatial center of excellence ngce of usda natural resources conservation service nrcs 2 2 agricultural basin stony creek black river subwatershed ny the stony creek black river subwatershed fig 3 is located in lewis county new york it has a total drainage area of 71 25 km2 and the total length of the nhd river network within it is 52 3 km the main stem river passing through this subwatershed is the black river which has a mean annual flow of about 100 m3 s as commonly found in agricultural landscapes in this watershed the flow is also conveyed by artificial drainage ditches testing our workflow in this watershed can demonstrate its efficiency when dealing with the presence of ditches and the related challenges for classic feature extraction algorithms a 1 m resolution lidar derived bare earth dem was provided by ngce with hydro flattening breaklines along the main stem river which will be used as reference to evaluate the performance of our method 2 3 urban landscape jersey lake whiteoak bayou subwatershed tx the jersey lake whiteoak bayou subwatershed is located in harris county texas as part of the greater metropolitan area of the city of houston it is a highly urbanized area with many artificial structures that act as barriers during the river network extraction fig 4 testing our workflow in this watershed can demonstrate its efficiency in challenging urban environments the total drainage area of this subwatershed is 59 68 km2 and the total length of the nhd river network within it is 22 2 km a lidar derived dem was provided by the texas natural resources information system tnris at a resolution of 1 5 m 5 ft tnris also provided manually delineated hydro flattening breaklines which will be used as reference to evaluate the performance of our method 3 methods 3 1 review of the original geonet workflow the original geonet workflow comprises three major parts nonlinear filtering of the elevation data identification of the skeleton the set of likely channelized pixels based on curvature and accumulation area computed on the filtered dem and a least cost path approach for channel network identification filtering is a common operation adopted in feature extraction from lidar data to remove local terrain variability the most common filter is the gaussian filter burt and adelson 1987 koenderink 1984 lashermes et al 2007 which can be expressed as 1 h x y t h 0 x y g x y t where denotes the convolution operation h0 x y represents the raw elevation at location x y g x y t is a two dimensional gaussian kernel with a standard deviation t centered at x y and h x y t represents the filtered elevation the value of t specifies the size of the local neighborhood involved in the filtering operation the main problem associated with this filter is the blurring of the feature edges which limits the ability to detect the exact location of the features of interest lashermes et al 2007 passalacqua et al 2010 therefore geonet implements a nonlinear filter perona and malik 1990 defined as 2 h x y t t c x y t h where c is the diffusion coefficient this coefficient can be computed as 3 c e h Œª 2 or 4 c 1 1 h Œª 2 in the equations above h is the absolute value of the elevation gradient at location x y t is the iteration time step and Œª is the edge stopping threshold computed as the 90th quantile of the gradient distribution perona and malik 1990 since gradients are high across feature boundaries the diffusion coefficient promotes diffusion within the feature boundaries and penalizes it across the boundaries so that the exact boundary location is preserved the difference between these two coefficient computation methods is that the first one favors high contrast over low contrast edges while the second favors large scale over small scale contrasts perona and malik 1990 therefore equation 3 is set as the default option in the current geonet program while the option for using equation 4 is also provided sangireddy et al 2016 once the filtering operation is completed curvature is computed on the filtered dem to help identify likely channelized pixels skeleton two kinds of curvature have been adopted in geonet the first one is the laplacian curvature which is defined as the gradient of the elevation gradient h 5 Œ∫ 2 h the second one is the geometric curvature which is defined as the gradient of the elevation gradient normalized by its magnitude 6 Œ∫ h h the geometric curvature works better for preserving all convergent features and thus it is predominantly used in natural landscapes while the laplacian curvature tends to favor the most convergent features and ignore those with smaller curvature passalacqua et al 2012 in the current version of geonet geometric curvature is chosen as the default option to use however when the study area is flat and human impacted the user should consider switching to laplacian for better performance passalacqua et al 2012 this curvature information is used for the extraction of the skeleton by identifying all the pixels with a curvature value larger than a threshold detected from a quantile quantile plot of curvature the value of curvature at which the curvature distribution deviates from a straight line in the positive tail marks the transition from hillslopes to valleys lashermes et al 2007 the skeleton of likely channelized pixels based on the curvature threshold can be noisy as small convergent areas that are not part of the channel network may be identified therefore a thinning operation is performed to thin the skeleton based on flow accumulation area passalacqua et al 2010 only pixels that pass both the curvature threshold and the accumulation threshold are retained in the final skeleton output the flow accumulation information is computed with the r watershed function from grass gis sangireddy et al 2016 once the skeleton is identified the channel heads and channel network are extracted based on geodesic minimization principles a geodesic cost function is first computed for each pixel to measure the degree of difficulty for water to pass through it this cost function is based on topographic attributes i e contributing area a curvature Œ∫ and skeleton s and expressed as 7 œà 1 1 a a m e a n s a m e a n Œ∫ where a m e a n is the mean flow accumulation area computed across the entire watershed which is used to normalize the difference in dimension and order of magnitude of the different terms the geodesic distance of each pixel from the output is then computed using the fast marching algorithm sethian 1996 as the minimum cost to travel from that pixel to the watershed outlet following the calculation of geodesic distance channel heads are identified automatically as the upstream end pixel with the largest geodesic distance of each connected skeleton component scanned using a search box river centerlines are then extracted as the least cost path from each channel head to the watershed outlet after the extraction of the river network the original geonet workflow allows the extraction of cross sections along the centerlines and the identification of bank locations at each cross section passalacqua et al 2012 for a given location x y the coordinates of stream cells along the same path five units away from it both upstream x u y u and downstream x d y d are used to estimate the streamline direction vector r as 8 r x d x u i y d y u j then the orthogonal vector c representing the transverse direction is computed as 9 c r 2 i r 1 j in this way a cross section with a predefined length l is drawn centered at x y following the direction of c to identify the channel bank locations the slope is computed along the transect and on each side of the centerline the location corresponding to the peak of the slope is marked as the bank location therefore the original geonet workflow can be used to identify banklines by connecting the bank points on adjacent cross sections from upstream to downstream along each centerline of the network this approach has some limitations as explained in the next section 3 2 new approach for hydro flattening and bank detection 3 2 1 junction detection and network segmentation the original geonet workflow extracts each channel separately from a channel head to the outlet resulting in overlapping centerlines on the main stem river fig 5 a we have added a post processing operation to properly segment the network at river confluences in this process after all the flowlines have been extracted the number of flowlines passing through each grid cell is counted flow network segments are constructed from sequences of grid cells that have the same count value when a change in pass count happens the original flowline is interrupted and the starting point of a segment with a larger pass count is identified as a junction unless it coincides with the watershed outlet fig 5b 3 2 2 network extraction workflow adjustment and cost function modification since our goal is to estimate the bankfull width and conduct the hydro flattening operation across a large domain we are only interested in perennial streams often referred to as blue line streams in topographic maps however the network density automatically detected from the lidar derived topography is usually too dense for general applications therefore instead of using the channel heads automatically detected by geonet we extract channel heads from the national hydrography dataset plus medium resolution nhdplus mr http www horizon systems com nhdplus nhdplusv2 home php mckay et al 2012 which is designed to be used for general applications to ensure a proper scale of the output network poppenga et al 2013 the nhdplus mr is a digital database that contains a national coverage stream network derived from the 30 m resolution national elevation dataset ned using a classic slope based approach with extensive manual corrections due to the coarse spatial resolution of the input elevation data and the extraction method the centerlines in the nhdplus network appear to deviate from the terrain concavities once overlaid on lidar data the channel heads extracted from the nhdplus mr are used as the starting point of the optimal path searching calculation in geonet the corresponding end point of each path could be the end point of the original flowline feature or the unique outlet of the entire watershed either alternative has its pros and cons if the end point of the original flowline is used the junction locations identified in the initial network are inherited thus the delineation of each flowline can be performed within a local drainage catchment independently leading to an easy parallel implementation also since the extraction is bounded by the local catchment boundary shortcuts of least cost paths across catchment boundaries are not possible using the outlet of the entire watershed as the end point of all paths instead allows geonet to automatically extract the network junctions however since in this case paths are searched within a large domain shortcuts across ridges may appear specific examples will be shown in the results section since the channel heads are predefined in our workflow we do not need to compute the geodesic distance at every pixel reducing computational expenses instead we only need to find the least cost path between each start end point pair by calling the route through array function from the python scikit image library with the local cost field defined later as the input in flat areas or areas with artificial structures elevation gradient based flow accumulation often results in inaccurate flow paths fig 6 c due to the significant elevation difference between the stream bed and the in channel structures captured during the survey passalacqua et al 2012 thus affecting the computation of the cost function taking a 104 104 pixels study domain as an example the order of magnitude for the accumulation term varies from 100 to 108 while for the curvature and skeleton the maximum order of magnitude is usually 100 the magnitude of the mean flow accumulation area is about 104 therefore even if a mean is used as a normalization parameter in the expression of the cost function equation 7 the curvature fig 6b and skeleton terms fig 6d are still small compared to the accumulation value of a downstream stream cell which may range from 106 to 108 thus flow accumulation tends to dominate the other two terms to address this issue we compute the local geodesic cost with a normalized flow accumulation term the normalization is implemented by first converting the accumulation values to log scale and then downscaling the log accumulation values from their original range to a range from 0 to 1 to keep the change consistent across different factors we also perform a normalization of the curvature term additionally we explored the introduction of new terms in the cost function based on our recently proposed channel extraction approach zheng et al 2018 in this method information from the nhdplus mr network is used as a prior sample space for the extraction of the river network from lidar data the assumption of this approach is that the reference river network only gets adjusted if nearby cells are lower or equal in elevation using this idea we add another binary term h to the cost function to represent the height of a pixel relative to its nearest original channel pixel the value of 1 is assigned to the pixels with a negative or zero h likely channel pixels and 0 to those with a positive h unlikely channel pixels fig 6e the value of h is obtained by identifying the nearest original channel pixel location for each pixel based on euclidean distance and then computing h as the pixel elevation minus the nearest stream cell elevation thus the modified cost function used in this study is given by 10 œà 1 a n Œ∫ n s h where the n subscript indicates that both the accumulation term a and the curvature term Œ∫ have been normalized in this formulation all the terms have the same weight since all of them take values in a range from 0 to 1 the complete flowchart of the modified river network extraction workflow is in fig 7 3 2 3 raster based hydro flattened zone extraction we propose a raster based method to hydro flatten high resolution topographic data based on curvature and network connectivity fig 8 a the theoretical foundation of this method is that the channel is a convergent zone of the terrain and thus has positive curvature while the banks represent the transition from the channel to the floodplain and have negative curvature therefore by setting a threshold on curvature equal to zero the channel pixels and any other pixels with zero curvature are above this threshold allowing us to also detect waterbodies in the watershed selecting pixels based on a curvature threshold may result in capturing also convergent features that do not connect to the channel network we solve this problem by checking the connectivity between the filtered pixels and the network centerlines previously extracted after identifying all the pixels with a non negative curvature we use this filtered raster to mask the flow directions derived from the flow accumulation calculation thus keeping only those pixels for which a continuous path can be traced to any centerline pixel following flow directions the distance from each connected pixel to the extracted centerline is computed and stored in the output raster small tributaries or ditches draining to the main river may be captured with this approach fig 8 however since their centerlines are not included as a part of our network these pixels have a relatively long distance to the centerline therefore we take advantage of this property to exclude these pixels from the final output by imposing a threshold to the cumulative distribution function of the distance to centerline and deleting all the pixels with a distance above this threshold we recommend using the 80th percentile as threshold which has been tested in multiple sites presented in this study finally the raster composed of all the identified pixels is converted to a hydro flattened polygon the complete flowchart of the hydro flattened zone extraction workflow is in fig 9 once the river network and the hydro flattened zone are extracted their elevations are modified according to the usgs lidar guidelines and base specifications heidemann 2012 in our workflow a constant slope is estimated by fitting a linear regression to the elevation of all the pixels within the extracted hydro flattened zone this slope is applied to obtain the centerline profile fig 10 b then the centerline elevation is extended to both sides along the transect direction to obtain a flattened water surface fig 10 c since cross sections along the extracted network at a constant interval can also be generated with geonet the cross section based breakline delineation approach is also supported by the current version of the workflow the comparison of the results generated with these two approaches is shown as fig 11 the main limitation of the cross section approach is that when the river width varies dramatically the constant length cross section may be too short to cover the entire water surface transect 3 2 4 raster based channel zone extraction and bankfull width estimation a challenge associated with the transect based approach used in the original geonet workflow for bank detection is that different transect lengths may need to be defined for different order streams therefore here we propose a raster based method for solving the bank extraction problem fig 12 to extract the geomorphic channel zone we rely on the slope information similarly to the original geonet bank detection routine it is assumed in the original routine that the bank points along a cross section should be the locations with the maximum slope on both sides of the detected centerline we introduce a filter based on the statistical analysis of the slope distribution all the pixels with a slope on the right tail of the slope cumulative distribution function are preserved a threshold equal to the 90th percentile is recommended to define the tail this threshold value has been tested in our case studies with different topographic settings since bank locations correspond to the maximum peaks of the slope along the transect these bankline pixels are preserved in the filtering operation the only part within the channel that is not in this zone is the convergent section at the center where the slope is close or equal to zero therefore we compensate the missing central section using the hydro flattened zone we identified in the previous step so that the entire channel zone is selected during the bank detection process finally the raster with all the selected cells is converted into a polygon and only polygons intersecting the centerlines are kept while the others are deleted the bankfull width of each river is computed as the channel zone area divided by the length of the centerline 4 results 4 1 natural basin first the river centerline network is extracted using the method described in section 3 2 2 the comparison fig 13 between our extracted river network and the original nhd flowline shows that our results better align with the high resolution terrain data fig 13a and satellite imagery fig 13b the hydro flattened zone is then extracted based on the curvature and connectivity analysis a problem was found at waterbodies when the distance based filtering was applied to extract the final hydro flattened zone since the width of the waterbodies is usually large fig 14 compared to the channel width part of the waterbodies was filtered out which resulted in an underestimation of the waterbody coverage this underestimation can be accounted for by masking the initial output with the nhd waterbody features before the clipping and then applying the filter only to the unmasked zone see fig 15 because the channels in this watershed are all small head water streams we found that the hydro flattened zone mostly coincides with the geomorphic channel zone therefore we converted the extracted hydro flattened zone into polygons and computed the corresponding bankfull width for each river fig 16 4 2 agricultural basin the agricultural basin test shows similar improvements in the extracted river network fig 17 since there are hundred meter wide main stem rivers and several meter wide artificial canals we implemented the clipping process by catchment with the 80th percentile threshold identified from the distance statistical distribution of each local catchment fig 18 in this basin the hydro flattened zone also coincides with the geomorphic channel zone therefore we computed the reach average bankfull width based on the extracted hydro flattened zone fig 19 4 3 urban basin since this is an urbanized flat area we adopted the laplacian curvature to favor the extraction of natural channels passalacqua et al 2012 the results show the improved accuracy of the extracted network fig 20 we identified a problematic area in the upper left corner of the watershed where a underground ditch served as the main drainage waterway but was not detectable in the terrain input fig 21 therefore our tool returned a path following the surface land gradient from the given channel head to the main stem river the hydro flattened zone extracted automatically with our proposed method is able to capture the majority of the manually delineated extent fig 22 to quantitatively measure the accuracy of the hydro flattened zones extracted using geonet we compute two performance metrics based on its comparison with the manually delineated hydro flattened zones the first one is the f index which is the ratio between the intersect area and the union area of the extracted zone and the reference zone it is a comprehensive assessment that reflects the overestimation and underestimation at the same time 11 f a r e a g e o n e t a r e a m a n u a l a r e a g e o n e t a r e a m a n u a l the second one is the c index which is the intersect area divided by the area of the reference zone 12 c a r e a g e o n e t a r e a m a n u a l a r e a m a n u a l c identifies the percentage of the manually delineated area covered by our automatic extraction to compute these two indices both the hydro flattened zones and the reference breaklines have been resampled as rasters at a resolution of 0 3048 m 1 ft with the same extent the comparison table 1 and fig 23 shows that the automatic hydro flattening strategy is able to correctly cover over 90 percent of the hydro flattened zone identified with manually delineated breaklines some overestimation is detected on the edge of the hydro flattened zone but it usually only extends over a single pixel band differently from the two previous areas here water only occupies a small portion of the channel cross section therefore we used the slope information as described in section 3 2 4 to obtain the final geomorphic channel zone fig 24 5 discussion to reduce the density of the extracted river network and required computational resources our approach operates only within the buffer zone of an existing river network therefore the quality of the existing river network significantly affects the accuracy of the final products in the current study we use the nhdplus mr bluelines for regions where better information is available such as the nhdplus high resolution network the user is recommended to use these datasets to define prior probabilities also while the river network datasets used here are only available in the u s our approach can be applied in other areas where an equivalent cartographic river network is available channel extraction with a modified cost function was successful in different topographic settings we identified inaccuracies where the channel is meandering due to the global cost being computed over a long distance to improve the results the user can either tune the weight parameters in the cost function or extract features in smaller sub areas and then combine the extraction results another factor that significantly affects the quality of the result is the input terrain data in natural environments if the river is flowing under the canopies the channel may be difficult to capture from the terrain data similar problems exist with underground structures in urban environments not all the man made features could be correctly extracted with a terrain analysis based method such as the culvert beneath the road even though the drainage network in the presence of these features is well outlined the surface landscape beyond these structures could be arbitrary and does not show any representative characteristics to indicate the existence of such drainage paths since our method is based on the analyses of the surface landscape we could not handle the network with such underground structures quite well however if the locations of these low water crossings are marked our approach is able to identify the surface drainage paths between different markers which could be a valuable direction for future research recent developments in parallel computation algorithms for hydrological terrain analysis fan et al 2014 yƒ±ldƒ±rƒ±m et al 2015 survila et al 2016 have not been integrated in grass functions therefore future work will include the development of efficient routines to be integrated into our workflow 6 conclusions in this paper we presented modifications and new modules of geonet a tool designed for automatic feature extraction from high resolution topographic data the new modules enable the automatic hydro flattening on the elevation data the form of the geodesic cost function used for river network extraction was modified to include information of a given reference river network and to correct the potential errors in the accumulation area term once the river centerline network is extracted curvature and connectivity to centerline information are used to identify the convergent sections of the terrain a threshold obtained from statistical analysis of horizontal distance to stream is applied to filter unwanted components combining the hydro flattened output with slope information we obtained the geomorphic channel zone of a river network and computed the reach average bankfull width for every river segment in the network providing critical information to improve the performance of large scale hydrological simulation and inundation mapping we tested our proposed workflow at three sites with natural agricultural and urban terrain settings compared to satellite imagery and approved products our results demonstrate competitive accuracy showing that our method can automatically perform hydro flattening and bank detection identification the comparison versus manually delineated breaklines shows that although our method tends to add an additional single pixel band on the edge of the actual extent which leads to some overestimation 59 in the test case the extracted area covers the majority of the extent 91 in the test case that used to be decided manually the results also show that at some sites the hydro flattened zone can be directly used for the bankfull width calculation without using slope information acknowledgements this research has been supported by national geospatial center of excellence ngce under grant number 68 7482 16 570 thanks to ngce and tnris for providing the lidar data set the code is stored in a github repository and downloadable from https github com passah2o geonet appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 02 007 
26227,hydro flattening is an operation required to generate deliverable terrain products after lidar survey collections that usually entails extensive manual intervention in this paper we develop new modules of geonet a computational tool for the automatic extraction of geomorphic channel features from high resolution topography for detecting channel banks and produce a hydro flattened digital terrain model dtm we first review the original geonet workflow and then describe how it has been modified to extract channels as the least cost paths from given channel heads to outlets a new code component enables hydro flattening on a given reference network based on curvature and connectivity a raster based routine for extracting the geomorphic channel zone based on a statistical analysis of slope has also been added we test our new components using three different test cases compared to manually delineated hydro flattened zones and satellite imagery our results show high consistency and the capability of our method to automatically hydro flatten high resolution topographic data keywords lidar high resolution topography hydro flattening river bank hydrological terrain analysis software name geonet developer geonet team contact paola austin utexas edu software required the python version of geonet requires grass gis and python libraries numpy scikitfmm and scipy availability https github com passah2o geonet 1 introduction over the last two decades the availability of high resolution terrain data obtained using remote sensing techniques has provided an unprecedented opportunity for better charaterizing earth surface features and understanding processes tarolli 2014 passalacqua et al 2015 among these data acquisition platforms airborne lidar light detection and ranging has been a game changer tarolli and dalla fontana 2009 roering et al 2013 both the raw data and information derived from point clouds have been widely used in geomorphology hilldale and raff 2008 notebaert et al 2009 hohenthal et al 2011 hydrology lyon et al 2015 flood risk evaluation casas et al 2006 bates 2012 chen et al 2017 especially inundation mapping in urban environments neal et al 2018 noh et al 2018 wang et al 2018 and natural resources management hudak et al 2009 in all these applications the extraction of land surface features such as coastlines stockdonf et al 2002 liu et al 2007 chust et al 2008 roads ferraz et al 2016 terrain depressions doctor and young 2013 waterbodies toscano 2015 and river networks passalacqua et al 2010 pelletier 2013 sangireddy et al 2016 has been challenging among features of interest on the earth surface the boundary between a river and its floodplain is critical for improving the performance of channel routing schemes and inundation mapping as distinct geometry and roughness characterize these two portions of a landscape being able to distinguish these two features has become even more important with the implementation of continental scale hydrologic simulations maidment 2017 a task similar to the bankline extraction and commonly conducted in cartography is hydro flattening hydro flattening is the process to generate a lidar derived dem where water surfaces appear and behave as those in traditional topographic dems generated with photogrammetry heidemann 2012 traditionally topographic dems are generated from mass points and breaklines collectively referred to as a dtm compiled through photogrammetric compilation heidemann 2012 photogrammetric dtms inherently contain breaklines that clearly define the edges of water surface which force flat and continuous water surfaces in the derived dems heidemann 2012 a triangulated irregular network tin surface is generated as an intermediate product from the dtm to the dem heidemann 2012 since the primary source of data for dems has been shifted to lidar which only records point clouds the lack of breaklines in the intermediate tin results in triangulation across waterbodies which refers to irregular unnatural and visually unappealing triangulation artifacts across the water surface heidemann 2012 therefore the usgs national geospatial program ngp requires an hydro flattening process to be conducted on the lidar derived dems before they are integrated into the standard national dem heidemann 2012 breaklines delineated with other techniques are used to modify a dem previously generated without them heidemann 2012 the hydro flattening process generates a bank to bank flat and level water surface and a gradient downhill centerline heidemann 2012 the centerline gradient can be either smooth or stair stepped heidemann 2012 due to the uncertainty in water surface elevation measurement using topographic lidar the usgs ngp does not require absolute accuracy of water surface elevations in lidar and dem deliveries heidemann 2012 the difference between cartographic breakline delineation and geomorphic bankline is that a breakline represents the instantaneous water land boundary during the survey and varies with flow conditions fig 1 while a bankline is overall static unless significant geomorphic change happens through time thus breaklines represent the land water boundary in regular flow conditions while banklines represent the land water boundary in flooding conditions in the past the delineation of banklines and breaklines has been conducted manually which was expensive time consuming and labor intensive automatic approaches have been developed and tested some of them rely on the raw point cloud for classification while others extract information from the elevation signal intensity and point density raster h√∂fle et al 2009 for example proposed a method that first applies an intensity correction and a dropout signal absent zone simulation and then uses an object based classification to identify the land water surface boundary from point cloud derived segments another study deshpande and yilmaz 2017 used the nhd flowline as reference to trace cross sections perpendicular to the reference line and identify the lowest elevation point along the cross section as water surface elevation then a constant level was added to that elevation to generate a virtual water surface and the intersection between this virtual surface and the raw surface identified the bank location the main limitation of this method is that it is semi automated as it requires the users inspection and experience in deciding the parameter values other methods extract breaklines by combining elevation and intensity data toscano et al 2013 2014 acharjee et al 2017 or point density information johansen et al 2011 2013 smeeckaert et al 2013 worstell et al 2014 using raw point clouds has the advantage of preserving information without smoothing the signal a challenge associated with these operations is that the size of the raw data is much greater than the processed raster products requiring more computational resources significant variation in return intensity can be present within the waterbodies acharjee et al 2017 chen et al 2017 making the low intensity assumption used in water classification problematic furthermore density based approaches may misclassify roads as water as the density difference between land and water may not be detectable if the water depth in a channel is shallow or the channel is located in survey strip overlapping zones here we propose a method to hydro flatten high resolution topographic data by using the elevation raster which is the most commonly available lidar data product with a reference channel network to define the expected network density and nodes our approach uses only few adjustable parameters whose values are applicable to a variety of areas the proposed method builds upon geonet passalacqua et al 2010 sangireddy et al 2016 an open source automatic tool for geomorphic feature extraction from high resolution topographic data in geonet a nonlinear filter is first applied to the elevation data to remove local terrain variability and enhance features of interest terrain attributes including curvature and flow accumulation are computed from the filtered dem based on a properly defined cost function channel centerlines are extracted as lines of minimum cost from each channel head to the watershed outlet the first version of geonet only extracted channel heads and centerlines passalacqua et al 2010 later on we added tools for extracting channel cross sections bank locations and bankfull water surface elevation passalacqua et al 2012 recently the channel head and centerline extraction portion has been rewritten in python from the original matlab and c version sangireddy et al 2016 however the cross section and bank extraction portion has not been added and its capability of detecting geomorphic banklines has not been tested the goal of this paper is to propose an improved solution for extracting geomorphic banks over large scale river networks and estimating reach average bankfull width using geonet we first describe the three study areas used to test our tool section 2 we then review the original geonet workflow and discuss its limitations in delineating channel banklines section 3 1 next we present our proposed approach to solve these limitations section 3 2 and implement it in our study sites we compare the results to observations to demonstrate the effectiveness of our method section 4 and identify its limitations section 5 finally we summarize the conclusions of this work section 6 2 study sites 2 1 natural basin east branch sturgeon river subwatershed mi the east branch sturgeon river subwatershed fig 2 is located in dickinson county michigan it has a total drainage area of 55 32 km2 and the total length of the medium resolution nhd river network mckay et al 2012 within it is 9 84 km the maximum annual mean flow of a given stream segment in this basin is less than 0 6 m3 s a 1 m resolution lidar derived bare earth dem was provided by the national geospatial center of excellence ngce of usda natural resources conservation service nrcs 2 2 agricultural basin stony creek black river subwatershed ny the stony creek black river subwatershed fig 3 is located in lewis county new york it has a total drainage area of 71 25 km2 and the total length of the nhd river network within it is 52 3 km the main stem river passing through this subwatershed is the black river which has a mean annual flow of about 100 m3 s as commonly found in agricultural landscapes in this watershed the flow is also conveyed by artificial drainage ditches testing our workflow in this watershed can demonstrate its efficiency when dealing with the presence of ditches and the related challenges for classic feature extraction algorithms a 1 m resolution lidar derived bare earth dem was provided by ngce with hydro flattening breaklines along the main stem river which will be used as reference to evaluate the performance of our method 2 3 urban landscape jersey lake whiteoak bayou subwatershed tx the jersey lake whiteoak bayou subwatershed is located in harris county texas as part of the greater metropolitan area of the city of houston it is a highly urbanized area with many artificial structures that act as barriers during the river network extraction fig 4 testing our workflow in this watershed can demonstrate its efficiency in challenging urban environments the total drainage area of this subwatershed is 59 68 km2 and the total length of the nhd river network within it is 22 2 km a lidar derived dem was provided by the texas natural resources information system tnris at a resolution of 1 5 m 5 ft tnris also provided manually delineated hydro flattening breaklines which will be used as reference to evaluate the performance of our method 3 methods 3 1 review of the original geonet workflow the original geonet workflow comprises three major parts nonlinear filtering of the elevation data identification of the skeleton the set of likely channelized pixels based on curvature and accumulation area computed on the filtered dem and a least cost path approach for channel network identification filtering is a common operation adopted in feature extraction from lidar data to remove local terrain variability the most common filter is the gaussian filter burt and adelson 1987 koenderink 1984 lashermes et al 2007 which can be expressed as 1 h x y t h 0 x y g x y t where denotes the convolution operation h0 x y represents the raw elevation at location x y g x y t is a two dimensional gaussian kernel with a standard deviation t centered at x y and h x y t represents the filtered elevation the value of t specifies the size of the local neighborhood involved in the filtering operation the main problem associated with this filter is the blurring of the feature edges which limits the ability to detect the exact location of the features of interest lashermes et al 2007 passalacqua et al 2010 therefore geonet implements a nonlinear filter perona and malik 1990 defined as 2 h x y t t c x y t h where c is the diffusion coefficient this coefficient can be computed as 3 c e h Œª 2 or 4 c 1 1 h Œª 2 in the equations above h is the absolute value of the elevation gradient at location x y t is the iteration time step and Œª is the edge stopping threshold computed as the 90th quantile of the gradient distribution perona and malik 1990 since gradients are high across feature boundaries the diffusion coefficient promotes diffusion within the feature boundaries and penalizes it across the boundaries so that the exact boundary location is preserved the difference between these two coefficient computation methods is that the first one favors high contrast over low contrast edges while the second favors large scale over small scale contrasts perona and malik 1990 therefore equation 3 is set as the default option in the current geonet program while the option for using equation 4 is also provided sangireddy et al 2016 once the filtering operation is completed curvature is computed on the filtered dem to help identify likely channelized pixels skeleton two kinds of curvature have been adopted in geonet the first one is the laplacian curvature which is defined as the gradient of the elevation gradient h 5 Œ∫ 2 h the second one is the geometric curvature which is defined as the gradient of the elevation gradient normalized by its magnitude 6 Œ∫ h h the geometric curvature works better for preserving all convergent features and thus it is predominantly used in natural landscapes while the laplacian curvature tends to favor the most convergent features and ignore those with smaller curvature passalacqua et al 2012 in the current version of geonet geometric curvature is chosen as the default option to use however when the study area is flat and human impacted the user should consider switching to laplacian for better performance passalacqua et al 2012 this curvature information is used for the extraction of the skeleton by identifying all the pixels with a curvature value larger than a threshold detected from a quantile quantile plot of curvature the value of curvature at which the curvature distribution deviates from a straight line in the positive tail marks the transition from hillslopes to valleys lashermes et al 2007 the skeleton of likely channelized pixels based on the curvature threshold can be noisy as small convergent areas that are not part of the channel network may be identified therefore a thinning operation is performed to thin the skeleton based on flow accumulation area passalacqua et al 2010 only pixels that pass both the curvature threshold and the accumulation threshold are retained in the final skeleton output the flow accumulation information is computed with the r watershed function from grass gis sangireddy et al 2016 once the skeleton is identified the channel heads and channel network are extracted based on geodesic minimization principles a geodesic cost function is first computed for each pixel to measure the degree of difficulty for water to pass through it this cost function is based on topographic attributes i e contributing area a curvature Œ∫ and skeleton s and expressed as 7 œà 1 1 a a m e a n s a m e a n Œ∫ where a m e a n is the mean flow accumulation area computed across the entire watershed which is used to normalize the difference in dimension and order of magnitude of the different terms the geodesic distance of each pixel from the output is then computed using the fast marching algorithm sethian 1996 as the minimum cost to travel from that pixel to the watershed outlet following the calculation of geodesic distance channel heads are identified automatically as the upstream end pixel with the largest geodesic distance of each connected skeleton component scanned using a search box river centerlines are then extracted as the least cost path from each channel head to the watershed outlet after the extraction of the river network the original geonet workflow allows the extraction of cross sections along the centerlines and the identification of bank locations at each cross section passalacqua et al 2012 for a given location x y the coordinates of stream cells along the same path five units away from it both upstream x u y u and downstream x d y d are used to estimate the streamline direction vector r as 8 r x d x u i y d y u j then the orthogonal vector c representing the transverse direction is computed as 9 c r 2 i r 1 j in this way a cross section with a predefined length l is drawn centered at x y following the direction of c to identify the channel bank locations the slope is computed along the transect and on each side of the centerline the location corresponding to the peak of the slope is marked as the bank location therefore the original geonet workflow can be used to identify banklines by connecting the bank points on adjacent cross sections from upstream to downstream along each centerline of the network this approach has some limitations as explained in the next section 3 2 new approach for hydro flattening and bank detection 3 2 1 junction detection and network segmentation the original geonet workflow extracts each channel separately from a channel head to the outlet resulting in overlapping centerlines on the main stem river fig 5 a we have added a post processing operation to properly segment the network at river confluences in this process after all the flowlines have been extracted the number of flowlines passing through each grid cell is counted flow network segments are constructed from sequences of grid cells that have the same count value when a change in pass count happens the original flowline is interrupted and the starting point of a segment with a larger pass count is identified as a junction unless it coincides with the watershed outlet fig 5b 3 2 2 network extraction workflow adjustment and cost function modification since our goal is to estimate the bankfull width and conduct the hydro flattening operation across a large domain we are only interested in perennial streams often referred to as blue line streams in topographic maps however the network density automatically detected from the lidar derived topography is usually too dense for general applications therefore instead of using the channel heads automatically detected by geonet we extract channel heads from the national hydrography dataset plus medium resolution nhdplus mr http www horizon systems com nhdplus nhdplusv2 home php mckay et al 2012 which is designed to be used for general applications to ensure a proper scale of the output network poppenga et al 2013 the nhdplus mr is a digital database that contains a national coverage stream network derived from the 30 m resolution national elevation dataset ned using a classic slope based approach with extensive manual corrections due to the coarse spatial resolution of the input elevation data and the extraction method the centerlines in the nhdplus network appear to deviate from the terrain concavities once overlaid on lidar data the channel heads extracted from the nhdplus mr are used as the starting point of the optimal path searching calculation in geonet the corresponding end point of each path could be the end point of the original flowline feature or the unique outlet of the entire watershed either alternative has its pros and cons if the end point of the original flowline is used the junction locations identified in the initial network are inherited thus the delineation of each flowline can be performed within a local drainage catchment independently leading to an easy parallel implementation also since the extraction is bounded by the local catchment boundary shortcuts of least cost paths across catchment boundaries are not possible using the outlet of the entire watershed as the end point of all paths instead allows geonet to automatically extract the network junctions however since in this case paths are searched within a large domain shortcuts across ridges may appear specific examples will be shown in the results section since the channel heads are predefined in our workflow we do not need to compute the geodesic distance at every pixel reducing computational expenses instead we only need to find the least cost path between each start end point pair by calling the route through array function from the python scikit image library with the local cost field defined later as the input in flat areas or areas with artificial structures elevation gradient based flow accumulation often results in inaccurate flow paths fig 6 c due to the significant elevation difference between the stream bed and the in channel structures captured during the survey passalacqua et al 2012 thus affecting the computation of the cost function taking a 104 104 pixels study domain as an example the order of magnitude for the accumulation term varies from 100 to 108 while for the curvature and skeleton the maximum order of magnitude is usually 100 the magnitude of the mean flow accumulation area is about 104 therefore even if a mean is used as a normalization parameter in the expression of the cost function equation 7 the curvature fig 6b and skeleton terms fig 6d are still small compared to the accumulation value of a downstream stream cell which may range from 106 to 108 thus flow accumulation tends to dominate the other two terms to address this issue we compute the local geodesic cost with a normalized flow accumulation term the normalization is implemented by first converting the accumulation values to log scale and then downscaling the log accumulation values from their original range to a range from 0 to 1 to keep the change consistent across different factors we also perform a normalization of the curvature term additionally we explored the introduction of new terms in the cost function based on our recently proposed channel extraction approach zheng et al 2018 in this method information from the nhdplus mr network is used as a prior sample space for the extraction of the river network from lidar data the assumption of this approach is that the reference river network only gets adjusted if nearby cells are lower or equal in elevation using this idea we add another binary term h to the cost function to represent the height of a pixel relative to its nearest original channel pixel the value of 1 is assigned to the pixels with a negative or zero h likely channel pixels and 0 to those with a positive h unlikely channel pixels fig 6e the value of h is obtained by identifying the nearest original channel pixel location for each pixel based on euclidean distance and then computing h as the pixel elevation minus the nearest stream cell elevation thus the modified cost function used in this study is given by 10 œà 1 a n Œ∫ n s h where the n subscript indicates that both the accumulation term a and the curvature term Œ∫ have been normalized in this formulation all the terms have the same weight since all of them take values in a range from 0 to 1 the complete flowchart of the modified river network extraction workflow is in fig 7 3 2 3 raster based hydro flattened zone extraction we propose a raster based method to hydro flatten high resolution topographic data based on curvature and network connectivity fig 8 a the theoretical foundation of this method is that the channel is a convergent zone of the terrain and thus has positive curvature while the banks represent the transition from the channel to the floodplain and have negative curvature therefore by setting a threshold on curvature equal to zero the channel pixels and any other pixels with zero curvature are above this threshold allowing us to also detect waterbodies in the watershed selecting pixels based on a curvature threshold may result in capturing also convergent features that do not connect to the channel network we solve this problem by checking the connectivity between the filtered pixels and the network centerlines previously extracted after identifying all the pixels with a non negative curvature we use this filtered raster to mask the flow directions derived from the flow accumulation calculation thus keeping only those pixels for which a continuous path can be traced to any centerline pixel following flow directions the distance from each connected pixel to the extracted centerline is computed and stored in the output raster small tributaries or ditches draining to the main river may be captured with this approach fig 8 however since their centerlines are not included as a part of our network these pixels have a relatively long distance to the centerline therefore we take advantage of this property to exclude these pixels from the final output by imposing a threshold to the cumulative distribution function of the distance to centerline and deleting all the pixels with a distance above this threshold we recommend using the 80th percentile as threshold which has been tested in multiple sites presented in this study finally the raster composed of all the identified pixels is converted to a hydro flattened polygon the complete flowchart of the hydro flattened zone extraction workflow is in fig 9 once the river network and the hydro flattened zone are extracted their elevations are modified according to the usgs lidar guidelines and base specifications heidemann 2012 in our workflow a constant slope is estimated by fitting a linear regression to the elevation of all the pixels within the extracted hydro flattened zone this slope is applied to obtain the centerline profile fig 10 b then the centerline elevation is extended to both sides along the transect direction to obtain a flattened water surface fig 10 c since cross sections along the extracted network at a constant interval can also be generated with geonet the cross section based breakline delineation approach is also supported by the current version of the workflow the comparison of the results generated with these two approaches is shown as fig 11 the main limitation of the cross section approach is that when the river width varies dramatically the constant length cross section may be too short to cover the entire water surface transect 3 2 4 raster based channel zone extraction and bankfull width estimation a challenge associated with the transect based approach used in the original geonet workflow for bank detection is that different transect lengths may need to be defined for different order streams therefore here we propose a raster based method for solving the bank extraction problem fig 12 to extract the geomorphic channel zone we rely on the slope information similarly to the original geonet bank detection routine it is assumed in the original routine that the bank points along a cross section should be the locations with the maximum slope on both sides of the detected centerline we introduce a filter based on the statistical analysis of the slope distribution all the pixels with a slope on the right tail of the slope cumulative distribution function are preserved a threshold equal to the 90th percentile is recommended to define the tail this threshold value has been tested in our case studies with different topographic settings since bank locations correspond to the maximum peaks of the slope along the transect these bankline pixels are preserved in the filtering operation the only part within the channel that is not in this zone is the convergent section at the center where the slope is close or equal to zero therefore we compensate the missing central section using the hydro flattened zone we identified in the previous step so that the entire channel zone is selected during the bank detection process finally the raster with all the selected cells is converted into a polygon and only polygons intersecting the centerlines are kept while the others are deleted the bankfull width of each river is computed as the channel zone area divided by the length of the centerline 4 results 4 1 natural basin first the river centerline network is extracted using the method described in section 3 2 2 the comparison fig 13 between our extracted river network and the original nhd flowline shows that our results better align with the high resolution terrain data fig 13a and satellite imagery fig 13b the hydro flattened zone is then extracted based on the curvature and connectivity analysis a problem was found at waterbodies when the distance based filtering was applied to extract the final hydro flattened zone since the width of the waterbodies is usually large fig 14 compared to the channel width part of the waterbodies was filtered out which resulted in an underestimation of the waterbody coverage this underestimation can be accounted for by masking the initial output with the nhd waterbody features before the clipping and then applying the filter only to the unmasked zone see fig 15 because the channels in this watershed are all small head water streams we found that the hydro flattened zone mostly coincides with the geomorphic channel zone therefore we converted the extracted hydro flattened zone into polygons and computed the corresponding bankfull width for each river fig 16 4 2 agricultural basin the agricultural basin test shows similar improvements in the extracted river network fig 17 since there are hundred meter wide main stem rivers and several meter wide artificial canals we implemented the clipping process by catchment with the 80th percentile threshold identified from the distance statistical distribution of each local catchment fig 18 in this basin the hydro flattened zone also coincides with the geomorphic channel zone therefore we computed the reach average bankfull width based on the extracted hydro flattened zone fig 19 4 3 urban basin since this is an urbanized flat area we adopted the laplacian curvature to favor the extraction of natural channels passalacqua et al 2012 the results show the improved accuracy of the extracted network fig 20 we identified a problematic area in the upper left corner of the watershed where a underground ditch served as the main drainage waterway but was not detectable in the terrain input fig 21 therefore our tool returned a path following the surface land gradient from the given channel head to the main stem river the hydro flattened zone extracted automatically with our proposed method is able to capture the majority of the manually delineated extent fig 22 to quantitatively measure the accuracy of the hydro flattened zones extracted using geonet we compute two performance metrics based on its comparison with the manually delineated hydro flattened zones the first one is the f index which is the ratio between the intersect area and the union area of the extracted zone and the reference zone it is a comprehensive assessment that reflects the overestimation and underestimation at the same time 11 f a r e a g e o n e t a r e a m a n u a l a r e a g e o n e t a r e a m a n u a l the second one is the c index which is the intersect area divided by the area of the reference zone 12 c a r e a g e o n e t a r e a m a n u a l a r e a m a n u a l c identifies the percentage of the manually delineated area covered by our automatic extraction to compute these two indices both the hydro flattened zones and the reference breaklines have been resampled as rasters at a resolution of 0 3048 m 1 ft with the same extent the comparison table 1 and fig 23 shows that the automatic hydro flattening strategy is able to correctly cover over 90 percent of the hydro flattened zone identified with manually delineated breaklines some overestimation is detected on the edge of the hydro flattened zone but it usually only extends over a single pixel band differently from the two previous areas here water only occupies a small portion of the channel cross section therefore we used the slope information as described in section 3 2 4 to obtain the final geomorphic channel zone fig 24 5 discussion to reduce the density of the extracted river network and required computational resources our approach operates only within the buffer zone of an existing river network therefore the quality of the existing river network significantly affects the accuracy of the final products in the current study we use the nhdplus mr bluelines for regions where better information is available such as the nhdplus high resolution network the user is recommended to use these datasets to define prior probabilities also while the river network datasets used here are only available in the u s our approach can be applied in other areas where an equivalent cartographic river network is available channel extraction with a modified cost function was successful in different topographic settings we identified inaccuracies where the channel is meandering due to the global cost being computed over a long distance to improve the results the user can either tune the weight parameters in the cost function or extract features in smaller sub areas and then combine the extraction results another factor that significantly affects the quality of the result is the input terrain data in natural environments if the river is flowing under the canopies the channel may be difficult to capture from the terrain data similar problems exist with underground structures in urban environments not all the man made features could be correctly extracted with a terrain analysis based method such as the culvert beneath the road even though the drainage network in the presence of these features is well outlined the surface landscape beyond these structures could be arbitrary and does not show any representative characteristics to indicate the existence of such drainage paths since our method is based on the analyses of the surface landscape we could not handle the network with such underground structures quite well however if the locations of these low water crossings are marked our approach is able to identify the surface drainage paths between different markers which could be a valuable direction for future research recent developments in parallel computation algorithms for hydrological terrain analysis fan et al 2014 yƒ±ldƒ±rƒ±m et al 2015 survila et al 2016 have not been integrated in grass functions therefore future work will include the development of efficient routines to be integrated into our workflow 6 conclusions in this paper we presented modifications and new modules of geonet a tool designed for automatic feature extraction from high resolution topographic data the new modules enable the automatic hydro flattening on the elevation data the form of the geodesic cost function used for river network extraction was modified to include information of a given reference river network and to correct the potential errors in the accumulation area term once the river centerline network is extracted curvature and connectivity to centerline information are used to identify the convergent sections of the terrain a threshold obtained from statistical analysis of horizontal distance to stream is applied to filter unwanted components combining the hydro flattened output with slope information we obtained the geomorphic channel zone of a river network and computed the reach average bankfull width for every river segment in the network providing critical information to improve the performance of large scale hydrological simulation and inundation mapping we tested our proposed workflow at three sites with natural agricultural and urban terrain settings compared to satellite imagery and approved products our results demonstrate competitive accuracy showing that our method can automatically perform hydro flattening and bank detection identification the comparison versus manually delineated breaklines shows that although our method tends to add an additional single pixel band on the edge of the actual extent which leads to some overestimation 59 in the test case the extracted area covers the majority of the extent 91 in the test case that used to be decided manually the results also show that at some sites the hydro flattened zone can be directly used for the bankfull width calculation without using slope information acknowledgements this research has been supported by national geospatial center of excellence ngce under grant number 68 7482 16 570 thanks to ngce and tnris for providing the lidar data set the code is stored in a github repository and downloadable from https github com passah2o geonet appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 02 007 
26228,a methodology rested on model based machine learning using simple linear regressions and the parameterizations of the main physics and chemistry processes has been developed to perform highly resolved air quality simulations the training of the methodology is i completed over a 6 month period using the outputs of the chemical transport model chimere and ii then applied over the subsequent 6 months despite rough assumptions this new methodology performs as well as the raw chimere simulation for daily mean concentrations of the main criteria air pollutants no2 ozone pm10 and pm2 5 with correlations ranging from 0 75 to 0 83 for the particulate matter and up to 0 86 for the maximum ozone concentrations some improvements are investigated to expand this methodology to several other uses but at this stage the method can be used for air quality forecasting analysis of pollution episodes and mapping this study also confirms that including a minimum set of selected physical parameterizations brings a high added value on machine learning processes keywords air quality modelling linear regression statistics metamodel resolution increment 1 introduction air pollution is the fourth leading fatal risk for human health globally and in the latest estimates more than 5 million pre mature deaths are linked with air pollution forouzanfar et al 2016 air pollution does not just cause danger to human health but also to the environment the economy and food security e g crop yield losses chemistry transport models ctms are useful tools to assess predict and analyse environmental policies to improve air quality since a large fraction of the population live in urban areas models must run up to the urban scale the model outputs can feed integrated models to assess emission reduction strategies up to the local scale amann et al 2017 anil et al 2018 in atmospheric sciences the race on high resolution simulations over large domains started few years ago for short term forecast or climate studies perspectives skamarock et al 2014 fuhrer et al 2018 in a near future meteorological 3d fields will be commonly available over large domains at a few kilometers resolution for air quality modelling increasing the resolution is expected to provide better results even for remote background sites increasing the resolution should improve the quality of simulations because the system is not fully linear mainly due to the use of non linear chemistry schemes the use of air quality models at high resolution is very computing time consuming mainly because of courant friedrichs lewy cfl conditions the cfl condition is a necessary condition for convergence while solving certain partial differential equations numerically by the method of finite differences this condition imposes to adapt the time step for the advection it is proportional to the grid size for the horizontal transport involving a dramatic increase of simulation durations although a 20 km 50 km resolution is sufficient to be representative of rural background concentrations the use of resolution between 1 and 10 km is essential to estimate urban background concentrations of pollutants for small to large cities the notion of urban increment or decrement for ozone due to the titration effect has been introduced to estimate the impact of a city emissions to the urban background concentrations amann et al 2007 ortiz and rainer 2013 the calculation of this increment is expected to be influenced by local primary emission sources and meteorological parameters the use of highly resolved bottom up emission inventory usually provides better results timmermans et al 2013 compared to top town downscaled emission datasets air quality models benefit from a downscaling of emissions dataset as mentioned by schaap et al 2015 terrenoire et al 2015 and colette et al 2014 by mainly reducing the bias over urbanized areas with sometimes lower correlations however for ozone an optimal resolution could be found as explained by valari and menut 2008 based on an analysis of the root mean square error in a scenario perspective the concept of urban and even street increment was used in kiesewetter et al 2014 2015 to estimate the impact of emission reductions strategies at a given site up to traffic influenced stations in previous studies the use of past observational air quality data crossed with predicted meteorological variables traffic emission prediction was implemented in neural networks to propose statistical models able to perform air quality model predictions catalano and galtioto 2017 and references therein usually these models can only be used at a given site but they offer robust performances recently mallet et al 2018 proposed a first metamodel of a local air quality model to simulate no2 and pm10 concentrations in our study the concept of urban increment is extended to grid cell increment to perform highly resolved air quality simulations rested on model based machine learning using low resolution ctm simulations this method avoids running the ctm at high resolution and therefore allows an impressive gain of computing time this technique is applied for the main criteria pollutants pm10 pm25 no2 and o3 concentrations the method is evaluated for daily or maximum concentrations for ozone only this paper will answer four main questions does this technique provide satisfactory results by passing the costly high resolution air quality simulation what is the expected gain on computing time for which type of application this technique can be used how this technique could be improved 2 material and methods 2 1 model set up in this study the chimere model menut et al 2013 mailler et al 2017 as improved in couvidat et al 2018 is used over two domains i a domain eu encompassing europe at 0 5 0 25 regular resolution lre over the 17 e 40 w and 32 n 70n window and ii a nested domain fr centered over france at 0 09375 0 046875 regular resolution hre over 5 e 10 w and 41 n 51 014 625 n fed by the eu simulation one way nesting the model configuration is summarized hereafter but the reader can refer to the recent chimere publications couvidat et al 2018 bessagnet et al 2017 mailler et al 2017 for details on the corresponding model components and references as well as non user specific model characteristics the particle size ranges from 10 nm to 10 Œºm over 9 bins according to these ranges from bin n 1 to bin n 9 which are respectively 10 00 nm 22 01 nm 2 20 nm 48 43 nm 48 43 nm 106 7 nm 106 7 nm 234 7 nm 234 7 nm 516 2 nm 516 2 nm 1 14 Œºm 1 14 Œºm 2 50 Œºm 2 5 mm 5 0 Œºm 5 0 Œºm 10 0 Œºm in this study the major pm species are considered secondary inorganic aerosol nitrate sulfate and ammonium secondary organic aerosols anthropogenic and biogenic in origins natural mineral dust sea salt and primary particle matter horizontal transport is solved with the second order van leer scheme van leer 1979 subgrid scale convective fluxes are considered once the depth of the boundary layer is computed vertical turbulent mixing can be applied following the k diffusion framework after the parameterisation without counter gradient term of troen and mahrt 1986 detailed in menut et al 2013 a minimal vertical eddy diffusion k is assumed with values of 0 01 m2 s 1 in the dry boundary layer and 1 m2 s 1 in the cloudy boundary layer k is capped to a maximal value of 500 m2 s 1 to avoid unrealistic mixing above the boundary layer a fixed value of 0 1 m2 s 1 is used the present setup also benefits from an improved representation of turbulent mixing in urban areas that yields lower horizontal wind and vertical mixing in order to better capture the difference between the urban canopy where the first chimere model level lies and the top of the urban sublayer of which the lowermost meteorological model level is representative terrenoire et al 2015 as an offline chemistry transport model chimere requires prescribed meteorological fields which were provided here by ecmwf with the integrated forecasting system ifs model at 0 125 resolution with data assimilation although the study is performed in a forecasting perspective meteorological data are issued from reanalyzes emissions of the main pollutants are issued from emep cooperative programme for monitoring and evaluation of long range transmission of air pollutants in europe programme available at http www emep int over france the spatialization of emissions is performed with a 1 km proxy based on the national bottom up emission inventory accessible at http emissions air developpement durable gouv fr that feeds the emission pre processor of chimere described in mailler et al 2017 2 2 air quality simulations a complete chimere simulation for the eu and fr domains is performed from 1st july 2010 to 30 june 2011 1 year with a 10 day spin up period in june 2010 for the initialization as explained in mailler et al 2017 the chimere simulation is divided in two main steps step 1 a pre processing to prepare intermediary files from raw data for meteorology emissions boundary conditions this step consists in performing horizontal vertical and time interpolations and calculation of diagnostic variables e g the vertical mixing coefficient so far this step is computed in a sequential way using only one processor step 2 the core chimere simulation which uses the previous intermediary files provides the main model outputs on an hourly frequency this computing step is parallelized but this step is the costliest and it can roughly represent 80 90 of the total computing time depending on the number of processors and the size and grid resolution of the domain the increment methodology described hereafter is a two steps method with i a training part for a given version of the chimere model on a given period here 6 months in 2010 and ii an application part on another period of interest here 6 months in 2011 all the programs are written in shell language using the nco package zender 2008 to efficiently handle output netcdf files the method is depicted in fig 1 and fully explained in the next subsections in this study the two distinct periods are defined the training period from 1st july 2010 to 31 december 2010 where the increment methodology called fr inc applied over the fr domain is trained to cover a large range of meteorological conditions summer winter and intermediate seasons this learning process uses the chimere simulation outputs and intermediary variables over the eu and fr domains eu lre and fr hre simulations an evaluation period from 1st january 2011 to 30 june 2011 where the increment technique is evaluated against observations and compared with the high resolution chimere simulation fr hre this period covers the full spring period with usual high pm concentrations in the western part of europe 2 3 principle of the increment methodology based on atmospheric diffusion theory potential determinants of urban increments and functional forms of their relationships have been hypothesized under neutral atmospheric conditions the vertical diffusion of a non reactive pollutant from a continuous point source can be described in general form through the following relationship amann et al 2007 seinfeld and pandis 1998 1 œÉ z 2 2 k x u with œÉ z 2 m2 indicating the variance of the vertical diffusion after a distance x from the source k m2 s 1 as the eddy diffusivity and u m s 1 the wind speed considering this assumption in amann et al 2011 the delta between urban and background concentrations Œ¥c of primary pm is given as 2 Œ¥ c d 8 k u q d is the characteristic length of the city q Œºg m2 s 1 is the low level emissions of the city this increment must be corrected to avoid emission double counting as explained in amann et al 2011 based on amann et al 2011 and amann et al 2007 we consider in this study that a given fine cell can behave as a city then the concentration delta Œ¥ c between a fine grid and a coarse grid simulation of a species concentration influenced by low level sources of primary pollutants can be extended with a revised predictor variable as 3 Œ¥ c c c Œ± 1 k u e d e d Œ≤ d Œ¥ x Œ¥ y d Œ¥ x Œ¥ y c and c Œºg m 3 are respectively the concentrations at the fine grid point and interpolated from the coarse grid to the fine mesh k is the vertical mixing coefficient m2 s 1 at the fine grid u is the 10 m horizontal wind speed m s 1 at the fine grid Œ¥x Œ¥y Œ¥x Œ¥y are respectively the coarse longitude latitude and the fine longitude latitude increments of the grids in our study they are constant but they can vary and for each fine grid cell an average value of the surrounding coarse grids can be used d and d are characteristic lengths respectively for the fine and coarse meshes they correspond here to an average of the grid cells size e and e Œºg m 2 s 1 are respectively the low level emission fluxes at the fine grid point and interpolated from the coarse grid for the pm2 5 and pm10 concentrations the sum of primary emissions is considered for no and no2 the nox emissions are considered in our methodology the emissions of the two first level sum of emissions approximatively below 30 m of chimere are taken into account Œ± and Œ≤ are coefficients integrating geographical physical and chemical processes that are lost in the simplification process they also account for unit changes note that Œ≤ here has not the same meaning than in amann et al 2007 2011 it is here residual value of the regression and would be expected to be close to 0 differently to amann et al 2007 the vertical eddy diffusion k is not integrated in the Œ± coefficient but included in the predictor 1 k u e d e d because this variable is calculated and stored during the chimere preprocessing moreover in our methodology e includes all low level emission sources in the coarse grid and can be eventually higher than e if the grid cell is in a remote areas surrounded by high emissions area this change is to break the implicit assumption assumed in amann et al 2007 2011 that background concentrations could be only influenced by the city emissions here the high resolution grid cell it is then noteworthy that in the case of a fine grid cell with low emissions compared to higher background concentrations the predictor could turn negative which makes intuitively sense while in amann et al 2007 2011 the delta is always positive at it refers to a city implicitly influenced by lower background concentrations the principle of our methodology is to calculate on a daily basis these two coefficients Œ± and Œ≤ for each fine grid cell on the training period where all the other variables are known the previous formula eq 3 is used for primary pm no and no2 concentrations the main assumption is to neglect chemistry processes that are partially included in Œ± and Œ≤ and also included in the low resolution chimere outputs 2 4 application of the methodology 2 4 1 treatment of pm2 5 pm10 no2 and no this increment methodology is then applied for pm2 5 pm10 no2 and no hourly ground concentrations over the evaluation period from 1st january to the 30 june 2011 with the Œ± and Œ≤ coefficients computed over the training period fig 1 4 c i j c i j Œ¥ c i j Œ¥ c i j Œ± i j Œ¥ c i j Œ≤ i j Œ¥ c i j 1 k i j u i j e i j d e i j d with c the concentration on the fine mesh determined by the increment technique c the interpolated coarse grid concentration on the fine mesh i and j referring to the coordinates indexes of a given fine mesh grid cell Œ¥ c is the predictor variable and Œ¥ c the increment value applied to the background concentration value c as discussed in the previous section the other variables were previously defined e e k and u are computed during the first step of the chimere preprocessing as depicted in fig 1 2 4 2 special treatment for ozone for a secondary pollutant like ozone involved in a non linear chemistry scheme the previous formulas eq 3 and 4 based on delta of concentrations are not suited during et al 2011 proposed a simplified stationary model to estimate ozone and no2 concentrations influenced by low level sources like traffic emissions the main two equations of the ozone chemistry involving nox and ozone are n o o 3 k r n o 2 o 2 n o 2 j n o o 3 with j s 1 the photolytic frequency of no2 k r ppb 1 s 1 the kinetic rate the differential equations describing the reactions of no2 no and o3 with a diffusion term due to transport and mixing gives 5 d o 3 d t k r n o Œ¥ o 3 j Œ¥ n o 2 o 3 b o 3 œÑ the first two terms on the right side describe the chemical transformation by thermal and photochemical reactions the last term describes the mixing as a function of concentration differences between a background concentration index b and the point at which the concentration should be calculated here on the fine mesh in a stationary regime assumption with d o 3 d t 0 the equation gives for ozone concentration the approximation c 6 c o 3 j œÑ c n o 2 c o 3 b 1 œÑ k r c n o œÑ Œ¥ x Œ¥ y 8 k u j 1 47 10 5 s w r d 4 84 10 9 s w r d 2 k r 3 9 10 4 p p b 1 s 1 c o 3 b is the interpolated concentration over the fine grid of the low resolution concentrations eu lre c n o 2 and c n o are the concentrations over the fine mesh fr œÑ is defined in amann et al 2007 as a simplification of diffusion for a box model as previously discussed Œ¥ x Œ¥ y is an average characteristic length of the grid cell we considered here j as a polynomial function of the short wave radiation swrd w m 2 as reported in trebs et al 2009 for low altitude sites considering an albedo equal to zero here we assumed the kinetic rate k r to be constant a statistical method similarly to primary pollutants is implemented two coefficients Œ± and Œ≤ are computed over the training period 7 c o 3 Œ± c o 3 Œ≤ over the training period c o 3 is known and c o 3 can be calculated each hour then Œ± and Œ≤ are calculated using a simple linear regression for each grid points on a daily basis this technique suppresses systematic biases and accounts for missing processes due to the simplifications afterwards over the evaluation period Œ± and Œ≤ are applied over hourly concentrations for each grid cell i j then the final ozone concentration c is simply computed as 8 c o 3 i j Œ± i j c o 3 i j Œ≤ i j 2 4 3 regression coefficients to avoid any overshoots when applying the methodology on hourly values concentrations and delta of concentrations are capped according values reported in table 1 in the supplementary material fig s1 the Œ± slope Œ≤ intercept coefficients and the pearson correlation of the linear regression calculated over the training period are reported for all pollutants for each grid cell the coefficients are calculated using 184 daily values 6 months the median correlations over the fr domain are 0 31 0 33 0 35 and 0 98 respectively for pm2 5 pm10 no2 and o3 with generally higher values over urban areas the low correlations result from the averaging process over the whole domain where low absolute values in remote areas deteriorate the statistic errors the correlation is the highest for ozone since the linear regression is based on the absolute concentrations mainly driven by the background values and not a delta of concentrations as for primary pollutants the Œ± coefficient is generally below 1 confirming the tendency that the high resolution simulation produces lower concentrations compared to the low resolution run 3 results and discussion 3 1 evaluation of the increment methodology for the european domain eu background pm2 5 pm10 no2 and o3 observational data are retrieved thanks to the emep ebas database t√∏rseth et al 2012 the increment methodology can be evaluated against the raw chimere simulation and observations over the fr domain for the 1st january 30 june 2011 the french geod air database provides hourly concentrations for all stations over france available online at https www geodair fr all background observations are used on a daily basis for the main pollutants ozone no2 pm2 5 and pm10 urban periurban and rural stations are considered daily maximum concentrations are only considered for ozone in this paper periurban and rural are lumped into the category rural to better isolate the urban signal only the french data are considered for the evaluation over the fr domain as we benefit from the most accurate emission data only in france since a bottom up emission proxy is used over this country at the european scale fig 2 chimere is able to retrieve the main patterns of air pollution with the highest concentrations of pm over the benelux south of poland and the north of italy the agreement with observations is rather good for pm2 5 concentrations over spain and uk with a clear underestimation over benelux the underestimation of pm is commonly observed in air quality modelling due to the underestimation of emissions wood burning issue in winter and weaknesses of models to represent high concentrations of ammonium nitrate during the early spring period bessagnet et al 2016 for o3 and no2 concentrations the model reproduces quite well the observations ozone concentrations are usually lower nearby emission areas and the highest over the sea due to accumulation effects and weak deposition over water bodies confirmed by observations in malta cyprus and crete and altitude sites massif central in france and alps due to transport of ozone from the free troposphere figs 3 and 4 show the added value on air quality simulations when using a higher horizontal resolution clearly the regional patterns computed at low resolution are important drivers of the fine resolution outputs for pm and ozone however an urban signal is clearly identified for the major cities and in some alpine valleys mainly due to the effect of stagnant conditions table 2 reports global error statistics of pollutant concentrations for the three models at urban and rural sites over the 1st january 30 june 2011 i the increment methodology applied over the fr domain fr inc ii the chimere high resolution simulation fr hre and iii the chimere low resolution simulation eu lre the pearson correlations are rather good for the raw chimere simulations generally in the range 0 7 0 8 for most of pollutants however the highest pm concentrations in winter and early spring are underestimated fig 5 for no2 differences are clearly identified over the cities and along the major road lines the root mean square error is systematically improved for this species using a higher resolution table 2 mainly due to a reduction of the bias ozone concentrations are on average lower using a higher resolution this is a usual behavior due to the non linearities in chemistry processes 2nd order kinetic reactions involving ozone for all pollutants the increment methodology provides very similar results compared to the high resolution simulation the patterns of mean values over the evaluation period are very close and the average timeseries for all stations in france fig 5 confirm this statement all along the period particularly for pm for no2 concentrations the increment methodology provides higher no2 concentrations over urban areas particularly in winter with coherent lower ozone concentrations due to the titration effect with no2 for all pollutants the bias is improved by the increment methodology with a slight improvement of the root mean square error while the correlation is sometimes slightly impaired or improved these results are very satisfactory and could be surprising taking into consideration the simplicity and rough assumptions of this methodology for pm the method produces better root mean square errors over the paris region the number of data used e g up to 39 520 daily values for no2 for the evaluation and the diversity of site locations in france is large enough to consider these results statistically representative a deficiency of the method should have appeared in the error statistics in the supplementary material the quantile quantile plots show these good performances particularly for the highest values fig s2 in supplementary material the main discrepancies of this technique occur for maximum ozone concentrations which become slightly underestimated however this behavior can be explained by the technique that is based on daily values and applied over hourly values with certainly a tendency to smooth the extreme values however this smoothing effect is not as large as expected and could be suppressed in the future by some improvements of the methodology moreover for the extreme values the increment technique provides better performances with values closer to the observations at rural and urban stations compared to the fr hre chimere simulation fig s3 in supplementary material this smoothing effect certainly helps the model to avoid some overshoots due to the extreme sensitivity of this type of air quality model like chimere to the parameterisation of the vertical diffusion coefficient which can be set to an arbitrary minimum value inducing unrealistic stagnant conditions in fig s4 of the supplementary material additional information on a comparison of the delta of concentration between the increment methodology and the reference high resolution simulation respectively inc lre versus hre lre is provided the overall statistic parameters are satisfactory with a slope close to 1 and a correlation coefficient between 0 69 for o3 to 0 84 for no2 as anticipated and expected the intercept is low close to 0 the delta on absolute values represents in the model values from 8 for o3 to 12 for pm of the total concentration on average however for no2 concentrations it represents up to 72 in our increment methodology on average compared to 54 in the hre simulation 3 2 possible improvement of the methodology this first version of the methodology already provides very good results such a technique based on first order linear regressions with some selected main physics and chemistry equations is sufficient to provide results as accurate as a usual simulation embedding much more detailed processes some improvements will be considered in the next version by investigating the following considerations the use of a 2nd order or higher linear regression could be tested for the incremental formula defined in section 2 4 such as 9 Œ¥ c i j Œ± i j Œ≤ i j Œ¥ c i j Œ≥ i j Œ¥ c i j 2 with Œ± Œ≤ and Œ≥ the regression coefficients for grid cell i j for ozone an improvement of the technique depicted in section 2 4 2 could be performed by the use of a multi variable methodology involving at least two predictors the terms between brackets in eq 10 10 c o 3 i j Œ± i j Œ≤ i j j œÑ c n o 2 1 œÑ k r c n o Œ≥ i j c o 3 b 1 œÑ k r c n o with Œ± Œ≤ and Œ≥ the regression coefficients for grid cell i j the regression techniques could also be defined per quantile of concentrations to improve the results on extreme concentrations more meteorological variables could be implemented in the regression formulas to improve the robustness of the methodology ozone and nitrogen oxides could be treated by a single methodology to better accounts for their chemical interactions the methodology deserves to be tested on finer meshes up to 1 km resolution with an adequate bottom up emission inventory to confirm these encouraging results and evaluate the linearity of scale dependency one must keep in mind that the technique must remain simple to make it fast robust and the current results are already totally satisfactory for various uses so far as it is currently designed the technique can be applied for short term forecasts analysis of episodes long term simulation of past years for mapping for emission reduction analyses the method is expected to work only for primary pm because primary emissions of pm are taken into account in the methodology for ozone and more generally secondary pollutants it could be less straightforward since the change of chemical regime and the voc volatile organic compound chemistry will be not sufficiently considered in the increment technique even through the statistical corrections however the sensitivity to emissions and meteorological changes is partially contained in the regression coefficients through the diversity of situations encountered over the training period 4 conclusions a surrogate model of the ctm chimere has been designed to downscale a coarse horizontal resolution simulation 0 50 0 25 to a finer mesh 0 09375 0 046875 this first version of the methodology is based on a training process over a 6 months period and applied over the subsequent 6 months with an evaluation against the performances of the raw chimere simulation at the highest resolution definitively our methodology is able to capture the main patterns and evolution of daily concentrations for the main pollutants and the gain in computing time is very important since the costliest step of the simulation process is by passed performances based on an evaluation against observations are similar to those obtained by the raw model simulations with chimere at high resolution performances are even sometimes better certainly due to smoothing effects that suppress some overshoots due to the sensitivity of the model to the parameterizations of the eddy diffusion indeed the vertical diffusion is a very sensitive parameter in ctms this coefficient is so sensitive that it is caped in all models and this bounding already results from a kind of learning process as it derives from the experience of the model developer during the calibration phase in its current state the methodology can work for short term air quality forecasts case studies and air quality concentration mapping to extend its use to emission reduction assessment the model could be improved particularly for ozone by i implementing additional variables available in the chimere pre processing ii increasing the order of the linear regression or ii the use of multivariable regressions however this type of methodology mixing statistics physics and chemistry is promising and could be run on mono processor devices and gain a lot of computing time while the core chimere simulation step 2 can last several days depending on the number of cores and resolution only few minutes is sufficient to perform a high resolution simulation with the increment technique therefore a minimum of 100 1000 less computing times is expected for this step the training process with nco procedures lasts about 3 h in our case to fit the regression parameters and this could be optimized with other programming languages this could open rooms to new fields of research and operational uses in air quality modelling that was so far limited by computing time these findings will also encourage the chimere development team to parallelize the chimere preprocessing step 1 to fully take advantage of this technique acknowledgements this work is partly funded by the french ministry in charge of ecology mtes the ebas database has largely been funded by the un ece clrtap emep amap and through nilu internal resources specific developments have been possible due to projects like eusaar eu fp5 ebas web interface ebas online norwegian research council infra upgrading of database platform and htap european commission dg env import and export routines to build a secondary repository in support of www htap org appendix 1 error statistics used to evaluate model performance m and o refer respectively with model and observations data and n is the number of observations bias m o with m 1 n i 1 n m i and o 1 n i 1 n o i root mean square error r m s e 1 n i 1 n m i o i 2 correlation coefficient r i 1 n m i m o i o i 1 n m i m 2 i 1 n o i o 2 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 02 017 
26228,a methodology rested on model based machine learning using simple linear regressions and the parameterizations of the main physics and chemistry processes has been developed to perform highly resolved air quality simulations the training of the methodology is i completed over a 6 month period using the outputs of the chemical transport model chimere and ii then applied over the subsequent 6 months despite rough assumptions this new methodology performs as well as the raw chimere simulation for daily mean concentrations of the main criteria air pollutants no2 ozone pm10 and pm2 5 with correlations ranging from 0 75 to 0 83 for the particulate matter and up to 0 86 for the maximum ozone concentrations some improvements are investigated to expand this methodology to several other uses but at this stage the method can be used for air quality forecasting analysis of pollution episodes and mapping this study also confirms that including a minimum set of selected physical parameterizations brings a high added value on machine learning processes keywords air quality modelling linear regression statistics metamodel resolution increment 1 introduction air pollution is the fourth leading fatal risk for human health globally and in the latest estimates more than 5 million pre mature deaths are linked with air pollution forouzanfar et al 2016 air pollution does not just cause danger to human health but also to the environment the economy and food security e g crop yield losses chemistry transport models ctms are useful tools to assess predict and analyse environmental policies to improve air quality since a large fraction of the population live in urban areas models must run up to the urban scale the model outputs can feed integrated models to assess emission reduction strategies up to the local scale amann et al 2017 anil et al 2018 in atmospheric sciences the race on high resolution simulations over large domains started few years ago for short term forecast or climate studies perspectives skamarock et al 2014 fuhrer et al 2018 in a near future meteorological 3d fields will be commonly available over large domains at a few kilometers resolution for air quality modelling increasing the resolution is expected to provide better results even for remote background sites increasing the resolution should improve the quality of simulations because the system is not fully linear mainly due to the use of non linear chemistry schemes the use of air quality models at high resolution is very computing time consuming mainly because of courant friedrichs lewy cfl conditions the cfl condition is a necessary condition for convergence while solving certain partial differential equations numerically by the method of finite differences this condition imposes to adapt the time step for the advection it is proportional to the grid size for the horizontal transport involving a dramatic increase of simulation durations although a 20 km 50 km resolution is sufficient to be representative of rural background concentrations the use of resolution between 1 and 10 km is essential to estimate urban background concentrations of pollutants for small to large cities the notion of urban increment or decrement for ozone due to the titration effect has been introduced to estimate the impact of a city emissions to the urban background concentrations amann et al 2007 ortiz and rainer 2013 the calculation of this increment is expected to be influenced by local primary emission sources and meteorological parameters the use of highly resolved bottom up emission inventory usually provides better results timmermans et al 2013 compared to top town downscaled emission datasets air quality models benefit from a downscaling of emissions dataset as mentioned by schaap et al 2015 terrenoire et al 2015 and colette et al 2014 by mainly reducing the bias over urbanized areas with sometimes lower correlations however for ozone an optimal resolution could be found as explained by valari and menut 2008 based on an analysis of the root mean square error in a scenario perspective the concept of urban and even street increment was used in kiesewetter et al 2014 2015 to estimate the impact of emission reductions strategies at a given site up to traffic influenced stations in previous studies the use of past observational air quality data crossed with predicted meteorological variables traffic emission prediction was implemented in neural networks to propose statistical models able to perform air quality model predictions catalano and galtioto 2017 and references therein usually these models can only be used at a given site but they offer robust performances recently mallet et al 2018 proposed a first metamodel of a local air quality model to simulate no2 and pm10 concentrations in our study the concept of urban increment is extended to grid cell increment to perform highly resolved air quality simulations rested on model based machine learning using low resolution ctm simulations this method avoids running the ctm at high resolution and therefore allows an impressive gain of computing time this technique is applied for the main criteria pollutants pm10 pm25 no2 and o3 concentrations the method is evaluated for daily or maximum concentrations for ozone only this paper will answer four main questions does this technique provide satisfactory results by passing the costly high resolution air quality simulation what is the expected gain on computing time for which type of application this technique can be used how this technique could be improved 2 material and methods 2 1 model set up in this study the chimere model menut et al 2013 mailler et al 2017 as improved in couvidat et al 2018 is used over two domains i a domain eu encompassing europe at 0 5 0 25 regular resolution lre over the 17 e 40 w and 32 n 70n window and ii a nested domain fr centered over france at 0 09375 0 046875 regular resolution hre over 5 e 10 w and 41 n 51 014 625 n fed by the eu simulation one way nesting the model configuration is summarized hereafter but the reader can refer to the recent chimere publications couvidat et al 2018 bessagnet et al 2017 mailler et al 2017 for details on the corresponding model components and references as well as non user specific model characteristics the particle size ranges from 10 nm to 10 Œºm over 9 bins according to these ranges from bin n 1 to bin n 9 which are respectively 10 00 nm 22 01 nm 2 20 nm 48 43 nm 48 43 nm 106 7 nm 106 7 nm 234 7 nm 234 7 nm 516 2 nm 516 2 nm 1 14 Œºm 1 14 Œºm 2 50 Œºm 2 5 mm 5 0 Œºm 5 0 Œºm 10 0 Œºm in this study the major pm species are considered secondary inorganic aerosol nitrate sulfate and ammonium secondary organic aerosols anthropogenic and biogenic in origins natural mineral dust sea salt and primary particle matter horizontal transport is solved with the second order van leer scheme van leer 1979 subgrid scale convective fluxes are considered once the depth of the boundary layer is computed vertical turbulent mixing can be applied following the k diffusion framework after the parameterisation without counter gradient term of troen and mahrt 1986 detailed in menut et al 2013 a minimal vertical eddy diffusion k is assumed with values of 0 01 m2 s 1 in the dry boundary layer and 1 m2 s 1 in the cloudy boundary layer k is capped to a maximal value of 500 m2 s 1 to avoid unrealistic mixing above the boundary layer a fixed value of 0 1 m2 s 1 is used the present setup also benefits from an improved representation of turbulent mixing in urban areas that yields lower horizontal wind and vertical mixing in order to better capture the difference between the urban canopy where the first chimere model level lies and the top of the urban sublayer of which the lowermost meteorological model level is representative terrenoire et al 2015 as an offline chemistry transport model chimere requires prescribed meteorological fields which were provided here by ecmwf with the integrated forecasting system ifs model at 0 125 resolution with data assimilation although the study is performed in a forecasting perspective meteorological data are issued from reanalyzes emissions of the main pollutants are issued from emep cooperative programme for monitoring and evaluation of long range transmission of air pollutants in europe programme available at http www emep int over france the spatialization of emissions is performed with a 1 km proxy based on the national bottom up emission inventory accessible at http emissions air developpement durable gouv fr that feeds the emission pre processor of chimere described in mailler et al 2017 2 2 air quality simulations a complete chimere simulation for the eu and fr domains is performed from 1st july 2010 to 30 june 2011 1 year with a 10 day spin up period in june 2010 for the initialization as explained in mailler et al 2017 the chimere simulation is divided in two main steps step 1 a pre processing to prepare intermediary files from raw data for meteorology emissions boundary conditions this step consists in performing horizontal vertical and time interpolations and calculation of diagnostic variables e g the vertical mixing coefficient so far this step is computed in a sequential way using only one processor step 2 the core chimere simulation which uses the previous intermediary files provides the main model outputs on an hourly frequency this computing step is parallelized but this step is the costliest and it can roughly represent 80 90 of the total computing time depending on the number of processors and the size and grid resolution of the domain the increment methodology described hereafter is a two steps method with i a training part for a given version of the chimere model on a given period here 6 months in 2010 and ii an application part on another period of interest here 6 months in 2011 all the programs are written in shell language using the nco package zender 2008 to efficiently handle output netcdf files the method is depicted in fig 1 and fully explained in the next subsections in this study the two distinct periods are defined the training period from 1st july 2010 to 31 december 2010 where the increment methodology called fr inc applied over the fr domain is trained to cover a large range of meteorological conditions summer winter and intermediate seasons this learning process uses the chimere simulation outputs and intermediary variables over the eu and fr domains eu lre and fr hre simulations an evaluation period from 1st january 2011 to 30 june 2011 where the increment technique is evaluated against observations and compared with the high resolution chimere simulation fr hre this period covers the full spring period with usual high pm concentrations in the western part of europe 2 3 principle of the increment methodology based on atmospheric diffusion theory potential determinants of urban increments and functional forms of their relationships have been hypothesized under neutral atmospheric conditions the vertical diffusion of a non reactive pollutant from a continuous point source can be described in general form through the following relationship amann et al 2007 seinfeld and pandis 1998 1 œÉ z 2 2 k x u with œÉ z 2 m2 indicating the variance of the vertical diffusion after a distance x from the source k m2 s 1 as the eddy diffusivity and u m s 1 the wind speed considering this assumption in amann et al 2011 the delta between urban and background concentrations Œ¥c of primary pm is given as 2 Œ¥ c d 8 k u q d is the characteristic length of the city q Œºg m2 s 1 is the low level emissions of the city this increment must be corrected to avoid emission double counting as explained in amann et al 2011 based on amann et al 2011 and amann et al 2007 we consider in this study that a given fine cell can behave as a city then the concentration delta Œ¥ c between a fine grid and a coarse grid simulation of a species concentration influenced by low level sources of primary pollutants can be extended with a revised predictor variable as 3 Œ¥ c c c Œ± 1 k u e d e d Œ≤ d Œ¥ x Œ¥ y d Œ¥ x Œ¥ y c and c Œºg m 3 are respectively the concentrations at the fine grid point and interpolated from the coarse grid to the fine mesh k is the vertical mixing coefficient m2 s 1 at the fine grid u is the 10 m horizontal wind speed m s 1 at the fine grid Œ¥x Œ¥y Œ¥x Œ¥y are respectively the coarse longitude latitude and the fine longitude latitude increments of the grids in our study they are constant but they can vary and for each fine grid cell an average value of the surrounding coarse grids can be used d and d are characteristic lengths respectively for the fine and coarse meshes they correspond here to an average of the grid cells size e and e Œºg m 2 s 1 are respectively the low level emission fluxes at the fine grid point and interpolated from the coarse grid for the pm2 5 and pm10 concentrations the sum of primary emissions is considered for no and no2 the nox emissions are considered in our methodology the emissions of the two first level sum of emissions approximatively below 30 m of chimere are taken into account Œ± and Œ≤ are coefficients integrating geographical physical and chemical processes that are lost in the simplification process they also account for unit changes note that Œ≤ here has not the same meaning than in amann et al 2007 2011 it is here residual value of the regression and would be expected to be close to 0 differently to amann et al 2007 the vertical eddy diffusion k is not integrated in the Œ± coefficient but included in the predictor 1 k u e d e d because this variable is calculated and stored during the chimere preprocessing moreover in our methodology e includes all low level emission sources in the coarse grid and can be eventually higher than e if the grid cell is in a remote areas surrounded by high emissions area this change is to break the implicit assumption assumed in amann et al 2007 2011 that background concentrations could be only influenced by the city emissions here the high resolution grid cell it is then noteworthy that in the case of a fine grid cell with low emissions compared to higher background concentrations the predictor could turn negative which makes intuitively sense while in amann et al 2007 2011 the delta is always positive at it refers to a city implicitly influenced by lower background concentrations the principle of our methodology is to calculate on a daily basis these two coefficients Œ± and Œ≤ for each fine grid cell on the training period where all the other variables are known the previous formula eq 3 is used for primary pm no and no2 concentrations the main assumption is to neglect chemistry processes that are partially included in Œ± and Œ≤ and also included in the low resolution chimere outputs 2 4 application of the methodology 2 4 1 treatment of pm2 5 pm10 no2 and no this increment methodology is then applied for pm2 5 pm10 no2 and no hourly ground concentrations over the evaluation period from 1st january to the 30 june 2011 with the Œ± and Œ≤ coefficients computed over the training period fig 1 4 c i j c i j Œ¥ c i j Œ¥ c i j Œ± i j Œ¥ c i j Œ≤ i j Œ¥ c i j 1 k i j u i j e i j d e i j d with c the concentration on the fine mesh determined by the increment technique c the interpolated coarse grid concentration on the fine mesh i and j referring to the coordinates indexes of a given fine mesh grid cell Œ¥ c is the predictor variable and Œ¥ c the increment value applied to the background concentration value c as discussed in the previous section the other variables were previously defined e e k and u are computed during the first step of the chimere preprocessing as depicted in fig 1 2 4 2 special treatment for ozone for a secondary pollutant like ozone involved in a non linear chemistry scheme the previous formulas eq 3 and 4 based on delta of concentrations are not suited during et al 2011 proposed a simplified stationary model to estimate ozone and no2 concentrations influenced by low level sources like traffic emissions the main two equations of the ozone chemistry involving nox and ozone are n o o 3 k r n o 2 o 2 n o 2 j n o o 3 with j s 1 the photolytic frequency of no2 k r ppb 1 s 1 the kinetic rate the differential equations describing the reactions of no2 no and o3 with a diffusion term due to transport and mixing gives 5 d o 3 d t k r n o Œ¥ o 3 j Œ¥ n o 2 o 3 b o 3 œÑ the first two terms on the right side describe the chemical transformation by thermal and photochemical reactions the last term describes the mixing as a function of concentration differences between a background concentration index b and the point at which the concentration should be calculated here on the fine mesh in a stationary regime assumption with d o 3 d t 0 the equation gives for ozone concentration the approximation c 6 c o 3 j œÑ c n o 2 c o 3 b 1 œÑ k r c n o œÑ Œ¥ x Œ¥ y 8 k u j 1 47 10 5 s w r d 4 84 10 9 s w r d 2 k r 3 9 10 4 p p b 1 s 1 c o 3 b is the interpolated concentration over the fine grid of the low resolution concentrations eu lre c n o 2 and c n o are the concentrations over the fine mesh fr œÑ is defined in amann et al 2007 as a simplification of diffusion for a box model as previously discussed Œ¥ x Œ¥ y is an average characteristic length of the grid cell we considered here j as a polynomial function of the short wave radiation swrd w m 2 as reported in trebs et al 2009 for low altitude sites considering an albedo equal to zero here we assumed the kinetic rate k r to be constant a statistical method similarly to primary pollutants is implemented two coefficients Œ± and Œ≤ are computed over the training period 7 c o 3 Œ± c o 3 Œ≤ over the training period c o 3 is known and c o 3 can be calculated each hour then Œ± and Œ≤ are calculated using a simple linear regression for each grid points on a daily basis this technique suppresses systematic biases and accounts for missing processes due to the simplifications afterwards over the evaluation period Œ± and Œ≤ are applied over hourly concentrations for each grid cell i j then the final ozone concentration c is simply computed as 8 c o 3 i j Œ± i j c o 3 i j Œ≤ i j 2 4 3 regression coefficients to avoid any overshoots when applying the methodology on hourly values concentrations and delta of concentrations are capped according values reported in table 1 in the supplementary material fig s1 the Œ± slope Œ≤ intercept coefficients and the pearson correlation of the linear regression calculated over the training period are reported for all pollutants for each grid cell the coefficients are calculated using 184 daily values 6 months the median correlations over the fr domain are 0 31 0 33 0 35 and 0 98 respectively for pm2 5 pm10 no2 and o3 with generally higher values over urban areas the low correlations result from the averaging process over the whole domain where low absolute values in remote areas deteriorate the statistic errors the correlation is the highest for ozone since the linear regression is based on the absolute concentrations mainly driven by the background values and not a delta of concentrations as for primary pollutants the Œ± coefficient is generally below 1 confirming the tendency that the high resolution simulation produces lower concentrations compared to the low resolution run 3 results and discussion 3 1 evaluation of the increment methodology for the european domain eu background pm2 5 pm10 no2 and o3 observational data are retrieved thanks to the emep ebas database t√∏rseth et al 2012 the increment methodology can be evaluated against the raw chimere simulation and observations over the fr domain for the 1st january 30 june 2011 the french geod air database provides hourly concentrations for all stations over france available online at https www geodair fr all background observations are used on a daily basis for the main pollutants ozone no2 pm2 5 and pm10 urban periurban and rural stations are considered daily maximum concentrations are only considered for ozone in this paper periurban and rural are lumped into the category rural to better isolate the urban signal only the french data are considered for the evaluation over the fr domain as we benefit from the most accurate emission data only in france since a bottom up emission proxy is used over this country at the european scale fig 2 chimere is able to retrieve the main patterns of air pollution with the highest concentrations of pm over the benelux south of poland and the north of italy the agreement with observations is rather good for pm2 5 concentrations over spain and uk with a clear underestimation over benelux the underestimation of pm is commonly observed in air quality modelling due to the underestimation of emissions wood burning issue in winter and weaknesses of models to represent high concentrations of ammonium nitrate during the early spring period bessagnet et al 2016 for o3 and no2 concentrations the model reproduces quite well the observations ozone concentrations are usually lower nearby emission areas and the highest over the sea due to accumulation effects and weak deposition over water bodies confirmed by observations in malta cyprus and crete and altitude sites massif central in france and alps due to transport of ozone from the free troposphere figs 3 and 4 show the added value on air quality simulations when using a higher horizontal resolution clearly the regional patterns computed at low resolution are important drivers of the fine resolution outputs for pm and ozone however an urban signal is clearly identified for the major cities and in some alpine valleys mainly due to the effect of stagnant conditions table 2 reports global error statistics of pollutant concentrations for the three models at urban and rural sites over the 1st january 30 june 2011 i the increment methodology applied over the fr domain fr inc ii the chimere high resolution simulation fr hre and iii the chimere low resolution simulation eu lre the pearson correlations are rather good for the raw chimere simulations generally in the range 0 7 0 8 for most of pollutants however the highest pm concentrations in winter and early spring are underestimated fig 5 for no2 differences are clearly identified over the cities and along the major road lines the root mean square error is systematically improved for this species using a higher resolution table 2 mainly due to a reduction of the bias ozone concentrations are on average lower using a higher resolution this is a usual behavior due to the non linearities in chemistry processes 2nd order kinetic reactions involving ozone for all pollutants the increment methodology provides very similar results compared to the high resolution simulation the patterns of mean values over the evaluation period are very close and the average timeseries for all stations in france fig 5 confirm this statement all along the period particularly for pm for no2 concentrations the increment methodology provides higher no2 concentrations over urban areas particularly in winter with coherent lower ozone concentrations due to the titration effect with no2 for all pollutants the bias is improved by the increment methodology with a slight improvement of the root mean square error while the correlation is sometimes slightly impaired or improved these results are very satisfactory and could be surprising taking into consideration the simplicity and rough assumptions of this methodology for pm the method produces better root mean square errors over the paris region the number of data used e g up to 39 520 daily values for no2 for the evaluation and the diversity of site locations in france is large enough to consider these results statistically representative a deficiency of the method should have appeared in the error statistics in the supplementary material the quantile quantile plots show these good performances particularly for the highest values fig s2 in supplementary material the main discrepancies of this technique occur for maximum ozone concentrations which become slightly underestimated however this behavior can be explained by the technique that is based on daily values and applied over hourly values with certainly a tendency to smooth the extreme values however this smoothing effect is not as large as expected and could be suppressed in the future by some improvements of the methodology moreover for the extreme values the increment technique provides better performances with values closer to the observations at rural and urban stations compared to the fr hre chimere simulation fig s3 in supplementary material this smoothing effect certainly helps the model to avoid some overshoots due to the extreme sensitivity of this type of air quality model like chimere to the parameterisation of the vertical diffusion coefficient which can be set to an arbitrary minimum value inducing unrealistic stagnant conditions in fig s4 of the supplementary material additional information on a comparison of the delta of concentration between the increment methodology and the reference high resolution simulation respectively inc lre versus hre lre is provided the overall statistic parameters are satisfactory with a slope close to 1 and a correlation coefficient between 0 69 for o3 to 0 84 for no2 as anticipated and expected the intercept is low close to 0 the delta on absolute values represents in the model values from 8 for o3 to 12 for pm of the total concentration on average however for no2 concentrations it represents up to 72 in our increment methodology on average compared to 54 in the hre simulation 3 2 possible improvement of the methodology this first version of the methodology already provides very good results such a technique based on first order linear regressions with some selected main physics and chemistry equations is sufficient to provide results as accurate as a usual simulation embedding much more detailed processes some improvements will be considered in the next version by investigating the following considerations the use of a 2nd order or higher linear regression could be tested for the incremental formula defined in section 2 4 such as 9 Œ¥ c i j Œ± i j Œ≤ i j Œ¥ c i j Œ≥ i j Œ¥ c i j 2 with Œ± Œ≤ and Œ≥ the regression coefficients for grid cell i j for ozone an improvement of the technique depicted in section 2 4 2 could be performed by the use of a multi variable methodology involving at least two predictors the terms between brackets in eq 10 10 c o 3 i j Œ± i j Œ≤ i j j œÑ c n o 2 1 œÑ k r c n o Œ≥ i j c o 3 b 1 œÑ k r c n o with Œ± Œ≤ and Œ≥ the regression coefficients for grid cell i j the regression techniques could also be defined per quantile of concentrations to improve the results on extreme concentrations more meteorological variables could be implemented in the regression formulas to improve the robustness of the methodology ozone and nitrogen oxides could be treated by a single methodology to better accounts for their chemical interactions the methodology deserves to be tested on finer meshes up to 1 km resolution with an adequate bottom up emission inventory to confirm these encouraging results and evaluate the linearity of scale dependency one must keep in mind that the technique must remain simple to make it fast robust and the current results are already totally satisfactory for various uses so far as it is currently designed the technique can be applied for short term forecasts analysis of episodes long term simulation of past years for mapping for emission reduction analyses the method is expected to work only for primary pm because primary emissions of pm are taken into account in the methodology for ozone and more generally secondary pollutants it could be less straightforward since the change of chemical regime and the voc volatile organic compound chemistry will be not sufficiently considered in the increment technique even through the statistical corrections however the sensitivity to emissions and meteorological changes is partially contained in the regression coefficients through the diversity of situations encountered over the training period 4 conclusions a surrogate model of the ctm chimere has been designed to downscale a coarse horizontal resolution simulation 0 50 0 25 to a finer mesh 0 09375 0 046875 this first version of the methodology is based on a training process over a 6 months period and applied over the subsequent 6 months with an evaluation against the performances of the raw chimere simulation at the highest resolution definitively our methodology is able to capture the main patterns and evolution of daily concentrations for the main pollutants and the gain in computing time is very important since the costliest step of the simulation process is by passed performances based on an evaluation against observations are similar to those obtained by the raw model simulations with chimere at high resolution performances are even sometimes better certainly due to smoothing effects that suppress some overshoots due to the sensitivity of the model to the parameterizations of the eddy diffusion indeed the vertical diffusion is a very sensitive parameter in ctms this coefficient is so sensitive that it is caped in all models and this bounding already results from a kind of learning process as it derives from the experience of the model developer during the calibration phase in its current state the methodology can work for short term air quality forecasts case studies and air quality concentration mapping to extend its use to emission reduction assessment the model could be improved particularly for ozone by i implementing additional variables available in the chimere pre processing ii increasing the order of the linear regression or ii the use of multivariable regressions however this type of methodology mixing statistics physics and chemistry is promising and could be run on mono processor devices and gain a lot of computing time while the core chimere simulation step 2 can last several days depending on the number of cores and resolution only few minutes is sufficient to perform a high resolution simulation with the increment technique therefore a minimum of 100 1000 less computing times is expected for this step the training process with nco procedures lasts about 3 h in our case to fit the regression parameters and this could be optimized with other programming languages this could open rooms to new fields of research and operational uses in air quality modelling that was so far limited by computing time these findings will also encourage the chimere development team to parallelize the chimere preprocessing step 1 to fully take advantage of this technique acknowledgements this work is partly funded by the french ministry in charge of ecology mtes the ebas database has largely been funded by the un ece clrtap emep amap and through nilu internal resources specific developments have been possible due to projects like eusaar eu fp5 ebas web interface ebas online norwegian research council infra upgrading of database platform and htap european commission dg env import and export routines to build a secondary repository in support of www htap org appendix 1 error statistics used to evaluate model performance m and o refer respectively with model and observations data and n is the number of observations bias m o with m 1 n i 1 n m i and o 1 n i 1 n o i root mean square error r m s e 1 n i 1 n m i o i 2 correlation coefficient r i 1 n m i m o i o i 1 n m i m 2 i 1 n o i o 2 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 02 017 
26229,flood inundation modelling is essential for understanding flood dynamics and implementing flood risk management despite its increasing demands it remains computationally expensive to perform 2 d flood inundation simulations at large scales or fine spatial resolutions this study presents a simple and novel algorithm for 2 d flood inundation modelling called the automatic domain updating method which dynamically updates the simulation domain of a 2 d flood inundation model the method limits the simulation domain where governing equations are solved to flooded areas in each time step by tracking the location of wet and dry interfaces the proposed algorithm was tested in an actual river basin for different types of flooding the results show that the adu method regulates the simulation domain to flooded areas and slashes the computational time for grid based 2 d flood inundation simulations typically by 5 20 with correspond to the ratio of flooded area to the whole simulation domain keywords 2 dimensional flood inundation modelling dynamic cell automation fast computation 1 introduction a flood is one of the most serious natural disasters and causes extensive damage to people and property for predicting the expected flooding and reducing resultant damage comprehensive flood inundation models have been developed and applied to flood risk assessment and real time flood forecasting the computational burden of flood inundation simulations is a key challenge to realize these countermeasures in flood risk assessment flood inundation models need to be run under rainfall discharge at various return periods and with various spatiotemporal rainfall patterns or hydrographs e g apel et al 2006 ernst et al 2010 tanaka et al 2015 for the timely issuance of evacuation warnings it is essential to conduct flood inundation simulations as quickly as possible with respect to meeting these practical demands a number of studies have proposed computationally efficient flood inundation modelling approaches the most straightforward approach to saving computational time of 2 d shallow water equations is the parallelization of simulation neal et al 2009 sanders et al 2010 yu 2010 an alternative to parallelization is the simplification of shallow water equations bates et al 2010 proposed a numerically efficient approximation called the local inertial formulation by adding the local inertial term to the diffusive wave equation that has been widely used for large scale flood modelling this formulation supports fast flood simulation by allowing larger time increment than an explicit diffusive wave model and realizes 2 d flood simulation without critical reduction of accuracy in some specific applications such as flood risk assessment or the estimation of maximum flood extent 2 d shallow water equations are not necessarily required for these applications a simplified modelling of the flood propagation process has been proposed krupka et al 2007 proposed the rapid flood inundation model that fills the flooded water volume to floodplain cells within the parameter of maximum storage until the entire volume is distributed an improvement to this model was later proposed by liu et al 2014 zhang et al 2014 proposed another flood inundation algorithm for estimating the final stage of the inundation extent from river reaches by identifying connected floodplain cells at a lower elevation than those next to river reaches a priori ghimire et al 2013 and liu et al 2014 developed 2 d flood inundation models based on cellular automata that distribute the water volume of a particular inundated cell to neighboring cells by combining manning s equation with a cellular automaton that minimizes the water surface elevation difference these studies enabled the direct utilization of high resolution dem such as at 5 m or 10 m by avoiding from solving shallow water equations these types of simplified flood propagation processes are quite effective when detailed information on flood processes is not necessary however the use of the momentum equation based on the shallow water equations are still important not only for describing detailed flood dynamics but also for reducing the uncertainty of a prediction due to pre fixed flood propagation scenarios and additional model parameters for simplicity to address both the accuracy and efficiency issues recent flood inundation modelling studies have proposed controlling the spatial resolution by such means as nesting system e g nguyen et al 2016 tanaka et al 2015 a coarsening approach yu and lane 2006 chen et al 2012 and adaptive grid techniques liang et al 2008 in these studies the shallow water equations are solved at coarser spatial resolutions in relatively irrelevant areas this reduces the computational burden by decreasing the number of computed cells to further improve the computational efficiency of these models yamaguchi et al 2007 proposed a dynamic domain defining method dynamic ddm that dynamically changes the simulation domain during the 2 d flood inundation simulation this method divides the domain as a whole into several blocks the simulation domain is initially limited to cells within blocks nearby river reaches and is then expanded to neighboring blocks if a boundary cell of a block is flooded the advantages of this concept are that it reduces computational time spent solving shallow water equations and it is applicable to various types of flood inundation models such as those mentioned above however this method still consumes computational time for dry check at all the cells within flooded blocks and for checking inundation of boundary cells in all flooded blocks probably for simpler implementation the latter significantly increases when the size of a unit block is small because the number of boundary cells becomes large thus they concluded that the appropriate block size depends on the scale of flood area as a more flexible and straightforward approach to the dynamical setting of a simulation domain this study presents the automatic domain updating adu method which controls the simulation domain at the cellular level similarly to yamaguchi et al 2007 the adu method skips to solve shallow water equations or their approximations the local inertial equations in this study for non flooded cells however simply checking to ascertain whether every cell is flooded repeatedly may take large time because flood extent is usually much smaller than the total simulation domain expect at around flood peak time the key concept of the adu method is to check the boundary cells of only a flooded area and the method automatically updates the location of the boundary between wet and dry areas this strategy does not require any alternation of governing equations and spatial resolutions therefore this is applicable to any type of 2 d flood inundation model and is free from the block size issue furthermore this can be combined with parallelization techniques in this study the proposed adu method is applied to two flood scenarios in the yodo river basin 8 240 km2 of japan and the improvement of computational efficiency from the simple dry check is demonstrated 1 1 2 dimensional flood inundation modelling flood inundation modelling has been widely developed in a large number of studies in order to meet high social and scientific demands flood inundation models are designed based on the scale of the target phenomena the model at the largest scale is oriented to global flood modelling that seeks to clarify the potential flood extent in large scale floodplains all over the world paiva et al 2011 yamazaki et al 2011 for analyzing more detailed behavior of flooding most studies have developed the propagation of river overflow with 2 d forms of shallow water equations e g o brien et al 1993 nguyen et al 2016 lee et al 2016 or one with approximated forms such as local inertial equations bates et al 2010 and diffusive kinematic wave equations although there are a variety of numerical solvers for these equations the basic structure that solves the momentum equations and then the mass balance equation is common among most explicit finite difference methods for fast flood inundation computation this study employs the following 2 d local inertial equations 1 q x t g h h z x g n 2 q x q x h 7 3 0 2 q y t g h h z y g n 2 q y q y h 7 3 0 3 h t q x x q y y q o where t is the time x and y are the horizontal and vertical directions in the rectangular coordinate system respectively q x and q y are the discharge per unit width in the x and y directions respectively h is the water depth z is the elevation n is the manning s roughness coefficient and q o is the overflow discharge from river cells per unit cell width following almeida et al 2012 the momentum equations are solved to calculate the discharge per unit width in the x and y directions then the mass balance equation is solved to update the water depth this is the case for most explicitly discretized forms of shallow water equations are solved using these two steps it should be noted that the adu method proposed below is applicable to any type of explicit finite difference method of shallow water equations and their approximations 3 algorithm of the adu method the most time consuming and meaningless procedure in 2 d flood inundation modelling is solving equations in non flooded cells one typical countermeasure for this is to check whether a cell is flooded and if not to skip solving equations this is simple and effective way to speed up the simulation however it still takes a lot of time to repeat this dry check process for all non flooded cells at every time step particularly when flood area is far smaller than the whole simulation domain in order to skip this we propose the adu method which limits a simulation domain to a flooded area and its surrounding cells and there solve the governing equations the surrounding cells which are the boundaries of a flooded area are called flood checker cells in this study a simulation domain is dynamically controlled by updating flood checker cells following the expansion of flooded area its concrete procedures are as follows 1 to allocate arrays of water depth cells for calculating the mass balance equation and intermediate discharge nodes for calculating the momentum equations h calc and q calc respectively having the same size as all the n m cells over the entire simulation domain see fig 1 a for a schematic drawing of h calc 2 to register potential flooded cells nodes that could be inundated in the next time step e g ones surrounding river cells in the river overflow simulation to h calc and q calc as described with yellow cells in fig 1 a n h calc and n q calc denote the number of registered cells nodes 3 to prepare an array of flood checker cells h fc and copy the water depth cells registered to h calc to h fc see fig 1 a where n fc denotes the number of flood checker cells 4 to solve the momentum equations at n q calc discharge nodes registered to q calc and the mass balance equation for n h calc water depth cells registered to h calc for one time step 5 to check whether each element of flood checker cells h fc is inundated or not 6 if a particular cell in flood checker cells h fc is inundated to exclude the cell from h fc blue cells in fig 1 b and to register neighboring cells to flood checker cells h fc red cells in fig 1 b and water depth cells for calculation h calc and intermediate nodes to discharge nodes where n fc n h calc and n q calc are also updated as described in fig 1 c and 7 to repeat steps 4 to 6 until the end of the simulation by introducing flood checker cells the number of cells for dry check becomes much smaller furthermore the cost for evaluating flood checker cells h fc is reduced by excluding flooded cells from h fc after the peak time of flooding the number of flooded cells reduces as flooded area shrinks in step 6 when a particular flood checker cell and all the surrounding cells are not inundated the adu shrinks the simulation domain by excluding the flood checker cells the surrounding cells from h calc and the intermediate nodes from h fc h calc and q calc respectively after excluding the flood checker cells from the list the surrounding cells inside the flood area those included in h calc are registered on the list of flood checker cells h fc see fig 2 the algorithms between dry check and the adu method for reducing computational time were investigated denote that computation time for dry check per unit cell is t fd ones for solving momentum and mass balance equations at one cell are t mo and t mb respectively then the total simulation time over one time step in typical flood modelling with a simple dry check t sdc is 4 t sdc t fd n m t mo n mo t mb n mb where n mo is the number of intermediate discharge nodes with flooded cell s and n mb is the number of cells connected to these nodes by definition n mo n q calc and n mb n h calc on the other hand denoting the computation times for including and excluding a flooded cell are t i and t e respectively then the total simulation time over one time step in the adu method t adu is 5 t adu t mo n q calc t mb n h calc t i Œ± n fc t e 1 Œ± n fc where Œ± 0 Œ± 1 is the ratio of the number of flooded cells to n fc therefore the reduction of computation time by the adu method is 6 t sdc t adu t fd n m t i Œ± t e 1 Œ± n fc pseudocodes for simple dry check and the adu are illustrated in fig 3 a and b respectively the dry check in the loop l c in fig 3 a costs the first term on the right hand side of eq 6 while the adu method costs the second term corresponding to update of boundary cells see fig 3 b as the result the reduction of computational time shown as the left hand side of eq 6 is assumed to be positive because n fc n m e g even if the whole area of n m is inundated the size of flood checker cells is only 2 n 2 m 2 if time for including excluding process t i Œ± t e 1 Œ± is not far larger than one for dry check time t fd this assumption is verified in the following applications in the adu method solving governing equations in flood area see the loop l s in fig 1 b was performed in parallel because solving the equations is independent at each cell the difference of computational time between the adu method and the dynamic ddm was also analyzed dynamic ddm costs the following computational expenses over one time step yamaguchi et al 2007 7 t ddm t fd n in n sub t e n in n sub t mo n q calc t mb n h calc t i n bd n sub where n sub is the number of flood blocks and n in and n bd are the number of inside and boundary cells per unit flood block respectively dynamic ddm improved the computational efficiency from the first term of eq 4 to that of eq 7 i e it reduces the range of a dry check process from the whole computation domain to flooded blocks compared with dynamic ddm the adu method further reduces the computation time by 8 t ddm t adu t fd n in n sub t e n in n sub 1 Œ± n fc t i n bd n sub Œ± n fc pseudocode for dynamic ddm is schematized in fig 3 c dynamic ddm still requires dry check process within all the flooded blocks described as the loop l c in fig 3 c which needs additional cost of t fd n in n sub on the other hand the adu method does not require any dry check process because the adu method exactly regulates a simulation domain to flooded areas the second and third terms of eq 8 shows that the adu method potentially takes longer time for simulation due to the control of a simulation domain when the number of boundary cells n fc is larger than all the boundary cells of flooded blocks n in n sub however it hardly occurs as long as flooded area is not diverse in typical flooding n fc is expected not to be larger than n in n sub or n bd n sub see fig 4 therefore the adu method is expected to more flexibly control simulation domain without the block size issue and critical addition of computational burden 4 application of the adu method 4 1 study area and simulation conditions the proposed adu method was applied to 2 d flood inundation modelling in a case study of two types of fluvial flooding in the kyoto city area 365 km2 of the yodo river basin 8 240 km2 see fig 5 the kyoto city area has three major tributary rivers the katsura the uji and the kizu rivers in this area a 2 d flood inundation model imcr see tanaka et al 2015 for details was constructed the imcr adopts the rectangular coordinate system and each cell is classified as a river cell or a floodplain cell where 1 d and 2 d local inertial equations are applied respectively the boundary river discharge for the three tributaries and the lateral inflow to the rivers within the simulation domain were provided by a kinematic wave based distributed rainfall runoff model 1k dhm see tanaka and tachikawa 2015 for details the adu method was tested in two types of flooding a bank breaking scenario causing local flooding and a severe river overflow scenario in which a large part of the domain is inundated in the bank breaking scenario rainfall data of typhoon no 18 in 2013 hereinafter referred to as the t18 event was input and at the peak time of the scenario the river dike was cut at two points where the river dike height was set to ground level in the severe river overflow scenario since this area has not experienced large scale flooding in historical storm events rainfall intensity of a frontal heavy rainfall event was uniformly intensified so that the total four day basin rainfall amounts to 700 mm hereinafter referred to as the r700 event the 2 d flood model with the adu method was compiled with intel c compiler 14 and performed on a linux machine on intel xeon processor cpu e5 2680 2 8 ghz 4 2 results the spatial distributions of inundated depth at the same time steps are shown in fig 6 which shows the flood propagation from and regression into bank breaking points during the storm screenshots of the type of each cell i e river cells flooded cells flood checker cells non flooded cells during the flooding period are shown in fig 7 2 d flood inundations simulation were performed on the red flood checker cells and light blue cells flooded cells figs 6 and 7 clearly show that the simulation domain was successfully controlled to the flooded area and its boundary cells by using the adu method the simulation domain was reduced to the minimum area to be calculated figs 8 and 9 show inundated depth distribution and screenshots of the type of each cell respectively for the r700 event this intensified event caused severe river overflow at many points and caused large scale inundation computation times of the two events when using the 2 d inundation model with the adu method and a simple dry check are compared in table 1 the results of both events clearly show the improved computational efficiency by implementing the adu method comparing the two events the computational efficiency of the t18 event is higher than that of the r700 event calculation time was reduced to 5 37 and 19 8 respectively fig 10 shows the relationship between the ratio of the number of flooded cells and the ratio of 1 h simulation time of the models with and without the adu method computation time increases as the ratio of the number of flooded cells increases red points and vice versa blue points in the t18 event flooded water did not expand as large as the r700 event and receded more quickly therefore the degree of decrease in computation time blue points is larger for the t18 event this indicated that the computational efficiency of the adu method is much higher at larger scales 4 3 discussions the simulation times of the momentum and mass balance equations themselves were similar between a simple dry check and the adu method because the number of simulated i e flooded cells was the same with each other the main cause of the improved computational efficiency was the dry check process the computation times of solving momentum mass balance equations and dry check averaged between the two flood cases per time step per cell were at the same order t mo t mb 5 88 10 8 s and t fd 3 95 10 8 s respectively therefore longer computation time of dry check in the whole simulation is attributed to that flood extent is much smaller than the dry area at most time steps the ratio of the number of flooded cells to all the cells during the simulation period i 1 n t n h calc i n q calc i n m i is each time step n t is the total number of time steps was only 3 98 and 12 4 in the t18 and r700 events respectively which often occurs in flood simulation at basin to global scales the calculation time of including excluding flood checker cells per time step per cell was actually smaller than that of dry check t fc 2 62 10 8 s on average between the two events more importantly the number of flood checker cells in the adu method was much smaller than that of dry check the ratios of the number of flood checker cells to all the cells n fc n fd for the t18 and r700 events were 0 73 and 1 98 respectively from these results and the right hand side of eq 6 it is indicated that the cost for updating flood checker cells the second term was much smaller than that for dry check the first term thus the total simulation time dramatically decreased the impact of parallelization on the improvement of computational efficiency by the adu method was also investigated parallelization was implemented by simply applying openmp to the loop of the 2 d momentum and mass balance equations respectively the loop l s in fig 3 b under openmp parallelization with eight cores the calculation time of the flood model with the adu method was smaller than that with the simple dry check by 19 9 and 38 5 for the t18 and r700 events respectively it is indicated that the adu method improves the computational efficiency under parallelization as well the smaller reduction of calculation time for parallel simulation than serial one is attributed to that at the scale of the study area the number of simulated cells and nodes in the adu method was small 5 conclusions flood inundation modelling is essential for rational flood risk management and early flood warning systems the computational efficiency of the model used is a key challenge in implementing these countermeasures in previous studies a reduction in the computational cost of flood inundation models has been addressed from various aspects such as fast numerical schemes and the simplification of flood simulation processes in addition to them optimization of the flood simulation domain results in a significant improvement in the computational efficiency of flood inundation simulations as a generalized approach to the optimization of simulation domain this study proposed an automatic domain updating adu method that limits the simulation domain to the flooded area and its surrounding cells to dynamically update a simulation domain without the critical increase of the simulation cost the adu updated the boundary cells of a flooded area flood checker cells this enabled the flexible control of simulation domain without the block size issue seen in a literature case studies for two flood events showed that the additional cost for updating flood checker cells is much less than the total simulation time thus the computational efficiency of the 2 d flood inundation simulation was dramatically improved the efficiency of the adu method becomes higher for larger scale scenarios in which the ratio of the flooded area becomes relatively small compared to the simulation domain as a whole we will apply the adu method to larger river basins in future research acknowledgement river cross section data of the katsura uji and kizu rivers were provided by the yodo river office kinki regional development bureau 
26229,flood inundation modelling is essential for understanding flood dynamics and implementing flood risk management despite its increasing demands it remains computationally expensive to perform 2 d flood inundation simulations at large scales or fine spatial resolutions this study presents a simple and novel algorithm for 2 d flood inundation modelling called the automatic domain updating method which dynamically updates the simulation domain of a 2 d flood inundation model the method limits the simulation domain where governing equations are solved to flooded areas in each time step by tracking the location of wet and dry interfaces the proposed algorithm was tested in an actual river basin for different types of flooding the results show that the adu method regulates the simulation domain to flooded areas and slashes the computational time for grid based 2 d flood inundation simulations typically by 5 20 with correspond to the ratio of flooded area to the whole simulation domain keywords 2 dimensional flood inundation modelling dynamic cell automation fast computation 1 introduction a flood is one of the most serious natural disasters and causes extensive damage to people and property for predicting the expected flooding and reducing resultant damage comprehensive flood inundation models have been developed and applied to flood risk assessment and real time flood forecasting the computational burden of flood inundation simulations is a key challenge to realize these countermeasures in flood risk assessment flood inundation models need to be run under rainfall discharge at various return periods and with various spatiotemporal rainfall patterns or hydrographs e g apel et al 2006 ernst et al 2010 tanaka et al 2015 for the timely issuance of evacuation warnings it is essential to conduct flood inundation simulations as quickly as possible with respect to meeting these practical demands a number of studies have proposed computationally efficient flood inundation modelling approaches the most straightforward approach to saving computational time of 2 d shallow water equations is the parallelization of simulation neal et al 2009 sanders et al 2010 yu 2010 an alternative to parallelization is the simplification of shallow water equations bates et al 2010 proposed a numerically efficient approximation called the local inertial formulation by adding the local inertial term to the diffusive wave equation that has been widely used for large scale flood modelling this formulation supports fast flood simulation by allowing larger time increment than an explicit diffusive wave model and realizes 2 d flood simulation without critical reduction of accuracy in some specific applications such as flood risk assessment or the estimation of maximum flood extent 2 d shallow water equations are not necessarily required for these applications a simplified modelling of the flood propagation process has been proposed krupka et al 2007 proposed the rapid flood inundation model that fills the flooded water volume to floodplain cells within the parameter of maximum storage until the entire volume is distributed an improvement to this model was later proposed by liu et al 2014 zhang et al 2014 proposed another flood inundation algorithm for estimating the final stage of the inundation extent from river reaches by identifying connected floodplain cells at a lower elevation than those next to river reaches a priori ghimire et al 2013 and liu et al 2014 developed 2 d flood inundation models based on cellular automata that distribute the water volume of a particular inundated cell to neighboring cells by combining manning s equation with a cellular automaton that minimizes the water surface elevation difference these studies enabled the direct utilization of high resolution dem such as at 5 m or 10 m by avoiding from solving shallow water equations these types of simplified flood propagation processes are quite effective when detailed information on flood processes is not necessary however the use of the momentum equation based on the shallow water equations are still important not only for describing detailed flood dynamics but also for reducing the uncertainty of a prediction due to pre fixed flood propagation scenarios and additional model parameters for simplicity to address both the accuracy and efficiency issues recent flood inundation modelling studies have proposed controlling the spatial resolution by such means as nesting system e g nguyen et al 2016 tanaka et al 2015 a coarsening approach yu and lane 2006 chen et al 2012 and adaptive grid techniques liang et al 2008 in these studies the shallow water equations are solved at coarser spatial resolutions in relatively irrelevant areas this reduces the computational burden by decreasing the number of computed cells to further improve the computational efficiency of these models yamaguchi et al 2007 proposed a dynamic domain defining method dynamic ddm that dynamically changes the simulation domain during the 2 d flood inundation simulation this method divides the domain as a whole into several blocks the simulation domain is initially limited to cells within blocks nearby river reaches and is then expanded to neighboring blocks if a boundary cell of a block is flooded the advantages of this concept are that it reduces computational time spent solving shallow water equations and it is applicable to various types of flood inundation models such as those mentioned above however this method still consumes computational time for dry check at all the cells within flooded blocks and for checking inundation of boundary cells in all flooded blocks probably for simpler implementation the latter significantly increases when the size of a unit block is small because the number of boundary cells becomes large thus they concluded that the appropriate block size depends on the scale of flood area as a more flexible and straightforward approach to the dynamical setting of a simulation domain this study presents the automatic domain updating adu method which controls the simulation domain at the cellular level similarly to yamaguchi et al 2007 the adu method skips to solve shallow water equations or their approximations the local inertial equations in this study for non flooded cells however simply checking to ascertain whether every cell is flooded repeatedly may take large time because flood extent is usually much smaller than the total simulation domain expect at around flood peak time the key concept of the adu method is to check the boundary cells of only a flooded area and the method automatically updates the location of the boundary between wet and dry areas this strategy does not require any alternation of governing equations and spatial resolutions therefore this is applicable to any type of 2 d flood inundation model and is free from the block size issue furthermore this can be combined with parallelization techniques in this study the proposed adu method is applied to two flood scenarios in the yodo river basin 8 240 km2 of japan and the improvement of computational efficiency from the simple dry check is demonstrated 1 1 2 dimensional flood inundation modelling flood inundation modelling has been widely developed in a large number of studies in order to meet high social and scientific demands flood inundation models are designed based on the scale of the target phenomena the model at the largest scale is oriented to global flood modelling that seeks to clarify the potential flood extent in large scale floodplains all over the world paiva et al 2011 yamazaki et al 2011 for analyzing more detailed behavior of flooding most studies have developed the propagation of river overflow with 2 d forms of shallow water equations e g o brien et al 1993 nguyen et al 2016 lee et al 2016 or one with approximated forms such as local inertial equations bates et al 2010 and diffusive kinematic wave equations although there are a variety of numerical solvers for these equations the basic structure that solves the momentum equations and then the mass balance equation is common among most explicit finite difference methods for fast flood inundation computation this study employs the following 2 d local inertial equations 1 q x t g h h z x g n 2 q x q x h 7 3 0 2 q y t g h h z y g n 2 q y q y h 7 3 0 3 h t q x x q y y q o where t is the time x and y are the horizontal and vertical directions in the rectangular coordinate system respectively q x and q y are the discharge per unit width in the x and y directions respectively h is the water depth z is the elevation n is the manning s roughness coefficient and q o is the overflow discharge from river cells per unit cell width following almeida et al 2012 the momentum equations are solved to calculate the discharge per unit width in the x and y directions then the mass balance equation is solved to update the water depth this is the case for most explicitly discretized forms of shallow water equations are solved using these two steps it should be noted that the adu method proposed below is applicable to any type of explicit finite difference method of shallow water equations and their approximations 3 algorithm of the adu method the most time consuming and meaningless procedure in 2 d flood inundation modelling is solving equations in non flooded cells one typical countermeasure for this is to check whether a cell is flooded and if not to skip solving equations this is simple and effective way to speed up the simulation however it still takes a lot of time to repeat this dry check process for all non flooded cells at every time step particularly when flood area is far smaller than the whole simulation domain in order to skip this we propose the adu method which limits a simulation domain to a flooded area and its surrounding cells and there solve the governing equations the surrounding cells which are the boundaries of a flooded area are called flood checker cells in this study a simulation domain is dynamically controlled by updating flood checker cells following the expansion of flooded area its concrete procedures are as follows 1 to allocate arrays of water depth cells for calculating the mass balance equation and intermediate discharge nodes for calculating the momentum equations h calc and q calc respectively having the same size as all the n m cells over the entire simulation domain see fig 1 a for a schematic drawing of h calc 2 to register potential flooded cells nodes that could be inundated in the next time step e g ones surrounding river cells in the river overflow simulation to h calc and q calc as described with yellow cells in fig 1 a n h calc and n q calc denote the number of registered cells nodes 3 to prepare an array of flood checker cells h fc and copy the water depth cells registered to h calc to h fc see fig 1 a where n fc denotes the number of flood checker cells 4 to solve the momentum equations at n q calc discharge nodes registered to q calc and the mass balance equation for n h calc water depth cells registered to h calc for one time step 5 to check whether each element of flood checker cells h fc is inundated or not 6 if a particular cell in flood checker cells h fc is inundated to exclude the cell from h fc blue cells in fig 1 b and to register neighboring cells to flood checker cells h fc red cells in fig 1 b and water depth cells for calculation h calc and intermediate nodes to discharge nodes where n fc n h calc and n q calc are also updated as described in fig 1 c and 7 to repeat steps 4 to 6 until the end of the simulation by introducing flood checker cells the number of cells for dry check becomes much smaller furthermore the cost for evaluating flood checker cells h fc is reduced by excluding flooded cells from h fc after the peak time of flooding the number of flooded cells reduces as flooded area shrinks in step 6 when a particular flood checker cell and all the surrounding cells are not inundated the adu shrinks the simulation domain by excluding the flood checker cells the surrounding cells from h calc and the intermediate nodes from h fc h calc and q calc respectively after excluding the flood checker cells from the list the surrounding cells inside the flood area those included in h calc are registered on the list of flood checker cells h fc see fig 2 the algorithms between dry check and the adu method for reducing computational time were investigated denote that computation time for dry check per unit cell is t fd ones for solving momentum and mass balance equations at one cell are t mo and t mb respectively then the total simulation time over one time step in typical flood modelling with a simple dry check t sdc is 4 t sdc t fd n m t mo n mo t mb n mb where n mo is the number of intermediate discharge nodes with flooded cell s and n mb is the number of cells connected to these nodes by definition n mo n q calc and n mb n h calc on the other hand denoting the computation times for including and excluding a flooded cell are t i and t e respectively then the total simulation time over one time step in the adu method t adu is 5 t adu t mo n q calc t mb n h calc t i Œ± n fc t e 1 Œ± n fc where Œ± 0 Œ± 1 is the ratio of the number of flooded cells to n fc therefore the reduction of computation time by the adu method is 6 t sdc t adu t fd n m t i Œ± t e 1 Œ± n fc pseudocodes for simple dry check and the adu are illustrated in fig 3 a and b respectively the dry check in the loop l c in fig 3 a costs the first term on the right hand side of eq 6 while the adu method costs the second term corresponding to update of boundary cells see fig 3 b as the result the reduction of computational time shown as the left hand side of eq 6 is assumed to be positive because n fc n m e g even if the whole area of n m is inundated the size of flood checker cells is only 2 n 2 m 2 if time for including excluding process t i Œ± t e 1 Œ± is not far larger than one for dry check time t fd this assumption is verified in the following applications in the adu method solving governing equations in flood area see the loop l s in fig 1 b was performed in parallel because solving the equations is independent at each cell the difference of computational time between the adu method and the dynamic ddm was also analyzed dynamic ddm costs the following computational expenses over one time step yamaguchi et al 2007 7 t ddm t fd n in n sub t e n in n sub t mo n q calc t mb n h calc t i n bd n sub where n sub is the number of flood blocks and n in and n bd are the number of inside and boundary cells per unit flood block respectively dynamic ddm improved the computational efficiency from the first term of eq 4 to that of eq 7 i e it reduces the range of a dry check process from the whole computation domain to flooded blocks compared with dynamic ddm the adu method further reduces the computation time by 8 t ddm t adu t fd n in n sub t e n in n sub 1 Œ± n fc t i n bd n sub Œ± n fc pseudocode for dynamic ddm is schematized in fig 3 c dynamic ddm still requires dry check process within all the flooded blocks described as the loop l c in fig 3 c which needs additional cost of t fd n in n sub on the other hand the adu method does not require any dry check process because the adu method exactly regulates a simulation domain to flooded areas the second and third terms of eq 8 shows that the adu method potentially takes longer time for simulation due to the control of a simulation domain when the number of boundary cells n fc is larger than all the boundary cells of flooded blocks n in n sub however it hardly occurs as long as flooded area is not diverse in typical flooding n fc is expected not to be larger than n in n sub or n bd n sub see fig 4 therefore the adu method is expected to more flexibly control simulation domain without the block size issue and critical addition of computational burden 4 application of the adu method 4 1 study area and simulation conditions the proposed adu method was applied to 2 d flood inundation modelling in a case study of two types of fluvial flooding in the kyoto city area 365 km2 of the yodo river basin 8 240 km2 see fig 5 the kyoto city area has three major tributary rivers the katsura the uji and the kizu rivers in this area a 2 d flood inundation model imcr see tanaka et al 2015 for details was constructed the imcr adopts the rectangular coordinate system and each cell is classified as a river cell or a floodplain cell where 1 d and 2 d local inertial equations are applied respectively the boundary river discharge for the three tributaries and the lateral inflow to the rivers within the simulation domain were provided by a kinematic wave based distributed rainfall runoff model 1k dhm see tanaka and tachikawa 2015 for details the adu method was tested in two types of flooding a bank breaking scenario causing local flooding and a severe river overflow scenario in which a large part of the domain is inundated in the bank breaking scenario rainfall data of typhoon no 18 in 2013 hereinafter referred to as the t18 event was input and at the peak time of the scenario the river dike was cut at two points where the river dike height was set to ground level in the severe river overflow scenario since this area has not experienced large scale flooding in historical storm events rainfall intensity of a frontal heavy rainfall event was uniformly intensified so that the total four day basin rainfall amounts to 700 mm hereinafter referred to as the r700 event the 2 d flood model with the adu method was compiled with intel c compiler 14 and performed on a linux machine on intel xeon processor cpu e5 2680 2 8 ghz 4 2 results the spatial distributions of inundated depth at the same time steps are shown in fig 6 which shows the flood propagation from and regression into bank breaking points during the storm screenshots of the type of each cell i e river cells flooded cells flood checker cells non flooded cells during the flooding period are shown in fig 7 2 d flood inundations simulation were performed on the red flood checker cells and light blue cells flooded cells figs 6 and 7 clearly show that the simulation domain was successfully controlled to the flooded area and its boundary cells by using the adu method the simulation domain was reduced to the minimum area to be calculated figs 8 and 9 show inundated depth distribution and screenshots of the type of each cell respectively for the r700 event this intensified event caused severe river overflow at many points and caused large scale inundation computation times of the two events when using the 2 d inundation model with the adu method and a simple dry check are compared in table 1 the results of both events clearly show the improved computational efficiency by implementing the adu method comparing the two events the computational efficiency of the t18 event is higher than that of the r700 event calculation time was reduced to 5 37 and 19 8 respectively fig 10 shows the relationship between the ratio of the number of flooded cells and the ratio of 1 h simulation time of the models with and without the adu method computation time increases as the ratio of the number of flooded cells increases red points and vice versa blue points in the t18 event flooded water did not expand as large as the r700 event and receded more quickly therefore the degree of decrease in computation time blue points is larger for the t18 event this indicated that the computational efficiency of the adu method is much higher at larger scales 4 3 discussions the simulation times of the momentum and mass balance equations themselves were similar between a simple dry check and the adu method because the number of simulated i e flooded cells was the same with each other the main cause of the improved computational efficiency was the dry check process the computation times of solving momentum mass balance equations and dry check averaged between the two flood cases per time step per cell were at the same order t mo t mb 5 88 10 8 s and t fd 3 95 10 8 s respectively therefore longer computation time of dry check in the whole simulation is attributed to that flood extent is much smaller than the dry area at most time steps the ratio of the number of flooded cells to all the cells during the simulation period i 1 n t n h calc i n q calc i n m i is each time step n t is the total number of time steps was only 3 98 and 12 4 in the t18 and r700 events respectively which often occurs in flood simulation at basin to global scales the calculation time of including excluding flood checker cells per time step per cell was actually smaller than that of dry check t fc 2 62 10 8 s on average between the two events more importantly the number of flood checker cells in the adu method was much smaller than that of dry check the ratios of the number of flood checker cells to all the cells n fc n fd for the t18 and r700 events were 0 73 and 1 98 respectively from these results and the right hand side of eq 6 it is indicated that the cost for updating flood checker cells the second term was much smaller than that for dry check the first term thus the total simulation time dramatically decreased the impact of parallelization on the improvement of computational efficiency by the adu method was also investigated parallelization was implemented by simply applying openmp to the loop of the 2 d momentum and mass balance equations respectively the loop l s in fig 3 b under openmp parallelization with eight cores the calculation time of the flood model with the adu method was smaller than that with the simple dry check by 19 9 and 38 5 for the t18 and r700 events respectively it is indicated that the adu method improves the computational efficiency under parallelization as well the smaller reduction of calculation time for parallel simulation than serial one is attributed to that at the scale of the study area the number of simulated cells and nodes in the adu method was small 5 conclusions flood inundation modelling is essential for rational flood risk management and early flood warning systems the computational efficiency of the model used is a key challenge in implementing these countermeasures in previous studies a reduction in the computational cost of flood inundation models has been addressed from various aspects such as fast numerical schemes and the simplification of flood simulation processes in addition to them optimization of the flood simulation domain results in a significant improvement in the computational efficiency of flood inundation simulations as a generalized approach to the optimization of simulation domain this study proposed an automatic domain updating adu method that limits the simulation domain to the flooded area and its surrounding cells to dynamically update a simulation domain without the critical increase of the simulation cost the adu updated the boundary cells of a flooded area flood checker cells this enabled the flexible control of simulation domain without the block size issue seen in a literature case studies for two flood events showed that the additional cost for updating flood checker cells is much less than the total simulation time thus the computational efficiency of the 2 d flood inundation simulation was dramatically improved the efficiency of the adu method becomes higher for larger scale scenarios in which the ratio of the flooded area becomes relatively small compared to the simulation domain as a whole we will apply the adu method to larger river basins in future research acknowledgement river cross section data of the katsura uji and kizu rivers were provided by the yodo river office kinki regional development bureau 
