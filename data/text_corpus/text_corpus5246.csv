index,text
26230,spatially distributed time series data support a range of environmental modeling and data research efforts a critical first step to any such effort is acquiring interpolated hydrometeorological data standardized tools to facilitate this process into analyses have not been readily available for watershed scale research here we introduce the observatory for gridded hydrometeorology ogh an open source python library that fills this critical software gap by providing a cyberinfrastructure component to fetch and manage distributed data processed from regional and continental scale gridded hydrometeorology products our approach involves annotating metadata to make gridded data products discoverable and usable within the software enabling interoperability and reproducibility of models that use the data this paper presents the design architecture and application of ogh using four commonly practiced use cases with gridded time series data at watershed scales ogh and its associated annotations are distributed via anaconda cloud within conda forge package repository the tutorial jupyter notebooks for each example use case are available within the freshwater initiative observatory repository https github com freshwater initiative observatory the examples are designed to utilize the compute resources and software libraries provided by hydroshare https www hydroshare org resource 87dc5742cf164126a11ff45c3307fd9d keywords python cloud computing shapefile based data retrieval watershed hydrometeorology 1 introduction gridded data products are extensively used in earth science research king et al 2013 gampe and ludwig 2017 ledesma and futter 2017 social vulnerability analysis cutter et al 2014 and population risk and estimate studies lloyd et al 2017 gridded hydrometeorological data products are produced by interpolating local observations to predetermined spatial temporal resolutions the purpose of developing gridded products is to extend spatial information beyond point locations and provide space and time dimensions to observations such that spatial temporal variability can be analyzed gridded data products also provide a means to compare and validate numerical weather prediction outputs short term forecasts and long term climate change prior studies in hydrometeorology have highlighted the growing usefulness of gridded data products in earth science modeling most recently reviewed in henn et al 2018 in this paper we introduce the observatory for gridded hydrometeorology ogh an open source python toolkit to streamline processes for interacting with gridded hydrometeorological data products at a user defined spatial scale of interest single location to regional watershed designed to support watershed scale science applications this tool fills a model pre processing gap of processing large regional datasets 1000 km2 for smaller scale geographic subsets 1 km2 100 km2 in the conterminous united states conus since the introduction of parameter elevation regressions on independent slopes model prism daly et al 1994 gridded meteorological data products are routinely interpolated using daily measurements from over 20 000 noaa coop observation stations maurer et al 2002 livneh et al 2013 with similar products in development for other regions around the world yanto et al 2017 in these data products each grid cell contains observation interpolated multivariate time series maurer et al 2002 it is common practice for the hydrologic community to incorporate gridded meteorological time series variables as inputs to land surface hydrologic models such as the variable infiltration capacity vic model liang et al 1994 a wealth of modeled land surface hydrologic states e g soil moisture and fluxes e g latent and sensible heat have been developed and used in earth science research livneh et al 2013 livneh et al 2015 in mountainous regions where ground level observation collection is not feasible the weather research forecasting wrf atmospheric model has been downscaled and similarly used to produce hydrologic model outputs however this process inevitably requires bias correction based on observational products salathé et al 2010 2014 recently gridded hydrometeorological data was combined with geo environmental and demographic surveys to yield gridded population data sets and new insight to enhance population modeling resolution and accuracy cutter et al 2014 lloyd et al 2017 it can be expected that gridded data products will continue to increase in abundance and complexity in how they represent the impacts of geography and landscape morphology before the potential usefulness of gridded data products can be realized for watershed scale actionable research several data and metadata access challenges need to be addressed continental scale gridded data products such as those from livneh et al 2013 2015 and salathé et al 2014 are increasingly being published as netcdf files for watershed researchers who are interested in studying physical processes netcdf files used for regional and continental scale gridded products 1000 km2 10 000 km2 contain information that far exceeds the geographic extent needed for local watershed scale research e g 1 100 km2 catchment area adding computational resource burden in exploratory research one alternative data product format is the 1d ascii time series files for a geographically specific gridded cell as observed in livneh et al 2013 2015 and salathé et al 2014 data products 1d ascii time series files are not self described with column names time series dates or value units so annotations are needed in order to perform analyses with files in this format information to locate and use the data files may be confined to elusive publications and documentation files even so the data files may be hosted in management structures for their study convenience e g universal transverse mercator boundaries making manual data retrieval non trivial and not intuitive by human interpretation hence annotating data provenance metadata provenance and file management structure are crucial steps towards making gridded data products findable accessible interoperable and reusable fair wilkinson et al 2016 mons et al 2017 for secondary analyses currently available tools for water data such as waterml wofpy gsflow geoknife offer access to time series of observation data kadlec et al 2015 gardner et al 2018 read et al 2016 however in areas where observations are unavailable such as heterogeneous landscapes at high elevation locations data sparsity can be addressed with krigged and model interpolated data products python libraries such as openclimategis offer access to netcdf gridded data products but these functionalities exclude legacy data sets provided in 1d ascii time series format more importantly aside from data access it is challenging to recognize differences between gridded data product such as aggregation into different gridded cell schemas the temporal resolution time period or long term trends for use in future modeling efforts and model validation operations to streamline the processes needed for interacting with gridded hydrometeorological data products in a fair manner promote use in earth modeling and interdisciplinary domains and support decision making for selecting gridded cells in a study site we designed ogh as an open source python toolkit to select gridded cells in a study site data download spatial temporal analyses and provide data visualization in this paper we describe the design of the ogh python library which contains functions to conduct basic data access and data processing operations to simplify gridded data product use in research users start with an esri shapefile that describes their study site e g huc12 units county boundaries state boundaries to generate a comma separated table that helps with file management of gridded cell data availability across gridded data products users can then conduct data retrieval in parallel from a number of gridded hydrometeorological data products with automated file management for analyses these functions are flexible to integrate new gridded products and publishing standards to address the absence and or variability in describing online gridded data products we propose a set of minimum annotation criteria metadata fields for describing ascii gridded hydrometeorological data products and the decision steps needed to access these gridded datasets in the methods section we describe ogh software design for gridded cell selection and visualization data download spatial temporal calculations and applied statistics functionalities ogh was designed to incorporate climatological and hydrometeorological gridded data products with comparable data structures to ascii and netcdf formats here we emphasize watershed scale applications using 1d ascii time series data products in the results section we demonstrate these functionalities using three watersheds of end member climatologies in conus two high alpine glacierized watershed in the high end of the precipitation gradient in the conus 3000 mm sauk suiattle wa and elwha river basin wa and a desert watershed with a large precipitation gradient from valleys to peaks 200 mm 3500 mm upper rio salado nm we compute precipitation and temperature spatial temporal statistics and exceedance probability calculations using gridded hydrometeorological data products from livneh et al 2013 and salathé et al 2014 ogh v 0 1 11 is publicly accessible at https github com freshwater initiative observatory and available by conda installation 2 methods ogh is a python library designed to perform gridded cell selection data download data processing for desired space time analytics and visualization of spatial temporal data a proposed set of metadata annotations was developed to provide default information about data product capacities using two case study gridded products we provide ogh examples reproducible on hydroshare a cyber infrastructure for sharing data and models heard et al 2014 horsburgh et al 2016 castronova et al 2017 ogh is not dependent on hydroshare though in the example use cases hydroshare is a platform providing dockerized or local server environments for community software including those used by ogh operations to manage compute and store directories of files user workflows scenario use cases and key socio technical needs were developed through key informant interviews diagramming and rapid prototype testing sessions baxter and sommerville 2011 devi et al 2012 four key principles of user centered design and engagement were maintained throughout the rapid prototype testing process described in supplementary table 1 devi et al 2012 2 1 software design we designed ogh using open source python 3 6 programming which is interoperable with major operating systems and computing environments used by personal desktop computers high performance computers and supercomputers ogh functions are written as modular components that leverage classes and methods from a number of python libraries time series analysis from pandas mckinney et al 2011 geospatial analytics from geopandas fiona shapely and matplotlib basemap mckinney 2011 gillies 2007 2011 jordahl et al 2014 and task management from multiprocessing and dask rocklin 2015 operations from these libraries are assembled into scripted functions which can be wrapped into sequential operations or applied in distributed computing practices ogh is intended to perform operations within computing environments where input and output files are managed within data sharing platforms and community data repositories e g hydroshare while hydroshare has many features we make use of the docker or server environment where file storage migration and computation can be performed with multi core resources hydroshare www hydroshare org is a collaborative platform that supports data sharing and model reproducibility in hydrologic research and it provides a cloud computing environment through the consortium of universities for the advancement of hydrologic science inc cuahsi jupyterhub server horsburgh et al 2016 the hydroshare rest api python library hs restclient is used to migrate files contained in hydroshare resources in and out of the jupyterhub docker environments hydroshare rest api python client library 2018 file storage within the computing environment is intended to be temporary storage for the duration of the computations the research workflows presented in this paper are designed to be guided by jupyter notebooks sharable code execution interfaces that operate within the jupyterhub docker environments castronova et al 2017 2 2 gridded data product annotations we annotated seven daily 1 16 6 km gridded data products from three studies published and made available online livneh et al 2013 2015 salathé et al 2014 the datasets published by livneh et al 2013 provided an interpolated climate station meteorology for conus and a meteorology data product that was bias corrected to the columbia river basin regional climatology in the time span from 1915 to 2011 expanding on livneh et al 2013 2015 includes a prism calibrated interpolated climate station meteorology extends from mexico to limited regions of canada however the period has 33 total years less data with a time span of 1950 2013 both interpolated meteorology data products were used to predict macro scale hydrologic fluxes at the 1 16 daily resolution by the vic model the wrf gridded data product provides model downscaled daily precipitation maximum and minimum air temperature and wind speed for the columbia river basin for the period of 1950 2010 salathé et al 2014 the data products can be further differentiated according to their type of analysis and reported spatial coverage table 1 the annotations describe the gridded data products published as ascii files where each file contains the gridded cell historic time series data annotation features include the data set short name information to locate the ascii files information about the file structure and sources of metadata and metadata about the file variables table 2 file locations can be represented or reconstructed given by the web protocol e g ftp https web domain and subdomain decision steps within the subdomain to locate the data file subdirectory e g centroid latitude given the spatial resolution bounding box bins the filename structure and the file format the file structure is described by the variable list left to right column order time series date range temporal resolution file delimiter and the data types and unit increment for each variable full annotations are provided in the ogh meta module 2 2 1 example use cases we present four example use cases in the form of jupyter notebooks to demonstrate the ogh operations fig 1 in the first use case we identify the subset gridded cells of interest for four watershed study sites using the treatgeoself function fig 1a the shapefiles for these watersheds are stored within a public hydroshare resource for ease of collaborative use sauk suiattle river basin available in bandaragoda c 2017 elwha river basin available in beveridge c 2017 and upper rio salado basin available in bandaragoda c 2017 watershed boundaries were defined in arcgis using 12 digit hydrologic unit code polygons from the national watershed boundary database in the second use case the time series data files are retrieved cataloged then summarized for data availability fig 1b in the third use case we focus on the sauk suiattle watershed to determine the monthly meteorological spatial temporal statistics computed using the livneh et al 2013 meteorology versus the salathé et al 2014 wrf model output data products fig 1c finally we compute potential runoff values using the vic hydrologic data product from livneh et al 2013 to approximate the 10 exceedance probability thresholds based on the daily time series in each dataset fig 1d functions introduced in each use case are summarized in supplementary table 2 for illustration five reference locations are used in the sauk suiattle watershed examples one gridded cell is identified at the highest average elevation value 2216 m above sea level two gridded cells were identified at the lowest average elevation value 164 m above sea level the darrington ranger station coop station 451992 is used as the reference source of meteorological observations with data collected at 167 m above sea level from jan 1 1931 through dec 31 2005 sauk river near sauk wa usgs 12198500 used as the reference source of observed streamflow discharge using data collected between jan 1 1950 through dec 31 2011 konrad et al 2012 2 2 2 general workflow and required files workflows for the use cases executed in the jupyterhub environment fig 2 are illustrated in detail in fig 1 the general workflow begins with three hydroshare resources as sources of input files fig 2 resource a a hydroshare resource that contains a jupyter notebooks to execute code for each example use case presented in this paper resource b a hydroshare resource with a user defined shapefile representing the region of interest e g a watershed and resource c a file of point locations describing conus gridded cell centroids only pre requisite for mapping watershed centroids from the web page for hydroshare resource a the jupyter notebooks are launched in the jupyterhub docker environment wherein the hydroshare rest api functions migrate in requisite data files from resource b and c fig 2 use case notebooks 1 through 4 progress through ogh operations in fig 2 identify watershed gridded cell centroids map watershed centroids download and display data availability data download summarize monthly meteorology data processing and compute exceedance probabilities another form of data processing fig 2 each use case notebook produces output data files plots data visualizations and finally migrates these output to new shareable hydroshare resources to conclude the use case demonstration 2 2 3 map watershed gridded cell centroids for each of the three example watersheds we generate a mapping file with the gridded cell centroids that spatially intersect these study sites fig 1a shapefiles were transformed into the 1984 world geodetic system wgs84 lat long coordinates system as the standard projection the study site was given a buffer region default buffer distance of 0 06 to include adjacent gridded cells conus 1 16 i e 0 06250 gridded cell esri shapefile identifies each gridded cell by the 5 digit centroid latitude longitude livneh et al 2013 livneh 2017 average elevation in the gridded cell in meters above sea level are based on the conus digital elevation model described in livneh et al 2013 the output from this use case includes three mapping files each denoting the latitude longitude and average elevation within the gridded cell other outputs include spatial visualizations of the study site maps and the elevation gradient among the gridded cells and plots showing the data for select grid cell traces 2 2 4 summarize data download and availability the mapping file guides data download from the seven gridded data products fig 1b target gridded cell files identified within the mapping file are web requested using data download wrapper functions e g the getdailymet livneh2013 request operations are distributed using multiprocessing pool operations downloaded files are cataloged into the mapping file using addcatalogtomap data availability is determined for each gridded data product and watershed study site using mappingfilesummary files that do not exist for retrieval are excluded from the catalog the output from this use case is a summary table that describes data availability and seven folders containing the downloaded files for all the watersheds 2 2 5 summarize monthly meteorology with the livneh et al 2013 interpolated meteorology and salathé et al 2014 wrf files we compare the monthly meteorology variables for the sauk suiattle river watershed using the 61 years of data from their mutual time series period i e jan 1 1950 through dec 31 2010 using the sauk suiattle mapping file each variable from the ascii gridded cell time series are compiled into data frames where rows are the daily time series and columns are denoted with the gridded cell centroids temperature trends are interpreted using monthly mean yearly mean and global mean expected values fig 3 a annual anomaly from the global mean value is used to identify years with extreme events highest and lowest data values precipitation trends are interpreted using period sums e g month yearly sums and yearly sums and mean of period sums e g mean monthly sums mean yearly sums and the global mean of monthly sums fig 3b the gridclim dict function is a series of wrapped operations to return a dictionary object of spatial temporal values across the ascii gridded cell time series fig 1c gridclim dict provides parameters to specify elevation ranges or time period selection where defaults are all gridded cells and the full time series gridclim dict wraps read files in vardf which performs distributed file reading to generate a variable data frame for each variable in the data product then applies aggregate space time average to compute summary statistics using the prefix suffix conventions for each variable fig 3a the suffix represents the gridded data product which can be user defined or default to the annotated gridded data product dataset name e g dailymet livneh2013 the first prefix appended to the suffix by underscore separation is the data product variable e g precip the second prefixes represent the statistical averages computed using the gridded cell dimensions columns and the temporal groupings rows the aggregate space time sum produces outputs with the following second prefixes meanbydaily daily averages by each gridded cell meanbymonth daily averages by each month and gridded cell meanbyyear daily averages by each year and gridded cell meandaily average values across gridded cells by each date meanmonth daily averages across gridded cells for each month meanyear daily averages across gridded cells by each year meanallyear global mean of daily values across all years and gridded cells and anomyear the residual between each yearly mean and the global mean to consider trends by period sums of daily events the aggregate space time sum function computes summary statistics of month yearly and yearly sums fig 3b the second prefixes here include monthsum month yearly sum of daily values by gridded station yearsum annual sum of daily values by gridded station meanbymonthsum mean of monthly sums for each calendar month and gridded cell meanbyyearsum mean of annual sums for each gridded cell meanmonthsum mean of month yearly sums across gridded cells meanyearsum mean of annual sums across gridded cells meanalldailysum global mean of the daily sums across all gridded cells meanallmonthsum global mean of month yearly sums across gridded cells meanallyearsum global mean of annual sums across gridded cells variations to these outputs are influenced by the gridded cells and time period parameters the output for this use case is a json dictionary object containing analytical data frames and data series as shown in fig 3 for each variable within a given time frame other outputs include maps and monthly boxplots of the corresponding grid cell values 2 2 6 compute exceedance probabilities for the sauk suiattle study watershed we approximate the 10 exceedance probability threshold using unrouted daily runoff for each calendar month and each gridded cell fig 1d this is useful for visualizing the 10 highest daily streamflow generated for each grid cell in the dataset which is a combined function of climate data soils land cover and other model parameters in each grid cell potential runoff rates are computed as the sum of baseflow rate mm s and surface flow runoff rate mm s from livneh et al 2013 vic model outputs converted the units to millimeters per day mm day for comparison with daily precipitation rates the same general operations were applied to livneh et al 2015 vic model outputs for each calendar month e g january and each gridded cell e g centroid lat long at 48 8723 121 8974 daily potential runoff rates are compiled into a cumulative distribution function using data from 1 jan 1950 through 31 dec 2011 62 years the mutual overlapping time series period between livneh et al 2013 and livneh et al 2015 data products each distribution has approximately n 1800 vic modeled observations the 10 monthly exceedance probabilities peak runoff threshold for each gridded cell is estimated by linear interpolation as the 90th percentile of the respective cumulative distributions vogel et al 2007 an exceedance probability developed from a population of daily runoff in a given month should not be confused with annual flood statistics which are developed by fitting a statistical distribution to a population of annual maximum daily streamflow the 10 exceedance probability of observed streamflow discharge measured at sauk river near sauk wa usgs 12189500 can be plotted with the modeled streamflow to provide an observed reference based on routed streamflow for relatively high flows the output for this use case includes low average and high elevation analytical data frames at the 10 exceedance threshold for vic results compared to observations other outputs include maps and monthly boxplots of the exceedance probability for each gridded cell 3 results 3 1 map watershed centroids functions used reprojshapefile treatgeoself multisitevisual griddedcellgradient sauk suiattle elwha and upper rio salado watersheds were processed to generate mapping files and gridded cell gradient visualizations fig 4 ninety nine grid cells were identified for the sauk suiattle river watershed displaying the largest elevation difference 162 m 2246 m among the three watersheds table 3 sauk suiattle river watershed is located in the northwestern region of the cascade mountains in washington state usa ranging from multiple high elevation areas in the southeast to a single outlet in the northwestern gridded cells fifty five gridded cells were identified for the elwha river watershed which has a comparable elevation difference to sauk suiattle the elwha river watershed is located on the northern region of the olympic peninsula where the elevation gradient 36 m 1642 m descends from the southern gridded cells by a single river draining to the northern gridded cells fig 4b thirty one grid cells were identified for the upper rio salado watershed with a higher elevation 1962 m 2669 m grid cells than the other two watersheds table 3 upper rio salado s elevation gradient descends from the southwest most to the northeast most gridded cell 3 2 summarize data download and availability functions used getdailymet livneh2013 getdailymet bclivneh2013 getdailymet livneh2015 getdailyvic livneh2013 getdailyvic livneh2015 getdailywrf salathe2014 getdailywrf bcsalathe2014 mappingfilesummary among the seven gridded data products 1d ascii time series files were fully represented for sauk suiattle mostly represented for elwha and substantially limited in representation for upper rio salado table 3 download tasks for the full time series ascii files were distributed across 5 10 parallel worker cpus computation efficiencies consisted of 693 sauk suiattle files 10 0 gb disk space downloaded in 3 min 56 s wall time 375 elwha files 5 4 gb took 1 min 59 s and 124 upper rio salado files 2 7 gb took 48 8 s all files were cataloged into their respective mapping files organized by gridded data product short name and gridded cell centroid elwha is located in the northwestern most region of washington state three gridded cells were available for all seven gridded data products although they were available for the bias corrected livneh et al 2013 meteorology and livneh et al 2015 meteorology and vic model output products differences among the elevation gradient suggest that these three gridded cells were the northern most low elevation gridded cell on the boundary of conus and columbia river basin extents fig 4 this poses certain limitations if multiple gridded data products for elwha were used for intercomparison these limitations are more obvious with upper rio salado which is located outside of the columbia river basin livneh et al 2013 2015 gridded products were consistently spatially available in each of the watersheds but the gridded products differed in the temporal extent historic time series included the overlap period between livneh et al 2013 2015 data products is jan 1 1950 through dec 31 2011 62 years livneh et al 2013 and salathé et al 2014 share the jan 1 1950 through dec 31 2010 61 years which is the same overlap period as livneh et al 2013 and salathé et al 2014 despite the spatial availability of time series data within a watershed gridded data product intercomparisons should consider the historic time period represented as well as data variabilities such as correction methods and algorithms used to generate the gridded product 3 3 summary monthly meteorology functions used findcentroidcode overlappingdates gridclim dict aggregate space time sum valuerange savedictofdf rendervalueinboxplot rendervalueinpoints the function gridclim dict generates a json dictionary for sauk suiattle that contains 36 analytical data frames for livneh et al 2013 meteorology and salathé et al 2014 wrf outputs each data frame was named according to the analysis method second prefix variable first prefix and gridded data product short name suffix in fig 5 average monthly total precipitation i e meanbymonthsum precip dailymet livneh2013 and meanbymonthsum precip dailywrf salathe2014 are depicted as boxplots to represent the distribution of values across the 99 gridded cells the livneh et al 2013 interpolated meteorology fig 5 top left indicates a greater variability of average monthly precipitation during the november through january months while salathé et al 2014 wrf model outputs fig 5 bottom left shows a higher median and greater variability from april through september average monthly precipitation for targeted high and low elevation grid cells fig 4 are plotted alongside the boxplots as well as the point observations from darrington ranger station fig 5 comparison of observations and modeled precipitation shows that observed precipitation at low elevations is less than the monthly averages modeled by salathé et al 2014 during spring and summer spatial variations observed with livneh et al 2013 show large deviations between neighboring cells especially in comparison to the smoother spatial trends organized with the elevation gradient can be observed from the salathé et al 2014 fig 5a f monthly temperature statistics were computed for each grid cell using the daily minimum and maximum temperature between jan 1 1950 through dec 31 2010 fig 6 the distribution of mean maximum temperature shows that livneh et al 2013 interpolated meteorology has greater variability than salathé et al 2014 wrf model outputs this effect is also observed when comparing the distribution of mean monthly minimum temperature noting that livneh et al 2013 has more extreme hot and cold trends sometimes up to 5 c difference compared with the salathé et al 2014 wrf model outputs reference meteorological observations from the darrington ranger station closely resemble the livneh et al 2013 interpolated meteorology livneh et al 2013 values are dependent on source observations clustered around low elevation gridded cells light and dark gray with coop stations sparse observations limit the performance assessment for high elevation gridded cells while average daily minimum temperature seems to be comparable salathé et al 2014 predicts colder maximum temperatures for low elevation areas for all months and warmer temperatures for higher elevation areas orange line from november through april 3 4 compute exceedance probabilities functions used monthlyexceedence mmday computesurfacearea cfs to mmday fig 7 displays the monthly 10 exceedance probability and average 50 thresholds for unrouted potential runoff using two vic gridded data products for the sauk suiattle watershed not to be confused with approximations like the 10 000 year flood which are based on empirical streamflow values the probabilities generated by this function are based on empirical unrouted model outputs which have limited numeric range and interpretation thereof each point in the distributions represent the potential runoff threshold at which there is only a 10 chance expectation of exceeding that value in that month both gridded data products display a slight increase in november where the muted impact of the highest extreme winter flood events on lower average monthly flows followed by a high peak flow in june july from snowmelt runoff using the same color scale and axis ranges fig 7 right side the major contribution to potential runoff in any given year is from the cascade mountain ranges during the snowmelt season june july although the 10 exceedance probability at the highest elevation are comparable between data products the trends at the low elevation stations indicate that livneh et al 2015 produces more potential runoff between november through april months than livneh et al 2013 livneh et al 2015 applies a bias correction using prism data as a proxy for observations which produces more precipitation from winter rainfall season compared to results without a bias correction a comparison between the two gridded data products illustrates that livneh et al 2015 boxplots have approximately 2 3 mm median increase in potential runoff between october to may compared to livneh et al 2013 fig 7 the monthly 10 exceedance probability with sauk river near sauk wa discharge observations range from 4 2 to 15 3 mm day across the calendar months the exceedance probability threshold peaks in two months november and june corresponding with fall atmospheric river rainfall dominated storm events and early summer snowmelt the first smaller peak is observed in november which aligns with both livneh et al 2013 2015 the boxplot distributions the second larger peak occurs in june where the mean decreases though the variance increases in july the sauk river near sauk gauge is approximate 81 m above sea level half of the average elevation in gridded cell 3 where it is located the spatially averaged mm day routed streamflow dynamics in sauk river near sauk can be expected to fall within the elevation mean of unrouted vic modeled runoff using the lower resolution 1 16 gridded cells the observed data at the outlet is provided for context and the modeled results is provided to demonstrate the spatial variability of the grid cells contributing to the modeled streamflow at the watershed outlet 4 discussion the primary step in the workflow presented here is treatgeoself which enables users to control gridded cell inclusion and exclusion using shapefile guided data selection it generates the mapping file to guide distributed computing for data download and data processing this catalogue allows for machine reading selection and sorting of available data the examples use watersheds defined by huc12 boundaries but the shape can be user defined e g census block legislative boundaries fish species migration spatial clusters geopandas operations enable treatgeoself to transform shapefiles of varying spatial projections and to include buffer regions in the early development stage treatgeoself applied unfiltered spatial intersection with each shape polygon in the shapefile resulting in slow mapping performance and interpretation difficulty when buffer regions were included at present study sites with multiple subpolygons are merged by spatial union into a single multipolygon object simplifying treatgeoself into a first order loop intercomparison of gridded data products of different gridded parcel schemas are expected to be enabled and more efficient with the use of the projection alignment and cross mapping functionalities data download operations are functionalized for distributed computing but the concurrent queue and transfer rate are limited by the computing resources allocated by the user and the data content provider data download was found to be rate limited to approximately 5 concurrent web requests to the livneh et al 2013 web domain all other gridded data product hosts enabled 10 or more concurrent web requests a rate limiter for the number of parallel data retrieval tasks was incorporated into the data download functions but not for local data processing operations the rate of data transfer would need to be assessed before ogh could integrate data servers with restful api such as erddap which could expedite mapping and retrieval of gridded data products and metadata simons and mendelssohn 2012 other limits include nuanced issues of data maintenance by the data publisher provider for example during production and testing of workflow and functions data products mentioned in livneh et al 2013 were migrated to a new web domain resulting in misdirected requests annotations and data retrieval functions may need updating over time we qualitatively described differences between two gridded data products of the same empirically estimated statistical exceedance probability approach the vic modeled gridded data products originate from unrouted flow modeling in contrast to empirically estimated 10 000 year flood at stream gauges empirically estimated exceedance probability for grid cells without flow routing may be limited in interpretations to potential runoff it is unclear how the different model simulations may affect these interpretations these concerns regarding model comparison would merit further research and development of functionalities for in watershed stream gauge selection and quantitative determinations for the goodness of fit between routed and unrouted modeled values relative to those estimated from at stream observations while we designed ogh specifically for users who primarily import ascii formatted files into hydrologic and earth surface model software e g landlab hobley et al 2017 a noteworthy limitation of using ascii file format is that as netcdf adoption is increasing as a data standard ascii time series may not be available for newer gridded data products this limitation is addressed using the proposed minimum information criteria and having initial criteria for conducting gridded data product intercomparisons netcdf files are embedded with metadata while ascii files are unannotated to inform the structure and use of the ascii files the proposed minimum information criteria serves as a road map for locating gridded data product files and considers the schema of the file organization and the features within each file gridded data products published by livneh et al 2013 partitioned files by spatial bounding box subfolders denoted by the file prefix west east south and north cardinal limits for livneh et al 2013 scrape domain and maptoblock functions were designed to abstract the bounding boxes then decide the subfolder identity by spatial intersection the spatial bounding box for gridded cells within british columbia canada did not follow this folder naming structure thus a separate annotation was provided for the spatial boundary in british columbia canada among the annotated ascii gridded data products we ve observed a variety of file organizations different gridded cell schemas spatial resolution or netcdf file organizations may be adaptable retrieval and data management of netcdf files in cloud computing environments would benefit from further design assessments as it is not yet clear how to conduct or evaluate netcdf to ascii intercomparison without a priori format preferences that may result in information loss in addition the development of a user centered reference of controlled vocabulary would improve the usefulness and adoption of a minimum information criteria that can be used across data formats these may help adapt climate and water resource information for researching interdisciplinary questions with other data products such as air quality or population data sets wohlstadter et al 2016 lloyd et al 2017 for use by researchers who are not hydrometeorology analyst netcdf files contain data outside the study area extent 1d ascii time series files may be the preferred format for small study areas 1 100 km2 we tested and developed the examples using hydroshare for the computing and data sharing environment an important benefit of hydroshare is that it hosts a rest api that enables data migration and the creation of new shareable data objects additionally as a community repository for hydrologic science fair publication of hydrologic data sets and software execution with reproducible workflows is demonstrated with the use cases developed in this work ogh operations are technically independent of hydroshare and minor changes would allow the code to operate in other similar computing and data sharing environments such as local servers cloud servers amazon aws microsoft azure and dockerized virtual environments with a jupyter instance data world dataone pangeo esiphub 5 conclusions ogh is a toolkit that makes download and processing of large climate datasets more efficient by leveraging distributed computing for watershed scale research and intercomparison of ascii gridded data products which extends climate modeling products to represent otherwise sparsely observed parts of the landscape the mapping file output is the key data management tool which catalogs the watershed gridded cells and downloaded files as a lens across gridded data products along with the proposed minimum information criteria to annotate ascii gridded data products these data management tools enable multiprocessing and dask distributed operations comparable to the efficiency of xarray for netcdf gridded data products this metadata component improves the standardization of gridded hydrometeorology products published for use by third party researchers and scientists the dictionary of analytical data frames is a key data management device that enables key value pair retrieval and exporting of summary outputs to address user needs for exploratory data analysis and visual control various data frames were rendered into different geographic and temporal modes of human readable visual inspection overall ogh is equipped with metadata framework and workflow that makes it a useful introduction and training tool for watershed studies using gridded data products and ascii time series data sets the data summary capabilities increase the efficiency of comparing multiple gridded hydrometeorology products without discontinuous use of different software ogh and the four use cases demonstrated are available for interactive use on hydroshare https www hydroshare org resource 87dc5742cf164126a11ff45c3307fd9d and also available for open development from the university of washington freshwater initiative observatory repository https github com freshwater initiative observatory copyright 2017 mit license this is an open access article distributed under the terms of the massachusetts institute of technology attribution license which permits unrestricted use free of charge distribution and reproduction in any medium without warranty provided the original author and source are credited in no event shall the authors or source be held liable for claims damages or liabilities arising from use of the software software and or data availability ogh can be installed from conda forge ogh v0 2 0 is released on github https github com freshwater initiative observatory and is freely available under an mit license this github repository and the hydroshare resources are maintained by the corresponding author christina bandaragoda this python library was developed using python 3 6 conventions within a jupyterhub unix docker environment hosted on the cuahsi hydroshare server use case notebooks can be found at the github repository https github com freshwater initiative observatory tree master tutorials and the hydroshare resource https www hydroshare org resource 87dc5742cf164126a11ff45c3307fd9d acknowledgements this work benefited from the contributions from university of washington uw watershed dynamics group who helped test and develop ogh and members uw escience institute that helped with developing and using the python toolkits this project was supported in part by national science foundation graduate research fellowship program dge 1762114 hydroshare cyberinfrastructure project aci 1148453 si2 ssi landlab project aci 1450412 predicting climate change impacts on shallow landslide risks cbet 1336725 institute for translational health sciences grant ul1tr002319 and clinical and translational sciences award program national center for data to health grant u24tr002306 uw civil environmental engineering department in collaboration with uw college of engineering and researchers and scientists of the sauk suiattle indian tribe and the skagit climate consortium with funding from the bureau of indian affairs a05av00078 this work was supported by the washington research foundation and data science environment project award from the gordon and betty moore foundaton award 2013 10 29 and alfred p sloan foundation award 3835 to the university of washington escience institute the project uses the hydroshare platform which is supported by the consortium of universities for the advancement of hydrologic sciences inc cuahsi a research organization supported by nsf cooperative agreement ear 1338606 with ongoing developments supported by nsf grants oac 1664061 oac 1664018 and oac 1664119 the content is solely the responsibility of the authors and does not represent the official views of these funding agencies special thanks to early reviewers who contributed to the editing of this paper drs dan ames emilio mayorga and nicoleta cristea abbreviations ogh observatory for gridded hydrometeorology conus conterminous united states wrf weather research forecasting fair findable accessible interoperable and reusable hs restclient hydroshare rest api python library vic variable infiltration capacity appendix a supplementary data the following are the supplementary data to this article supp table2 v3 v4 supp table2 v3 v4 final ogh manuscript v4 v2 docx 29 final ogh manuscript v4 v2 docx 29 data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 007 baxter and sommerville 2011 g baxter i sommerville socio technical systems from design methods to systems engineering interact comput 23 1 2011 jan 4 17 castronova et al 2017 a m castronova l brazil m seul cloud based jupyter notebooks for water data analysis agu fall meeting abstracts 2017 dec cutter et al 2014 s l cutter k d ash c t emrich the geographies of community disaster resilience glob environ chang 29 2014 nov 30 65 77 https doi org 10 1016 j gloenvcha 2014 08 005 daly et al 1994 c daly r p neilson d l phillips a statistical topographic model for mapping climatological precipitation over mountainous terrain j appl meteorol 33 2 1994 feb 140 158 devi et al 2012 k r devi a m sen k hemachandran a working framework for the user centered design approach and a survey of the available methods international journal of scientific and research publications 2 4 2012 apr gampe and ludwig 2017 d gampe r ludwig evaluation of gridded precipitation data products for hydrological applications in complex topography hydrology 4 4 2017 nov 16 53 https doi org 10 3390 hydrology4040053 gardner et al 2018 m a gardner c g morton j l huntington r g niswonger w r henson input data processing tools for the integrated hydrologic model gsflow environ model softw 2018 aug 9 https doi org 10 1016 j envsoft 2018 07 020 gillies 2007 sean gillies shapely manipulation and analysis of geometric objects toblerity org 2007 https github com toblerity shapely gillies 2011 sean gillies fiona is ogr s neat nimble no nonsense api toblerity org 2011 https github com toblerity fiona heard et al 2014 j heard d tarboton r idaszak j horsburgh d ames a bedig a castronova a couch an architectural overview of hydroshare a next generation hydrologic information system 2014 cuny academic works https academicworks cuny edu cc conf hic 311 henn et al 2018 b henn a j newman b livneh c daly j d lundquist an assessment of differences in gridded precipitation datasets in complex terrain j hydrol 556 2018 jan 1 1205 1219 https doi org 10 1016 j jhydrol 2017 03 008 hobley et al 2017 d e j hobley j m adams s s nudurupati e w h hutton n m gasparini e istanbulluoglu g e tucker creative computing with landlab an open source toolkit for building coupling and exploring two dimensional numerical models of earth surface dynamics earth surface dynamics 2017 10 5194 esurf 5 21 2017 horsburgh et al 2016 j s horsburgh m m morsy a m castronova j l goodall t gan h yi m j stealey d g tarboton hydroshare sharing diverse environmental data types and models as social objects with application to the hydrology domain jawra journal of the american water resources association 52 4 2016 aug 1 873 889 https doi org 10 1111 1752 1688 12363 hydroshare rest api python client library 2018 hydroshare rest api python client library release v1 2 12 2018 http hs restclient readthedocs io en latest last accessed in aug 2018 jordahl 2014 k geopandas jordahl python tools for geographic data 2014 https github com geopandas geopandas kadlec et al 2015 j kadlec b stclair d p ames r a gill waterml r package for managing ecological experiment data on a cuahsi hydroserver ecol inf 28 2015 jul 31 19 28 https doi org 10 1016 j ecoinf 2015 05 002 king et al 2013 a d king l v alexander m g donat the efficacy of using gridded data to examine extreme rainfall characteristics a case study for australia int j climatol 33 10 2013 aug 1 2376 2387 https doi org 10 1002 joc 3588 konrad and voss 2012 c p konrad f d voss analysis of streamflow gaging network for monitoring stormwater in small streams in the puget sound basin washington u s geological survey scientific investigations report 2012 2012 5020 16 p ledesma and futter 2017 j l ledesma m n futter gridded climate data products are an alternative to instrumental measurements as inputs to rainfall runoff models hydrol process 31 18 2017 aug 30 3283 3293 https doi org 10 1002 hyp 11269 liang et al 1994 x liang d p lettenmaier e f wood s j burges a simple hydrologically based model of land surface water and energy fluxes for general circulation models j geophys res atmosphere 99 d7 1994 jul 20 14415 14428 https doi org 10 1029 94jd00483 livneh et al 2013 b livneh e a rosenberg c lin b nijssen v mishra k m andreadis e p maurer d p lettenmaier a long term hydrologically based dataset of land surface fluxes and states for the conterminous united states update and extensions j clim 26 23 2013 dec 9384 9392 https doi org 10 1175 jcli d 12 00508 1 livneh et al 2015 b livneh t j bohn d w pierce f munoz arriola b nijssen r vose d r cayan l brekke a spatially comprehensive hydrometeorological data set for mexico the us and southern canada 1950 2013 scientific data 2 2015 aug 18 150042 https doi org 10 1038 sdata 2015 42 lloyd et al 2017 c t lloyd a sorichetta a j tatem high resolution global gridded data for use in population studies scientific data 4 2017 jan 31 170001 https doi org 10 1038 sdata 2017 1 maurer et al 2002 e p maurer a w wood j c adam d p lettenmaier b nijssen a long term hydrologically based dataset of land surface fluxes and states for the conterminous united states j clim 15 22 2002 nov 3237 51 https doi org 10 1175 1520 0442 2002 015 3237 althbd 2 0 co 2 mckinney 2011 w mckinney pandas a foundational python library for data analysis and statistics python for high performance and scientific computing 2011 jun 1 9 mons et al 2017 b mons c neylon j velterop m dumontier l o da silva santos m d wilkinson cloudy increasingly fair revisiting the fair data guiding principles for the european open science cloud inf serv use 37 1 2017 jan 1 49 56 https doi org 10 3233 isu 170824 read et al 201 j s read j i walker a p appling d l blodgett e k read l a winslow geoknife reproducible web processing of large gridded datasets ecography 39 4 2016 apr 354 360 rocklin 2015 m rocklin dask parallel computation with blocked algorithms and task scheduling proceedings of the 14th python in science conference 2015 no 130 136 salathé et al 2014 e p salathé jr a f hamlet c f mass s y lee m stumbaugh r steed estimates of twenty first century flood risk in the pacific northwest based on regional climate model simulations j hydrometeorol 15 5 2014 oct 1881 1899 https doi org 10 1175 jhm d 13 0137 1 salathé et al 2010 e p salathé l r leung y qian y zhang regional climate model projections for the state of washington clim change 102 1 2 2010 sep 1 51 75 simons and mendelssohn 2012 r a simons r mendelssohn erddap a brokering data server for gridded and tabular datasets agu fall meeting abstracts 2012 dec vogel et al 2007 r m vogel n c matalas j f england jr a castellarin an assessment of exceedance probabilities of envelope curves water resour res 43 7 2007 jul https doi org 10 1029 2006wr005586 wilkinson et al 2016 m d wilkinson m dumontier i j aalbersberg g appleton m axton a baak n blomberg j w boiten l b da silva santos p e bourne j bouwman the fair guiding principles for scientific data management and stewardship scientific data 3 2016 http doi org 10 1038 sdata 2016 18 wohlstadter et al 2016 m wohlstadter l shoaib j posey j welsh j fishman a python toolkit for visualizing greenhouse gas emissions at sub county scales environ model softw 83 2016 sep 1 237 244 https doi org 10 1016 j envsoft 2016 05 016 yanto et al 2017 yanto b livneh b rajagopalan development of a gridded meteorological dataset over java island indonesia 1985 2014 sci data 4 170072 2017 may 23 10 1038 sdata 2017 72 data 
26230,spatially distributed time series data support a range of environmental modeling and data research efforts a critical first step to any such effort is acquiring interpolated hydrometeorological data standardized tools to facilitate this process into analyses have not been readily available for watershed scale research here we introduce the observatory for gridded hydrometeorology ogh an open source python library that fills this critical software gap by providing a cyberinfrastructure component to fetch and manage distributed data processed from regional and continental scale gridded hydrometeorology products our approach involves annotating metadata to make gridded data products discoverable and usable within the software enabling interoperability and reproducibility of models that use the data this paper presents the design architecture and application of ogh using four commonly practiced use cases with gridded time series data at watershed scales ogh and its associated annotations are distributed via anaconda cloud within conda forge package repository the tutorial jupyter notebooks for each example use case are available within the freshwater initiative observatory repository https github com freshwater initiative observatory the examples are designed to utilize the compute resources and software libraries provided by hydroshare https www hydroshare org resource 87dc5742cf164126a11ff45c3307fd9d keywords python cloud computing shapefile based data retrieval watershed hydrometeorology 1 introduction gridded data products are extensively used in earth science research king et al 2013 gampe and ludwig 2017 ledesma and futter 2017 social vulnerability analysis cutter et al 2014 and population risk and estimate studies lloyd et al 2017 gridded hydrometeorological data products are produced by interpolating local observations to predetermined spatial temporal resolutions the purpose of developing gridded products is to extend spatial information beyond point locations and provide space and time dimensions to observations such that spatial temporal variability can be analyzed gridded data products also provide a means to compare and validate numerical weather prediction outputs short term forecasts and long term climate change prior studies in hydrometeorology have highlighted the growing usefulness of gridded data products in earth science modeling most recently reviewed in henn et al 2018 in this paper we introduce the observatory for gridded hydrometeorology ogh an open source python toolkit to streamline processes for interacting with gridded hydrometeorological data products at a user defined spatial scale of interest single location to regional watershed designed to support watershed scale science applications this tool fills a model pre processing gap of processing large regional datasets 1000 km2 for smaller scale geographic subsets 1 km2 100 km2 in the conterminous united states conus since the introduction of parameter elevation regressions on independent slopes model prism daly et al 1994 gridded meteorological data products are routinely interpolated using daily measurements from over 20 000 noaa coop observation stations maurer et al 2002 livneh et al 2013 with similar products in development for other regions around the world yanto et al 2017 in these data products each grid cell contains observation interpolated multivariate time series maurer et al 2002 it is common practice for the hydrologic community to incorporate gridded meteorological time series variables as inputs to land surface hydrologic models such as the variable infiltration capacity vic model liang et al 1994 a wealth of modeled land surface hydrologic states e g soil moisture and fluxes e g latent and sensible heat have been developed and used in earth science research livneh et al 2013 livneh et al 2015 in mountainous regions where ground level observation collection is not feasible the weather research forecasting wrf atmospheric model has been downscaled and similarly used to produce hydrologic model outputs however this process inevitably requires bias correction based on observational products salathé et al 2010 2014 recently gridded hydrometeorological data was combined with geo environmental and demographic surveys to yield gridded population data sets and new insight to enhance population modeling resolution and accuracy cutter et al 2014 lloyd et al 2017 it can be expected that gridded data products will continue to increase in abundance and complexity in how they represent the impacts of geography and landscape morphology before the potential usefulness of gridded data products can be realized for watershed scale actionable research several data and metadata access challenges need to be addressed continental scale gridded data products such as those from livneh et al 2013 2015 and salathé et al 2014 are increasingly being published as netcdf files for watershed researchers who are interested in studying physical processes netcdf files used for regional and continental scale gridded products 1000 km2 10 000 km2 contain information that far exceeds the geographic extent needed for local watershed scale research e g 1 100 km2 catchment area adding computational resource burden in exploratory research one alternative data product format is the 1d ascii time series files for a geographically specific gridded cell as observed in livneh et al 2013 2015 and salathé et al 2014 data products 1d ascii time series files are not self described with column names time series dates or value units so annotations are needed in order to perform analyses with files in this format information to locate and use the data files may be confined to elusive publications and documentation files even so the data files may be hosted in management structures for their study convenience e g universal transverse mercator boundaries making manual data retrieval non trivial and not intuitive by human interpretation hence annotating data provenance metadata provenance and file management structure are crucial steps towards making gridded data products findable accessible interoperable and reusable fair wilkinson et al 2016 mons et al 2017 for secondary analyses currently available tools for water data such as waterml wofpy gsflow geoknife offer access to time series of observation data kadlec et al 2015 gardner et al 2018 read et al 2016 however in areas where observations are unavailable such as heterogeneous landscapes at high elevation locations data sparsity can be addressed with krigged and model interpolated data products python libraries such as openclimategis offer access to netcdf gridded data products but these functionalities exclude legacy data sets provided in 1d ascii time series format more importantly aside from data access it is challenging to recognize differences between gridded data product such as aggregation into different gridded cell schemas the temporal resolution time period or long term trends for use in future modeling efforts and model validation operations to streamline the processes needed for interacting with gridded hydrometeorological data products in a fair manner promote use in earth modeling and interdisciplinary domains and support decision making for selecting gridded cells in a study site we designed ogh as an open source python toolkit to select gridded cells in a study site data download spatial temporal analyses and provide data visualization in this paper we describe the design of the ogh python library which contains functions to conduct basic data access and data processing operations to simplify gridded data product use in research users start with an esri shapefile that describes their study site e g huc12 units county boundaries state boundaries to generate a comma separated table that helps with file management of gridded cell data availability across gridded data products users can then conduct data retrieval in parallel from a number of gridded hydrometeorological data products with automated file management for analyses these functions are flexible to integrate new gridded products and publishing standards to address the absence and or variability in describing online gridded data products we propose a set of minimum annotation criteria metadata fields for describing ascii gridded hydrometeorological data products and the decision steps needed to access these gridded datasets in the methods section we describe ogh software design for gridded cell selection and visualization data download spatial temporal calculations and applied statistics functionalities ogh was designed to incorporate climatological and hydrometeorological gridded data products with comparable data structures to ascii and netcdf formats here we emphasize watershed scale applications using 1d ascii time series data products in the results section we demonstrate these functionalities using three watersheds of end member climatologies in conus two high alpine glacierized watershed in the high end of the precipitation gradient in the conus 3000 mm sauk suiattle wa and elwha river basin wa and a desert watershed with a large precipitation gradient from valleys to peaks 200 mm 3500 mm upper rio salado nm we compute precipitation and temperature spatial temporal statistics and exceedance probability calculations using gridded hydrometeorological data products from livneh et al 2013 and salathé et al 2014 ogh v 0 1 11 is publicly accessible at https github com freshwater initiative observatory and available by conda installation 2 methods ogh is a python library designed to perform gridded cell selection data download data processing for desired space time analytics and visualization of spatial temporal data a proposed set of metadata annotations was developed to provide default information about data product capacities using two case study gridded products we provide ogh examples reproducible on hydroshare a cyber infrastructure for sharing data and models heard et al 2014 horsburgh et al 2016 castronova et al 2017 ogh is not dependent on hydroshare though in the example use cases hydroshare is a platform providing dockerized or local server environments for community software including those used by ogh operations to manage compute and store directories of files user workflows scenario use cases and key socio technical needs were developed through key informant interviews diagramming and rapid prototype testing sessions baxter and sommerville 2011 devi et al 2012 four key principles of user centered design and engagement were maintained throughout the rapid prototype testing process described in supplementary table 1 devi et al 2012 2 1 software design we designed ogh using open source python 3 6 programming which is interoperable with major operating systems and computing environments used by personal desktop computers high performance computers and supercomputers ogh functions are written as modular components that leverage classes and methods from a number of python libraries time series analysis from pandas mckinney et al 2011 geospatial analytics from geopandas fiona shapely and matplotlib basemap mckinney 2011 gillies 2007 2011 jordahl et al 2014 and task management from multiprocessing and dask rocklin 2015 operations from these libraries are assembled into scripted functions which can be wrapped into sequential operations or applied in distributed computing practices ogh is intended to perform operations within computing environments where input and output files are managed within data sharing platforms and community data repositories e g hydroshare while hydroshare has many features we make use of the docker or server environment where file storage migration and computation can be performed with multi core resources hydroshare www hydroshare org is a collaborative platform that supports data sharing and model reproducibility in hydrologic research and it provides a cloud computing environment through the consortium of universities for the advancement of hydrologic science inc cuahsi jupyterhub server horsburgh et al 2016 the hydroshare rest api python library hs restclient is used to migrate files contained in hydroshare resources in and out of the jupyterhub docker environments hydroshare rest api python client library 2018 file storage within the computing environment is intended to be temporary storage for the duration of the computations the research workflows presented in this paper are designed to be guided by jupyter notebooks sharable code execution interfaces that operate within the jupyterhub docker environments castronova et al 2017 2 2 gridded data product annotations we annotated seven daily 1 16 6 km gridded data products from three studies published and made available online livneh et al 2013 2015 salathé et al 2014 the datasets published by livneh et al 2013 provided an interpolated climate station meteorology for conus and a meteorology data product that was bias corrected to the columbia river basin regional climatology in the time span from 1915 to 2011 expanding on livneh et al 2013 2015 includes a prism calibrated interpolated climate station meteorology extends from mexico to limited regions of canada however the period has 33 total years less data with a time span of 1950 2013 both interpolated meteorology data products were used to predict macro scale hydrologic fluxes at the 1 16 daily resolution by the vic model the wrf gridded data product provides model downscaled daily precipitation maximum and minimum air temperature and wind speed for the columbia river basin for the period of 1950 2010 salathé et al 2014 the data products can be further differentiated according to their type of analysis and reported spatial coverage table 1 the annotations describe the gridded data products published as ascii files where each file contains the gridded cell historic time series data annotation features include the data set short name information to locate the ascii files information about the file structure and sources of metadata and metadata about the file variables table 2 file locations can be represented or reconstructed given by the web protocol e g ftp https web domain and subdomain decision steps within the subdomain to locate the data file subdirectory e g centroid latitude given the spatial resolution bounding box bins the filename structure and the file format the file structure is described by the variable list left to right column order time series date range temporal resolution file delimiter and the data types and unit increment for each variable full annotations are provided in the ogh meta module 2 2 1 example use cases we present four example use cases in the form of jupyter notebooks to demonstrate the ogh operations fig 1 in the first use case we identify the subset gridded cells of interest for four watershed study sites using the treatgeoself function fig 1a the shapefiles for these watersheds are stored within a public hydroshare resource for ease of collaborative use sauk suiattle river basin available in bandaragoda c 2017 elwha river basin available in beveridge c 2017 and upper rio salado basin available in bandaragoda c 2017 watershed boundaries were defined in arcgis using 12 digit hydrologic unit code polygons from the national watershed boundary database in the second use case the time series data files are retrieved cataloged then summarized for data availability fig 1b in the third use case we focus on the sauk suiattle watershed to determine the monthly meteorological spatial temporal statistics computed using the livneh et al 2013 meteorology versus the salathé et al 2014 wrf model output data products fig 1c finally we compute potential runoff values using the vic hydrologic data product from livneh et al 2013 to approximate the 10 exceedance probability thresholds based on the daily time series in each dataset fig 1d functions introduced in each use case are summarized in supplementary table 2 for illustration five reference locations are used in the sauk suiattle watershed examples one gridded cell is identified at the highest average elevation value 2216 m above sea level two gridded cells were identified at the lowest average elevation value 164 m above sea level the darrington ranger station coop station 451992 is used as the reference source of meteorological observations with data collected at 167 m above sea level from jan 1 1931 through dec 31 2005 sauk river near sauk wa usgs 12198500 used as the reference source of observed streamflow discharge using data collected between jan 1 1950 through dec 31 2011 konrad et al 2012 2 2 2 general workflow and required files workflows for the use cases executed in the jupyterhub environment fig 2 are illustrated in detail in fig 1 the general workflow begins with three hydroshare resources as sources of input files fig 2 resource a a hydroshare resource that contains a jupyter notebooks to execute code for each example use case presented in this paper resource b a hydroshare resource with a user defined shapefile representing the region of interest e g a watershed and resource c a file of point locations describing conus gridded cell centroids only pre requisite for mapping watershed centroids from the web page for hydroshare resource a the jupyter notebooks are launched in the jupyterhub docker environment wherein the hydroshare rest api functions migrate in requisite data files from resource b and c fig 2 use case notebooks 1 through 4 progress through ogh operations in fig 2 identify watershed gridded cell centroids map watershed centroids download and display data availability data download summarize monthly meteorology data processing and compute exceedance probabilities another form of data processing fig 2 each use case notebook produces output data files plots data visualizations and finally migrates these output to new shareable hydroshare resources to conclude the use case demonstration 2 2 3 map watershed gridded cell centroids for each of the three example watersheds we generate a mapping file with the gridded cell centroids that spatially intersect these study sites fig 1a shapefiles were transformed into the 1984 world geodetic system wgs84 lat long coordinates system as the standard projection the study site was given a buffer region default buffer distance of 0 06 to include adjacent gridded cells conus 1 16 i e 0 06250 gridded cell esri shapefile identifies each gridded cell by the 5 digit centroid latitude longitude livneh et al 2013 livneh 2017 average elevation in the gridded cell in meters above sea level are based on the conus digital elevation model described in livneh et al 2013 the output from this use case includes three mapping files each denoting the latitude longitude and average elevation within the gridded cell other outputs include spatial visualizations of the study site maps and the elevation gradient among the gridded cells and plots showing the data for select grid cell traces 2 2 4 summarize data download and availability the mapping file guides data download from the seven gridded data products fig 1b target gridded cell files identified within the mapping file are web requested using data download wrapper functions e g the getdailymet livneh2013 request operations are distributed using multiprocessing pool operations downloaded files are cataloged into the mapping file using addcatalogtomap data availability is determined for each gridded data product and watershed study site using mappingfilesummary files that do not exist for retrieval are excluded from the catalog the output from this use case is a summary table that describes data availability and seven folders containing the downloaded files for all the watersheds 2 2 5 summarize monthly meteorology with the livneh et al 2013 interpolated meteorology and salathé et al 2014 wrf files we compare the monthly meteorology variables for the sauk suiattle river watershed using the 61 years of data from their mutual time series period i e jan 1 1950 through dec 31 2010 using the sauk suiattle mapping file each variable from the ascii gridded cell time series are compiled into data frames where rows are the daily time series and columns are denoted with the gridded cell centroids temperature trends are interpreted using monthly mean yearly mean and global mean expected values fig 3 a annual anomaly from the global mean value is used to identify years with extreme events highest and lowest data values precipitation trends are interpreted using period sums e g month yearly sums and yearly sums and mean of period sums e g mean monthly sums mean yearly sums and the global mean of monthly sums fig 3b the gridclim dict function is a series of wrapped operations to return a dictionary object of spatial temporal values across the ascii gridded cell time series fig 1c gridclim dict provides parameters to specify elevation ranges or time period selection where defaults are all gridded cells and the full time series gridclim dict wraps read files in vardf which performs distributed file reading to generate a variable data frame for each variable in the data product then applies aggregate space time average to compute summary statistics using the prefix suffix conventions for each variable fig 3a the suffix represents the gridded data product which can be user defined or default to the annotated gridded data product dataset name e g dailymet livneh2013 the first prefix appended to the suffix by underscore separation is the data product variable e g precip the second prefixes represent the statistical averages computed using the gridded cell dimensions columns and the temporal groupings rows the aggregate space time sum produces outputs with the following second prefixes meanbydaily daily averages by each gridded cell meanbymonth daily averages by each month and gridded cell meanbyyear daily averages by each year and gridded cell meandaily average values across gridded cells by each date meanmonth daily averages across gridded cells for each month meanyear daily averages across gridded cells by each year meanallyear global mean of daily values across all years and gridded cells and anomyear the residual between each yearly mean and the global mean to consider trends by period sums of daily events the aggregate space time sum function computes summary statistics of month yearly and yearly sums fig 3b the second prefixes here include monthsum month yearly sum of daily values by gridded station yearsum annual sum of daily values by gridded station meanbymonthsum mean of monthly sums for each calendar month and gridded cell meanbyyearsum mean of annual sums for each gridded cell meanmonthsum mean of month yearly sums across gridded cells meanyearsum mean of annual sums across gridded cells meanalldailysum global mean of the daily sums across all gridded cells meanallmonthsum global mean of month yearly sums across gridded cells meanallyearsum global mean of annual sums across gridded cells variations to these outputs are influenced by the gridded cells and time period parameters the output for this use case is a json dictionary object containing analytical data frames and data series as shown in fig 3 for each variable within a given time frame other outputs include maps and monthly boxplots of the corresponding grid cell values 2 2 6 compute exceedance probabilities for the sauk suiattle study watershed we approximate the 10 exceedance probability threshold using unrouted daily runoff for each calendar month and each gridded cell fig 1d this is useful for visualizing the 10 highest daily streamflow generated for each grid cell in the dataset which is a combined function of climate data soils land cover and other model parameters in each grid cell potential runoff rates are computed as the sum of baseflow rate mm s and surface flow runoff rate mm s from livneh et al 2013 vic model outputs converted the units to millimeters per day mm day for comparison with daily precipitation rates the same general operations were applied to livneh et al 2015 vic model outputs for each calendar month e g january and each gridded cell e g centroid lat long at 48 8723 121 8974 daily potential runoff rates are compiled into a cumulative distribution function using data from 1 jan 1950 through 31 dec 2011 62 years the mutual overlapping time series period between livneh et al 2013 and livneh et al 2015 data products each distribution has approximately n 1800 vic modeled observations the 10 monthly exceedance probabilities peak runoff threshold for each gridded cell is estimated by linear interpolation as the 90th percentile of the respective cumulative distributions vogel et al 2007 an exceedance probability developed from a population of daily runoff in a given month should not be confused with annual flood statistics which are developed by fitting a statistical distribution to a population of annual maximum daily streamflow the 10 exceedance probability of observed streamflow discharge measured at sauk river near sauk wa usgs 12189500 can be plotted with the modeled streamflow to provide an observed reference based on routed streamflow for relatively high flows the output for this use case includes low average and high elevation analytical data frames at the 10 exceedance threshold for vic results compared to observations other outputs include maps and monthly boxplots of the exceedance probability for each gridded cell 3 results 3 1 map watershed centroids functions used reprojshapefile treatgeoself multisitevisual griddedcellgradient sauk suiattle elwha and upper rio salado watersheds were processed to generate mapping files and gridded cell gradient visualizations fig 4 ninety nine grid cells were identified for the sauk suiattle river watershed displaying the largest elevation difference 162 m 2246 m among the three watersheds table 3 sauk suiattle river watershed is located in the northwestern region of the cascade mountains in washington state usa ranging from multiple high elevation areas in the southeast to a single outlet in the northwestern gridded cells fifty five gridded cells were identified for the elwha river watershed which has a comparable elevation difference to sauk suiattle the elwha river watershed is located on the northern region of the olympic peninsula where the elevation gradient 36 m 1642 m descends from the southern gridded cells by a single river draining to the northern gridded cells fig 4b thirty one grid cells were identified for the upper rio salado watershed with a higher elevation 1962 m 2669 m grid cells than the other two watersheds table 3 upper rio salado s elevation gradient descends from the southwest most to the northeast most gridded cell 3 2 summarize data download and availability functions used getdailymet livneh2013 getdailymet bclivneh2013 getdailymet livneh2015 getdailyvic livneh2013 getdailyvic livneh2015 getdailywrf salathe2014 getdailywrf bcsalathe2014 mappingfilesummary among the seven gridded data products 1d ascii time series files were fully represented for sauk suiattle mostly represented for elwha and substantially limited in representation for upper rio salado table 3 download tasks for the full time series ascii files were distributed across 5 10 parallel worker cpus computation efficiencies consisted of 693 sauk suiattle files 10 0 gb disk space downloaded in 3 min 56 s wall time 375 elwha files 5 4 gb took 1 min 59 s and 124 upper rio salado files 2 7 gb took 48 8 s all files were cataloged into their respective mapping files organized by gridded data product short name and gridded cell centroid elwha is located in the northwestern most region of washington state three gridded cells were available for all seven gridded data products although they were available for the bias corrected livneh et al 2013 meteorology and livneh et al 2015 meteorology and vic model output products differences among the elevation gradient suggest that these three gridded cells were the northern most low elevation gridded cell on the boundary of conus and columbia river basin extents fig 4 this poses certain limitations if multiple gridded data products for elwha were used for intercomparison these limitations are more obvious with upper rio salado which is located outside of the columbia river basin livneh et al 2013 2015 gridded products were consistently spatially available in each of the watersheds but the gridded products differed in the temporal extent historic time series included the overlap period between livneh et al 2013 2015 data products is jan 1 1950 through dec 31 2011 62 years livneh et al 2013 and salathé et al 2014 share the jan 1 1950 through dec 31 2010 61 years which is the same overlap period as livneh et al 2013 and salathé et al 2014 despite the spatial availability of time series data within a watershed gridded data product intercomparisons should consider the historic time period represented as well as data variabilities such as correction methods and algorithms used to generate the gridded product 3 3 summary monthly meteorology functions used findcentroidcode overlappingdates gridclim dict aggregate space time sum valuerange savedictofdf rendervalueinboxplot rendervalueinpoints the function gridclim dict generates a json dictionary for sauk suiattle that contains 36 analytical data frames for livneh et al 2013 meteorology and salathé et al 2014 wrf outputs each data frame was named according to the analysis method second prefix variable first prefix and gridded data product short name suffix in fig 5 average monthly total precipitation i e meanbymonthsum precip dailymet livneh2013 and meanbymonthsum precip dailywrf salathe2014 are depicted as boxplots to represent the distribution of values across the 99 gridded cells the livneh et al 2013 interpolated meteorology fig 5 top left indicates a greater variability of average monthly precipitation during the november through january months while salathé et al 2014 wrf model outputs fig 5 bottom left shows a higher median and greater variability from april through september average monthly precipitation for targeted high and low elevation grid cells fig 4 are plotted alongside the boxplots as well as the point observations from darrington ranger station fig 5 comparison of observations and modeled precipitation shows that observed precipitation at low elevations is less than the monthly averages modeled by salathé et al 2014 during spring and summer spatial variations observed with livneh et al 2013 show large deviations between neighboring cells especially in comparison to the smoother spatial trends organized with the elevation gradient can be observed from the salathé et al 2014 fig 5a f monthly temperature statistics were computed for each grid cell using the daily minimum and maximum temperature between jan 1 1950 through dec 31 2010 fig 6 the distribution of mean maximum temperature shows that livneh et al 2013 interpolated meteorology has greater variability than salathé et al 2014 wrf model outputs this effect is also observed when comparing the distribution of mean monthly minimum temperature noting that livneh et al 2013 has more extreme hot and cold trends sometimes up to 5 c difference compared with the salathé et al 2014 wrf model outputs reference meteorological observations from the darrington ranger station closely resemble the livneh et al 2013 interpolated meteorology livneh et al 2013 values are dependent on source observations clustered around low elevation gridded cells light and dark gray with coop stations sparse observations limit the performance assessment for high elevation gridded cells while average daily minimum temperature seems to be comparable salathé et al 2014 predicts colder maximum temperatures for low elevation areas for all months and warmer temperatures for higher elevation areas orange line from november through april 3 4 compute exceedance probabilities functions used monthlyexceedence mmday computesurfacearea cfs to mmday fig 7 displays the monthly 10 exceedance probability and average 50 thresholds for unrouted potential runoff using two vic gridded data products for the sauk suiattle watershed not to be confused with approximations like the 10 000 year flood which are based on empirical streamflow values the probabilities generated by this function are based on empirical unrouted model outputs which have limited numeric range and interpretation thereof each point in the distributions represent the potential runoff threshold at which there is only a 10 chance expectation of exceeding that value in that month both gridded data products display a slight increase in november where the muted impact of the highest extreme winter flood events on lower average monthly flows followed by a high peak flow in june july from snowmelt runoff using the same color scale and axis ranges fig 7 right side the major contribution to potential runoff in any given year is from the cascade mountain ranges during the snowmelt season june july although the 10 exceedance probability at the highest elevation are comparable between data products the trends at the low elevation stations indicate that livneh et al 2015 produces more potential runoff between november through april months than livneh et al 2013 livneh et al 2015 applies a bias correction using prism data as a proxy for observations which produces more precipitation from winter rainfall season compared to results without a bias correction a comparison between the two gridded data products illustrates that livneh et al 2015 boxplots have approximately 2 3 mm median increase in potential runoff between october to may compared to livneh et al 2013 fig 7 the monthly 10 exceedance probability with sauk river near sauk wa discharge observations range from 4 2 to 15 3 mm day across the calendar months the exceedance probability threshold peaks in two months november and june corresponding with fall atmospheric river rainfall dominated storm events and early summer snowmelt the first smaller peak is observed in november which aligns with both livneh et al 2013 2015 the boxplot distributions the second larger peak occurs in june where the mean decreases though the variance increases in july the sauk river near sauk gauge is approximate 81 m above sea level half of the average elevation in gridded cell 3 where it is located the spatially averaged mm day routed streamflow dynamics in sauk river near sauk can be expected to fall within the elevation mean of unrouted vic modeled runoff using the lower resolution 1 16 gridded cells the observed data at the outlet is provided for context and the modeled results is provided to demonstrate the spatial variability of the grid cells contributing to the modeled streamflow at the watershed outlet 4 discussion the primary step in the workflow presented here is treatgeoself which enables users to control gridded cell inclusion and exclusion using shapefile guided data selection it generates the mapping file to guide distributed computing for data download and data processing this catalogue allows for machine reading selection and sorting of available data the examples use watersheds defined by huc12 boundaries but the shape can be user defined e g census block legislative boundaries fish species migration spatial clusters geopandas operations enable treatgeoself to transform shapefiles of varying spatial projections and to include buffer regions in the early development stage treatgeoself applied unfiltered spatial intersection with each shape polygon in the shapefile resulting in slow mapping performance and interpretation difficulty when buffer regions were included at present study sites with multiple subpolygons are merged by spatial union into a single multipolygon object simplifying treatgeoself into a first order loop intercomparison of gridded data products of different gridded parcel schemas are expected to be enabled and more efficient with the use of the projection alignment and cross mapping functionalities data download operations are functionalized for distributed computing but the concurrent queue and transfer rate are limited by the computing resources allocated by the user and the data content provider data download was found to be rate limited to approximately 5 concurrent web requests to the livneh et al 2013 web domain all other gridded data product hosts enabled 10 or more concurrent web requests a rate limiter for the number of parallel data retrieval tasks was incorporated into the data download functions but not for local data processing operations the rate of data transfer would need to be assessed before ogh could integrate data servers with restful api such as erddap which could expedite mapping and retrieval of gridded data products and metadata simons and mendelssohn 2012 other limits include nuanced issues of data maintenance by the data publisher provider for example during production and testing of workflow and functions data products mentioned in livneh et al 2013 were migrated to a new web domain resulting in misdirected requests annotations and data retrieval functions may need updating over time we qualitatively described differences between two gridded data products of the same empirically estimated statistical exceedance probability approach the vic modeled gridded data products originate from unrouted flow modeling in contrast to empirically estimated 10 000 year flood at stream gauges empirically estimated exceedance probability for grid cells without flow routing may be limited in interpretations to potential runoff it is unclear how the different model simulations may affect these interpretations these concerns regarding model comparison would merit further research and development of functionalities for in watershed stream gauge selection and quantitative determinations for the goodness of fit between routed and unrouted modeled values relative to those estimated from at stream observations while we designed ogh specifically for users who primarily import ascii formatted files into hydrologic and earth surface model software e g landlab hobley et al 2017 a noteworthy limitation of using ascii file format is that as netcdf adoption is increasing as a data standard ascii time series may not be available for newer gridded data products this limitation is addressed using the proposed minimum information criteria and having initial criteria for conducting gridded data product intercomparisons netcdf files are embedded with metadata while ascii files are unannotated to inform the structure and use of the ascii files the proposed minimum information criteria serves as a road map for locating gridded data product files and considers the schema of the file organization and the features within each file gridded data products published by livneh et al 2013 partitioned files by spatial bounding box subfolders denoted by the file prefix west east south and north cardinal limits for livneh et al 2013 scrape domain and maptoblock functions were designed to abstract the bounding boxes then decide the subfolder identity by spatial intersection the spatial bounding box for gridded cells within british columbia canada did not follow this folder naming structure thus a separate annotation was provided for the spatial boundary in british columbia canada among the annotated ascii gridded data products we ve observed a variety of file organizations different gridded cell schemas spatial resolution or netcdf file organizations may be adaptable retrieval and data management of netcdf files in cloud computing environments would benefit from further design assessments as it is not yet clear how to conduct or evaluate netcdf to ascii intercomparison without a priori format preferences that may result in information loss in addition the development of a user centered reference of controlled vocabulary would improve the usefulness and adoption of a minimum information criteria that can be used across data formats these may help adapt climate and water resource information for researching interdisciplinary questions with other data products such as air quality or population data sets wohlstadter et al 2016 lloyd et al 2017 for use by researchers who are not hydrometeorology analyst netcdf files contain data outside the study area extent 1d ascii time series files may be the preferred format for small study areas 1 100 km2 we tested and developed the examples using hydroshare for the computing and data sharing environment an important benefit of hydroshare is that it hosts a rest api that enables data migration and the creation of new shareable data objects additionally as a community repository for hydrologic science fair publication of hydrologic data sets and software execution with reproducible workflows is demonstrated with the use cases developed in this work ogh operations are technically independent of hydroshare and minor changes would allow the code to operate in other similar computing and data sharing environments such as local servers cloud servers amazon aws microsoft azure and dockerized virtual environments with a jupyter instance data world dataone pangeo esiphub 5 conclusions ogh is a toolkit that makes download and processing of large climate datasets more efficient by leveraging distributed computing for watershed scale research and intercomparison of ascii gridded data products which extends climate modeling products to represent otherwise sparsely observed parts of the landscape the mapping file output is the key data management tool which catalogs the watershed gridded cells and downloaded files as a lens across gridded data products along with the proposed minimum information criteria to annotate ascii gridded data products these data management tools enable multiprocessing and dask distributed operations comparable to the efficiency of xarray for netcdf gridded data products this metadata component improves the standardization of gridded hydrometeorology products published for use by third party researchers and scientists the dictionary of analytical data frames is a key data management device that enables key value pair retrieval and exporting of summary outputs to address user needs for exploratory data analysis and visual control various data frames were rendered into different geographic and temporal modes of human readable visual inspection overall ogh is equipped with metadata framework and workflow that makes it a useful introduction and training tool for watershed studies using gridded data products and ascii time series data sets the data summary capabilities increase the efficiency of comparing multiple gridded hydrometeorology products without discontinuous use of different software ogh and the four use cases demonstrated are available for interactive use on hydroshare https www hydroshare org resource 87dc5742cf164126a11ff45c3307fd9d and also available for open development from the university of washington freshwater initiative observatory repository https github com freshwater initiative observatory copyright 2017 mit license this is an open access article distributed under the terms of the massachusetts institute of technology attribution license which permits unrestricted use free of charge distribution and reproduction in any medium without warranty provided the original author and source are credited in no event shall the authors or source be held liable for claims damages or liabilities arising from use of the software software and or data availability ogh can be installed from conda forge ogh v0 2 0 is released on github https github com freshwater initiative observatory and is freely available under an mit license this github repository and the hydroshare resources are maintained by the corresponding author christina bandaragoda this python library was developed using python 3 6 conventions within a jupyterhub unix docker environment hosted on the cuahsi hydroshare server use case notebooks can be found at the github repository https github com freshwater initiative observatory tree master tutorials and the hydroshare resource https www hydroshare org resource 87dc5742cf164126a11ff45c3307fd9d acknowledgements this work benefited from the contributions from university of washington uw watershed dynamics group who helped test and develop ogh and members uw escience institute that helped with developing and using the python toolkits this project was supported in part by national science foundation graduate research fellowship program dge 1762114 hydroshare cyberinfrastructure project aci 1148453 si2 ssi landlab project aci 1450412 predicting climate change impacts on shallow landslide risks cbet 1336725 institute for translational health sciences grant ul1tr002319 and clinical and translational sciences award program national center for data to health grant u24tr002306 uw civil environmental engineering department in collaboration with uw college of engineering and researchers and scientists of the sauk suiattle indian tribe and the skagit climate consortium with funding from the bureau of indian affairs a05av00078 this work was supported by the washington research foundation and data science environment project award from the gordon and betty moore foundaton award 2013 10 29 and alfred p sloan foundation award 3835 to the university of washington escience institute the project uses the hydroshare platform which is supported by the consortium of universities for the advancement of hydrologic sciences inc cuahsi a research organization supported by nsf cooperative agreement ear 1338606 with ongoing developments supported by nsf grants oac 1664061 oac 1664018 and oac 1664119 the content is solely the responsibility of the authors and does not represent the official views of these funding agencies special thanks to early reviewers who contributed to the editing of this paper drs dan ames emilio mayorga and nicoleta cristea abbreviations ogh observatory for gridded hydrometeorology conus conterminous united states wrf weather research forecasting fair findable accessible interoperable and reusable hs restclient hydroshare rest api python library vic variable infiltration capacity appendix a supplementary data the following are the supplementary data to this article supp table2 v3 v4 supp table2 v3 v4 final ogh manuscript v4 v2 docx 29 final ogh manuscript v4 v2 docx 29 data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 007 baxter and sommerville 2011 g baxter i sommerville socio technical systems from design methods to systems engineering interact comput 23 1 2011 jan 4 17 castronova et al 2017 a m castronova l brazil m seul cloud based jupyter notebooks for water data analysis agu fall meeting abstracts 2017 dec cutter et al 2014 s l cutter k d ash c t emrich the geographies of community disaster resilience glob environ chang 29 2014 nov 30 65 77 https doi org 10 1016 j gloenvcha 2014 08 005 daly et al 1994 c daly r p neilson d l phillips a statistical topographic model for mapping climatological precipitation over mountainous terrain j appl meteorol 33 2 1994 feb 140 158 devi et al 2012 k r devi a m sen k hemachandran a working framework for the user centered design approach and a survey of the available methods international journal of scientific and research publications 2 4 2012 apr gampe and ludwig 2017 d gampe r ludwig evaluation of gridded precipitation data products for hydrological applications in complex topography hydrology 4 4 2017 nov 16 53 https doi org 10 3390 hydrology4040053 gardner et al 2018 m a gardner c g morton j l huntington r g niswonger w r henson input data processing tools for the integrated hydrologic model gsflow environ model softw 2018 aug 9 https doi org 10 1016 j envsoft 2018 07 020 gillies 2007 sean gillies shapely manipulation and analysis of geometric objects toblerity org 2007 https github com toblerity shapely gillies 2011 sean gillies fiona is ogr s neat nimble no nonsense api toblerity org 2011 https github com toblerity fiona heard et al 2014 j heard d tarboton r idaszak j horsburgh d ames a bedig a castronova a couch an architectural overview of hydroshare a next generation hydrologic information system 2014 cuny academic works https academicworks cuny edu cc conf hic 311 henn et al 2018 b henn a j newman b livneh c daly j d lundquist an assessment of differences in gridded precipitation datasets in complex terrain j hydrol 556 2018 jan 1 1205 1219 https doi org 10 1016 j jhydrol 2017 03 008 hobley et al 2017 d e j hobley j m adams s s nudurupati e w h hutton n m gasparini e istanbulluoglu g e tucker creative computing with landlab an open source toolkit for building coupling and exploring two dimensional numerical models of earth surface dynamics earth surface dynamics 2017 10 5194 esurf 5 21 2017 horsburgh et al 2016 j s horsburgh m m morsy a m castronova j l goodall t gan h yi m j stealey d g tarboton hydroshare sharing diverse environmental data types and models as social objects with application to the hydrology domain jawra journal of the american water resources association 52 4 2016 aug 1 873 889 https doi org 10 1111 1752 1688 12363 hydroshare rest api python client library 2018 hydroshare rest api python client library release v1 2 12 2018 http hs restclient readthedocs io en latest last accessed in aug 2018 jordahl 2014 k geopandas jordahl python tools for geographic data 2014 https github com geopandas geopandas kadlec et al 2015 j kadlec b stclair d p ames r a gill waterml r package for managing ecological experiment data on a cuahsi hydroserver ecol inf 28 2015 jul 31 19 28 https doi org 10 1016 j ecoinf 2015 05 002 king et al 2013 a d king l v alexander m g donat the efficacy of using gridded data to examine extreme rainfall characteristics a case study for australia int j climatol 33 10 2013 aug 1 2376 2387 https doi org 10 1002 joc 3588 konrad and voss 2012 c p konrad f d voss analysis of streamflow gaging network for monitoring stormwater in small streams in the puget sound basin washington u s geological survey scientific investigations report 2012 2012 5020 16 p ledesma and futter 2017 j l ledesma m n futter gridded climate data products are an alternative to instrumental measurements as inputs to rainfall runoff models hydrol process 31 18 2017 aug 30 3283 3293 https doi org 10 1002 hyp 11269 liang et al 1994 x liang d p lettenmaier e f wood s j burges a simple hydrologically based model of land surface water and energy fluxes for general circulation models j geophys res atmosphere 99 d7 1994 jul 20 14415 14428 https doi org 10 1029 94jd00483 livneh et al 2013 b livneh e a rosenberg c lin b nijssen v mishra k m andreadis e p maurer d p lettenmaier a long term hydrologically based dataset of land surface fluxes and states for the conterminous united states update and extensions j clim 26 23 2013 dec 9384 9392 https doi org 10 1175 jcli d 12 00508 1 livneh et al 2015 b livneh t j bohn d w pierce f munoz arriola b nijssen r vose d r cayan l brekke a spatially comprehensive hydrometeorological data set for mexico the us and southern canada 1950 2013 scientific data 2 2015 aug 18 150042 https doi org 10 1038 sdata 2015 42 lloyd et al 2017 c t lloyd a sorichetta a j tatem high resolution global gridded data for use in population studies scientific data 4 2017 jan 31 170001 https doi org 10 1038 sdata 2017 1 maurer et al 2002 e p maurer a w wood j c adam d p lettenmaier b nijssen a long term hydrologically based dataset of land surface fluxes and states for the conterminous united states j clim 15 22 2002 nov 3237 51 https doi org 10 1175 1520 0442 2002 015 3237 althbd 2 0 co 2 mckinney 2011 w mckinney pandas a foundational python library for data analysis and statistics python for high performance and scientific computing 2011 jun 1 9 mons et al 2017 b mons c neylon j velterop m dumontier l o da silva santos m d wilkinson cloudy increasingly fair revisiting the fair data guiding principles for the european open science cloud inf serv use 37 1 2017 jan 1 49 56 https doi org 10 3233 isu 170824 read et al 201 j s read j i walker a p appling d l blodgett e k read l a winslow geoknife reproducible web processing of large gridded datasets ecography 39 4 2016 apr 354 360 rocklin 2015 m rocklin dask parallel computation with blocked algorithms and task scheduling proceedings of the 14th python in science conference 2015 no 130 136 salathé et al 2014 e p salathé jr a f hamlet c f mass s y lee m stumbaugh r steed estimates of twenty first century flood risk in the pacific northwest based on regional climate model simulations j hydrometeorol 15 5 2014 oct 1881 1899 https doi org 10 1175 jhm d 13 0137 1 salathé et al 2010 e p salathé l r leung y qian y zhang regional climate model projections for the state of washington clim change 102 1 2 2010 sep 1 51 75 simons and mendelssohn 2012 r a simons r mendelssohn erddap a brokering data server for gridded and tabular datasets agu fall meeting abstracts 2012 dec vogel et al 2007 r m vogel n c matalas j f england jr a castellarin an assessment of exceedance probabilities of envelope curves water resour res 43 7 2007 jul https doi org 10 1029 2006wr005586 wilkinson et al 2016 m d wilkinson m dumontier i j aalbersberg g appleton m axton a baak n blomberg j w boiten l b da silva santos p e bourne j bouwman the fair guiding principles for scientific data management and stewardship scientific data 3 2016 http doi org 10 1038 sdata 2016 18 wohlstadter et al 2016 m wohlstadter l shoaib j posey j welsh j fishman a python toolkit for visualizing greenhouse gas emissions at sub county scales environ model softw 83 2016 sep 1 237 244 https doi org 10 1016 j envsoft 2016 05 016 yanto et al 2017 yanto b livneh b rajagopalan development of a gridded meteorological dataset over java island indonesia 1985 2014 sci data 4 170072 2017 may 23 10 1038 sdata 2017 72 data 
26231,rapid evolution of internet of things is driving the increased deployment of smart sensors in environmental applications contributing to many big data characteristics of environmental monitoring most of the current environmental monitoring systems are not designed to handle real time datastreams and the best practices for datastream processing and predictive analytics are yet to be established this work presents a complex event processing cep engine for detecting anomalies in real time and demonstrates it using a series of real monitoring data from the geological carbon sequestration domain we show that the service based cep engine is instrumental for enabling environmental intelligent monitoring systems to ingest heterogeneous datastreams with scalable performance our cep framework requires minimal coding from the user and can be easily extended to other similar environmental monitoring applications keywords intelligent monitoring complex event processing machine learning anomaly detection geological carbon sequestration 1 introduction intelligent monitoring is an integrative system management technology that combines real time sensing with project specific data processing event detection predictive analytics and collaborative tools for data interpretation and decision making although the concept of intelligent monitoring has been around since the popularization of pc in 1990s bache et al 1990 sixsmith 2000 athanasiadis and mitkas 2004 recent years saw a surge of interests and applications largely because of the rapid evolution of internet of things iot the more accessible cyberinfrastructure and advances in artificial intelligence a recent report predicted that there will be between 25 and 50 billion connected devices by year 2025 manyika et al 2015 each smart device capable of sensing its surrounding environment and sharing information across a network becomes a potential data generator together these devices will shape the so called big data economy creating 4 v volume variety velocity veracity information assets that demand timely processing for enhanced insight and decision making gandomi and haider 2015 the capability to collect process and analyze big data in real time is still lacking in many fields key requirements of an environmental intelligent monitoring system ims are data wrangling e g data extraction transformation and loading event detection and visualization many legacy environmental ims platforms however are built on relational databases requiring data to be first stored and indexed before they can be processed creating a significant latency the ubiquitous presence of smart devices and sensor networks is calling for a fundamental shift in design paradigm from the client server based ims design to cloud based or even edge based design where the bulk of computing is done at the edge e g smart monitoring devices shi et al 2016 wong and kerkez 2016 granell et al 2016 regardless of the platform a fundamental task is related to processing the information flow continuously as they arrive with or without data persistence complex event processing cep refers to data processing techniques that operate according to a set of predefined rules dictating how information flows should be processed and what new event streams should be produced as outputs cugola and margara 2012 events can be thought of as single occurrences of a quantity of interest e g higher than normal pressure readings while complex events are distilled events corresponding to situations or patterns that comprise a particular meaning for the system e g consecutive high pressure readings luckham 2002 a cep engine is a software system consisting of a suite of data processing algorithms and knowledge representation sets working in a distributed manner cep engines for datastream processing typically have state management fault tolerance and high performance features environmental monitoring is inherently stateful requiring the cep engine to keep track of the state of the system including event arrival ingestion and processing times castro fernandez et al 2013 to handle datastreams a cep engine needs to process a large amount of continuous data with low latency performance and in case of system outrage the engine needs be able to quickly restore fault tolerance currently quite a few open source and commercial cep products are available under the apache software foundation there are more than a half dozen projects with different levels of maturity such as apache kafka flink storm and samza a comprehensive review of cep engines pre 2012 was provided by cugola and margara 2012 and more recent surveys on big data oriented cep engines can be found in liu et al 2014 flouris et al 2017 de assuncao et al 2018 so far few environmental monitoring systems have tapped into the power of cep granell et al 2016 in this study we focus on apache kafka which was initially created and open sourced by the social network company linkedin in 2011 originally designed as a messaging queue middleware i e software acting as mediators between applications or systems kafka has evolved into a high performance distributed streaming platform that provides three main functionalities a publish and subscribe to real time applications or topics b store event streams or data records in a fault tolerant way and c process data as they occur apache kafka 2018 kafka is now one of the most mature cep engines in the open source world recently apache kafka was integrated by confluent io https www confluent io product confluent open source into a real time streaming processing ecosystem consisting of a large collection of infrastructure services such as database connectors and configuration managers adaptation of cep is domain specific especially with regard to the notion of intelligence which means the cep engine needs to have a reasonable knowledge representation of its world understand what is happening in terms of events and know what reactions and processes it should invoke in this work we present a cep engine for anomaly detection in geological carbon sequestration gcs projects carbon capture and storage is a geoengineering measure for reducing anthropogenic greenhouse gas emission to the atmosphere haszeldine 2009 bickle 2009 potential gcs repositories may include depleted oil gas reservoirs and deep saline aquifers all having leakage risks the safe and efficient operation of gcs repositories thus requires integrated monitoring to track the injected carbon dioxide co2 plume as it moves in a storage formation current gcs projects are data intensive as a result of proliferation of digital instrumentation and smart technologies the success of gcs thus depends in a large part on the monitoring system s capability to access assimilate and analyze heterogeneous data in a timely manner and to provide high level intelligent information to the operators so far few gcs studies have attempted to integrate computing components in an online environment to support intelligent monitoring sun et al 2018 the major contributions of this work are in a adapting a high performance cep engine for gcs monitoring and b creating a streamlined intelligent monitoring workflow that requires minimal coding effort from domain users all modules of our system are loosely coupled so that the system is flexible and expandable and its components are replaceable when connecting to new monitoring devices or applications as part of the demonstration we showcase the system features using both scalar and vector data collected during a gcs field campaign 2 data and methods 2 1 system architecture fig 1 shows the overall system design for the ims which consists of three layers namely the data layer processing layer and knowledge discovery layer in the data layer the input data types may include time series measurements of point values and vectors multidimensional data and model outputs multidimensional data we use a no sql non relational database influxdb https github com influxdata influxdb to store the monitoring data as opposed to the traditional sql databases using predefined database schema no sql databases use dynamic schema and are best suited for applications requiring high performance flexibility and scalability the processing layer hosts the cep engine we use the kafka ecosystem distributed by confluent the discovery layer is a web portal supporting collaborative visual analytics kafka is designed around four key concepts broker topic producer and consumer a kafka topic provides a way of organizing messages which in turn serves as intermediate data containers for records to be transmitted between applications systems the topic data schema is defined by the user for different sensor types internally each topic is organized in a number of partitions for faster information retrieval and data redundancy a kafka producer writes to a topic while a kafka consumer reads from a partition a kafka broker is a hardware node in a distributed system that handles the actual reading and writing and load balancing the user is responsible for defining producer s and consumer s kafka provides application programming interfaces api for developers to create customized producers and consumers in addition the kafka connectors allow configuration of sources sinks that connect kafka topics to known applications or data systems via standard interfaces such as jdbc relational sql databases hadoop distributed file system hdfs and amazon s3 custom connectors are used to link the cep to the data layer and knowledge discovery layer automating the information exchange between cep and those layers we also developed producer and consumer templates for our use case such that the workflow can be easily adapted to different types of sensor datastreams visualization is instrumental for assisting knowledge discovery and decision support especially in environmental sciences laniak et al 2013 a number of open source data portals have appeared in business intelligence in recent years after preliminary evaluation of several products we choose the apache superset https superset incubator apache org because of its rich collection of data visualization tools including maps easy to use interface for uploading and transforming data and seamless integration with commonly used data stores we emphasize that the role of the event database shown in fig 1 is only for storing processed results while all datastream processing is done in the cep the service oriented architecture presented in fig 1 is general for demonstration we deploy both kafka confluent v4 0 and superset v0 24 on the same ubuntu linux system v16 04 running on a cloud based virtual machine instance which is hosted on a cluster node with intel xeon haswell processor and 128 gb ram the superset is served using the nginx https www nginx com web server all programming is done in python 2 2 data and event processing for the purpose of this work event processing is related to detecting anomalies in sensor data algorithms for anomaly detection have long been studied in statistics and computer science surveys of general and conventional event detection algorithms are provided in chandola et al 2009 aggarwal 2015 surveys of machine learning algorithms for anomaly detection are provided in zohrevand et al 2017 challenges specific to geosciences are a data tends to vary both spatially and temporally representing disparate scales and storage formats chen et al 2014 b no single anomaly detection algorithm fits all purposes c the nominal model or baseline is elusive in many situations d anomalies may be shadowed by noise e labeled anomaly data is rare creating imbalance in the training data and finally f establishing the causal mechanism i e event attribution can be challenging for subsurface processes 2 2 1 data data used in this study was collected from a series of field experiments conducted in january 2015 at cranfield an active oil field located in natchez mississippi u s the original purpose of the experiments was to demonstrate a time lapse pressure based leakage detection technique using modulated injection patterns three wells are located at the experimental site a co2 injector denoted as f1 and two monitoring wells denoted as f2 and f3 the experiments consisted of two phases in the first phase the bottom hole pressure and well casing temperature were monitored in the monitoring well f2 to establish the base case the raw pressure data was recorded every 2 s using a high resolution downhole gauge ranger gauge systems sugar land texas usa the raw distributed temperature sensing dts data was collected using a fiber optic sensing device silixa ltd houston usa in 10 min intervals in the second phase controlled co2 release tests were conducted in the adjacent well f3 to create leakage events while monitoring data was continuously acquired from f2 the bottom hole distance between f1 and f2 is 60 m between f1 and f3 it is 93 m and between f2 and f3 it is 33 5 m more details on the experimental setup and data collection methods are provided in sun et al 2016 2 2 2 event processing methods the cranfield data set provides a unique opportunity to demonstrate geospatial intelligent monitoring in real time in particular pressure data are analyzed using the isolationforest ifo algorithm liu et al 2008 anomalies in time series may manifest as abrupt changes in signals or as shifts in the temporal trend traditional anomaly detection methods include both regression based e g autoregressive integrated moving average model arima and classification based methods e g support vector machine svm aggarwal 2015 many of these traditional methods however are optimized to capture the normal data patterns but not anomalies ifo is specially designed to detect anomalies it is based on the premises that anomalies have attribute values that are very different from the rest of the data instances and that anomaly instances are relatively few to isolate anomalies from a data set ifo partitions data samples recursively using an ensemble of random trees when a sample has anomalous attributes the number of partitions required to isolate the data sample is smaller than that for a normal sample in other words anomalies are more susceptible to isolation under random partitioning ifo calculates an anomaly score by averaging path lengths equivalent to number of partitions over all random trees to help understanding these main concepts of ifo are illustrated in the schematic plot in fig 2 the algorithm only requires two user parameters namely the number of trees to build and the subsample size subsampling is devised to alleviate the effect of masking too many anomalies concealing their own presence and swamping normal instances located too close to anomalies thus helping to build better trees more efficiently liu et al 2008 computational wise ifo has a linear time complexity and a low memory requirement and has the capacity to scale up to handle extremely large data size liu et al 2008 we use the ifo function from the python machine learning library scikit learn pedregosa et al 2011 dts data were sampled every 12 cm along the well casing resulting in a large number of data points per sampling time along the 3227 m total sampling depth visual inspection of the dts data shown in the supporting information si fig s1 reveals that the data is highly correlated both spatially and temporally thus it makes sense to reduce the data dimension first for this purpose 50 sampling points in the 1000 2000 m interval are selected with an average distance of about 20 m between consecutive points see si section s1 for locations of selected points then a subspace anomaly detection method is applied on the 50 sampling points for which the general idea is to determine a small set of latent variables in which the most important anomalies are revealed as quickly as possible aggarwal 2015 ifo is mainly designed for processing single time series here we need to use a subspace anomaly detection algorithm that can operate on all selected time series but in a reduced data space the anomaly detector we adopted is based on the algorithm described in yin et al 2012 and summarized in algorithm s1 in si briefly the algorithm uses principal component analysis pca to reduce the dimension of training data matrix assuming that the nominal temperature profile can be effectively represented by using only a few principal components the resulting principal components are used to calculate a test statistic t 2 derived from the f distribution the threshold of t 2 defined for a certain significance level e g 95 is then applied online to detect anomalies on new data instances the singular value decomposition svd function from the numpy library http www numpy org is used to perform pca and the number of principal components retained is 3 the trained pressure and dts anomaly detectors are embedded in respective kafka consumers to process monitoring data in real time the resulting events are sent to the event database through kafka producers we comment that online anomaly detection in high dimensional datastreams is still a challenging research topic due to the fact that anomalies may often be buried in small combinations of dimensions in a high dimensional data set aggarwal 2015 the most appropriate algorithm needs to be determined on a case by case basis by incorporating domain insights 3 results the base case pressure data are aggregated into 1 min intervals using influxdb web queries which are then used to train and test the ifo model each row of the data matrix is a sliding window that includes information of pressure data and injection rate in a 90 min interval corresponding to the duration of a full pulsing cycle used in the cranfield experiments the data matrix consists of a total of 360 sliding windows i e each sliding window is a shift of 1 min from the previous window of which the first 70 are used for training and the rest for testing the use of the full pulsing cycle duration as the width for sliding windows is critical for the machine learning algorithms to learn normal patterns that are related to injection rate changes only so that pressure changes unrelated to injection rate changes e g due to leakage can be identified the number of trees and subsample size are both set to 200 for the pressure data set fig 3 a shows the ifo training and testing results on the 90 min base case the training period shows a single anomaly right at the beginning of the base case experiment probably because of the initial perturbations when the trained ifo model is applied to the controlled release data sequentially the model correctly labels almost every data instance as anomaly fig 3 b in comparison fig s3 in si shows the results of an svm classifier which has an overwhelmingly large false detection rate in this case the main challenge in this case is that the base case and controlled release data have very similar sinusoidal temporal patterns making it hard for svm to distinguish normal data and anomalies the dts results are given in fig 4 the entire dts period has two major anomalies associated with controlled release events as can be seen on the raw data plot in fig s1 during each release event the warm co2 at reservoir temperature quickly rose along the wellbore and then started to absorb heat due to the joule thompson expansion effect pruess 2008 as a result an abrupt disruption in the temperature profile can be observed for training an eventless period on jan 24 2015 is chosen from which the t 2 threshold see algorithm s1 for definitions is determined to be 8 4 after training the detector is applied in a sequential manner on each test sample and then compared to the threshold value fig 4 b shows that the t 2 statistic correctly identifies the temperature anomalies associated with the two controlled release events as shown by the two spikes in t 2 values the two examples here illuminate the aforementioned challenges associated with anomaly detection in geosciences namely no single algorithm fits all purposes and a significant amount of prior knowledge and insight is required to develop customized event processors thus the modular service oriented design adopted in our case has a significant advantage the apache superset visualization platform offers highly customizable dashboards for decision makers it can be set up to provide a holistic view of the project under monitoring the cranfield dashboard is shown in fig 5 which shows a map of site location the raw pressure and dts time series from 5 different depths and the event status reported by the cep 4 conclusion the need for real time analysis will continue to push the development of low latency real time complex event processing cep engines in the iot era gartner 2017 recent advances in big data analytics and distributed computing provide new abstractions to deal with complex data and simplify programming of scalable and parallel systems intelligent environment monitoring as a subdiscipline of environmental data science gibert et al 2018 needs to adapt to the ever increasing speed of new data generation and leverage the data in time to create new services and intelligent information to maximize the value of information in this work we develop and demonstrate a cep workflow for detecting anomalies in real gcs monitoring data we show that a problem specific online machine learning algorithms need to be carefully selected and trained to achieve robust performance in real time b the microservice oriented distributed architecture is instrumental for scaling up computing systems to deal with syntactic and semantic heterogeneity and 3 the use of highly interactive easy to use web interfaces should be an integral component of intelligent monitoring system ims development because they enable non programmer domain users to stay in the loop as well as to have easy access of the information and knowledge generated by the cep our case study is limited to a single site with high temporal frequency structured data cep involving many monitoring sites and or unstructured data is still a challenge for environmental ims while large volumes of data can be handled by horizontal scaling i e adding more nodes processing of large arrays may still be required e g in the case of pca wu et al 2018 future ims may require combining both distributed computing and edge computing to further improve system performance software availability the web system created in this work is hosted at http 129 114 110 45 login please contact the corresponding author for login authorization programming language python acknowledgements the work was supported by the u s department of energy national energy technology laboratory under grant number de fe0026515 we are grateful to the texas advanced computer center at ut austin for providing cloud resources through their chameleon cloud project we thank the associate editor and two anonymous reviewers for their constructive comments appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 02 015 
26231,rapid evolution of internet of things is driving the increased deployment of smart sensors in environmental applications contributing to many big data characteristics of environmental monitoring most of the current environmental monitoring systems are not designed to handle real time datastreams and the best practices for datastream processing and predictive analytics are yet to be established this work presents a complex event processing cep engine for detecting anomalies in real time and demonstrates it using a series of real monitoring data from the geological carbon sequestration domain we show that the service based cep engine is instrumental for enabling environmental intelligent monitoring systems to ingest heterogeneous datastreams with scalable performance our cep framework requires minimal coding from the user and can be easily extended to other similar environmental monitoring applications keywords intelligent monitoring complex event processing machine learning anomaly detection geological carbon sequestration 1 introduction intelligent monitoring is an integrative system management technology that combines real time sensing with project specific data processing event detection predictive analytics and collaborative tools for data interpretation and decision making although the concept of intelligent monitoring has been around since the popularization of pc in 1990s bache et al 1990 sixsmith 2000 athanasiadis and mitkas 2004 recent years saw a surge of interests and applications largely because of the rapid evolution of internet of things iot the more accessible cyberinfrastructure and advances in artificial intelligence a recent report predicted that there will be between 25 and 50 billion connected devices by year 2025 manyika et al 2015 each smart device capable of sensing its surrounding environment and sharing information across a network becomes a potential data generator together these devices will shape the so called big data economy creating 4 v volume variety velocity veracity information assets that demand timely processing for enhanced insight and decision making gandomi and haider 2015 the capability to collect process and analyze big data in real time is still lacking in many fields key requirements of an environmental intelligent monitoring system ims are data wrangling e g data extraction transformation and loading event detection and visualization many legacy environmental ims platforms however are built on relational databases requiring data to be first stored and indexed before they can be processed creating a significant latency the ubiquitous presence of smart devices and sensor networks is calling for a fundamental shift in design paradigm from the client server based ims design to cloud based or even edge based design where the bulk of computing is done at the edge e g smart monitoring devices shi et al 2016 wong and kerkez 2016 granell et al 2016 regardless of the platform a fundamental task is related to processing the information flow continuously as they arrive with or without data persistence complex event processing cep refers to data processing techniques that operate according to a set of predefined rules dictating how information flows should be processed and what new event streams should be produced as outputs cugola and margara 2012 events can be thought of as single occurrences of a quantity of interest e g higher than normal pressure readings while complex events are distilled events corresponding to situations or patterns that comprise a particular meaning for the system e g consecutive high pressure readings luckham 2002 a cep engine is a software system consisting of a suite of data processing algorithms and knowledge representation sets working in a distributed manner cep engines for datastream processing typically have state management fault tolerance and high performance features environmental monitoring is inherently stateful requiring the cep engine to keep track of the state of the system including event arrival ingestion and processing times castro fernandez et al 2013 to handle datastreams a cep engine needs to process a large amount of continuous data with low latency performance and in case of system outrage the engine needs be able to quickly restore fault tolerance currently quite a few open source and commercial cep products are available under the apache software foundation there are more than a half dozen projects with different levels of maturity such as apache kafka flink storm and samza a comprehensive review of cep engines pre 2012 was provided by cugola and margara 2012 and more recent surveys on big data oriented cep engines can be found in liu et al 2014 flouris et al 2017 de assuncao et al 2018 so far few environmental monitoring systems have tapped into the power of cep granell et al 2016 in this study we focus on apache kafka which was initially created and open sourced by the social network company linkedin in 2011 originally designed as a messaging queue middleware i e software acting as mediators between applications or systems kafka has evolved into a high performance distributed streaming platform that provides three main functionalities a publish and subscribe to real time applications or topics b store event streams or data records in a fault tolerant way and c process data as they occur apache kafka 2018 kafka is now one of the most mature cep engines in the open source world recently apache kafka was integrated by confluent io https www confluent io product confluent open source into a real time streaming processing ecosystem consisting of a large collection of infrastructure services such as database connectors and configuration managers adaptation of cep is domain specific especially with regard to the notion of intelligence which means the cep engine needs to have a reasonable knowledge representation of its world understand what is happening in terms of events and know what reactions and processes it should invoke in this work we present a cep engine for anomaly detection in geological carbon sequestration gcs projects carbon capture and storage is a geoengineering measure for reducing anthropogenic greenhouse gas emission to the atmosphere haszeldine 2009 bickle 2009 potential gcs repositories may include depleted oil gas reservoirs and deep saline aquifers all having leakage risks the safe and efficient operation of gcs repositories thus requires integrated monitoring to track the injected carbon dioxide co2 plume as it moves in a storage formation current gcs projects are data intensive as a result of proliferation of digital instrumentation and smart technologies the success of gcs thus depends in a large part on the monitoring system s capability to access assimilate and analyze heterogeneous data in a timely manner and to provide high level intelligent information to the operators so far few gcs studies have attempted to integrate computing components in an online environment to support intelligent monitoring sun et al 2018 the major contributions of this work are in a adapting a high performance cep engine for gcs monitoring and b creating a streamlined intelligent monitoring workflow that requires minimal coding effort from domain users all modules of our system are loosely coupled so that the system is flexible and expandable and its components are replaceable when connecting to new monitoring devices or applications as part of the demonstration we showcase the system features using both scalar and vector data collected during a gcs field campaign 2 data and methods 2 1 system architecture fig 1 shows the overall system design for the ims which consists of three layers namely the data layer processing layer and knowledge discovery layer in the data layer the input data types may include time series measurements of point values and vectors multidimensional data and model outputs multidimensional data we use a no sql non relational database influxdb https github com influxdata influxdb to store the monitoring data as opposed to the traditional sql databases using predefined database schema no sql databases use dynamic schema and are best suited for applications requiring high performance flexibility and scalability the processing layer hosts the cep engine we use the kafka ecosystem distributed by confluent the discovery layer is a web portal supporting collaborative visual analytics kafka is designed around four key concepts broker topic producer and consumer a kafka topic provides a way of organizing messages which in turn serves as intermediate data containers for records to be transmitted between applications systems the topic data schema is defined by the user for different sensor types internally each topic is organized in a number of partitions for faster information retrieval and data redundancy a kafka producer writes to a topic while a kafka consumer reads from a partition a kafka broker is a hardware node in a distributed system that handles the actual reading and writing and load balancing the user is responsible for defining producer s and consumer s kafka provides application programming interfaces api for developers to create customized producers and consumers in addition the kafka connectors allow configuration of sources sinks that connect kafka topics to known applications or data systems via standard interfaces such as jdbc relational sql databases hadoop distributed file system hdfs and amazon s3 custom connectors are used to link the cep to the data layer and knowledge discovery layer automating the information exchange between cep and those layers we also developed producer and consumer templates for our use case such that the workflow can be easily adapted to different types of sensor datastreams visualization is instrumental for assisting knowledge discovery and decision support especially in environmental sciences laniak et al 2013 a number of open source data portals have appeared in business intelligence in recent years after preliminary evaluation of several products we choose the apache superset https superset incubator apache org because of its rich collection of data visualization tools including maps easy to use interface for uploading and transforming data and seamless integration with commonly used data stores we emphasize that the role of the event database shown in fig 1 is only for storing processed results while all datastream processing is done in the cep the service oriented architecture presented in fig 1 is general for demonstration we deploy both kafka confluent v4 0 and superset v0 24 on the same ubuntu linux system v16 04 running on a cloud based virtual machine instance which is hosted on a cluster node with intel xeon haswell processor and 128 gb ram the superset is served using the nginx https www nginx com web server all programming is done in python 2 2 data and event processing for the purpose of this work event processing is related to detecting anomalies in sensor data algorithms for anomaly detection have long been studied in statistics and computer science surveys of general and conventional event detection algorithms are provided in chandola et al 2009 aggarwal 2015 surveys of machine learning algorithms for anomaly detection are provided in zohrevand et al 2017 challenges specific to geosciences are a data tends to vary both spatially and temporally representing disparate scales and storage formats chen et al 2014 b no single anomaly detection algorithm fits all purposes c the nominal model or baseline is elusive in many situations d anomalies may be shadowed by noise e labeled anomaly data is rare creating imbalance in the training data and finally f establishing the causal mechanism i e event attribution can be challenging for subsurface processes 2 2 1 data data used in this study was collected from a series of field experiments conducted in january 2015 at cranfield an active oil field located in natchez mississippi u s the original purpose of the experiments was to demonstrate a time lapse pressure based leakage detection technique using modulated injection patterns three wells are located at the experimental site a co2 injector denoted as f1 and two monitoring wells denoted as f2 and f3 the experiments consisted of two phases in the first phase the bottom hole pressure and well casing temperature were monitored in the monitoring well f2 to establish the base case the raw pressure data was recorded every 2 s using a high resolution downhole gauge ranger gauge systems sugar land texas usa the raw distributed temperature sensing dts data was collected using a fiber optic sensing device silixa ltd houston usa in 10 min intervals in the second phase controlled co2 release tests were conducted in the adjacent well f3 to create leakage events while monitoring data was continuously acquired from f2 the bottom hole distance between f1 and f2 is 60 m between f1 and f3 it is 93 m and between f2 and f3 it is 33 5 m more details on the experimental setup and data collection methods are provided in sun et al 2016 2 2 2 event processing methods the cranfield data set provides a unique opportunity to demonstrate geospatial intelligent monitoring in real time in particular pressure data are analyzed using the isolationforest ifo algorithm liu et al 2008 anomalies in time series may manifest as abrupt changes in signals or as shifts in the temporal trend traditional anomaly detection methods include both regression based e g autoregressive integrated moving average model arima and classification based methods e g support vector machine svm aggarwal 2015 many of these traditional methods however are optimized to capture the normal data patterns but not anomalies ifo is specially designed to detect anomalies it is based on the premises that anomalies have attribute values that are very different from the rest of the data instances and that anomaly instances are relatively few to isolate anomalies from a data set ifo partitions data samples recursively using an ensemble of random trees when a sample has anomalous attributes the number of partitions required to isolate the data sample is smaller than that for a normal sample in other words anomalies are more susceptible to isolation under random partitioning ifo calculates an anomaly score by averaging path lengths equivalent to number of partitions over all random trees to help understanding these main concepts of ifo are illustrated in the schematic plot in fig 2 the algorithm only requires two user parameters namely the number of trees to build and the subsample size subsampling is devised to alleviate the effect of masking too many anomalies concealing their own presence and swamping normal instances located too close to anomalies thus helping to build better trees more efficiently liu et al 2008 computational wise ifo has a linear time complexity and a low memory requirement and has the capacity to scale up to handle extremely large data size liu et al 2008 we use the ifo function from the python machine learning library scikit learn pedregosa et al 2011 dts data were sampled every 12 cm along the well casing resulting in a large number of data points per sampling time along the 3227 m total sampling depth visual inspection of the dts data shown in the supporting information si fig s1 reveals that the data is highly correlated both spatially and temporally thus it makes sense to reduce the data dimension first for this purpose 50 sampling points in the 1000 2000 m interval are selected with an average distance of about 20 m between consecutive points see si section s1 for locations of selected points then a subspace anomaly detection method is applied on the 50 sampling points for which the general idea is to determine a small set of latent variables in which the most important anomalies are revealed as quickly as possible aggarwal 2015 ifo is mainly designed for processing single time series here we need to use a subspace anomaly detection algorithm that can operate on all selected time series but in a reduced data space the anomaly detector we adopted is based on the algorithm described in yin et al 2012 and summarized in algorithm s1 in si briefly the algorithm uses principal component analysis pca to reduce the dimension of training data matrix assuming that the nominal temperature profile can be effectively represented by using only a few principal components the resulting principal components are used to calculate a test statistic t 2 derived from the f distribution the threshold of t 2 defined for a certain significance level e g 95 is then applied online to detect anomalies on new data instances the singular value decomposition svd function from the numpy library http www numpy org is used to perform pca and the number of principal components retained is 3 the trained pressure and dts anomaly detectors are embedded in respective kafka consumers to process monitoring data in real time the resulting events are sent to the event database through kafka producers we comment that online anomaly detection in high dimensional datastreams is still a challenging research topic due to the fact that anomalies may often be buried in small combinations of dimensions in a high dimensional data set aggarwal 2015 the most appropriate algorithm needs to be determined on a case by case basis by incorporating domain insights 3 results the base case pressure data are aggregated into 1 min intervals using influxdb web queries which are then used to train and test the ifo model each row of the data matrix is a sliding window that includes information of pressure data and injection rate in a 90 min interval corresponding to the duration of a full pulsing cycle used in the cranfield experiments the data matrix consists of a total of 360 sliding windows i e each sliding window is a shift of 1 min from the previous window of which the first 70 are used for training and the rest for testing the use of the full pulsing cycle duration as the width for sliding windows is critical for the machine learning algorithms to learn normal patterns that are related to injection rate changes only so that pressure changes unrelated to injection rate changes e g due to leakage can be identified the number of trees and subsample size are both set to 200 for the pressure data set fig 3 a shows the ifo training and testing results on the 90 min base case the training period shows a single anomaly right at the beginning of the base case experiment probably because of the initial perturbations when the trained ifo model is applied to the controlled release data sequentially the model correctly labels almost every data instance as anomaly fig 3 b in comparison fig s3 in si shows the results of an svm classifier which has an overwhelmingly large false detection rate in this case the main challenge in this case is that the base case and controlled release data have very similar sinusoidal temporal patterns making it hard for svm to distinguish normal data and anomalies the dts results are given in fig 4 the entire dts period has two major anomalies associated with controlled release events as can be seen on the raw data plot in fig s1 during each release event the warm co2 at reservoir temperature quickly rose along the wellbore and then started to absorb heat due to the joule thompson expansion effect pruess 2008 as a result an abrupt disruption in the temperature profile can be observed for training an eventless period on jan 24 2015 is chosen from which the t 2 threshold see algorithm s1 for definitions is determined to be 8 4 after training the detector is applied in a sequential manner on each test sample and then compared to the threshold value fig 4 b shows that the t 2 statistic correctly identifies the temperature anomalies associated with the two controlled release events as shown by the two spikes in t 2 values the two examples here illuminate the aforementioned challenges associated with anomaly detection in geosciences namely no single algorithm fits all purposes and a significant amount of prior knowledge and insight is required to develop customized event processors thus the modular service oriented design adopted in our case has a significant advantage the apache superset visualization platform offers highly customizable dashboards for decision makers it can be set up to provide a holistic view of the project under monitoring the cranfield dashboard is shown in fig 5 which shows a map of site location the raw pressure and dts time series from 5 different depths and the event status reported by the cep 4 conclusion the need for real time analysis will continue to push the development of low latency real time complex event processing cep engines in the iot era gartner 2017 recent advances in big data analytics and distributed computing provide new abstractions to deal with complex data and simplify programming of scalable and parallel systems intelligent environment monitoring as a subdiscipline of environmental data science gibert et al 2018 needs to adapt to the ever increasing speed of new data generation and leverage the data in time to create new services and intelligent information to maximize the value of information in this work we develop and demonstrate a cep workflow for detecting anomalies in real gcs monitoring data we show that a problem specific online machine learning algorithms need to be carefully selected and trained to achieve robust performance in real time b the microservice oriented distributed architecture is instrumental for scaling up computing systems to deal with syntactic and semantic heterogeneity and 3 the use of highly interactive easy to use web interfaces should be an integral component of intelligent monitoring system ims development because they enable non programmer domain users to stay in the loop as well as to have easy access of the information and knowledge generated by the cep our case study is limited to a single site with high temporal frequency structured data cep involving many monitoring sites and or unstructured data is still a challenge for environmental ims while large volumes of data can be handled by horizontal scaling i e adding more nodes processing of large arrays may still be required e g in the case of pca wu et al 2018 future ims may require combining both distributed computing and edge computing to further improve system performance software availability the web system created in this work is hosted at http 129 114 110 45 login please contact the corresponding author for login authorization programming language python acknowledgements the work was supported by the u s department of energy national energy technology laboratory under grant number de fe0026515 we are grateful to the texas advanced computer center at ut austin for providing cloud resources through their chameleon cloud project we thank the associate editor and two anonymous reviewers for their constructive comments appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 02 015 
26232,reservoir sedimentation is a major environmental issue and integrated sediment modeling provides a holistic picture of the sediment transport processes in watershed management planning this study proposes a sediment modeling framework by coupling a watershed loading model soil and water assessment tool swat with a receiving water body model environmental fluid dynamics code efdc in the proposed framework upstream sediment load estimates made by swat are dynamically linked to efdc to predict sediment deposition in a reservoir facilitating multiple model runs required for what if scenario analyses in developing sediment management plans and for optimization using sampling based search algorithms the modeling framework was applied to predict the spatial distributions of sediment depositions in an agricultural reservoir and quantify their uncertainty this article introduces how the two models swat and efdc are linked for integrated sediment modeling and demonstrates how the proposed framework can be applied in a case study keywords sediment transport reservoir sedimentation model coupling swat efdc uncertainty 1 introduction reservoirs have been commonly used to control water resources for a long time providing storage space for flood control irrigation water supply and other uses yeh 1985 biemans et al 2011 sedimentation which is a naturally occurring process decreases a reservoir s capacity to hold water and maintain open space to mitigate degradation in the capacity of reservoirs there are several ways available such as sediment flushing and dredging where geometry is favorable kondolf et al 2014 sedimentation can be slowed by reducing sediment loading to reservoirs and nonpoint source nps pollution control practices have provided an efficient measure to alleviate the sedimentation and water quality issues of reservoirs dillaha et al 1989 baker 1992 hao et al 2012 estimating reservoir sedimentation rates is a critical process in developing watershed management plans lee and foster 2013 the sediment load and reservoir sedimentation mass balance are often quantified using mathematical models as monitoring and sampling are too expensive and time consuming to implement in practice as erosion sediment transport and settling mixing processes are controlled by different mechanisms strategies and approaches for their modeling have been developed independently watershed loading and receiving water body models merritt et al 2003 papanicolaou et al 2008 there are many models that we can use to describe sediment transport process and predict the amount of sediment loaded to a downstream water body so called watershed loading models which include water erosion prediction project wepp flanagan and nearing 1995 dun et al 2009 boll et al 2015 soil and water assessment tool swat arnold et al 1998 and hydrological simulation program fortran hspf donigian et al 1983 there are also several models capable of simulating the hydrodynamic sediment mixing and settling processes of a water body including hydrologic engineering center hec 6 thomas and prasuhn 1977 environmental fluid dynamics code efdc hamrick 1992 and estuarine coastal and ocean model sediment transport elcomsed blumberg and mellor 1987 often called the receiving water body model although reservoir sedimentation is a sequential process starting from the head drainage watersheds of a reservoir to the outlet there are only a few modeling tools that can provide a holistic view of the sediment transport processes from loading to mixing and settling liu et al 2008 in this study we coupled a watershed loading model swat to a receiving water body model efdc to provide a simulation tool for integrated watershed and reservoir management planning in this article we describe how the coupled model works and demonstrate its applicability in a case study as the coupled modeling requires many parameters and often sedimentation observations are very limited for parameter calibration we demonstrated how to quantify the uncertainty in the integrated sediment modeling using the proposed tool under the generalized likelihood uncertainty estimation glue approach 2 model description 2 1 modeling tools swat and efdc were selected in this study as tools to simulate streamflow soil loss and sediment transport processes occurring in upstream watersheds and sediment deposition in a downstream reservoir respectively swat is one of the most popularly used watershed loading models in hydrology and water quality simulation the model can describe the effects of agricultural best management practices bmps on water sediment and agricultural chemical yields in watersheds with varying soils land use and management arnold et al 1998 srinivasan et al 1998 efdc is a receiving water body model capable of simulating hydrodynamics sediment and nutrient processes and toxic contaminant transport and the model has been applied to various water bodies including rivers lakes reservoirs wetlands estuaries and coastal regions for environmental assessment and management gao and li 2014 2 2 model coupling the swat and efdc models are coupled by linking swat outputs to the boundary conditions of the efdc modeling fig 1 for the coupling a set of matlab codes were developed to automatically convert the swat outputs to the input files of efdc including runoff discharge qser inp and sediment loads sdser inp and snser inp as the boundary conditions see sample input files provided as supporting information see appendix c when no reservoir operation record is available the modeling framework uses a reservoir water balance model module based hydrologic analysis system for agricultural watersheds masa to estimate the amount of water released through a spillway in this case the flow outputs of the swat model are first fed to the masa model to calculate reservoir outflows and then converted into the discharge input file qser inp of the efdc model the sediment load outputs of the swat model are separated by sand and silt according to the grain size distributions observed in each watershed and converted into the sediment concentration input files sdser inp for sand and snser inp for silt of the efdc model once a parameter value set for the swat model is determined sediment mixing settling and reservoir mass balance as well as runoff and sediment transport processes are automatically simulated by the coupled modeling framework the model coupling is expected to facilitate scenario analyses with different management practices and weather patterns that require multiple model runs and simulations in addition the coupling allows us to investigate the uncertainty propagation from one model to the other the uncertainty of watershed sediment loading swat modeling is expected to be transferred to the water body sediment dynamic efdc modeling in this study we demonstrated how the coupled modeling framework could help to quantify the impact of equifinality of the swat modeling on the efdc modeling outputs using a glue method setegn et al 2010 her and chaubey 2015 her and seong 2018 in the demonstration the efdc simulation processes described in fig 1 were performed iteratively with each of the behavioral swat runoff and sediment load outputs and then the uncertainty or variations of sediment deposition simulated using the efdc models were quantified in terms of deposition thickness for multiple runs of the efdc model the parfor parallel for loop of matlab was utilized and individual loop iterations were randomly executed on multiple e g twelve logical threads the six cores of intel i7 8700 k of a personal computer finally the grid based daily outputs of sediment behavior modeling were saved in ascii format rather than efdc explorer binary format so that the following post processing and uncertainty analysis could be implemented easily with common computational software such as excel matlab and r 3 model implementation 3 1 example application area for the purpose of demonstration the integrated sediment modeling framework was applied to an agricultural area the ipjang reservoir watershed located in south korea fig 2 the reservoir receives water drained from its upstream areas of 7 4 km2 and the major land use is forest 82 1 followed by other agricultural land uses 6 2 including rice paddy fields and uplands and pasture 5 7 there are two upper streams named yangdae river yang and no name river nnm which flow into the ipjang reservoir 3 2 watershed loading modeling with swat a 5 m digital elevation map dem was generated from a topographic map provided by the korea national spatial data infrastructure portal nsdi and it was used to delineate the watershed and sub watershed boundaries land use and soil maps were obtained from the korean ministry of environment me and the national institute of agricultural science nas respectively land use and soil maps were overlaid on the dem to define hydrologic response units hrus and the thresholds for land use soil and slope were set to 10 her et al 2015 the yang watershed was divided into 17 sub basins and 104 hrus and the nnm watershed was split into 13 hrus within one sub basin fig 2 daily weather observations were compiled from multiple weather stations operated by the korea ministry of land transport and maritime affairs http hrfco go kr sumun rainfalllist do and korea meteorological administration https data kma go kr rainfall data were obtained from the nearest rain gauge station the seoun automatic weather station aws and other weather data were collected from the hagaejung aws temperature and wind velocity the cheonan station relative humidity and the cheongju station solar radiation fig 2a the swat models were set up to predict daily streamflow soil erosion and sediment transport of the yang and nnm river watersheds separately then each of the models was calibrated to streamflow and sediment observations made at the watershed outlets in 2017 a spin up period of three years from 2014 to 2016 was used to improve the initial conditions including soil water content the swat model parameters were selected based on the literature and our understanding of the parameters abbaspour et al 2007 douglas mankin et al 2010 arnold et al 2012 her and chaubey 2015 see appendix a a sampling based heuristic optimization algorithm called a multi algorithm genetically adaptive multi objective amalgam was employed to locate parameter sets that provide acceptable accuracy represented by the nash sutcliffe efficiency nse nash and sutcliffe 1970 in a multi objective water discharge and sediment loads calibration practice detailed descriptions of the algorithm can be found in vrugt and robinson 2007 and zhang et al 2010 the total number of parameter samples to be populated was set to 9 600 and many parameter sets that gave equally good model performance were identified under the glue framework beven and binley 1992 her and chaubey 2015 her et al 2015 in the following uncertainty analysis a threshold nse value of 0 8 was selected to quantify the equifinality of the calibration results based on the performance evaluation criteria suggested for watershed scale modeling moriasi et al 2015 3 3 receiving water body modeling with efdc the efdc model was configured to represent three dimensional sediment movement in the ipjang reservoir for a year from january 1 to december 31 2017 on a daily basis the computational time step was set to 1 s considering the numerical stability and running time of the efdc model such efdc modeling configuration allowed a 1 year sediment simulation to take approximately 4 5 h to complete with a personal computer that has an intel i7 8700 k 3 70 ghz processor and 32 gb ram under the windows 10 operating system installed on a 256 gb solid state drive ssd the spatial extent of the efdc modeling covered the entire submerged areas of the reservoir the computational grid consisted of three horizontal layers and each horizontal layer or plane had 153 cells so that the three dimensional water and sediment dynamics could be simulated with 459 grid cells the grid size of the horizontal plane was selected with consideration of the efficiency and stability of the efdc numerical modeling the average cell width and length in the grid were 25 6 m and 47 0 m respectively the cell depths ranged from 1 0 m to 7 1 m with an average cell depth of 4 1 m the high resolution bathymetric data of the reservoir were obtained by interpolating the field measured reservoir bottom elevation the initial water surface elevation of the reservoir was set to 65 4 m based on water level observations made at the ipjang reservoir by the korea rural community corporation due to the lack of sediment monitoring data the initial sediment concentration was assumed to be zero based on an understanding of the reservoir hydrology the model parameters related to sediment simulation were determined based on the literature tetra tech 1999 see appendix b weather variables were incorporated as the surface boundary conditions into the efdc modeling including daily solar radiation wind speed air temperature atmospheric pressure relative humidity and cloud cover which were obtained from the same sources as those of the swat modeling the water and sediment inflows from the yang and nnm rivers to the reservoir were represented as the lateral inflow boundary conditions of the efdc model and spillway release and irrigation water supply were considered to be the outflow boundary conditions in the efdc modeling the inflow boundary conditions were extracted from the daily outputs of the swat modeling by converting sediment loads tons day to concentrations mg l and reformatting the converted data for efdc there was no measured outflow data for the reservoir but reservoir water level measurements made from 2014 to 2017 were available thus this study estimated the amount of water discharged out of the reservoir using water level measurements and the mass balance relationship between 1 inflow from the drainage watersheds through the yang and nnm rivers 2 outflow including spillway release and irrigation water supply and 3 changes in the water stored in the reservoir we employed an agricultural reservoir water balance model masa to estimate the reservoir outflow components from water level measurements song 2017 in the masa model the reservoir release for agricultural use was calculated considering the irrigation water requirements for rice paddies and the irrigation efficiency for conveyance and distribution to the field the water requirements were estimated based on the paddy water balance concept and the recommended parameter values from previous studies were adopted song 2017 irrigation efficiency is a function of reservoir operation and agricultural water management practices we calibrated the irrigation efficiency by setting the water level observations as a calibration target the spillway release was simulated by calculating the difference between the simulated levels and the operation levels a more detailed explanation of the reservoir outflow simulation can be found in song et al 2016 4 results the flow and sediment outputs of the swat simulation were compared with observations fig 3 the calibrated swat model reproduced daily flow and sediment loads of the study watersheds at acceptable accuracies nses of 0 86 and 0 90 for flow and sediment respectively in the case of yang nses of 0 86 and 0 99 for flow and sediment respectively in the nnm case arnold et al 2012 moriasi et al 2015 in the parameter calibration 4824 and 5035 behavioral parameter sets were identified for the swat model prepared for the yang and nnm watersheds respectively and the corresponding behavioral outputs were used to develop the uncertainty bands fig 3 because of the limited watershed flow and sediment monitoring period from july 15 to october 25 2017 a large storm event occurred on july 16 2017 must have heavily influenced the calibration and the calibrated model has not been validated in this study multiple i e 500 swat flow modeling outputs were selected from the behavioral sets and fed to the masa model to calculate the reservoir outflows including spillway runoff and irrigation water supply the masa model reproduced the overall trends of the reservoir water level fig 4 examples of sediment deposition simulated using the integrated modeling framework proposed in this study are presented in fig 5 as seen in figs 3 and 5 the application of the modeling tool shows how sediment is transported from upstream areas to a downstream water body to be deposited at the bottom how the sediment deposition can spread out over time in a reservoir and how much uncertainty exists in the spatially distributed modeling outputs the model coupling enabled us to try many different flow and sediment loading scenarios or behavioral swat model runs and thus investigate modeling uncertainty propagation in the following simulations or the efdc modeling the amount of uncertainty propagated into the receiving water modeling from the watershed loading modeling was as large as the median values of sediment thickness simulated using the behavioral sediment loading outputs emphasizing the importance of watershed loading modeling accuracy in reservoir sedimentation studies and planning when 100 days elapsed sediment loaded to the reservoir had been mainly deposited right behind the dam as time passed the sedimentation moved toward the center of the reservoir increasing the depth of the sediment in the vicinity of the inflow point or the outlets of the watersheds the spatiotemporal progress of sedimentation is in accord with our understanding of reservoir hydraulics wohl and cenderelli 2000 elçi et al 2007 the amount of the net sediment deposition was simulated based on many assumptions as mentioned previously in addition the spatiotemporal distributions of sedimentation have not been calibrated and validated due to the lack of measurement thus the uncertainty solely attributed by the efdc modeling could not be quantified and the uncertainty presented in fig 5 originated from the swat modeling only with the expected modeling limitations analysis through integrated sediment modeling can benefit from the coupling of two simulation models in watershed and reservoir management planning 5 conclusions this study proposed a coupled modeling tool for integrated sediment transport analyses and management planning in the coupling the watershed loading model swat was prepared to describe erosion and sediment transport processes in upstream watersheds and the receiving water body model efdc was linked to the swat model so that the sediment loading and spatiotemporal distributions of sedimentation in a downstream reservoir could be simulated the utility and performance of the coupled modeling tool were demonstrated in the application to an agricultural reservoir located in korea results showed how integrated sediment process modeling could be possible by coupling commonly used watershed loading and receiving water body models in addition the model coupling facilitated an uncertainty analysis that employs a sampling based calibration algorithm requiring many model runs with different parameter sets under a glue approach in this study the model coupling permitted the quantification of the uncertainty bounds of watershed flow and sediment loads and their propagation to the sedimentation modeling for a downstream reservoir the coupled models showed the potential to be a useful tool for integrated sediment modeling and management planning especially for scenario analyses and optimization of sediment bmps that require intensive simulation practices the main source codes used to couple the two models are provided in appendix c and available online https github com ems2018900 couplingswatand efdc software availability type a set of scripts for coupling swat and efdc year first released 2018 hardware required a personal computer pc tested on a pc with an intel i7 8700 k processor and 32 gb random access memory ram software required ms windows tested on windows 10 programming language matlab program size 13 4 kb source code only availability and cost publicly available at https github com ems2018900 couplingswatandefdc contact for further information yher ufl edu acknowledgements this work was supported by the the usda national institute of food and agriculture hatch project fla trc 005551 and the national research foundation of korea nrf grant funded by the korea ministry of science and ict no 2015r1a2a2a01008152 appendix a list of the swat model parameters considered in the uncertainty analysis file parameter definition range default value lower upper hru ov n manning s n value for overland flow 0 5 0 5 0 14 slope average slope steepness m m 0 5 0 5 0 41 dep imp depth to impervious layer in soil profile mm 0 6000 2094 90 epco plant uptake compensation factor 0 01 1 0 35 gw ahpha bf baseflow alpha factor days 0 1 0 84 gw delay groundwater delay days 0 30 85 75 gw revap groundwater revap coefficient 0 02 0 2 0 15 gwqmn threshold depth of water in the shallow aquifer required for return flow to occur mm 0 6000 1165 10 revapmn threshold depth of water in the shallow aquifer for revap to occur mm 1 6000 54 31 mgt cn f initial scs cn ii value 0 25 0 25 0 07 rte ch kii effective hydraulic conductivity mm hr 0 5 0 5 46 25 ch nii manning s n value for the main channel 0 5 0 5 0 11 ch sii main channel slope m m 0 5 0 5 0 17 sol sol awc available water capacity of the soil layer 0 25 0 25 0 21 sol k saturated hydraulic conductivity mm hr 0 25 0 25 0 86 sol z depth from soil surface to bottom of layer mm 0 25 0 25 32 00 sub ch ki effective hydraulic conductivity in tributary channel mm hr 0 5 0 5 0 09 ch ni manning s n value for the tributary channels 0 5 0 5 0 04 ch si average slope of tributary channel m m 0 5 0 5 0 53 bsn esco surface runoff lag time days 0 01 1 0 31 surlag surface runoff lag time days 0 0001 0 01 0 00 spcon linear parameter for calculating the maximum amount of sediment that can be reentrained during channel sediment routing 0 2 1 00 prf bsn peak rate adjustment factor for sediment routing in the main channel 1 1 5 1 00 spexp exponent parameter for calculating sediment reentrained in channel sediment routing 0 1 10 1 33 appendix b list of the efdc model parameters considered in the uncertainty analysis parameter value sand silt effective particle size class mm 0 2349 0 0144 deposition settling velocity mm s 24 784 0 142 empirical settling constant 0 0 critical shear stress for deposition n m2 0 08 erosion critical shear stress for erosion n m2 0 94 0 94 erosion rate g m2 s 0 0075 empirical erosion constant 2 2 bedload median diameter of bed sediment 0 1467 angle of repose 30 0 appendix c matlab script for coupling swat and efdc main script image 1 sub function swatrun image 2 sub function swat combine image 3 sub function swat ua image 4 sub function snser sdser image 5 sub function masa run reservoir shin image 6 sub function qser image 7 sub function runefdc fun image 8 
26232,reservoir sedimentation is a major environmental issue and integrated sediment modeling provides a holistic picture of the sediment transport processes in watershed management planning this study proposes a sediment modeling framework by coupling a watershed loading model soil and water assessment tool swat with a receiving water body model environmental fluid dynamics code efdc in the proposed framework upstream sediment load estimates made by swat are dynamically linked to efdc to predict sediment deposition in a reservoir facilitating multiple model runs required for what if scenario analyses in developing sediment management plans and for optimization using sampling based search algorithms the modeling framework was applied to predict the spatial distributions of sediment depositions in an agricultural reservoir and quantify their uncertainty this article introduces how the two models swat and efdc are linked for integrated sediment modeling and demonstrates how the proposed framework can be applied in a case study keywords sediment transport reservoir sedimentation model coupling swat efdc uncertainty 1 introduction reservoirs have been commonly used to control water resources for a long time providing storage space for flood control irrigation water supply and other uses yeh 1985 biemans et al 2011 sedimentation which is a naturally occurring process decreases a reservoir s capacity to hold water and maintain open space to mitigate degradation in the capacity of reservoirs there are several ways available such as sediment flushing and dredging where geometry is favorable kondolf et al 2014 sedimentation can be slowed by reducing sediment loading to reservoirs and nonpoint source nps pollution control practices have provided an efficient measure to alleviate the sedimentation and water quality issues of reservoirs dillaha et al 1989 baker 1992 hao et al 2012 estimating reservoir sedimentation rates is a critical process in developing watershed management plans lee and foster 2013 the sediment load and reservoir sedimentation mass balance are often quantified using mathematical models as monitoring and sampling are too expensive and time consuming to implement in practice as erosion sediment transport and settling mixing processes are controlled by different mechanisms strategies and approaches for their modeling have been developed independently watershed loading and receiving water body models merritt et al 2003 papanicolaou et al 2008 there are many models that we can use to describe sediment transport process and predict the amount of sediment loaded to a downstream water body so called watershed loading models which include water erosion prediction project wepp flanagan and nearing 1995 dun et al 2009 boll et al 2015 soil and water assessment tool swat arnold et al 1998 and hydrological simulation program fortran hspf donigian et al 1983 there are also several models capable of simulating the hydrodynamic sediment mixing and settling processes of a water body including hydrologic engineering center hec 6 thomas and prasuhn 1977 environmental fluid dynamics code efdc hamrick 1992 and estuarine coastal and ocean model sediment transport elcomsed blumberg and mellor 1987 often called the receiving water body model although reservoir sedimentation is a sequential process starting from the head drainage watersheds of a reservoir to the outlet there are only a few modeling tools that can provide a holistic view of the sediment transport processes from loading to mixing and settling liu et al 2008 in this study we coupled a watershed loading model swat to a receiving water body model efdc to provide a simulation tool for integrated watershed and reservoir management planning in this article we describe how the coupled model works and demonstrate its applicability in a case study as the coupled modeling requires many parameters and often sedimentation observations are very limited for parameter calibration we demonstrated how to quantify the uncertainty in the integrated sediment modeling using the proposed tool under the generalized likelihood uncertainty estimation glue approach 2 model description 2 1 modeling tools swat and efdc were selected in this study as tools to simulate streamflow soil loss and sediment transport processes occurring in upstream watersheds and sediment deposition in a downstream reservoir respectively swat is one of the most popularly used watershed loading models in hydrology and water quality simulation the model can describe the effects of agricultural best management practices bmps on water sediment and agricultural chemical yields in watersheds with varying soils land use and management arnold et al 1998 srinivasan et al 1998 efdc is a receiving water body model capable of simulating hydrodynamics sediment and nutrient processes and toxic contaminant transport and the model has been applied to various water bodies including rivers lakes reservoirs wetlands estuaries and coastal regions for environmental assessment and management gao and li 2014 2 2 model coupling the swat and efdc models are coupled by linking swat outputs to the boundary conditions of the efdc modeling fig 1 for the coupling a set of matlab codes were developed to automatically convert the swat outputs to the input files of efdc including runoff discharge qser inp and sediment loads sdser inp and snser inp as the boundary conditions see sample input files provided as supporting information see appendix c when no reservoir operation record is available the modeling framework uses a reservoir water balance model module based hydrologic analysis system for agricultural watersheds masa to estimate the amount of water released through a spillway in this case the flow outputs of the swat model are first fed to the masa model to calculate reservoir outflows and then converted into the discharge input file qser inp of the efdc model the sediment load outputs of the swat model are separated by sand and silt according to the grain size distributions observed in each watershed and converted into the sediment concentration input files sdser inp for sand and snser inp for silt of the efdc model once a parameter value set for the swat model is determined sediment mixing settling and reservoir mass balance as well as runoff and sediment transport processes are automatically simulated by the coupled modeling framework the model coupling is expected to facilitate scenario analyses with different management practices and weather patterns that require multiple model runs and simulations in addition the coupling allows us to investigate the uncertainty propagation from one model to the other the uncertainty of watershed sediment loading swat modeling is expected to be transferred to the water body sediment dynamic efdc modeling in this study we demonstrated how the coupled modeling framework could help to quantify the impact of equifinality of the swat modeling on the efdc modeling outputs using a glue method setegn et al 2010 her and chaubey 2015 her and seong 2018 in the demonstration the efdc simulation processes described in fig 1 were performed iteratively with each of the behavioral swat runoff and sediment load outputs and then the uncertainty or variations of sediment deposition simulated using the efdc models were quantified in terms of deposition thickness for multiple runs of the efdc model the parfor parallel for loop of matlab was utilized and individual loop iterations were randomly executed on multiple e g twelve logical threads the six cores of intel i7 8700 k of a personal computer finally the grid based daily outputs of sediment behavior modeling were saved in ascii format rather than efdc explorer binary format so that the following post processing and uncertainty analysis could be implemented easily with common computational software such as excel matlab and r 3 model implementation 3 1 example application area for the purpose of demonstration the integrated sediment modeling framework was applied to an agricultural area the ipjang reservoir watershed located in south korea fig 2 the reservoir receives water drained from its upstream areas of 7 4 km2 and the major land use is forest 82 1 followed by other agricultural land uses 6 2 including rice paddy fields and uplands and pasture 5 7 there are two upper streams named yangdae river yang and no name river nnm which flow into the ipjang reservoir 3 2 watershed loading modeling with swat a 5 m digital elevation map dem was generated from a topographic map provided by the korea national spatial data infrastructure portal nsdi and it was used to delineate the watershed and sub watershed boundaries land use and soil maps were obtained from the korean ministry of environment me and the national institute of agricultural science nas respectively land use and soil maps were overlaid on the dem to define hydrologic response units hrus and the thresholds for land use soil and slope were set to 10 her et al 2015 the yang watershed was divided into 17 sub basins and 104 hrus and the nnm watershed was split into 13 hrus within one sub basin fig 2 daily weather observations were compiled from multiple weather stations operated by the korea ministry of land transport and maritime affairs http hrfco go kr sumun rainfalllist do and korea meteorological administration https data kma go kr rainfall data were obtained from the nearest rain gauge station the seoun automatic weather station aws and other weather data were collected from the hagaejung aws temperature and wind velocity the cheonan station relative humidity and the cheongju station solar radiation fig 2a the swat models were set up to predict daily streamflow soil erosion and sediment transport of the yang and nnm river watersheds separately then each of the models was calibrated to streamflow and sediment observations made at the watershed outlets in 2017 a spin up period of three years from 2014 to 2016 was used to improve the initial conditions including soil water content the swat model parameters were selected based on the literature and our understanding of the parameters abbaspour et al 2007 douglas mankin et al 2010 arnold et al 2012 her and chaubey 2015 see appendix a a sampling based heuristic optimization algorithm called a multi algorithm genetically adaptive multi objective amalgam was employed to locate parameter sets that provide acceptable accuracy represented by the nash sutcliffe efficiency nse nash and sutcliffe 1970 in a multi objective water discharge and sediment loads calibration practice detailed descriptions of the algorithm can be found in vrugt and robinson 2007 and zhang et al 2010 the total number of parameter samples to be populated was set to 9 600 and many parameter sets that gave equally good model performance were identified under the glue framework beven and binley 1992 her and chaubey 2015 her et al 2015 in the following uncertainty analysis a threshold nse value of 0 8 was selected to quantify the equifinality of the calibration results based on the performance evaluation criteria suggested for watershed scale modeling moriasi et al 2015 3 3 receiving water body modeling with efdc the efdc model was configured to represent three dimensional sediment movement in the ipjang reservoir for a year from january 1 to december 31 2017 on a daily basis the computational time step was set to 1 s considering the numerical stability and running time of the efdc model such efdc modeling configuration allowed a 1 year sediment simulation to take approximately 4 5 h to complete with a personal computer that has an intel i7 8700 k 3 70 ghz processor and 32 gb ram under the windows 10 operating system installed on a 256 gb solid state drive ssd the spatial extent of the efdc modeling covered the entire submerged areas of the reservoir the computational grid consisted of three horizontal layers and each horizontal layer or plane had 153 cells so that the three dimensional water and sediment dynamics could be simulated with 459 grid cells the grid size of the horizontal plane was selected with consideration of the efficiency and stability of the efdc numerical modeling the average cell width and length in the grid were 25 6 m and 47 0 m respectively the cell depths ranged from 1 0 m to 7 1 m with an average cell depth of 4 1 m the high resolution bathymetric data of the reservoir were obtained by interpolating the field measured reservoir bottom elevation the initial water surface elevation of the reservoir was set to 65 4 m based on water level observations made at the ipjang reservoir by the korea rural community corporation due to the lack of sediment monitoring data the initial sediment concentration was assumed to be zero based on an understanding of the reservoir hydrology the model parameters related to sediment simulation were determined based on the literature tetra tech 1999 see appendix b weather variables were incorporated as the surface boundary conditions into the efdc modeling including daily solar radiation wind speed air temperature atmospheric pressure relative humidity and cloud cover which were obtained from the same sources as those of the swat modeling the water and sediment inflows from the yang and nnm rivers to the reservoir were represented as the lateral inflow boundary conditions of the efdc model and spillway release and irrigation water supply were considered to be the outflow boundary conditions in the efdc modeling the inflow boundary conditions were extracted from the daily outputs of the swat modeling by converting sediment loads tons day to concentrations mg l and reformatting the converted data for efdc there was no measured outflow data for the reservoir but reservoir water level measurements made from 2014 to 2017 were available thus this study estimated the amount of water discharged out of the reservoir using water level measurements and the mass balance relationship between 1 inflow from the drainage watersheds through the yang and nnm rivers 2 outflow including spillway release and irrigation water supply and 3 changes in the water stored in the reservoir we employed an agricultural reservoir water balance model masa to estimate the reservoir outflow components from water level measurements song 2017 in the masa model the reservoir release for agricultural use was calculated considering the irrigation water requirements for rice paddies and the irrigation efficiency for conveyance and distribution to the field the water requirements were estimated based on the paddy water balance concept and the recommended parameter values from previous studies were adopted song 2017 irrigation efficiency is a function of reservoir operation and agricultural water management practices we calibrated the irrigation efficiency by setting the water level observations as a calibration target the spillway release was simulated by calculating the difference between the simulated levels and the operation levels a more detailed explanation of the reservoir outflow simulation can be found in song et al 2016 4 results the flow and sediment outputs of the swat simulation were compared with observations fig 3 the calibrated swat model reproduced daily flow and sediment loads of the study watersheds at acceptable accuracies nses of 0 86 and 0 90 for flow and sediment respectively in the case of yang nses of 0 86 and 0 99 for flow and sediment respectively in the nnm case arnold et al 2012 moriasi et al 2015 in the parameter calibration 4824 and 5035 behavioral parameter sets were identified for the swat model prepared for the yang and nnm watersheds respectively and the corresponding behavioral outputs were used to develop the uncertainty bands fig 3 because of the limited watershed flow and sediment monitoring period from july 15 to october 25 2017 a large storm event occurred on july 16 2017 must have heavily influenced the calibration and the calibrated model has not been validated in this study multiple i e 500 swat flow modeling outputs were selected from the behavioral sets and fed to the masa model to calculate the reservoir outflows including spillway runoff and irrigation water supply the masa model reproduced the overall trends of the reservoir water level fig 4 examples of sediment deposition simulated using the integrated modeling framework proposed in this study are presented in fig 5 as seen in figs 3 and 5 the application of the modeling tool shows how sediment is transported from upstream areas to a downstream water body to be deposited at the bottom how the sediment deposition can spread out over time in a reservoir and how much uncertainty exists in the spatially distributed modeling outputs the model coupling enabled us to try many different flow and sediment loading scenarios or behavioral swat model runs and thus investigate modeling uncertainty propagation in the following simulations or the efdc modeling the amount of uncertainty propagated into the receiving water modeling from the watershed loading modeling was as large as the median values of sediment thickness simulated using the behavioral sediment loading outputs emphasizing the importance of watershed loading modeling accuracy in reservoir sedimentation studies and planning when 100 days elapsed sediment loaded to the reservoir had been mainly deposited right behind the dam as time passed the sedimentation moved toward the center of the reservoir increasing the depth of the sediment in the vicinity of the inflow point or the outlets of the watersheds the spatiotemporal progress of sedimentation is in accord with our understanding of reservoir hydraulics wohl and cenderelli 2000 elçi et al 2007 the amount of the net sediment deposition was simulated based on many assumptions as mentioned previously in addition the spatiotemporal distributions of sedimentation have not been calibrated and validated due to the lack of measurement thus the uncertainty solely attributed by the efdc modeling could not be quantified and the uncertainty presented in fig 5 originated from the swat modeling only with the expected modeling limitations analysis through integrated sediment modeling can benefit from the coupling of two simulation models in watershed and reservoir management planning 5 conclusions this study proposed a coupled modeling tool for integrated sediment transport analyses and management planning in the coupling the watershed loading model swat was prepared to describe erosion and sediment transport processes in upstream watersheds and the receiving water body model efdc was linked to the swat model so that the sediment loading and spatiotemporal distributions of sedimentation in a downstream reservoir could be simulated the utility and performance of the coupled modeling tool were demonstrated in the application to an agricultural reservoir located in korea results showed how integrated sediment process modeling could be possible by coupling commonly used watershed loading and receiving water body models in addition the model coupling facilitated an uncertainty analysis that employs a sampling based calibration algorithm requiring many model runs with different parameter sets under a glue approach in this study the model coupling permitted the quantification of the uncertainty bounds of watershed flow and sediment loads and their propagation to the sedimentation modeling for a downstream reservoir the coupled models showed the potential to be a useful tool for integrated sediment modeling and management planning especially for scenario analyses and optimization of sediment bmps that require intensive simulation practices the main source codes used to couple the two models are provided in appendix c and available online https github com ems2018900 couplingswatand efdc software availability type a set of scripts for coupling swat and efdc year first released 2018 hardware required a personal computer pc tested on a pc with an intel i7 8700 k processor and 32 gb random access memory ram software required ms windows tested on windows 10 programming language matlab program size 13 4 kb source code only availability and cost publicly available at https github com ems2018900 couplingswatandefdc contact for further information yher ufl edu acknowledgements this work was supported by the the usda national institute of food and agriculture hatch project fla trc 005551 and the national research foundation of korea nrf grant funded by the korea ministry of science and ict no 2015r1a2a2a01008152 appendix a list of the swat model parameters considered in the uncertainty analysis file parameter definition range default value lower upper hru ov n manning s n value for overland flow 0 5 0 5 0 14 slope average slope steepness m m 0 5 0 5 0 41 dep imp depth to impervious layer in soil profile mm 0 6000 2094 90 epco plant uptake compensation factor 0 01 1 0 35 gw ahpha bf baseflow alpha factor days 0 1 0 84 gw delay groundwater delay days 0 30 85 75 gw revap groundwater revap coefficient 0 02 0 2 0 15 gwqmn threshold depth of water in the shallow aquifer required for return flow to occur mm 0 6000 1165 10 revapmn threshold depth of water in the shallow aquifer for revap to occur mm 1 6000 54 31 mgt cn f initial scs cn ii value 0 25 0 25 0 07 rte ch kii effective hydraulic conductivity mm hr 0 5 0 5 46 25 ch nii manning s n value for the main channel 0 5 0 5 0 11 ch sii main channel slope m m 0 5 0 5 0 17 sol sol awc available water capacity of the soil layer 0 25 0 25 0 21 sol k saturated hydraulic conductivity mm hr 0 25 0 25 0 86 sol z depth from soil surface to bottom of layer mm 0 25 0 25 32 00 sub ch ki effective hydraulic conductivity in tributary channel mm hr 0 5 0 5 0 09 ch ni manning s n value for the tributary channels 0 5 0 5 0 04 ch si average slope of tributary channel m m 0 5 0 5 0 53 bsn esco surface runoff lag time days 0 01 1 0 31 surlag surface runoff lag time days 0 0001 0 01 0 00 spcon linear parameter for calculating the maximum amount of sediment that can be reentrained during channel sediment routing 0 2 1 00 prf bsn peak rate adjustment factor for sediment routing in the main channel 1 1 5 1 00 spexp exponent parameter for calculating sediment reentrained in channel sediment routing 0 1 10 1 33 appendix b list of the efdc model parameters considered in the uncertainty analysis parameter value sand silt effective particle size class mm 0 2349 0 0144 deposition settling velocity mm s 24 784 0 142 empirical settling constant 0 0 critical shear stress for deposition n m2 0 08 erosion critical shear stress for erosion n m2 0 94 0 94 erosion rate g m2 s 0 0075 empirical erosion constant 2 2 bedload median diameter of bed sediment 0 1467 angle of repose 30 0 appendix c matlab script for coupling swat and efdc main script image 1 sub function swatrun image 2 sub function swat combine image 3 sub function swat ua image 4 sub function snser sdser image 5 sub function masa run reservoir shin image 6 sub function qser image 7 sub function runefdc fun image 8 
26233,bayesian networks bns are useful methods of probabilistically modelling environmental systems bn performance is sensitive to the number of variables included in the model framework the selection of the optimum set of variables to include in a bn variable selection is therefore a key part of the bn modelling process while variable selection is an issue dealt with in the wider bn and machine learning literature it remains largely absent from environmental bn applications to date due in large part to a lack of software designed to work with available bn packages cvnetica vs is an open source python module that extends the functionality of netica a commonly used commercial bn software package to perform variable selection cvnetica vs uses wrapper based variable selection and cross validation to search for the optimum variable set to use in a bn the software will aid in objectifying and automating the development of bns in environmental applications 1 introduction bayesian networks bns are powerful probabilistic tools for modelling causal systems originating in the fields of computer science and artificial intelligence their use in environmental applications has increased significantly over the past two decades aguilera et al 2011 primarily due to the availability of off the shelf commercial software packages such as netica norsys software corporation 2016 hugin expert hugin expert a s 2017 and bayesialab bayesia 2018 among others key advantages of bns in environmental applications include their low computational cost ability to deal with missing data and data from different sources explicit inclusion of uncertainties and their simple and intuitive graphical structure uusitalo 2007 chen and pollino 2012 beuzen et al 2018b bns have been used in a variety of environmental applications to date including natural resource management varis 1997 castelletti and soncini sessa 2007 barton et al 2012 ecological risk assessment marcot et al 2006 pollino et al 2007 modelling coastal erosion hapke and plant 2010 beuzen et al 2017 climate change impact modelling gutierrez et al 2011 and emulation of process based models fienen et al 2013 poelhekke et al 2016 bns represent systems as a network of interactions between cause input and effect response variables pearl 1988 a key challenge of using bns is determining which input variables should be used to model a particular response variable the input variables included in a bn can have a significant effect on the ability of the bn to model the response variable using too many input variables some of which may be irrelevant or redundant can increase model complexity cause model overfitting and reduce the performance of the bn however using too few variables may not provide enough information to adequately model the response variable blum and langley 1997 guyon and elisseeff 2003 in complex multi variate environmental systems a practitioner commonly has access to a range of observed or modelled input variables however the optimal set of inputs for modelling a particular response variable in a bn framework is often not known selecting the optimum set of inputs i e variable selection is therefore a key part of bn modelling while variable selection is an issue discussed in the broader bn and machine learning literature e g hall 2000 guyon et al 2008 galelli et al 2014 many of the commonly used commercial bn software packages e g netica the package used in this contribution do not have inbuilt variable selection capabilities the present contribution details the python module cvnetica vs which extends netica s functionality to allow users to perform variable selection for bn model optimisation cvnetica vs utilises wrapper based variable selection exhaustively iterating through user specified combinations of input variables to search for the subset that yields the best bn model performance as measured by cross validation cv cvnetica vs builds upon cvnetica a python module developed by fienen and plant 2015 that enables cross validation testing with netica and which has previously been used for bn modelling of environmental systems nolan et al 2015 fienen et al 2016 an example application of cvnetica vs to a coastal storm erosion dataset is presented in order to demonstrate the module and show its capabilities in identifying the optimum bn for modelling the dataset 2 bayesian networks a bn is a graphical representation of the joint probability distribution of a system pearl 1988 the structure of a bn is formally known as a directed acyclic graph and is composed of nodes representing variables in the system and arcs representing dependencies between nodes nodes take values that may be boolean discrete states or for continuous variables discrete ranges that are discretised into bins together the nodes and arcs characterise conditional dependencies amongst variables which are quantified from a representative dataset once the structure and conditional dependencies in the bn have been determined calculations predictions can be made using bayes theorem 1 p r i o j p o j r i p r i p o j where p r i o j is the predicted or posterior probability of a response r i conditioned on the observation s o j p o j r i is the likelihood function and p r i and p o j are the prior probabilities of the response and observation s respectively for a thorough introduction into bns the reader is referred to pearl 1988 and charniak 1991 the variables included in the bn determine its structure and conditional dependencies the number of conditional probabilities to specify from a dataset such that equation 1 can be applied increases considerably with the number of variables in the bn korb and nicholson 2010 as a result the inclusion of insignificant variables can increase the complexity of the network reduce the sensitivity of the response variable to important input variables cause model overfitting and reduce the overall performance of the bn blum and langley 1997 guyon and elisseeff 2003 chen and pollino 2012 the selection of the optimal and parsimonious set of input variables to include in the bn is therefore crucial 3 cvnetica vs cvnetica vs is a python module that allows users to perform variable selection on bns created with the netica software package cvnetica vs is an extension of the python based open source cvnetica package developed by fienen and plant 2015 that enables cross validation testing with netica netica is a commercial bn package offering a limited capability free download or purchasable full download cvnetica vs is open source and can be used with both the licensed and free version of netica in this section we first describe the method cvnetica vs uses to perform variable selection and then describe the program structure of cvnetica vs 3 1 methods the core functionality of cvnetica vs is based on wrapper variable selection kohavi and john 1997 wrapper based variable selection uses the model itself in this case a bn to test possible variable subsets while searching for the subset that yields the best model performance there are two alternative variable selection methods available filter methods and embedded methods filter methods rank the importance of input variables based on the intrinsic properties of the available data and a relevant metric e g correlation or mutual information these methods are computationally simple but are independent of the model and are often only univariate meaning that potentially important input variable interactions are ignored galelli et al 2014 embedded methods perform variable selection and model development concurrently typically using an objective function to balance model accuracy and model complexity embedded methods are less computationally intensive than wrapper methods but are specific to certain machine learning techniques such as classification and regression trees or methods based on regularization guyon and elisseeff 2003 galelli et al 2014 while wrapper selection is typically the most computationally expensive of the available variable selection methods it was implemented in cvnetica vs because of its ability to identify the globally optimal input variable subset for the bn model framework cvnetica vs functions as follows 1 the user provides cvnetica vs with a dataset and bn containing a response variable and any number of input variables to be tested e g fig 3 2 cvnetica vs calculates all user specified combinations of input variable subsets e g all 2 3 and 4 input variable subsets 3 a unique 2 layer bn 1 input layer and 1 response layer is made for each variable combination to be tested 4 each bn is evaluated using k fold cross validation cv 5 performance metrics are stored for each combination tested in cvnetica vs only 2 layer bns are evaluated while bns can have an arbitrary number of layers increasing the number of layers can dilute the sensitivity of the response variable to the input variables and increase uncertainty propagated through the network chen and pollino 2012 in general it is suggested that bns have less than five layers marcot et al 2006 cvnetica vs focuses on 2 layer networks to maximise the sensitivity of the response node to conditionally important input variable combinations the arcs between nodes in cvnetica vs are also fixed from input to response to represent a causal and physically meaningful structure of the model for the development of multi layered or multi response bns cvnetica vs can be used iteratively to inform the development of consecutive bn layers for example to develop a 3 layer bn cvnetica vs can first be used to find the optimum input variable subset layer 2 to model a particular response variable layer 1 each input variable in the identified optimal subset can then be used as a response variable in additional iterations of cvnetica vs and the variables found to be optimal for modelling these form layer 3 cvnetica vs uses the cv package developed by fienen and plant 2015 to evaluate the performance of each bn variable subset tested in k fold cv in the input dataset is randomly without replacement split into k folds or partitions where k is typically 10 marcot 2012 a bn is trained on all but one fold of the data and then tested on the one withheld fold for all k permutations of training and testing sets in this way a robust evaluation of bn performance can be obtained cvnetica vs outputs several cv performance metrics covering regression and classification performance regression performance is summarised by the metric skill 2 s k 1 σ e 2 σ o 2 where σ e 2 is the mean squared error between observations and the expected value of bn predictions computed from posterior probability distributions and σ o 2 is the variance of the observations fienen and plant 2015 this metric is discussed in detail in fienen and plant 2015 a skill value of unity indicates perfect agreement of the bn predictions with observational data while a value of zero indicates substantial discrepancy in addition to skill cvnetica vs reports more conventional classification performance metrics including log loss error rate experience quadratic loss brier score mutual information entropy and variance reduction sensitivity all of which are described by norsys software corporation 2010 as different bns different input variable subsets may optimise different metrics cvnetica vs outputs a csv file that summarises the performance of every bn that is tested this allows the user to identify the optimal bn input subset relevant to their problem an example application of cvnetica vs that demonstrates this concept is provided in section 4 3 2 program structure installation dependencies cvnetica vs interfaces with the netica software package using the netica c api which is freely available as a dynamic linked library dll for windows norsys software corporation 2010 cvnetica vs is compatible with both python 2 7 and python 3 however it cannot be used on macintosh and unix based platforms at the present time the main function of cvnetica vs is provided in cvnetica vs run py as run vs which takes as an input the settings described below with which the user can control the functionality of cvnetica vs this function can be run using any program that executes python script however jupyter notebooks pérez and granger 2007 is a convenient tool to call the cvnetica vs function and will be used in examples hereafter the cvnetica vs python module provides the pyneticatools class in pythonneticatools py a class which contains low level functions directly wrapping the netica c api functions using the python module ctypes for details on the use of ctypes to interface with the api the reader is referred to fienen and plant 2015 these low level functions form the basis of intermediate level functions for manipulating bns and reading network output and performance statistics these intermediate level functions are provided as the pynetica class in pythonnetica py these two classes are retained from cvnetica fienen and plant 2015 and have been slightly modified to add features and ensure stability to perform wrapper variable selection a number of classes are provided in cvnetica vs tools py which are called by the main function to organise and store bn output and performance metrics the main function to run cvnetica vs is provided as run cv in cvnetica vs run py run cv takes as a single argument a python dictionary containing settings specified by the user as shown in fig 1 the user must provide the path to a netica bn with or without directed arcs and as a netica neta file to the basenet key this bn should contain all input and response variables any existing connections between the input and response nodes are removed however connections between input nodes are retained during testing new bns are constructed connecting only the subset variables of interest in each case and their conditional probability tables cpts are re trained using the provided data contained in a netica cas file on the path specified to the key basecas counting learning see korb and nicholson 2010 pp 185 190 is the default method for learning cpts in netica the key voodoopar can be used to tune cpt learning when using counting learning expectation maximisation see korb and nicholson 2010 pp 194 can instead be used to learn cpts using the key emflag if the user has a license file for the full version of netica it can be provided to the key pwdfile alternatively providing a dummy file name e g na txt will allow use of the free version of netica to undertake variable selection the input and response variables are set using the keys input vars and response vars respectively the key combinations is a list of integers that specifies what input variables subsets cvnetica vs should test e g all combinations of 1 2 and 3 input variables all bns constructed by cvnetica vs are temporarily stored and run in the directory specified to the key working dir k fold cv can be undertaken when testing each cvnetica vs subset bn by setting the cvflag key to true and specifying the number of desired folds using the key numfolds if the cvflag is set to false each bn is trained and tested on the full dataset cvnetica vs collects the performance metrics of each variable subset and these metrics are output to a csv file at a path specified using the key output file the verbosity of cvnetica vs can be set using the verboselvl key with a smaller value providing less information additionally to maintain the cleanliness of the output space any warnings produced while running cvnetica vs are sent to the file warnings txt by setting the key warningtofile to true as shown by the jupyter notebook example in fig 2 these settings can be provided to the function run vs which will initialise a netica environment build all the possible network configurations e g input and response variable combinations specified by the user and run these configurations this function returns a scenario store class from cvnetica vs tools which contains for each bn built the path to the network and information on the input and response nodes it also returns a performance store module from cvnetica vs tools which contains a pandas mckinney 2010 dataframe object containing tabular data with true false values reflecting the presence of each input and response node and values for various performance metrics described in section 3 1 for each of the combinations the data can also be viewed in the csv file specified to output file where output metrics are provided alongside the input and response variables to determine the most skilful input variable combination s 4 example application for the purpose of demonstration in this paper cvnetica vs is used to identify the optimum bn for modelling a real dataset describing coastal storm erosion in june 2016 a large coastal storm event impacted the east coast of australia resulting in the largest amount of coastal erosion observed on this coastline for 40 years immediate pre and post storm airborne lidar surveys of the impacted coastline provided a high resolution dataset capturing the erosion caused by this storm event at 1400 spatial locations harley et al 2017 here we wish to develop a bn that models the observed erosion eight potentially predictive input variables and the response variable beach erosion fig 3 are defined at each of the 1400 locations and cvnetica vs is used to determine the input variable subset that is optimum for predicting the observed erosion we specify cvnetica vs to test all combinations of input variables all combinations of 1 2 3 4 5 6 7 and 8 input variables 255 combinations using 10 fold cv figs 1 and 2 illustrate this model setup on a standard desktop computer this run took 2 h to complete a portion of the result file produced by cvnetica vs is shown in table 1 here the aim was to optimise classification accuracy minimise error rate therefore results in table 1 have been sorted in order of ascending mean error rate it can be seen from table 1 that a network containing the variables beach volume profile orientation and wave steepness is optimal for modelling beach erosion fig 4 with these variables achieving a maximum mean prediction accuracy of 85 8 error rate of 14 2 from 10 fold cv and appearing consistently within the top 10 performing subsets it is important to note that cvnetica vs is intended for variable selection and the cv performance metrics only give an indication of true model performance when using cv for model selection random chance can influence the selection of the best performing model e g by fortunate splitting of the data during cv friedman et al 2001 to improve confidence in the results it is recommended to compare the optimal variable subset identified by cvnetica vs to results from different variable selection routines such as filter variable selection e g ranking variable importance using correlation furthermore for a true measure of predictive performance metrics it is recommended to test the identified optimal model optimal variable subset on separate test data 5 conclusion and future work cvnetica vs is an open source python module that extends the functionality of the netica software package norsys software corporation 2016 to allow wrapper based variable selection cvnetica vs allows a user to efficiently objectively and autonomously explore a range of possible bn configurations and select the optimum input variables for predicting a given response variable in a bn framework here we document the functionality of cvnetica vs and provide an example application of cvnetica vs used to determine the optimum bn for predicting coastal storm erosion at present cvnetica vs focuses on 2 layer networks and a single response variable for more complex networks of multiple layers and or response variables results from different iterations of cvnetica vs can be combined as discussed in section 3 1 however future developments of cvnetica vs will automate the exploration of multi layered bns with multiple responses cvnetica vs does not currently support optimisation of the discretization intervals for the variables being tested it should be noted that the discretization of variables in a bn can significantly affect model performance nojavan et al 2017 beuzen et al 2018a while the tuning of discretization intervals for a single network is facilitated by the base module cvnetica fienen and plant 2015 pairing this functionality with the exhaustive wrapper based variable selection of cvnetica vs would result in significant computational overhead future work will look to incorporate alternative variable subset search methods within cvnetica vs e g genetic algorithms greedy searches that would reduce computational cost and allow for tuning of discretization intervals we expect that the availability of cvnetica vs will aid in the future application of bns to environmental modelling cvnetica vs is available for download at https github com simmonsja cvnetica vs and the authors welcome proposed contributions to code development acknowledgements data used in this study was funded by the australian research council dp150101339 with assistance by the nsw office of environment and heritage coastal processes and responses node and northern beaches council the authors would like to thank dr michael a kinsela of the nsw office of environment and heritage oeh for providing the regional wave modelling used in this study and prof jason h middleton and peter j mumford from the unsw school of aviation for conducting the airborne lidar surveys and lidar data pre processing tide and waverider buoy data were provided by the nsw oeh and managed by the manly hydraulics laboratory the lead author is funded under the australian postgraduate research training program the second author is funded through the nsw environmental trust environmental research program rd 2015 0128 
26233,bayesian networks bns are useful methods of probabilistically modelling environmental systems bn performance is sensitive to the number of variables included in the model framework the selection of the optimum set of variables to include in a bn variable selection is therefore a key part of the bn modelling process while variable selection is an issue dealt with in the wider bn and machine learning literature it remains largely absent from environmental bn applications to date due in large part to a lack of software designed to work with available bn packages cvnetica vs is an open source python module that extends the functionality of netica a commonly used commercial bn software package to perform variable selection cvnetica vs uses wrapper based variable selection and cross validation to search for the optimum variable set to use in a bn the software will aid in objectifying and automating the development of bns in environmental applications 1 introduction bayesian networks bns are powerful probabilistic tools for modelling causal systems originating in the fields of computer science and artificial intelligence their use in environmental applications has increased significantly over the past two decades aguilera et al 2011 primarily due to the availability of off the shelf commercial software packages such as netica norsys software corporation 2016 hugin expert hugin expert a s 2017 and bayesialab bayesia 2018 among others key advantages of bns in environmental applications include their low computational cost ability to deal with missing data and data from different sources explicit inclusion of uncertainties and their simple and intuitive graphical structure uusitalo 2007 chen and pollino 2012 beuzen et al 2018b bns have been used in a variety of environmental applications to date including natural resource management varis 1997 castelletti and soncini sessa 2007 barton et al 2012 ecological risk assessment marcot et al 2006 pollino et al 2007 modelling coastal erosion hapke and plant 2010 beuzen et al 2017 climate change impact modelling gutierrez et al 2011 and emulation of process based models fienen et al 2013 poelhekke et al 2016 bns represent systems as a network of interactions between cause input and effect response variables pearl 1988 a key challenge of using bns is determining which input variables should be used to model a particular response variable the input variables included in a bn can have a significant effect on the ability of the bn to model the response variable using too many input variables some of which may be irrelevant or redundant can increase model complexity cause model overfitting and reduce the performance of the bn however using too few variables may not provide enough information to adequately model the response variable blum and langley 1997 guyon and elisseeff 2003 in complex multi variate environmental systems a practitioner commonly has access to a range of observed or modelled input variables however the optimal set of inputs for modelling a particular response variable in a bn framework is often not known selecting the optimum set of inputs i e variable selection is therefore a key part of bn modelling while variable selection is an issue discussed in the broader bn and machine learning literature e g hall 2000 guyon et al 2008 galelli et al 2014 many of the commonly used commercial bn software packages e g netica the package used in this contribution do not have inbuilt variable selection capabilities the present contribution details the python module cvnetica vs which extends netica s functionality to allow users to perform variable selection for bn model optimisation cvnetica vs utilises wrapper based variable selection exhaustively iterating through user specified combinations of input variables to search for the subset that yields the best bn model performance as measured by cross validation cv cvnetica vs builds upon cvnetica a python module developed by fienen and plant 2015 that enables cross validation testing with netica and which has previously been used for bn modelling of environmental systems nolan et al 2015 fienen et al 2016 an example application of cvnetica vs to a coastal storm erosion dataset is presented in order to demonstrate the module and show its capabilities in identifying the optimum bn for modelling the dataset 2 bayesian networks a bn is a graphical representation of the joint probability distribution of a system pearl 1988 the structure of a bn is formally known as a directed acyclic graph and is composed of nodes representing variables in the system and arcs representing dependencies between nodes nodes take values that may be boolean discrete states or for continuous variables discrete ranges that are discretised into bins together the nodes and arcs characterise conditional dependencies amongst variables which are quantified from a representative dataset once the structure and conditional dependencies in the bn have been determined calculations predictions can be made using bayes theorem 1 p r i o j p o j r i p r i p o j where p r i o j is the predicted or posterior probability of a response r i conditioned on the observation s o j p o j r i is the likelihood function and p r i and p o j are the prior probabilities of the response and observation s respectively for a thorough introduction into bns the reader is referred to pearl 1988 and charniak 1991 the variables included in the bn determine its structure and conditional dependencies the number of conditional probabilities to specify from a dataset such that equation 1 can be applied increases considerably with the number of variables in the bn korb and nicholson 2010 as a result the inclusion of insignificant variables can increase the complexity of the network reduce the sensitivity of the response variable to important input variables cause model overfitting and reduce the overall performance of the bn blum and langley 1997 guyon and elisseeff 2003 chen and pollino 2012 the selection of the optimal and parsimonious set of input variables to include in the bn is therefore crucial 3 cvnetica vs cvnetica vs is a python module that allows users to perform variable selection on bns created with the netica software package cvnetica vs is an extension of the python based open source cvnetica package developed by fienen and plant 2015 that enables cross validation testing with netica netica is a commercial bn package offering a limited capability free download or purchasable full download cvnetica vs is open source and can be used with both the licensed and free version of netica in this section we first describe the method cvnetica vs uses to perform variable selection and then describe the program structure of cvnetica vs 3 1 methods the core functionality of cvnetica vs is based on wrapper variable selection kohavi and john 1997 wrapper based variable selection uses the model itself in this case a bn to test possible variable subsets while searching for the subset that yields the best model performance there are two alternative variable selection methods available filter methods and embedded methods filter methods rank the importance of input variables based on the intrinsic properties of the available data and a relevant metric e g correlation or mutual information these methods are computationally simple but are independent of the model and are often only univariate meaning that potentially important input variable interactions are ignored galelli et al 2014 embedded methods perform variable selection and model development concurrently typically using an objective function to balance model accuracy and model complexity embedded methods are less computationally intensive than wrapper methods but are specific to certain machine learning techniques such as classification and regression trees or methods based on regularization guyon and elisseeff 2003 galelli et al 2014 while wrapper selection is typically the most computationally expensive of the available variable selection methods it was implemented in cvnetica vs because of its ability to identify the globally optimal input variable subset for the bn model framework cvnetica vs functions as follows 1 the user provides cvnetica vs with a dataset and bn containing a response variable and any number of input variables to be tested e g fig 3 2 cvnetica vs calculates all user specified combinations of input variable subsets e g all 2 3 and 4 input variable subsets 3 a unique 2 layer bn 1 input layer and 1 response layer is made for each variable combination to be tested 4 each bn is evaluated using k fold cross validation cv 5 performance metrics are stored for each combination tested in cvnetica vs only 2 layer bns are evaluated while bns can have an arbitrary number of layers increasing the number of layers can dilute the sensitivity of the response variable to the input variables and increase uncertainty propagated through the network chen and pollino 2012 in general it is suggested that bns have less than five layers marcot et al 2006 cvnetica vs focuses on 2 layer networks to maximise the sensitivity of the response node to conditionally important input variable combinations the arcs between nodes in cvnetica vs are also fixed from input to response to represent a causal and physically meaningful structure of the model for the development of multi layered or multi response bns cvnetica vs can be used iteratively to inform the development of consecutive bn layers for example to develop a 3 layer bn cvnetica vs can first be used to find the optimum input variable subset layer 2 to model a particular response variable layer 1 each input variable in the identified optimal subset can then be used as a response variable in additional iterations of cvnetica vs and the variables found to be optimal for modelling these form layer 3 cvnetica vs uses the cv package developed by fienen and plant 2015 to evaluate the performance of each bn variable subset tested in k fold cv in the input dataset is randomly without replacement split into k folds or partitions where k is typically 10 marcot 2012 a bn is trained on all but one fold of the data and then tested on the one withheld fold for all k permutations of training and testing sets in this way a robust evaluation of bn performance can be obtained cvnetica vs outputs several cv performance metrics covering regression and classification performance regression performance is summarised by the metric skill 2 s k 1 σ e 2 σ o 2 where σ e 2 is the mean squared error between observations and the expected value of bn predictions computed from posterior probability distributions and σ o 2 is the variance of the observations fienen and plant 2015 this metric is discussed in detail in fienen and plant 2015 a skill value of unity indicates perfect agreement of the bn predictions with observational data while a value of zero indicates substantial discrepancy in addition to skill cvnetica vs reports more conventional classification performance metrics including log loss error rate experience quadratic loss brier score mutual information entropy and variance reduction sensitivity all of which are described by norsys software corporation 2010 as different bns different input variable subsets may optimise different metrics cvnetica vs outputs a csv file that summarises the performance of every bn that is tested this allows the user to identify the optimal bn input subset relevant to their problem an example application of cvnetica vs that demonstrates this concept is provided in section 4 3 2 program structure installation dependencies cvnetica vs interfaces with the netica software package using the netica c api which is freely available as a dynamic linked library dll for windows norsys software corporation 2010 cvnetica vs is compatible with both python 2 7 and python 3 however it cannot be used on macintosh and unix based platforms at the present time the main function of cvnetica vs is provided in cvnetica vs run py as run vs which takes as an input the settings described below with which the user can control the functionality of cvnetica vs this function can be run using any program that executes python script however jupyter notebooks pérez and granger 2007 is a convenient tool to call the cvnetica vs function and will be used in examples hereafter the cvnetica vs python module provides the pyneticatools class in pythonneticatools py a class which contains low level functions directly wrapping the netica c api functions using the python module ctypes for details on the use of ctypes to interface with the api the reader is referred to fienen and plant 2015 these low level functions form the basis of intermediate level functions for manipulating bns and reading network output and performance statistics these intermediate level functions are provided as the pynetica class in pythonnetica py these two classes are retained from cvnetica fienen and plant 2015 and have been slightly modified to add features and ensure stability to perform wrapper variable selection a number of classes are provided in cvnetica vs tools py which are called by the main function to organise and store bn output and performance metrics the main function to run cvnetica vs is provided as run cv in cvnetica vs run py run cv takes as a single argument a python dictionary containing settings specified by the user as shown in fig 1 the user must provide the path to a netica bn with or without directed arcs and as a netica neta file to the basenet key this bn should contain all input and response variables any existing connections between the input and response nodes are removed however connections between input nodes are retained during testing new bns are constructed connecting only the subset variables of interest in each case and their conditional probability tables cpts are re trained using the provided data contained in a netica cas file on the path specified to the key basecas counting learning see korb and nicholson 2010 pp 185 190 is the default method for learning cpts in netica the key voodoopar can be used to tune cpt learning when using counting learning expectation maximisation see korb and nicholson 2010 pp 194 can instead be used to learn cpts using the key emflag if the user has a license file for the full version of netica it can be provided to the key pwdfile alternatively providing a dummy file name e g na txt will allow use of the free version of netica to undertake variable selection the input and response variables are set using the keys input vars and response vars respectively the key combinations is a list of integers that specifies what input variables subsets cvnetica vs should test e g all combinations of 1 2 and 3 input variables all bns constructed by cvnetica vs are temporarily stored and run in the directory specified to the key working dir k fold cv can be undertaken when testing each cvnetica vs subset bn by setting the cvflag key to true and specifying the number of desired folds using the key numfolds if the cvflag is set to false each bn is trained and tested on the full dataset cvnetica vs collects the performance metrics of each variable subset and these metrics are output to a csv file at a path specified using the key output file the verbosity of cvnetica vs can be set using the verboselvl key with a smaller value providing less information additionally to maintain the cleanliness of the output space any warnings produced while running cvnetica vs are sent to the file warnings txt by setting the key warningtofile to true as shown by the jupyter notebook example in fig 2 these settings can be provided to the function run vs which will initialise a netica environment build all the possible network configurations e g input and response variable combinations specified by the user and run these configurations this function returns a scenario store class from cvnetica vs tools which contains for each bn built the path to the network and information on the input and response nodes it also returns a performance store module from cvnetica vs tools which contains a pandas mckinney 2010 dataframe object containing tabular data with true false values reflecting the presence of each input and response node and values for various performance metrics described in section 3 1 for each of the combinations the data can also be viewed in the csv file specified to output file where output metrics are provided alongside the input and response variables to determine the most skilful input variable combination s 4 example application for the purpose of demonstration in this paper cvnetica vs is used to identify the optimum bn for modelling a real dataset describing coastal storm erosion in june 2016 a large coastal storm event impacted the east coast of australia resulting in the largest amount of coastal erosion observed on this coastline for 40 years immediate pre and post storm airborne lidar surveys of the impacted coastline provided a high resolution dataset capturing the erosion caused by this storm event at 1400 spatial locations harley et al 2017 here we wish to develop a bn that models the observed erosion eight potentially predictive input variables and the response variable beach erosion fig 3 are defined at each of the 1400 locations and cvnetica vs is used to determine the input variable subset that is optimum for predicting the observed erosion we specify cvnetica vs to test all combinations of input variables all combinations of 1 2 3 4 5 6 7 and 8 input variables 255 combinations using 10 fold cv figs 1 and 2 illustrate this model setup on a standard desktop computer this run took 2 h to complete a portion of the result file produced by cvnetica vs is shown in table 1 here the aim was to optimise classification accuracy minimise error rate therefore results in table 1 have been sorted in order of ascending mean error rate it can be seen from table 1 that a network containing the variables beach volume profile orientation and wave steepness is optimal for modelling beach erosion fig 4 with these variables achieving a maximum mean prediction accuracy of 85 8 error rate of 14 2 from 10 fold cv and appearing consistently within the top 10 performing subsets it is important to note that cvnetica vs is intended for variable selection and the cv performance metrics only give an indication of true model performance when using cv for model selection random chance can influence the selection of the best performing model e g by fortunate splitting of the data during cv friedman et al 2001 to improve confidence in the results it is recommended to compare the optimal variable subset identified by cvnetica vs to results from different variable selection routines such as filter variable selection e g ranking variable importance using correlation furthermore for a true measure of predictive performance metrics it is recommended to test the identified optimal model optimal variable subset on separate test data 5 conclusion and future work cvnetica vs is an open source python module that extends the functionality of the netica software package norsys software corporation 2016 to allow wrapper based variable selection cvnetica vs allows a user to efficiently objectively and autonomously explore a range of possible bn configurations and select the optimum input variables for predicting a given response variable in a bn framework here we document the functionality of cvnetica vs and provide an example application of cvnetica vs used to determine the optimum bn for predicting coastal storm erosion at present cvnetica vs focuses on 2 layer networks and a single response variable for more complex networks of multiple layers and or response variables results from different iterations of cvnetica vs can be combined as discussed in section 3 1 however future developments of cvnetica vs will automate the exploration of multi layered bns with multiple responses cvnetica vs does not currently support optimisation of the discretization intervals for the variables being tested it should be noted that the discretization of variables in a bn can significantly affect model performance nojavan et al 2017 beuzen et al 2018a while the tuning of discretization intervals for a single network is facilitated by the base module cvnetica fienen and plant 2015 pairing this functionality with the exhaustive wrapper based variable selection of cvnetica vs would result in significant computational overhead future work will look to incorporate alternative variable subset search methods within cvnetica vs e g genetic algorithms greedy searches that would reduce computational cost and allow for tuning of discretization intervals we expect that the availability of cvnetica vs will aid in the future application of bns to environmental modelling cvnetica vs is available for download at https github com simmonsja cvnetica vs and the authors welcome proposed contributions to code development acknowledgements data used in this study was funded by the australian research council dp150101339 with assistance by the nsw office of environment and heritage coastal processes and responses node and northern beaches council the authors would like to thank dr michael a kinsela of the nsw office of environment and heritage oeh for providing the regional wave modelling used in this study and prof jason h middleton and peter j mumford from the unsw school of aviation for conducting the airborne lidar surveys and lidar data pre processing tide and waverider buoy data were provided by the nsw oeh and managed by the manly hydraulics laboratory the lead author is funded under the australian postgraduate research training program the second author is funded through the nsw environmental trust environmental research program rd 2015 0128 
26234,the unique canadian whole ecosystem study on the impact of 17α ethinylestradiol ee2 on a freshwater food web karen et al 2014 provides evidence of the value of whole ecosystem experiments for understanding indirect effects of endocrine disrupting compounds edcs and other aquatic stressors to further explain the indirect effects of ee2 observed in the experimental lake an ecosystem model based on aquatox equations was successfully developed and calibrated discussions with the scientists who gathered the experimental data were necessary to ensure the consistency of the parameters used in the model and the realism of the biomass dynamics observed in such ecosystems the prediction results helped further explain how the other fishes were impacted by the fathead minnow collapse this study also suggests that a mix of reduced gamete production increased gamete mortality and fish mortality is a potential mechanism for the collapse of the fathead minnow population keywords ecotoxicology hormone mechanistic model micropollutant trophic interactions ppcp 1 introduction endocrine disrupting compounds edcs one category of mps of great concern might affect the health of humans and animal species by either mimicking or blocking the behavior of natural hormones endocrine disruption was first observed in 1994 in caged trout exposed to sewage effluents purdom et al 1994 and since then has attracted much interest arlos et al 2018 auriol et al 2006 gross et al 2017 ternes et al 1999 edcs include hormones pharmaceuticals and personal care products ppcps pesticides industrial chemicals combustion by products and surfactants numerous chemicals present in the environment still remain unidentified and are considered suspicious as potential edcs fuhrman et al 2015 many laboratory experiments and field measurements have been performed to characterize the biological processes involved in endocrine disruption and their consequences on aquatic and terrestrial species chang et al 2009 chen and hsieh 2017 tetreault et al 2011 bahamonde et al 2013 endocrine disruption has first been highlighted in fish and then in the whole food web i e invertebrates amphibians reptiles birds and mammals clotfelter et al 2004 de castro et al 2015 fent et al 2006 despite the growing concern towards edcs impact on wild populations and consequences on whole ecosystems remain unclear indeed experimental approaches to characterize ecological impacts are costly and time consuming and thus single species tests are often preferred the problem is that such data alone may not be suitable for specifically addressing the question of environmental effects and subsequently the hazard and risk assessment ruhí et al 2016 studied a river food web composed of macroinvertebrates and put forward the notion that both waterborne exposure and trophic interactions need to be taken into account when assessing the potential ecological risks of emerging pollutants in aquatic ecosystems with the exception of the few studies reviewed in arnold et al 2014 little research has been conducted on higher trophic levels of wildlife species under natural conditions in the receiving environment or under simulated environmental exposure to edcs probably the best known example of edcs affecting wildlife species is the canadian multi year whole ecosystem study performed at an experimental lake with exposure of well defined fish and lower trophic level populations to environmentally relevant concentrations of the synthetic hormone 17α ethinylestradiol ee2 kidd et al 2007 ee2 was chosen because it is one of the most widespread and potent edcs and its environmental concentration is known to impact the endocrine system and the reproductive functions of aquatic organisms for the first time both direct and indirect effects of ee2 on the abundance of fish populations were demonstrated with fathead minnow declining dramatically after 2 years of ee2 addition kidd et al 2014 however little evidence of direct effects of this synthetic oestrogen were observed on lower trophic level organisms which is unlikely expected at low nanogram per litre concentrations of ee2 still increases in some taxa such as the insect chaoborus crustacean zooplankton rotifers and total invertebrates occurred during the experiment those changes could be explained as indirect effects based on a reduction in predation by fishes in the lake food web but these explanations remained speculative the results of their study provide evidence of the value of whole ecosystem experiments for understanding indirect effects of endocrine disrupting compounds edcs and other aquatic stressors mechanistic models can help further understand the impact of contaminants on aquatic environments and assess their ecological risk indeed ecological models have been increasingly developed for environmental risk assessment era de laender et al 2008 galic et al 2010 pastorok et al 2008 since the goal of era is to maintain ecosystem functions and services preziosi and pastorok 2008 called attention to the need for greater incorporation of food web analysis carlman et al 2015 also highlighted that ecosystem models are fundamental for sustainable decision making however implementation to decision making has been very slow mainly due to the high uncertainty accompanying ecosystem models gal et al 2014 even if some authors offer a framework and guidance to confronting uncertainties in models refsgaard et al 2007 this study takes up the challenge to develop and calibrate an ecosystem model that will help further explain the indirect effects of ee2 observed on a freshwater food web during the canadian whole lake experiment with ee2 kidd et al 2014 indeed ecosystem models have been reported in the literature for assessing the impact and risk of different anthropogenic stressors gilboa et al 2014 grechi et al 2016 sourisseau et al 2008 taffi et al 2015 but none has been found for edcs the developed ecosystem model includes the reproductive and development endpoints affected by endocrine disruption in fish and is able to predict the indirect effects on the whole ecosystem through ecological interactions i e feeding and competition along with the lake stratification and physico chemical dynamics the aim of this paper is to describe the results of both the development and the calibration of the ecosystem model and to analyze the prediction results of the indirect effects of ee2 on a freshwater food web with the experimental data 2 materials and methods 2 1 step 1 collecting experimental data the unique canadian whole lake experiment with ee2 kidd et al 2014 has provided numerous data including biomass concentration and diet composition that are well suited for the development and calibration of an ecosystem model therefore the experimental data used for the model come from lake 260 at the experimental lakes area ela in northwestern ontario canada kidd et al 2007 this experimental lake is oligotrophic high oxygen and low nutrient concentrations and typical of boreal shield lakes six other experimental lakes in the same area were also studied as reference systems the knowledge and experience of the scientists who worked on those lakes were used to better understand the biological and physico chemical dynamics of lake 260 the study started in 1999 with baseline data collected until 2000 each year of 2001 2003 ee2 was added to the epilimnion for 20 21 weeks during lake stratification seasonal mean concentrations for the summer were 5 0 6 1 and 4 8 ng l for 2001 through 2003 respectively lower concentrations were measured under the ice during winter details on the additions water sampling and analyses to quantify ee2 are given in palace et al 2006 the study continued 7 years after ee2 addition was stopped to measure ecosystem stability and recovery after stressor removal blanchfield et al 2015 the physico chemical data were collected monthly during the open water season phytoplankton and zooplankton samples were collected biweekly fish abundance data were based on catch and release methods using trap nets spring and autumn all species and short 30 min evening gill net sets on spawning shoals for lake trout autumn biomass of the minnow species was estimated as the product of abundance and mean size from minnow trap captures standardized by lake area mark and recapture techniques were used to estimate the abundance of lake trout autumn data and white sucker spring data biomass was estimated as the product of abundance estimates and mean size standardized by lake area 2 2 step 2 selecting the modelling approach the us epa model aquatox park and clough 2010 is an integrated fate and effects model combining water quality food web interactions chemical fate and ecotoxicological processes aquatox is probably the best known tool in risk assessment that accounts for the complexity of communities and ecosystems while the studies using aquatox are at the very least based on qualitative biomonitoring none are supported by a comprehensive quantitative analysis of the biomass and diet composition of the modelled organisms lombardo et al 2015 in most existing aquatox studies only simple verification checks or partial calibrations of the model are typically possible indeed aquatox is an open source model and has become very complex with time making the parametrization step difficult a dynamic ecosystem model was previously constructed to predict the effects of metals and pesticides on lentic ecosystems in an object oriented framework using simplified aquatox equations and the software package west mikebydhi com de laender et al 2008 the simulation results were compared to experimental results obtained from micro and mesocosm studies the model was successful in predicting ecological effects of chemicals by considering direct effects but also ecological interactions feeding and competition relationships in this study the simplified aquatox model of de laender et al 2008 was used this model was already implemented in the software package west and the equations were modified when it was needed to describe the lake food web dynamics 2 3 step 3 selecting the model structure a simplified food web was built with the most relevant populations of plankton and fish naturally present in the experimental lake 260 fig 1 the different species of plankton were grouped according to their annual dynamics i e blooms group 1 chlorophyte dinoflagellates cyanophyta group 2 crysophyta cryptophyta group 3 diatoms the main fish species selected for the model are characterized by different spawning periods and habitats hypo or epilimnion offshore bottom or littoral based on back and forward discussions with the ecologists who gathered the experimental data lake trout was decided to be the only species spending most of the time in the hypolimnion but still feeding on the fishes that mainly live in the epilimnion and move around the lake for food in aquatox the model consists of a set of objects and each object describes the growth of a model population in terms of its biomass concentration using differential equations including biological processes such as assimilation photosynthesis respiration consumption or mortality and additional processes such as migration diffusion or loading by connecting different objects and defining feeding relationships between them a customized food web can be designed fig 2 the number of populations that can be modelled is unlimited and available objects are phytoplankton zooplankton planktivorous fish and piscivorous fish intrinsically identical objects e g various groups of phytoplankton can be differentiated by parameter tuning e g spring vs summer populations dynamic driver variables also called forcing functions indicated on fig 1 at the top of the big box are used as external factors daily values of the dynamic driver variables are contained in an input file which is read by the ecosystem model during simulation the important toxicological endpoint for modelling endocrine disruption is the reproductive ability of fish for this purpose two fish classes are used in the model juveniles and adults fig 3 intersex fish are not considered explicitly because some of them are assumed to still be able to reproduce females and males are not differentiated explicitly either but the sex ratio is and it is important because it determines the gamete quantity that is produced and thus the juveniles being recruited the newly adopted equations are presented and discussed in the results section among the gametes released by the adults some are lost to the detritus and some turn into new fish which is called juveniles recruitment the juveniles become adults when they can reproduce which is called juveniles promotion the model consists of i state variables such as biomass dynamics and also dissolved organic matter dom particulate organic matter pom nitrogen n and phosphorus p ii forcing functions which are measured time series used as model input such as photoactive radiation par photoperiod temperature t conce ikntrations of oxygen o2 and 17α ethinylestradiol ee2 and iii exchanges between the two stratified layers fig 4 supported by the experimental data collected after lake stratification in the spring until shortly before turnover in the autumn two stratified layers are modelled epilimnion and hypolimnion with different biological and physico chemical compositions the epilimnion is defined as the surface layer of water with uniform temperature ignoring any shallow temporary stratification phenomena while the hypolimnion is defined as the bottom layer of water also with uniform temperature 2 4 step 4 calibrating the model the forcing functions used to calibrate the ecosystem model table 1 come from data collected from the lake in 2000 from may 2nd to october 29th before the addition of ee2 on may 2nd 2000 the lake was already stratified and the overturn of the lake started on october 8th so most of the simulation results correspond to the dynamics of a stratified lake the biomass concentrations measured in the experimental lake in may 2nd 2000 were used as initial biomass concentrations and were not changed during calibration the model calibration was conducted following a stepwise procedure fig 5 similar to the procedure adopted by corominas et al 2011 and mannina et al 2011 by which the fit to multiple relevant variables is incrementally pursued in essence nutrients are fitted first and subsequently the trophic levels are added to the multivariate fitting objective each time a trophic level is added the parameters related to that level are updated to achieve a best fit to the data corresponding to that level however since this addition of trophic level may affect the trophic levels already present a recalibration is done of the other lower trophic levels according to the same sequence as they were added multiple iterations over these trophic levels may be necessary before the next trophic level can be added and the model calibration goes forth the fine tuning of the parameters was performed following a trial and error approach based on the comparison of the simulation results with the experimental results the quality of the model fits to the data was evaluated by performing the mean square relative error msre statistical test hauduc et al 2015 the aquatox default parameter values were selected as starting point for the model calibration park and clough 2010 moreover the parameter values that were used can be found back in the source code that is coming with the paper the first simulations were performed with the nutrients nh4 no3 po4 and organic matter om present in the epilimnion a sensitivity analysis was performed by visual inspection of the simulation results after perturbation of the parameter under study by comparing the effect of different parameters a ranking of the most influential parameters was obtained for this purpose each parameter was separately multiplied or divided by two then changed by 20 to study the consequences of smaller changes and the consequences on the concentrations of nutrients and organic matter observed from there the most influencing parameters were selected and tuned until the simulation results were sufficiently close to the experimental data the sensitivity analysis was performed by visual inspection of the simulation results after perturbation of the parameter under study by comparing the effect of different parameters a ranking of the most influential parameters was obtained in the second round of simulations the nutrients in the hypolimnion were added and connected to the epilimnion through the stratification and mixing processes naturally occurring in lakes after performing the sensitivity analysis and tuning the most influencing parameters for nutrients phytoplankton were added one group at a time 1 chryso and cryptophyta 2 diatoms 3 chloro and dinophyta cyanobacteria the model calibration continued following the same procedure with zooplankton fish in the epilimnion 1 fathead minnow 2 pearl dace 3 white sucker and finally fish in the hypolimnion 4 lake trout after each group is added a sensitivity analysis is performed and the most influencing parameters tuned before adding the next group the simulation results were compared to the experimental data and back and forward discussions with the scientists who gathered the data were necessary to ensure the consistency of the physico chemical and biological parameters used in the model and the realism of the biomass dynamics in such ecosystems 2 5 step 5 predicting indirect effect of ee2 during the unique canadian whole lake experiment with ee2 kidd et al 2014 the strongest direct effect of ee2 was observed on fathead minnow with a collapse of the fish species in the second year of adding ee2 due to endocrine disruption the objective of the developed ecosystem model is to further explain the indirect effects of ee2 observed on the food web of the experimental lake therefore specific parameters were calibrated in fathead minnow in order to describe the direct effect of ee2 on fathead minnow and then the indirect effect on other fish species was analyzed after back and forth discussions with the scientists who gathered the experimental data the parameters involved in reducing gamete production increasing gamete mortality and or increasing fish mortality were identified as potential hypotheses the experimental data collected during the second year of adding ee2 to the lake kidd et al 2007 were used to fine tune the selected parameters 3 results and discussion the aim of this paper is to describe the results of both the development and the calibration of the ecosystem model and to compare the prediction results of the indirect effects of ee2 on a freshwater food web with the experimental data the ecosystem model was built according to the different compartments found in the experimental lake in addition to the biota the physical and chemical characteristics were also considered the developed ecosystem model is inspired by the simplified aquatox equations of de laender et al 2008 see their paper for a full list of equations parameters state variables and forcing functions that form the basis of the model presented here in the results and discussion section only the simplified aquatox equations of de laender et al 2008 that were changed and the equations that were added are presented regarding the calibration results present the best fit obtained between the experimental data and the simulation outputs including all the boxes of fig 5 the sections model parameters and calibration results correspond to the last box of fig 5 called calibrated values the final model has a considerable number of parameters distributed over 16 objects fishes organic matter phyto and zooplankton species for each object a number of parameters did not have to be calibrated because they represent constant lake characteristics volume ph depth etc and detritus ratios ratio of phosphorus to organic matter ratio of nitrogen to ammonia etc for the other parameters a sensitivity analysis was performed to find the limited subset of parameters that still allows getting a good model fit to the data 3 1 physico chemical characteristics 3 1 1 stratification the percentage of sedimented organic particles sed from the epilimnion to the hypolimnion is calculated using the following equation where sed is assumed to be constant over time 1 s e d v o f f s h o r e v e p i where v offshore m 3 is the volume from where the particles sediment to the hypolimnion and v epi m 3 is the total volume of the epilimnion fig 6 in aquatox the stratification is considered to occur when the mean water temperature exceeds 4 c and the difference in temperature between the epilimnion and hypolimnion exceeds 3 c when those two conditions are not met the two layers mix which normally occurs during the spring and the fall the equations used in the ecosystem model for the mixing of organic matter and nutrients were presented by vallet et al 2014 if tmean t 4 c and tepi t thypo t 3 c 2 t h e n m i x t 0 where mix g m3 d is the lake overturn rate t mean c the mean water temperature t epi c the water temperature in the epilimnion and t hypo c the water temperature in the hypolimnion 3 else m i x t q m i x v t o t x i n t x t where mix g m 3 d is the lake overturn rate q mix m 3 d is the mixing flow v tot m 3 the total volume of the lake x g m 3 the concentration of the variable being modelled nutrient nh4 no3 or po4 or organic matter dom pom or som x in g m 3 the concentration of the variable entering the modelled layer epilimnion or hypolimnion 3 1 2 oxygen and temperature oxygen concentrations in the sediments are different from the oxygen concentrations in the water fig 6 thus a correction factor was applied both in the epilimnion and the hypolimnion to differentiate the reactions happening in the sediments from the ones in water 4 d o c o r r t o 2 t o 2 t h a l f s a t o where do corr unitless is the oxygen correction factor o 2 g m 3 the dissolved oxygen concentration and halfsato g m 3 the half saturation constant for oxygen temperature is another important controlling factor in the model involved in the stratification of the lake but also in the biotic and chemical fate processes the same correction factor as the one used by de laender et al 2008 was applied to the corresponding equations in order to calculate the temperature correction for microbial processes 3 1 3 transport a genetic screening of the fathead population studied during the recovery phase of the ee2 lake experiment demonstrated that there was no fish migration from surrounding lakes blanchfield et al 2015 besides the lake inflow and outflow were shown to be negligible compared to the total volume of the lake and because it is an experimental area there was no fishing either therefore the lake could be modelled as a closed system with no exchanges with the outside consequently the transport equations used in aquatox for the biota nutrients and organic matter such as loading washout migration fishing etc could be removed from the model 3 2 organic matter and nutrients 3 2 1 model equations the organic matter mass balance is composed of the losses coming from the different biota and is divided into 3 groups 1 dissolved organic matter dom 2 particulate organic matter pom 3 and sedimented organic matter som compared to the simplified model developed by de laender et al 2008 a fraction of dead biota was added to the dom according to the equations from aquatox for readability the term t is not included on the right hand side of differential equations 5 d d o m d t e x c r e t b i o t a m o r t a l i t y b i o t a f m o r t d o m d e c o m p d o m m i x where excret biota g m 3 d is the total excretion rate of the biota mortality biota g m 3 d the total dead biota rate f mortdom unitless the fraction of dead biota transformed into dom decomp dom g m 3 d the dom loss rate due to decomposition and mix g m 3 d the lake overturn rate see equations 3 and 4 for the pom the gametes lost by fish not turning into juveniles see fig 3 were added to the simplified aquatox equations of de laender et al 2008 6 d p o m d t m o r t a l i t y b i o t a f m o r t p o m g a m e t e l o s s d e c o m p p o m s e d p o m i n g e s t p o m where mortality biota g m 3 d is the total dead biota rate f mortalitypom unitless the fraction of dead biota transformed into pom gameteloss g m 3 d the loss rate for gametes decomp pom g m 3 d the pom loss rate due to decomposition sed pom g m 3 d the sedimentation rate of pom and ingest pom g m 3 d the consumption rate of pom by zooplankton for the som the main difference with the simplified aquatox equations of de laender et al 2008 was to consider the lake stratification according to the equation presented earlier equation 1 thus when modelling the hypolimnion part of the som coming from the epilimnion is considered the decomposition of the three organic matter groups is based on a maximum decay rate decay max g g d corrected for suboptimal temperature dissolved oxygen and ph the difference with the simplified aquatox equation used by de laender et al 2008 is that in the developed ecosystem model the variation of oxygen concentrations between the water column and the water sediment interface is considered see equation 4 the dom and pom are considered being decomposed in the water column only while som also decomposes in the sediments a fraction of the decomposed organic matter is converted into nutrients nh4 no3 po4 the main additions made to the simplified aquatox equations used by de laender et al 2008 for the nutrient pool are the addition of no3 assimilation by phytoplankton and the nitrification denitrification processes happening both in the water column and at the water sediment interface 7 d n h 4 d t d e c o m p o m e x c r e t b i o t a r e s p b i o t a n 2 o m a s s i m n h 4 n i t r i f w a t e r n i t r i f s e d m i x where nh 4 g m 3 is the ammonia concentration decomp om g m 3 d the organic matter decomposition rate excret biota g m 3 d the total excretion rate of the biota resp biota g m 3 d the total respiration rate of the biota n2om unitless the n om ratio assim nh4 g m 3 d the nh4 assimilation rate by phytoplankton nitrif water g m 3 d the nh4 nitrification rate in the water column nitrif sed g m 3 d the nh4 nitrification rate at the water sediment interface and mix g m 3 d the lake overturn rate 8 d n o 3 d t n i t r i f w a t e r n i t r i f s e d d e n i t w a t e r d e n i t s e d a s s i m n o 3 m i x where no 3 g m 3 is the nitrate concentration denit water g m 3 d the no3 denitrification rate in the water column denit sed g m 3 d the no3 denitrification rate at the water sediment interface and assim no3 g m 3 d the no3 assimilation rate by phytoplankton 9 d p o 4 d t d e c o m p o m e x c r e t b i o t a r e s p b i o t a p 2 o m a s s i m p o 4 m i x where po 4 g m 3 is the phosphate concentration p2om unitless the p om ratio and assim po4 g m 3 d the po4 assimilation by phytoplankton 3 2 2 calibration results during the calibration of the nutrient and organic matter parameters the most influencing parameters selected with the sensitivity analysis are related to the nitrification denitrification processes and decomposition of organic matter table 2 the values presented were obtained after model fit fig 7 and correspond to the final iteration of the calibration procedure see fig 5 last box called calibrated values maximum rates for the denitrification process are similar between the epi and the hypolimnion while nitrification happens at higher maximum rates in the epilimnion which makes sense since the nitrification process requires aerobic conditions when looking at the graph presenting final results after fine tuning of the calibrated parameters fig 7a it appears that nh4 mainly occurs in the epilimnion where the nitrification is at its highest rate the graph clearly shows that both nh4 and no3 accumulated during the winter when nitrification and denitrification rates are very low due to low temperatures and then as soon as spring starts both nh4 and no3 are eliminated through nitrification and denitrification processes early in the fall when temperatures drop again nh4 starts accumulating again the simulation results obtained with the calibrated parameters succeeded at catching those dynamics regarding som and dom decomposition the maximum rates appeared to be higher in the epilimnion while higher in the hypolimnion for the pom when looking at the pom graph fig 7b concentrations are higher in the hypolimnion where the pom accumulates once again the simulation results obtained with the calibrated parameters succeeded at matching the trend observed in the experimental data back and forward discussions on the nutrient and organic matter dynamics with the freshwater system ecologists who gathered the experimental data confirmed that the calibrated parameters succeeded at simulating their dynamics during the open water season 3 3 phytoplankton 3 3 1 model equations the phytoplankton mass balance is described by the following equation 10 d p h y t o d t p h o t o r e s p e x c r m o r t s i n k p r e d where phyto g m 3 is the phytoplankton biomass photo g m 3 d the rate of photosynthesis resp g m 3 d the respiratory loss excr g m 3 d the rate of excretion mort g m 3 d the rate of non predatory mortality sink g m 3 d the loss due to sinking to the bottom and pred g m 3 d the consumption of phytoplankton by zooplankton photosynthesis is modelled as a maximum rate which is reduced by nutrient temperature and light limitation factors de laender et al 2008 for each species optimal photosynthesis is reached at optimal temperature and depth and is directly connected to the photosynthetically active solar radiation par following aquatox equations when there is no ice cover on the lake tmean 3 c the light is entered in the ecosystem model as an input using the measured values of photoactive radiation par table 1 if there is an ice cover a factor of 0 3 is applied to the values of par there are two main limiting factors to the light used for photosynthesis the first one is the photoperiod representing the fraction of the day with daylight and entered in the ecosystem model as an input table 1 the second one is the extinction of light extinct when entering the water due to self shading of the phytoplankton organic particles and dissolved organic matter compared to the simplified model developed by de laender et al 2008 the light limitation factor was changed for the equation used in aquatox 11 l t l i m i t t 0 85 e p h o t o p e r i o d t l t a t d e p t h t l t a t t o p t e x t i n c t t d e p t h b o t t o m d e p t h t o p where ltlimit unitless is the light limitation 0 85 unitless the correction factor for daily formulation e 2 718 unitless the base of natural logarithms photoperiod unitless the fraction of the day with daylight ltatdepth unitless the limitation of algal growth due to light ltattop unitless the limitation due to insufficient light extinct 1 m the total light extinction from self shading of the phytoplankton organic particles and dissolved organic matter depth bottom m the maximum depth and depth top m the depth of the top layer nitrogen and phosphorus compounds are assimilated during the photosynthesis process which is modelled in the nutrient equations see equations 7 9 regarding the assimilation of nitrogen because only 23 percent of the weight of nitrate is nitrogen and 78 percent of ammonia is nitrogen this results in an apparent preference for ammonia thus compared to the simplified model developed by de laender et al 2008 a preference factor nh4pref inspired by aquatox was added 12 n h 4 p r e f t 14 18 n h 4 t 14 62 n o 3 t k n 14 18 n o 3 t k n 14 62 n o 3 t 14 18 n h 4 t k n 14 18 n h 4 t 14 62 n o 3 t k n 14 62 n o 3 t where nh4pref unitless is the ammonia preference factor 14 18 unitless the ratio of nitrogen to ammonia 14 62 unitless the ratio of nitrogen to nitrate kn gn m 3 the half saturation constant for nitrogen uptake nh4 g m 3 the concentration of ammonia and no3 g m 3 the concentration of nitrate regarding the other biotic processes i e respiration excretion mortality sinking and predation the equations used in the developed ecosystem model are described in de laender et al 2008 except for the respiratory loss which was replaced by the aquatox equation respiratory loss is exponential with temperature and since the developed ecosystem model is to be applied to stratified lakes implying changes of temperature the aquatox equation was preferred over the simplified equation of de laender et al 2008 that was used for non stratified micro and mesocosms 13 r e s p t r e s p 20 1 045 t e m p t 20 p h y t o t where resp g m 3 d is the respiratory loss resp20 g g d the respiration rate at 20 c 1 045 c the exponential temperature coefficient temp c the ambient water temperature and phyto g m 3 the phytoplankton biomass 3 3 2 calibration results during the calibration of the phytoplankton parameters the most influencing parameters selected with the sensitivity analysis are mainly related to photosynthesis table 3 group 1 and group 3 seem to assimilate better p than n while group 2 assimilates p and n similarly group 3 has the lowest maximum photosynthesis rate parameters for mortality sedimentation and temperature were also selected as influencing parameters experimental data for phytoplankton biomass suffered from high variability because of the many species in each group but also because of the experimental uncertainty resulting in high variability of the data collected the values presented in table 3 were obtained after model fit within the error bars of the experimental data fig 8 and correspond to the final iteration of the calibration procedure for the three groups of phytoplankton see fig 5 last box called calibrated values back and forward discussions on the phytoplankton biomass dynamics with the scientists who gathered the experimental data confirmed that the calibrated parameters managed to simulate relatively well the main trends during the open water season considering the high variability of the experimental data and except for the initial decline the simulation results captured that group 1 organisms were more abundant than group 2 and that group 3 had the lowest concentrations besides group 1 is characterized by a bloom between april and june which explains the high initial biomass concentration followed by a drop fig 8a blooms for group 2 and group 3 start later which can be seen with lower drops on the graphs fig 8b and 8c 3 4 zooplankton 3 4 1 model equations the zooplankton mass balance is described by the following equation de laender et al 2008 14 d z o o d t c o n s d e f r e s p e x c m o r t p r e d where zoo g m 3 is the zooplankton biomass cons g m 3 d the consumption of phytoplankton and pom def g m 3 d the defecation of unassimilated food resp g m 3 d the respiratory loss excr g m 3 d the excretion of dissolved organic matter mort g m 3 d the non predatory mortality and pred g m 3 d the consumption of zooplankton by planktivorous fish 3 4 2 calibration results during the calibration of the zooplankton parameters the most influencing parameters selected with the sensitivity analysis are mainly related to the consumption of phytoplankton and pom table 4 parameters for excretion respiration mortality and temperature were also selected as influencing parameters the values presented were obtained after model fit for the three groups of zooplankton fig 9 and correspond to the final iteration of the calibration procedure see fig 5 last box called calibrated values the final values of the parameters resulted in rotifers only eating pom while rotifers normally eat some phytoplankton their main diet consists of dead or decomposing organic material due to their microscopic size the scientists who gathered the experimental data confirmed that it was an acceptable simplification of the model when looking at the graph presenting final results after fine tuning of the calibrated parameters fig 9 it appears that the simulation results succeeded at predicting the zooplankton dynamics during the open water season except for a faster growth of cladocera and copepods in may back and forward discussions on the zooplankton biomass dynamics with the scientists who gathered the experimental data confirmed that when looking at experimental data for other years or in the reference lakes cladocera and copepods have earlier growth peaks starting early may for copepods and mid may for cladocera 3 5 fish 3 5 1 model equations for the fish mass balance de laender et al 2008 used the same equation as for the zooplankton mass balance presented in equation 14 in order to model endocrine disruption two fish classes were added juveniles adults see fig 3 and three reproductive terms added to the mass balance accordingly to aquatox juveniles 15 d f i s h j u v d t c o n s d e f r e s p e x c m o r t p r e d p r o m o r e c r u i t adults 16 d f i s h a d u l t d t c o n s d e f r e s p e x c m o r t p r e d g a m e t e l o s s p r o m o where fish juv g m 2 is the juvenile biomass fish adult g m 2 is the adult biomass cons g m 2 d the consumption of pom def g m 2 d the defecation of unassimilated food resp g m 2 d the respiratory loss excr g m 2 d the excretion of dissolved organic matter mort g m 2 d the non predatory mortality pred g m 2 d the consumption of fish by piscivorous fish gameteloss g m 2 d the loss of gametes during spawning promo g m 2 d the promotion from juveniles to adults and recruit g m 2 d the recruitment from viable gametes to juveniles eggs and sperm modelled as gametes can be a significant fraction of adult biomass because only a small fraction of these gametes results in viable young when shed at the time of spawning entered as parameters in the model spawningstart and spawningend the remaining fraction is lost to detritus gameteloss if spawningstart t spawningend then 17 g a m e t e l o s s t g m o r t i n c r m o r t f r a c a d u l t s t p c t g a m e t e b i o t else g a m e t e l o s s t 0 where gameteloss g m 2 d is the loss of gametes during spawning spawningstart d the date when spawning starts spawningend d the date when spawning ends gmort 1 d the gamete mortality incrmort 1 d the increased gamete and embryo mortality due to toxicant fracadults unitless the fraction of biomass that is adult pctgamete unitless the fraction of adult biomass that is in gametes and bio g m 2 the biomass as the biomass of a fish population reaches its carrying capacity which is the maximum sustainable biomass reproduction is usually reduced due to stress in the model this results in assuming the proportion of adults and the fraction of biomass in gametes at a maximum 18 f r a c a d u l t s t 1 c a p a c i t y t k c a p if biofish t kcap then 19 c a p a c i t y t k c a p b i o f i s h t else c a p a c i t y t 0 where fracadults unitless is the fraction of biomass that is adult capacity g m 2 the biomass capacity kcap g m 2 the carrying capacity and bio g m 2 the biomass during spawning gametes are lost from the adults and the juveniles gain the viable gametes through recruitment which is in other words the biomass gained from successful spawning if spawningstart t spawningend then 20 r e c r u i t t 1 g m o r t i n c r m o r t f r a c a d u l t s t p c t g a m e t e b i o t else r e c r u i t t 0 where recruit g m 2 d is the recruitment from viable gametes to juveniles spawningstart d the date when spawning starts spawningend d the date when spawning ends gmort 1 d the gamete mortality incrmort 1 d the increased gamete and embryo mortality due to toxicant fracadults unitless the fraction of biomass that is adult pctgamete unitless the fraction of adult biomass that is in gametes and bio g m 2 the biomass the juveniles promoted to adults is determined in the model by the rate of growth considered as the sum of consumption and the loss terms other than mortality 21 p r o m o t k p r o c o n s t d e f t r e s p t e x c t where promo g m 2 d is the promotion from juveniles to adults kpro unitless the fraction of growth that goes to promotion cons g m 2 d the consumption of phytoplankton and pom def g m 2 d the defecation of unassimilated food resp g m 2 d the respiratory loss and excr g m 2 d the excretion of dissolved organic matter 3 5 2 calibration results for fathead minnow and pearl dace during the calibration of the fathead minnow and pearl dace parameters the most influencing parameters selected with the sensitivity analysis are mainly related to reproduction and food consumption tables 5 and 6 parameters for excretion respiration mortality and temperature were also selected as influencing parameters experimental data were collected in the spring may 16 and the adult fish concentrations were 0 23 g m2 and 0 28 g m2 for fathead minnow and pearl dace respectively the simulation results were quantitatively discussed with ecological specialists in order to validate the biomass dynamics obtained during the open water season for both juveniles and adults the values presented in tables 5 and 6 correspond to the final iteration of the calibration procedure see fig 5 last box called calibrated values and were obtained after the model was fitted with the experimental data and the experts validated the biomass dynamics fig 10 for fathead minnow fig 10a the experimental data 0 23 g m2 on may 16 was collected before the spawning period which lasted from the end of june to the end of july the beginning of spawning can be seen on the graph with a drop of the adult biomass occurring after june 21st correlated to an increase of the juvenile biomass before spawning the adult biomass increases because they are producing gametes and then when spawning starts the gametes are lost which explains the decrease in the adult biomass conversely the juvenile biomass increases when spawning starts because viable gametes turn into young fish after spawning from july 21st to the fall biomass of both adults and juveniles increases first due to high temperature and food availability during summer and then decreases when fall arrives due to a decrease in temperature and food availability for pearl dace fig 10b the spawning period starts earlier mid april and ends around mid may which means the experimental data 0 28 g m2 on may 16 was collected just at the end of the spawning period may 16 the same pattern than for fathead minnow occurs but earlier the decrease of adult biomass and increase of juvenile biomass started before the beginning of the graph and ended around june 17th when spawning ended 3 5 3 calibration results for white sucker and lake trout during the calibration of the white sucker and lake trout parameters the most influencing parameters selected with the sensitivity analysis are mainly related to reproduction and food consumption tables 7 and 8 parameters for excretion respiration mortality and temperature were also selected as influencing parameters experimental data were collected in the spring may 16 for adult white sucker and the biomass concentration was measured at 7 42 g m2 for the adult lake trout the data were collected in the fall october 3rd and the biomass concentration was measured at 4 04 g m2 the simulation results were quantitatively discussed with ecological specialists in order to validate the biomass dynamics obtained during the open water season for both juveniles and adults the values presented in tables 7 and 8 correspond to the final iteration of the calibration procedure see fig 5 last box called calibrated values and were obtained after the model had been fitted with the experimental data and the experts validated the biomass dynamics fig 11 adult white sucker have the same spawning period as adult pearl dace which is between mid april and mid may thus similar dynamics can be observed on both graphs figs 10b and 11c however due to higher mean weights tables 7 and 8 the biomass changes observed in white sucker are of lower magnitude compared to pearl dace both for adults and juveniles the piscivorous fish lake trout fig 11d have a later and longer spawning period mid august to mid october compared to the other planktivorous fishes fathead minnow pearl dace and white sucker nevertheless a similar pattern occurs which is an increase in both adult and juveniles biomass in the summer and then a decrease in adult biomass when spawning starts due to gamete loss 3 5 4 endocrine disruption after having successfully calibrated the ecosystem model with a unique set of experimental data collected in 2000 from may 2nd to october 29th before the addition of ee2 the experimental data collected during the second year of adding ee2 to the lake kidd et al 2007 were used to help further explain the indirect effects on the lake food web the experimental results showed that the strongest direct effect of ee2 was observed on fathead minnow with a collapse of the fish species in the second year of adding ee2 due to endocrine disruption therefore discussions with experts in ecotoxicology and edcs helped identify specific parameters to be modified in fathead minnow in order to test a potential hypothesis for the biological processes involved in endocrine disruption in fathead minnow table 9 when looking at the graph presenting final results after fine tuning of the selected parameters fig 12e and 12f it appears that a mix of reduced gamete production pctgamete increased gamete mortality incr mort and increase of both adults and juveniles mortality k mort adult and k mort juv is a potential hypothesis for explaining the collapse of both adult and juvenile fathead minnow due to ee2 addition kidd et al 2007 the simulation results also help further explain how the other fishes were impacted by the collapse of fathead minnow indeed endocrine disruption in fathead minnow did not only affect its own population but also other fish populations kidd et al 2014 after three summers of ee2 addition the experimental results showed a 58 reduction of the pearl dace population however a reduction of the pearl dace population was also observed in the reference lakes and thus no conclusion on the link with ee2 could be made the model prediction results provided a new insight regarding a potential link with ee2 with 16 reduction of the adult pearl dace population and no significant changes for the juveniles population due to ee2 fig 12g regarding the white sucker population the experimental results did not indicate significant changes which corresponded with the simulation results that predicted no changes for the adult population however a 62 reduction was predicted for the juveniles population but this may have gone unnoticed experimentally since they represent the population class that is most difficult to sample due to the smaller sizes fig 12g regarding the lake trout population the experimental results and the model predictions both showed a reduction of around 25 juveniles and adults fig 12h back and forward discussions with the scientists who gathered the data and experts in ecology validated the simulation results since lake trout could no longer feed on fathead minnow they turned to pearl dace and white sucker the consequence was a decrease of both prey biomass fig 12g and a change in lake trout biomass fig 12h therefore the calibrated model was successful in predicting the indirect effects of ee2 on the lake food web and thus further explain the experimental data 4 conclusion an ecosystem model that can help further explain indirect effects of endocrine disruption in fish in a lake food web was successfully developed and calibrated with a unique set of experimental data coming from the whole lake study of kidd et al 2007 2014 back and forward discussions with the scientists who gathered the data and experts in ecology were necessary to ensure the consistency of the physico chemical and biological parameters used in the model and the realism of the biomass dynamics in such ecosystems this study suggests that a mix of reduced gamete production increased gamete mortality and fish mortality produced a similar pattern in fathead minnow as observed in the second year of exposure to ee2 the simulation results also helped explain how the other fishes were impacted by the collapse of fathead minnow to further investigate the indirect effect of ee2 an edc of great concern the ecosystem model will be used to better understand the ecological interactions and how endocrine disruption in fishes such as fathead minnow can impact a whole lake food web indeed more research is needed to develop ecosystem models like the one presented in this paper that can support era of edcs and other aquatic stressors software availability name of the software ela fish model library software requirements west 3 7 6 or higher program language model specification language msl see vanhooren et al 2003 for an explanation on msl code program size approximately 25 mb availability the source code for the ela fish model library can be obtained via github at the following url https github com modeleau ela msl git acknowledgements this study was funded through the canada research chair in water quality modelling held by peter vanrolleghem and a research grant from the natural sciences and engineering research council of canada nserc of canada through the strategic grants program 430646 2012 the authors thank karen kidd michael paterson michael rennie paul blanchfield and alain dupuis for sharing the unique data they collected during their whole ecosystem study and for the numerous discussions we had around the experimental data and simulation results appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 013 
26234,the unique canadian whole ecosystem study on the impact of 17α ethinylestradiol ee2 on a freshwater food web karen et al 2014 provides evidence of the value of whole ecosystem experiments for understanding indirect effects of endocrine disrupting compounds edcs and other aquatic stressors to further explain the indirect effects of ee2 observed in the experimental lake an ecosystem model based on aquatox equations was successfully developed and calibrated discussions with the scientists who gathered the experimental data were necessary to ensure the consistency of the parameters used in the model and the realism of the biomass dynamics observed in such ecosystems the prediction results helped further explain how the other fishes were impacted by the fathead minnow collapse this study also suggests that a mix of reduced gamete production increased gamete mortality and fish mortality is a potential mechanism for the collapse of the fathead minnow population keywords ecotoxicology hormone mechanistic model micropollutant trophic interactions ppcp 1 introduction endocrine disrupting compounds edcs one category of mps of great concern might affect the health of humans and animal species by either mimicking or blocking the behavior of natural hormones endocrine disruption was first observed in 1994 in caged trout exposed to sewage effluents purdom et al 1994 and since then has attracted much interest arlos et al 2018 auriol et al 2006 gross et al 2017 ternes et al 1999 edcs include hormones pharmaceuticals and personal care products ppcps pesticides industrial chemicals combustion by products and surfactants numerous chemicals present in the environment still remain unidentified and are considered suspicious as potential edcs fuhrman et al 2015 many laboratory experiments and field measurements have been performed to characterize the biological processes involved in endocrine disruption and their consequences on aquatic and terrestrial species chang et al 2009 chen and hsieh 2017 tetreault et al 2011 bahamonde et al 2013 endocrine disruption has first been highlighted in fish and then in the whole food web i e invertebrates amphibians reptiles birds and mammals clotfelter et al 2004 de castro et al 2015 fent et al 2006 despite the growing concern towards edcs impact on wild populations and consequences on whole ecosystems remain unclear indeed experimental approaches to characterize ecological impacts are costly and time consuming and thus single species tests are often preferred the problem is that such data alone may not be suitable for specifically addressing the question of environmental effects and subsequently the hazard and risk assessment ruhí et al 2016 studied a river food web composed of macroinvertebrates and put forward the notion that both waterborne exposure and trophic interactions need to be taken into account when assessing the potential ecological risks of emerging pollutants in aquatic ecosystems with the exception of the few studies reviewed in arnold et al 2014 little research has been conducted on higher trophic levels of wildlife species under natural conditions in the receiving environment or under simulated environmental exposure to edcs probably the best known example of edcs affecting wildlife species is the canadian multi year whole ecosystem study performed at an experimental lake with exposure of well defined fish and lower trophic level populations to environmentally relevant concentrations of the synthetic hormone 17α ethinylestradiol ee2 kidd et al 2007 ee2 was chosen because it is one of the most widespread and potent edcs and its environmental concentration is known to impact the endocrine system and the reproductive functions of aquatic organisms for the first time both direct and indirect effects of ee2 on the abundance of fish populations were demonstrated with fathead minnow declining dramatically after 2 years of ee2 addition kidd et al 2014 however little evidence of direct effects of this synthetic oestrogen were observed on lower trophic level organisms which is unlikely expected at low nanogram per litre concentrations of ee2 still increases in some taxa such as the insect chaoborus crustacean zooplankton rotifers and total invertebrates occurred during the experiment those changes could be explained as indirect effects based on a reduction in predation by fishes in the lake food web but these explanations remained speculative the results of their study provide evidence of the value of whole ecosystem experiments for understanding indirect effects of endocrine disrupting compounds edcs and other aquatic stressors mechanistic models can help further understand the impact of contaminants on aquatic environments and assess their ecological risk indeed ecological models have been increasingly developed for environmental risk assessment era de laender et al 2008 galic et al 2010 pastorok et al 2008 since the goal of era is to maintain ecosystem functions and services preziosi and pastorok 2008 called attention to the need for greater incorporation of food web analysis carlman et al 2015 also highlighted that ecosystem models are fundamental for sustainable decision making however implementation to decision making has been very slow mainly due to the high uncertainty accompanying ecosystem models gal et al 2014 even if some authors offer a framework and guidance to confronting uncertainties in models refsgaard et al 2007 this study takes up the challenge to develop and calibrate an ecosystem model that will help further explain the indirect effects of ee2 observed on a freshwater food web during the canadian whole lake experiment with ee2 kidd et al 2014 indeed ecosystem models have been reported in the literature for assessing the impact and risk of different anthropogenic stressors gilboa et al 2014 grechi et al 2016 sourisseau et al 2008 taffi et al 2015 but none has been found for edcs the developed ecosystem model includes the reproductive and development endpoints affected by endocrine disruption in fish and is able to predict the indirect effects on the whole ecosystem through ecological interactions i e feeding and competition along with the lake stratification and physico chemical dynamics the aim of this paper is to describe the results of both the development and the calibration of the ecosystem model and to analyze the prediction results of the indirect effects of ee2 on a freshwater food web with the experimental data 2 materials and methods 2 1 step 1 collecting experimental data the unique canadian whole lake experiment with ee2 kidd et al 2014 has provided numerous data including biomass concentration and diet composition that are well suited for the development and calibration of an ecosystem model therefore the experimental data used for the model come from lake 260 at the experimental lakes area ela in northwestern ontario canada kidd et al 2007 this experimental lake is oligotrophic high oxygen and low nutrient concentrations and typical of boreal shield lakes six other experimental lakes in the same area were also studied as reference systems the knowledge and experience of the scientists who worked on those lakes were used to better understand the biological and physico chemical dynamics of lake 260 the study started in 1999 with baseline data collected until 2000 each year of 2001 2003 ee2 was added to the epilimnion for 20 21 weeks during lake stratification seasonal mean concentrations for the summer were 5 0 6 1 and 4 8 ng l for 2001 through 2003 respectively lower concentrations were measured under the ice during winter details on the additions water sampling and analyses to quantify ee2 are given in palace et al 2006 the study continued 7 years after ee2 addition was stopped to measure ecosystem stability and recovery after stressor removal blanchfield et al 2015 the physico chemical data were collected monthly during the open water season phytoplankton and zooplankton samples were collected biweekly fish abundance data were based on catch and release methods using trap nets spring and autumn all species and short 30 min evening gill net sets on spawning shoals for lake trout autumn biomass of the minnow species was estimated as the product of abundance and mean size from minnow trap captures standardized by lake area mark and recapture techniques were used to estimate the abundance of lake trout autumn data and white sucker spring data biomass was estimated as the product of abundance estimates and mean size standardized by lake area 2 2 step 2 selecting the modelling approach the us epa model aquatox park and clough 2010 is an integrated fate and effects model combining water quality food web interactions chemical fate and ecotoxicological processes aquatox is probably the best known tool in risk assessment that accounts for the complexity of communities and ecosystems while the studies using aquatox are at the very least based on qualitative biomonitoring none are supported by a comprehensive quantitative analysis of the biomass and diet composition of the modelled organisms lombardo et al 2015 in most existing aquatox studies only simple verification checks or partial calibrations of the model are typically possible indeed aquatox is an open source model and has become very complex with time making the parametrization step difficult a dynamic ecosystem model was previously constructed to predict the effects of metals and pesticides on lentic ecosystems in an object oriented framework using simplified aquatox equations and the software package west mikebydhi com de laender et al 2008 the simulation results were compared to experimental results obtained from micro and mesocosm studies the model was successful in predicting ecological effects of chemicals by considering direct effects but also ecological interactions feeding and competition relationships in this study the simplified aquatox model of de laender et al 2008 was used this model was already implemented in the software package west and the equations were modified when it was needed to describe the lake food web dynamics 2 3 step 3 selecting the model structure a simplified food web was built with the most relevant populations of plankton and fish naturally present in the experimental lake 260 fig 1 the different species of plankton were grouped according to their annual dynamics i e blooms group 1 chlorophyte dinoflagellates cyanophyta group 2 crysophyta cryptophyta group 3 diatoms the main fish species selected for the model are characterized by different spawning periods and habitats hypo or epilimnion offshore bottom or littoral based on back and forward discussions with the ecologists who gathered the experimental data lake trout was decided to be the only species spending most of the time in the hypolimnion but still feeding on the fishes that mainly live in the epilimnion and move around the lake for food in aquatox the model consists of a set of objects and each object describes the growth of a model population in terms of its biomass concentration using differential equations including biological processes such as assimilation photosynthesis respiration consumption or mortality and additional processes such as migration diffusion or loading by connecting different objects and defining feeding relationships between them a customized food web can be designed fig 2 the number of populations that can be modelled is unlimited and available objects are phytoplankton zooplankton planktivorous fish and piscivorous fish intrinsically identical objects e g various groups of phytoplankton can be differentiated by parameter tuning e g spring vs summer populations dynamic driver variables also called forcing functions indicated on fig 1 at the top of the big box are used as external factors daily values of the dynamic driver variables are contained in an input file which is read by the ecosystem model during simulation the important toxicological endpoint for modelling endocrine disruption is the reproductive ability of fish for this purpose two fish classes are used in the model juveniles and adults fig 3 intersex fish are not considered explicitly because some of them are assumed to still be able to reproduce females and males are not differentiated explicitly either but the sex ratio is and it is important because it determines the gamete quantity that is produced and thus the juveniles being recruited the newly adopted equations are presented and discussed in the results section among the gametes released by the adults some are lost to the detritus and some turn into new fish which is called juveniles recruitment the juveniles become adults when they can reproduce which is called juveniles promotion the model consists of i state variables such as biomass dynamics and also dissolved organic matter dom particulate organic matter pom nitrogen n and phosphorus p ii forcing functions which are measured time series used as model input such as photoactive radiation par photoperiod temperature t conce ikntrations of oxygen o2 and 17α ethinylestradiol ee2 and iii exchanges between the two stratified layers fig 4 supported by the experimental data collected after lake stratification in the spring until shortly before turnover in the autumn two stratified layers are modelled epilimnion and hypolimnion with different biological and physico chemical compositions the epilimnion is defined as the surface layer of water with uniform temperature ignoring any shallow temporary stratification phenomena while the hypolimnion is defined as the bottom layer of water also with uniform temperature 2 4 step 4 calibrating the model the forcing functions used to calibrate the ecosystem model table 1 come from data collected from the lake in 2000 from may 2nd to october 29th before the addition of ee2 on may 2nd 2000 the lake was already stratified and the overturn of the lake started on october 8th so most of the simulation results correspond to the dynamics of a stratified lake the biomass concentrations measured in the experimental lake in may 2nd 2000 were used as initial biomass concentrations and were not changed during calibration the model calibration was conducted following a stepwise procedure fig 5 similar to the procedure adopted by corominas et al 2011 and mannina et al 2011 by which the fit to multiple relevant variables is incrementally pursued in essence nutrients are fitted first and subsequently the trophic levels are added to the multivariate fitting objective each time a trophic level is added the parameters related to that level are updated to achieve a best fit to the data corresponding to that level however since this addition of trophic level may affect the trophic levels already present a recalibration is done of the other lower trophic levels according to the same sequence as they were added multiple iterations over these trophic levels may be necessary before the next trophic level can be added and the model calibration goes forth the fine tuning of the parameters was performed following a trial and error approach based on the comparison of the simulation results with the experimental results the quality of the model fits to the data was evaluated by performing the mean square relative error msre statistical test hauduc et al 2015 the aquatox default parameter values were selected as starting point for the model calibration park and clough 2010 moreover the parameter values that were used can be found back in the source code that is coming with the paper the first simulations were performed with the nutrients nh4 no3 po4 and organic matter om present in the epilimnion a sensitivity analysis was performed by visual inspection of the simulation results after perturbation of the parameter under study by comparing the effect of different parameters a ranking of the most influential parameters was obtained for this purpose each parameter was separately multiplied or divided by two then changed by 20 to study the consequences of smaller changes and the consequences on the concentrations of nutrients and organic matter observed from there the most influencing parameters were selected and tuned until the simulation results were sufficiently close to the experimental data the sensitivity analysis was performed by visual inspection of the simulation results after perturbation of the parameter under study by comparing the effect of different parameters a ranking of the most influential parameters was obtained in the second round of simulations the nutrients in the hypolimnion were added and connected to the epilimnion through the stratification and mixing processes naturally occurring in lakes after performing the sensitivity analysis and tuning the most influencing parameters for nutrients phytoplankton were added one group at a time 1 chryso and cryptophyta 2 diatoms 3 chloro and dinophyta cyanobacteria the model calibration continued following the same procedure with zooplankton fish in the epilimnion 1 fathead minnow 2 pearl dace 3 white sucker and finally fish in the hypolimnion 4 lake trout after each group is added a sensitivity analysis is performed and the most influencing parameters tuned before adding the next group the simulation results were compared to the experimental data and back and forward discussions with the scientists who gathered the data were necessary to ensure the consistency of the physico chemical and biological parameters used in the model and the realism of the biomass dynamics in such ecosystems 2 5 step 5 predicting indirect effect of ee2 during the unique canadian whole lake experiment with ee2 kidd et al 2014 the strongest direct effect of ee2 was observed on fathead minnow with a collapse of the fish species in the second year of adding ee2 due to endocrine disruption the objective of the developed ecosystem model is to further explain the indirect effects of ee2 observed on the food web of the experimental lake therefore specific parameters were calibrated in fathead minnow in order to describe the direct effect of ee2 on fathead minnow and then the indirect effect on other fish species was analyzed after back and forth discussions with the scientists who gathered the experimental data the parameters involved in reducing gamete production increasing gamete mortality and or increasing fish mortality were identified as potential hypotheses the experimental data collected during the second year of adding ee2 to the lake kidd et al 2007 were used to fine tune the selected parameters 3 results and discussion the aim of this paper is to describe the results of both the development and the calibration of the ecosystem model and to compare the prediction results of the indirect effects of ee2 on a freshwater food web with the experimental data the ecosystem model was built according to the different compartments found in the experimental lake in addition to the biota the physical and chemical characteristics were also considered the developed ecosystem model is inspired by the simplified aquatox equations of de laender et al 2008 see their paper for a full list of equations parameters state variables and forcing functions that form the basis of the model presented here in the results and discussion section only the simplified aquatox equations of de laender et al 2008 that were changed and the equations that were added are presented regarding the calibration results present the best fit obtained between the experimental data and the simulation outputs including all the boxes of fig 5 the sections model parameters and calibration results correspond to the last box of fig 5 called calibrated values the final model has a considerable number of parameters distributed over 16 objects fishes organic matter phyto and zooplankton species for each object a number of parameters did not have to be calibrated because they represent constant lake characteristics volume ph depth etc and detritus ratios ratio of phosphorus to organic matter ratio of nitrogen to ammonia etc for the other parameters a sensitivity analysis was performed to find the limited subset of parameters that still allows getting a good model fit to the data 3 1 physico chemical characteristics 3 1 1 stratification the percentage of sedimented organic particles sed from the epilimnion to the hypolimnion is calculated using the following equation where sed is assumed to be constant over time 1 s e d v o f f s h o r e v e p i where v offshore m 3 is the volume from where the particles sediment to the hypolimnion and v epi m 3 is the total volume of the epilimnion fig 6 in aquatox the stratification is considered to occur when the mean water temperature exceeds 4 c and the difference in temperature between the epilimnion and hypolimnion exceeds 3 c when those two conditions are not met the two layers mix which normally occurs during the spring and the fall the equations used in the ecosystem model for the mixing of organic matter and nutrients were presented by vallet et al 2014 if tmean t 4 c and tepi t thypo t 3 c 2 t h e n m i x t 0 where mix g m3 d is the lake overturn rate t mean c the mean water temperature t epi c the water temperature in the epilimnion and t hypo c the water temperature in the hypolimnion 3 else m i x t q m i x v t o t x i n t x t where mix g m 3 d is the lake overturn rate q mix m 3 d is the mixing flow v tot m 3 the total volume of the lake x g m 3 the concentration of the variable being modelled nutrient nh4 no3 or po4 or organic matter dom pom or som x in g m 3 the concentration of the variable entering the modelled layer epilimnion or hypolimnion 3 1 2 oxygen and temperature oxygen concentrations in the sediments are different from the oxygen concentrations in the water fig 6 thus a correction factor was applied both in the epilimnion and the hypolimnion to differentiate the reactions happening in the sediments from the ones in water 4 d o c o r r t o 2 t o 2 t h a l f s a t o where do corr unitless is the oxygen correction factor o 2 g m 3 the dissolved oxygen concentration and halfsato g m 3 the half saturation constant for oxygen temperature is another important controlling factor in the model involved in the stratification of the lake but also in the biotic and chemical fate processes the same correction factor as the one used by de laender et al 2008 was applied to the corresponding equations in order to calculate the temperature correction for microbial processes 3 1 3 transport a genetic screening of the fathead population studied during the recovery phase of the ee2 lake experiment demonstrated that there was no fish migration from surrounding lakes blanchfield et al 2015 besides the lake inflow and outflow were shown to be negligible compared to the total volume of the lake and because it is an experimental area there was no fishing either therefore the lake could be modelled as a closed system with no exchanges with the outside consequently the transport equations used in aquatox for the biota nutrients and organic matter such as loading washout migration fishing etc could be removed from the model 3 2 organic matter and nutrients 3 2 1 model equations the organic matter mass balance is composed of the losses coming from the different biota and is divided into 3 groups 1 dissolved organic matter dom 2 particulate organic matter pom 3 and sedimented organic matter som compared to the simplified model developed by de laender et al 2008 a fraction of dead biota was added to the dom according to the equations from aquatox for readability the term t is not included on the right hand side of differential equations 5 d d o m d t e x c r e t b i o t a m o r t a l i t y b i o t a f m o r t d o m d e c o m p d o m m i x where excret biota g m 3 d is the total excretion rate of the biota mortality biota g m 3 d the total dead biota rate f mortdom unitless the fraction of dead biota transformed into dom decomp dom g m 3 d the dom loss rate due to decomposition and mix g m 3 d the lake overturn rate see equations 3 and 4 for the pom the gametes lost by fish not turning into juveniles see fig 3 were added to the simplified aquatox equations of de laender et al 2008 6 d p o m d t m o r t a l i t y b i o t a f m o r t p o m g a m e t e l o s s d e c o m p p o m s e d p o m i n g e s t p o m where mortality biota g m 3 d is the total dead biota rate f mortalitypom unitless the fraction of dead biota transformed into pom gameteloss g m 3 d the loss rate for gametes decomp pom g m 3 d the pom loss rate due to decomposition sed pom g m 3 d the sedimentation rate of pom and ingest pom g m 3 d the consumption rate of pom by zooplankton for the som the main difference with the simplified aquatox equations of de laender et al 2008 was to consider the lake stratification according to the equation presented earlier equation 1 thus when modelling the hypolimnion part of the som coming from the epilimnion is considered the decomposition of the three organic matter groups is based on a maximum decay rate decay max g g d corrected for suboptimal temperature dissolved oxygen and ph the difference with the simplified aquatox equation used by de laender et al 2008 is that in the developed ecosystem model the variation of oxygen concentrations between the water column and the water sediment interface is considered see equation 4 the dom and pom are considered being decomposed in the water column only while som also decomposes in the sediments a fraction of the decomposed organic matter is converted into nutrients nh4 no3 po4 the main additions made to the simplified aquatox equations used by de laender et al 2008 for the nutrient pool are the addition of no3 assimilation by phytoplankton and the nitrification denitrification processes happening both in the water column and at the water sediment interface 7 d n h 4 d t d e c o m p o m e x c r e t b i o t a r e s p b i o t a n 2 o m a s s i m n h 4 n i t r i f w a t e r n i t r i f s e d m i x where nh 4 g m 3 is the ammonia concentration decomp om g m 3 d the organic matter decomposition rate excret biota g m 3 d the total excretion rate of the biota resp biota g m 3 d the total respiration rate of the biota n2om unitless the n om ratio assim nh4 g m 3 d the nh4 assimilation rate by phytoplankton nitrif water g m 3 d the nh4 nitrification rate in the water column nitrif sed g m 3 d the nh4 nitrification rate at the water sediment interface and mix g m 3 d the lake overturn rate 8 d n o 3 d t n i t r i f w a t e r n i t r i f s e d d e n i t w a t e r d e n i t s e d a s s i m n o 3 m i x where no 3 g m 3 is the nitrate concentration denit water g m 3 d the no3 denitrification rate in the water column denit sed g m 3 d the no3 denitrification rate at the water sediment interface and assim no3 g m 3 d the no3 assimilation rate by phytoplankton 9 d p o 4 d t d e c o m p o m e x c r e t b i o t a r e s p b i o t a p 2 o m a s s i m p o 4 m i x where po 4 g m 3 is the phosphate concentration p2om unitless the p om ratio and assim po4 g m 3 d the po4 assimilation by phytoplankton 3 2 2 calibration results during the calibration of the nutrient and organic matter parameters the most influencing parameters selected with the sensitivity analysis are related to the nitrification denitrification processes and decomposition of organic matter table 2 the values presented were obtained after model fit fig 7 and correspond to the final iteration of the calibration procedure see fig 5 last box called calibrated values maximum rates for the denitrification process are similar between the epi and the hypolimnion while nitrification happens at higher maximum rates in the epilimnion which makes sense since the nitrification process requires aerobic conditions when looking at the graph presenting final results after fine tuning of the calibrated parameters fig 7a it appears that nh4 mainly occurs in the epilimnion where the nitrification is at its highest rate the graph clearly shows that both nh4 and no3 accumulated during the winter when nitrification and denitrification rates are very low due to low temperatures and then as soon as spring starts both nh4 and no3 are eliminated through nitrification and denitrification processes early in the fall when temperatures drop again nh4 starts accumulating again the simulation results obtained with the calibrated parameters succeeded at catching those dynamics regarding som and dom decomposition the maximum rates appeared to be higher in the epilimnion while higher in the hypolimnion for the pom when looking at the pom graph fig 7b concentrations are higher in the hypolimnion where the pom accumulates once again the simulation results obtained with the calibrated parameters succeeded at matching the trend observed in the experimental data back and forward discussions on the nutrient and organic matter dynamics with the freshwater system ecologists who gathered the experimental data confirmed that the calibrated parameters succeeded at simulating their dynamics during the open water season 3 3 phytoplankton 3 3 1 model equations the phytoplankton mass balance is described by the following equation 10 d p h y t o d t p h o t o r e s p e x c r m o r t s i n k p r e d where phyto g m 3 is the phytoplankton biomass photo g m 3 d the rate of photosynthesis resp g m 3 d the respiratory loss excr g m 3 d the rate of excretion mort g m 3 d the rate of non predatory mortality sink g m 3 d the loss due to sinking to the bottom and pred g m 3 d the consumption of phytoplankton by zooplankton photosynthesis is modelled as a maximum rate which is reduced by nutrient temperature and light limitation factors de laender et al 2008 for each species optimal photosynthesis is reached at optimal temperature and depth and is directly connected to the photosynthetically active solar radiation par following aquatox equations when there is no ice cover on the lake tmean 3 c the light is entered in the ecosystem model as an input using the measured values of photoactive radiation par table 1 if there is an ice cover a factor of 0 3 is applied to the values of par there are two main limiting factors to the light used for photosynthesis the first one is the photoperiod representing the fraction of the day with daylight and entered in the ecosystem model as an input table 1 the second one is the extinction of light extinct when entering the water due to self shading of the phytoplankton organic particles and dissolved organic matter compared to the simplified model developed by de laender et al 2008 the light limitation factor was changed for the equation used in aquatox 11 l t l i m i t t 0 85 e p h o t o p e r i o d t l t a t d e p t h t l t a t t o p t e x t i n c t t d e p t h b o t t o m d e p t h t o p where ltlimit unitless is the light limitation 0 85 unitless the correction factor for daily formulation e 2 718 unitless the base of natural logarithms photoperiod unitless the fraction of the day with daylight ltatdepth unitless the limitation of algal growth due to light ltattop unitless the limitation due to insufficient light extinct 1 m the total light extinction from self shading of the phytoplankton organic particles and dissolved organic matter depth bottom m the maximum depth and depth top m the depth of the top layer nitrogen and phosphorus compounds are assimilated during the photosynthesis process which is modelled in the nutrient equations see equations 7 9 regarding the assimilation of nitrogen because only 23 percent of the weight of nitrate is nitrogen and 78 percent of ammonia is nitrogen this results in an apparent preference for ammonia thus compared to the simplified model developed by de laender et al 2008 a preference factor nh4pref inspired by aquatox was added 12 n h 4 p r e f t 14 18 n h 4 t 14 62 n o 3 t k n 14 18 n o 3 t k n 14 62 n o 3 t 14 18 n h 4 t k n 14 18 n h 4 t 14 62 n o 3 t k n 14 62 n o 3 t where nh4pref unitless is the ammonia preference factor 14 18 unitless the ratio of nitrogen to ammonia 14 62 unitless the ratio of nitrogen to nitrate kn gn m 3 the half saturation constant for nitrogen uptake nh4 g m 3 the concentration of ammonia and no3 g m 3 the concentration of nitrate regarding the other biotic processes i e respiration excretion mortality sinking and predation the equations used in the developed ecosystem model are described in de laender et al 2008 except for the respiratory loss which was replaced by the aquatox equation respiratory loss is exponential with temperature and since the developed ecosystem model is to be applied to stratified lakes implying changes of temperature the aquatox equation was preferred over the simplified equation of de laender et al 2008 that was used for non stratified micro and mesocosms 13 r e s p t r e s p 20 1 045 t e m p t 20 p h y t o t where resp g m 3 d is the respiratory loss resp20 g g d the respiration rate at 20 c 1 045 c the exponential temperature coefficient temp c the ambient water temperature and phyto g m 3 the phytoplankton biomass 3 3 2 calibration results during the calibration of the phytoplankton parameters the most influencing parameters selected with the sensitivity analysis are mainly related to photosynthesis table 3 group 1 and group 3 seem to assimilate better p than n while group 2 assimilates p and n similarly group 3 has the lowest maximum photosynthesis rate parameters for mortality sedimentation and temperature were also selected as influencing parameters experimental data for phytoplankton biomass suffered from high variability because of the many species in each group but also because of the experimental uncertainty resulting in high variability of the data collected the values presented in table 3 were obtained after model fit within the error bars of the experimental data fig 8 and correspond to the final iteration of the calibration procedure for the three groups of phytoplankton see fig 5 last box called calibrated values back and forward discussions on the phytoplankton biomass dynamics with the scientists who gathered the experimental data confirmed that the calibrated parameters managed to simulate relatively well the main trends during the open water season considering the high variability of the experimental data and except for the initial decline the simulation results captured that group 1 organisms were more abundant than group 2 and that group 3 had the lowest concentrations besides group 1 is characterized by a bloom between april and june which explains the high initial biomass concentration followed by a drop fig 8a blooms for group 2 and group 3 start later which can be seen with lower drops on the graphs fig 8b and 8c 3 4 zooplankton 3 4 1 model equations the zooplankton mass balance is described by the following equation de laender et al 2008 14 d z o o d t c o n s d e f r e s p e x c m o r t p r e d where zoo g m 3 is the zooplankton biomass cons g m 3 d the consumption of phytoplankton and pom def g m 3 d the defecation of unassimilated food resp g m 3 d the respiratory loss excr g m 3 d the excretion of dissolved organic matter mort g m 3 d the non predatory mortality and pred g m 3 d the consumption of zooplankton by planktivorous fish 3 4 2 calibration results during the calibration of the zooplankton parameters the most influencing parameters selected with the sensitivity analysis are mainly related to the consumption of phytoplankton and pom table 4 parameters for excretion respiration mortality and temperature were also selected as influencing parameters the values presented were obtained after model fit for the three groups of zooplankton fig 9 and correspond to the final iteration of the calibration procedure see fig 5 last box called calibrated values the final values of the parameters resulted in rotifers only eating pom while rotifers normally eat some phytoplankton their main diet consists of dead or decomposing organic material due to their microscopic size the scientists who gathered the experimental data confirmed that it was an acceptable simplification of the model when looking at the graph presenting final results after fine tuning of the calibrated parameters fig 9 it appears that the simulation results succeeded at predicting the zooplankton dynamics during the open water season except for a faster growth of cladocera and copepods in may back and forward discussions on the zooplankton biomass dynamics with the scientists who gathered the experimental data confirmed that when looking at experimental data for other years or in the reference lakes cladocera and copepods have earlier growth peaks starting early may for copepods and mid may for cladocera 3 5 fish 3 5 1 model equations for the fish mass balance de laender et al 2008 used the same equation as for the zooplankton mass balance presented in equation 14 in order to model endocrine disruption two fish classes were added juveniles adults see fig 3 and three reproductive terms added to the mass balance accordingly to aquatox juveniles 15 d f i s h j u v d t c o n s d e f r e s p e x c m o r t p r e d p r o m o r e c r u i t adults 16 d f i s h a d u l t d t c o n s d e f r e s p e x c m o r t p r e d g a m e t e l o s s p r o m o where fish juv g m 2 is the juvenile biomass fish adult g m 2 is the adult biomass cons g m 2 d the consumption of pom def g m 2 d the defecation of unassimilated food resp g m 2 d the respiratory loss excr g m 2 d the excretion of dissolved organic matter mort g m 2 d the non predatory mortality pred g m 2 d the consumption of fish by piscivorous fish gameteloss g m 2 d the loss of gametes during spawning promo g m 2 d the promotion from juveniles to adults and recruit g m 2 d the recruitment from viable gametes to juveniles eggs and sperm modelled as gametes can be a significant fraction of adult biomass because only a small fraction of these gametes results in viable young when shed at the time of spawning entered as parameters in the model spawningstart and spawningend the remaining fraction is lost to detritus gameteloss if spawningstart t spawningend then 17 g a m e t e l o s s t g m o r t i n c r m o r t f r a c a d u l t s t p c t g a m e t e b i o t else g a m e t e l o s s t 0 where gameteloss g m 2 d is the loss of gametes during spawning spawningstart d the date when spawning starts spawningend d the date when spawning ends gmort 1 d the gamete mortality incrmort 1 d the increased gamete and embryo mortality due to toxicant fracadults unitless the fraction of biomass that is adult pctgamete unitless the fraction of adult biomass that is in gametes and bio g m 2 the biomass as the biomass of a fish population reaches its carrying capacity which is the maximum sustainable biomass reproduction is usually reduced due to stress in the model this results in assuming the proportion of adults and the fraction of biomass in gametes at a maximum 18 f r a c a d u l t s t 1 c a p a c i t y t k c a p if biofish t kcap then 19 c a p a c i t y t k c a p b i o f i s h t else c a p a c i t y t 0 where fracadults unitless is the fraction of biomass that is adult capacity g m 2 the biomass capacity kcap g m 2 the carrying capacity and bio g m 2 the biomass during spawning gametes are lost from the adults and the juveniles gain the viable gametes through recruitment which is in other words the biomass gained from successful spawning if spawningstart t spawningend then 20 r e c r u i t t 1 g m o r t i n c r m o r t f r a c a d u l t s t p c t g a m e t e b i o t else r e c r u i t t 0 where recruit g m 2 d is the recruitment from viable gametes to juveniles spawningstart d the date when spawning starts spawningend d the date when spawning ends gmort 1 d the gamete mortality incrmort 1 d the increased gamete and embryo mortality due to toxicant fracadults unitless the fraction of biomass that is adult pctgamete unitless the fraction of adult biomass that is in gametes and bio g m 2 the biomass the juveniles promoted to adults is determined in the model by the rate of growth considered as the sum of consumption and the loss terms other than mortality 21 p r o m o t k p r o c o n s t d e f t r e s p t e x c t where promo g m 2 d is the promotion from juveniles to adults kpro unitless the fraction of growth that goes to promotion cons g m 2 d the consumption of phytoplankton and pom def g m 2 d the defecation of unassimilated food resp g m 2 d the respiratory loss and excr g m 2 d the excretion of dissolved organic matter 3 5 2 calibration results for fathead minnow and pearl dace during the calibration of the fathead minnow and pearl dace parameters the most influencing parameters selected with the sensitivity analysis are mainly related to reproduction and food consumption tables 5 and 6 parameters for excretion respiration mortality and temperature were also selected as influencing parameters experimental data were collected in the spring may 16 and the adult fish concentrations were 0 23 g m2 and 0 28 g m2 for fathead minnow and pearl dace respectively the simulation results were quantitatively discussed with ecological specialists in order to validate the biomass dynamics obtained during the open water season for both juveniles and adults the values presented in tables 5 and 6 correspond to the final iteration of the calibration procedure see fig 5 last box called calibrated values and were obtained after the model was fitted with the experimental data and the experts validated the biomass dynamics fig 10 for fathead minnow fig 10a the experimental data 0 23 g m2 on may 16 was collected before the spawning period which lasted from the end of june to the end of july the beginning of spawning can be seen on the graph with a drop of the adult biomass occurring after june 21st correlated to an increase of the juvenile biomass before spawning the adult biomass increases because they are producing gametes and then when spawning starts the gametes are lost which explains the decrease in the adult biomass conversely the juvenile biomass increases when spawning starts because viable gametes turn into young fish after spawning from july 21st to the fall biomass of both adults and juveniles increases first due to high temperature and food availability during summer and then decreases when fall arrives due to a decrease in temperature and food availability for pearl dace fig 10b the spawning period starts earlier mid april and ends around mid may which means the experimental data 0 28 g m2 on may 16 was collected just at the end of the spawning period may 16 the same pattern than for fathead minnow occurs but earlier the decrease of adult biomass and increase of juvenile biomass started before the beginning of the graph and ended around june 17th when spawning ended 3 5 3 calibration results for white sucker and lake trout during the calibration of the white sucker and lake trout parameters the most influencing parameters selected with the sensitivity analysis are mainly related to reproduction and food consumption tables 7 and 8 parameters for excretion respiration mortality and temperature were also selected as influencing parameters experimental data were collected in the spring may 16 for adult white sucker and the biomass concentration was measured at 7 42 g m2 for the adult lake trout the data were collected in the fall october 3rd and the biomass concentration was measured at 4 04 g m2 the simulation results were quantitatively discussed with ecological specialists in order to validate the biomass dynamics obtained during the open water season for both juveniles and adults the values presented in tables 7 and 8 correspond to the final iteration of the calibration procedure see fig 5 last box called calibrated values and were obtained after the model had been fitted with the experimental data and the experts validated the biomass dynamics fig 11 adult white sucker have the same spawning period as adult pearl dace which is between mid april and mid may thus similar dynamics can be observed on both graphs figs 10b and 11c however due to higher mean weights tables 7 and 8 the biomass changes observed in white sucker are of lower magnitude compared to pearl dace both for adults and juveniles the piscivorous fish lake trout fig 11d have a later and longer spawning period mid august to mid october compared to the other planktivorous fishes fathead minnow pearl dace and white sucker nevertheless a similar pattern occurs which is an increase in both adult and juveniles biomass in the summer and then a decrease in adult biomass when spawning starts due to gamete loss 3 5 4 endocrine disruption after having successfully calibrated the ecosystem model with a unique set of experimental data collected in 2000 from may 2nd to october 29th before the addition of ee2 the experimental data collected during the second year of adding ee2 to the lake kidd et al 2007 were used to help further explain the indirect effects on the lake food web the experimental results showed that the strongest direct effect of ee2 was observed on fathead minnow with a collapse of the fish species in the second year of adding ee2 due to endocrine disruption therefore discussions with experts in ecotoxicology and edcs helped identify specific parameters to be modified in fathead minnow in order to test a potential hypothesis for the biological processes involved in endocrine disruption in fathead minnow table 9 when looking at the graph presenting final results after fine tuning of the selected parameters fig 12e and 12f it appears that a mix of reduced gamete production pctgamete increased gamete mortality incr mort and increase of both adults and juveniles mortality k mort adult and k mort juv is a potential hypothesis for explaining the collapse of both adult and juvenile fathead minnow due to ee2 addition kidd et al 2007 the simulation results also help further explain how the other fishes were impacted by the collapse of fathead minnow indeed endocrine disruption in fathead minnow did not only affect its own population but also other fish populations kidd et al 2014 after three summers of ee2 addition the experimental results showed a 58 reduction of the pearl dace population however a reduction of the pearl dace population was also observed in the reference lakes and thus no conclusion on the link with ee2 could be made the model prediction results provided a new insight regarding a potential link with ee2 with 16 reduction of the adult pearl dace population and no significant changes for the juveniles population due to ee2 fig 12g regarding the white sucker population the experimental results did not indicate significant changes which corresponded with the simulation results that predicted no changes for the adult population however a 62 reduction was predicted for the juveniles population but this may have gone unnoticed experimentally since they represent the population class that is most difficult to sample due to the smaller sizes fig 12g regarding the lake trout population the experimental results and the model predictions both showed a reduction of around 25 juveniles and adults fig 12h back and forward discussions with the scientists who gathered the data and experts in ecology validated the simulation results since lake trout could no longer feed on fathead minnow they turned to pearl dace and white sucker the consequence was a decrease of both prey biomass fig 12g and a change in lake trout biomass fig 12h therefore the calibrated model was successful in predicting the indirect effects of ee2 on the lake food web and thus further explain the experimental data 4 conclusion an ecosystem model that can help further explain indirect effects of endocrine disruption in fish in a lake food web was successfully developed and calibrated with a unique set of experimental data coming from the whole lake study of kidd et al 2007 2014 back and forward discussions with the scientists who gathered the data and experts in ecology were necessary to ensure the consistency of the physico chemical and biological parameters used in the model and the realism of the biomass dynamics in such ecosystems this study suggests that a mix of reduced gamete production increased gamete mortality and fish mortality produced a similar pattern in fathead minnow as observed in the second year of exposure to ee2 the simulation results also helped explain how the other fishes were impacted by the collapse of fathead minnow to further investigate the indirect effect of ee2 an edc of great concern the ecosystem model will be used to better understand the ecological interactions and how endocrine disruption in fishes such as fathead minnow can impact a whole lake food web indeed more research is needed to develop ecosystem models like the one presented in this paper that can support era of edcs and other aquatic stressors software availability name of the software ela fish model library software requirements west 3 7 6 or higher program language model specification language msl see vanhooren et al 2003 for an explanation on msl code program size approximately 25 mb availability the source code for the ela fish model library can be obtained via github at the following url https github com modeleau ela msl git acknowledgements this study was funded through the canada research chair in water quality modelling held by peter vanrolleghem and a research grant from the natural sciences and engineering research council of canada nserc of canada through the strategic grants program 430646 2012 the authors thank karen kidd michael paterson michael rennie paul blanchfield and alain dupuis for sharing the unique data they collected during their whole ecosystem study and for the numerous discussions we had around the experimental data and simulation results appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 013 
