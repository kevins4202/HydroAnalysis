index,text
4195,in recent years the artificial intelligence and data mining ai dm models have become popular tools in assisting various aspects of reservoir operation however the practical uses are still rarely reported comparison experiment of many ai dm models over a large number of reservoir cases is particularly valuable to help reservoir operators first examine the usefulness and transferability of different ai dm models and then identify the most stable and reliable ai dm model in assist of various decision making processes in this study a total of 12 ai dm models with different parameterizations and simulation scenarios are comprehensively tested out and compared in simulating the controlled reservoir outflows of 33 reservoir cases over the upper colorado region united states results show that the random forecast and the long short term memory model could consistently derive the best statistical performance than other models under the baseline simulation scenario the employed ai dm models could obtain satisfactory statistical interquartile ranges 25 75 between 0 6 0 9 0 3 0 8 and 0 2 0 8 for corr nse and kge measurements respectively and 1 5 6 5 15 to 20 and 0 5 8 5 for the normalized rmse pbias and rsr measurements respectively results also show multi layer perceptron model and extreme gradient boosting tree algorithm produced more stable and superior performance than other models under more complex input scenarios we also found that the performance of different ai dm models are closely relevant to the reservoir elevations sizes and functionalities discussions were made about the sensitivity of ai dm models parameterizations and the key advantages of ai dm models over the rule based reservoir models we further identify that the main advantage of ai dm models is the flexibility in designing input structures whereas the rule based simulation model is rather limited future studies were suggested regarding the best way reservoir operators and researchers could use select and apply different ai dm models in simulating reservoir releases under different natural and modeling environments this comparison study also serves as a reference and a piece of groundwork for further promoting the practical uses of ai dm models in assisting reservoir operation keywords artificial intelligence data mining reservoir operation decision making 1 introduction reservoirs and dams are fundamental human built multi functional water infrastructures that collect store and deliver fresh surface water for a multitude of uses including flood and fire control recreation wildlife habitat residential industrial and agricultural water supply hydro electric power generation supply source during droughts and more the reservoir release decisions directly influence various aspects of social economic functioning and our nation s security yang et al 2016 yang et al 2020b in recent years more frequent and severe abrupt weather extremes climate change natural hazards aging infrastructure and increases in water demands due to population growth have placed another great barrier to prevent effective sustainable and flexible operation for our nations reservoir systems for example in may 2020 due to extended extreme precipitation water behind the two consecutive reservoirs in michigan reached the reservoir storage capacity and caused catastrophic dam breaks flooding the tittabawassee river and completely drained the reservoirs cnn 2020 in 2017 the addicks and barkers reservoirs near the houston area were intentionally operated to release additional water downstream this operation happened during the same period when hurricane harvey hit the houston area causing an additional 8000 houses near these two reservoirs were flooded the federal judge ruled the u s army corps of engineers usace is liable for flooding these homes cnn 2019 houstonchronicle 2019 houstonpublicmedia 2019 in the same year a sudden water flux reaching the designed spillway capacity of the oroville dam california caused irreversible damage to the dam and triggered a large scale emergency evacuation of more than 180 000 people living downstream mercurynews 2017 newyorktimes 2017 these recent dam failure cases indicate the need for continuous developments of effective and flexible reservoir operation tools and modeling schemes the approaches for reservoir operation and decision support can be categorized into optimization models and simulation models labadie 2004 reddy and kumar 2006 yeh 1985 differs from reservoir optimization models reservoir simulation models are useful in assisting operators in estimating reservoir yields and quantifying system behaviors based on pre defined operating rules louks and sigvaldason 1981 in early studies sigvaldson 1976 developed an innovative approach for simulating reservoir responses using a priority ranking concept chaturvedi and srivastava 1981 developed a screen simulation model based on linear programming methods for a large complex water resources system the reservoir simulation models have rapidly evolved from excel sheets based models in early times to coupled hydrological and hydraulic models to support various types of operations such as predicting how the system behaves under the current hydrological situation inflow storage extractions releases on different temporal scales many reservoir simulation models have become operational and widely used in the u s e g the hec 5 model developed by usace bonner 1989 dwrsim developed by cdwr barnes and chung 1986 chung et al 1989 the weap21 model yates et al 2005 the calsim model draper et al 2004 the riverware models zagona et al 2001 and the cram water resources modeling tool lynkertech 2018 etc lund and guzman 1999 concluded that simulation models were more likely to be trusted as a standard by operators as compared to reservoir optimization models in practice these reservoir simulation models rely on so called reservoir operating rule curves which define an empirically desired reservoir storage release relationship louks and sigvaldason 1981 these rule curves are subject to approval by governmental authorities and are defined beforehand usually the formulation of the release rule is based on historical data or design scenarios the set of rules must be defined in such a way that for as many as situations and the operational goals e g power production water supply minimum flow are conservatively met under given constraints e g dam safety requirements flood control requirements environmental obligations an advantage of rule based operations is that the set of rules is usually transparent clear and can easily be integrated into simulation models for the water system however oliveira and loucks 1997 pointed out in many situations the operators will still operate the system in a way that deviates from these pre defined rule curves to adapt to specific conditions objectives or constraints that may change over time draper et al 2004 also criticized that many simulation models were severely restricted by the explicit implementation of operating rules as hard model constraints which jeopardized the flexibility of using such tools to adjust to different environmental settings in other words a drawback of rule based operations is that the control actions are not necessarily the optimal strategy for the current situation and could not cover various conditions from changing environment to give an example a target water level for a reservoir on a specific day in the year would account for both dry and wet situations in order to be able to cope with both water scarcity and flood issues in a dry situation it might be possible to operate the reservoir with a higher target water level this would be beneficial for hydropower production and water supply but is of course only suitable if one can afford to operate with a lower flood control room if on the contrary a substantial increase in inflow is expected the target water level should be even lower than the rules might say in such a situation it will make sense to pre release water in order to generate a sufficient flood control room with respect to these advantages and disadvantage of rule based reservoir operation and simulation models in recent years the artificial intelligence and data mining ai dm techniques become popular tools in assisting reservoir operation and decision making cancelliere et al 2002 chaves and chang 2008 cheng et al 2020 coulibaly et al 2001 coulibaly et al 2000 esmaeilzadeh et al 2017 jain et al 1999 kişi 2007 maier et al 2010 wu et al 2009 yang et al 2017b zhang et al 2019 the ai dm models are powerful tools in data classification and regression but they purely rely on the statistical relationship between the target variables and the input data e g the input features by setting different combinations of model training data and target variables the uses of ai dm models also appear to be versatile for example these models can be used to simulate reservoir release to extract the existing operation rules and to predict reservoir inflows flows and uncertainties and to manage reservoir storage and water levels etc ashaary et al 2015 bessler et al 2003 castelletti et al 2012 castelletti et al 2010 chang et al 2016 cheng et al 2008 rahnamay naeini et al 2020 wei and hsu 2008 however one big challenge of applying these ai dm tools in assisting reservoir operation is the lack of scalability and transferability different ai dm models employ distinct data classification and regression philosophies in which none of them are subject to the mass balance equations with physical constraints this hinders the practitioner from trusting the modeling results especially when operators are used to the traditional rule based simulation models in addition the ai dm model s performance may substantially vary based on user selected model structural parameters and the way different ai dm models are set up in short it is an extremely tedious work for reservoir operators to vet a set of ai dm models identify the most suitable approach and associated parameters for one particular application and carry out verification experiments for another reservoir case or another problem setting to make sure the ai dm models are transferable and scalable under different simulation environment differs from the rule based reservoir simulation models the performance of ai dm models are likely to change when the training data changes furthermore the field of artificial intelligence is still rapidly evolving newer and stronger ai dm models are becoming readily available for applications before an older and simpler model being thoroughly evaluated is assisting of reservoir operation and decision making this makes reservoir operators even more hesitant to trust and practice an alternative ai dm model over their existing and functional rule based reservoir simulation models in the research community there have been numerous studies to apply a variety of ai dm models to assist reservoir operation hydrology and water resources management adnan et al 2019 shabani et al 2020 shamshirband et al 2020 yuan et al 2018 some popular ai dm models include linear regression model support vector machines smv k nearest neighbors regression knn decision tree dt model multi layer reception mpl model i e artificial neural network model and deep learning algorithm i e the convolutional neural network and recurrent neural network family etc however each ai dm model is subject to specific pros and cons and there is no commonly accepted agreement on which modeling scheme is consistently effective than others across different study cases the linear regression model is the most straightforward statistical technique and early approach used to quantify the linear relationship in hydrological time series though it is relatively simple and old fashioned it has been widely used to investigate hydrologic variables adnan et al 2020 caldwell et al 2015 li et al 2016 ombadi et al 2020 ren et al 2020 sahoo and jha 2013 schmidt et al 2020 yuan et al 2018 derive reservoir operation rule policy ghimire et al 2020 liu et al 2019 zhou et al 2016 predict reservoir inflow and streamflow adnan et al 2019 lima and lall 2010b masselot et al 2016 estimate hydraulic behaviors adnan et al 2021 and assist water quality management zhao et al 2018 and drought prediction li et al 2020 the linear regression model s advantage is its simplicity and efficiency when the decision variables and target variables have an underlying linear correlation however the disadvantage is also apparent if only a nonlinear correlation exists between the decision variables and target variables such always fails to capture such a complex relationship and can only simplify the regression with a linear estimation unlike the linear models the support vector machine svm model acknowledges the existence of a possible nonlinear relationship between the features and the target variable the svm models could be further categorized into support vector classification svc and support vector regression svr models based on the nature of problems in the svm model a hyperplane will be created and used to separate the feature samples in the feature space this hyperplane could be either linear or nonlinear based on the user selected kernel functions because the hyperplane in svm models could adaptively partition the data samples the svm models could address the disadvantage of linear models for nonlinear regression and guarantee a unique and globally optimal solution when searching for the hyperplane lin et al 2006 theoretically the svm models could minimize the errors in the learning process and effectively reduce data overfitting if a proper kernel function is applied lal and datta 2018 yu et al 2006b the svm model has been applied to solve various water resources problems such as reservoir operation aboutalebi et al 2016 bozorg haddad et al 2018 ji et al 2014 liu et al 2017a xie et al 2012 and reservoir inflow and streamflow forecasting babaei et al 2019 feng et al 2020 liu et al et al 2017b malik et al 2020 samadianfard et al 2019 tao et al 2018 the svm model s advantage is that it can easily overcome the high dimensionality problem hand 2007 however a major drawback associated with the svm model is its low training efficiency wei 2015 when applying the svm models to datasets with large samples the training time tends to increase exponentially with the total number of data samples which prohibits some real world applications especially when quick model training and decision making are needed the k nearest neighbors knn model is an instance based learning or lazy learning method which was originally developed by fix 1951 and further improved by many others altman 1992 coomans and massart 1982 cover and hart 1967 the knn model can be applied to both classification and regression problems the classification procedure uses a neighbor search algorithm to recursively find the closeness of a total number of k training examples in the feature space after classification a regression could be carried out by averaging the target values from the k nearest neighbor samples atkeson et al 1997 the applications of knn for reservoir operation and water resources management are also numerous for example nikoo et al 2014 applied knn for water and wastewater allocation in the dez reservoir river system in iran and obtained good model statistical performance ahmadi et al 2010 combined knn and a genetic algorithm in a reservoir simulation optimization model and successfully incorporated forecast uncertainties of inflow into optimal reservoir operation yang et al 2020a tested a knn forecasting model to generate medium to long term inflow forecasts for the danjiangkou reservoir in china and the results proved the validity and reliability of the proposed knn prediction method the advantages of knn lie in its capability in achieving computational tractability toussaint 2005 and high effectiveness in approximating the target variables using a limited number of decision variables in a small subset of data samples two drawbacks of knn include 1 the training procedure sometimes overlooks the similarity and statistical relationship of the entire training samples thus did not work well on datasets that contain a high level of noises and 2 all calculations are deferred until classification bremner et al 2005 similar critiques on knn models also exist for example akbari et al 2011 pointed out that successful applications of the knn model rely on the similarity of the output values within the defined neighbors where the feature vectors are to be relatively close to each other however the number of neighbors in each group can be different and some neighbors may belong to none of the clusters in such cases the prediction made by knn algorithms may be risky and unreliable in other words the application of knn models would require a strong and local correlation between decision and target variables though such a correlation relationship may not exist globally nevertheless the use of knn models in reservoir operation is still popular in the literature another popular ai dm model set in support of reservoir management is the decision tree dt model breiman 2001 breiman et al 1984 chen and guestrin 2016 loh 2014 quinlan 1986 the dt model is a set of white box machine learning models which rely on building a sequence of simple boolean if then and true false logic to explain how complex data samples could be partitioned into smaller subsets or classes based on the feature values a comprehensive review of dt models can be found in mosavi et al 2018 in other words each data sample is regarded as a sequence of logical decision outcomes from the feature variables and similar decision outcomes could be traced back to the feature values thresholds used in the training process or the tree growing process therefore the advantage of dt models is the transparency to users and it shares a very similar procedure of how reservoir release decision is made for example dam operators typically use current storage level and rule curves i e the relationship between discharge and storage to decide whether to release a certain amount of water from reservoirs raso et al 2014 schwanenberg et al 2012 schwanenberg et al 2014 uysal et al 2020 zagona et al 2001 zhang et al 2020 some recent applications of dt models to reservoir release simulation have proven their usefulness in assisting different reservoir operation such as release scheduling ji et al 2016 rahnamay naeini et al 2020 wei 2012 yang et al 2015 yang et al 2020b zhang et al 2019 and reservoir inflow forecast erdal and karakurt 2013 tongal and booij 2018 yang et al 2017b specifically yang et al 2016 tested out a few different dt models to simulate the reservoir releases from 9 major reservoirs in california and concluded that the advantage of dt models is their effectiveness in capturing how reservoir releases are following the storage and inflow conditions which is a similar process used in the traditional rule based simulation model in another study yang et al 2020b found that different dt models in various input conditions could generate varying simulations with high variations and concluded that dt models are very sensitive to training data and may have a low transferability in general the implementations of dt models for reservoir simulation are still limited in practice one reason is the lack of accuracy of ensemble forecasts and significant hydrological uncertainty and the other reason is that over and under fitting can occur in dt models for datasets with a small number of samples or the training samples value becomes too sparse in short low robustness and accuracy are identified when scaling up dt models to different reservoir case studies brodley and utgoff 1995 dietterich 1995 mingers 1989 the multiple layers perceptron mlp or artificial neural network ann model is also one of the popular ai dm model sets in the broader field of hydrology community mlp or ann model has been widely applied to solve many types of problems including the rainfall runoff simulation reservoir operation and streamflow forecast problems coulibaly et al 2000 dawson and wilby 1998 kişi 2004 xu and li 2002 the main advantage of the ann model is its ability to detect complex nonlinear relationships between input features and outputs through the flexible learning process and the ann model can incorporate all necessary relationships through training procedures without requiring a priori knowledge of the underlying process daliakopoulos et al 2005 french et al 1992 for example sattari et al 2012 applied the ann model with an early stopped training approach to enhance the model s predictive performance they demonstrated that the ann model approach has substantially better accuracy than other baseline models in forecasting the daily reservoir inflow time series zhang et al 2018 compared three ai models including an ann model an svr model and a deep learning model to assist reservoir operation at different time scales concluding that the ann model s applicability on limited amounts of data is better than the other two ai models niu et al 2019 also compared three ai models ann svr and extreme learning machine and a multi variable linear regression for deriving the hydropower reservoir operation rule of hongjiadu reservoir in southwest china concluding that the three ann models have some unique merits and have better performance than the other employed regression models in many other studies the ann or the mlp models have shown superior performance over other traditional statistical time series models i e auto regressive integrated moving average arima and or other ai dm models in different hydrological and reservoir operation studies adamowski et al 2012 babaei et al 2019 jain et al 1996 lin et al 2009 lohani et al 2012 raman and sunilkumar 1995 valipour et al 2013 however the mlp or ann models still suffer from some weakness caused by 1 the lack of physical interpretation of its structure 2 difficulties in finding the global optimum of connecting weights and 3 uncertainties from the model parameters and settings govindaraju 2000 kim et al 2020 specifically the inputs number of layers and nodes activation function and training algorithm are considered as the major sources of uncertainty kasiviswanathan and sudheer 2017 for example the most commonly used activation function is the logistic sigmoid function hsu et al 1995 however the use of the hyperbolic tangent activation function could be better than the logistic sigmoid function in other applications zadeh et al 2010 in addition the generalization of the ann model can be highly challenging as the model performance can vary from one training dataset to another even with the same model structure and parameters in other words when applying ann or mlp models to different reservoir case studies practitioners need a lot of efforts to tune the model parameters and the parameters selection process could be challenging and time consuming these drawbacks of mlp or ann models substantially prevent a broader use of the models in assisting various types of reservoir operation practices the long short term memory lstm model is a class of deep machine learning algorithms and is rapidly gaining popularity in more recent years the lstm model is a special kind of recurrent neural network rnn which enables the output in the previous step to be used as input in the current step thus it has merits for simulating and predicting sequential time series data several recent studies have reported the superior performance of applying the lstm model to predict hydrological time series apaydin et al 2020 fan et al 2020 kao et al 2020 sahoo et al 2019 yuan et al 2018 and most of the studies are relatively new as compared to other ai dm models for example kratzert et al 2018 tested the lstm model in rainfall runoff forecasting and compared the results to the well known process based hydrologic model i e sacramento soil moisture accounting model or sac sma model they concluded that the lstm model is competitive in predicting runoff from meteorological observations compared to the process based sac sma model zhang et al 2018 tested out the lstm model in predicting the long term inflow time series of the danjiangkou reservoir which is the head water source for china s south to north water diversion project middle route and verified the outstanding advantage of the lstm model in effectively learning the sequential dependencies of reservoir inflow variabilities zhu et al 2020 developed an improved lstm model applied it to predict the daily streamflow at four stations in the upper yangtze river and compared the performance against three benchmark models ann model generalized linear model and heteroscedastic gaussian process model the results showed that the improved lstm model has satisfying performance compared to the benchmark models for high flow forecasting zolfaghari and golabi 2021 compared a few dt models the lstm model and six other benchmark models to predict the hydropower generation of the mahabad dam in iran and concluded that the lstm model is one of the emerging ai dm models and has great potential in assisting reservoir operation and hydropower scheduling the major advantage of the lstm model is its capability of taking sequential data as inputs instead of independent training samples this feature benefits to model s capability of dealing with more extended historic hydrologic observations with temporal dependence wu et al 2020 which is a common feature associated with many types of hydrological time series however the lstm model also has some limitations inherited from the black boxed model which is the lack of explicit internal representation of the water balance kratzert et al 2018 moreover the training of lstm could also be computationally expensive in some cases since the model trains on data sequence with user defined time steps the longer the time steps or the data sequence the more expensive the computation will be nonetheless many recent studies demonstrated a strong potential of the lstm model in hydrological time series forecasting and reservoir operation bai et al 2021 sahoo et al 2019 xiang et al 2020 with respect to the above literature review though ai dm models are becoming more and more popular in the field of reservoir operation the performance of different models still vary and each model has its own strengths and weakness large scale comparison is critically needed to comprehensively evaluate the performance of various ai dm models as well as to prevent ai dm models from becoming an alchemy hutson 2018 in the hydrology and water resources management research community it is essential to establish a standard evaluation testbed that includes a large number of case studies at least 10 to identify whether the ai dm models and the commonly employed parameters are transferable from one case to another and to verify the model s predictive performance is stable and reliable across different reservoir cases such a comparison study can further verify the usefulness of ai dm models in assisting reservoir operation and help decision makers to rebuild the confidence for future uses of ai dm models in practice however as far as the authors knowledge such a study that compares multiple methods over a large number of study cases for reservoir operation is rare in the literature most of the existing studies only compare two or three ai dm models in a limited number of studies without further examining the transferability of their proposed models therefore in this study we apply a total of 12 popular ai dm models which consist of two linear models two svm models two knn models three dt models two ann models and one deep learning model to simulate the daily reservoir releases over 33 study cases over the upper colorado region in the u s in order to comprehensively examine the models performance we compared the simulated release time series with observation via six commonly accepted statistical metrics under three different input scenarios through this study we want to answer the questions that 1 whether these ai dm models could reasonably mimic the human s release decisions using the limited information of historical reservoir inflow and storage time series 2 which model performs the best or worse on what conditions and evaluation criterion 3 can the employed ai dm models generate reliable and stable predictions across different study cases and 4 whether a model is transferable to other untested cases with high statistical confidence the experiment setting and findings of this study will also provide a technical reference of model evaluation and operation guidance on the topic of applying ai dm methods to reservoir operation for interested researchers and operators the rest of the paper is organized as follows section 2 summarizes the applied ai dm models and employed statistical measures the data and experiment settings are described in section 3 in section 4 we demonstrate the experiment results the discussion and conclusion are provided in sections 5 and 6 respectively the supplementary material includes all the calculated statistics of each reservoir under different scenarios and the detailed model setting and parameters the main body of the paper provides further analysis and a summary of the obtained simulation results 2 methodology in this study we applied a total of twelve ai dm models with different model complexity and parameterizations the twelve models could be categorized into six groups based on the data classification and regression mechanism used 1 the first modeling group includes a basic multi variable linear regression model and a second linear model termed the ridge regression model hoerl and kennard 1970 marquardt and snee 1975 both of them belong to the most basic linear model and the simplest regression model set 2 the second model set consists of two types of supportive vector regression svr models boser et al 1992 cortes and vapnik 1995 vapnik 2013 vapnik 1999 one svr model is implemented with the radial basis function kernel svr rbf and another uses the polynomial function kernel svr poly 3 the third model group consists of two k nearest neighbors knn regression models altman 1992 coomans and massart 1982 cover and hart 1967 with the number of neighbors being 3 and 10 knn 3 and knn 10 respectively 4 the fourth model group belongs to the decision tree dt based models breiman et al 1984 mingers 1989 quinlan 1986 there are three dt models being tested in this study namely the classification and regression tree cart breiman et al 1984 the random forest rf algorithm breiman 2001 and the extreme gradient boosting tree xgboost model chen and guestrin 2016 5 the fifth group includes two multiple layer perceptron mlp models jain et al 1996 implemented with the hyperbolic tangent activation function mlp tanh and the polynomial activation function mlp poly last 6 the sixth model group contains one popular deep learning model i e the long short term memory lstm model hochreiter and schmidhuber 1997 in the following sections we briefly introduce these employed ai dm models for conciseness we only summarize the key concept used in each model the detailed mathematical definition is summarized in supplementary material section 1 for interested readers 2 1 linear model linear regression is one of the most widely used mathematical techniques to predict a target variable or vector y i based on the values of a set of dependent variables or called feature vector x i kutner et al 2005 makridakis et al 2008 montgomery et al 2012 a linear relationship is assumed to be existing between the dependent variable and the target makridakis et al 2008 montgomery et al 2012 the coefficient vector β n could be estimated by fitting a linear line between the historical observations of x n i and y i in the context of machine learning the similar process is called model training once the values of the coefficient vector β n being identified from the training process the same set of β n could be used to predict the values of the target variables using a new data point x n i which is from either in a future phase or a testing dataset that the training process never uses marill 2004 seber and lee 2012 specifically for linear regression models we have the following eq 1 1 y i β 0 β 1 x 1 i β 2 x 2 i β n x n i ε where y i is the i th observation of the dependent variable x 1 i x 2 i x n i are respectively the i th observation of the independent variables x 1 x 2 x n n is the number of variables β 0 β 1 β n are the model parameters which are also called regression coefficients ε is the error term assumed to be normally distributed with zero mean and variance σ 2 the linear ridge regression is a modified model of the basic linear regression model with an l2 norm penalty regularization term hoerl and kennard 1970 marquardt and snee 1975 the penalty term intentionally shrinks and controls the regression coefficient of the linear model to avoid the poorly determined and high variance coefficient problems the process of introducing the penalty term to a regression model is called regularization the goal of regularization is to improve the conditioning of the prediction problem i e overfitting or underfitting and essentially to reduce the prediction variance when the input variables have some levels of dependency or the training dataset is biased adding an l2 norm penalty term onto the linear regression model eq 1 could effectively constrain the values of the coefficient vector and control the bias variance trade off hastie et al 2009 myers and myers 1990 hence the linear ridge regression algorithm may build a model with a fewer number of parameters than the simple linear multivariable regression model and it is found to be less sensitive and less overfitting than the regular linear model lima and lall 2010a yu and liong 2007 the procedure of building in the l 2 panelty term into the standard multi variable linear regression model is presented in supplementary material section 1 1 2 2 support vector machine svm and support vector regression svr the support vector machine svm is a supervised machine learning algorithm which was introduced as a statistical learning algorithm for complicated data classification and regression boser et al 1992 cortes and vapnik 1995 vapnik 2013 vapnik 1999 based on the purpose of use the svm can be further categorized into the support vector classification svc and support vector regression svr the key concept of svm is to identify and search for a hyperplane i e an conceptual and high dimensional fitting line which will optimally partition the training data by its feature values the identified hyperplane will become the decision boundary and be further used to predict the continuous output in the regression model comparing to the basic linear models the svm model acknowledges the presence of non linearity in the data i e the input features are nonlinearly correlated making it hard to identify a linear line or plane to effectively separate the input data to address this issue the svm models will first create an n 1 higher dimensional feature space with n equals to the dimension of original input data because in the higher dimensional feature space all input features will be linearly correlated the formulation of a linear separation line i e the hyperplane will be feasible gunn 1998 noble 2006 smola and schölkopf 2004 the identified hyperplane will be transferred back to the original n dimensional space to partition the input data and further used as the regression fitting line to find the optimal hyperplane in the n 1 dimentional space the svm algorithm implements a learning algorithm that provides a globally optimal solution by minimizing the upper bound of the generalization error between the support points deka 2014 haykin and network 2004 hipni et al 2013 in summary the svm algorithm consists of two essential steps 1 to project the input data into a higher dimensional feature space and 2 to find a global optimal hyperplane to split the data by evaluating the offsets of each data point to this hyperplane deka 2014 vapnik 2013 in the svm model framework one of the key parameters is the selection of the kernel function a kernel function is used to transform the inputs into a required dot product format as well as to describe how input features are linearly separated in the n 1 dimensional feature space in this study we tested two common kernel functions namely the radial basis function kernel and the polynomial kernel function following the prior suggestions from the study in applying svr on hydrological time series forecasting and reservoir simulation using the svm models adnan et al 2020 cheng et al 2020 yu et al 2006a zhang et al 2018 1 polynomial kernel function k x i x j γ x i x j c d 2 radial basis kernel function k x i x j e x p γ x i x j 2 where γ is the structural parameter in the polynomial function c is the residuals and d is the degree of the polynomial term x i and x j represents the data in the original data space and the transformed n 1 dimensional space for conciseness the detailed mathematical derivation and representation of the svm model are summarized in the supplementary material section 1 2 for interested readers 2 3 k nearest neighbors knn regression model the knn model is an unsupervised machine learning approach which was originally introduced by fix 1951 the knn algorithm was further developed by many others altman 1992 coomans and massart 1982 cover and hart 1967 over the years the knn model has become a simple but effective method for solving both classification and regression problems in a variety of research fields the key procedure in the knn model is to group the training samples into multiple classes with the same user predefined size k or the number of data samples in each class the data separation process follows the rule that the k data points in each class will be the closest with respect to any other new data point in the training sample in other words the knn algorithm partitions the data based on the closeness or the shortest distance among the proximity of a total of k samples bhatia 2010 cunningham and delany 2020 the distance can in general be any metric measure while the standard euclidean distance is the most common choice imandoust and bolandraftar 2013 the number of samples k can be a user defined constant or vary based on the local density of points radius based neighbor learning in this study we set the number of samples in the nearest neighbors as 3 and 10 as knn 3 and knn 10 respectively also see supplementary material section 1 3 some initial test was carried out to identify the values of this parameter that the number of nearest neighbors beyond 10 did not improve and sometimes deteriorate the model performance more discussion will be provided in later sections about the choices of ai dm model parameterization and sensitivity analysis 2 4 decision tree dt based models cart rf and xgboost tree model the decision tree dt model is a non parametric white box statistical learning approach breiman et al 1984 mingers 1989 quinlan 1986 that uses a tree structured classifier to recursively partition the training dataset into smaller subsets i e nodes following identified splitting rules when splitting the data into smaller subsets each data partition will primarily follow a simple true or false boolean logic i e whether the value of a feature or data greater or smaller than a threshold freund and mason 1999 rokach and maimon 2005 by repeatedly partitioning the data into smaller subsets the final classes will only contain the samples that are distinguishable from other subsets of data based on a sequence of characteristics in the feature space buntine and niblett 1992 the major advantage of the dt is that it follows a highly intuitive and efficient splitting rule where decision makers can easily see the logic of interpreting the data james et al 2013 krishnan et al 1999 yang et al 2020b furthermore the establishment of splitting rules to partition the data or the so called tree growing process requires less effort for data preparation and pre processing in other words there is no need to make strict assumptions about the distribution of data or the raw value scales of the training data the tree based models are also found suitable for dealing with unbalanced data classification and regression ganganwar 2012 ke et al 2017 pradhan 2013 however there are still some disadvantages of the dt model for example over and under fitting can occur on datasets with a small number of samples or the training samples value becomes too sparse the dt model could result in a lack of robustness and low accuracy for unseen data brodley and utgoff 1995 dietterich 1995 mingers 1989 to deal with those problems in dt models primarily two enhancement strategies were developed i e the bagging and boosting techniques in this study we employed three dts models namely the classical classification and regression tree cart breiman et al 1984 the random forest rf algorithm breiman 2001 and the extreme gradient boosting xgboost tree algorithm chen and guestrin 2016 the cart model is one of the early and classical dt models while the rf and xgboost algorithms are two newer dt models which incorporated with the bagging and boosting enhancement techniques respectively the classification and regression tree cart was originally introduced by breiman et al 1984 it divides the data items into homogenous subsets using binary recursive partitions aiming to classify datasets and has been proven as a powerful tool for both classification and prediction problems de ath and fabricius 2000 loh 2014 steinberg and colla 2009 the cart algorithm provides a base regression model for the later developments of both rf and xgboost algorithms the mathematical description of the cart algorithm its tree growing process and splitting criteria are briefly introduced in the supplementary material section 1 4 which follows the summary from hastie et al 2009 and the original development from breiman et al 1984 the random forest rf is an ensemble learning algorithm introduced by breiman 2001 the key concept used in the rf algorithm is to build and combine multiple candidates of the standard cart models based on a bagging strategy to avoid overfitting and or underfitting liaw and wiener 2002 specifically the rf algorithm starts with generating several bootstrapped samples of input features from a given training dataset then different cart models are trained on each of the bootstrap samples during this process both strong and weak learners are presented finally the model output can be achieved by aggregating the outputs from all the candidate cart models which are individually built from the bootstrapped samples efron 1987 1992 johnson 2001 the ensemble strategy used in the rf algorithm ensures the prediction model contains both weak learners and strong learners from the original training dataset by assembling multiple candidates cart models the final ensemble model will be more robust and less overfitting than the single cart model brokamp et al 2017 mathematically let s assume that there are e number of trees and their model output is y i i 1 2 e then the final output of the rf regression can be obtained by averaging the outputs of all the trees as presented in eq 2 2 y 1 e i 1 e y i where y i is the prediction results from each ith cart model trained on different bootstrapped data and input features e is a user defined parameter and represents the total number of ensemble candidate cart models to be used as ensemble candidates and y is the final ensemble model of the rf algorithm the detailed model parameters used in the rf algorithm are summarized in supplementary material section 1 5 in contrast to the rf and cart algorithm the xgboost tree model is another recently developed dt algorithm which was introduced by chen and guestrin 2016 the xgboost tree algorithm was based on the greedy gradient boosting framework for parallel tree boosting friedman 2001 2002 but was enhanced by a novel tree learning algorithm is for handling sparse data and a theoretically justified weighted quantile sketch procedure to enable the handling of instance weights chen and guestrin 2016 like the rf model the xgboost model uses an ensemble of cart however unlike the rf algorithm that develops ensemble candidates on bootstrapped training data and features the xgboost model is further designed to boost the performance of weak learners by performing additive training strategies boutaba et al 2018 ke et al 2017 the gradient boosting approach allows new models to be trained to predict the residuals i e errors of prior models in other words instead of training multiple models in isolation of one another the gradient boosting strategy trains models in succession the new model will be trained to correct the errors made by the previously trained models the boosted models are then added together sequentially until no further improvements could be made eventually by developing an ensemble of the boosted model candidates the model s predictive performance will be improved as the performance of all the candidate ensemble models have been boosted by correcting the residuals the mathematical derivation of gradient boosting tree from a based cart algorithm is briefly introduced in our supplementary material section 1 6 and interested readers shall also refer to the original development research from chen and guestrin 2016 2 5 multiple layer perceptron mlp models or artificial neural network ann the multiple layer perceptron mlp model or artificial neural networks ann model hoskins and himmelblau 1988 mcculloch and pitts 1943 is one of the most widely used machine learning algorithms in many fields the development of the mlp or ann model was inspired by the biological neural network of the human brain jain et al 1996 the key concept of ann is to build a network like model structure with the goal of finding the intrinsic patterns or relationships in a given dataset through a learning or training process called backpropagation asce 2000 goh 1995 hecht nielsen 1992 in general an ann model consists of three types of basic layers e g the input hidden and output layers and the nodes between these layers are interconnected by tunable connection weight parameters the input layer takes either the raw or normalized training data and the value of one particular node in the input layer is weighted and passed to a successor node in the hidden layer the corresponding node in the hidden layer will process the weighted sum value and information obtained from the input layer within each node in the hidden layer the weighted sum values from prior nodes will be further processed by a pre defined transformation function or called activation function and then becomes the output from the hidden node the same weighting procedure is performed to combine the outputs from hidden layer nodes into the information feed to the nodes in the output layer node if an mpl model has multiple hidden layers the procedures of both weighting and transformation are repeated until the output layer is reached the values of the connection weights are identified by calculating the accumulated errors from all output layers and hidden layers and by minimizing a loss function in the training process the role of the activation function is important as it enables nonlinear statistical modeling with complex data hsu et al 1995 zealand et al 1999 in this study we used two types of activation functions the hyperbolic tangent activation function tanh and the logistic activation function log the tangent and logistic functions for any variable t are defined by eqs 3 and 4 respectively 3 tanh t 2 1 e 2 t 1 4 l o g i s t i c t k 1 c e r x where c is the constant from integration r is the proportionality constant and k is the threshold limit assuming the k c and r all equal to 1 the logistic activation will become the standard sigmoid activation function the detailed mathematical definition of an mlp or ann model and the applied model parameters number of layers number of hidden nodes learning rate regularization term training optimizer maximum iteration numbers etc are introduced in supplementary material section 1 7 2 6 long short term memory lstm model the lstm model is a deep learning algorithm introduced by hochreiter and schmidhuber 1997 it is a particular type of recurrent neural network rnn that is specialized in dealing with time sequential data prediction gers et al 2002 graves and schmidhuber 2005 rumelhart et al 1986 differs from the standard feedforward ann or mlp models the lstm model assumes the training samples are temporally correlated instead of importing continuous data as independent training samples the lstm model trains the time series of both feature and target variables with a user defined time step the key benefit of the lstm model is that it is operated by using memory cells with input output and forget gates which capable of not only learning long term range decencies but also overcoming the gradient vanishing and exploding problems of rnn hochreiter 1998 hochreiter et al 2001 the basic structure of lstm consists of input hidden and output layers similar to the mlp model and it has an unfolded structure due to the recurrent connections of hidden states in the hidden layer the mathematical description of the internal operations of a memory cell in lstm can be briefly described in gers et al 1999 and yu et al 2019 and summarized in supplementary material section 1 8 2 7 statistical measures in this study we employed six different statistical measurements based on the suggestions from moriasi et al 2007 the employed statistical measures include the correlation coefficient corr the root mean square error rmse the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 the kling gupta efficiency kge gupta et al 2009 the rmse observation standard deviation ratio rsr and the percentage of biases pbias the equations value range and optimal values are listed in table 1 in table 1 the q o b s i and q s i m i are the observed and simulated reservoir daily discharges at the time step t respectively the q o b s i and q s i m i represent the mean of the observed and simulated values respectively the variable n is the total number of time steps in the compared time series in the kge calculation μs and σs represent the mean and standard deviation of the simulated discharges and μo and σo are the mean and standard deviation of the observed hydropower releases respectively the selection of these statistical measures is based on the commonly accepted standards of streamflow simulation and model evaluation in the field of hydrology specifically the measurements of corr rmse and nse are widely used statistical measures to quantify how the simulated streamflow matches the observed streamflow corr measures how a simulated time series will vary in corresponding to observation rmse quantifies the accumulated biases between the simulation and observation and it is a similar measure of the mean absolute error mae with relatively higher sensitivity due to the mathematical operation of taking the square of the error term nse is a combined statistic of both rmse and corr in which both the bias and temporal variation will affect the nse value in addition according to gupta et al 2009 the kling gupta efficiency kge was developed amending some shortcomings of the nse measurements the kge measurement was able to decompose the nse values into linear correlation bias and variability components between simulated and observed time series and it is thusly able to analyze the relative importance of each of the terms that contribute to the nse index according to moriasi et al 2007 and singh et al 2005 the rmse observation standard deviation ratio rsr standardizes rmse using the observations standard deviation and it combines both an error index and the additional information legates and mccabe 1999 according to moriasi et al 2007 and gupta et al 2009 the rmse and rsr with a value equal to zero and or the nse or kge value of 1 is the indication of the best accuracy between a model simulated time series and the observation the larger the rmse and rsr value or the smaller the nse and kge value the poorer performance a model is some reference ranges for the nse and kge values are summarized below gupta et al 2009 moriasi et al 2007 unsatisfactory nse 0 4 acceptable 0 40 nse kge 0 50 satisfactory 0 50 nse kge 0 65 good 0 65 nse kge 0 75 and very good 0 75 nse kge 1 00 the variable pbias quantifies the percentage of biases between a simulated time series and the reference positive and negative values indicate underestimation and overestimation relative to the measured data respectively while values of 0 are desired satisfactory values of pbias vary for different constituents and must consider the level of measurement uncertainty the higher the pbias value either towards positive or negative the worser the model accuracy besides the tabular and numerical statistical measurements in this study we also employed a graphical model evaluation tool termed the taylor diagram taylor 2001 the taylor diagram provides a way of showing how three complementary model performance statistics i e the correlation coefficient r the standard deviation sigma and the centered root mean square error simultaneously in one single 2 d graph according to taylor 2001 the plotting of multiple statistics in one graph is based on the geometric relationship cosine law between the correlation coefficient r the centered root mean square error rmse and the standard deviations between the simulation and observation the taylor diagram has been extensively used in climate model studies miao et al 2014 tao et al 2018 yang et al 2018 as well as the topic of ai dm model evaluation and comparison in the field of hydrology adnan et al 2021 kargar et al 2020 shabani et al 2020 3 data and experiments setting 3 1 study cases the upper colorado river basin is comprised of four states including colorado new mexico utah and wyoming collectively the upper colorado basin contributes a majority of the fresh surface water supplies coming into the entire colorado river basin primarily through winter snowpack and streamflow with the impacts of climate change altering the amount of snowpack and timing of spring runoff water supply in the colorado river is increasingly strained the reservoir systems in the upper colorado region plays an inevitably crucial role in managing the surface water for multiple uses such as flood control hydropower creation ski boating fishing as well as direct water supplies to residential industrial and irrigation over the states of colorado new mexico utah and wyoming the colorado river provides water to nearly 40 million people and drives a 1 4 trillion economy climate change and increasing water demand due to an expanding population is and will continue to present significant challenges if left unaddressed the varying weather and climate will impact our regional and national economies degrade the environment challenge our agricultural heritage and food production and limit recreational opportunities from fishing and boating to skiing therefore the focus on the upper colorado basin reflects the importance of both regional and national social economic benefits the upper colorado region consists of complex terrains and is prone to changing climate of precipitation snowpack and temperature the refills of the reservoir system over upper colorado primarily are from the spring snowpack melting and direct streamflow from rainfall runoff hydrology this setting is typical in many other regions where reservoir plays the role of changing the timing and amount of water flowing to downstream regions as mentioned in the introduction the development of advanced management and decision making tools will significantly promote our capability in controlling the surface water resources and better enable us to mitigate the potential impacts of climate the upper colorado region itself serves as an ideal region that the impacts of climate and extreme rainfall snows could easily reflect in the reservoir inflows variability based on the above two reasons the reservoir system within the upper colorado river basin is selected for this comparison study of different ai dm models in this research we choose 33 reservoirs in the upper colorado region under the jurisdiction of the u s bureau of reclamation usbr the following table 2 lists the short name full name locations and data lengths employed in this research the locations of the selected reservoirs are presented in fig 1 we choose these reservoirs based on the criteria that 1 the reservoir shall have a complete set of data records for reservoir inflow storage and outflow at a daily time step and 2 the data records are continuous without significant missing data over ten days the reservoir inflow storage and outflow data are obtained from the usbr water operation archive https www usbr gov rsvrwater historicalapp html in this data repository some reservoirs among the 33 selected ones have an earlier start date and longer data record however in our initial data screening we found some data were either missing or unavailable at earlier records and the numbers of missing data are significant therefore in the experiments we manually checked the data values for each reservoir and selected the start data when all daily inflow storage and outflow data are continuously available from that year in other words the starting date for each reservoir is different table 2 while the ending date for all simulations is set consistently as december 31st 2020 among the selected 33 reservoir cases the shortest data record is about 10 years and the longest data record is about 50 years table 2 also lists the elevation of each reservoir which ranges from 1323 to 2847 m above sea level 3 2 experiment setting in this study we use the daily inflow storage and seasonality months as model inputs to estimate the daily reservoir outflow according to several prior studies on the topic of reservoir time series analysis hejazi et al 2008 zhao et al 2012 the one and two time step delayed reservoir inflow and storage information have strong correlations to the current time step reservoir outflow following their findings we designed three different input scenarios to drive the ai dm models and our experiment design which is shown in the following fig 2 in the first designed simulation scenario s1 we use the current inflow storage and seasonality months as the default inputs to ai dm models in the second simulation scenario no 2 s2 the model input categories further extend to cover the 1 step 1 day delayed information along with the default model inputs under s1 in the last and third scenario s3 we design the ai dm model inputs to further include both the one and two step delayed inflow storage and seasonality as well as the default input set defined under s1 in other words the complexity and number of inputs to the ai dm models are increasing from s1 s2 to s3 we expect the performance of ai dm models will be sensitive to the input training time series being used and the differences will be analyzed in the later result section in all of the performed experiments 80 of the data is used to train the ai dm models and reminder 20 of the data record is used as validation the underline assumption is that the employed ai dm models will be trained on a subset of the entire data and be tested on a new subset of data that the model never sees during its training process note that this partition ratio of data 80 20 is consistent across each reservoir case however since each reservoir has different data lengths table 2 the data lengths used in training and validation are different for each reservoir nevertheless for each reservoir the training and validation data used are identical across different ai dm models which ensures the model comparison is fair last but not least when we train individual ai dm models all model inputs and target values are normalized into the range of 1 1 after the model training process and during the validation period the model predicted values are transformed back to the normal range of daily outflows for each reservoir the evaluation and assessment of statistical measurements are conducted using the transformed model predictions which are in the normal range of daily reservoir outflow observations instead of the normalized range of 1 1 most of the model hyperparameters are set prior to the experiments by manual trial and error to avoid overfitting and this is also the reason we did not put a separate third sub dataset as testing the specific model hyperparameters and training settings for each employed ai dm model are listed in supplementary material section 1 for interested readers 4 results 4 1 statistical results of the baseline scenario no 1 s1 in this section we will present the obtained statistics of all reservoirs under the baseline scenario no 1 s1 and then compares the performance across different models and reservoir cases note that all calculated raw statistical measurements between simulated and observed reservoir outflows under all simulation scenarios are presented in the supplementary material tables 1 33 and this section will present a further summary and analysis specifically in the following table 3 we summarize the maximum minimum and average of the statistical measures across all employed ai dm models over the validation periods for each reservoir these maximum minimum and average statistical values are drawn from the raw statistical performance of each individual reservoir supplementary material tables 1 33 across all employed ai dm models in order to obtain an overview of the statistical performance of all reservoirs at the ending rows of table 3 we further take a numerical maximum minimum and average of all corresponding values from the statistical summary of prior rows of table 3 for all reservoirs the results in table 3 indicate how well the employed ai dm methods could capture the variations of daily reservoir releases regardless individual model used the ending bolded rows of table 3 indicate how well the statistical performance between simulation and observation across all employed reservoir cases in general according to table 3 in most of the reservoir cases the employed ai dm models could achieve satisfactory statistical performance over the validation periods as evidenced by the minimum values row for each reservoir the worst corr value e g corr column and min row for each reservoir is 0 215 in the reservoir case of sta followed by the second worst corr value being 0 295 in the case of ste however the average of all of the worst min corr values across all reservoir cases is 0 634 e g corr column and min row at the average section at the end of table 3 the worst min nse value is observed as 0 476 for reservoir nav followed by the second worst nse value being 0 133 in the reservoir case of fgr nevertheless in most of the cases the worst nse values across all reservoir cases are consistently above 0 5 which indicates still a good match between the simulated and observed daily reservoir outflow according to the nse satisfactory category set by moriasi et al 2007 the average of all worst nse values in all employed reservoirs is 0 351 e g nse column and min row at average section at the end table 3 table 3 also indicates the worst kge value is 0 242 for reservoir usr as compared to other cases the average of all of the worst kge across all studied cases is 0 398 referred to the average row at the end of table 3 except for these worst scenario cases in general the statistical measures are rather satisfactory with high corr nse and kge minimum values for each reservoir i e see individual min rows under each reservoir case for pbias the highest absolute bias in the simulated time series appears to be 122 in the reservoir case of usr indicating at least one of the employed ai dm models significantly underestimated the reservoir outflow however in the same case of usr the average pbias value is 25 which means if averaging the performance of all employed ai dm models the prediction bias could be reduced and the high bias of underestimation was only associated with few modeling scenarios when taking a further examination of raw statistics from the supplementary material tables 32 18 and 27 we found out that using svr rbf svr poly mpl tanh model in simulating the outflows from the reservoir of ubr mcp and sco individually the pbias values are 122 843 49 236 49 197 respectively for the last rsr statistical measurement the performance of all employed ai dm models are similar and the worst max value is observed to be 1 215 for the reservoir case of nav followed by the second worst value of 1 001 observed for the reservoir case of sta in general table 3 shows that when simulating the daily reservoir outflow decisions using default reservoir inflow and storage time series though the performance of each ai dm model will vary the averaged statistical performance over all 12 employed models and all 33 reservoir cases are still satisfactory with the following averaged statistical value ranges 0 634 corr 0 853 5 580 rmse unit m3 s 8 195 0 351 nse 0 718 0 398 kge 0 759 18 495 pbias unit 15 357 and 0 506 rsr 0 779 for all the reservoir cases 4 2 reservoir outflow simulation and model taylor diagram in the following figs 3 5 we plot the ai dm model simulated daily reservoir discharge against the observed time series for reservoirs bsr cau cry dcr dil ebr ech ecr fgr fon and gmr in fig 3 hnr hyr jor jvr lcr lem mcp mcr mpr nav and pin in fig 4 and rfr rid roc rue sco sjr sta ste tpr usr and val in fig 5 the data records in figs 3 5 cover the validation periods for each reservoir listed in prior table 2 in figs 3 5 the y axis is daily discharge from the reservoir m3 s and the x axis is the number of days in the validation periods different color lines indicate simulated reservoir outflows by different ai dm models and the observed time series are plotted with dotted black lines aside from each subplot in figs 3 5 we also draw the taylor diagram showing the model performance by color dots in all the taylor diagrams the observation is located at the bottom axis the closer of the color dots to the observation the better performance in the context of the pearson correlation coefficient the root mean square error rmse error and the standard deviation by geometric cosine law taylor 2001 according to figs 3 5 the variation of daily reservoir releases could be well captured with many ai dm models though the simulated reservoir outflows may deviate from observations from one simulation period to another among all studied reservoirs under s1 the performance of ai dm models could also vary from one case to another in the prior statistical result table 3 we identify the reservoir cases fgr sta ste nav and usr which are contributing to the worst scenarios of different statistical measurements in this section we further analyze the results for these cases along with the time series plots and taylor diagrams presented in figs 3 5 the simulated results for reservoir fgr sta ste nav and usr are presented in fig 3 i fig 5 g fig 5 h fig 4 j and fig 5 j respectively according to the model simulated results of cases sta in fig 5 g we observe that both linear model the ridge model and the lstm model failed to identify the patterns of reservoir releases these models showed significant overestimation over low flow conditions whereas the reservoir release shall be close to zero and demonstrated underestimation over high flow conditions for example the simulated reservoir outflow by the linear ridge and the lstm models are fluctuating around 5 m3 s without capturing the outflow peaks that are above 10 m3 s in contrast the mlp log model could well capture the low flow conditions but exhibit some overestimation in a few peaks in these reservoir cases by examining the detailed models statistical measures in reservoir sta supplementary material table 29 under scenarios no 1 the corr values obtained by the linear model ridge model and the mlp model with logistic activation function are 0 215 0 215 and 0 255 respectively while other models could reach a much higher corr value exceeding 0 540 in most of the employed ai dm models similar behaviors of the linear model ridge model and the mlp log model are also observed in another reservoir case of sco fig 5e for the reservoir case of sco the corr values obtained by the linear model ridge model and mlp log model are 0 295 0 295 and 0 361 respectively under the default scenario no 1 supplementary material table 27 however in other cases such as reservoir ebr fig 3f ecr fig 3h and ste fig 5h the obtained corr values by the mlp log model are consistently and significantly higher than the linear model and ridge model supplementary material tables 6 8 and 30 the non competitive performances of the linear model and ridge models in simulating reservoir outflows are also observed in some other reservoir cases for example the results of bsr fig 3a hnr fig 4a and pin fig 4k respectively in all those mentioned cases we observe that linear and ridge models are performing poorly as compared to other more complex ai dm models this indicates the reservoir release decision is not a simple and linear process with respect to inflow amount and storage volumes the linear assumption may not be valid when using reservoir inflow storage to estimate reservoir outflow and more complex ai dm models are needed when we are analyzing obtained nse values in the prior table 3 we observe that the worst nse value occurred in the reservoir case of nav with the time series plots and taylor diagram of reservoir case nav fig 4j we found that the worst nse value is due to the poor performance associated with the cart model and knn algorithms the time series plots in fig 4 j indicate both the cart model light green line and knn algorithms tend to overestimate the peaks and generate significant overestimations across the validation periods based on the detailed statistics in supplementary material table 21 the obtained nse values for knn 10 knn 3 and cart models are 0 332 0 476 and 0 365 respectively these negative nse values indicate unsatisfactory and poor model predictive performance of these models similar issues are also identified in other cases including the reservoirs ech ecr fgr in fig 3 g h i respectively the reservoirs mpr and pin inf fig 4 c and k respectively and the reservoirs rfr rid sco sta ste and tpr in fig 5 a b e g h i respectively in all the mentioned cases the simulated daily reservoir releases from the knn and cart models have relatively low nse values compared to other ai dm models furthermore in the case of fgr fig 3i we notice the lstm model dark red line fails to predict the low flows of the reservoir releases and the obtained nse value for the lstm model is the lowest 0 133 as compared to other ai dm models supplementary material table 9 similar cases include the reservoir cases of lcr fig 4e pin fig 4k rfr fig 5a sco fig 5e and sta fig 5g in all these identified cases though the lstm model could well capture peak flows the low flows are consistently overestimated the obtained nse values by the lstm model in these reservoirs are also lower than those obtained by other employed ai dm models with respect to the kge value in earlier results we identify among all employed models the worst kge value happens in reservoir usr with a negative kge value of is 0 242 fig 5j according to table 3 another two cases when kge exhibits negative values are reservoir cases of ste and sta we noticed that in reservoir ste and sta both linear model and ridge model perform poorly if excluding the linear models in reservoirs ste and sta other models could consistently derive positive kge values table 3 in reservoir usr fig 5j the two worst models are svr models with radial basis and polynomial kernels which generate kge values of 0 242 and 0 081 respectively supplementary material table 32 in the same case of usr the simulation results derived from other models are consistently higher than 0 444 which belongs to a satisfactory kge value in addition we also notice that in reservoir usr fig 5j the daily reservoir outflows are zeros in most of the time steps and there are only a few peaks within the validation periods the only similar cases are dil fig 3e and mcp fig 4g where most of the daily releases are very small and a few peaks exist across the validation period in the reservoir case of dil fig 3e the two worst performing models are lstm and svr with polynomial kernel function which produces a kge value of 0 584 and 0 608 respectively supplementary material table 5 however the svr model shows an evident overestimation in the later parts of the validation period around data point 1700 fig 3e at this data point from dil many ai dm models significantly overestimated the observation including the lstm model the knn 10 model and the svr poly model among these problematic models the svr poly model tends to perform the worst when compared to others we further noticed that in the reservoir case of mcp fig 4g and according to the obtained kge value over the validation period the two worst performing models are lstm and svr poly the kge values obtained from the simulated outflows from the lstm and svr poly models over the mcp reservoir are 0 275 and 0 372 supplementary material table 18 respectively under simulation scenarios no 1 s1 in summary we infer that if the reservoir presents consistent small daily releases close to zeros for most of the days the lstm and svr poly model seems to be not capable of obtaining satisfactory kge values of the simulated reservoir outflow according to the taylor diagrams presented along with each time series plot in figs 3 5 there are three major observations first it seems that the model simulation results from different types of ai dm models are similar and consistently good for reservoir cases of cau fig 3b and cry fig 3c all the colored dots in the taylor diagrams are nicely clustered and are consistently close to the observation point second for reservoir cases of pin fig 4k and mcr fig 4h the lstm model performs significantly better than other ai dm models and the lstm dot dark red is the closest to the observation point in the corresponding taylor diagrams with apparent superiority however the lstm model turns to be the worst performing model when switching to reservoir cases of mcp fig 4g and rid fig 5b and their corresponding locations in the taylor diagram are further away from the observation as compared to that of other models third in reservoir cases of rfr sco and sta fig 5a e and g respectively the mlp tanh model demonstrates relatively poorer performance than that in other reservoir cases the level of predictive performance of the mlp tanh model is similar and low as it shows with some linear models note that these above findings are specifically based on the taylor diagram results which are mainly drawn from the closeness of model generated color dots towards the observation the inference of models predictive performance may or may not correspond to the findings observed from the tabular statistical results nevertheless the taylor diagram provides an easy integrated and visually direct quantification of the raw model performance in the context of the standard deviation corr and rmse 4 3 model uncertainty and simulation stability in the following fig 6 we present the box plot of the statistics obtained by each ai dm model over the validation periods of all 33 reservoir cases under scenario no 1 note that this figure depicts a different aspect of statistical performance as compared to the statistical summary in the prior table 3 specifically table 3 indicates the ai dm models overall performance of maximum minimum and average for each employed reservoir while fig 6 shows the performance of each employed ai dm model across all 33 reservoir cases and each box plot was made using 33 data points for each corresponding statistical measurement according to fig 6 most of the ai dm models could obtain the interquartile range 25 75 percentiles of 0 6 0 9 0 3 0 8 and 0 2 0 8 for corr nse and kge fig 6a c respectively and 1 5 6 5 15 to 20 and 0 5 8 5 for the normalized rmse pbias and rsr fig 6d f respectively if comparing the statistical performance of all employed models we notice that 1 the tree based models cart rf and xgboost could achieve a higher corr and kge values interquartile range which indicates a better overall performance than other ai dm models across all studied reservoirs and 2 the linear models ridge and linear generally produce the poorest interquartile ranges as compared to others with respect to the statistics of corr nse kge and rsr the lstm model shows as a competitive model as compared to others in most of the reservoir cases however we also notice the ranges of interquartile obtained by the lstm model are relatively larger than other ai dm models for the statistic metrics of corr nse kge rsr and pbias this observation means that the lstm model s performance are less stable than other ai dm models though in some cases it can obtain a superior performance under the input scenario no 1 similar findings are also discussed in the prior section about the taylor diagram in general the lstm model performance indicated in this fig 6 and that in the prior taylor diagrams are in agreement with each other in addition the lstm model also generates the highest pbias interquartile range fig 6e which is not desired in summary the lstm model has less transferability from one reservoir case to another under the baseline input scenario no 1 and it demonstrates low stability when applied in different simulation cases the results in fig 6 also indicate that 1 evaluating the performance of ai dm models needs an examination of a comprehensive set of statistical measurements as the superiority of one particular statistical performance cannot guarantee the same performance in other measurements and 2 any conclusion about the performance of ai dm models on single or few study cases would rather be biased and when changing the study cases the model s performance are likely to vary for example we notice that knn models knn 3 and knn 10 could reach very satisfactory kge interquartile ranges and they are significantly better than the svr poly model under scenario no 1 fig 6c however the simulated outflow from all 33 reservoirs by the svr poly model has a lower bias pbias value closer to zero than all knn models fig 6e similarly though the multiple layer perceptron models mlp log and mlp tanh show superior performance over linear model sets in corr nse and kge statistics fig 6a c the overall performance of reducing rmes and pbias is similar to that of the linear model sets fig 6d e more discussion will be provided in the later section regarding the best performing models under different simulation scenarios and statistical measurements 4 4 statistical results for baseline of scenario no 2 3 s2 s3 instead of presenting the same statistical analysis similar to prior table 4 and figs 3 5 under scenario no 1 s1 in this sub section we focus on comparing the results under scenarios no 2 and 3 s2 and s3 in terms of improvement or deterioration comparing to the baseline s1 all calculated raw statistical measurements under scenarios no 2 and 3 are also available in supplementary material tables 1 33 and this section is intended to highlight the performance changes of the employed ai dm models when switching the input from baseline to more complex scenarios in the following table 4 we summarize the averaged statistical improvement percentages for each reservoir under s2 and s3 note that the improvement percentages in table 4 are averaged across all employed ai dm models for each reservoir in another set of results table 5 we carried out a similar comparison and obtained the averaged statistical improvement percentages for each employed ai dm model differs from the results in table 4 the improvement percentages shown in table 5 are averaged across all 33 reservoir cases and the percentages are organized for each ai dm model in other words table 4 indicates how well the simulation of each reservoir could be improved by adding delayed inputs regardless of the employed ai dm methods while table 5 reveals how sensitive each ai dm model will respond to the additional inflow and storage information across different reservoir cases in both tables 4 and 5 a positive percentage implies an improvement with respect to the baseline scenario no 1 and a negative percentage means the statistical performance deteriorates as compared to the baseline s1 according to the results shown in tables 4 and 5 there is a consistent observation that when delayed inflow and storage are added to the employed ai dm models the statistical performance will improve in general specifically according to the s2 results in table 4 the corr rmse nse kge pbias and rse values will improve 4 8 10 9 14 7 8 2 90 8 and 10 9 respectively when averaging across all reservoir cases if the ai dm model inputs are further extended to include the two step delayed inflow and storage values the averaged model performance for all studied reservoir could further improve up to 7 6 16 5 28 12 8 96 1 16 5 and 26 4 for corr rmse nse kge pbias and rse table 4 last row respectively according to tables 4 and 5 we observe that the additional inflow and storage inputs with delayed time series will consistently improve the outflow simulation accuracy except for the reservoir sco according to the averaged improvement percentages shown in the last two columns of table 4 we can conclude that in most of the reservoir cases the statistical performance under s3 are better than that under both s1 and s2 though the magnitudes of improvement could slightly vary from one case to another according to the results shown in table 5 when averaging the statistical improvement percentages across all studied reservoirs most of the ai dm models show similar improvement patterns as delayed reservoir inflow and storage information being added as additional model inputs however we notice that the predictive performance of the lstm model may either deteriorate or improve when adding delayed information table 5 which is subject to the study case and employed statistical measurement in other words there is no consistent improvement or deterioration of the lstm model as we observed from other ai dm models when switching from s1 to s2 and to s3 specifically when using the lstm model the rmse and pbias values have been improved however the corr nse and kge values show some levels of deterioration in comparison the linear model ridge model knn models and tree based models demonstrate consistent improvements when adding delayed information we suspect this is because the lstm model by design could take the current inflow and storage values as well as its prior values as model inputs to estimate the outflow at the current time step therefore manually setting delayed inflow and storage volumes as additional lstm inputs did not essentially promote the model accuracy other employed ai dm models do not carry the same feature as the lstm model and the delayed information is associated with complementary predictability than the original model inputs in the baseline s1 furthermore according to the results in table 5 the corr rmse nse kge pbias and rse values are improved by 5 8 10 5 12 7 8 8 91 1 and 10 5 respectively for all models under scenario no 2 the same statistical measurement values are further improved by 8 9 16 6 22 8 13 5 104 8 and 16 6 respectively when switching the model simulation to s3 according to the last two columns in table 5 similar patterns are observed when comparing the percentage improvements for all statistical measures under s2 and s3 beyond the statistical comparison among different scenarios in the following table 6 we further summarize the best and the second best performing models for each statistic over the validation periods the best performing models are identified by comparing how many reservoir cases that a particular model can outperform other models on each individual statistical measurement for example according to table 6 under the baseline scenario no 1 both rf and lstm model could generate the best corr values over the validation period for 10 out of the 33 employed reservoir cases therefore rf and lstm models are identified as the best performing models under s1 and for the corr category the second best model under the same statistic measure e g s1 and corr is the svr model with radial basis kernel function svr rbf which outperforms 4 out of 33 studied cases using the same logic we summarize the best and the second best performing models as well as the number of reservoirs that the corresponding model prevailed other employed ai dm models in the following table 6 according to the results in table 6 both rf and lstm models are identified as the best and second best performing models under scenario no 1 s1 respectively they prevailed the highest and second highest numbers of reservoirs than other employed models this result indicates that both rf and lstm models are more reliable and transferable than other models when simulating reservoir outflow using the current time step inflow and storage volume as model inputs the performance of the rf and lstm model are consistent across different statistical measurements except in two cases when the svr rfb and knn 3 model outperformed 4 and 6 reservoirs out of 33 studied cases and ranked as the second best performing model under s1 respectively however when comparing the number of prevailing cases under s2 and s3 both the rf and lstm model will no longer perform as the best and second best models instead the mlp model with hyperbolic tangent activation function e g mlp tanh becomes the best performing model and the xgboost tree model becomes the second best performing model across different statistical measurements the mlp tanh model shows slightly better performance over the xgboost tree model for statistics corr nse kge and rsr while the xgboost tree algorithm outperforms the mlp model in kge measurement under both s2 and s3 this result indicates that the performance of both mlp and xgboost tree models are stable and consistent across different statistical measurements when additional delayed information to simulate reservoir outflows in addition the total number of reservoirs that the mlp tanh model prevails other ai dm models slightly increased from 10 11 11 and 11 for statistics corr nse kge and rsr under s2 to 12 12 12 and 12 under the s3 respectively similarly the total number of reservoirs that the xgboost tree model outperforms others also increases from 7 7 7 and 7 for the same statistics under s2 to the values of 8 9 9 and 9 under s3 this result implies that the reliability of the mlp tanh model and xgboost tree model is slightly improved with the increases of input complexity combined with the findings from earlier tables 4 and 5 we further infer that these two models are capable of handling additional delayed information and can derive consistently good statistical performance with continuous improvements over the baseline input set under scenario no 1 the results in table 6 also show that the svr model can generate the best pbias values that are closer to zero than other ai dm models for 7 out of 33 studied cases under s2 and prevails other models for 7 out of 33 reservoirs under s3 respectively for the rsr measurement both of the knn and svr models sometimes could outperform other models and succeed in the largest number of reservoirs under different input scenarios of s1 3 however none of them could obtain better statistics than the mpl tanh the lstm the rf and the xgboost tree model with respect to all other commonly used statistical measurements corr rmse nse and kge 5 discussion for most of the hydrological time series it is likely that the water flow either natural flow or controlled flow e g the reservoir outflow is continuous and the time series inevitably have a certain level of autocorrelation with respect to previous time steps one of the advantages of ai dm models is the flexibility of handling additional inputs for data classification and prediction echoing the presented simulation results in section 4 we can reasonably expect that when adding delayed inflows and storage volumes in previous days the ai dm model could better capture the variability of reservoir outflows at the current time step similar correlation studies were available in the literature that the reservoir outflow decisions are also related to previous inflow and storage conditions up to two days in a retrospective manner hejazi et al 2008 zhao et al 2012 specifically our designed scenarios no 2 and 3 experiments fig 2 will answer the question of whether the employed ai dm models with additional and delayed inflow and storage could achieve better or worse performance over the baseline scenario no 1 the experiments with additional delayed ai dm model inputs s2 and s3 can help us further investigate the flexible uses of different ai dm models in simulating the reservoir outflow decisions the results in table 6 indicate that the lstm and rf model could effectively use limited information from the inflow and storage at the current time step to simulate reservoir outflows by achieving the best statistical measurement values over a larger number of study cases than other ai dm models however when manually adding delayed input information the performance of mlp models and xgboost tree based models could be significantly improved and even outperform the lstm and rf models in the baseline scenario in other words the mlp and xgboost tree models are less sensitive to the training information as compared to other ai dm models as long as the proper information is used to train the model the mlp tanh and xgboost tree models performance stay competitive and stable when switching input scenarios from no 2 to no 3 this finding indicates that the selection of ai dm models should rely on what information being used as predictors during the model training process and what statistical measurements are employed to evaluate the model performance after training in general there is no single model that could consistently outperform other ai dm models with respect to all possible statistical measurements and across all studied reservoir cases the large scale comparison studies in this paper are intended to explore the pros and cons of different ai dm models in the field of hydrology and water resources based on our experiment results and findings ai dm model practitioners shall investigate as many similar cases as possible and include multiple evaluation statistical measurements before putting them into real world application diligent quality control and evaluation study of different ai dm models will help operators comprehensively understand which model works well under what evaluation criteria we believe that it is also highly subjective to simply claim the superiority of one particular ai dm model based on a limited number of statistical measurements as well as draw a conclusion from only a few study cases our current experiments also indicate that though the nature of the problem could be similar e g estimating reservoir outflows using inflow and storage the performance of the ai dm models will likely be significantly different when the training data changes parameter changes and the core regression techniques changes it is also questionable to infer the performance of the ai dm model based on one or a limited number of case studies to a large number of other untested cases without any evaluations most ai dm models predictive performance has uncertainty and the obtained statistical measurements may also have large variations fig 6 among different models the prediction uncertainty and variability are inevitable because the ai dm model purely relies on the training data regardless of the physical dynamics and mass continuity kim et al 2021 there is no guarantee that the success of one or few case studies can become sufficient evidence that the same model functioning and behavior will occur in other study cases the authors would like to make the point clear that though ai dm models are popular and powerful tools in many current studies of hydrological simulation and reservoir operation the pros and cons of each model still remain not fully understood as evidenced in our simulation results figs 3 5 not always one type of ai dm model will consistently perform well in all study cases and therefore large scale experiments such as 1 the one presented in this study that intercompares various models on a large number of case studies or 2 the investigation study to directly compare ai dm models with physical rule based or process based modeling schemes are critically needed before the practical uses of ai dm models in any field due to the limitation of accessing the rule curves of the 33 reservoirs this study only serves as the former type of comparative study a further comparison of the simulated ai dm models against rule curves as well as the examination of how ai dm simulated reservoir release decisions could reasonably meet all types of hard engineering constraints soft operation limits downstream supply and environmental water and hydroelectric power demands and regional ordinances are out of great importance to promote the practical applications of ai dm models furthermore we suspect that the ai dm model performance are also related to the elevations of the dam maximum capacity of the reservoir forebay storage as well as the primary functionalities of each reservoir table 2 for the influence of elevation if taking a detailed analysis on some of the reservoirs with the highest elevations for example tpr fig 5i dil fig 3e sjr fig 5f with elevations of 2847 m 2751 m and 2725 m respectively we notice most ai dm models could produce consistent results with the normalized standard deviation above and higher than 0 5 the circular lines in all taylor diagrams in contrast when it comes to some of the reservoirs with the lowest elevation for instance ebr fig 3f pin fig 4k with elevations of 1323 m and 1427 m respectively we observe that results produced by different ai dm models are having a larger discrepancy among each other this is also evidenced by the normalized standard deviation shown in ebr fig 3f and pin fig 4k that the color dots have a larger spread in the taylor diagram than other reservoirs in other words we infer that the reservoirs at lower elevations are more complex to manage due to the influences of water routing in upstream river basins and the possible discharges from reservoirs in higher elevations note that our finding is not contradictory with the conclusion about the impacts of elevation on reservoir discharge simulation from a prior study on reservoirs in california yang et al 2016 specifically the elevations of reservoirs in yang et al 2016 are significantly lower than ours and their studied reservoirs are more closer to water consumption areas such as irrigation and residential districts the reservoirs in our study region are also believed to be dominated by seasonal snowmelt mountainous hydrology and natural streamflow instead of the direct reservoir refills due to the atmospheric river events that bring heavy precipitation over the northern california region regarding the influences of storage capacity we infer that the larger the maximum reservoir storage the more challenging the ai dm models could capture the human s discharge decisions and vice versa this speculation is drawn based on the statistical performance of the following four reservoirs the reservoir cases of fgr and ebr with maximum storage capacities of 4673 532 million m3 and 2547 149 million m3 table 2 respectively which are the two largest reservoirs in this study in contrast the two smallest reservoirs are also analyzed i e hnr and cau with storage capacities of 6 685 million m3 and 9 707 million m3 table 2 respectively according to table 3 the averaged nse values derived by all ai dm models for reservoir case fgr ebr hnr and cau are 0 235 0 496 0 690 and 0 918 respectively the former two i e the largest two reservoirs are significantly lower than that from the latter two i e the smallest two reservoirs the same observation occurs in the kge evaluation in which the averaged kge values across all ai dm models for reservoir case fgr ebr hnr and cau are 0 286 0 583 0 607 and 0 877 respectively in addition we further noticed in the two largest reservoirs fgr and ebr at least one ai dm model performed poorly which is shown in the min sub row under each reservoir row in table 3 for nse the lowest values for fgr and ebr are 0 133 and 0 006 respectively the corresponding worst performing model for the two smallest reservoirs hnr and cau can produce an nse value of 0 256 and 0 885 respectively a similar pattern is also observed in other statistical measures summarized in table 3 with this understanding it is likely that larger reservoirs are associated with more complex constraints natural environmental variabilities and operating criteria which conjunctively place a higher challenge for the applications of ai dm models on the contrary smaller reservoirs tend to be easy to be managed and are more flexible in adjusting to variations of reservoir inflows and changing reservoir storage volumes therefore different ai dm models have stronger advantages in capturing the human s decision making process and effectively simulate the reservoir outflows than the applications on reservoirs with larger maximum capacities in addition reservoir functionality and its primary purpose also play an important role in defining the complexity of the decision making process both of the simulated and observed releases show a certain pattern in our studied cases from the obtained results figs 3 5 we observe that most of the flood control reservoirs table 2 have a distinct seasonal variation in the controlled reservoir outflows examples include the reservoir cases of fon fig 3j fgr fig 3i gmr fig 3k val fig 5k etc among these cases there is a clear periodic cycle repeating itself from one year to another throughout the validation periods most of the employed ai dm models are able to well capture these changes and identify that this is due to the operation to empty reservoir storage for the required flood control rooms during fall winter time before the spring snowmelt for reservoirs with the functionality of hydropower generation the water diverted to the powerhouse is also included in the total outflow discharge simulation and observation however we notice in three of the reservoirs with hydropower functionalities i e cry fig 3c fgr fig 3i mpr fig 4j the observed and simulated reservoir outflows show many non smooth variations at low flow regimes this may be due to the corresponding hydropower generation during peak hours or intermittent hydroelectric supplies to meet the energy demands or the provision of spinning reserves in the power grid all of these are the benefits of hydroelectric power and reservoir systems which could be flexibly turned on and off quickly and effectively to stabilize the power grid variations ding et al 2021 one drawback of our current study is that the total number of reservoirs employed is still limited across the u s and worldwide there is a variety of reservoirs with different functionalities length of data records different climate and weather conditions physical settings of how water being controlled and releases and the sources of reservoir inflows e g either generated from snowmelt or direct runoff from upper watersheds based on specific features of the reservoirs the experiment setting could be very different when applying the ai dm models when designing model inputs such as to include snowmelt rainfall lake evaporation water losses upstream water level or reservoir downstream demand information etc the work presented here may only cover few subsets of reservoir features and there is no possible way to across validate the model performance over an unlimited number of other reservoirs worldwide nevertheless the way of how we apply the ai dm models and test out the model sensitivity and variabilities are universally adaptable to other study cases the employed experiment setting is based on the common knowledge that the reservoir outflow decision is governed by the rule curves and is determined by the water conditions in reservoir forebay afterbay and storage etc given that all existing rule based simulation schemes are more or less built upon this key philosophy we suggest that the uses of ai dm models to assist reservoir operation shall not discard these underlying physics with the help of ai dm models the investigation of whether additional decision factors such as local hydrology global climate interactions anthropogenic influences policy and social economic variables are made possible we highly believe that the ai dm models are and will continue to be a promising tool in advancing the research frontier of the cross disciplinary studies between computer and environmental sciences the usefulness of ai dm tools is not only limited to the topic of reservoir operation but also beneficial to a broader aspect of hydrology meteorology climate and integrated water resources management another limitation in this current study is the lack of a full spectrum of sensitivity analysis on the model s structural parameters and the investigation of available prior post ai enhancement techniques on one hand different ai dm models are associated with various model parameters such as learning rate other activation functions number of hidden layers and hidden nodes for mlp models maximum tree depth maximum for tree based models other kernel functions for svm models other numbers of neighbors in the knn models etc the current study only selects a limited number of default settings when implementing these ai dm models before conducting the outflow simulations for the 33 reservoir cases in this study an initial test has been conducted to pre select the essential model parameters in order to control the level of overfitting for the employed ai dm models however in the presented simulation results we still observe some overfitting or underfitting phenomenon in some of the reservoir cases a full spectrum of model parameter sensitivity tests is highly encouraged before applying a particular ai dm model in practice which is another grant challenge the operator faced when applying ai dm models on the other hand the employed ai dm models in this study are rather the original and standard applications of the popular models in computer science and we are not able to fully examine all possible ai dm models with various customized further developments training algorithms and prior post procedures the number of different variations and successors of popular ai dm algorithms is countless and each version may have its own strengths in a specific application for example fuzzy inference system adnan et al 2019 could be jointly used with mlp models to adaptively improve the predictive performance wavelet theory could also be embedded in ai dm models to refine more representative predictors esmaeilzadeh et al 2017 adding heuristic optimization algorithms could increase the model training accuracy but with a sacrifice of computational speed shamshirband et al 2020 yang et al 2017a yuan et al 2018 and hybrid modeling framework or model ensemble techniques could further reduce the prediction uncertainty of different ai dm models tao et al 2018 yang et al 2020b we suggest future ai dm model enhancements and applications in reservoir operation hydrology and water resources management shall at least include 3 5 standard and original machine learning algorithms with different core regression techniques for the purpose of comprehensive comparison last but not least the performed large scale comparison experiments in this study indicate a good potential of different ai dm models in accurately simulating the controlled reservoir outflows using flexible input designs which are evidenced by the simulated 33 reservoirs outflows and their corresponding statistical performance figs 3 5 tables 3 5 and supplementary material tables 1 33 the flexibilities of ai dm models in taking various human designed input features and identifying the intrinsic relationship between decision and target variables remain as the main advantages of ai dm models many implicit hard to find and internal causal relationships within the natural systems could be discovered and quantified through the proper uses of suitable ai dm models comparison studies especially on applying ai dm models with hydrological and environmental data shall include as many mature statistical measurements as possible to thoroughly examine different aspects of time series i e biases correlation extremes flow regimes seasonality trends etc 6 conclusion in this study a total number of 12 ai dm models with different parameterizations are employed to simulate the daily reservoir outflows of 33 reservoirs over the upper colorado region in the united states we designed three model input scenarios for model training and evaluation using the reservoir inflow storage and their corresponding delayed time series as inputs a total of six commonly accepted statistical metrics were used to quantitatively measure the performance of different ai dm models under three simulation scenarios overall the simulation results are satisfactory and the employed ai dm models could achieve high statistical performance in most of the study cases over the validation periods however the performance of different ai dm models may significantly vary based on the studied reservoir cases the employed regression techniques the statistical measurements evaluated as well as the characteristics of the reservoir i e elevation size and the primary functionality a number of study cases and scenarios were further examined whereas the simulation generated by different ai dm models are with relatively low statistical performance based on our experiments the following specific conclusions are drawn 1 different ai dm models may have individual strengths and weaknesses in simulating the reservoir outflows and assisting reservoir modeling no single model could consistently outperform others in the 33 reservoir cases compared in this study the model s performance is likely to vary by the modeling schemes by the ways of training data structure as well as by the statistical measurement used it is suggested to include multiple statistical measurements and as many ai dm models as possible to comprehensively understand the pros and cons of different ai dm models when applying ai dm models to reservoir simulation 2 the rf and lstm models are found to be the best and second best performing model when using the inflow and storage at the current time step to simulate the reservoir outflows under the baseline scenario no 1 however the mlp model with hyperbolic tangent activation function and the xgboost tree algorithm appears to be the most reliable and stable models when additional delayed inflow and storage are further included in the model training process scenarios no 2 and 3 we infer that the mlp model and xgboost tree model are more capable of handling large and complex training data than other models and the model s predictive performance are found to be more stable than other employed ai dm models in the employed 33 reservoir cases 3 reservoir elevation maximum capacity primary functionality local hydrology could conjunctively affect the effectiveness of ai dm models we found that the reservoirs located at a lower elevation are more challenging to simulate and the model daily outflow simulations tend to disagree with each other when different ai dm models are used this finding is possibly due to the high variabilities in water routing in upstream river basins and the influences of discharge from an upper source reservoir lake in addition we also found out that larger reservoirs are harder for the employed ai dm models to capture the patterns of human controlled release decision and vice versa in our limited study we observe that the statistical performance of ai dm results over the two largest reservoirs are significantly poorer than that of the two smallest reservoirs the functionality of reservoir and its primary purpose could also affect the daily and seasonal variations periodic patterns are observed in both observed and simulated reservoir outflow time series the applications of ai dm models should comprehensively consider these factors in practical uses 4 the delayed information reservoir operation time series could significantly increase the model performance in general according to the comparison between s2 s3 versus the baseline s1 we identify that lstm and random forest model are less sensitive to the manually delayed reservoir storage and inflow time series than many other models but the lstm and random forest model may not outperform other popular and standard ai dm models under the scenarios that additional decision variables are added as model inputs in contrast properly configured mlp i e ann and xgboost models are more likely to be benefited from adding additional model inputs and produce better simulation results based on that we suggest practitioners who use ai dm models to simulate reservoir operation shall test out possible ancillary information and additional model inputs that are closely related to the reservoir decision making process and select the most suitable ai dm models in corresponding to the training data and study cases 5 from this study we also conclude that the advantage of ai dm models lies in their flexibility in incorporating different types of input data and identifying the implicit relationship between features and target variables the traditional rule based modeling scheme might be limited in this regard as the process based governing equation can only take fixed inputs it can be inferred that with different reservoir and problem settings practitioners of ai dm models can identify the most suitable combination of input data structure and model parameterizations to obtain the best possible model outcomes but this process may require a large number of trial and error experiments there are currently not commonly accepted golden rules on how to design the implementations of different ai dm models in assist of reservoir operation at this stage of research and scientific investigation large scale comparison and model evaluation studies are still needed in order to fully understand the pros and cons of different ai dm models and to identify the best way of using ai dm models to help manage the surface water resources understand the local hydrology and water cycle as well as to support the decision making of water infrastructures such as dams and reservoirs 6 future work may include 1 the direct comparison between ai dm models with rule based reservoir simulation and examination of the predicted ai dm time series will meet various types of engineering hard and soft constraints demands and operation goals 2 the development of hybrid modeling schemes that combine the physical rule based model with ai dm model to enhance simulation accuracy and our transcendent capability in the preparedness of possible weather and climate extremes 3 the quantification study of reservoir inflow uncertainty and the corresponding experiments of multiple model ensemble techniques to reduce the uncertainty and discrepancy among the simulation results from different ai dm models 4 incorporate ensemble hydrological forecasts to guide reservoir operation given that one of the key strengths of ai dm model is its structural flexibility that allows the inclusion of various types of forecast information to be added to support decision making some recent studies over the tres marías dam mainardi et al 2016 the yuvacik dam uysal et al 2018 uysal et al 2020 and salto grande dam sinnige and alvarado montero 2019 have demonstrated the combination of ai dm models with model predictive control scheme could significantly improve the reservoir system performance in terms of flood reduction by more than 25 compared to deterministic forecast based techniques there are many innovative and new research domains that the ai dm model could potentially contribute nevertheless the fundamental model comparison and comprehensive evaluation are out of great importance to position the role of ai dm models in assisting reservoir operation and to better reveal the merits of these new technologies in solving any classical engineering and environmental science problems credit authorship contribution statement tiantian yang conceptualization methodology software writing original draft supervision funding acquisition lujun zhang data curation visualization writing review editing formal analysis taereem kim visualization software validation writing review editing yang hong writing review editing di zhang writing review editing qidong peng writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is partially supported by the u s department of energy doe prime award de ia0000018 this work is also financially supported by the national key research and development program of china no 2018yfe0196000 the material is based upon work supported by the national science foundation under grant no oia 1946093 and its subaward no epscor 2020 3 and the national science foundation under grant no nsf1802872 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126723 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4195,in recent years the artificial intelligence and data mining ai dm models have become popular tools in assisting various aspects of reservoir operation however the practical uses are still rarely reported comparison experiment of many ai dm models over a large number of reservoir cases is particularly valuable to help reservoir operators first examine the usefulness and transferability of different ai dm models and then identify the most stable and reliable ai dm model in assist of various decision making processes in this study a total of 12 ai dm models with different parameterizations and simulation scenarios are comprehensively tested out and compared in simulating the controlled reservoir outflows of 33 reservoir cases over the upper colorado region united states results show that the random forecast and the long short term memory model could consistently derive the best statistical performance than other models under the baseline simulation scenario the employed ai dm models could obtain satisfactory statistical interquartile ranges 25 75 between 0 6 0 9 0 3 0 8 and 0 2 0 8 for corr nse and kge measurements respectively and 1 5 6 5 15 to 20 and 0 5 8 5 for the normalized rmse pbias and rsr measurements respectively results also show multi layer perceptron model and extreme gradient boosting tree algorithm produced more stable and superior performance than other models under more complex input scenarios we also found that the performance of different ai dm models are closely relevant to the reservoir elevations sizes and functionalities discussions were made about the sensitivity of ai dm models parameterizations and the key advantages of ai dm models over the rule based reservoir models we further identify that the main advantage of ai dm models is the flexibility in designing input structures whereas the rule based simulation model is rather limited future studies were suggested regarding the best way reservoir operators and researchers could use select and apply different ai dm models in simulating reservoir releases under different natural and modeling environments this comparison study also serves as a reference and a piece of groundwork for further promoting the practical uses of ai dm models in assisting reservoir operation keywords artificial intelligence data mining reservoir operation decision making 1 introduction reservoirs and dams are fundamental human built multi functional water infrastructures that collect store and deliver fresh surface water for a multitude of uses including flood and fire control recreation wildlife habitat residential industrial and agricultural water supply hydro electric power generation supply source during droughts and more the reservoir release decisions directly influence various aspects of social economic functioning and our nation s security yang et al 2016 yang et al 2020b in recent years more frequent and severe abrupt weather extremes climate change natural hazards aging infrastructure and increases in water demands due to population growth have placed another great barrier to prevent effective sustainable and flexible operation for our nations reservoir systems for example in may 2020 due to extended extreme precipitation water behind the two consecutive reservoirs in michigan reached the reservoir storage capacity and caused catastrophic dam breaks flooding the tittabawassee river and completely drained the reservoirs cnn 2020 in 2017 the addicks and barkers reservoirs near the houston area were intentionally operated to release additional water downstream this operation happened during the same period when hurricane harvey hit the houston area causing an additional 8000 houses near these two reservoirs were flooded the federal judge ruled the u s army corps of engineers usace is liable for flooding these homes cnn 2019 houstonchronicle 2019 houstonpublicmedia 2019 in the same year a sudden water flux reaching the designed spillway capacity of the oroville dam california caused irreversible damage to the dam and triggered a large scale emergency evacuation of more than 180 000 people living downstream mercurynews 2017 newyorktimes 2017 these recent dam failure cases indicate the need for continuous developments of effective and flexible reservoir operation tools and modeling schemes the approaches for reservoir operation and decision support can be categorized into optimization models and simulation models labadie 2004 reddy and kumar 2006 yeh 1985 differs from reservoir optimization models reservoir simulation models are useful in assisting operators in estimating reservoir yields and quantifying system behaviors based on pre defined operating rules louks and sigvaldason 1981 in early studies sigvaldson 1976 developed an innovative approach for simulating reservoir responses using a priority ranking concept chaturvedi and srivastava 1981 developed a screen simulation model based on linear programming methods for a large complex water resources system the reservoir simulation models have rapidly evolved from excel sheets based models in early times to coupled hydrological and hydraulic models to support various types of operations such as predicting how the system behaves under the current hydrological situation inflow storage extractions releases on different temporal scales many reservoir simulation models have become operational and widely used in the u s e g the hec 5 model developed by usace bonner 1989 dwrsim developed by cdwr barnes and chung 1986 chung et al 1989 the weap21 model yates et al 2005 the calsim model draper et al 2004 the riverware models zagona et al 2001 and the cram water resources modeling tool lynkertech 2018 etc lund and guzman 1999 concluded that simulation models were more likely to be trusted as a standard by operators as compared to reservoir optimization models in practice these reservoir simulation models rely on so called reservoir operating rule curves which define an empirically desired reservoir storage release relationship louks and sigvaldason 1981 these rule curves are subject to approval by governmental authorities and are defined beforehand usually the formulation of the release rule is based on historical data or design scenarios the set of rules must be defined in such a way that for as many as situations and the operational goals e g power production water supply minimum flow are conservatively met under given constraints e g dam safety requirements flood control requirements environmental obligations an advantage of rule based operations is that the set of rules is usually transparent clear and can easily be integrated into simulation models for the water system however oliveira and loucks 1997 pointed out in many situations the operators will still operate the system in a way that deviates from these pre defined rule curves to adapt to specific conditions objectives or constraints that may change over time draper et al 2004 also criticized that many simulation models were severely restricted by the explicit implementation of operating rules as hard model constraints which jeopardized the flexibility of using such tools to adjust to different environmental settings in other words a drawback of rule based operations is that the control actions are not necessarily the optimal strategy for the current situation and could not cover various conditions from changing environment to give an example a target water level for a reservoir on a specific day in the year would account for both dry and wet situations in order to be able to cope with both water scarcity and flood issues in a dry situation it might be possible to operate the reservoir with a higher target water level this would be beneficial for hydropower production and water supply but is of course only suitable if one can afford to operate with a lower flood control room if on the contrary a substantial increase in inflow is expected the target water level should be even lower than the rules might say in such a situation it will make sense to pre release water in order to generate a sufficient flood control room with respect to these advantages and disadvantage of rule based reservoir operation and simulation models in recent years the artificial intelligence and data mining ai dm techniques become popular tools in assisting reservoir operation and decision making cancelliere et al 2002 chaves and chang 2008 cheng et al 2020 coulibaly et al 2001 coulibaly et al 2000 esmaeilzadeh et al 2017 jain et al 1999 kişi 2007 maier et al 2010 wu et al 2009 yang et al 2017b zhang et al 2019 the ai dm models are powerful tools in data classification and regression but they purely rely on the statistical relationship between the target variables and the input data e g the input features by setting different combinations of model training data and target variables the uses of ai dm models also appear to be versatile for example these models can be used to simulate reservoir release to extract the existing operation rules and to predict reservoir inflows flows and uncertainties and to manage reservoir storage and water levels etc ashaary et al 2015 bessler et al 2003 castelletti et al 2012 castelletti et al 2010 chang et al 2016 cheng et al 2008 rahnamay naeini et al 2020 wei and hsu 2008 however one big challenge of applying these ai dm tools in assisting reservoir operation is the lack of scalability and transferability different ai dm models employ distinct data classification and regression philosophies in which none of them are subject to the mass balance equations with physical constraints this hinders the practitioner from trusting the modeling results especially when operators are used to the traditional rule based simulation models in addition the ai dm model s performance may substantially vary based on user selected model structural parameters and the way different ai dm models are set up in short it is an extremely tedious work for reservoir operators to vet a set of ai dm models identify the most suitable approach and associated parameters for one particular application and carry out verification experiments for another reservoir case or another problem setting to make sure the ai dm models are transferable and scalable under different simulation environment differs from the rule based reservoir simulation models the performance of ai dm models are likely to change when the training data changes furthermore the field of artificial intelligence is still rapidly evolving newer and stronger ai dm models are becoming readily available for applications before an older and simpler model being thoroughly evaluated is assisting of reservoir operation and decision making this makes reservoir operators even more hesitant to trust and practice an alternative ai dm model over their existing and functional rule based reservoir simulation models in the research community there have been numerous studies to apply a variety of ai dm models to assist reservoir operation hydrology and water resources management adnan et al 2019 shabani et al 2020 shamshirband et al 2020 yuan et al 2018 some popular ai dm models include linear regression model support vector machines smv k nearest neighbors regression knn decision tree dt model multi layer reception mpl model i e artificial neural network model and deep learning algorithm i e the convolutional neural network and recurrent neural network family etc however each ai dm model is subject to specific pros and cons and there is no commonly accepted agreement on which modeling scheme is consistently effective than others across different study cases the linear regression model is the most straightforward statistical technique and early approach used to quantify the linear relationship in hydrological time series though it is relatively simple and old fashioned it has been widely used to investigate hydrologic variables adnan et al 2020 caldwell et al 2015 li et al 2016 ombadi et al 2020 ren et al 2020 sahoo and jha 2013 schmidt et al 2020 yuan et al 2018 derive reservoir operation rule policy ghimire et al 2020 liu et al 2019 zhou et al 2016 predict reservoir inflow and streamflow adnan et al 2019 lima and lall 2010b masselot et al 2016 estimate hydraulic behaviors adnan et al 2021 and assist water quality management zhao et al 2018 and drought prediction li et al 2020 the linear regression model s advantage is its simplicity and efficiency when the decision variables and target variables have an underlying linear correlation however the disadvantage is also apparent if only a nonlinear correlation exists between the decision variables and target variables such always fails to capture such a complex relationship and can only simplify the regression with a linear estimation unlike the linear models the support vector machine svm model acknowledges the existence of a possible nonlinear relationship between the features and the target variable the svm models could be further categorized into support vector classification svc and support vector regression svr models based on the nature of problems in the svm model a hyperplane will be created and used to separate the feature samples in the feature space this hyperplane could be either linear or nonlinear based on the user selected kernel functions because the hyperplane in svm models could adaptively partition the data samples the svm models could address the disadvantage of linear models for nonlinear regression and guarantee a unique and globally optimal solution when searching for the hyperplane lin et al 2006 theoretically the svm models could minimize the errors in the learning process and effectively reduce data overfitting if a proper kernel function is applied lal and datta 2018 yu et al 2006b the svm model has been applied to solve various water resources problems such as reservoir operation aboutalebi et al 2016 bozorg haddad et al 2018 ji et al 2014 liu et al 2017a xie et al 2012 and reservoir inflow and streamflow forecasting babaei et al 2019 feng et al 2020 liu et al et al 2017b malik et al 2020 samadianfard et al 2019 tao et al 2018 the svm model s advantage is that it can easily overcome the high dimensionality problem hand 2007 however a major drawback associated with the svm model is its low training efficiency wei 2015 when applying the svm models to datasets with large samples the training time tends to increase exponentially with the total number of data samples which prohibits some real world applications especially when quick model training and decision making are needed the k nearest neighbors knn model is an instance based learning or lazy learning method which was originally developed by fix 1951 and further improved by many others altman 1992 coomans and massart 1982 cover and hart 1967 the knn model can be applied to both classification and regression problems the classification procedure uses a neighbor search algorithm to recursively find the closeness of a total number of k training examples in the feature space after classification a regression could be carried out by averaging the target values from the k nearest neighbor samples atkeson et al 1997 the applications of knn for reservoir operation and water resources management are also numerous for example nikoo et al 2014 applied knn for water and wastewater allocation in the dez reservoir river system in iran and obtained good model statistical performance ahmadi et al 2010 combined knn and a genetic algorithm in a reservoir simulation optimization model and successfully incorporated forecast uncertainties of inflow into optimal reservoir operation yang et al 2020a tested a knn forecasting model to generate medium to long term inflow forecasts for the danjiangkou reservoir in china and the results proved the validity and reliability of the proposed knn prediction method the advantages of knn lie in its capability in achieving computational tractability toussaint 2005 and high effectiveness in approximating the target variables using a limited number of decision variables in a small subset of data samples two drawbacks of knn include 1 the training procedure sometimes overlooks the similarity and statistical relationship of the entire training samples thus did not work well on datasets that contain a high level of noises and 2 all calculations are deferred until classification bremner et al 2005 similar critiques on knn models also exist for example akbari et al 2011 pointed out that successful applications of the knn model rely on the similarity of the output values within the defined neighbors where the feature vectors are to be relatively close to each other however the number of neighbors in each group can be different and some neighbors may belong to none of the clusters in such cases the prediction made by knn algorithms may be risky and unreliable in other words the application of knn models would require a strong and local correlation between decision and target variables though such a correlation relationship may not exist globally nevertheless the use of knn models in reservoir operation is still popular in the literature another popular ai dm model set in support of reservoir management is the decision tree dt model breiman 2001 breiman et al 1984 chen and guestrin 2016 loh 2014 quinlan 1986 the dt model is a set of white box machine learning models which rely on building a sequence of simple boolean if then and true false logic to explain how complex data samples could be partitioned into smaller subsets or classes based on the feature values a comprehensive review of dt models can be found in mosavi et al 2018 in other words each data sample is regarded as a sequence of logical decision outcomes from the feature variables and similar decision outcomes could be traced back to the feature values thresholds used in the training process or the tree growing process therefore the advantage of dt models is the transparency to users and it shares a very similar procedure of how reservoir release decision is made for example dam operators typically use current storage level and rule curves i e the relationship between discharge and storage to decide whether to release a certain amount of water from reservoirs raso et al 2014 schwanenberg et al 2012 schwanenberg et al 2014 uysal et al 2020 zagona et al 2001 zhang et al 2020 some recent applications of dt models to reservoir release simulation have proven their usefulness in assisting different reservoir operation such as release scheduling ji et al 2016 rahnamay naeini et al 2020 wei 2012 yang et al 2015 yang et al 2020b zhang et al 2019 and reservoir inflow forecast erdal and karakurt 2013 tongal and booij 2018 yang et al 2017b specifically yang et al 2016 tested out a few different dt models to simulate the reservoir releases from 9 major reservoirs in california and concluded that the advantage of dt models is their effectiveness in capturing how reservoir releases are following the storage and inflow conditions which is a similar process used in the traditional rule based simulation model in another study yang et al 2020b found that different dt models in various input conditions could generate varying simulations with high variations and concluded that dt models are very sensitive to training data and may have a low transferability in general the implementations of dt models for reservoir simulation are still limited in practice one reason is the lack of accuracy of ensemble forecasts and significant hydrological uncertainty and the other reason is that over and under fitting can occur in dt models for datasets with a small number of samples or the training samples value becomes too sparse in short low robustness and accuracy are identified when scaling up dt models to different reservoir case studies brodley and utgoff 1995 dietterich 1995 mingers 1989 the multiple layers perceptron mlp or artificial neural network ann model is also one of the popular ai dm model sets in the broader field of hydrology community mlp or ann model has been widely applied to solve many types of problems including the rainfall runoff simulation reservoir operation and streamflow forecast problems coulibaly et al 2000 dawson and wilby 1998 kişi 2004 xu and li 2002 the main advantage of the ann model is its ability to detect complex nonlinear relationships between input features and outputs through the flexible learning process and the ann model can incorporate all necessary relationships through training procedures without requiring a priori knowledge of the underlying process daliakopoulos et al 2005 french et al 1992 for example sattari et al 2012 applied the ann model with an early stopped training approach to enhance the model s predictive performance they demonstrated that the ann model approach has substantially better accuracy than other baseline models in forecasting the daily reservoir inflow time series zhang et al 2018 compared three ai models including an ann model an svr model and a deep learning model to assist reservoir operation at different time scales concluding that the ann model s applicability on limited amounts of data is better than the other two ai models niu et al 2019 also compared three ai models ann svr and extreme learning machine and a multi variable linear regression for deriving the hydropower reservoir operation rule of hongjiadu reservoir in southwest china concluding that the three ann models have some unique merits and have better performance than the other employed regression models in many other studies the ann or the mlp models have shown superior performance over other traditional statistical time series models i e auto regressive integrated moving average arima and or other ai dm models in different hydrological and reservoir operation studies adamowski et al 2012 babaei et al 2019 jain et al 1996 lin et al 2009 lohani et al 2012 raman and sunilkumar 1995 valipour et al 2013 however the mlp or ann models still suffer from some weakness caused by 1 the lack of physical interpretation of its structure 2 difficulties in finding the global optimum of connecting weights and 3 uncertainties from the model parameters and settings govindaraju 2000 kim et al 2020 specifically the inputs number of layers and nodes activation function and training algorithm are considered as the major sources of uncertainty kasiviswanathan and sudheer 2017 for example the most commonly used activation function is the logistic sigmoid function hsu et al 1995 however the use of the hyperbolic tangent activation function could be better than the logistic sigmoid function in other applications zadeh et al 2010 in addition the generalization of the ann model can be highly challenging as the model performance can vary from one training dataset to another even with the same model structure and parameters in other words when applying ann or mlp models to different reservoir case studies practitioners need a lot of efforts to tune the model parameters and the parameters selection process could be challenging and time consuming these drawbacks of mlp or ann models substantially prevent a broader use of the models in assisting various types of reservoir operation practices the long short term memory lstm model is a class of deep machine learning algorithms and is rapidly gaining popularity in more recent years the lstm model is a special kind of recurrent neural network rnn which enables the output in the previous step to be used as input in the current step thus it has merits for simulating and predicting sequential time series data several recent studies have reported the superior performance of applying the lstm model to predict hydrological time series apaydin et al 2020 fan et al 2020 kao et al 2020 sahoo et al 2019 yuan et al 2018 and most of the studies are relatively new as compared to other ai dm models for example kratzert et al 2018 tested the lstm model in rainfall runoff forecasting and compared the results to the well known process based hydrologic model i e sacramento soil moisture accounting model or sac sma model they concluded that the lstm model is competitive in predicting runoff from meteorological observations compared to the process based sac sma model zhang et al 2018 tested out the lstm model in predicting the long term inflow time series of the danjiangkou reservoir which is the head water source for china s south to north water diversion project middle route and verified the outstanding advantage of the lstm model in effectively learning the sequential dependencies of reservoir inflow variabilities zhu et al 2020 developed an improved lstm model applied it to predict the daily streamflow at four stations in the upper yangtze river and compared the performance against three benchmark models ann model generalized linear model and heteroscedastic gaussian process model the results showed that the improved lstm model has satisfying performance compared to the benchmark models for high flow forecasting zolfaghari and golabi 2021 compared a few dt models the lstm model and six other benchmark models to predict the hydropower generation of the mahabad dam in iran and concluded that the lstm model is one of the emerging ai dm models and has great potential in assisting reservoir operation and hydropower scheduling the major advantage of the lstm model is its capability of taking sequential data as inputs instead of independent training samples this feature benefits to model s capability of dealing with more extended historic hydrologic observations with temporal dependence wu et al 2020 which is a common feature associated with many types of hydrological time series however the lstm model also has some limitations inherited from the black boxed model which is the lack of explicit internal representation of the water balance kratzert et al 2018 moreover the training of lstm could also be computationally expensive in some cases since the model trains on data sequence with user defined time steps the longer the time steps or the data sequence the more expensive the computation will be nonetheless many recent studies demonstrated a strong potential of the lstm model in hydrological time series forecasting and reservoir operation bai et al 2021 sahoo et al 2019 xiang et al 2020 with respect to the above literature review though ai dm models are becoming more and more popular in the field of reservoir operation the performance of different models still vary and each model has its own strengths and weakness large scale comparison is critically needed to comprehensively evaluate the performance of various ai dm models as well as to prevent ai dm models from becoming an alchemy hutson 2018 in the hydrology and water resources management research community it is essential to establish a standard evaluation testbed that includes a large number of case studies at least 10 to identify whether the ai dm models and the commonly employed parameters are transferable from one case to another and to verify the model s predictive performance is stable and reliable across different reservoir cases such a comparison study can further verify the usefulness of ai dm models in assisting reservoir operation and help decision makers to rebuild the confidence for future uses of ai dm models in practice however as far as the authors knowledge such a study that compares multiple methods over a large number of study cases for reservoir operation is rare in the literature most of the existing studies only compare two or three ai dm models in a limited number of studies without further examining the transferability of their proposed models therefore in this study we apply a total of 12 popular ai dm models which consist of two linear models two svm models two knn models three dt models two ann models and one deep learning model to simulate the daily reservoir releases over 33 study cases over the upper colorado region in the u s in order to comprehensively examine the models performance we compared the simulated release time series with observation via six commonly accepted statistical metrics under three different input scenarios through this study we want to answer the questions that 1 whether these ai dm models could reasonably mimic the human s release decisions using the limited information of historical reservoir inflow and storage time series 2 which model performs the best or worse on what conditions and evaluation criterion 3 can the employed ai dm models generate reliable and stable predictions across different study cases and 4 whether a model is transferable to other untested cases with high statistical confidence the experiment setting and findings of this study will also provide a technical reference of model evaluation and operation guidance on the topic of applying ai dm methods to reservoir operation for interested researchers and operators the rest of the paper is organized as follows section 2 summarizes the applied ai dm models and employed statistical measures the data and experiment settings are described in section 3 in section 4 we demonstrate the experiment results the discussion and conclusion are provided in sections 5 and 6 respectively the supplementary material includes all the calculated statistics of each reservoir under different scenarios and the detailed model setting and parameters the main body of the paper provides further analysis and a summary of the obtained simulation results 2 methodology in this study we applied a total of twelve ai dm models with different model complexity and parameterizations the twelve models could be categorized into six groups based on the data classification and regression mechanism used 1 the first modeling group includes a basic multi variable linear regression model and a second linear model termed the ridge regression model hoerl and kennard 1970 marquardt and snee 1975 both of them belong to the most basic linear model and the simplest regression model set 2 the second model set consists of two types of supportive vector regression svr models boser et al 1992 cortes and vapnik 1995 vapnik 2013 vapnik 1999 one svr model is implemented with the radial basis function kernel svr rbf and another uses the polynomial function kernel svr poly 3 the third model group consists of two k nearest neighbors knn regression models altman 1992 coomans and massart 1982 cover and hart 1967 with the number of neighbors being 3 and 10 knn 3 and knn 10 respectively 4 the fourth model group belongs to the decision tree dt based models breiman et al 1984 mingers 1989 quinlan 1986 there are three dt models being tested in this study namely the classification and regression tree cart breiman et al 1984 the random forest rf algorithm breiman 2001 and the extreme gradient boosting tree xgboost model chen and guestrin 2016 5 the fifth group includes two multiple layer perceptron mlp models jain et al 1996 implemented with the hyperbolic tangent activation function mlp tanh and the polynomial activation function mlp poly last 6 the sixth model group contains one popular deep learning model i e the long short term memory lstm model hochreiter and schmidhuber 1997 in the following sections we briefly introduce these employed ai dm models for conciseness we only summarize the key concept used in each model the detailed mathematical definition is summarized in supplementary material section 1 for interested readers 2 1 linear model linear regression is one of the most widely used mathematical techniques to predict a target variable or vector y i based on the values of a set of dependent variables or called feature vector x i kutner et al 2005 makridakis et al 2008 montgomery et al 2012 a linear relationship is assumed to be existing between the dependent variable and the target makridakis et al 2008 montgomery et al 2012 the coefficient vector β n could be estimated by fitting a linear line between the historical observations of x n i and y i in the context of machine learning the similar process is called model training once the values of the coefficient vector β n being identified from the training process the same set of β n could be used to predict the values of the target variables using a new data point x n i which is from either in a future phase or a testing dataset that the training process never uses marill 2004 seber and lee 2012 specifically for linear regression models we have the following eq 1 1 y i β 0 β 1 x 1 i β 2 x 2 i β n x n i ε where y i is the i th observation of the dependent variable x 1 i x 2 i x n i are respectively the i th observation of the independent variables x 1 x 2 x n n is the number of variables β 0 β 1 β n are the model parameters which are also called regression coefficients ε is the error term assumed to be normally distributed with zero mean and variance σ 2 the linear ridge regression is a modified model of the basic linear regression model with an l2 norm penalty regularization term hoerl and kennard 1970 marquardt and snee 1975 the penalty term intentionally shrinks and controls the regression coefficient of the linear model to avoid the poorly determined and high variance coefficient problems the process of introducing the penalty term to a regression model is called regularization the goal of regularization is to improve the conditioning of the prediction problem i e overfitting or underfitting and essentially to reduce the prediction variance when the input variables have some levels of dependency or the training dataset is biased adding an l2 norm penalty term onto the linear regression model eq 1 could effectively constrain the values of the coefficient vector and control the bias variance trade off hastie et al 2009 myers and myers 1990 hence the linear ridge regression algorithm may build a model with a fewer number of parameters than the simple linear multivariable regression model and it is found to be less sensitive and less overfitting than the regular linear model lima and lall 2010a yu and liong 2007 the procedure of building in the l 2 panelty term into the standard multi variable linear regression model is presented in supplementary material section 1 1 2 2 support vector machine svm and support vector regression svr the support vector machine svm is a supervised machine learning algorithm which was introduced as a statistical learning algorithm for complicated data classification and regression boser et al 1992 cortes and vapnik 1995 vapnik 2013 vapnik 1999 based on the purpose of use the svm can be further categorized into the support vector classification svc and support vector regression svr the key concept of svm is to identify and search for a hyperplane i e an conceptual and high dimensional fitting line which will optimally partition the training data by its feature values the identified hyperplane will become the decision boundary and be further used to predict the continuous output in the regression model comparing to the basic linear models the svm model acknowledges the presence of non linearity in the data i e the input features are nonlinearly correlated making it hard to identify a linear line or plane to effectively separate the input data to address this issue the svm models will first create an n 1 higher dimensional feature space with n equals to the dimension of original input data because in the higher dimensional feature space all input features will be linearly correlated the formulation of a linear separation line i e the hyperplane will be feasible gunn 1998 noble 2006 smola and schölkopf 2004 the identified hyperplane will be transferred back to the original n dimensional space to partition the input data and further used as the regression fitting line to find the optimal hyperplane in the n 1 dimentional space the svm algorithm implements a learning algorithm that provides a globally optimal solution by minimizing the upper bound of the generalization error between the support points deka 2014 haykin and network 2004 hipni et al 2013 in summary the svm algorithm consists of two essential steps 1 to project the input data into a higher dimensional feature space and 2 to find a global optimal hyperplane to split the data by evaluating the offsets of each data point to this hyperplane deka 2014 vapnik 2013 in the svm model framework one of the key parameters is the selection of the kernel function a kernel function is used to transform the inputs into a required dot product format as well as to describe how input features are linearly separated in the n 1 dimensional feature space in this study we tested two common kernel functions namely the radial basis function kernel and the polynomial kernel function following the prior suggestions from the study in applying svr on hydrological time series forecasting and reservoir simulation using the svm models adnan et al 2020 cheng et al 2020 yu et al 2006a zhang et al 2018 1 polynomial kernel function k x i x j γ x i x j c d 2 radial basis kernel function k x i x j e x p γ x i x j 2 where γ is the structural parameter in the polynomial function c is the residuals and d is the degree of the polynomial term x i and x j represents the data in the original data space and the transformed n 1 dimensional space for conciseness the detailed mathematical derivation and representation of the svm model are summarized in the supplementary material section 1 2 for interested readers 2 3 k nearest neighbors knn regression model the knn model is an unsupervised machine learning approach which was originally introduced by fix 1951 the knn algorithm was further developed by many others altman 1992 coomans and massart 1982 cover and hart 1967 over the years the knn model has become a simple but effective method for solving both classification and regression problems in a variety of research fields the key procedure in the knn model is to group the training samples into multiple classes with the same user predefined size k or the number of data samples in each class the data separation process follows the rule that the k data points in each class will be the closest with respect to any other new data point in the training sample in other words the knn algorithm partitions the data based on the closeness or the shortest distance among the proximity of a total of k samples bhatia 2010 cunningham and delany 2020 the distance can in general be any metric measure while the standard euclidean distance is the most common choice imandoust and bolandraftar 2013 the number of samples k can be a user defined constant or vary based on the local density of points radius based neighbor learning in this study we set the number of samples in the nearest neighbors as 3 and 10 as knn 3 and knn 10 respectively also see supplementary material section 1 3 some initial test was carried out to identify the values of this parameter that the number of nearest neighbors beyond 10 did not improve and sometimes deteriorate the model performance more discussion will be provided in later sections about the choices of ai dm model parameterization and sensitivity analysis 2 4 decision tree dt based models cart rf and xgboost tree model the decision tree dt model is a non parametric white box statistical learning approach breiman et al 1984 mingers 1989 quinlan 1986 that uses a tree structured classifier to recursively partition the training dataset into smaller subsets i e nodes following identified splitting rules when splitting the data into smaller subsets each data partition will primarily follow a simple true or false boolean logic i e whether the value of a feature or data greater or smaller than a threshold freund and mason 1999 rokach and maimon 2005 by repeatedly partitioning the data into smaller subsets the final classes will only contain the samples that are distinguishable from other subsets of data based on a sequence of characteristics in the feature space buntine and niblett 1992 the major advantage of the dt is that it follows a highly intuitive and efficient splitting rule where decision makers can easily see the logic of interpreting the data james et al 2013 krishnan et al 1999 yang et al 2020b furthermore the establishment of splitting rules to partition the data or the so called tree growing process requires less effort for data preparation and pre processing in other words there is no need to make strict assumptions about the distribution of data or the raw value scales of the training data the tree based models are also found suitable for dealing with unbalanced data classification and regression ganganwar 2012 ke et al 2017 pradhan 2013 however there are still some disadvantages of the dt model for example over and under fitting can occur on datasets with a small number of samples or the training samples value becomes too sparse the dt model could result in a lack of robustness and low accuracy for unseen data brodley and utgoff 1995 dietterich 1995 mingers 1989 to deal with those problems in dt models primarily two enhancement strategies were developed i e the bagging and boosting techniques in this study we employed three dts models namely the classical classification and regression tree cart breiman et al 1984 the random forest rf algorithm breiman 2001 and the extreme gradient boosting xgboost tree algorithm chen and guestrin 2016 the cart model is one of the early and classical dt models while the rf and xgboost algorithms are two newer dt models which incorporated with the bagging and boosting enhancement techniques respectively the classification and regression tree cart was originally introduced by breiman et al 1984 it divides the data items into homogenous subsets using binary recursive partitions aiming to classify datasets and has been proven as a powerful tool for both classification and prediction problems de ath and fabricius 2000 loh 2014 steinberg and colla 2009 the cart algorithm provides a base regression model for the later developments of both rf and xgboost algorithms the mathematical description of the cart algorithm its tree growing process and splitting criteria are briefly introduced in the supplementary material section 1 4 which follows the summary from hastie et al 2009 and the original development from breiman et al 1984 the random forest rf is an ensemble learning algorithm introduced by breiman 2001 the key concept used in the rf algorithm is to build and combine multiple candidates of the standard cart models based on a bagging strategy to avoid overfitting and or underfitting liaw and wiener 2002 specifically the rf algorithm starts with generating several bootstrapped samples of input features from a given training dataset then different cart models are trained on each of the bootstrap samples during this process both strong and weak learners are presented finally the model output can be achieved by aggregating the outputs from all the candidate cart models which are individually built from the bootstrapped samples efron 1987 1992 johnson 2001 the ensemble strategy used in the rf algorithm ensures the prediction model contains both weak learners and strong learners from the original training dataset by assembling multiple candidates cart models the final ensemble model will be more robust and less overfitting than the single cart model brokamp et al 2017 mathematically let s assume that there are e number of trees and their model output is y i i 1 2 e then the final output of the rf regression can be obtained by averaging the outputs of all the trees as presented in eq 2 2 y 1 e i 1 e y i where y i is the prediction results from each ith cart model trained on different bootstrapped data and input features e is a user defined parameter and represents the total number of ensemble candidate cart models to be used as ensemble candidates and y is the final ensemble model of the rf algorithm the detailed model parameters used in the rf algorithm are summarized in supplementary material section 1 5 in contrast to the rf and cart algorithm the xgboost tree model is another recently developed dt algorithm which was introduced by chen and guestrin 2016 the xgboost tree algorithm was based on the greedy gradient boosting framework for parallel tree boosting friedman 2001 2002 but was enhanced by a novel tree learning algorithm is for handling sparse data and a theoretically justified weighted quantile sketch procedure to enable the handling of instance weights chen and guestrin 2016 like the rf model the xgboost model uses an ensemble of cart however unlike the rf algorithm that develops ensemble candidates on bootstrapped training data and features the xgboost model is further designed to boost the performance of weak learners by performing additive training strategies boutaba et al 2018 ke et al 2017 the gradient boosting approach allows new models to be trained to predict the residuals i e errors of prior models in other words instead of training multiple models in isolation of one another the gradient boosting strategy trains models in succession the new model will be trained to correct the errors made by the previously trained models the boosted models are then added together sequentially until no further improvements could be made eventually by developing an ensemble of the boosted model candidates the model s predictive performance will be improved as the performance of all the candidate ensemble models have been boosted by correcting the residuals the mathematical derivation of gradient boosting tree from a based cart algorithm is briefly introduced in our supplementary material section 1 6 and interested readers shall also refer to the original development research from chen and guestrin 2016 2 5 multiple layer perceptron mlp models or artificial neural network ann the multiple layer perceptron mlp model or artificial neural networks ann model hoskins and himmelblau 1988 mcculloch and pitts 1943 is one of the most widely used machine learning algorithms in many fields the development of the mlp or ann model was inspired by the biological neural network of the human brain jain et al 1996 the key concept of ann is to build a network like model structure with the goal of finding the intrinsic patterns or relationships in a given dataset through a learning or training process called backpropagation asce 2000 goh 1995 hecht nielsen 1992 in general an ann model consists of three types of basic layers e g the input hidden and output layers and the nodes between these layers are interconnected by tunable connection weight parameters the input layer takes either the raw or normalized training data and the value of one particular node in the input layer is weighted and passed to a successor node in the hidden layer the corresponding node in the hidden layer will process the weighted sum value and information obtained from the input layer within each node in the hidden layer the weighted sum values from prior nodes will be further processed by a pre defined transformation function or called activation function and then becomes the output from the hidden node the same weighting procedure is performed to combine the outputs from hidden layer nodes into the information feed to the nodes in the output layer node if an mpl model has multiple hidden layers the procedures of both weighting and transformation are repeated until the output layer is reached the values of the connection weights are identified by calculating the accumulated errors from all output layers and hidden layers and by minimizing a loss function in the training process the role of the activation function is important as it enables nonlinear statistical modeling with complex data hsu et al 1995 zealand et al 1999 in this study we used two types of activation functions the hyperbolic tangent activation function tanh and the logistic activation function log the tangent and logistic functions for any variable t are defined by eqs 3 and 4 respectively 3 tanh t 2 1 e 2 t 1 4 l o g i s t i c t k 1 c e r x where c is the constant from integration r is the proportionality constant and k is the threshold limit assuming the k c and r all equal to 1 the logistic activation will become the standard sigmoid activation function the detailed mathematical definition of an mlp or ann model and the applied model parameters number of layers number of hidden nodes learning rate regularization term training optimizer maximum iteration numbers etc are introduced in supplementary material section 1 7 2 6 long short term memory lstm model the lstm model is a deep learning algorithm introduced by hochreiter and schmidhuber 1997 it is a particular type of recurrent neural network rnn that is specialized in dealing with time sequential data prediction gers et al 2002 graves and schmidhuber 2005 rumelhart et al 1986 differs from the standard feedforward ann or mlp models the lstm model assumes the training samples are temporally correlated instead of importing continuous data as independent training samples the lstm model trains the time series of both feature and target variables with a user defined time step the key benefit of the lstm model is that it is operated by using memory cells with input output and forget gates which capable of not only learning long term range decencies but also overcoming the gradient vanishing and exploding problems of rnn hochreiter 1998 hochreiter et al 2001 the basic structure of lstm consists of input hidden and output layers similar to the mlp model and it has an unfolded structure due to the recurrent connections of hidden states in the hidden layer the mathematical description of the internal operations of a memory cell in lstm can be briefly described in gers et al 1999 and yu et al 2019 and summarized in supplementary material section 1 8 2 7 statistical measures in this study we employed six different statistical measurements based on the suggestions from moriasi et al 2007 the employed statistical measures include the correlation coefficient corr the root mean square error rmse the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 the kling gupta efficiency kge gupta et al 2009 the rmse observation standard deviation ratio rsr and the percentage of biases pbias the equations value range and optimal values are listed in table 1 in table 1 the q o b s i and q s i m i are the observed and simulated reservoir daily discharges at the time step t respectively the q o b s i and q s i m i represent the mean of the observed and simulated values respectively the variable n is the total number of time steps in the compared time series in the kge calculation μs and σs represent the mean and standard deviation of the simulated discharges and μo and σo are the mean and standard deviation of the observed hydropower releases respectively the selection of these statistical measures is based on the commonly accepted standards of streamflow simulation and model evaluation in the field of hydrology specifically the measurements of corr rmse and nse are widely used statistical measures to quantify how the simulated streamflow matches the observed streamflow corr measures how a simulated time series will vary in corresponding to observation rmse quantifies the accumulated biases between the simulation and observation and it is a similar measure of the mean absolute error mae with relatively higher sensitivity due to the mathematical operation of taking the square of the error term nse is a combined statistic of both rmse and corr in which both the bias and temporal variation will affect the nse value in addition according to gupta et al 2009 the kling gupta efficiency kge was developed amending some shortcomings of the nse measurements the kge measurement was able to decompose the nse values into linear correlation bias and variability components between simulated and observed time series and it is thusly able to analyze the relative importance of each of the terms that contribute to the nse index according to moriasi et al 2007 and singh et al 2005 the rmse observation standard deviation ratio rsr standardizes rmse using the observations standard deviation and it combines both an error index and the additional information legates and mccabe 1999 according to moriasi et al 2007 and gupta et al 2009 the rmse and rsr with a value equal to zero and or the nse or kge value of 1 is the indication of the best accuracy between a model simulated time series and the observation the larger the rmse and rsr value or the smaller the nse and kge value the poorer performance a model is some reference ranges for the nse and kge values are summarized below gupta et al 2009 moriasi et al 2007 unsatisfactory nse 0 4 acceptable 0 40 nse kge 0 50 satisfactory 0 50 nse kge 0 65 good 0 65 nse kge 0 75 and very good 0 75 nse kge 1 00 the variable pbias quantifies the percentage of biases between a simulated time series and the reference positive and negative values indicate underestimation and overestimation relative to the measured data respectively while values of 0 are desired satisfactory values of pbias vary for different constituents and must consider the level of measurement uncertainty the higher the pbias value either towards positive or negative the worser the model accuracy besides the tabular and numerical statistical measurements in this study we also employed a graphical model evaluation tool termed the taylor diagram taylor 2001 the taylor diagram provides a way of showing how three complementary model performance statistics i e the correlation coefficient r the standard deviation sigma and the centered root mean square error simultaneously in one single 2 d graph according to taylor 2001 the plotting of multiple statistics in one graph is based on the geometric relationship cosine law between the correlation coefficient r the centered root mean square error rmse and the standard deviations between the simulation and observation the taylor diagram has been extensively used in climate model studies miao et al 2014 tao et al 2018 yang et al 2018 as well as the topic of ai dm model evaluation and comparison in the field of hydrology adnan et al 2021 kargar et al 2020 shabani et al 2020 3 data and experiments setting 3 1 study cases the upper colorado river basin is comprised of four states including colorado new mexico utah and wyoming collectively the upper colorado basin contributes a majority of the fresh surface water supplies coming into the entire colorado river basin primarily through winter snowpack and streamflow with the impacts of climate change altering the amount of snowpack and timing of spring runoff water supply in the colorado river is increasingly strained the reservoir systems in the upper colorado region plays an inevitably crucial role in managing the surface water for multiple uses such as flood control hydropower creation ski boating fishing as well as direct water supplies to residential industrial and irrigation over the states of colorado new mexico utah and wyoming the colorado river provides water to nearly 40 million people and drives a 1 4 trillion economy climate change and increasing water demand due to an expanding population is and will continue to present significant challenges if left unaddressed the varying weather and climate will impact our regional and national economies degrade the environment challenge our agricultural heritage and food production and limit recreational opportunities from fishing and boating to skiing therefore the focus on the upper colorado basin reflects the importance of both regional and national social economic benefits the upper colorado region consists of complex terrains and is prone to changing climate of precipitation snowpack and temperature the refills of the reservoir system over upper colorado primarily are from the spring snowpack melting and direct streamflow from rainfall runoff hydrology this setting is typical in many other regions where reservoir plays the role of changing the timing and amount of water flowing to downstream regions as mentioned in the introduction the development of advanced management and decision making tools will significantly promote our capability in controlling the surface water resources and better enable us to mitigate the potential impacts of climate the upper colorado region itself serves as an ideal region that the impacts of climate and extreme rainfall snows could easily reflect in the reservoir inflows variability based on the above two reasons the reservoir system within the upper colorado river basin is selected for this comparison study of different ai dm models in this research we choose 33 reservoirs in the upper colorado region under the jurisdiction of the u s bureau of reclamation usbr the following table 2 lists the short name full name locations and data lengths employed in this research the locations of the selected reservoirs are presented in fig 1 we choose these reservoirs based on the criteria that 1 the reservoir shall have a complete set of data records for reservoir inflow storage and outflow at a daily time step and 2 the data records are continuous without significant missing data over ten days the reservoir inflow storage and outflow data are obtained from the usbr water operation archive https www usbr gov rsvrwater historicalapp html in this data repository some reservoirs among the 33 selected ones have an earlier start date and longer data record however in our initial data screening we found some data were either missing or unavailable at earlier records and the numbers of missing data are significant therefore in the experiments we manually checked the data values for each reservoir and selected the start data when all daily inflow storage and outflow data are continuously available from that year in other words the starting date for each reservoir is different table 2 while the ending date for all simulations is set consistently as december 31st 2020 among the selected 33 reservoir cases the shortest data record is about 10 years and the longest data record is about 50 years table 2 also lists the elevation of each reservoir which ranges from 1323 to 2847 m above sea level 3 2 experiment setting in this study we use the daily inflow storage and seasonality months as model inputs to estimate the daily reservoir outflow according to several prior studies on the topic of reservoir time series analysis hejazi et al 2008 zhao et al 2012 the one and two time step delayed reservoir inflow and storage information have strong correlations to the current time step reservoir outflow following their findings we designed three different input scenarios to drive the ai dm models and our experiment design which is shown in the following fig 2 in the first designed simulation scenario s1 we use the current inflow storage and seasonality months as the default inputs to ai dm models in the second simulation scenario no 2 s2 the model input categories further extend to cover the 1 step 1 day delayed information along with the default model inputs under s1 in the last and third scenario s3 we design the ai dm model inputs to further include both the one and two step delayed inflow storage and seasonality as well as the default input set defined under s1 in other words the complexity and number of inputs to the ai dm models are increasing from s1 s2 to s3 we expect the performance of ai dm models will be sensitive to the input training time series being used and the differences will be analyzed in the later result section in all of the performed experiments 80 of the data is used to train the ai dm models and reminder 20 of the data record is used as validation the underline assumption is that the employed ai dm models will be trained on a subset of the entire data and be tested on a new subset of data that the model never sees during its training process note that this partition ratio of data 80 20 is consistent across each reservoir case however since each reservoir has different data lengths table 2 the data lengths used in training and validation are different for each reservoir nevertheless for each reservoir the training and validation data used are identical across different ai dm models which ensures the model comparison is fair last but not least when we train individual ai dm models all model inputs and target values are normalized into the range of 1 1 after the model training process and during the validation period the model predicted values are transformed back to the normal range of daily outflows for each reservoir the evaluation and assessment of statistical measurements are conducted using the transformed model predictions which are in the normal range of daily reservoir outflow observations instead of the normalized range of 1 1 most of the model hyperparameters are set prior to the experiments by manual trial and error to avoid overfitting and this is also the reason we did not put a separate third sub dataset as testing the specific model hyperparameters and training settings for each employed ai dm model are listed in supplementary material section 1 for interested readers 4 results 4 1 statistical results of the baseline scenario no 1 s1 in this section we will present the obtained statistics of all reservoirs under the baseline scenario no 1 s1 and then compares the performance across different models and reservoir cases note that all calculated raw statistical measurements between simulated and observed reservoir outflows under all simulation scenarios are presented in the supplementary material tables 1 33 and this section will present a further summary and analysis specifically in the following table 3 we summarize the maximum minimum and average of the statistical measures across all employed ai dm models over the validation periods for each reservoir these maximum minimum and average statistical values are drawn from the raw statistical performance of each individual reservoir supplementary material tables 1 33 across all employed ai dm models in order to obtain an overview of the statistical performance of all reservoirs at the ending rows of table 3 we further take a numerical maximum minimum and average of all corresponding values from the statistical summary of prior rows of table 3 for all reservoirs the results in table 3 indicate how well the employed ai dm methods could capture the variations of daily reservoir releases regardless individual model used the ending bolded rows of table 3 indicate how well the statistical performance between simulation and observation across all employed reservoir cases in general according to table 3 in most of the reservoir cases the employed ai dm models could achieve satisfactory statistical performance over the validation periods as evidenced by the minimum values row for each reservoir the worst corr value e g corr column and min row for each reservoir is 0 215 in the reservoir case of sta followed by the second worst corr value being 0 295 in the case of ste however the average of all of the worst min corr values across all reservoir cases is 0 634 e g corr column and min row at the average section at the end of table 3 the worst min nse value is observed as 0 476 for reservoir nav followed by the second worst nse value being 0 133 in the reservoir case of fgr nevertheless in most of the cases the worst nse values across all reservoir cases are consistently above 0 5 which indicates still a good match between the simulated and observed daily reservoir outflow according to the nse satisfactory category set by moriasi et al 2007 the average of all worst nse values in all employed reservoirs is 0 351 e g nse column and min row at average section at the end table 3 table 3 also indicates the worst kge value is 0 242 for reservoir usr as compared to other cases the average of all of the worst kge across all studied cases is 0 398 referred to the average row at the end of table 3 except for these worst scenario cases in general the statistical measures are rather satisfactory with high corr nse and kge minimum values for each reservoir i e see individual min rows under each reservoir case for pbias the highest absolute bias in the simulated time series appears to be 122 in the reservoir case of usr indicating at least one of the employed ai dm models significantly underestimated the reservoir outflow however in the same case of usr the average pbias value is 25 which means if averaging the performance of all employed ai dm models the prediction bias could be reduced and the high bias of underestimation was only associated with few modeling scenarios when taking a further examination of raw statistics from the supplementary material tables 32 18 and 27 we found out that using svr rbf svr poly mpl tanh model in simulating the outflows from the reservoir of ubr mcp and sco individually the pbias values are 122 843 49 236 49 197 respectively for the last rsr statistical measurement the performance of all employed ai dm models are similar and the worst max value is observed to be 1 215 for the reservoir case of nav followed by the second worst value of 1 001 observed for the reservoir case of sta in general table 3 shows that when simulating the daily reservoir outflow decisions using default reservoir inflow and storage time series though the performance of each ai dm model will vary the averaged statistical performance over all 12 employed models and all 33 reservoir cases are still satisfactory with the following averaged statistical value ranges 0 634 corr 0 853 5 580 rmse unit m3 s 8 195 0 351 nse 0 718 0 398 kge 0 759 18 495 pbias unit 15 357 and 0 506 rsr 0 779 for all the reservoir cases 4 2 reservoir outflow simulation and model taylor diagram in the following figs 3 5 we plot the ai dm model simulated daily reservoir discharge against the observed time series for reservoirs bsr cau cry dcr dil ebr ech ecr fgr fon and gmr in fig 3 hnr hyr jor jvr lcr lem mcp mcr mpr nav and pin in fig 4 and rfr rid roc rue sco sjr sta ste tpr usr and val in fig 5 the data records in figs 3 5 cover the validation periods for each reservoir listed in prior table 2 in figs 3 5 the y axis is daily discharge from the reservoir m3 s and the x axis is the number of days in the validation periods different color lines indicate simulated reservoir outflows by different ai dm models and the observed time series are plotted with dotted black lines aside from each subplot in figs 3 5 we also draw the taylor diagram showing the model performance by color dots in all the taylor diagrams the observation is located at the bottom axis the closer of the color dots to the observation the better performance in the context of the pearson correlation coefficient the root mean square error rmse error and the standard deviation by geometric cosine law taylor 2001 according to figs 3 5 the variation of daily reservoir releases could be well captured with many ai dm models though the simulated reservoir outflows may deviate from observations from one simulation period to another among all studied reservoirs under s1 the performance of ai dm models could also vary from one case to another in the prior statistical result table 3 we identify the reservoir cases fgr sta ste nav and usr which are contributing to the worst scenarios of different statistical measurements in this section we further analyze the results for these cases along with the time series plots and taylor diagrams presented in figs 3 5 the simulated results for reservoir fgr sta ste nav and usr are presented in fig 3 i fig 5 g fig 5 h fig 4 j and fig 5 j respectively according to the model simulated results of cases sta in fig 5 g we observe that both linear model the ridge model and the lstm model failed to identify the patterns of reservoir releases these models showed significant overestimation over low flow conditions whereas the reservoir release shall be close to zero and demonstrated underestimation over high flow conditions for example the simulated reservoir outflow by the linear ridge and the lstm models are fluctuating around 5 m3 s without capturing the outflow peaks that are above 10 m3 s in contrast the mlp log model could well capture the low flow conditions but exhibit some overestimation in a few peaks in these reservoir cases by examining the detailed models statistical measures in reservoir sta supplementary material table 29 under scenarios no 1 the corr values obtained by the linear model ridge model and the mlp model with logistic activation function are 0 215 0 215 and 0 255 respectively while other models could reach a much higher corr value exceeding 0 540 in most of the employed ai dm models similar behaviors of the linear model ridge model and the mlp log model are also observed in another reservoir case of sco fig 5e for the reservoir case of sco the corr values obtained by the linear model ridge model and mlp log model are 0 295 0 295 and 0 361 respectively under the default scenario no 1 supplementary material table 27 however in other cases such as reservoir ebr fig 3f ecr fig 3h and ste fig 5h the obtained corr values by the mlp log model are consistently and significantly higher than the linear model and ridge model supplementary material tables 6 8 and 30 the non competitive performances of the linear model and ridge models in simulating reservoir outflows are also observed in some other reservoir cases for example the results of bsr fig 3a hnr fig 4a and pin fig 4k respectively in all those mentioned cases we observe that linear and ridge models are performing poorly as compared to other more complex ai dm models this indicates the reservoir release decision is not a simple and linear process with respect to inflow amount and storage volumes the linear assumption may not be valid when using reservoir inflow storage to estimate reservoir outflow and more complex ai dm models are needed when we are analyzing obtained nse values in the prior table 3 we observe that the worst nse value occurred in the reservoir case of nav with the time series plots and taylor diagram of reservoir case nav fig 4j we found that the worst nse value is due to the poor performance associated with the cart model and knn algorithms the time series plots in fig 4 j indicate both the cart model light green line and knn algorithms tend to overestimate the peaks and generate significant overestimations across the validation periods based on the detailed statistics in supplementary material table 21 the obtained nse values for knn 10 knn 3 and cart models are 0 332 0 476 and 0 365 respectively these negative nse values indicate unsatisfactory and poor model predictive performance of these models similar issues are also identified in other cases including the reservoirs ech ecr fgr in fig 3 g h i respectively the reservoirs mpr and pin inf fig 4 c and k respectively and the reservoirs rfr rid sco sta ste and tpr in fig 5 a b e g h i respectively in all the mentioned cases the simulated daily reservoir releases from the knn and cart models have relatively low nse values compared to other ai dm models furthermore in the case of fgr fig 3i we notice the lstm model dark red line fails to predict the low flows of the reservoir releases and the obtained nse value for the lstm model is the lowest 0 133 as compared to other ai dm models supplementary material table 9 similar cases include the reservoir cases of lcr fig 4e pin fig 4k rfr fig 5a sco fig 5e and sta fig 5g in all these identified cases though the lstm model could well capture peak flows the low flows are consistently overestimated the obtained nse values by the lstm model in these reservoirs are also lower than those obtained by other employed ai dm models with respect to the kge value in earlier results we identify among all employed models the worst kge value happens in reservoir usr with a negative kge value of is 0 242 fig 5j according to table 3 another two cases when kge exhibits negative values are reservoir cases of ste and sta we noticed that in reservoir ste and sta both linear model and ridge model perform poorly if excluding the linear models in reservoirs ste and sta other models could consistently derive positive kge values table 3 in reservoir usr fig 5j the two worst models are svr models with radial basis and polynomial kernels which generate kge values of 0 242 and 0 081 respectively supplementary material table 32 in the same case of usr the simulation results derived from other models are consistently higher than 0 444 which belongs to a satisfactory kge value in addition we also notice that in reservoir usr fig 5j the daily reservoir outflows are zeros in most of the time steps and there are only a few peaks within the validation periods the only similar cases are dil fig 3e and mcp fig 4g where most of the daily releases are very small and a few peaks exist across the validation period in the reservoir case of dil fig 3e the two worst performing models are lstm and svr with polynomial kernel function which produces a kge value of 0 584 and 0 608 respectively supplementary material table 5 however the svr model shows an evident overestimation in the later parts of the validation period around data point 1700 fig 3e at this data point from dil many ai dm models significantly overestimated the observation including the lstm model the knn 10 model and the svr poly model among these problematic models the svr poly model tends to perform the worst when compared to others we further noticed that in the reservoir case of mcp fig 4g and according to the obtained kge value over the validation period the two worst performing models are lstm and svr poly the kge values obtained from the simulated outflows from the lstm and svr poly models over the mcp reservoir are 0 275 and 0 372 supplementary material table 18 respectively under simulation scenarios no 1 s1 in summary we infer that if the reservoir presents consistent small daily releases close to zeros for most of the days the lstm and svr poly model seems to be not capable of obtaining satisfactory kge values of the simulated reservoir outflow according to the taylor diagrams presented along with each time series plot in figs 3 5 there are three major observations first it seems that the model simulation results from different types of ai dm models are similar and consistently good for reservoir cases of cau fig 3b and cry fig 3c all the colored dots in the taylor diagrams are nicely clustered and are consistently close to the observation point second for reservoir cases of pin fig 4k and mcr fig 4h the lstm model performs significantly better than other ai dm models and the lstm dot dark red is the closest to the observation point in the corresponding taylor diagrams with apparent superiority however the lstm model turns to be the worst performing model when switching to reservoir cases of mcp fig 4g and rid fig 5b and their corresponding locations in the taylor diagram are further away from the observation as compared to that of other models third in reservoir cases of rfr sco and sta fig 5a e and g respectively the mlp tanh model demonstrates relatively poorer performance than that in other reservoir cases the level of predictive performance of the mlp tanh model is similar and low as it shows with some linear models note that these above findings are specifically based on the taylor diagram results which are mainly drawn from the closeness of model generated color dots towards the observation the inference of models predictive performance may or may not correspond to the findings observed from the tabular statistical results nevertheless the taylor diagram provides an easy integrated and visually direct quantification of the raw model performance in the context of the standard deviation corr and rmse 4 3 model uncertainty and simulation stability in the following fig 6 we present the box plot of the statistics obtained by each ai dm model over the validation periods of all 33 reservoir cases under scenario no 1 note that this figure depicts a different aspect of statistical performance as compared to the statistical summary in the prior table 3 specifically table 3 indicates the ai dm models overall performance of maximum minimum and average for each employed reservoir while fig 6 shows the performance of each employed ai dm model across all 33 reservoir cases and each box plot was made using 33 data points for each corresponding statistical measurement according to fig 6 most of the ai dm models could obtain the interquartile range 25 75 percentiles of 0 6 0 9 0 3 0 8 and 0 2 0 8 for corr nse and kge fig 6a c respectively and 1 5 6 5 15 to 20 and 0 5 8 5 for the normalized rmse pbias and rsr fig 6d f respectively if comparing the statistical performance of all employed models we notice that 1 the tree based models cart rf and xgboost could achieve a higher corr and kge values interquartile range which indicates a better overall performance than other ai dm models across all studied reservoirs and 2 the linear models ridge and linear generally produce the poorest interquartile ranges as compared to others with respect to the statistics of corr nse kge and rsr the lstm model shows as a competitive model as compared to others in most of the reservoir cases however we also notice the ranges of interquartile obtained by the lstm model are relatively larger than other ai dm models for the statistic metrics of corr nse kge rsr and pbias this observation means that the lstm model s performance are less stable than other ai dm models though in some cases it can obtain a superior performance under the input scenario no 1 similar findings are also discussed in the prior section about the taylor diagram in general the lstm model performance indicated in this fig 6 and that in the prior taylor diagrams are in agreement with each other in addition the lstm model also generates the highest pbias interquartile range fig 6e which is not desired in summary the lstm model has less transferability from one reservoir case to another under the baseline input scenario no 1 and it demonstrates low stability when applied in different simulation cases the results in fig 6 also indicate that 1 evaluating the performance of ai dm models needs an examination of a comprehensive set of statistical measurements as the superiority of one particular statistical performance cannot guarantee the same performance in other measurements and 2 any conclusion about the performance of ai dm models on single or few study cases would rather be biased and when changing the study cases the model s performance are likely to vary for example we notice that knn models knn 3 and knn 10 could reach very satisfactory kge interquartile ranges and they are significantly better than the svr poly model under scenario no 1 fig 6c however the simulated outflow from all 33 reservoirs by the svr poly model has a lower bias pbias value closer to zero than all knn models fig 6e similarly though the multiple layer perceptron models mlp log and mlp tanh show superior performance over linear model sets in corr nse and kge statistics fig 6a c the overall performance of reducing rmes and pbias is similar to that of the linear model sets fig 6d e more discussion will be provided in the later section regarding the best performing models under different simulation scenarios and statistical measurements 4 4 statistical results for baseline of scenario no 2 3 s2 s3 instead of presenting the same statistical analysis similar to prior table 4 and figs 3 5 under scenario no 1 s1 in this sub section we focus on comparing the results under scenarios no 2 and 3 s2 and s3 in terms of improvement or deterioration comparing to the baseline s1 all calculated raw statistical measurements under scenarios no 2 and 3 are also available in supplementary material tables 1 33 and this section is intended to highlight the performance changes of the employed ai dm models when switching the input from baseline to more complex scenarios in the following table 4 we summarize the averaged statistical improvement percentages for each reservoir under s2 and s3 note that the improvement percentages in table 4 are averaged across all employed ai dm models for each reservoir in another set of results table 5 we carried out a similar comparison and obtained the averaged statistical improvement percentages for each employed ai dm model differs from the results in table 4 the improvement percentages shown in table 5 are averaged across all 33 reservoir cases and the percentages are organized for each ai dm model in other words table 4 indicates how well the simulation of each reservoir could be improved by adding delayed inputs regardless of the employed ai dm methods while table 5 reveals how sensitive each ai dm model will respond to the additional inflow and storage information across different reservoir cases in both tables 4 and 5 a positive percentage implies an improvement with respect to the baseline scenario no 1 and a negative percentage means the statistical performance deteriorates as compared to the baseline s1 according to the results shown in tables 4 and 5 there is a consistent observation that when delayed inflow and storage are added to the employed ai dm models the statistical performance will improve in general specifically according to the s2 results in table 4 the corr rmse nse kge pbias and rse values will improve 4 8 10 9 14 7 8 2 90 8 and 10 9 respectively when averaging across all reservoir cases if the ai dm model inputs are further extended to include the two step delayed inflow and storage values the averaged model performance for all studied reservoir could further improve up to 7 6 16 5 28 12 8 96 1 16 5 and 26 4 for corr rmse nse kge pbias and rse table 4 last row respectively according to tables 4 and 5 we observe that the additional inflow and storage inputs with delayed time series will consistently improve the outflow simulation accuracy except for the reservoir sco according to the averaged improvement percentages shown in the last two columns of table 4 we can conclude that in most of the reservoir cases the statistical performance under s3 are better than that under both s1 and s2 though the magnitudes of improvement could slightly vary from one case to another according to the results shown in table 5 when averaging the statistical improvement percentages across all studied reservoirs most of the ai dm models show similar improvement patterns as delayed reservoir inflow and storage information being added as additional model inputs however we notice that the predictive performance of the lstm model may either deteriorate or improve when adding delayed information table 5 which is subject to the study case and employed statistical measurement in other words there is no consistent improvement or deterioration of the lstm model as we observed from other ai dm models when switching from s1 to s2 and to s3 specifically when using the lstm model the rmse and pbias values have been improved however the corr nse and kge values show some levels of deterioration in comparison the linear model ridge model knn models and tree based models demonstrate consistent improvements when adding delayed information we suspect this is because the lstm model by design could take the current inflow and storage values as well as its prior values as model inputs to estimate the outflow at the current time step therefore manually setting delayed inflow and storage volumes as additional lstm inputs did not essentially promote the model accuracy other employed ai dm models do not carry the same feature as the lstm model and the delayed information is associated with complementary predictability than the original model inputs in the baseline s1 furthermore according to the results in table 5 the corr rmse nse kge pbias and rse values are improved by 5 8 10 5 12 7 8 8 91 1 and 10 5 respectively for all models under scenario no 2 the same statistical measurement values are further improved by 8 9 16 6 22 8 13 5 104 8 and 16 6 respectively when switching the model simulation to s3 according to the last two columns in table 5 similar patterns are observed when comparing the percentage improvements for all statistical measures under s2 and s3 beyond the statistical comparison among different scenarios in the following table 6 we further summarize the best and the second best performing models for each statistic over the validation periods the best performing models are identified by comparing how many reservoir cases that a particular model can outperform other models on each individual statistical measurement for example according to table 6 under the baseline scenario no 1 both rf and lstm model could generate the best corr values over the validation period for 10 out of the 33 employed reservoir cases therefore rf and lstm models are identified as the best performing models under s1 and for the corr category the second best model under the same statistic measure e g s1 and corr is the svr model with radial basis kernel function svr rbf which outperforms 4 out of 33 studied cases using the same logic we summarize the best and the second best performing models as well as the number of reservoirs that the corresponding model prevailed other employed ai dm models in the following table 6 according to the results in table 6 both rf and lstm models are identified as the best and second best performing models under scenario no 1 s1 respectively they prevailed the highest and second highest numbers of reservoirs than other employed models this result indicates that both rf and lstm models are more reliable and transferable than other models when simulating reservoir outflow using the current time step inflow and storage volume as model inputs the performance of the rf and lstm model are consistent across different statistical measurements except in two cases when the svr rfb and knn 3 model outperformed 4 and 6 reservoirs out of 33 studied cases and ranked as the second best performing model under s1 respectively however when comparing the number of prevailing cases under s2 and s3 both the rf and lstm model will no longer perform as the best and second best models instead the mlp model with hyperbolic tangent activation function e g mlp tanh becomes the best performing model and the xgboost tree model becomes the second best performing model across different statistical measurements the mlp tanh model shows slightly better performance over the xgboost tree model for statistics corr nse kge and rsr while the xgboost tree algorithm outperforms the mlp model in kge measurement under both s2 and s3 this result indicates that the performance of both mlp and xgboost tree models are stable and consistent across different statistical measurements when additional delayed information to simulate reservoir outflows in addition the total number of reservoirs that the mlp tanh model prevails other ai dm models slightly increased from 10 11 11 and 11 for statistics corr nse kge and rsr under s2 to 12 12 12 and 12 under the s3 respectively similarly the total number of reservoirs that the xgboost tree model outperforms others also increases from 7 7 7 and 7 for the same statistics under s2 to the values of 8 9 9 and 9 under s3 this result implies that the reliability of the mlp tanh model and xgboost tree model is slightly improved with the increases of input complexity combined with the findings from earlier tables 4 and 5 we further infer that these two models are capable of handling additional delayed information and can derive consistently good statistical performance with continuous improvements over the baseline input set under scenario no 1 the results in table 6 also show that the svr model can generate the best pbias values that are closer to zero than other ai dm models for 7 out of 33 studied cases under s2 and prevails other models for 7 out of 33 reservoirs under s3 respectively for the rsr measurement both of the knn and svr models sometimes could outperform other models and succeed in the largest number of reservoirs under different input scenarios of s1 3 however none of them could obtain better statistics than the mpl tanh the lstm the rf and the xgboost tree model with respect to all other commonly used statistical measurements corr rmse nse and kge 5 discussion for most of the hydrological time series it is likely that the water flow either natural flow or controlled flow e g the reservoir outflow is continuous and the time series inevitably have a certain level of autocorrelation with respect to previous time steps one of the advantages of ai dm models is the flexibility of handling additional inputs for data classification and prediction echoing the presented simulation results in section 4 we can reasonably expect that when adding delayed inflows and storage volumes in previous days the ai dm model could better capture the variability of reservoir outflows at the current time step similar correlation studies were available in the literature that the reservoir outflow decisions are also related to previous inflow and storage conditions up to two days in a retrospective manner hejazi et al 2008 zhao et al 2012 specifically our designed scenarios no 2 and 3 experiments fig 2 will answer the question of whether the employed ai dm models with additional and delayed inflow and storage could achieve better or worse performance over the baseline scenario no 1 the experiments with additional delayed ai dm model inputs s2 and s3 can help us further investigate the flexible uses of different ai dm models in simulating the reservoir outflow decisions the results in table 6 indicate that the lstm and rf model could effectively use limited information from the inflow and storage at the current time step to simulate reservoir outflows by achieving the best statistical measurement values over a larger number of study cases than other ai dm models however when manually adding delayed input information the performance of mlp models and xgboost tree based models could be significantly improved and even outperform the lstm and rf models in the baseline scenario in other words the mlp and xgboost tree models are less sensitive to the training information as compared to other ai dm models as long as the proper information is used to train the model the mlp tanh and xgboost tree models performance stay competitive and stable when switching input scenarios from no 2 to no 3 this finding indicates that the selection of ai dm models should rely on what information being used as predictors during the model training process and what statistical measurements are employed to evaluate the model performance after training in general there is no single model that could consistently outperform other ai dm models with respect to all possible statistical measurements and across all studied reservoir cases the large scale comparison studies in this paper are intended to explore the pros and cons of different ai dm models in the field of hydrology and water resources based on our experiment results and findings ai dm model practitioners shall investigate as many similar cases as possible and include multiple evaluation statistical measurements before putting them into real world application diligent quality control and evaluation study of different ai dm models will help operators comprehensively understand which model works well under what evaluation criteria we believe that it is also highly subjective to simply claim the superiority of one particular ai dm model based on a limited number of statistical measurements as well as draw a conclusion from only a few study cases our current experiments also indicate that though the nature of the problem could be similar e g estimating reservoir outflows using inflow and storage the performance of the ai dm models will likely be significantly different when the training data changes parameter changes and the core regression techniques changes it is also questionable to infer the performance of the ai dm model based on one or a limited number of case studies to a large number of other untested cases without any evaluations most ai dm models predictive performance has uncertainty and the obtained statistical measurements may also have large variations fig 6 among different models the prediction uncertainty and variability are inevitable because the ai dm model purely relies on the training data regardless of the physical dynamics and mass continuity kim et al 2021 there is no guarantee that the success of one or few case studies can become sufficient evidence that the same model functioning and behavior will occur in other study cases the authors would like to make the point clear that though ai dm models are popular and powerful tools in many current studies of hydrological simulation and reservoir operation the pros and cons of each model still remain not fully understood as evidenced in our simulation results figs 3 5 not always one type of ai dm model will consistently perform well in all study cases and therefore large scale experiments such as 1 the one presented in this study that intercompares various models on a large number of case studies or 2 the investigation study to directly compare ai dm models with physical rule based or process based modeling schemes are critically needed before the practical uses of ai dm models in any field due to the limitation of accessing the rule curves of the 33 reservoirs this study only serves as the former type of comparative study a further comparison of the simulated ai dm models against rule curves as well as the examination of how ai dm simulated reservoir release decisions could reasonably meet all types of hard engineering constraints soft operation limits downstream supply and environmental water and hydroelectric power demands and regional ordinances are out of great importance to promote the practical applications of ai dm models furthermore we suspect that the ai dm model performance are also related to the elevations of the dam maximum capacity of the reservoir forebay storage as well as the primary functionalities of each reservoir table 2 for the influence of elevation if taking a detailed analysis on some of the reservoirs with the highest elevations for example tpr fig 5i dil fig 3e sjr fig 5f with elevations of 2847 m 2751 m and 2725 m respectively we notice most ai dm models could produce consistent results with the normalized standard deviation above and higher than 0 5 the circular lines in all taylor diagrams in contrast when it comes to some of the reservoirs with the lowest elevation for instance ebr fig 3f pin fig 4k with elevations of 1323 m and 1427 m respectively we observe that results produced by different ai dm models are having a larger discrepancy among each other this is also evidenced by the normalized standard deviation shown in ebr fig 3f and pin fig 4k that the color dots have a larger spread in the taylor diagram than other reservoirs in other words we infer that the reservoirs at lower elevations are more complex to manage due to the influences of water routing in upstream river basins and the possible discharges from reservoirs in higher elevations note that our finding is not contradictory with the conclusion about the impacts of elevation on reservoir discharge simulation from a prior study on reservoirs in california yang et al 2016 specifically the elevations of reservoirs in yang et al 2016 are significantly lower than ours and their studied reservoirs are more closer to water consumption areas such as irrigation and residential districts the reservoirs in our study region are also believed to be dominated by seasonal snowmelt mountainous hydrology and natural streamflow instead of the direct reservoir refills due to the atmospheric river events that bring heavy precipitation over the northern california region regarding the influences of storage capacity we infer that the larger the maximum reservoir storage the more challenging the ai dm models could capture the human s discharge decisions and vice versa this speculation is drawn based on the statistical performance of the following four reservoirs the reservoir cases of fgr and ebr with maximum storage capacities of 4673 532 million m3 and 2547 149 million m3 table 2 respectively which are the two largest reservoirs in this study in contrast the two smallest reservoirs are also analyzed i e hnr and cau with storage capacities of 6 685 million m3 and 9 707 million m3 table 2 respectively according to table 3 the averaged nse values derived by all ai dm models for reservoir case fgr ebr hnr and cau are 0 235 0 496 0 690 and 0 918 respectively the former two i e the largest two reservoirs are significantly lower than that from the latter two i e the smallest two reservoirs the same observation occurs in the kge evaluation in which the averaged kge values across all ai dm models for reservoir case fgr ebr hnr and cau are 0 286 0 583 0 607 and 0 877 respectively in addition we further noticed in the two largest reservoirs fgr and ebr at least one ai dm model performed poorly which is shown in the min sub row under each reservoir row in table 3 for nse the lowest values for fgr and ebr are 0 133 and 0 006 respectively the corresponding worst performing model for the two smallest reservoirs hnr and cau can produce an nse value of 0 256 and 0 885 respectively a similar pattern is also observed in other statistical measures summarized in table 3 with this understanding it is likely that larger reservoirs are associated with more complex constraints natural environmental variabilities and operating criteria which conjunctively place a higher challenge for the applications of ai dm models on the contrary smaller reservoirs tend to be easy to be managed and are more flexible in adjusting to variations of reservoir inflows and changing reservoir storage volumes therefore different ai dm models have stronger advantages in capturing the human s decision making process and effectively simulate the reservoir outflows than the applications on reservoirs with larger maximum capacities in addition reservoir functionality and its primary purpose also play an important role in defining the complexity of the decision making process both of the simulated and observed releases show a certain pattern in our studied cases from the obtained results figs 3 5 we observe that most of the flood control reservoirs table 2 have a distinct seasonal variation in the controlled reservoir outflows examples include the reservoir cases of fon fig 3j fgr fig 3i gmr fig 3k val fig 5k etc among these cases there is a clear periodic cycle repeating itself from one year to another throughout the validation periods most of the employed ai dm models are able to well capture these changes and identify that this is due to the operation to empty reservoir storage for the required flood control rooms during fall winter time before the spring snowmelt for reservoirs with the functionality of hydropower generation the water diverted to the powerhouse is also included in the total outflow discharge simulation and observation however we notice in three of the reservoirs with hydropower functionalities i e cry fig 3c fgr fig 3i mpr fig 4j the observed and simulated reservoir outflows show many non smooth variations at low flow regimes this may be due to the corresponding hydropower generation during peak hours or intermittent hydroelectric supplies to meet the energy demands or the provision of spinning reserves in the power grid all of these are the benefits of hydroelectric power and reservoir systems which could be flexibly turned on and off quickly and effectively to stabilize the power grid variations ding et al 2021 one drawback of our current study is that the total number of reservoirs employed is still limited across the u s and worldwide there is a variety of reservoirs with different functionalities length of data records different climate and weather conditions physical settings of how water being controlled and releases and the sources of reservoir inflows e g either generated from snowmelt or direct runoff from upper watersheds based on specific features of the reservoirs the experiment setting could be very different when applying the ai dm models when designing model inputs such as to include snowmelt rainfall lake evaporation water losses upstream water level or reservoir downstream demand information etc the work presented here may only cover few subsets of reservoir features and there is no possible way to across validate the model performance over an unlimited number of other reservoirs worldwide nevertheless the way of how we apply the ai dm models and test out the model sensitivity and variabilities are universally adaptable to other study cases the employed experiment setting is based on the common knowledge that the reservoir outflow decision is governed by the rule curves and is determined by the water conditions in reservoir forebay afterbay and storage etc given that all existing rule based simulation schemes are more or less built upon this key philosophy we suggest that the uses of ai dm models to assist reservoir operation shall not discard these underlying physics with the help of ai dm models the investigation of whether additional decision factors such as local hydrology global climate interactions anthropogenic influences policy and social economic variables are made possible we highly believe that the ai dm models are and will continue to be a promising tool in advancing the research frontier of the cross disciplinary studies between computer and environmental sciences the usefulness of ai dm tools is not only limited to the topic of reservoir operation but also beneficial to a broader aspect of hydrology meteorology climate and integrated water resources management another limitation in this current study is the lack of a full spectrum of sensitivity analysis on the model s structural parameters and the investigation of available prior post ai enhancement techniques on one hand different ai dm models are associated with various model parameters such as learning rate other activation functions number of hidden layers and hidden nodes for mlp models maximum tree depth maximum for tree based models other kernel functions for svm models other numbers of neighbors in the knn models etc the current study only selects a limited number of default settings when implementing these ai dm models before conducting the outflow simulations for the 33 reservoir cases in this study an initial test has been conducted to pre select the essential model parameters in order to control the level of overfitting for the employed ai dm models however in the presented simulation results we still observe some overfitting or underfitting phenomenon in some of the reservoir cases a full spectrum of model parameter sensitivity tests is highly encouraged before applying a particular ai dm model in practice which is another grant challenge the operator faced when applying ai dm models on the other hand the employed ai dm models in this study are rather the original and standard applications of the popular models in computer science and we are not able to fully examine all possible ai dm models with various customized further developments training algorithms and prior post procedures the number of different variations and successors of popular ai dm algorithms is countless and each version may have its own strengths in a specific application for example fuzzy inference system adnan et al 2019 could be jointly used with mlp models to adaptively improve the predictive performance wavelet theory could also be embedded in ai dm models to refine more representative predictors esmaeilzadeh et al 2017 adding heuristic optimization algorithms could increase the model training accuracy but with a sacrifice of computational speed shamshirband et al 2020 yang et al 2017a yuan et al 2018 and hybrid modeling framework or model ensemble techniques could further reduce the prediction uncertainty of different ai dm models tao et al 2018 yang et al 2020b we suggest future ai dm model enhancements and applications in reservoir operation hydrology and water resources management shall at least include 3 5 standard and original machine learning algorithms with different core regression techniques for the purpose of comprehensive comparison last but not least the performed large scale comparison experiments in this study indicate a good potential of different ai dm models in accurately simulating the controlled reservoir outflows using flexible input designs which are evidenced by the simulated 33 reservoirs outflows and their corresponding statistical performance figs 3 5 tables 3 5 and supplementary material tables 1 33 the flexibilities of ai dm models in taking various human designed input features and identifying the intrinsic relationship between decision and target variables remain as the main advantages of ai dm models many implicit hard to find and internal causal relationships within the natural systems could be discovered and quantified through the proper uses of suitable ai dm models comparison studies especially on applying ai dm models with hydrological and environmental data shall include as many mature statistical measurements as possible to thoroughly examine different aspects of time series i e biases correlation extremes flow regimes seasonality trends etc 6 conclusion in this study a total number of 12 ai dm models with different parameterizations are employed to simulate the daily reservoir outflows of 33 reservoirs over the upper colorado region in the united states we designed three model input scenarios for model training and evaluation using the reservoir inflow storage and their corresponding delayed time series as inputs a total of six commonly accepted statistical metrics were used to quantitatively measure the performance of different ai dm models under three simulation scenarios overall the simulation results are satisfactory and the employed ai dm models could achieve high statistical performance in most of the study cases over the validation periods however the performance of different ai dm models may significantly vary based on the studied reservoir cases the employed regression techniques the statistical measurements evaluated as well as the characteristics of the reservoir i e elevation size and the primary functionality a number of study cases and scenarios were further examined whereas the simulation generated by different ai dm models are with relatively low statistical performance based on our experiments the following specific conclusions are drawn 1 different ai dm models may have individual strengths and weaknesses in simulating the reservoir outflows and assisting reservoir modeling no single model could consistently outperform others in the 33 reservoir cases compared in this study the model s performance is likely to vary by the modeling schemes by the ways of training data structure as well as by the statistical measurement used it is suggested to include multiple statistical measurements and as many ai dm models as possible to comprehensively understand the pros and cons of different ai dm models when applying ai dm models to reservoir simulation 2 the rf and lstm models are found to be the best and second best performing model when using the inflow and storage at the current time step to simulate the reservoir outflows under the baseline scenario no 1 however the mlp model with hyperbolic tangent activation function and the xgboost tree algorithm appears to be the most reliable and stable models when additional delayed inflow and storage are further included in the model training process scenarios no 2 and 3 we infer that the mlp model and xgboost tree model are more capable of handling large and complex training data than other models and the model s predictive performance are found to be more stable than other employed ai dm models in the employed 33 reservoir cases 3 reservoir elevation maximum capacity primary functionality local hydrology could conjunctively affect the effectiveness of ai dm models we found that the reservoirs located at a lower elevation are more challenging to simulate and the model daily outflow simulations tend to disagree with each other when different ai dm models are used this finding is possibly due to the high variabilities in water routing in upstream river basins and the influences of discharge from an upper source reservoir lake in addition we also found out that larger reservoirs are harder for the employed ai dm models to capture the patterns of human controlled release decision and vice versa in our limited study we observe that the statistical performance of ai dm results over the two largest reservoirs are significantly poorer than that of the two smallest reservoirs the functionality of reservoir and its primary purpose could also affect the daily and seasonal variations periodic patterns are observed in both observed and simulated reservoir outflow time series the applications of ai dm models should comprehensively consider these factors in practical uses 4 the delayed information reservoir operation time series could significantly increase the model performance in general according to the comparison between s2 s3 versus the baseline s1 we identify that lstm and random forest model are less sensitive to the manually delayed reservoir storage and inflow time series than many other models but the lstm and random forest model may not outperform other popular and standard ai dm models under the scenarios that additional decision variables are added as model inputs in contrast properly configured mlp i e ann and xgboost models are more likely to be benefited from adding additional model inputs and produce better simulation results based on that we suggest practitioners who use ai dm models to simulate reservoir operation shall test out possible ancillary information and additional model inputs that are closely related to the reservoir decision making process and select the most suitable ai dm models in corresponding to the training data and study cases 5 from this study we also conclude that the advantage of ai dm models lies in their flexibility in incorporating different types of input data and identifying the implicit relationship between features and target variables the traditional rule based modeling scheme might be limited in this regard as the process based governing equation can only take fixed inputs it can be inferred that with different reservoir and problem settings practitioners of ai dm models can identify the most suitable combination of input data structure and model parameterizations to obtain the best possible model outcomes but this process may require a large number of trial and error experiments there are currently not commonly accepted golden rules on how to design the implementations of different ai dm models in assist of reservoir operation at this stage of research and scientific investigation large scale comparison and model evaluation studies are still needed in order to fully understand the pros and cons of different ai dm models and to identify the best way of using ai dm models to help manage the surface water resources understand the local hydrology and water cycle as well as to support the decision making of water infrastructures such as dams and reservoirs 6 future work may include 1 the direct comparison between ai dm models with rule based reservoir simulation and examination of the predicted ai dm time series will meet various types of engineering hard and soft constraints demands and operation goals 2 the development of hybrid modeling schemes that combine the physical rule based model with ai dm model to enhance simulation accuracy and our transcendent capability in the preparedness of possible weather and climate extremes 3 the quantification study of reservoir inflow uncertainty and the corresponding experiments of multiple model ensemble techniques to reduce the uncertainty and discrepancy among the simulation results from different ai dm models 4 incorporate ensemble hydrological forecasts to guide reservoir operation given that one of the key strengths of ai dm model is its structural flexibility that allows the inclusion of various types of forecast information to be added to support decision making some recent studies over the tres marías dam mainardi et al 2016 the yuvacik dam uysal et al 2018 uysal et al 2020 and salto grande dam sinnige and alvarado montero 2019 have demonstrated the combination of ai dm models with model predictive control scheme could significantly improve the reservoir system performance in terms of flood reduction by more than 25 compared to deterministic forecast based techniques there are many innovative and new research domains that the ai dm model could potentially contribute nevertheless the fundamental model comparison and comprehensive evaluation are out of great importance to position the role of ai dm models in assisting reservoir operation and to better reveal the merits of these new technologies in solving any classical engineering and environmental science problems credit authorship contribution statement tiantian yang conceptualization methodology software writing original draft supervision funding acquisition lujun zhang data curation visualization writing review editing formal analysis taereem kim visualization software validation writing review editing yang hong writing review editing di zhang writing review editing qidong peng writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is partially supported by the u s department of energy doe prime award de ia0000018 this work is also financially supported by the national key research and development program of china no 2018yfe0196000 the material is based upon work supported by the national science foundation under grant no oia 1946093 and its subaward no epscor 2020 3 and the national science foundation under grant no nsf1802872 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126723 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4196,heterogeneous conglomerates preserving matrix and clasts caused the complex distribution of pore spaces and can eventually influence fluid flow to evaluate the influence of complex heterogeneous features in conglomerate rock on both pore distribution and fluid flow two conglomerate cores were analyzed using x ray computed tomography x ray ct conglomerate cores core i and core ii characterized by different distributions of matrix and clasts represented highly heterogeneous features in core i clast supported conglomerate clasts were evenly distributed through the entire core but in core ii matrix supported conglomerate large sized clasts were dominantly positioned at the bottom of core such heterogeneous features caused differences in volume of pores core i 7 409 54 m m 3 and core ii 17 525 83 m m 3 based on x ray ct image analysis the numerical model single phase fluid simulation was implemented to evaluate the permeability of conglomerate at different scales especially due to inner heterogeneous features e g matrix clasts and cracks in conglomerate cores permeabilities were changed significantly dependent on the selected location and size of sub domains representing a few centimeters k h and k v 1 18 10 12 to 1 01 10 10 m 2 this suggests that the effect of small scale heterogeneity should be evaluated carefully when heterogeneous conglomerate cores were analyzed additionally the measurement scale for x ray ct analysis should be large enough to capture such heterogeneous features e g the size of clasts keywords conglomerate heterogeneity x ray ct petrophysical properties 1 introduction conglomerates consisting of both gravel size 2 mm rock fragments and matrix composing of sand size 2mm and silt size 1 16 mm grains are characterized as highly heterogeneous rocks because of various sizes and distribution of the clasts different mineral compositions dependent on parent rocks unsorted features of clasts and matrix and variation of sedimentary facies boggs jr 2014 ziqi et al 2007 such heterogeneous characteristics resulting from complex distribution of pore spaces eventually influence fluid flow for example the presence of clasts causes almost zero porosity in the conglomerate rock and thus fluids preferentially migrate through the matrix clarke 1979 due to the highly heterogeneous features conglomerate rocks have not been studied much for the purpose of oil and gas exploration and geological co2 sequestration gcs compared to the relatively homogeneous sandstone rocks that preserve high porosity and permeability changfu et al 2011 kigam 2017 michael et al 2010 zhou et al 2018 despite such complex hydrogeological characteristics of conglomerate rocks recent studies targeted conglomerate rocks as potential co2 storage formations for the janggi gcs project kigam 2017 kim et al 2018 the janggi gcs project was the first onshore pilot scale co2 injection 10 000 tons project in south korea launched to inject co2 at the janggi conglomerate sedimentary formation previously conglomerate cores were collected from the janggi conglomerate sedimentary formation in addition kim et al 2018 conducted co2 core flooding experiments and observed that co2 transport in the heterogeneous conglomerate core was significantly different from homogeneous sandstone cores in order to characterize hydrogeological properties of cores laboratory or in situ experiments are typically conducted laboratory tests include thin section or mercury injection analyses to measure the porosity and core flooding tests to measure the permeability lock et al 2012 schmidt and mcdonald 1979 additionally in situ experiments such as the pumping or slug and bail down tests are used to measure field scale permeability while observing changes in the hydraulic head or pressure de lucena et al 2016 ingebritsen and sanford 1999 although such experiments provide a good estimation of porosities and permeabilities within their own measurement scales setting up the laboratory facilities or preparation of in situ field experiments are often tedious laborious and expensive the x ray computed tomography x ray ct technique originally developed for medical purposes in the early 1970 s has been employed widely to reconstruct 3d heterogeneous features preserved in porous rocks arns et al 2005 blunt et al 2013 callow et al 2018 thomson et al 2019 recently the effectiveness of x ray ct technique has been evaluated and applied to various subsurface studies such as soil science petrology petroleum geology paleontology and sedimentology because it is a non destructive analysis offering 3d visualization arnold et al 1983 haubitz et al 1988 mees et al 2003 petrovic et al 1982 raynaud et al 1989 renter 1989 vinegar 1986 vinegar and wellington 1987 the x ray ct technique is useful to evaluate subsurface rocks typically hosting fluids by estimating their porosity and permeability van geet et al 2000 despite such benefits the x ray ct technique has the restriction of measurement scale related to the resolution for example when medical or industrial ct scanners were utilized pores smaller than the volumetric pixel or voxel of x ray ct images can not be identifiable therefore in order to analyze the micropores preserved in dense carbonate rocks and pelagic sediments high resolution micro ct images taken from a micro focused ct scanner were necessary mees et al 2003 nevertheless to obtain such high resolution micro ct images the size of the targeting object must be reduced to a few cm in size du plessis et al 2017 kroll et al 2009 neuser and suppes 2007 in this situation a reduced object might not be able to capture heterogeneous characteristics preserved in the original rock specifically for highly heterogeneous conglomerate rocks highlighted by complex distributions of clasts and matrix the reduced size of the sample causes a loss in the effective representation of the spatial heterogeneity in this study highly heterogeneous conglomerate rocks were imaged with the x ray ct technique to evaluate the influence on fluid flow two conglomerate cores highlighting different distributions of matrix and clasts were analyzed to quantify the pore size and their distribution subsequently the absolute and effective porosities were identified and their spatial patterns were compared to the distribution of both the clasts and matrix 2 janggi conglomerate core 2 1 geologic description of the janggi basin pohang republic of korea the janggi basin located in the southeastern region of south korea is close to one of the largest co2 emission sources in south korea such as steel making plant emitting co2 with a rate of approximately 70 000 tons per day fig 1 a the distance between the plant and potential co2 injection site is approximately 15 5 km huh et al 2010 geologically the janggi basin is composed of non marine sedimentary and volcanogenic rocks divided into the basaltic janggi group and beomgockri group tateiwa 1924 the k ar whole rock age between 19 and 22 ma of volcanic rocks and plant fossils within the sedimentary rocks indicate that the janggi basin was presumably formed in the miocene huzioka 1972 jin et al 1988 kim et al 1975 kim et al 1986 lee et al 1992 yoon 1992 based on the relative age and structural configuration such as the attitude i e strike and dip of beddings the janggi basin is classified into five sub basins called the guryongpo ocheon noeseongsan yeongam ri and yangpo blocks fig 1a cheon et al 2012 jung et al 2012 kim et al 2015 kim et al 2011 among them the noeseongsan block was evaluated as a potential site for geologic co2 sequestration due to its stratigraphy and geologic structures kigam 2017 kim et al 2015 the stratigraphy of the noeseongsan block consists of the lower beomgockri 265 to 1 000 m and upper janggi 0 to 265 m groups fig 1b the lower beomgockri group is composed of early miocene dacitic volcanic deposits and the upper janggi group is characterized by late miocene basaltic volcanic deposits son et al 2013 specifically the beomgockri group is subdivided into the seongdongri and janggi conglomerate formations kim et al 2011 according to kigam 2017 the janggi conglomerate has an average porosity and permeability of 0 161 and 8 41 10 15 m2 respectively and is interbedded with layers of mudstones dacitic tuffaceous sandstones conglomerates and conglomeratic sandstones the seongdongri formation overlying the janggi conglomerate formation is composed of dacitic tuff tuffaceous sedimentary rock and mudstone because of the relatively high ratio of volcanic materials and fine sediments in the seongdongri formation the value of permeability is expected to be lower than that of the janggi conglomerate formation additionally the uppermost noeseongsan basaltic rock including dominant eruptive lava flows intrusive basalt and tuffaceous breccia is expected to be impermeable 2 2 petrophysical properties of cores collected from janggi formation for the purpose of site characterization 7 boreholes jg 1 2 3 4 5 6 and 7 1 were drilled at the noeseongsan block to evaluate the hydrogeological properties porosity and permeability kigam 2017 utilizing the mercury and helium porosimeter a total of 321 porosities were measured at various depths from multiple boreholes fig 2 a the measured porosities ranged from 0 015 to 0 385 with the mean and standard deviation being 0 181 and 0 065 respectively in general the porosity was correlated with depth d 2 636 9 ϕ 1 132 2 r2 0 40 indicating that these sedimentary lithofacies experienced compaction processes nevertheless at certain depths a wide range of porosities was observed suggesting that these lithofacies evidently preserved heterogeneous features even within the same sediment unit fig 2a in addition to the porosities a total of 71 permeabilities were measured from selected cores in the boreholes jg 1 3 4 6 and 7 1 fig 2b after injecting the inert n2 gas volumetric flow rate q was measured when the steady state condition was reached then permeability k was calculated as follows 1 k q μ l a δ p where μ is gas viscosity l is the length of core sample a is the area of core sample and δ p is the pressure difference between inlet and outlet finally the measured permeabilities ranged from 0 01 10 15 to 89 0 10 15 m2 kigam 2017 kim et al 2015 kim et al 2011 porosities and permeabilities of 9 conglomerate 39 sandstone and 3 mudstone samples were plotted in the semi log graph fig 2c overall the porosities and permeabilities showed positive correlations the regression line of conglomerate red dotted line showed greater permeability than that of sandstone green dotted line at equivalent porosities in the case of mudstone the slope of the regression line was the largest but both porosities and permeabilities were significantly lower than those of conglomerate and sandstone based on experimentally measured permeability and porosity data the korea institute of geoscience and mineral resources kigam launched a site investigation for onshore co2 storage in the southern part of the noeseongsan block the janggi conglomerate dominantly consisting of conglomerate and sandstone was chosen as a potential co2 storage formation 2 3 petrophysical and mineralogical properties of janggi conglomerate in the jg 7 1 borehole specific to multiple cores sandstone mudstone and conglomerate cores collected at the jg 7 1 borehole which is considered to be a potential co2 injection well values of porosity and permeability ranged from 0 046 to 0 176 and 0 02 10 15 to 2 70 10 15 m 2 respectively kigam 2017 for conglomerate the mean and standard deviation of porosity were 0 131 and 0 035 respectively with the porosity ranging from 0 106 to 0 155 the mean and standard deviation of permeability were 1 0 10 15 m 2 and 0 9 10 15 m 2 permeability ranged from 0 03 10 15 to 2 70 10 15 m 2 for more detailed evaluation of the in situ janggi conglomerate two drilling core samples core i and core ii were obtained from the jg 7 1 borehole supplement fig s1 both core i and core ii were collected from depths between 936 and 967 m representing the janggi conglomerate formation red box in fig 1b both cores have an equal diameter 48 mm but the heights of core i and core ii were 133 04 mm and 146 59 mm respectively supplement fig s1 illustrates the photos of core i and core ii and associated vertical cross sections captured by the x ray ct scan the conglomerate cores highlighted distinct heterogeneous features even within the small size cores various sizes and shapes of clasts were ubiquitously distributed within the matrix for example in core i clast supported conglomerate clasts were evenly distributed through the entire core but in core ii matrix supported conglomerate large sized clasts were dominantly positioned at the bottom of core such different patterns of the matrix and clasts collected at the same borehole are expected to affect the co2 migration storage capacity and injectivity therefore in the following section the petrophysical properties for identifying fluid flow behavior were evaluated using x ray ct image analysis 3 methodology 3 1 x ray image acquisition and process to acquire the 3d x ray images a computed tomography ct equipment suited in the korea institute of geoscience and mineral resources kigam was utilized it is equipped with x ray tube of 225 kv and one flat panel detector with 2 048 2 048 pixels projections of the core samples were taken every 0 5 for angles from 0 to 360 using an exposure time of 2 s the voxel size of the reconstructed volumes of core i and ii was 62 μm and 58 μm respectively the voxel size lies between that of medical ct scanners 500 μm and micro focused ct scanners 1 5 μm the latter of which however operates at a low acceleration voltage 100 kv that cannot penetrate core sized samples after scanning unrefined x ray ct images of core i and ii preprocessing for cropping and filtering was performed to improve the accuracy and precision of the image analyses the cropping process removes unnecessary parts of x ray ct images and deletes zones where severe imaging errors or artifacts e g the margin of the core damaged core surfaces and beam hardening artifacts are preserved supplement figs s2a and s2b consequently cylindrical 3 d images with 40 mm diameter and 113 mm height were obtained from each core sample supplement figs s2c and s2d following the cropping process the beam hardening correction was conducted to improve the quality of x ray ct images the beam hardening artifact was observed during the preprocess of x ray ct images supplement fig s3a the average x ray intensity within voxels radially increased from the center 0 mm to the edge 20 mm red line in supplement fig s3b after the beam hardening correction the average radial x ray intensity became smoothed and normalized blue line in supplement fig s3b 3 2 segmentation and pore network model to identify the pore characteristics i e porosity pore size distribution and intrinsic permeability from the x ray images the segmentation process is essential this step involves the conversion of the reconstructed gray level 3d images to binary 3d images by determining the threshold value without any reference data the choice of a threshold value separating pores from the solid phase could be subjective to minimize such uncertainty the threshold value was determined from the experimentally measured porosity in detail the heterogeneous conglomerate rock consists of both clasts and matrix where large pore spaces are dominantly preserved in the matrix less than 10 10 10 mm3 of matrix samples were cut from the conglomerate rock and the porosity 0 173 of the matrix was experimentally measured with the mercury injection capillary porosimeter micp subsequently the global segmentation was conducted by way of adjusting the threshold values of the matrix represented by the gray scale level as similar to jin et al 2016 and okabe and blunt 2004 the adjusting process was repeated until the porosity of the matrix in x ray images was best matched with an experimentally measured porosity 0 173 to determine the targeted threshold value supplement figs s2e and s2f after the segmentation process the pore network model pnm was constructed to quantify pore characteristics such as pore number and pore surface area the pore structure and connectivity were identified by finding a skeleton within the pore space based on a hybrid method that combines thinning and distance maps youssef et al 2007 throughout the analysis of x ray ct images the voxelized pore spaces dividing into pore and throat were identified then the generated voxelized pore spaces were converted to spherical pore and cylindrical throat represented as the pnm supplement fig s4 while converting to the pnm the pore size was defined by the radius of the sphere having an equivalent volume of pore body i e the volume of pore segment 4 3 π r 3 to preserve characteristics of the pore volume 3 3 permeability estimation numerical model to numerically evaluate the permeability k of the conglomerate core consisting of matrix matrix with clasts or matrix clast and fractures within clasts a single phase fluid i e water flow simulation was conducted at extracted sub domains representing the connected pore spaces supplement fig s5 prior to flow simulations the representative sub domains i e matrix matrix clast and fractures within clasts were extracted at locations where each sub domain represents its pore characteristics fig 3 the k was calculated by solving the stokes equation as shown below 2 u 0 3 μ 2 u p here u is the velocity vector of the fluid flowing through the porous domain p is the pressure and μ is the viscosity of water 1 10 3 p a s once the stokes equation is solved for fluid flow the global flow rate q through the porous domain is calculated and therefore darcy s law can be applied to solve for intrinsic permeability k 4 k q μ l δ p here q is the darcy s velocity μ is the dynamic viscosity l is the length of sub domain towards the flow direction and δ p is the pressure difference between the inlet and outlet boundary 4 results 4 1 quantification of porosities through x ray ct image analyses 4 1 1 analyses of pore sizes and distribution in core images fig 4 a and 4b different colors represented the pore distributions green yellow and red and clasts were highlighted by a light grey color additionally both the matrix and clasts were delineated by the representative sliced 2 d images as shown in fig 4c d e and f in core i where clasts were evenly distributed the matrix was preserved in between these clasts where numerous pores with different diameters were distributed fig 4a conversely relatively large sized clasts were concentrated at the bottom of core ii and thus most of the pores were observed at the upper part fig 4b by comparing these two cores the conglomerates were characterized as highly heterogeneous dependent on clast positions and sizes the number and volume of pores were different even in core samples collected from the same wellbore the analysis of x ray ct images revealed that the total number of pores was 59 255 and 216 787 within core i and ii respectively table 1 note that the number of pores was counted at relatively homogeneous portions where the matrix was mainly dominant fig 4a and b in both cores the pore diameters varied from 0 077 to 3 51 mm and the average 0 54 mm and 0 47 mm and standard deviation 0 21 mm and 0 17 mm of pore diameters differed in core i and ii respectively table 1 to investigate further the pore distribution was categorized into 3 groups group i green 1 0 mm 1 0 mm group ii yellow 2 0 mm and group iii red 2 0 mm from the pores 59 255 in core i the number of pores within group i was 57 221 96 57 and group ii and iii occupied 3 43 pore numbers 2 026 and 8 respectively indicating that most of the pores belong to group i similarly in core ii the total number of pores was prevalent in group i 98 98 pore numbers 214 567 the remaining pores in group ii and iii were 1 02 thus the difference in the total number of pores between core i and ii was 157 532 such a large difference in the total number of pores resulted from the difference in small pores categorized in group i finally core ii possessed more pore volume and porosity 17 525 83 m m 3 and 0 125 than core i 7 409 54 m m 3 and 0 052 the difference in pore volume and porosity was 10 116 29 m m 3 and 0 071 which is also caused by the difference of pore volume in group i the difference of pore volume and porosity in group i between core i and ii was 10 115 81 m m 3 and 0 071 heterogeneity according to the distribution of clasts and matrix in core i and ii resulted in the difference in pore space highlighting that the presence of the matrix primarily controlled the amount of pore space 4 1 2 absolute and effective porosities both absolute for a total pore space and effective for a connected pore space porosities were calculated along the vertical profile z axis in both core i and ii after separating connected pore space from the total pore space porosities in x y planes were calculated fig 5 a and 6a to obtain connected pores 2 parallel boundary layers i e top and bottom x y planes were selected and all voxels connected through at least one common edge between these 2 layers or 18 neighborhood were determined prado et al 2016 sufian and russell 2013 then connected voxels were calculated to be connected pore space the average absolute and effective porosities were 0 069 and 0 052 in core i and 0 131 and 0 125 in core ii respectively which were within the range 0 046 to 0 176 of the experimentally measured porosity as discussed in section 2 3 fig 2 in core i the absolute red line and effective blue line porosities varied from 0 025 to 0 132 and 0 003 to 0 126 respectively both absolute and effective porosities tended to decrease where clasts were dominant although both porosities showed a similar pattern along the vertical axis discrepancy exists at certain parts due to the presence of isolated pores for example a difference of 0 043 between absolute and effective porosities was observed at a height of 62 7 mm absolute porosity 0 066 and effective porosity 0 023 fig 5b which is the sliced image at 62 7 mm height showed the largest portion of isolated pores red color in core i in core ii absolute and effective porosities ranged from 0 015 to 0 204 and 0 004 to 0 196 respectively fig 6 a both porosities were high in the upper part where the matrix was dominant but approached almost 0 porosity at the bottom where large clasts were positioned compared to core i core ii revealed that the portion of isolated pores was small the difference between the average of absolute and effective porosity was 0 017 and 0 007 in core i and ii respectively fig 5a and 6a in core ii the largest portion of isolated pores was measured at a height of 1 8 mm where a crack existed within the clast here the difference between the absolute and effective porosity was 0 031 fig 6a the isolated pores or cracks within the clast at a height of 1 8 mm are shown in fig 6b due to the presence of isolated pores and clasts at the bottom the effective porosity was much smaller the connected pores were rarely observed but primarily distributed at the core margin 4 2 quantification of intrinsic permeabilities a total of 11 sub domains with varying porosity 0 043 to 0 174 were extracted from core i and ii fig 3 they were categorized into 3 groups matrix mi and mii matrix clast mci and mcii and fractures within clasts fi and fii fig 7 b d in these sub domains the horizontal and vertical k were numerically evaluated and they varied from 1 18 10 12 to 1 44 10 10 m 2 fig 7a and table 2 in the sub domains representing the matrix c1mi c1mii c2mi and c2mii connected pores were distributed almost uniformly in core i and ii with ϕ being equivalent to 0 173 fig 7b the numerically calculated kh and kv ranged from 6 40 10 12 to 3 87 10 11 m 2 with the smallest standard deviation of 9 89 10 12 m 2 as presented in table 2 additionally the mean anisotropic ratio i e the mean ratio of k v to k h was 0 70 indicating that the water could migrate through any direction without preferential pathways in the matrix in the sub domains representing matrix clast c1mci c1mcii c2mci and c2mcii the ϕ ranged from 0 093 to 0 158 which was smaller than that of the matrix sub domain and had a greater variation fig 7c the calculated k showed a variation of 10 2 times from 1 18 10 12 to 1 01 10 10 m 2 table 2 clearly highlighting the different properties even when the sampling location of these sub domains was separated by a few centimeters unlike the matrix sub domain the matrix clast sub domain highlighted anisotropic characteristics mean anisotropic ratio γ m 0 583 due to the increase in heterogeneity i e addition of clasts finally in the fracture sub domain c1fi c1fii and c2fi fractures and connected pores were preserved within clasts as clasts occupied most of the sub domain the calculated porosities were the lowest ranging from 0 043 to 0 060 fig 7d additionally connected fractures and pores were aligned to a specific direction the calculated k was high even though ϕ was small both kh and kv ranged from 1 51 10 11 to 1 44 10 10 m 2 their average was 5 93 10 11 m 2 which was the greatest among the three sub domains matrix 1 31 10 11 m 2 and matix clast 3 83 10 11 m 2 table 2 and fig 7a interestingly in the fracture sub domain c1fi and c2fii γ over 1 kv kh was shown denoting that fractures extended vertically the effective k of the whole core measured from laboratory experiments was 10 3 10 4 times smaller than numerically calculated k such differences were presumably related to the low resolution of x ray ct images which was inevitably chosen to capture the heterogeneous distribution of matrix and clasts in conglomerate cores fig 3 for example in the laboratory core flooding test the fluid was able to migrate through connected micropores which cannot be captured in x ray ct images although clasts which have almost zero porosity can block fluid migration the existence of such micropores within matrix and clasts contributed to the slow migration of fluid during the core flooding test presumably resulting in small k through this analysis it should be noted that the laboratory core flooding test provided k without much information of the in situ pore distribution and connection alternatively the x ray ct images were able to capture the 3d distribution of pores visualizing their heterogeneous characteristics but such pores did not contain micro sized pores where fluids hardly migrated due to the low resolution of x ray ct images 4 3 quantification of flow patterns in heterogeneous conglomerate rocks according to the distribution of the matrix and clasts the size of pores pathway and velocity of the fluid significantly varied within the heterogeneous conglomerate core the magnitude and pattern of simulated velocities were delineated in connected pores representing the sub domains of c1mi c1mci and c1fi fig 8 in c1mi connected pores were evenly distributed and the fluid flowed through most pores fig 8a and 8d as the portion of clasts increased i e c1mci the number of flow pathways decreased fig 8b and 8e indicating that the fluid was unable to flow through clasts even if connected pores existed within both matrix and clasts fluid did not flow through such pores or the flow velocity within them was negligible this suggested that connected pores did not always serve as major fluid pathways highlighting the importance of pore connectivity throughout the entire domain finally fig 8c represents the fracture sub domain or c1fi wherein pores were preserved within the fracture as the fluid was concentrated through narrow pathways representing fractures the flow velocity was the largest among all sub domains fig 8f in addition to evaluating the influence of the matrix and clast ratio on fluid flow the measurement scale also influences fluid flow in highly heterogeneous conglomerate rock depending on the measurement scale the volumetric ratio of clasts to matrix can vary significantly in order to evaluate the scale dependent fluid flow an enlarged sub domain of en c1mi with dimensions of 20 20 20 m m 3 was selected fig 3 and fig 9 a the en c1mi included the sub domain of c1m1 highlighted with an inner bounding box fig 9b compared to c1mi the size of the en c1mi sub domain was doubled but the effective porosity was decreased by more than a factor of 5 c1mi 0 172 to en c1mi 0 036 this was because of the increase in the portion of clasts and isolated pores in en c1mi reflecting the influence of heterogeneity even in such a small scale the clast and isolated pores affected fluid flow by blocking it and thus the overall permeability was decreased from 1 13 10 11 m 2 to 1 05 10 12 m 2 the layered x ray ct images were selected at two different heights fig 9c and 9d where clasts isolated red and connected pores blue were delineated depending on the height the variation of clasts matrix and isolated connected pores was severe resulting in variation of fluid velocity fig 9b 5 conclusions in the study digital 3 d conglomerate cores were reconstructed using the x ray ct technique absolute and effective porosities were identified and their spatial patterns were evaluated according to the distribution of the clasts and matrix additionally by implementing the numerical single phase computation fluid dynamics approaches the permeabilities of highly heterogeneous conglomerate cores were estimated and subsequently compared to experimentally measured permeability at the core scale e g core flooding test finally scale dependent permeability was evaluated in the heterogeneous conglomerate core conglomerate cores displayed highly heterogeneous features the clasts were evenly distributed throughout core i while large sized clasts were dominant in core ii such unique heterogeneous features observed in these cores caused differences of the pore volume for example the pore volume of core ii 17 525 83 m m 3 was greater than that of core i 7 409 54 m m 3 and therefore the absolute and effective porosity of core i 0 069 and 0 052 was lower than that of core ii 0 131 and 0 125 at the sub domain scale the heterogeneous features were highlighted by significant changes in the porosity and permeability pores in the matrix sub domain were evenly distributed and well connected showing large porosity and permeability while as the portion of clasts increased porosity and permeability decreased and anisotropy increased in the fracture sub domain the porosity was the lowest but the permeability increased in addition to distinct changes in the porosity and permeability depending on chosen spatial locations within the core the heterogeneity of conglomerate caused scale dependent petrophysical properties this suggests that the effect of small scale heterogeneity should be considered when a heterogeneous conglomerate is analyzed another important aspect to consider for a successful application of x ray ct imaging to the analysis of heterogeneous conglomerate core is the identification of the relationship between resolution and object sizes high x ray ct resolution is required to decrease the object size however this will not preserve spatial heterogeneity in the conglomerate core to capture heterogeneous characteristics in the conglomerate cores a sufficiently large sized sample is necessary unfortunately large sized samples would inevitably result in a low voxel resolution of x ray ct images this study utilized ct images with low resolution i e voxel resolution 58 62 μ m however the low resolution could overestimate the pore size and cause an overestimation of permeability peng et al 2012 peng et al 2014 further increment of resolution for the ct images will be required to compensate for potential limitations caused by overestimation hence ct image analysis at multiple resolutions will be able to enhance the evaluation of spatial heterogeneity and petrophysical properties of the conglomerate cores botha and sheppard 2016 jiang and arns 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by basic science research program through the national research foundation nrf of korea funded by the ministry of education project number 2016r1d1a1b01008715 and 2021m2e1a1085187 by the korea environmental industry and technology institute keiti project numbers 2018002440003 by the demonstration scale offshore co2 storage project in pohang basin republic of korea funded from energy efficiency resources of the korea institute of energy technology evaluation and planning ketep which was granted financial resource from the ministry of trade industry energy republic of korea project number 20162010201980 the authors also appreciate financial support by the basic research project of the korea institute of geoscience and mineral resources kigam funded by the ministry of science and ict this research was partially supported by the graduate school of yonsei university research scholarship grants in 2020 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126736 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4196,heterogeneous conglomerates preserving matrix and clasts caused the complex distribution of pore spaces and can eventually influence fluid flow to evaluate the influence of complex heterogeneous features in conglomerate rock on both pore distribution and fluid flow two conglomerate cores were analyzed using x ray computed tomography x ray ct conglomerate cores core i and core ii characterized by different distributions of matrix and clasts represented highly heterogeneous features in core i clast supported conglomerate clasts were evenly distributed through the entire core but in core ii matrix supported conglomerate large sized clasts were dominantly positioned at the bottom of core such heterogeneous features caused differences in volume of pores core i 7 409 54 m m 3 and core ii 17 525 83 m m 3 based on x ray ct image analysis the numerical model single phase fluid simulation was implemented to evaluate the permeability of conglomerate at different scales especially due to inner heterogeneous features e g matrix clasts and cracks in conglomerate cores permeabilities were changed significantly dependent on the selected location and size of sub domains representing a few centimeters k h and k v 1 18 10 12 to 1 01 10 10 m 2 this suggests that the effect of small scale heterogeneity should be evaluated carefully when heterogeneous conglomerate cores were analyzed additionally the measurement scale for x ray ct analysis should be large enough to capture such heterogeneous features e g the size of clasts keywords conglomerate heterogeneity x ray ct petrophysical properties 1 introduction conglomerates consisting of both gravel size 2 mm rock fragments and matrix composing of sand size 2mm and silt size 1 16 mm grains are characterized as highly heterogeneous rocks because of various sizes and distribution of the clasts different mineral compositions dependent on parent rocks unsorted features of clasts and matrix and variation of sedimentary facies boggs jr 2014 ziqi et al 2007 such heterogeneous characteristics resulting from complex distribution of pore spaces eventually influence fluid flow for example the presence of clasts causes almost zero porosity in the conglomerate rock and thus fluids preferentially migrate through the matrix clarke 1979 due to the highly heterogeneous features conglomerate rocks have not been studied much for the purpose of oil and gas exploration and geological co2 sequestration gcs compared to the relatively homogeneous sandstone rocks that preserve high porosity and permeability changfu et al 2011 kigam 2017 michael et al 2010 zhou et al 2018 despite such complex hydrogeological characteristics of conglomerate rocks recent studies targeted conglomerate rocks as potential co2 storage formations for the janggi gcs project kigam 2017 kim et al 2018 the janggi gcs project was the first onshore pilot scale co2 injection 10 000 tons project in south korea launched to inject co2 at the janggi conglomerate sedimentary formation previously conglomerate cores were collected from the janggi conglomerate sedimentary formation in addition kim et al 2018 conducted co2 core flooding experiments and observed that co2 transport in the heterogeneous conglomerate core was significantly different from homogeneous sandstone cores in order to characterize hydrogeological properties of cores laboratory or in situ experiments are typically conducted laboratory tests include thin section or mercury injection analyses to measure the porosity and core flooding tests to measure the permeability lock et al 2012 schmidt and mcdonald 1979 additionally in situ experiments such as the pumping or slug and bail down tests are used to measure field scale permeability while observing changes in the hydraulic head or pressure de lucena et al 2016 ingebritsen and sanford 1999 although such experiments provide a good estimation of porosities and permeabilities within their own measurement scales setting up the laboratory facilities or preparation of in situ field experiments are often tedious laborious and expensive the x ray computed tomography x ray ct technique originally developed for medical purposes in the early 1970 s has been employed widely to reconstruct 3d heterogeneous features preserved in porous rocks arns et al 2005 blunt et al 2013 callow et al 2018 thomson et al 2019 recently the effectiveness of x ray ct technique has been evaluated and applied to various subsurface studies such as soil science petrology petroleum geology paleontology and sedimentology because it is a non destructive analysis offering 3d visualization arnold et al 1983 haubitz et al 1988 mees et al 2003 petrovic et al 1982 raynaud et al 1989 renter 1989 vinegar 1986 vinegar and wellington 1987 the x ray ct technique is useful to evaluate subsurface rocks typically hosting fluids by estimating their porosity and permeability van geet et al 2000 despite such benefits the x ray ct technique has the restriction of measurement scale related to the resolution for example when medical or industrial ct scanners were utilized pores smaller than the volumetric pixel or voxel of x ray ct images can not be identifiable therefore in order to analyze the micropores preserved in dense carbonate rocks and pelagic sediments high resolution micro ct images taken from a micro focused ct scanner were necessary mees et al 2003 nevertheless to obtain such high resolution micro ct images the size of the targeting object must be reduced to a few cm in size du plessis et al 2017 kroll et al 2009 neuser and suppes 2007 in this situation a reduced object might not be able to capture heterogeneous characteristics preserved in the original rock specifically for highly heterogeneous conglomerate rocks highlighted by complex distributions of clasts and matrix the reduced size of the sample causes a loss in the effective representation of the spatial heterogeneity in this study highly heterogeneous conglomerate rocks were imaged with the x ray ct technique to evaluate the influence on fluid flow two conglomerate cores highlighting different distributions of matrix and clasts were analyzed to quantify the pore size and their distribution subsequently the absolute and effective porosities were identified and their spatial patterns were compared to the distribution of both the clasts and matrix 2 janggi conglomerate core 2 1 geologic description of the janggi basin pohang republic of korea the janggi basin located in the southeastern region of south korea is close to one of the largest co2 emission sources in south korea such as steel making plant emitting co2 with a rate of approximately 70 000 tons per day fig 1 a the distance between the plant and potential co2 injection site is approximately 15 5 km huh et al 2010 geologically the janggi basin is composed of non marine sedimentary and volcanogenic rocks divided into the basaltic janggi group and beomgockri group tateiwa 1924 the k ar whole rock age between 19 and 22 ma of volcanic rocks and plant fossils within the sedimentary rocks indicate that the janggi basin was presumably formed in the miocene huzioka 1972 jin et al 1988 kim et al 1975 kim et al 1986 lee et al 1992 yoon 1992 based on the relative age and structural configuration such as the attitude i e strike and dip of beddings the janggi basin is classified into five sub basins called the guryongpo ocheon noeseongsan yeongam ri and yangpo blocks fig 1a cheon et al 2012 jung et al 2012 kim et al 2015 kim et al 2011 among them the noeseongsan block was evaluated as a potential site for geologic co2 sequestration due to its stratigraphy and geologic structures kigam 2017 kim et al 2015 the stratigraphy of the noeseongsan block consists of the lower beomgockri 265 to 1 000 m and upper janggi 0 to 265 m groups fig 1b the lower beomgockri group is composed of early miocene dacitic volcanic deposits and the upper janggi group is characterized by late miocene basaltic volcanic deposits son et al 2013 specifically the beomgockri group is subdivided into the seongdongri and janggi conglomerate formations kim et al 2011 according to kigam 2017 the janggi conglomerate has an average porosity and permeability of 0 161 and 8 41 10 15 m2 respectively and is interbedded with layers of mudstones dacitic tuffaceous sandstones conglomerates and conglomeratic sandstones the seongdongri formation overlying the janggi conglomerate formation is composed of dacitic tuff tuffaceous sedimentary rock and mudstone because of the relatively high ratio of volcanic materials and fine sediments in the seongdongri formation the value of permeability is expected to be lower than that of the janggi conglomerate formation additionally the uppermost noeseongsan basaltic rock including dominant eruptive lava flows intrusive basalt and tuffaceous breccia is expected to be impermeable 2 2 petrophysical properties of cores collected from janggi formation for the purpose of site characterization 7 boreholes jg 1 2 3 4 5 6 and 7 1 were drilled at the noeseongsan block to evaluate the hydrogeological properties porosity and permeability kigam 2017 utilizing the mercury and helium porosimeter a total of 321 porosities were measured at various depths from multiple boreholes fig 2 a the measured porosities ranged from 0 015 to 0 385 with the mean and standard deviation being 0 181 and 0 065 respectively in general the porosity was correlated with depth d 2 636 9 ϕ 1 132 2 r2 0 40 indicating that these sedimentary lithofacies experienced compaction processes nevertheless at certain depths a wide range of porosities was observed suggesting that these lithofacies evidently preserved heterogeneous features even within the same sediment unit fig 2a in addition to the porosities a total of 71 permeabilities were measured from selected cores in the boreholes jg 1 3 4 6 and 7 1 fig 2b after injecting the inert n2 gas volumetric flow rate q was measured when the steady state condition was reached then permeability k was calculated as follows 1 k q μ l a δ p where μ is gas viscosity l is the length of core sample a is the area of core sample and δ p is the pressure difference between inlet and outlet finally the measured permeabilities ranged from 0 01 10 15 to 89 0 10 15 m2 kigam 2017 kim et al 2015 kim et al 2011 porosities and permeabilities of 9 conglomerate 39 sandstone and 3 mudstone samples were plotted in the semi log graph fig 2c overall the porosities and permeabilities showed positive correlations the regression line of conglomerate red dotted line showed greater permeability than that of sandstone green dotted line at equivalent porosities in the case of mudstone the slope of the regression line was the largest but both porosities and permeabilities were significantly lower than those of conglomerate and sandstone based on experimentally measured permeability and porosity data the korea institute of geoscience and mineral resources kigam launched a site investigation for onshore co2 storage in the southern part of the noeseongsan block the janggi conglomerate dominantly consisting of conglomerate and sandstone was chosen as a potential co2 storage formation 2 3 petrophysical and mineralogical properties of janggi conglomerate in the jg 7 1 borehole specific to multiple cores sandstone mudstone and conglomerate cores collected at the jg 7 1 borehole which is considered to be a potential co2 injection well values of porosity and permeability ranged from 0 046 to 0 176 and 0 02 10 15 to 2 70 10 15 m 2 respectively kigam 2017 for conglomerate the mean and standard deviation of porosity were 0 131 and 0 035 respectively with the porosity ranging from 0 106 to 0 155 the mean and standard deviation of permeability were 1 0 10 15 m 2 and 0 9 10 15 m 2 permeability ranged from 0 03 10 15 to 2 70 10 15 m 2 for more detailed evaluation of the in situ janggi conglomerate two drilling core samples core i and core ii were obtained from the jg 7 1 borehole supplement fig s1 both core i and core ii were collected from depths between 936 and 967 m representing the janggi conglomerate formation red box in fig 1b both cores have an equal diameter 48 mm but the heights of core i and core ii were 133 04 mm and 146 59 mm respectively supplement fig s1 illustrates the photos of core i and core ii and associated vertical cross sections captured by the x ray ct scan the conglomerate cores highlighted distinct heterogeneous features even within the small size cores various sizes and shapes of clasts were ubiquitously distributed within the matrix for example in core i clast supported conglomerate clasts were evenly distributed through the entire core but in core ii matrix supported conglomerate large sized clasts were dominantly positioned at the bottom of core such different patterns of the matrix and clasts collected at the same borehole are expected to affect the co2 migration storage capacity and injectivity therefore in the following section the petrophysical properties for identifying fluid flow behavior were evaluated using x ray ct image analysis 3 methodology 3 1 x ray image acquisition and process to acquire the 3d x ray images a computed tomography ct equipment suited in the korea institute of geoscience and mineral resources kigam was utilized it is equipped with x ray tube of 225 kv and one flat panel detector with 2 048 2 048 pixels projections of the core samples were taken every 0 5 for angles from 0 to 360 using an exposure time of 2 s the voxel size of the reconstructed volumes of core i and ii was 62 μm and 58 μm respectively the voxel size lies between that of medical ct scanners 500 μm and micro focused ct scanners 1 5 μm the latter of which however operates at a low acceleration voltage 100 kv that cannot penetrate core sized samples after scanning unrefined x ray ct images of core i and ii preprocessing for cropping and filtering was performed to improve the accuracy and precision of the image analyses the cropping process removes unnecessary parts of x ray ct images and deletes zones where severe imaging errors or artifacts e g the margin of the core damaged core surfaces and beam hardening artifacts are preserved supplement figs s2a and s2b consequently cylindrical 3 d images with 40 mm diameter and 113 mm height were obtained from each core sample supplement figs s2c and s2d following the cropping process the beam hardening correction was conducted to improve the quality of x ray ct images the beam hardening artifact was observed during the preprocess of x ray ct images supplement fig s3a the average x ray intensity within voxels radially increased from the center 0 mm to the edge 20 mm red line in supplement fig s3b after the beam hardening correction the average radial x ray intensity became smoothed and normalized blue line in supplement fig s3b 3 2 segmentation and pore network model to identify the pore characteristics i e porosity pore size distribution and intrinsic permeability from the x ray images the segmentation process is essential this step involves the conversion of the reconstructed gray level 3d images to binary 3d images by determining the threshold value without any reference data the choice of a threshold value separating pores from the solid phase could be subjective to minimize such uncertainty the threshold value was determined from the experimentally measured porosity in detail the heterogeneous conglomerate rock consists of both clasts and matrix where large pore spaces are dominantly preserved in the matrix less than 10 10 10 mm3 of matrix samples were cut from the conglomerate rock and the porosity 0 173 of the matrix was experimentally measured with the mercury injection capillary porosimeter micp subsequently the global segmentation was conducted by way of adjusting the threshold values of the matrix represented by the gray scale level as similar to jin et al 2016 and okabe and blunt 2004 the adjusting process was repeated until the porosity of the matrix in x ray images was best matched with an experimentally measured porosity 0 173 to determine the targeted threshold value supplement figs s2e and s2f after the segmentation process the pore network model pnm was constructed to quantify pore characteristics such as pore number and pore surface area the pore structure and connectivity were identified by finding a skeleton within the pore space based on a hybrid method that combines thinning and distance maps youssef et al 2007 throughout the analysis of x ray ct images the voxelized pore spaces dividing into pore and throat were identified then the generated voxelized pore spaces were converted to spherical pore and cylindrical throat represented as the pnm supplement fig s4 while converting to the pnm the pore size was defined by the radius of the sphere having an equivalent volume of pore body i e the volume of pore segment 4 3 π r 3 to preserve characteristics of the pore volume 3 3 permeability estimation numerical model to numerically evaluate the permeability k of the conglomerate core consisting of matrix matrix with clasts or matrix clast and fractures within clasts a single phase fluid i e water flow simulation was conducted at extracted sub domains representing the connected pore spaces supplement fig s5 prior to flow simulations the representative sub domains i e matrix matrix clast and fractures within clasts were extracted at locations where each sub domain represents its pore characteristics fig 3 the k was calculated by solving the stokes equation as shown below 2 u 0 3 μ 2 u p here u is the velocity vector of the fluid flowing through the porous domain p is the pressure and μ is the viscosity of water 1 10 3 p a s once the stokes equation is solved for fluid flow the global flow rate q through the porous domain is calculated and therefore darcy s law can be applied to solve for intrinsic permeability k 4 k q μ l δ p here q is the darcy s velocity μ is the dynamic viscosity l is the length of sub domain towards the flow direction and δ p is the pressure difference between the inlet and outlet boundary 4 results 4 1 quantification of porosities through x ray ct image analyses 4 1 1 analyses of pore sizes and distribution in core images fig 4 a and 4b different colors represented the pore distributions green yellow and red and clasts were highlighted by a light grey color additionally both the matrix and clasts were delineated by the representative sliced 2 d images as shown in fig 4c d e and f in core i where clasts were evenly distributed the matrix was preserved in between these clasts where numerous pores with different diameters were distributed fig 4a conversely relatively large sized clasts were concentrated at the bottom of core ii and thus most of the pores were observed at the upper part fig 4b by comparing these two cores the conglomerates were characterized as highly heterogeneous dependent on clast positions and sizes the number and volume of pores were different even in core samples collected from the same wellbore the analysis of x ray ct images revealed that the total number of pores was 59 255 and 216 787 within core i and ii respectively table 1 note that the number of pores was counted at relatively homogeneous portions where the matrix was mainly dominant fig 4a and b in both cores the pore diameters varied from 0 077 to 3 51 mm and the average 0 54 mm and 0 47 mm and standard deviation 0 21 mm and 0 17 mm of pore diameters differed in core i and ii respectively table 1 to investigate further the pore distribution was categorized into 3 groups group i green 1 0 mm 1 0 mm group ii yellow 2 0 mm and group iii red 2 0 mm from the pores 59 255 in core i the number of pores within group i was 57 221 96 57 and group ii and iii occupied 3 43 pore numbers 2 026 and 8 respectively indicating that most of the pores belong to group i similarly in core ii the total number of pores was prevalent in group i 98 98 pore numbers 214 567 the remaining pores in group ii and iii were 1 02 thus the difference in the total number of pores between core i and ii was 157 532 such a large difference in the total number of pores resulted from the difference in small pores categorized in group i finally core ii possessed more pore volume and porosity 17 525 83 m m 3 and 0 125 than core i 7 409 54 m m 3 and 0 052 the difference in pore volume and porosity was 10 116 29 m m 3 and 0 071 which is also caused by the difference of pore volume in group i the difference of pore volume and porosity in group i between core i and ii was 10 115 81 m m 3 and 0 071 heterogeneity according to the distribution of clasts and matrix in core i and ii resulted in the difference in pore space highlighting that the presence of the matrix primarily controlled the amount of pore space 4 1 2 absolute and effective porosities both absolute for a total pore space and effective for a connected pore space porosities were calculated along the vertical profile z axis in both core i and ii after separating connected pore space from the total pore space porosities in x y planes were calculated fig 5 a and 6a to obtain connected pores 2 parallel boundary layers i e top and bottom x y planes were selected and all voxels connected through at least one common edge between these 2 layers or 18 neighborhood were determined prado et al 2016 sufian and russell 2013 then connected voxels were calculated to be connected pore space the average absolute and effective porosities were 0 069 and 0 052 in core i and 0 131 and 0 125 in core ii respectively which were within the range 0 046 to 0 176 of the experimentally measured porosity as discussed in section 2 3 fig 2 in core i the absolute red line and effective blue line porosities varied from 0 025 to 0 132 and 0 003 to 0 126 respectively both absolute and effective porosities tended to decrease where clasts were dominant although both porosities showed a similar pattern along the vertical axis discrepancy exists at certain parts due to the presence of isolated pores for example a difference of 0 043 between absolute and effective porosities was observed at a height of 62 7 mm absolute porosity 0 066 and effective porosity 0 023 fig 5b which is the sliced image at 62 7 mm height showed the largest portion of isolated pores red color in core i in core ii absolute and effective porosities ranged from 0 015 to 0 204 and 0 004 to 0 196 respectively fig 6 a both porosities were high in the upper part where the matrix was dominant but approached almost 0 porosity at the bottom where large clasts were positioned compared to core i core ii revealed that the portion of isolated pores was small the difference between the average of absolute and effective porosity was 0 017 and 0 007 in core i and ii respectively fig 5a and 6a in core ii the largest portion of isolated pores was measured at a height of 1 8 mm where a crack existed within the clast here the difference between the absolute and effective porosity was 0 031 fig 6a the isolated pores or cracks within the clast at a height of 1 8 mm are shown in fig 6b due to the presence of isolated pores and clasts at the bottom the effective porosity was much smaller the connected pores were rarely observed but primarily distributed at the core margin 4 2 quantification of intrinsic permeabilities a total of 11 sub domains with varying porosity 0 043 to 0 174 were extracted from core i and ii fig 3 they were categorized into 3 groups matrix mi and mii matrix clast mci and mcii and fractures within clasts fi and fii fig 7 b d in these sub domains the horizontal and vertical k were numerically evaluated and they varied from 1 18 10 12 to 1 44 10 10 m 2 fig 7a and table 2 in the sub domains representing the matrix c1mi c1mii c2mi and c2mii connected pores were distributed almost uniformly in core i and ii with ϕ being equivalent to 0 173 fig 7b the numerically calculated kh and kv ranged from 6 40 10 12 to 3 87 10 11 m 2 with the smallest standard deviation of 9 89 10 12 m 2 as presented in table 2 additionally the mean anisotropic ratio i e the mean ratio of k v to k h was 0 70 indicating that the water could migrate through any direction without preferential pathways in the matrix in the sub domains representing matrix clast c1mci c1mcii c2mci and c2mcii the ϕ ranged from 0 093 to 0 158 which was smaller than that of the matrix sub domain and had a greater variation fig 7c the calculated k showed a variation of 10 2 times from 1 18 10 12 to 1 01 10 10 m 2 table 2 clearly highlighting the different properties even when the sampling location of these sub domains was separated by a few centimeters unlike the matrix sub domain the matrix clast sub domain highlighted anisotropic characteristics mean anisotropic ratio γ m 0 583 due to the increase in heterogeneity i e addition of clasts finally in the fracture sub domain c1fi c1fii and c2fi fractures and connected pores were preserved within clasts as clasts occupied most of the sub domain the calculated porosities were the lowest ranging from 0 043 to 0 060 fig 7d additionally connected fractures and pores were aligned to a specific direction the calculated k was high even though ϕ was small both kh and kv ranged from 1 51 10 11 to 1 44 10 10 m 2 their average was 5 93 10 11 m 2 which was the greatest among the three sub domains matrix 1 31 10 11 m 2 and matix clast 3 83 10 11 m 2 table 2 and fig 7a interestingly in the fracture sub domain c1fi and c2fii γ over 1 kv kh was shown denoting that fractures extended vertically the effective k of the whole core measured from laboratory experiments was 10 3 10 4 times smaller than numerically calculated k such differences were presumably related to the low resolution of x ray ct images which was inevitably chosen to capture the heterogeneous distribution of matrix and clasts in conglomerate cores fig 3 for example in the laboratory core flooding test the fluid was able to migrate through connected micropores which cannot be captured in x ray ct images although clasts which have almost zero porosity can block fluid migration the existence of such micropores within matrix and clasts contributed to the slow migration of fluid during the core flooding test presumably resulting in small k through this analysis it should be noted that the laboratory core flooding test provided k without much information of the in situ pore distribution and connection alternatively the x ray ct images were able to capture the 3d distribution of pores visualizing their heterogeneous characteristics but such pores did not contain micro sized pores where fluids hardly migrated due to the low resolution of x ray ct images 4 3 quantification of flow patterns in heterogeneous conglomerate rocks according to the distribution of the matrix and clasts the size of pores pathway and velocity of the fluid significantly varied within the heterogeneous conglomerate core the magnitude and pattern of simulated velocities were delineated in connected pores representing the sub domains of c1mi c1mci and c1fi fig 8 in c1mi connected pores were evenly distributed and the fluid flowed through most pores fig 8a and 8d as the portion of clasts increased i e c1mci the number of flow pathways decreased fig 8b and 8e indicating that the fluid was unable to flow through clasts even if connected pores existed within both matrix and clasts fluid did not flow through such pores or the flow velocity within them was negligible this suggested that connected pores did not always serve as major fluid pathways highlighting the importance of pore connectivity throughout the entire domain finally fig 8c represents the fracture sub domain or c1fi wherein pores were preserved within the fracture as the fluid was concentrated through narrow pathways representing fractures the flow velocity was the largest among all sub domains fig 8f in addition to evaluating the influence of the matrix and clast ratio on fluid flow the measurement scale also influences fluid flow in highly heterogeneous conglomerate rock depending on the measurement scale the volumetric ratio of clasts to matrix can vary significantly in order to evaluate the scale dependent fluid flow an enlarged sub domain of en c1mi with dimensions of 20 20 20 m m 3 was selected fig 3 and fig 9 a the en c1mi included the sub domain of c1m1 highlighted with an inner bounding box fig 9b compared to c1mi the size of the en c1mi sub domain was doubled but the effective porosity was decreased by more than a factor of 5 c1mi 0 172 to en c1mi 0 036 this was because of the increase in the portion of clasts and isolated pores in en c1mi reflecting the influence of heterogeneity even in such a small scale the clast and isolated pores affected fluid flow by blocking it and thus the overall permeability was decreased from 1 13 10 11 m 2 to 1 05 10 12 m 2 the layered x ray ct images were selected at two different heights fig 9c and 9d where clasts isolated red and connected pores blue were delineated depending on the height the variation of clasts matrix and isolated connected pores was severe resulting in variation of fluid velocity fig 9b 5 conclusions in the study digital 3 d conglomerate cores were reconstructed using the x ray ct technique absolute and effective porosities were identified and their spatial patterns were evaluated according to the distribution of the clasts and matrix additionally by implementing the numerical single phase computation fluid dynamics approaches the permeabilities of highly heterogeneous conglomerate cores were estimated and subsequently compared to experimentally measured permeability at the core scale e g core flooding test finally scale dependent permeability was evaluated in the heterogeneous conglomerate core conglomerate cores displayed highly heterogeneous features the clasts were evenly distributed throughout core i while large sized clasts were dominant in core ii such unique heterogeneous features observed in these cores caused differences of the pore volume for example the pore volume of core ii 17 525 83 m m 3 was greater than that of core i 7 409 54 m m 3 and therefore the absolute and effective porosity of core i 0 069 and 0 052 was lower than that of core ii 0 131 and 0 125 at the sub domain scale the heterogeneous features were highlighted by significant changes in the porosity and permeability pores in the matrix sub domain were evenly distributed and well connected showing large porosity and permeability while as the portion of clasts increased porosity and permeability decreased and anisotropy increased in the fracture sub domain the porosity was the lowest but the permeability increased in addition to distinct changes in the porosity and permeability depending on chosen spatial locations within the core the heterogeneity of conglomerate caused scale dependent petrophysical properties this suggests that the effect of small scale heterogeneity should be considered when a heterogeneous conglomerate is analyzed another important aspect to consider for a successful application of x ray ct imaging to the analysis of heterogeneous conglomerate core is the identification of the relationship between resolution and object sizes high x ray ct resolution is required to decrease the object size however this will not preserve spatial heterogeneity in the conglomerate core to capture heterogeneous characteristics in the conglomerate cores a sufficiently large sized sample is necessary unfortunately large sized samples would inevitably result in a low voxel resolution of x ray ct images this study utilized ct images with low resolution i e voxel resolution 58 62 μ m however the low resolution could overestimate the pore size and cause an overestimation of permeability peng et al 2012 peng et al 2014 further increment of resolution for the ct images will be required to compensate for potential limitations caused by overestimation hence ct image analysis at multiple resolutions will be able to enhance the evaluation of spatial heterogeneity and petrophysical properties of the conglomerate cores botha and sheppard 2016 jiang and arns 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by basic science research program through the national research foundation nrf of korea funded by the ministry of education project number 2016r1d1a1b01008715 and 2021m2e1a1085187 by the korea environmental industry and technology institute keiti project numbers 2018002440003 by the demonstration scale offshore co2 storage project in pohang basin republic of korea funded from energy efficiency resources of the korea institute of energy technology evaluation and planning ketep which was granted financial resource from the ministry of trade industry energy republic of korea project number 20162010201980 the authors also appreciate financial support by the basic research project of the korea institute of geoscience and mineral resources kigam funded by the ministry of science and ict this research was partially supported by the graduate school of yonsei university research scholarship grants in 2020 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126736 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4197,in recent years water quality monitoring has been crucial to improve water resource protection and management under the relevant laws and regulations environmental protection department agencies monitor lakes streams rivers and other types of water bodies to assess water quality conditions the valid and high quality data generated from these monitoring activities help water resource managers understand the existing pollution situations energy consumption problems and pollution control needs however there are inevitably many problems with water quality data in the real world due to human mistakes or system failures one of the most frequently occurring issues is missing data although most existing studies have explored classic statistical methods or emerging machine deep learning methods to fill gaps in data these methods are not suitable for large scale consecutive missing data problems to address this issue this paper proposes a novel algorithm called tradaboost lstm which integrates state of the art deep learning theory through long short term memory lstm and instance based transfer learning through tradaboost this model inherits the full advantages of the lstm model and transfer learning technique namely the powerful ability to capture the long term dependencies among time series and the flexibility of leveraging the related knowledge from complete datasets to fill in large scale consecutive missing data a case study involving dissolved oxygen concentrations obtained from water quality monitoring stations is conducted to validate the effectiveness and superiority of the proposed method the results show that the proposed tradaboost lstm model not only improves the imputation accuracy by 15 25 compared with that of alternative models based on the obtained performance indicators but also provides potential ideas for similar future research keywords water quality transfer learning lstm tradaboost large scale consecutive missing data 1 introduction 1 1 background with the rapid development of industrialization and urbanization water resource protection and water pollution treatment have become popular and important topics worldwide hasanzadeh et al 2020 li et al 2020a zhang et al 2017 to control water pollution and reduce its adverse impact on the aqueous ecological system and human society kisi and parmar 2016 li et al 2017b many studies on temporal and spatial predictions of water quality chen et al 2020 evaluations of the factors that influence water quality pollution xu et al 2019 and data driven water quality models jin et al 2019 have focused on improving water quality monitoring in small watersheds chapman et al 2016 murphy and sprague 2019 when performing such studies a valid and high quality water quality dataset is a priority for generating reliable and reasonable results zhou 2020 however most water quality data e g ammonium nitrogen nh 3 n pondus hydrogenii ph dissolved oxygen do chemical oxygen demand co d cr etc are automatically sampled by bioheavy metal sensors at different monitoring stations zhang et al 2019 equipment failures regular maintenance inadequate sampling procedures artificial changes in sensor parameter settings and other factors may lead to numerous missing values in raw water quality datasets ma et al 2020a these missing data will significantly limit water quality prediction and influential factor evaluation therefore as an increasing number of studies on water quality have turned into data driven analysis it has become urgent to solve the missing data problem in this domain 1 2 literature review in recent years several scholars have realized the severity of the missing data problem and explored many approaches to reduce the negative effects of missing data seyed babak haji seyed asadollah et al 2021 the most typical approaches for dealing with missing data fall into two major categories deletion processing and imputation processing armijo olivo et al 2009 although deletion processing simply discards the missing observations in many cases and is easily implemented some studies have shown that it can extensively decrease the statistical performance of subsequent models and produce bias in experiments different from deletion processing imputation processing maintains the length of time series by estimating missing values various imputation mechanisms have been proposed and used in a wide range of applications one of the most straightforward imputation methods is linear statistic interpolations in which data gaps are filled using the corresponding mean median or mode of the existing observations however these methods can generate flat and dumb filling values ma et al 2020b in geospatial analysis of hydrology li et al 2017a some spatial interpolation methods e g inverse distance weighting idw ke et al 2011 kriging dai et al 2003 and splines giang et al 2013 interpolation etc are widely used to impute data however it is unwise to consider only spatial elements in the missing value estimation time series statistical models e g arima tencaliec et al 2015 can reflect the fluctuating trends of true values but lead to phase lag in the filling processing with the popularity of machine learning and deep learning methods that utilize series of internal data features to train imputation models an increasing number of related methods have been proposed for missing data imputation ahmad sharafati 2020 for example the expectation maximization em algorithm gaetan and yao 2003 fills missing values using maximum likelihood estimations obtained based on the existing observations gaetan and yao 2003 fuzzy clustering method fcm and k nearest neighbour knn method adopt the mean value of relevant observations from neighbouring complete sequences as the imputation value tang et al 2015 zhang and software 2012 support vector regression svr and artificial neural network ann have also been utilized as common tools to generate reliable filling results seow and ziegler 2017 tabari and hosseinzadeh talaee 2015 in the state of the art deep learning domain che et al che et al 2018 developed a deep learning model that employs gated recurrent unit gru for missing data imputation they merged the time interval and coverage of missing data in the gru model to capture the long term time dependence of time series data yu et al yu et al 2013 proposed a regularized extreme learning machine r elm to impute missing values the main advantages of r elm are twofold first the time complexity is very low and second there is no need to tune the related parameters in advance however standard machine learning and deep learning methods require large amounts of labelled data for training şahin et al 2011 so they are not well suited for imputing missing water quality data notably in such datasets gaps in labelled data are generally consecutive and can be extensive 1 3 motivation and contributions this paper investigates the qiantang river basin that flows through hangzhou zhejiang province china as a case study the total length of the qiantang river mainstream is 319 8 km and the basin drainage area is 13 902 k m 2 hu et al 2012 yaseen et al 2018 based on the existing literature few water quality monitoring stations are currently deployed in the qiantang river basin excluding in areas with special environments or that are difficult to sample e g sea zones is few and the estimated price required to purchase a set of water quality monitoring equipment is very high li et al 2019 therefore there are two scenarios considered in the water quality sampling process first due to sensor limitations the data sampled at the water quality monitoring stations are large proportionally consecutive missing second expensive monitoring equipment special environments and areas that are difficult to sample results in blind monitoring spots which can significantly affect the subsequent water quality analysis the key to solving the above two types of water quality problems is determining how to fill in the numerous historical data gaps moreover most previous studies verified that emerging machine and deep learning methods can provide more remarkable performance in missing data imputation when compared with classical statistical methods however the existing techniques have only been applied in the situations where the missing rate was low such as less than 30 and the missing values were random anh et al 2011 consecutive missing values accounting for 40 90 of a dataset have not been imputed in fact it is difficult to solve the large scale consecutive missing data issue because as the missing rate increases the relevant prior statistical information or training samples are insufficient for tuning the accuracy of the model some advanced artificial intelligence techniques have illuminated various study fields but limited research have explored their use in addressing the large scale consecutive missing data imputation problems in water quality monitoring in this paper we explore a novel model called tradaboost lstm to solve this problem the proposed algorithm encompasses the advantages of the transfer learning technique and the lstm model and it respectively aims to fill large scale consecutive gaps in missing data while capturing excellent long dependent trends from observations of similar time series additionally the new algorithm utilizes the similarity of water basin partitions learns from adequate and long period data information obtained at other water quality monitoring stations and transfers knowledge to produce the complete time series using an lstm neural network the proposed algorithm is provided with full advantages of the transfer learning technique and the lstm model where it respectively aims at the issue of large scale consecutive missing data and captures excellent long dependent moving trends from the observations of similar time series the essence of the new algorithm proposed in this paper aims to utilize the similarity of water basin partitions and then to learn from adequate and long period data information of the other water quality monitoring stations and finally to transfer the knowledge into the incomplete time series using lstm neural network additional details about this novel method will be introduced in subsequent chapters the main contributions and novelty of this study are summarized as the following four aspects our research object focuses on large scale consecutive missing data a hybrid transfer learning based deep learning method is proposed and illustrated the feasibility of the proposed algorithm in actual water monitoring is explored negative transfer is controlled within limits by a novel and classical algorithm to verify the effectiveness of the proposed method a case study in the qiantang river basin is performed the large scale consecutive missing dissolved oxygen do concentrations are imputed the results show that as the missing rate increases the proposed method can improve the estimation accuracy of missing imputations compared with those of other methods the rest of the paper is organized as follows section 2 details the proposed methodological framework including the transfer learning based tradaboost and deep learning based lstm section 3 presents and analyses the experimental results in missing data imputation for an actual water quality monitoring case section 4 and section 5 are extended to analyze and discuss the experiment results respectively and section 6 concludes this study and elaborates on possible future work 2 methodological framework design as illustrated in fig 1 in this paper the methodological framework for missing data imputation can be divided into two parts data pre processing and algorithm execution in the data pre processing firstly an incomplete data sequence collected from a sensor at a water quality monitoring station is cleaned standardized normalized and defined as the experiment data in the target domain secondly we explore the time series similar method e g dynamic time warping dtw to find out the monitoring stations for which the associated complete monitoring data are the most similar to the target domain experimental data then the data from these stations are used as the source domain reference finally the sliding windows algorithm is used to construct time series of training and testing samples in the process of algorithm execution the proposed tradaboost lstm method which integrates instance based transfer learning tradaboost and advanced deep learning lstm neural network is designed to address the large scale consecutive missing data problem at the water quality monitoring stations the following sections will present different components of the methodological framework in detail 2 1 transfer learning technique since this paper adopts the transfer learning method tradaboost we first provide a brief review of the transfer learning field as a novel branch of artificial intelligence transfer learning pan et al 2009 has recently attracted considerable interest because it focuses on solving the concern that the labelled data in the target domain may be too limited to effectively used to train a model the transfer learning method allows the distributions of training samples to be different lv et al 2019 in other words transfer learning is a unique technique that leverages or adapts well established knowledge from a related source domain to boost the learning in the target domain weiss et al 2016 in this study due to the large scale consecutive missing data issues at target monitoring stations the unavailability of time series of labelled training data leads to unsatisfactory filling performance therefore transfer learning is applied to transfer knowledge from the complete sequences obtained at adjacent monitoring stations to an incomplete sequence in the target domain in general based on the learning pattern conventional transfer learning methods can be roughly categorized as instance transfer model transfer feature parameter representation transfer or relation knowledge transfer among them the instance based transfer method is intuitively comprehensive and explicit dai et al dai et al 2007 proposed a well known instance based transfer learning algorithm named tradaboost which is an extension of adaboost and assumes that only the distributions of source and target domain data are different the motivation of the tradaboost algorithm is based on the concept that a portion of the inherent data information in the source domain with a similar distribution as the target training data may be more valuable than other portions of data therefore the tradaboost algorithm attempts to iteratively reupdate the weight of each sample of target and source domain data in accordance with the corresponding negative or positive contribution tang et al 2020 in this paper when conducting large scale missing data imputation for data from the target monitoring station the tradaboost based transfer learning approach is utilized to learn the inherent long term trend characteristics from the complete time series of data obtained at the source monitoring stations 2 2 selection of source domain samples as mentioned in section 2 1 transfer learning is one of the most important techniques used in this study the core foundation of transfer learning is identifying the similarity between source and target domains to guarantee the successful transfer of knowledge however when the source domain is weakly related to the target domain source domain data will exert a negative influence on learning in the target domain pan et al 2009 noted that it is very significant to avoid negative transfer in the transfer learning research and the key to keeping negative transfer within limits is to develop a suitable criterion to measure similarity hence after raw water quality data sampling cleaning and normalization we should first determine the source domain with data most similar to those in the target domain in this study when imputing missing water quality data the sequence with missing data represents the target domain and the complete sequences are potential source domain data based on the existing literature folgado et al 2018 fu 2011b li et al 2020b there are several commonly used similarity measurement criteria such as the euclidean distance ed pearson correlation coefficient pearson kullback leibler divergence k l divergence dynamic time warping dtw and longest common subsequence lcss etc overall an appropriate similarity measurement criterion depends on the inherent data properties of the time series the length of the time series must be equal in the ed approach but this limitation may lead to the inappropriate transimission of time information pearson s correlation coefficient is widely used to measure the strength of the correlation between two variables but it is not ideal for nonlinear cases k l divergence is a common indicator used to measure the similarity between two probability distributions and the more distinct the difference between the two distributions the larger the k l divergence is however the main disadvantage of k l divergence lies in not considering distance and asymmetry the lcss is mainly related to shape similarity rather than the location similarity and is highly time consuming the dtw algorithm can use time series of different lengths and is capable of accurately capturing the similarity between the trends of two time series in this paper considering the important fact that the length of continuous gaps in large scale water quality data varies dtw is selected as the key similarity measurement criterion to inhibit the negative transfer in this study we define s t as the incomplete time series namely the target domain data this series can be denoted as s t s t 1 s t 2 where s t 1 is the complete time series without missing data s t 2 is the consecutive missing data the next important step is to select source domain time series s s where 1 s n s t and n is the total number of water monitoring stations the dtw algorithm refer to the appendix for details is used to determine the suitable source domain most similar to the target domain based on the following constraint 1 s m min s s d t w s s s t 1 1 s n and s t where s m denotes the suitable source domain among the domains of water quality monitoring stations 2 3 sliding window when exploiting the proposed transfer learning based method for processing time series data the original time series can be transformed into input feature and output label subsequences for better model training to do this the sliding window sw zhang et al 2019 zivot and wang 2007 method is implemented to construct features inputs and labels outputs for training samples based on continuous time series of observations a time series training sample e g the window size is h in fig 2 obtained by the sw method can be expressed as follows 2 s t h s t h 1 s t 3 s t 2 s t 1 s t where s s 1 s 2 s 3 s n is a complete sequence s t h s t h 1 s t 3 s t 2 s t 1 represents one feature input at time t s t represents the output label at time t and h represents the sliding window size the sliding window method works by anchoring the left side point s 1 and then moving or scanning through the time interval n h to reach the right side point s n h of the time series during this process each moving or scanning step provides a training sample the sliding window size h not only makes a significant difference in the number of time series training samples but also affects the feature input subsequences associated with each training sample it can be concluded that a small window size will provide more training samples than a large window size but the samples may not contain enough feature input information a larger window size will result in fewer training samples and irrelevant interference information will be included in the model input set therefore it is significant to select a suitable sliding window size h as will be revealed in a later section 2 4 long short term memory lstm model to accurately generate missing water quality data through imputation in this paper we use the long short term memory lstm as the foremost component of the imputation framework here we review the evolution of lstm methods chen and wang 2019 hochreiter and schmidhuber 1997 li et al 2017c shun chen 2019 tian et al 2018 unlike the traditional artificial neural network ann which fails to strongly associate information from previous time series with the that from the current series a deep learning architecture recurrent neural network rnn is designed to promote dynamic temporal behaviour by replacing the input of the current state with the output of the previous state unfortunately as the distance between the relevant information and the location where it is needed increases the rnn cannot maintain the long term dependencies in the input sequences therefore lstm was developed to address the vanishing or exploding gradient problems in the rnn computational process lstm an essentially improved extension of rnns excels in interpreting long time series of historical data the common architecture of an lstm unit is shown in fig 3 the lstm unit is composed of a cell and three different components usually called gates associated with the flow of information inside the unit intuitively thecellis responsible for remembering the dependencies among the elements in the input sequences and three gates namely the input gate inputgat e t the forget gate forgetgat e t and the output gate outputgate t cooperatively enable the lstm cell to perform backpropagation and store information from previous time series the formula for the current state h t of an lstm unit at time t can be expressed as follows 3 inputgat e t s i g w i x t h t 1 η i 4 forgetgat e t s i g w f x t h t 1 η f 5 c t tanh w c x t h t 1 η c 6 outputgat e t s i g w o x t h t 1 η o 7 c t i n p u t g a t e t c t 1 f o r g e t g a t e t c t 8 h t c t o u t p u t g a t e t where tanh denotes the activation function tanh x e x e x e x e x sig denotes thelogistic sigmoid function sig x 1 1 e x c t 1 denotes the state of the previous cell at time t 1 denotes the hadamard product x t denotes the inputs at time t w denotes the weights in the lstm unit and η denotes the offset vector 2 5 tradaboost lstm algorithm design the lstm model has been applied in various domains due to its excellent performance in effectively extracting long term information from time series data however lstm has extreme limitations in yielding accurate results when there are not enough training data the reason for this phenomenon lies in the fact that machine or deep learning methods such as lstm networks necessarily use many previous training values to predict a current unknown value thus if sufficient prior information and historical knowledge are not available the lstm model will generate improper results when applied to fill in the missing data especially for large scale consecutive missing values in this paper in this study we propose the tradaboost lstm algorithm to address this problem the working process of this novel large scale consecutive missing data imputation algorithm which was designed based on tradaboost lstm is shown in fig 4 first the source and target domain samples are mixed to construct the training sample set as the model input second the tradaboost algorithm constructs a series of weak learners lstm for regression based prediction and a tradaboost based encoding layer which reupdate the weight of each sample for the current weaker learner this process depends on the learning error of the previous weak learner in the test set associated with the target domain fig 5 illustrates the tradaboost based encoding layer as the number of iterations increases for the weak learner the weights of source domain samples similar to the target domain training samples increase and the weights of source domain samples that are not similar to the target domain training samples decrease finally the lstm based tradaboost algorithm is established to implement large scale consecutive missing data imputation and a weighted averaging strategy is utilized to predict the corresponding imputation values the description of notations used in the algorithm is listed in table 1 additional details of the proposed process of transfer based large scale consecutive missing data imputation are presented in table 2 it is worth noting that the values of some parameters in the proposed algorithm may influence the accuracy of the imputation results one example is 1 the number of source training samples m generally increasing the number of samples can improve the learning performance of each weaker learner but time requirement and computational complexity will increase therefore we adopt a compromise strategy in which the ratio of the length of the source domain to the length of the target domain is set as approximately 6 another example is 2 the missing data rate λ in the target domain the missing data rate λ is defined as λ length m i s s d a t a length f u l l d a t a as this rate approaches zero λ 0 the number of training data in the target domain will boost and the accuracy of the imputation results will be more dependent on the training data in the target domain alternatively as the missing data rate approaches one λ 1 the number of training samples in the target domain will decrease and the accuracy of the imputation results will be affected by the training data in the source domain to further improve the imputation accuracy for large scale consecutive missing data we use an imputation value from the source domain as a reference and it can be formulated as s s m e a n t t finally the output values of the proposed algorithm and the source domain reference are combined to generate the final reliable results based on the missing rate parameter λ the corresponding formula can be expressed as follows 9 f i n a l v a l u e k λ s s k m e a n t t 1 λ i m p v a l u e k where s s denotes the source domain data in a given period mean t t denotes the average value of the target domain data and i m p v a l u e denotes the imputation result from the proposed algorithm in summary in this paper a novel methodological framework based on instance based transfer learning tradaboost and temporal model based deep learning long short term memory lstm is proposed to improve the imputations of large scale consecutive missing data to evaluate the effectiveness of this proposed algorithm a case study of consecutive missing dissolved oxygen do concentrations will be carried out in the following sections 3 case study 3 1 data explanation in this paper a case study was conducted in the qiantang river basin in hangzhou zhejiang province china we collected the dissolved oxygen do concentration as raw experimental dataset records data source https www zjemc org cn obtained every four hours reported by 10 water quality monitoring stations in the qiantang river basin for two years from august 2018 to august 2020 the reason for choosing do is that the level of do is an important indicator of the water quality and self purification level of water bodies in water quality and self purification evaluations a high do concentration is beneficial for the degradation of pollutants while a do concentration that is too low is not conducive for the growth of aquatic organisms hypoxia often occurs in the qiantang river the largest river in hangzhou mainly due to the considerable pollution burden thus to explore some approaches for addressing the issue of hypoxia it is necessary to monitor the do concentration the distribution of the 10 water quality monitoring stations in the qiantang river basin is shown in fig 6 and a detailed summary of raw do concentration data summary is presented in table 3 table 3 shows that 10 water quality time series are generated based on the 10 monitoring stations 3 2 data pre processing this study was performed to investigate a new framework for imputing missing do concentrations in observed datasets as shown in fig 1 the quality of raw data will strongly influence the accuracy of the imputation outcome however the obtained water quality samples are always influenced by noise and external outliers therefore water quality data should be cleaned before running the proposed algorithm the whole process of data cleaning is described in fig 7 first the pauta criterion byer david 2005 is adopted for the detection of external outliers it is assumed that a sample that exceeds three times the average is judged as an external outlier and removed second we use classical wavelet transform he et al 2008 to remove noise e g gaussian white noise from data at last the box plots of clean data after pre processing in ten monitoring stations are shown as fig 8 to clarify this point besides to narrow feature dimension differences and accelerate the convergence of weight parameters for model training the do dataset finally were standardized to 0 1 because the sampled original do data do not include any missing values this study assumes that the original do data of a certain station are deficient to validate the imputation performance of the tradaboost lstm algorithm for large scale consecutive missing values for illustration the dongzi monitoring station s 4 with 702 complete samples ranging from 12 07 2019 to 08 12 2019 is selected as the target domain for missing data imputation in this paper detailed information on the consecutive missing data is listed in table 4 for five types of missing rate λ 0 1 0 3 0 5 0 7 0 9 3 3 experimental setup as mentioned in section 2 2 we use the time series sequence of station s 4 to establish the target domain s t 4 which can be categorized into two parts s t 4 1 and s t 4 2 where s t 4 1 is the complete sequence and s t 4 2 is the sequence with consecutive missing values it can be deduced that the missing data rate is expressed as λ length s t 4 2 length s t 4 when the proposed tradaboost lstm algorithm is applied to impute the missing values s t 4 2 the most suitable source domain should be first determined in this paper the dtw algorithm is adopted as a similarity measurement method for the target and source domains according to the calculation process in section 2 2 when s t 4 is chosen for the target domain the sequence with consecutive missing values the similarities between the remaining stations and target station s 4 are presented in table 5 the results show that when the missing data rates are preset to λ 0 1 0 3 0 5 0 7 0 9 s 3 is determined to be the most suitable source station based on the high similarity to the target monitoring station we define the experimental data of monitoring station s 3 as the source domain s t 3 hence s t 3 the source domain and s t 4 the target domain are chosen for the subsequent experiments note that after the experimental datasets for the source and target domains are prepared the training and testing datasets can be separately expressed as s t 4 1 s t 3 and s t 4 2 for transfer algorithm inputs and outputs in this paper finally in accordance with the content in section 2 3 the sliding window sw is used to convert the experimental time series data into temporal samples with features and labels the temporal samples of the training data are established as shown in eq 10 and the temporal samples of the testing data are established as shown in eq 11 10 s t 3 h s t 3 h 1 s t 3 3 s t 3 2 s t 3 1 s t 3 s t 4 h 1 s t 4 h 1 1 s t 4 3 1 s t 4 2 1 s t 4 1 1 s t 4 1 0 t 3 l e n g t h s t 3 h a n d 0 t 4 l e n g t h s t 4 1 h 11 s t 4 h 2 s t 4 h 1 2 s t 4 3 2 s t 4 2 2 s t 4 1 2 s t 4 2 0 t 4 l e n g t h s t 4 2 h where h represents the sliding window size and the most optimal window size for the case study will be determined later 3 4 evaluation criteria four indicators including the root mean square error rmse mean absolute percentage error mape mean absolute error mae and fitting coefficient r2 are introduced to evaluate the performance of the deterministic imputation algorithms the corresponding calculations of these four indicators were detailed in previous studies maría elisa quinteros et al 2018 tao su et al 2021 which are expressed as eq 12 15 low values of rmse mape mae and high values of r 2 indicate high imputation accuracy and excellent performance 12 rmse t 1 n y t y t 2 n 13 mape 1 n t 1 n y t y t y t 14 mae 1 n t 1 n y t y t 15 r 2 1 t 1 n y t y t 2 t 1 n y t y t 2 where y t y t and y t means the predictive imputation value i e model output observed true data and the average of the observed true data at the time point t respectively n means the number of the whole temporal samples it is noted from the above eq 12 15 that the lower value of rmse mape mae and higher value of r 2 indicate better performance of the model 4 experimental results 4 1 determination of the window size and network parameters as mentioned the most appropriate sliding window size h should be selected to optimize the imputation of large scale consecutive missing data in this paper based on some existing literature fu 2011a ma et al 2020c ma et al 2020d the candidate of h 2 4 6 8 10 is explored to select the optimal window size when the missing data rate is 0 1 0 3 0 5 0 7 and 0 9 finally we use the four metrics in section 3 4 to obtain a feasible and stable comparison of different cases table 6 presents the results it can be concluded that as sliding window size slowly increases the rmse mape and mae decrease and the r 2 increases until reaching an optimal value at h 6 note that when the missing data rate is 0 9 the most appropriate sliding window size is particularly chosen as 4 because the number of training samples is comparatively low therefore in this paper the best sliding window size h is 6 in addition to the determination of the sliding window size the tradaboost lstm network parameters and structures should also be identified the optimal set of network parameters is reported to improve the imputation accuracy by 5 20 referring to previous studies kao et al 2020 xiang et al 2017 our prime stacked lstm architecture is comprised of one input layer a certain number of hidden layers one fully connected fc layer added at the end of the hidden layers and one output layer after the time series samples are obtained using the sliding window the observation variable input layer has 6 dimensions and the number of output layers is set to 1 namely the single step output the tanh function is selected as the activation function for the lstm layer all the weights and offset vectors are determined during the learning process and match the training data the adam optimizer is adopted because water quality data are generally noisy and the adam algorithm is suitable for addressing this problem some parameters are chosen according to our engineering experience for example the min batch size remains 32 and both the number of iterations in tradaboost tradaboost iteration and epochs is preset as 100 nevertheless the lstm structure also involves many parameters that should be optimized such as lstm hide layer the number of lstm layers learning rate lstm neuron the number of neurons in each lstm layer and fc neuron the number of neurons in the fully connected layer rather than relying on engineering experience based on the existing literature chen and wang 2019 vu et al 2020 xiang et al 2017 yan tian et al 2018 we first vary lstm hide layer as 1 2 3 4 5 learning rate as 0 001 0 005 0 01 0 05 0 1 lstm neuron as 16 32 64 128 and fc neuron as 16 32 64 128 256 and then use the grid search method to determine the most appropriate values within these ranges for overfitting or underfitting problems we further choose these optimal hyperparameters by implementing 10 fold cross validation cv in modelling and testing the entire training and validation learning process took four hours in matlab on a workstation with an intel i5 4200h 3 4 ghz cpu 8 gb memory and a single nvida gtx 950 gpu after conducting the above experiments the optimal values of the parameters in the lstm model were obtained in table 7 the results indicate that when lstm hide layer is 3 learning rate is 0 05 lstm neuron is 32 and fc neuron is 128 the tradaboost lstm network can achieve the minimum 10 fold cv rmse and the maximum r 2 value therefore this set of parameters was adopted in subsequent experiments 4 2 missing data imputation for field water quality information in this section to further intuitively and clearly verify the feasibility of the proposed algorithm we implement a series of experiments that involve imputing the field dissolved oxygen do concentration data to fill gaps in measurements at water quality monitoring stations taking the 50 missing data rate as an illustration the dataset is mixed with two parts one is the 2 year time series of observations from august 2018 to august 2020 from the source domain s 3 and the other is the nearly 2 month time series of observations from july 2019 to august 2019 and from november 2019 to december 2019 from the target domain s 4 note that after applying sliding window method the above mixed samples would be divided into the training partition that accounts for 90 of the data and the validating partition that accounts for 10 of the data the testing partitions from august 2019 to november 2019 are shown in table 4 in section 3 2 to visually display the training validation testing parts in the data samples of the experiment process they are explicitly represented as fig 9 for one case of missing rate as 0 5 furthermore to justify the overfitting problem we conduct training process for the tradaboost lstm algorithm and the rmse mae mape and r2 values for the training process are depicted in table 8 when the number of tradaboost iteration reaches the maximum the final filling result will be obtained and comparisons between the true values and predicted values are shown in fig 10 for different missing data rates figure 10 a b c d e illustrates that the imputation results obtained by the proposed algorithm are satisfactory especially for large scale consecutive data gaps by analyzing the statistical coefficient rmse r2 of each case the actual imputation accuracy is very high and the proposed model exhibits more trivial deviations between the original and post imputation data than those of traditional models this is because all features or knowledge of the source domain and target domain has been captured from historical samples by the novel tradaboost lstm model besides it is worth noting that when missing rate is small such as 0 1 the final imputation accuracy is obviously higher than that of high missing rate such as 0 9 the reason lies the fact that more data points of the target domain can be trained and validated in the case of missing rate of 0 1 fig 10a than in the case of missing rate of 0 9 fig 10e moreover this paper aims to study the large scale consecutive missing data so it can be concluded from fig 10 d e that although the actual observations produce huge fluctuations the final imputation of tradaboost lstm are always more stable and robust over the other traditional methods finally in view of negative transfer the imputation accuracy still has huge room for improvement especially referring to large scale consecutive missing data gaps and these limitaions and future works will be later summarized in the section 6 5 discussion in this section first we compare the proposed method with the other baseline methods in terms of their effectiveness in imputing large scale consecutive missing values second studies on different stations are carried out by the proposed method finally the mechanism behind the effectiveness of the proposed method over the others are mainly discussed 5 1 comparison of different imputation algorithms and missing data rates in this paper the tradaboost lstm algorithm based on transfer learning and deep learning is designed as the crucial component of the large scale consecutive missing data imputation framework to show the competitive performance of the proposed algorithm the imputation results are compared with those of several other baseline methods including the mean autoregressive integrated moving average arima support vector regression svr traditional lstm adaboost lstm and tradaboost elm which merges tradaboost and extreme learning machine elm among these methods the mean method is the easiest to implement arima excels at time series processing the svr scheme is considered as a probabilistic imputation method lstm and adaboost lstm as newly emerging advanced deep learning methods are reported to have immense potential for improving the imputation performance tradaboost elm chen and wang 2019 is proved as an effective prediction algorithm based on transfer learning and it performs better than most conventional methods since large scale consecutive missing data imputation is more complicated than random missing data imputation it is necessary to dive into the performance of the tradaboost lstm algorithm in detail for large scale consecutive time series based on different missing data rates this paper assumes that the last segment of the complete time series block the target monitoring station s 4 has no existing observations and the missing data rate is set as λ 0 1 0 3 0 5 0 7 0 9 the results of the comparison between the tradaboost lstm algorithm and other methods for different consecutive missing data rates are shown in table 10 first as the missing data rate boosts the values of the rmse mape and mae generally increase while the values of r 2 decrease for all the algorithms this result is reasonable because as the missing data rate increases more prior information associated with training samples can be exploited for accurate imputation furthermore we specifically plot the r 2 values for all algorithms as presented in fig 11 notably the traditional statistical and machine learning methods arima and svr exhibit poor performance overall in addressing the imputation problem and deep learning based methods lstm adaboost lstm tradaboost elm and tradaboost lstm generate higher r 2 values reflecting better imputation performance second from table 10 and fig 11 as the missing data rate increases especially higher than 0 5 the value of r 2 for the proposed algorithm is not only significantly higher than the corresponding values for the other algorithms but also displays the slowest decreasing trend is the slowest to better illustrate and compare the r 2 trend for different missing data rates the δ r 2 metric is defined in this paper as expressed in eq 16 16 δ r 2 r 2 λ 0 1 r 2 λ 0 9 r 2 λ 0 1 where λ is the missing data rate and δ r 2 means the sensitivity of r 2 to λ the calculated δ r 2 values are shown in table 9 it can be concluded that the δ r 2 values generated by the tradaboost lstm algorithm are the lowest mainly because when the missing data rate is higher than 0 5 the proposed algorithm can take full advantage of the transfer learning to transfer related knowledge or information from the source domain to the target domain in summary the tradaboost lstm algorithm outperforms other methods and is less sensitive to missing data rate 5 2 studies on different stations in the previous sections we only choose one monitoring station s 4 as the target domain to conduct the experiments to further provide a better overview of the performance of the proposed algorithm supplemental studies involving the remaining nine stations with different missing data rates were conducted to validate the imputation accuracy and model effectiveness table 11 shows the r 2 values of all sequences calculated by tradaboost lstm at different stations with different missing rates table 11 show that the r 2 values of s 1 s 5 and s 10 are the lowest because the monitoring stations deployed around them are sparse from fig 6 in section 3 1 and valid knowledge cannot be extracted and transferred into the imputation process this finding suggests that when referring to large regions and long observation periods the more concentrated the deployment of monitoring stations is the stronger the model robustness to missing data therefore the tradaboost lstm method applied to overcome such issues fits intuitive cognition well 5 3 mechanism behind the tradaboost lstm model in this paper there are two unique aspects worthy of attention first is the issue definition little studies explored the problem of large scale consecutive missing data gaps for the small proportions of missing data that randomly distributed the conventional methods can perform well however large scale consecutive missing data would make conventional methods using sliding windows short of enough knowledge to transfer the nearby existing values for imputation therefore the paper attempts to use the transfer learning technique to address this challenging problem the second point is the unique development of state of art transfer learning based method namely tradaboost lstm compared with other traditional models the proposed novel algorithm called tradaboost lstm combined the lstm model and transfer learning technique lstm can capture the effective long term dependencies and transfer learning can transfer the knowledge and information of the most similar samples in the source domain to the missing data in the target domain the proposed model is applied to impute missing data gaps for water quality do concentrations therefore after conducting a series of experiments results show that the imputation accuracy is significantly improved by the tradaboost lstm model and the novel model also has a great generalization ability for large scale consecutive missing gaps at different corresponding stations with various missing rates 6 conclusions and future work with the arrival of the big data era incomplete data exists in most big data environments due to human errors or system failure subsequently missing data have a serious influence on the performance of data driven tasks and the accuracy of data based decision making therefore as an increasing number of studies have focused on data mining and analysis effectively and scientifically address missing data issues has become a critical concern in this paper we mainly focus on missing data imputation in the water quality field most existing missing data imputation methods for water quality data have only been applied in the random and small or short scale missing data scenarios to overcome these limitations this paper proposes an advanced large scale consecutive missing data imputation framework that is based on the fusion of transfer learning and deep learning tradaboost lstm the tradaboost lstm algorithm takes full advantage of a long short term memory lstm network and instance based transfer learning with tradaboost lstm has been reported to be excellent for time series processing and easily captures the long term dependencies among the time series and transfer learning is implemented to leverage the knowledge or related information from complete time series source domain to incomplete time series target domain finally in this paper the proposed algorithm is applied to impute do concentrations in sample sets obtained by 10 water quality monitoring sensors in the qiantang river basin located in hangzhou zhejiang province china moreover to further reduce the negative transfer problem we adopt the dynamic time warping dtw method to choose the suitable source domain with complete data most similar to the target domain with incomplete data after conducting a series of experiments the results show that the proposed algorithm yields approximately 15 25 higher imputation performance than other imputation methods in addition to the above contributions the proposed imputation algorithm still presents some limitations first a single station s 3 was adopted as the best target domain based on the dtw values however table 5 shows that the average dtw values of stations s 1 s 3 and s 8 are slightly different in future works to potentially improve the imputation accuracy it is expected that the combination of the stations s 1 s 3 and s 8 has the chance to be compared with those for single station s 3 second the feature selection approach used in this study which involves a sliding window is commonly used to extract subsequence information from original long time series but the results of feature selection are not always satisfactory therefore an effective nonlinear feature selection method such as the gamma test method can be extended to calibrate the optimal features finally if do concentrations are not available at adjacent monitoring sites it is unreasonable to utilize transfer learning for missing data imputation in this case however corresponding studies masoud haghbin et al 2020 have noted that there are certain correlations among various kinds of water quality monitoring data e g ph do co d cr and temperature therefore when do concentration data are missing we could transfer other water quality information to impute the missing data in short it is expected that the proposed novel algorithm could be utilized to extend our research to address these limitations in the future credit authorship contribution statement zeng chen conceptualization methodology software data curation writing original draft huan xu formal analysis methodology peng jiang funding acquisition project administration resources shanen yu conceptualization validation guang lin visualization igor bychkov data curation software alexey hmelnov gennady ruzhnikov software formal analysis ning zhu investigation zhen liu conceptualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the leading talents of science and technology innovation in zhejiang provincial ten thousands plan no 2018r52040 and the international science and technology cooperation program of zhejiang province for joint research in high tech industry no 2016c54007 appendix dynamic time warping dtw algorithm dtw is a nonlinear programming technique that involves model similarity matching for time series by bending and aligning the time axis dtw can be designed to calculate the similarity between two time series and select the shortest distance between values the details of the dtw algorithm are presented as follows step 1 suppose x x 1 x 2 x m and y y 1 y 2 y n represent two kinds of time series m and n are the lengths of two sequence respectively the two time series sequences can be formulated as an m n distance matrix d m n and the elements d ij d x i y j of the distance matrix d m n denote the distance between x i and y j step 2 the optimal warping path in dtw denoted by w w 1 w 2 w k consists of adjacent elements in the matrix d m n where represents the kth element of w the warping path must satisfy the following three conditions a max m n k m n 1 b boundary limits w 1 d 11 and w k d mn c two adjacent elements in w must also be adjacent in d m n and extend forward namely w k a b and w k 1 a b the corresponding points between the two kinds of time series must not intersect i e 0 a a 1 and 0 b b 1 step 3 calculate the dtw distance d dtw i j between the starting points i and j of the two sequences the warping path can be obtained by dynamic programming using formulas 1 2 1 d dtw i j min d dtw i 1 j 1 d dtw i j 1 d dtw i 1 j d ij d dtw 1 1 d 11 2 d x i y j x i y j 2 d m n step 4 the d dtw i j at the endpoint in the two sequences is the dtw distance for the two time series 
4197,in recent years water quality monitoring has been crucial to improve water resource protection and management under the relevant laws and regulations environmental protection department agencies monitor lakes streams rivers and other types of water bodies to assess water quality conditions the valid and high quality data generated from these monitoring activities help water resource managers understand the existing pollution situations energy consumption problems and pollution control needs however there are inevitably many problems with water quality data in the real world due to human mistakes or system failures one of the most frequently occurring issues is missing data although most existing studies have explored classic statistical methods or emerging machine deep learning methods to fill gaps in data these methods are not suitable for large scale consecutive missing data problems to address this issue this paper proposes a novel algorithm called tradaboost lstm which integrates state of the art deep learning theory through long short term memory lstm and instance based transfer learning through tradaboost this model inherits the full advantages of the lstm model and transfer learning technique namely the powerful ability to capture the long term dependencies among time series and the flexibility of leveraging the related knowledge from complete datasets to fill in large scale consecutive missing data a case study involving dissolved oxygen concentrations obtained from water quality monitoring stations is conducted to validate the effectiveness and superiority of the proposed method the results show that the proposed tradaboost lstm model not only improves the imputation accuracy by 15 25 compared with that of alternative models based on the obtained performance indicators but also provides potential ideas for similar future research keywords water quality transfer learning lstm tradaboost large scale consecutive missing data 1 introduction 1 1 background with the rapid development of industrialization and urbanization water resource protection and water pollution treatment have become popular and important topics worldwide hasanzadeh et al 2020 li et al 2020a zhang et al 2017 to control water pollution and reduce its adverse impact on the aqueous ecological system and human society kisi and parmar 2016 li et al 2017b many studies on temporal and spatial predictions of water quality chen et al 2020 evaluations of the factors that influence water quality pollution xu et al 2019 and data driven water quality models jin et al 2019 have focused on improving water quality monitoring in small watersheds chapman et al 2016 murphy and sprague 2019 when performing such studies a valid and high quality water quality dataset is a priority for generating reliable and reasonable results zhou 2020 however most water quality data e g ammonium nitrogen nh 3 n pondus hydrogenii ph dissolved oxygen do chemical oxygen demand co d cr etc are automatically sampled by bioheavy metal sensors at different monitoring stations zhang et al 2019 equipment failures regular maintenance inadequate sampling procedures artificial changes in sensor parameter settings and other factors may lead to numerous missing values in raw water quality datasets ma et al 2020a these missing data will significantly limit water quality prediction and influential factor evaluation therefore as an increasing number of studies on water quality have turned into data driven analysis it has become urgent to solve the missing data problem in this domain 1 2 literature review in recent years several scholars have realized the severity of the missing data problem and explored many approaches to reduce the negative effects of missing data seyed babak haji seyed asadollah et al 2021 the most typical approaches for dealing with missing data fall into two major categories deletion processing and imputation processing armijo olivo et al 2009 although deletion processing simply discards the missing observations in many cases and is easily implemented some studies have shown that it can extensively decrease the statistical performance of subsequent models and produce bias in experiments different from deletion processing imputation processing maintains the length of time series by estimating missing values various imputation mechanisms have been proposed and used in a wide range of applications one of the most straightforward imputation methods is linear statistic interpolations in which data gaps are filled using the corresponding mean median or mode of the existing observations however these methods can generate flat and dumb filling values ma et al 2020b in geospatial analysis of hydrology li et al 2017a some spatial interpolation methods e g inverse distance weighting idw ke et al 2011 kriging dai et al 2003 and splines giang et al 2013 interpolation etc are widely used to impute data however it is unwise to consider only spatial elements in the missing value estimation time series statistical models e g arima tencaliec et al 2015 can reflect the fluctuating trends of true values but lead to phase lag in the filling processing with the popularity of machine learning and deep learning methods that utilize series of internal data features to train imputation models an increasing number of related methods have been proposed for missing data imputation ahmad sharafati 2020 for example the expectation maximization em algorithm gaetan and yao 2003 fills missing values using maximum likelihood estimations obtained based on the existing observations gaetan and yao 2003 fuzzy clustering method fcm and k nearest neighbour knn method adopt the mean value of relevant observations from neighbouring complete sequences as the imputation value tang et al 2015 zhang and software 2012 support vector regression svr and artificial neural network ann have also been utilized as common tools to generate reliable filling results seow and ziegler 2017 tabari and hosseinzadeh talaee 2015 in the state of the art deep learning domain che et al che et al 2018 developed a deep learning model that employs gated recurrent unit gru for missing data imputation they merged the time interval and coverage of missing data in the gru model to capture the long term time dependence of time series data yu et al yu et al 2013 proposed a regularized extreme learning machine r elm to impute missing values the main advantages of r elm are twofold first the time complexity is very low and second there is no need to tune the related parameters in advance however standard machine learning and deep learning methods require large amounts of labelled data for training şahin et al 2011 so they are not well suited for imputing missing water quality data notably in such datasets gaps in labelled data are generally consecutive and can be extensive 1 3 motivation and contributions this paper investigates the qiantang river basin that flows through hangzhou zhejiang province china as a case study the total length of the qiantang river mainstream is 319 8 km and the basin drainage area is 13 902 k m 2 hu et al 2012 yaseen et al 2018 based on the existing literature few water quality monitoring stations are currently deployed in the qiantang river basin excluding in areas with special environments or that are difficult to sample e g sea zones is few and the estimated price required to purchase a set of water quality monitoring equipment is very high li et al 2019 therefore there are two scenarios considered in the water quality sampling process first due to sensor limitations the data sampled at the water quality monitoring stations are large proportionally consecutive missing second expensive monitoring equipment special environments and areas that are difficult to sample results in blind monitoring spots which can significantly affect the subsequent water quality analysis the key to solving the above two types of water quality problems is determining how to fill in the numerous historical data gaps moreover most previous studies verified that emerging machine and deep learning methods can provide more remarkable performance in missing data imputation when compared with classical statistical methods however the existing techniques have only been applied in the situations where the missing rate was low such as less than 30 and the missing values were random anh et al 2011 consecutive missing values accounting for 40 90 of a dataset have not been imputed in fact it is difficult to solve the large scale consecutive missing data issue because as the missing rate increases the relevant prior statistical information or training samples are insufficient for tuning the accuracy of the model some advanced artificial intelligence techniques have illuminated various study fields but limited research have explored their use in addressing the large scale consecutive missing data imputation problems in water quality monitoring in this paper we explore a novel model called tradaboost lstm to solve this problem the proposed algorithm encompasses the advantages of the transfer learning technique and the lstm model and it respectively aims to fill large scale consecutive gaps in missing data while capturing excellent long dependent trends from observations of similar time series additionally the new algorithm utilizes the similarity of water basin partitions learns from adequate and long period data information obtained at other water quality monitoring stations and transfers knowledge to produce the complete time series using an lstm neural network the proposed algorithm is provided with full advantages of the transfer learning technique and the lstm model where it respectively aims at the issue of large scale consecutive missing data and captures excellent long dependent moving trends from the observations of similar time series the essence of the new algorithm proposed in this paper aims to utilize the similarity of water basin partitions and then to learn from adequate and long period data information of the other water quality monitoring stations and finally to transfer the knowledge into the incomplete time series using lstm neural network additional details about this novel method will be introduced in subsequent chapters the main contributions and novelty of this study are summarized as the following four aspects our research object focuses on large scale consecutive missing data a hybrid transfer learning based deep learning method is proposed and illustrated the feasibility of the proposed algorithm in actual water monitoring is explored negative transfer is controlled within limits by a novel and classical algorithm to verify the effectiveness of the proposed method a case study in the qiantang river basin is performed the large scale consecutive missing dissolved oxygen do concentrations are imputed the results show that as the missing rate increases the proposed method can improve the estimation accuracy of missing imputations compared with those of other methods the rest of the paper is organized as follows section 2 details the proposed methodological framework including the transfer learning based tradaboost and deep learning based lstm section 3 presents and analyses the experimental results in missing data imputation for an actual water quality monitoring case section 4 and section 5 are extended to analyze and discuss the experiment results respectively and section 6 concludes this study and elaborates on possible future work 2 methodological framework design as illustrated in fig 1 in this paper the methodological framework for missing data imputation can be divided into two parts data pre processing and algorithm execution in the data pre processing firstly an incomplete data sequence collected from a sensor at a water quality monitoring station is cleaned standardized normalized and defined as the experiment data in the target domain secondly we explore the time series similar method e g dynamic time warping dtw to find out the monitoring stations for which the associated complete monitoring data are the most similar to the target domain experimental data then the data from these stations are used as the source domain reference finally the sliding windows algorithm is used to construct time series of training and testing samples in the process of algorithm execution the proposed tradaboost lstm method which integrates instance based transfer learning tradaboost and advanced deep learning lstm neural network is designed to address the large scale consecutive missing data problem at the water quality monitoring stations the following sections will present different components of the methodological framework in detail 2 1 transfer learning technique since this paper adopts the transfer learning method tradaboost we first provide a brief review of the transfer learning field as a novel branch of artificial intelligence transfer learning pan et al 2009 has recently attracted considerable interest because it focuses on solving the concern that the labelled data in the target domain may be too limited to effectively used to train a model the transfer learning method allows the distributions of training samples to be different lv et al 2019 in other words transfer learning is a unique technique that leverages or adapts well established knowledge from a related source domain to boost the learning in the target domain weiss et al 2016 in this study due to the large scale consecutive missing data issues at target monitoring stations the unavailability of time series of labelled training data leads to unsatisfactory filling performance therefore transfer learning is applied to transfer knowledge from the complete sequences obtained at adjacent monitoring stations to an incomplete sequence in the target domain in general based on the learning pattern conventional transfer learning methods can be roughly categorized as instance transfer model transfer feature parameter representation transfer or relation knowledge transfer among them the instance based transfer method is intuitively comprehensive and explicit dai et al dai et al 2007 proposed a well known instance based transfer learning algorithm named tradaboost which is an extension of adaboost and assumes that only the distributions of source and target domain data are different the motivation of the tradaboost algorithm is based on the concept that a portion of the inherent data information in the source domain with a similar distribution as the target training data may be more valuable than other portions of data therefore the tradaboost algorithm attempts to iteratively reupdate the weight of each sample of target and source domain data in accordance with the corresponding negative or positive contribution tang et al 2020 in this paper when conducting large scale missing data imputation for data from the target monitoring station the tradaboost based transfer learning approach is utilized to learn the inherent long term trend characteristics from the complete time series of data obtained at the source monitoring stations 2 2 selection of source domain samples as mentioned in section 2 1 transfer learning is one of the most important techniques used in this study the core foundation of transfer learning is identifying the similarity between source and target domains to guarantee the successful transfer of knowledge however when the source domain is weakly related to the target domain source domain data will exert a negative influence on learning in the target domain pan et al 2009 noted that it is very significant to avoid negative transfer in the transfer learning research and the key to keeping negative transfer within limits is to develop a suitable criterion to measure similarity hence after raw water quality data sampling cleaning and normalization we should first determine the source domain with data most similar to those in the target domain in this study when imputing missing water quality data the sequence with missing data represents the target domain and the complete sequences are potential source domain data based on the existing literature folgado et al 2018 fu 2011b li et al 2020b there are several commonly used similarity measurement criteria such as the euclidean distance ed pearson correlation coefficient pearson kullback leibler divergence k l divergence dynamic time warping dtw and longest common subsequence lcss etc overall an appropriate similarity measurement criterion depends on the inherent data properties of the time series the length of the time series must be equal in the ed approach but this limitation may lead to the inappropriate transimission of time information pearson s correlation coefficient is widely used to measure the strength of the correlation between two variables but it is not ideal for nonlinear cases k l divergence is a common indicator used to measure the similarity between two probability distributions and the more distinct the difference between the two distributions the larger the k l divergence is however the main disadvantage of k l divergence lies in not considering distance and asymmetry the lcss is mainly related to shape similarity rather than the location similarity and is highly time consuming the dtw algorithm can use time series of different lengths and is capable of accurately capturing the similarity between the trends of two time series in this paper considering the important fact that the length of continuous gaps in large scale water quality data varies dtw is selected as the key similarity measurement criterion to inhibit the negative transfer in this study we define s t as the incomplete time series namely the target domain data this series can be denoted as s t s t 1 s t 2 where s t 1 is the complete time series without missing data s t 2 is the consecutive missing data the next important step is to select source domain time series s s where 1 s n s t and n is the total number of water monitoring stations the dtw algorithm refer to the appendix for details is used to determine the suitable source domain most similar to the target domain based on the following constraint 1 s m min s s d t w s s s t 1 1 s n and s t where s m denotes the suitable source domain among the domains of water quality monitoring stations 2 3 sliding window when exploiting the proposed transfer learning based method for processing time series data the original time series can be transformed into input feature and output label subsequences for better model training to do this the sliding window sw zhang et al 2019 zivot and wang 2007 method is implemented to construct features inputs and labels outputs for training samples based on continuous time series of observations a time series training sample e g the window size is h in fig 2 obtained by the sw method can be expressed as follows 2 s t h s t h 1 s t 3 s t 2 s t 1 s t where s s 1 s 2 s 3 s n is a complete sequence s t h s t h 1 s t 3 s t 2 s t 1 represents one feature input at time t s t represents the output label at time t and h represents the sliding window size the sliding window method works by anchoring the left side point s 1 and then moving or scanning through the time interval n h to reach the right side point s n h of the time series during this process each moving or scanning step provides a training sample the sliding window size h not only makes a significant difference in the number of time series training samples but also affects the feature input subsequences associated with each training sample it can be concluded that a small window size will provide more training samples than a large window size but the samples may not contain enough feature input information a larger window size will result in fewer training samples and irrelevant interference information will be included in the model input set therefore it is significant to select a suitable sliding window size h as will be revealed in a later section 2 4 long short term memory lstm model to accurately generate missing water quality data through imputation in this paper we use the long short term memory lstm as the foremost component of the imputation framework here we review the evolution of lstm methods chen and wang 2019 hochreiter and schmidhuber 1997 li et al 2017c shun chen 2019 tian et al 2018 unlike the traditional artificial neural network ann which fails to strongly associate information from previous time series with the that from the current series a deep learning architecture recurrent neural network rnn is designed to promote dynamic temporal behaviour by replacing the input of the current state with the output of the previous state unfortunately as the distance between the relevant information and the location where it is needed increases the rnn cannot maintain the long term dependencies in the input sequences therefore lstm was developed to address the vanishing or exploding gradient problems in the rnn computational process lstm an essentially improved extension of rnns excels in interpreting long time series of historical data the common architecture of an lstm unit is shown in fig 3 the lstm unit is composed of a cell and three different components usually called gates associated with the flow of information inside the unit intuitively thecellis responsible for remembering the dependencies among the elements in the input sequences and three gates namely the input gate inputgat e t the forget gate forgetgat e t and the output gate outputgate t cooperatively enable the lstm cell to perform backpropagation and store information from previous time series the formula for the current state h t of an lstm unit at time t can be expressed as follows 3 inputgat e t s i g w i x t h t 1 η i 4 forgetgat e t s i g w f x t h t 1 η f 5 c t tanh w c x t h t 1 η c 6 outputgat e t s i g w o x t h t 1 η o 7 c t i n p u t g a t e t c t 1 f o r g e t g a t e t c t 8 h t c t o u t p u t g a t e t where tanh denotes the activation function tanh x e x e x e x e x sig denotes thelogistic sigmoid function sig x 1 1 e x c t 1 denotes the state of the previous cell at time t 1 denotes the hadamard product x t denotes the inputs at time t w denotes the weights in the lstm unit and η denotes the offset vector 2 5 tradaboost lstm algorithm design the lstm model has been applied in various domains due to its excellent performance in effectively extracting long term information from time series data however lstm has extreme limitations in yielding accurate results when there are not enough training data the reason for this phenomenon lies in the fact that machine or deep learning methods such as lstm networks necessarily use many previous training values to predict a current unknown value thus if sufficient prior information and historical knowledge are not available the lstm model will generate improper results when applied to fill in the missing data especially for large scale consecutive missing values in this paper in this study we propose the tradaboost lstm algorithm to address this problem the working process of this novel large scale consecutive missing data imputation algorithm which was designed based on tradaboost lstm is shown in fig 4 first the source and target domain samples are mixed to construct the training sample set as the model input second the tradaboost algorithm constructs a series of weak learners lstm for regression based prediction and a tradaboost based encoding layer which reupdate the weight of each sample for the current weaker learner this process depends on the learning error of the previous weak learner in the test set associated with the target domain fig 5 illustrates the tradaboost based encoding layer as the number of iterations increases for the weak learner the weights of source domain samples similar to the target domain training samples increase and the weights of source domain samples that are not similar to the target domain training samples decrease finally the lstm based tradaboost algorithm is established to implement large scale consecutive missing data imputation and a weighted averaging strategy is utilized to predict the corresponding imputation values the description of notations used in the algorithm is listed in table 1 additional details of the proposed process of transfer based large scale consecutive missing data imputation are presented in table 2 it is worth noting that the values of some parameters in the proposed algorithm may influence the accuracy of the imputation results one example is 1 the number of source training samples m generally increasing the number of samples can improve the learning performance of each weaker learner but time requirement and computational complexity will increase therefore we adopt a compromise strategy in which the ratio of the length of the source domain to the length of the target domain is set as approximately 6 another example is 2 the missing data rate λ in the target domain the missing data rate λ is defined as λ length m i s s d a t a length f u l l d a t a as this rate approaches zero λ 0 the number of training data in the target domain will boost and the accuracy of the imputation results will be more dependent on the training data in the target domain alternatively as the missing data rate approaches one λ 1 the number of training samples in the target domain will decrease and the accuracy of the imputation results will be affected by the training data in the source domain to further improve the imputation accuracy for large scale consecutive missing data we use an imputation value from the source domain as a reference and it can be formulated as s s m e a n t t finally the output values of the proposed algorithm and the source domain reference are combined to generate the final reliable results based on the missing rate parameter λ the corresponding formula can be expressed as follows 9 f i n a l v a l u e k λ s s k m e a n t t 1 λ i m p v a l u e k where s s denotes the source domain data in a given period mean t t denotes the average value of the target domain data and i m p v a l u e denotes the imputation result from the proposed algorithm in summary in this paper a novel methodological framework based on instance based transfer learning tradaboost and temporal model based deep learning long short term memory lstm is proposed to improve the imputations of large scale consecutive missing data to evaluate the effectiveness of this proposed algorithm a case study of consecutive missing dissolved oxygen do concentrations will be carried out in the following sections 3 case study 3 1 data explanation in this paper a case study was conducted in the qiantang river basin in hangzhou zhejiang province china we collected the dissolved oxygen do concentration as raw experimental dataset records data source https www zjemc org cn obtained every four hours reported by 10 water quality monitoring stations in the qiantang river basin for two years from august 2018 to august 2020 the reason for choosing do is that the level of do is an important indicator of the water quality and self purification level of water bodies in water quality and self purification evaluations a high do concentration is beneficial for the degradation of pollutants while a do concentration that is too low is not conducive for the growth of aquatic organisms hypoxia often occurs in the qiantang river the largest river in hangzhou mainly due to the considerable pollution burden thus to explore some approaches for addressing the issue of hypoxia it is necessary to monitor the do concentration the distribution of the 10 water quality monitoring stations in the qiantang river basin is shown in fig 6 and a detailed summary of raw do concentration data summary is presented in table 3 table 3 shows that 10 water quality time series are generated based on the 10 monitoring stations 3 2 data pre processing this study was performed to investigate a new framework for imputing missing do concentrations in observed datasets as shown in fig 1 the quality of raw data will strongly influence the accuracy of the imputation outcome however the obtained water quality samples are always influenced by noise and external outliers therefore water quality data should be cleaned before running the proposed algorithm the whole process of data cleaning is described in fig 7 first the pauta criterion byer david 2005 is adopted for the detection of external outliers it is assumed that a sample that exceeds three times the average is judged as an external outlier and removed second we use classical wavelet transform he et al 2008 to remove noise e g gaussian white noise from data at last the box plots of clean data after pre processing in ten monitoring stations are shown as fig 8 to clarify this point besides to narrow feature dimension differences and accelerate the convergence of weight parameters for model training the do dataset finally were standardized to 0 1 because the sampled original do data do not include any missing values this study assumes that the original do data of a certain station are deficient to validate the imputation performance of the tradaboost lstm algorithm for large scale consecutive missing values for illustration the dongzi monitoring station s 4 with 702 complete samples ranging from 12 07 2019 to 08 12 2019 is selected as the target domain for missing data imputation in this paper detailed information on the consecutive missing data is listed in table 4 for five types of missing rate λ 0 1 0 3 0 5 0 7 0 9 3 3 experimental setup as mentioned in section 2 2 we use the time series sequence of station s 4 to establish the target domain s t 4 which can be categorized into two parts s t 4 1 and s t 4 2 where s t 4 1 is the complete sequence and s t 4 2 is the sequence with consecutive missing values it can be deduced that the missing data rate is expressed as λ length s t 4 2 length s t 4 when the proposed tradaboost lstm algorithm is applied to impute the missing values s t 4 2 the most suitable source domain should be first determined in this paper the dtw algorithm is adopted as a similarity measurement method for the target and source domains according to the calculation process in section 2 2 when s t 4 is chosen for the target domain the sequence with consecutive missing values the similarities between the remaining stations and target station s 4 are presented in table 5 the results show that when the missing data rates are preset to λ 0 1 0 3 0 5 0 7 0 9 s 3 is determined to be the most suitable source station based on the high similarity to the target monitoring station we define the experimental data of monitoring station s 3 as the source domain s t 3 hence s t 3 the source domain and s t 4 the target domain are chosen for the subsequent experiments note that after the experimental datasets for the source and target domains are prepared the training and testing datasets can be separately expressed as s t 4 1 s t 3 and s t 4 2 for transfer algorithm inputs and outputs in this paper finally in accordance with the content in section 2 3 the sliding window sw is used to convert the experimental time series data into temporal samples with features and labels the temporal samples of the training data are established as shown in eq 10 and the temporal samples of the testing data are established as shown in eq 11 10 s t 3 h s t 3 h 1 s t 3 3 s t 3 2 s t 3 1 s t 3 s t 4 h 1 s t 4 h 1 1 s t 4 3 1 s t 4 2 1 s t 4 1 1 s t 4 1 0 t 3 l e n g t h s t 3 h a n d 0 t 4 l e n g t h s t 4 1 h 11 s t 4 h 2 s t 4 h 1 2 s t 4 3 2 s t 4 2 2 s t 4 1 2 s t 4 2 0 t 4 l e n g t h s t 4 2 h where h represents the sliding window size and the most optimal window size for the case study will be determined later 3 4 evaluation criteria four indicators including the root mean square error rmse mean absolute percentage error mape mean absolute error mae and fitting coefficient r2 are introduced to evaluate the performance of the deterministic imputation algorithms the corresponding calculations of these four indicators were detailed in previous studies maría elisa quinteros et al 2018 tao su et al 2021 which are expressed as eq 12 15 low values of rmse mape mae and high values of r 2 indicate high imputation accuracy and excellent performance 12 rmse t 1 n y t y t 2 n 13 mape 1 n t 1 n y t y t y t 14 mae 1 n t 1 n y t y t 15 r 2 1 t 1 n y t y t 2 t 1 n y t y t 2 where y t y t and y t means the predictive imputation value i e model output observed true data and the average of the observed true data at the time point t respectively n means the number of the whole temporal samples it is noted from the above eq 12 15 that the lower value of rmse mape mae and higher value of r 2 indicate better performance of the model 4 experimental results 4 1 determination of the window size and network parameters as mentioned the most appropriate sliding window size h should be selected to optimize the imputation of large scale consecutive missing data in this paper based on some existing literature fu 2011a ma et al 2020c ma et al 2020d the candidate of h 2 4 6 8 10 is explored to select the optimal window size when the missing data rate is 0 1 0 3 0 5 0 7 and 0 9 finally we use the four metrics in section 3 4 to obtain a feasible and stable comparison of different cases table 6 presents the results it can be concluded that as sliding window size slowly increases the rmse mape and mae decrease and the r 2 increases until reaching an optimal value at h 6 note that when the missing data rate is 0 9 the most appropriate sliding window size is particularly chosen as 4 because the number of training samples is comparatively low therefore in this paper the best sliding window size h is 6 in addition to the determination of the sliding window size the tradaboost lstm network parameters and structures should also be identified the optimal set of network parameters is reported to improve the imputation accuracy by 5 20 referring to previous studies kao et al 2020 xiang et al 2017 our prime stacked lstm architecture is comprised of one input layer a certain number of hidden layers one fully connected fc layer added at the end of the hidden layers and one output layer after the time series samples are obtained using the sliding window the observation variable input layer has 6 dimensions and the number of output layers is set to 1 namely the single step output the tanh function is selected as the activation function for the lstm layer all the weights and offset vectors are determined during the learning process and match the training data the adam optimizer is adopted because water quality data are generally noisy and the adam algorithm is suitable for addressing this problem some parameters are chosen according to our engineering experience for example the min batch size remains 32 and both the number of iterations in tradaboost tradaboost iteration and epochs is preset as 100 nevertheless the lstm structure also involves many parameters that should be optimized such as lstm hide layer the number of lstm layers learning rate lstm neuron the number of neurons in each lstm layer and fc neuron the number of neurons in the fully connected layer rather than relying on engineering experience based on the existing literature chen and wang 2019 vu et al 2020 xiang et al 2017 yan tian et al 2018 we first vary lstm hide layer as 1 2 3 4 5 learning rate as 0 001 0 005 0 01 0 05 0 1 lstm neuron as 16 32 64 128 and fc neuron as 16 32 64 128 256 and then use the grid search method to determine the most appropriate values within these ranges for overfitting or underfitting problems we further choose these optimal hyperparameters by implementing 10 fold cross validation cv in modelling and testing the entire training and validation learning process took four hours in matlab on a workstation with an intel i5 4200h 3 4 ghz cpu 8 gb memory and a single nvida gtx 950 gpu after conducting the above experiments the optimal values of the parameters in the lstm model were obtained in table 7 the results indicate that when lstm hide layer is 3 learning rate is 0 05 lstm neuron is 32 and fc neuron is 128 the tradaboost lstm network can achieve the minimum 10 fold cv rmse and the maximum r 2 value therefore this set of parameters was adopted in subsequent experiments 4 2 missing data imputation for field water quality information in this section to further intuitively and clearly verify the feasibility of the proposed algorithm we implement a series of experiments that involve imputing the field dissolved oxygen do concentration data to fill gaps in measurements at water quality monitoring stations taking the 50 missing data rate as an illustration the dataset is mixed with two parts one is the 2 year time series of observations from august 2018 to august 2020 from the source domain s 3 and the other is the nearly 2 month time series of observations from july 2019 to august 2019 and from november 2019 to december 2019 from the target domain s 4 note that after applying sliding window method the above mixed samples would be divided into the training partition that accounts for 90 of the data and the validating partition that accounts for 10 of the data the testing partitions from august 2019 to november 2019 are shown in table 4 in section 3 2 to visually display the training validation testing parts in the data samples of the experiment process they are explicitly represented as fig 9 for one case of missing rate as 0 5 furthermore to justify the overfitting problem we conduct training process for the tradaboost lstm algorithm and the rmse mae mape and r2 values for the training process are depicted in table 8 when the number of tradaboost iteration reaches the maximum the final filling result will be obtained and comparisons between the true values and predicted values are shown in fig 10 for different missing data rates figure 10 a b c d e illustrates that the imputation results obtained by the proposed algorithm are satisfactory especially for large scale consecutive data gaps by analyzing the statistical coefficient rmse r2 of each case the actual imputation accuracy is very high and the proposed model exhibits more trivial deviations between the original and post imputation data than those of traditional models this is because all features or knowledge of the source domain and target domain has been captured from historical samples by the novel tradaboost lstm model besides it is worth noting that when missing rate is small such as 0 1 the final imputation accuracy is obviously higher than that of high missing rate such as 0 9 the reason lies the fact that more data points of the target domain can be trained and validated in the case of missing rate of 0 1 fig 10a than in the case of missing rate of 0 9 fig 10e moreover this paper aims to study the large scale consecutive missing data so it can be concluded from fig 10 d e that although the actual observations produce huge fluctuations the final imputation of tradaboost lstm are always more stable and robust over the other traditional methods finally in view of negative transfer the imputation accuracy still has huge room for improvement especially referring to large scale consecutive missing data gaps and these limitaions and future works will be later summarized in the section 6 5 discussion in this section first we compare the proposed method with the other baseline methods in terms of their effectiveness in imputing large scale consecutive missing values second studies on different stations are carried out by the proposed method finally the mechanism behind the effectiveness of the proposed method over the others are mainly discussed 5 1 comparison of different imputation algorithms and missing data rates in this paper the tradaboost lstm algorithm based on transfer learning and deep learning is designed as the crucial component of the large scale consecutive missing data imputation framework to show the competitive performance of the proposed algorithm the imputation results are compared with those of several other baseline methods including the mean autoregressive integrated moving average arima support vector regression svr traditional lstm adaboost lstm and tradaboost elm which merges tradaboost and extreme learning machine elm among these methods the mean method is the easiest to implement arima excels at time series processing the svr scheme is considered as a probabilistic imputation method lstm and adaboost lstm as newly emerging advanced deep learning methods are reported to have immense potential for improving the imputation performance tradaboost elm chen and wang 2019 is proved as an effective prediction algorithm based on transfer learning and it performs better than most conventional methods since large scale consecutive missing data imputation is more complicated than random missing data imputation it is necessary to dive into the performance of the tradaboost lstm algorithm in detail for large scale consecutive time series based on different missing data rates this paper assumes that the last segment of the complete time series block the target monitoring station s 4 has no existing observations and the missing data rate is set as λ 0 1 0 3 0 5 0 7 0 9 the results of the comparison between the tradaboost lstm algorithm and other methods for different consecutive missing data rates are shown in table 10 first as the missing data rate boosts the values of the rmse mape and mae generally increase while the values of r 2 decrease for all the algorithms this result is reasonable because as the missing data rate increases more prior information associated with training samples can be exploited for accurate imputation furthermore we specifically plot the r 2 values for all algorithms as presented in fig 11 notably the traditional statistical and machine learning methods arima and svr exhibit poor performance overall in addressing the imputation problem and deep learning based methods lstm adaboost lstm tradaboost elm and tradaboost lstm generate higher r 2 values reflecting better imputation performance second from table 10 and fig 11 as the missing data rate increases especially higher than 0 5 the value of r 2 for the proposed algorithm is not only significantly higher than the corresponding values for the other algorithms but also displays the slowest decreasing trend is the slowest to better illustrate and compare the r 2 trend for different missing data rates the δ r 2 metric is defined in this paper as expressed in eq 16 16 δ r 2 r 2 λ 0 1 r 2 λ 0 9 r 2 λ 0 1 where λ is the missing data rate and δ r 2 means the sensitivity of r 2 to λ the calculated δ r 2 values are shown in table 9 it can be concluded that the δ r 2 values generated by the tradaboost lstm algorithm are the lowest mainly because when the missing data rate is higher than 0 5 the proposed algorithm can take full advantage of the transfer learning to transfer related knowledge or information from the source domain to the target domain in summary the tradaboost lstm algorithm outperforms other methods and is less sensitive to missing data rate 5 2 studies on different stations in the previous sections we only choose one monitoring station s 4 as the target domain to conduct the experiments to further provide a better overview of the performance of the proposed algorithm supplemental studies involving the remaining nine stations with different missing data rates were conducted to validate the imputation accuracy and model effectiveness table 11 shows the r 2 values of all sequences calculated by tradaboost lstm at different stations with different missing rates table 11 show that the r 2 values of s 1 s 5 and s 10 are the lowest because the monitoring stations deployed around them are sparse from fig 6 in section 3 1 and valid knowledge cannot be extracted and transferred into the imputation process this finding suggests that when referring to large regions and long observation periods the more concentrated the deployment of monitoring stations is the stronger the model robustness to missing data therefore the tradaboost lstm method applied to overcome such issues fits intuitive cognition well 5 3 mechanism behind the tradaboost lstm model in this paper there are two unique aspects worthy of attention first is the issue definition little studies explored the problem of large scale consecutive missing data gaps for the small proportions of missing data that randomly distributed the conventional methods can perform well however large scale consecutive missing data would make conventional methods using sliding windows short of enough knowledge to transfer the nearby existing values for imputation therefore the paper attempts to use the transfer learning technique to address this challenging problem the second point is the unique development of state of art transfer learning based method namely tradaboost lstm compared with other traditional models the proposed novel algorithm called tradaboost lstm combined the lstm model and transfer learning technique lstm can capture the effective long term dependencies and transfer learning can transfer the knowledge and information of the most similar samples in the source domain to the missing data in the target domain the proposed model is applied to impute missing data gaps for water quality do concentrations therefore after conducting a series of experiments results show that the imputation accuracy is significantly improved by the tradaboost lstm model and the novel model also has a great generalization ability for large scale consecutive missing gaps at different corresponding stations with various missing rates 6 conclusions and future work with the arrival of the big data era incomplete data exists in most big data environments due to human errors or system failure subsequently missing data have a serious influence on the performance of data driven tasks and the accuracy of data based decision making therefore as an increasing number of studies have focused on data mining and analysis effectively and scientifically address missing data issues has become a critical concern in this paper we mainly focus on missing data imputation in the water quality field most existing missing data imputation methods for water quality data have only been applied in the random and small or short scale missing data scenarios to overcome these limitations this paper proposes an advanced large scale consecutive missing data imputation framework that is based on the fusion of transfer learning and deep learning tradaboost lstm the tradaboost lstm algorithm takes full advantage of a long short term memory lstm network and instance based transfer learning with tradaboost lstm has been reported to be excellent for time series processing and easily captures the long term dependencies among the time series and transfer learning is implemented to leverage the knowledge or related information from complete time series source domain to incomplete time series target domain finally in this paper the proposed algorithm is applied to impute do concentrations in sample sets obtained by 10 water quality monitoring sensors in the qiantang river basin located in hangzhou zhejiang province china moreover to further reduce the negative transfer problem we adopt the dynamic time warping dtw method to choose the suitable source domain with complete data most similar to the target domain with incomplete data after conducting a series of experiments the results show that the proposed algorithm yields approximately 15 25 higher imputation performance than other imputation methods in addition to the above contributions the proposed imputation algorithm still presents some limitations first a single station s 3 was adopted as the best target domain based on the dtw values however table 5 shows that the average dtw values of stations s 1 s 3 and s 8 are slightly different in future works to potentially improve the imputation accuracy it is expected that the combination of the stations s 1 s 3 and s 8 has the chance to be compared with those for single station s 3 second the feature selection approach used in this study which involves a sliding window is commonly used to extract subsequence information from original long time series but the results of feature selection are not always satisfactory therefore an effective nonlinear feature selection method such as the gamma test method can be extended to calibrate the optimal features finally if do concentrations are not available at adjacent monitoring sites it is unreasonable to utilize transfer learning for missing data imputation in this case however corresponding studies masoud haghbin et al 2020 have noted that there are certain correlations among various kinds of water quality monitoring data e g ph do co d cr and temperature therefore when do concentration data are missing we could transfer other water quality information to impute the missing data in short it is expected that the proposed novel algorithm could be utilized to extend our research to address these limitations in the future credit authorship contribution statement zeng chen conceptualization methodology software data curation writing original draft huan xu formal analysis methodology peng jiang funding acquisition project administration resources shanen yu conceptualization validation guang lin visualization igor bychkov data curation software alexey hmelnov gennady ruzhnikov software formal analysis ning zhu investigation zhen liu conceptualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the leading talents of science and technology innovation in zhejiang provincial ten thousands plan no 2018r52040 and the international science and technology cooperation program of zhejiang province for joint research in high tech industry no 2016c54007 appendix dynamic time warping dtw algorithm dtw is a nonlinear programming technique that involves model similarity matching for time series by bending and aligning the time axis dtw can be designed to calculate the similarity between two time series and select the shortest distance between values the details of the dtw algorithm are presented as follows step 1 suppose x x 1 x 2 x m and y y 1 y 2 y n represent two kinds of time series m and n are the lengths of two sequence respectively the two time series sequences can be formulated as an m n distance matrix d m n and the elements d ij d x i y j of the distance matrix d m n denote the distance between x i and y j step 2 the optimal warping path in dtw denoted by w w 1 w 2 w k consists of adjacent elements in the matrix d m n where represents the kth element of w the warping path must satisfy the following three conditions a max m n k m n 1 b boundary limits w 1 d 11 and w k d mn c two adjacent elements in w must also be adjacent in d m n and extend forward namely w k a b and w k 1 a b the corresponding points between the two kinds of time series must not intersect i e 0 a a 1 and 0 b b 1 step 3 calculate the dtw distance d dtw i j between the starting points i and j of the two sequences the warping path can be obtained by dynamic programming using formulas 1 2 1 d dtw i j min d dtw i 1 j 1 d dtw i j 1 d dtw i 1 j d ij d dtw 1 1 d 11 2 d x i y j x i y j 2 d m n step 4 the d dtw i j at the endpoint in the two sequences is the dtw distance for the two time series 
4198,with increasing spatiotemporal resolution and accuracy the satellite based precipitation estimations have made their exploitation in hydrological applications possible especially in poorly gauged regions however their hydrological utilities still need to be evaluated in different catchments to provide useful knowledge to the end users this study investigates the performance of latest version v06 of integrated multi satellite retrievals for global precipitation measurement imerg products and their capacity to guide hydrological modelling in south china both near real time i e early run imerg e and post real time i e final run imerg f imerg products are evaluated and the post real time trmm multi satellite precipitation analysis tmpa 3b42 product is employed for intercomparison the hydrological utilities of abovementioned products are further explored over six different medium sized catchments in south china based on the grid based xinanjiang model the results indicate that the imerg f outperforms the imerg e in south china and the quality of tmpa 3b42 falls between them in terms of k g e and its components for the hydrological utility using gauge based model parameters the imerg e forced simulations show poor performance in most of the selected catchments whereas much better performance is observed for the imerg f forced simulations implementation of input specific calibration significantly improves the simulation performance of three spes and such improvement is much more pronounced for the imerg e the tmpa 3b42 forced simulations generally illustrate comparable reliability to these of imerg f but with a slightly degraded performance overall the imerg e shows high values in facilitating hydrological modelling over most ungauged catchments and the imerg f shows much better hydrological performance however the best performing imerg f forced simulations are still inferior to these driven by the benchmark precipitation and it s thus still hard to replace gauge observations completely with the spes this study provides a helpful knowledge about hydrological performance of satellite based precipitation estimations which potentially promote their widespread applications in hydrological modelling keywords satellite precipitation imerg tmpa hydrological validation south china 1 introduction as a fundamental component of global water cycle precipitation serves as the most crucial source of earth s freshwater kidd and huffman 2011 skofronick jackson et al 2017 accurate information of when where and how much precipitation falls is vital to the modeling of hydrological processes the analyses of climatic changes and the understanding of extreme weather events acharya et al 2019 however due to the strong spatiotemporal variations obtaining detailed precipitation information from the gauge network is always challenging especially over remote areas and developing countries maggioni et al 2016 besides the time latency for accessing to the gauge observations further limits their value in real time applications e g flood forecasting and landslide prediction zhang et al 2021 therefore additional data sources are urgently needed to serve as complements of gauge based observations the rapid development of satellite observations and retrieval techniques has prompted the continuous improvement of satellite based precipitation estimations spes providing new opportunities for obtaining accurate precipitation information since the launch of the tropical rainfall measuring mission trmm satellite in 1997 a series of spes is publicly available among them several well known quasi global spes namely trmm multi satellite precipitation analysis tmpa huffman et al 2007 climate prediction center morphing technique cmorph joyce et al 2004 precipitation estimation from remotely sensed information using artificial neural networks persiann hsu et al 1997 and global satellite mapping of precipitation gsmap kubota et al 2007 have been widely assessed and applied yielding substantial scientific and societal benefits li et al 2017b yang et al 2016 for instance a global flood retrospective simulation system http flood umd edu was constructed by employing the tmpa as precipitation inputs highlighting the great value of spes in flood simulation wu et al 2014 guo et al 2016 showed that the persiann climate data record can well capture both spatial and temporal patterns of drought over eastern china gao et al 2017 demonstrated the benefit of using the tmpa for flood frequency analysis over poorly gauged tributaries of the yangtze river basin although the trmm satellite stopped working on june 15 2015 it had achieved great success in providing mass quasi global precipitation data and promoting the continuous improvement of retrieval algorithms sun et al 2018 as the trmm s successor the global precipitation measurement gpm mission is the state of the art satellite based precipitation measurement program over the world skofronick jackson et al 2018 it is an international constellation of satellites including one core observatory co satellite and approximately ten partner satellites with the cooperation of these satellites the gpm mission is expected to provide accurate and timely precipitation estimation globally the integrated multisatellite retrievals for gpm imerg algorithm was thus developed to produce global precipitation estimation for the gpm mission at relatively fine spatiotemporal resolution 0 1 and 0 5 h huffman et al 2019 given its high spatiotemporal resolution the imerg products can provide detailed precipitation information compared to the tmpa the imerg algorithm uses a morphing scheme and improved retrieval algorithms for input passive microwave pmw sensors infrared satellite images and calibration reference hence the evaluation and application of the imerg products have gained much attention since the first version released on 2015 the chinese mainland has a complex topography and diverse climate conditions which makes it a good testbed for assessing the imerg products thus far several works have been conducted to assess the imerg products using gauge observations for instance su et al 2019a assessed the imerg and tmpa products and highlighted the higher accuracy of the near real time imerg products tang et al 2020 comprehensively analyzed the quality of the imerg products at hourly and daily scales from 2000 to 2018 and demonstrated that the quality of the imerg products have been improved significantly over time due to the increasing number of passive microwave samples until now the imerg products have been updated several times i e from original v03 to latest v06 leading to continuous improvement of data quality wang et al 2018 hence a timely assessment of the latest imerg products i e v06 is very important for both the potential data users and the retrieval algorithm developers given the global availability and increasing accuracy the imerg products have been widely used as input of hydrological models to obtain river discharge estimations jiang and wang 2019 maggioni and massari 2018 zhang et al 2020 over the chinese mainland several studies have attempted to provide guidelines for the use of imerg products in hydrological modelling for instance tang et al 2016 and li et al 2017a forced the coupled routing and excess storage model with the imerg product and showed that the imerg product is barely suitable for simulating the daily discharge in the gan river basin in the southern part of china yuan et al 2018 assessed the hydrological utility of the imerg products in the yellow river source region located in the northeast qinghai tibet plateau using the grid based xinanjiang model the results demonstrated that the daily discharge simulation forced with the imerg product shows comparable performance to that using the gauge based precipitation the hydrological utility of the imerg products was also evaluated in the beijiang river basin wang et al 2017 and the mishui basin jiang et al 2018b in south china results showed that the imerg products were inferior to the gauge based precipitation in the daily hydrological simulation besides su et al 2019b and yuan et al 2018 have suggested that the imerg products should be used with caution in flood simulation it is evident that performance of the imerg forced hydrologic simulations varies significantly in the above mentioned studies such differences are probably related to the heterogeneous quality of imerg products and the different characteristics of the study basins e g discharge area initial soil moisture conditions and so on besides it should be noted that various hydrological models were employed by abovementioned studies since different hydrological models have diverse foci in describing hydrological physical processes the model formulation and computational mechanisms of them have significantly differences chen et al 2013 therefore even forced by the same meteorological inputs the simulation performance from different models in the same basin may be different meanwhile a variety of model parameter calibration algorithms was adopted by these studies since the simulation performance is highly sensitive to the model parameters chen et al 2013 the uncertainty of model parameters further complicates the intercomparison of assessment results in this context it is of great value to establish a hydrological assessment framework which includes a widely used hydrological model and an effective automatic model parameter calibration algorithm to assess the hydrological utility of spes over different catchments with diverse landscapes topography and climatic conditions by this way one can get the general guidelines about hydrological utility of the spe besides previous explorations and studies mainly focused on the assessments of post real time spes few of them pay attention to the near real time products that have highly potential value in promoting the early warning system therefore the objectives of this study are twofold 1 statistically assess the quality of the imerg v06 products particularly for its near real time product based on gauge observed precipitation in south china and 2 comprehensively investigate the hydrological utility of the imerg products in different catchments by a hydrological assessment framework in addition the performance of the post real time tmpa product is also investigated and compared to the imerg products the aim of this work is to provide a timely helpful guidance about the suitability of the imerg products in hydrological modeling for the potential end users and the retrieval algorithm developers the paper is organized as follows the study area and data sets are briefly described in section 2 section 3 presents the methodology used in this research the results and discussion are arranged in sections 4 and 5 respectively and the conclusions are summarized in section 6 2 study area and datasets 2 1 study area this study focuses on the southern part of the chinese mainland which is located between 97 e 122 5 e and 20 n 36 5 n fig 1 it should be noted that the south china defined in this study covers 14 provinces and is much broader than some traditional definitions according to the china statistical yearbook of 2018 http www stats gov cn tjsj ndsj 2018 indexch htm the south china contains 56 14 population and contributes more than half of the gross domestic product of the whole china due to the combined impact of climate and hilly topography the rainstorm related disasters e g flash floods landslides and debris flows happened frequently over this area and caused heavy casualties and severe economic losses ma et al 2018 hence the accurate precipitation measurements especially for the real time precipitation monitoring are particularly crucial for this area to help establishing early warning system for disaster response to explore the hydrological utility of the latest imerg products as shown in fig 1b six medium sized catchments are selected namely beijiang river basin bjrb upper ganjiang river basin uganrb upper fuchun river basin ufcrb rongshui river basin rsrb qu river basin qurb and upper huaihe river basin uhuairb table 1 presents an overview of basic characteristics of these catchments for the period between january 2010 and december 2016 the bjrb and rsrb are controlled by the pearl river system and subjected to the typical subtropical humid monsoon climate wang et al 2017 the uganrb is the headwater catchment of the gangjiang river basin accounting for approximately 19 of the whole gangjiang river basin in area while contributing over 36 of the total discharge zhou et al 2020 the uganrb is one of the typical rainstorm regions in china which suffers from east asian monsoon climate whereby more than 70 of the total annual rainfall concentrates in the period from april to june the strong rainfall and complex terrain often result in frequent rainstorm triggered floods in the uganrb li et al 2017a the ufcrb is the source region of the fuchun river basin which locates in a densely populated and industrial area the ufcrb also suffers from the east asian monsoon climate which makes the ufcrb to be a typical flood vulnerable area the well drained qurb is the largest tributary of the right bank of the jialing river mainstem the qurb is also dominated by the east asian monsoon climate and most of the annual precipitation occurs between june and october resulting in high flood risk for the qurb gao et al 2017 the uhuairb is a typical mountain basin characterized by large streamflow fluctuations due to the large river slopes and rapid water conflux su et al 2019b the uhuairb situates across the transition area between the humid south and arid north climates of china the annual average precipitation is approximately 900 mm and 50 80 of which occurs between may and september the uhuairb faces high flood risk in the flood season which threatens the mengwa flood detention zone downstream 2 2 datasets three types of hydrometeorological datasets are employed in this study the satellite based precipitation products the benchmark precipitation product and other model inputs and validation datasets required in the hydrological modelling limited by availability of the above mentioned datasets this study focuses on the period from january 2010 to december 2016 2 2 1 satellite based precipitation products two types of satellite based precipitation products namely the tmpa and imerg products were selected in this study the gridded tmpa products are publicly available at the spatiotemporal resolution of 0 25 and 3 hour between 50 n 50 s the latest version of the tmpa products is version 7 including both near real time and post real time products the near real time tmpa relies only on a climatological bias adjustment while the post real time tmpa hereafter referred as tmpa 3b42 uses an actual monthly rain gauge analysis for bias adjustment detailed introduction of the tmpa products can be found in huffman et al 2007 based on the gpm mission the imerg algorithm aims to provide precipitation estimations by inter calibrating merging and interpolating all microwave satellite estimates together with microwave calibrated infrared satellite estimates gauge analyses and other potential precipitation estimators first the imerg algorithm merges the precipitation information from the gpm microwave imager and gpm constellation of passive microwave pmw sensors with precipitation retrieved using the 2017 version of the goddard profling algorithm gprof2017 then the imerg algorithm inter calibrates the gridded precipitation information with the gpm combined radar radiometer analysis product and creats half hourly 0 1 0 1 precipitation maps in order to fill the temporal and spatial gaps the quasi lagrangian time interpolation joyce et al 2004 is employed to the combined pmw product using displacement vectors derived from the merra2 and geos fp vertically integrated vapor tqv fields in this procedure the half hourly infrared ir precipitation rate irprecipitation maps are produced from ir based precipitation retrieval method persiann ccs hong et al 2004 later on the morphed estimations are updated with ir based precipitation observations using kalman filter weights if the time distance from the nearest pmw observation is longer than 30 min to create integrated half hourly precipitation maps precipitationuncal finally the multi satellite precipitation estimations produced in the previous step are adjusted by gauge observations to create research quality products precipitationcal the imerg algorithm runs three times for each observation time to accommodate different user requirements the first run i e early run imerg e gives a quick precipitation estimation the successive run i e late run imerg l provides a better precipitation estimation since more data are available both the imerg e and imerg l are adjusted by climatological coefficients which depend on the time and location of the observations while the last run i e final run imerg f benefits from actual monthly gauge observations hence the imerg e and imerg l are available about 4 h and 12 h after the observation time respectively but the imerg f is released approximately two months after the month of available observation besides the imerg e has only forward propagation of the microwave data in time unlike both imerg l and imerg f who use uses morphing in both directions forward and backward propagation for a more detailed description of the imerg products can be referred to huffman et al 2019 in this study the tmpa 3b42 and the latest version version 06b of imerg e and imerg f products are utilized the tmpa 3b42 was re gridded to a finer spatial resolution of 0 1 by the widely used linear interpolation method tang et al 2020 then the 3 hourly tmpa 3b42 and the half hourly imerg products are aggregated to daily following yuan et al 2018 2 2 2 benchmark precipitation products to statistically assess the selected spes the hourly china merge precipitation analysis product hereafter called cmpa at 0 1 resolution was selected as the validation dataset which can be publicly downloaded from the china meteorological data service center http data cma cn this dataset was generated by merging the hourly gauge observations from more than 30 000 automatic weather stations awss over the chinese mainland with the precipitation estimation from the cmorph products using the improved probability density function optimal interpolation method benefited from the high density of the awss e g fig 2 a for the study area and rigorous quality control of the observations the cmpa performs reasonably well in china and can capture varying features of hourly precipitation in heavy weather events tang et al 2017 independent assessment of the cmpa against the precipitation observations from approximately 2400 national weather stations also confirmed its excellent performance shen et al 2014 note that the awss employed in producing the cmpa were not used for the production of both imerg and tmpa products besides considering the possible overlap between the cmpa and the selected spes the cmorph algorithm is also used in the production of the imerg products this study only selects the cmpa grids that contain at least one aws to reduce this impact on the assessment results similarly as shown in fig 2b the periods when the aws observations are missing are also masked out from the assessment 2 2 3 model forcing and validation datasets in this study the grid based xinanjiang model is applied to the six selected catchments four precipitation datasets namely the cmpa tmpa 3b42 and two imerg products i e the imerg e and imerg f are used as model inputs to perform historical daily streamflow simulations respectively the rest of the model inputs are the near surface daily minimum and maximum temperatures obtained from the china meteorological assimilation driving datasets for the soil and water assessment tool model meng and wang 2017 the temperature inputs are also re gridded into the regular 0 1 grids and aggregated to daily temporal resolution to drive the grid based xinanjiang model the daily discharge observations of the six selected catchments are collected from the corresponding hydrometric stations see fig 1b which are available from the china annual hydrological report that published by the ministry of water resources 3 methodology in this study we first assess the imerg products against the cmpa over the entire south china and compare them with the tmpa 3b42 product later on the feasibility of using them as the precipitation input of the grid based xinanjiang model is explored over the six selected catchments 3 1 statistical metrics for assessing the satellite based precipitation estimations to quantify the accuracy of the selected spes the modified kling gupta efficiency k g e along with its three individual components are employed i e correlation coefficient cc bias ratio br and variability ratio vr kling et al 2012 among them the cc is a measure of the linear correlation between the selected spes and the cmpa the br is able to reflect the overestimation br 1 or underestimation br 1 of the selected spes indicating the average tendency of bias and the vr provides a relative measure of dispersion the k g e is an integrated statistic metric that balances the contributions of correlation bias and variability term providing an overall performance of the selected spes this set of metrics has been adopted and recommended by many other researches baez villanueva et al 2018 wang et al 2018 zambrano bigiarini et al 2017 in addition two categorical metrics namely the probability of detection pod and the false alarm ratio far are also employed wilks 2006 the pod gives the fraction of actual precipitation events that are correctly detected by the spes while the far calculates the fraction of precipitation evens that are incorrectly identified by spes in practice the categorical metrics far and pod are very effective in assessing rain area delineation and detecting the happen of raining or not raining according to tian et al 2009 and yong et al 2016 a small value i e 1 mm day is used as the rain no rain threshold to calculate the categorical metrics formulas and perfect values of abovementioned statistical metrics are listed in table 2 3 2 grid based xinanjiang model and hydrological simulation xinanjiang model is a famous lumped conceptual hydrological model which was first developed by zhao 1992 and widely applied to humid and semi humid catchments of china it is an operational hydrological model employed by numerous watershed management institutions in china yuan et al 2018 for improving its applicability in the medium to large sized watershed a grid based xinanjiang model was developed to accommodate the user demand li et al 2007 according to yuan and ren 2004 and yuan et al 2005 the grid based xinanjiang model inherits the three soil layer evapotranspiration and saturation excess runoff schemes of the lumped version for simulating actual evapotranspiration and total runoff meanwhile for modeling the hillslope runoff a gravitational water reservoir with bottom and side outlets is employed to divide the total runoff into its three components i e surface interflow and groundwater runoffs then three linear reservoirs are adopted to perform the slope runoff convergence respectively finally the muskingum routing method is used to obtain the discharge series at the outlet of the given catchment representing the routing effect of the river channel system in this study the grid based xinanjiang model is adopted for the daily discharge simulations over the six selected catchments the model is driven by gridded precipitation and potential evapotranspiration that is calculated by the air temperature based hargreaves method using the daily minimum and maximum near surface temperature hargreaves and samani 1982 the grid based xinanjiang model contains 15 parameters with 8 parameters used to simulate the evapotranspiration and runoff and 7 parameters for hillslope runoff concentration and streamflow routing yuan et al 2018 detailed information on the model parameters and their numerical range is referred to jiang et al 2017 and jiang et al 2018a in this study the model parameters are automatically calibrated using the effective and efficient shuffled complex evolution global optimization algorithm sce ua developed by duan et al 1994 to comprehensively characterize both high and low flow processes the maximum sum of the nash sutcliffe model efficiency coefficient nse and its log transformed value lognse are used as the objective function of the sce ua to search the optimal model parameters yuan et al 2018 the objective function is defined as 1 f m a x n s e l o g n s e n s e 1 i 1 n q s i q o i 2 i 1 n q o i q o 2 l o g n s e 1 i 1 n l o g q s i l o g q o i 2 i 1 n l o g q o i log q o 2 where n represents the sample number q o i and q s i are observed and simulated discharge m 3 s respectively q o and log q o are mean values of corresponding elements besides the abovementioned kg e is also considered for assessing model performance as it can provide a robust measure for a wider range of flow conditions camici et al 2018 in this study the grid based xinanjiang model is operated at the resolution of 0 1 and two model calibration scenarios are designed to better investigate the hydrological utility of the selected spes scenario i the grid based xinanjiang model is forced with the benchmark precipitation i e the cmpa for calibrating the model parameters and the calibrated model is then driven by the selected spes for both calibration and validation periods this scenario is a widely used and known as the standard calibration procedure which can directly investigate the hydrological utility of the selected spes in this study the calibration and validation periods are january 2010 to may 2014 and june 2014 to december 2016 respectively note that the year of 2014 is excluded from the hydrological simulation over the ufcrb since the corresponding discharge observations are not available scenario ii the grid based xinanjiang model is forced with the selected spes for calibrating and validating the model parameters in both calibration and validation periods this scenario is defined as the input specific calibration procedure that the input specific parameters are adopted to conduct streamflow simulations which is an important option for hydrological applications in ungauged basins without gauge precipitation observations 4 results this section first presents the overall evaluation results of the daily aggregated spes using the cmpa over the whole south china and the selected catchments section 4 1 subsequently the hydrological utility of the daily aggregated spes is evaluated using the grid based xinanjiang model over the six selected catchments section 4 2 4 1 statistical evaluation of satellite based precipitation estimations 4 1 1 mean precipitation and precipitation frequency figs 3 and 4 show the average annual precipitation and corresponding precipitation frequency derived from the three spes and cmpa respectively note that the value of 1 mm day same as the threshold used to calculate the categorical metrics is also employed to determine the occurrence of precipitation event for calculating the precipitation frequency as shown in fig 3d a high spatial variability of average annual precipitation is observed over the south china the regions with high precipitation are mainly in the southeastern parts that are affected by the southeast monsoon from the pacific ocean and the southwest monsoon from the bay of bengal which bring abundant water vapor and thus generate a large amount of precipitation the water vapor carried by the monsoon declines along the move to the northwestern parts leading to the decrease of precipitation accumulation accordingly the three selected spes generally capture well the spatial variability of the cmpa precipitation which however tend to produce larger values in the central parts i e hunan and jiangxi provinces besides the spatial heterogeneities of precipitation from the imerg f and tmpa 3b42 are lower than these of the imerg e and cmpa in terms of precipitation frequency interestingly two imerg products show almost identical distributions of precipitation frequency over the entire south china this means that the imerg f does not necessary outperform the imerg e in detecting precipitation events even though the imerg f employed more additional microwave sensor information and was calibrated by the monthly gpcc data although the three spes generally capture the spatial distribution of precipitation frequency obtained from the cmpa there are still some notable regional differences for instance the three spes tend to underestimate the high precipitation frequency obtained by the cmpa over the central part of sichuan province r1 in fig 4d and the juncture area of guangdong guangxi and hunan provinces r2 in fig 4d the imerg products demonstrate a high precipitation frequency over the juncture area of hubei hunan and jiangxi provinces r3 in fig 4a that is not observed in the tmpa 3b42 and cmpa the tmpa 3b42 generally presents a lower precipitation frequency than both imerg products and cmpa over most areas of the south china besides there are several patches of unusually high precipitation frequency given by the tmpa 3b42 which are not observed in the imerg and cmpa products 4 1 2 statistical evaluation over the south china in order to spatially analyze the performance of the three spes the temporal kg e and its error components as well as the categorical metrics are calculated over the cmpa grids including at least one aws the spatial distributions of these statistical metrics are shown in figs 5 and 6 respectively and the corresponding boxplots are shown in fig 7 as shown in fig 7a the kg e values of the imerg f are generally higher than these of the imerg e due to that the imerg f was calibrated by the monthly gpcc data although the tmpa 3b42 was also calibrated by the monthly gpcc data the kg e scores of the tmpa 3b42 are also lower than these of the imerg f the reasons for this phenomenon may be manifold the most important factors can be attributed to the improvement of imerg algorithm with respect to tmpa algorithm e g usage of morphing technology in imerg products secondly the coarser grid spacing is also a potential factor affecting the performance of tmpa 3b42 although the tmpa 3b42 was re gridded to a finer grid spacing of 0 1 the precipitation estimate is still resolved at the 0 25 scale besides the usage of additional passive microwave samples that also potentially increases the accuracy of the imerg f tang et al 2020 regarding to the spatial variations fig 5a c the kg e values of the three selected spes demonstrate similar spatial patterns and remarkable improvements i e the higher kg e values can be observed for the imerg f and tmpa 3b42 over the henan hubei and hunan provinces green circle area in fig 5a in comparison to the imerg e when decomposing the overall performance of kg e into its three components i e the cc br and vr the three spes also show similar spatial variations of performance metrics see fig 5 the imerg f has the best score of the cc metrics and the near real time imerg e provides the same averaged cc value as the tmpa 3b42 fig 7b with the averaged br values around 1 20 all the three spes show overestimation across most parts of south china fig 7c and 5 it can be also noted that the lower kg e values of the imerg e over the green circle area are mainly caused by the severe overestimation with the br values generally larger than 1 6 in terms of the vr metric fig 7d the box plots of the vr metric for the three spes are centered around 1 i e the perfect values of the vr and show strong symmetry meanwhile the box range for the imerg f and tmpa 3b42 are more compact than that for the imerg e indicating a better capacity in representing the temporal variation of the cmpa precipitation for the capacity of precipitation detection in terms of pod and far metrics as shown in fig 6 the imerg f shows similar spatial distributions of pod and far in compared to the imerg e the high pod values of the two imerg products are in the northeastern part of the south china while the low pod values occur in the juncture area of the guizhou sichuan and chongqing provinces meanwhile it is interesting to find that the areas with high pod values generally show high far values as well beyond that the detection performance of the imerg f is better than imerg e in absolute terms for example imerg f clearly shows lower far values in the northern part of the south china and higher pod in the guizhou the tmpa 3b42 also demonstrates similar spatial distributions of pod and far values as these of the imerg products but with lower values figs 6 and 7 4 1 3 statistical evaluation over the six selected catchments fig 8 shows the statistical metrics of the three spes in the six selected catchments which are calculated from the basin averaged daily precipitation similar as the findings found in the entire south china see section 4 1 2 the imerg f demonstrates the best overall performance in the selected catchments as indicated by the highest kg e values fig 8a it can be observed that the kg e values of the imerg f vary significantly across different catchments and the higher values are noted for the bjrb ufcrb and qurb the imerg e generally provides lower kg e values than the imerg f and the differences between them are relatively smaller in the bjrb and qurb the kg e values of the tmpa 3b42 are also lower than these of the imerg f which are even lower than these of the imerg e in the bjrb and qurb after decomposing the kg e metric into its three components the imerg f presents the highest cc values among the three spes over all six catchments fig 8b regarding to the br metric fig 8c all the three spes tend to overestimate the cmpa precipitation and the overestimation is most noticeable in the uhuairb according to the vr metric fig 8d the most outstanding feature is that the imerg e generally shows larger temporal variations i e larger vr values than the imerg f and tmpa 3b42 this indicates that the implementation of gauge calibration using the monthly gpcc data can reduce the temporal variations of the post real time spes besides the imerg f generally provides the higher pod and lower far values fig 8e and f indicating its better detection capacity in comparison to the imerg e at basin scale the monthly gauge adjustment in the imerg f may contributes most to such improvement meanwhile given that only forward morphing technology is used in the imerg e the use of the forward and backward morphing technology in the imerg f also contributes to the improved detection capacity in the imerg f similar as the abovementioned finding in the south china the imerg products generally present higher pod values than that of the tmpa 3b42 in the six selected catchments which however contain more false precipitation events i e higher far values as well moreover the detection capacity of the three spes also shows remarkable regional difference with higher far values noted in the bjrb and uhuairb and lower pod values in the rsrb to further examine the basin scale performance table 3 presents the statistical metrics of three spes calculated from the basin averaged daily precipitation in the six selected catchments since the spatial aggregation can cancel out some random errors the basin scale performance of the three spes are significantly better than their grid based performance comparing table 3 with fig 8 even so lower kg e values of the imerg e are also noted in the uganrb rsrb and uhuairb less than 0 7 the lower kg e values of the imerg e are mainly caused by the higher overestimation of precipitation i e larger br values in the uhuairb and by its higher temporal heterogeneity i e larger vr values in the uganrb and rsrb benefiting from the gauge calibration using the monthly gpcc data the overall performance of the imerg f is significantly better than that of the imerg e which however also suffers from lower kge values in the uganrb rsrb and uhuairb the performance of tmpa 3b42 generally falls between the two imerg products 4 2 evaluation of hydrological utility the statistical evaluation described in section 4 1 suggests that the imerg f shows the best performance over the six selected catchments while the imerg e has a shorter latency time that makes it more suitable for the early warning system e g flood forecasting and landslide prediction in this section the grid based xinanjiang model is first forced with the cmpa precipitation and calibrated validated over the six selected catchments then the calibrated model is driven by the three spes to investigate their hydrological utilities i e scenario i standard calibration in addition the grid based xinanjiang model is also directly forced with the selected spes and calibrated validated over the six selected catchments i e scenario ii input specific calibration to explore the potential hydrological utility of the corresponding spe in hypothetical ungauged catchments 4 2 1 calibration and validation of the xinanjiang model forced with the cmpa precipitation fig 9 shows the time series of observed and simulated daily hydrographs forced by the cmpa precipitation over the six selected catchments table 4 provides the corresponding statistical metrics i e nse lognse and kg e for both calibration and validation periods in general the simulated hydrographs forced by the cmpa precipitation agree well with the observations in both calibration and validation periods over the selected catchments except the uhuairb with the nse values larger than 0 86 this indicates the excellent performance of the grid based xinanjiang model and the cmpa precipitation over these catchments in the uhuairb the simulations cannot capture the observed peaks which also largely underestimate the observed discharge that generally follow the cmpa precipitation in the first half year of 2010 interestingly the simulations in the wetter catchments with larger annual precipitation see table 1 i e the bjrb uganrb and ufcrb are better than these of other catchments as evidenced by higher nse and k g e values this can be explained by the fact that the xinanjiang model was specifically developed for simulating hydrological processes in humid area zhao 1992 meanwhile it should be noted that the simulations in the uhuairb are largely improved in the validation period with the nse values increased from 0 73 to 0 85 this can be also attributed to the wetter climate in the validation period which is more suitable for the application of the grid based xinanjiang model lower lognse values less than 0 75 are obtained in the ufcrb and qurb indicating that the simulations cannot well capture the base flow processes in these two catchments for the ufcrb the noted deficiency can be attributed to the frequent human activities that disrupt the natural runoff processes xu et al 2013 and the complex topography steep slopes of river and rapid water conflux complicate the hydrological simulation in the qurb song et al 2020 in summary both the calibration and validation results indicate that the grid based xinanjiang model is able to capture the key features of observed hydrographs and is thus suitable for hydrological simulations in the selected catchments therefore it is reasonable to use the calibrated grid based xinanjiang model to evaluate the hydrological utility of the spes over these catchments 4 2 2 hydrological simulation based on the standard calibration scenario i the calibrated grid based xinanjiang model as described in section 4 2 1 is forced by the three spes over the selected six catchments to investigate their hydrological utilities directly time series of observed and simulated daily discharges driven by the two imerg products are shown in fig 10 and similar analyses for the tmpa 3b42 are presented in the appendix fig a1 the statistical metrics computed between the observed and simulated hydrographs forced by the three spes are given in table 5 compared to the cmpa based simulations the spes forced simulations show remarkable degradation of performance in both calibration and validation periods comparing tables 5 with 4 the main reason can be related to the errors noted for these spes see section 4 1 and the model parameters calibrated by the cmpa are not optimal for the spes can be the other important reason as shown in fig 10 the imerg e based simulations show poor performance in the selected catchments and generally overestimate the high flow of the observed hydrographs as indicated by the low nse values the imerg f forced simulations can catch the essential feature of observed hydrographs and perform much better than the imerg e over the selected catchments it can be noted that the imerg f forced simulations provide unreliable results in the uhuairb characterized by low nse lognse and kg e values in both calibration and validation periods in addition it is found that the performance difference between the imerg e and the imerg f in hydrological simulation is more pronounced than that in statistical evaluation as shown in section 4 1 in other words the gauge calibration using the monthly gpcc data performed in the imerg f provides additional benefits to hydrological simulation highlighting the necessity of bias correction before using spes as the input of the hydrological model the tpma p generally presents comparable hydrological utility to that of the imerg f with slightly lower performance metrics over most of the selected catchments 4 2 3 hydrological simulation based on the input specific calibration scenario ii to further assess the hydrological utilities of three selected spes in the ungauged basins the grid based xinanjiang model is recalibrated and validated by employing the individual spe as the precipitation input for the same period as described in the scenario i fig 11 shows the imerg based simulations against the observed hydrographs over the selected catchments and similar comparisons for the tmpa 3b42 are presented in the appendix fig a2 tabl 6 summarizes the associated diagnostic metrics computed between the observed and simulated hydrographs comparison between tables 5 and 6 shows that the performance of the spe based simulations in the scenario ii is much better than these in the scenario i as evidenced by higher nse lognse and kg e values this indicates that the input specific calibration of the model parameters can effectively improve the hydrological utilities of the spes on the other hand it also confirms that the cmpa based model parameters cannot be directly transplanted to the spe forced model in addition the diagnostic metrics of the spe forced simulations based on the cmpa calibrated model parameters vary significantly between the calibration and validation periods table 5 while the performance is more stable using the input specific model parameters table 6 compared to the spe based simulations of scenario i fig 10 the improvement of performance in the scenario ii fig 11 mainly benefits from the improved accuracy in simulating high flow as indicated by large increase of nse values however the simulation performance of the grid based xinanjiang model forced by the three spes is hard to surpass the simulation forced by the cmpa in the selected catchments this reflects that the performance improvement resulting from the model recalculation is still limited by the uncertainty of spe products similar as the findings in the scenario i the imerg f forced simulations perform best among the three spes followed by the tmpa 3b42 the imerg e performs poorest in the daily discharge simulations however the performance improvement of the imerg e in hydrological simulation due to the parameter recalibration e g the averaged nse value of the six selected catchments increased from approximately 0 12 to about 0 57 in the validation period is much higher than these of the imerg f and tmpa 3b42 highlighting the potential application value of the imerg e in the early warning system it should be noted that the imerg e forced simulations using the input specific model parameters tend to underestimate the peak flow in some selected catchments e g uganrb ufcrb and rsrb which may bring uncertainty in flood related simulation 5 discussion the wide spatial coverage and increasing spatiotemporal resolution of spes have promoted their applications this study investigated the performances of imerg and tmpa products in the south china with special focus on their hydrological utilities it s generally expected that the use of gauge calibration e g using the monthly gpcc data can significantly improve the performance of the post real time spe products however only a slight improvement is observed in this study for the imerg f compared to the imerg e in terms of kg e metric and its components fig 7 since the simple monthly gauge calibration fails to correct the rain areas delineation and rain no rain detection le et al 2020 su et al 2018 tang et al 2020 the imerg f demonstrates limited improvement in the categorical scores as well in contrast the gauge calibrated spes i e the imerg f and tmpa perform much better than the near real time spe i e the imerg e in streamflow simulation particularly in scenario i which employs the cmpa based model parameters this finding suggests that the gauge correction can provide more benefits to hydrological applications and highlights the necessity of gauge correction for hydrological application since the error of spe is characterized by evident temporal heterogeneity tang et al 2020 su et al 2017 advised that the gauge calibration performed at finer time scale is much more effective than that at monthly scale results from le et al 2020 also confirmed the necessity of incorporating daily gauge observations in the gauge calibration procedures therefore we recommend that the monthly gauge calibration implemented by the imerg f needs to be further improved on the other hand the dense distribution of gauge observations also plays an important role in the gauge adjustment of spes sun et al 2016 given that the mainstream spes are generally calibrated at the global quasi global scale with limited rain gauge observations i e only 194 rain gauge stations are adopted by the gpcc tang et al 2016 regional or national scale gauge calibration employing more available rain gauge observations is highly recommended to further improve the performance of spes in terms of the hydrological utility the simulation performances highly depend on a complex interaction among the characteristics of input data hydrological model formulation and model parameters as well as the characteristics of specific river basins camici et al 2020 precipitation is a critical input for the hydrologic models and the accuracy of precipitation data is thus essential for reliable hydrologic predictions since the hydrologic models are simplifications of highly complex and non linear hydrological processes the relation between quality of precipitation input and hydrological performance is not always straightforward qi et al 2016 in this study it is hard to judge the accuracy of hydrological performance of spes only from statistical metrics of themselves in fact there are little effective general guidelines about which performance metric s and associated range can be used to identify the best performance of rainfall product for discharge simulation camici et al 2020 qi et al 2016 hence the special attention should be paid to the error propagation from precipitation into the modeled discharge in addition the uncertainties of other model inputs also affect the simulation performance since the cmpa based simulations present excellent performance in the selected catchments as shown in fig 9 the uncertainties from other model inputs except the precipitation are not considered in this study up to now various hydrological models have been used to evaluate hydrological utility of spes in different catchments maggioni and massari 2018 however different hydrological models have diverse foci in describing hydrological processes and each of them has its own characteristics and computational mechanisms chen et al 2013 jiang et al 2017 which imply that tolerance degree to precipitation error can be distinct among different hydrological models currently the hydrological utilities of spes are mainly evaluated in individual catchments by different hydrological models jiang and wang 2019 the combined effects of model uncertainty and basin characteristics e g differences in discharge area initial soil moisture conditions land use and land cover complicate the comparability of different studies and it s thus hard to get general guidelines about which spes should be favored and how reliable are the spes facilitate hydrological modelling in this study to obtain the general guideline a hydrological modeling framework based on the grid based xinanjiang model and a global optimization algorithm i e sec ua was implemented to evaluate the hydrological utilities of spes in six medium sized catchments the results highlight the potential value of the imerg e in the early warning system and confirm the better hydrological performance of the imerg f which thus provide a helpful guidance for the hydrological utilities of spes in the south china in addition the performance of discharge simulations is also strongly sensitive to model parameters qi et al 2016 traditionally the model parameters are calibrated and validated relying on the input of gauge based precipitation observations referred as real precipitation status standard calibration of scenario i in this study however the calibrated model parameters forced by the gauge based precipitation are not optimal for the spes duo to differences noted between the spes and gauge observations in terms of temporal dynamic and spatial distribution fig 8 and table 3 which may degrade the hydrological performance of spes fig 10 and table 5 an input specific model recalibration scenario ii in this study can overcome abovementioned drawbacks and significantly improve the hydrological simulations of spes fig 11 and table 6 zeng et al 2018 also reported that the recalibrated model parameters are able to correct the rainfall time shift allowing to obtain good hydrological performances from the imperfect precipitation estimation given that higher uncertainty was noted in the imerg e the input specific model recalibration significantly increases the application value of the imerg e in the early warning system figs 11 vs 10 it should be noted that model parameters calibrated relying on the biased precipitation inputs e g spes may not be sufficient to characterize the natural basin feature and can even exceed their reasonable ranges su et al 2019a yuan et al 2018 meanwhile the inappropriate model parameters may result in lower predictive capability in internal sub basins jiang and wang 2019 to avoid parameter overflow a physical based searching space is defined for the sce ua optimization algorithm in this study following the suggestion of zhao 1992 the limitation of this study is that the spatially uniform parameters are employed for each catchment which can be unreliable for catchments with complex climate terrain and land cover condition leading to degradation of hydrological utility of spes therefore it s highly recommended to collect detailed hydrological features of basins e g topography land cover and soil texture information and then estimate the physical based model parameters for the distributed hydrological models in follow up studies it should be also noted that the diagnostic metrics of the imerg e forced simulations vary dramatically in the selected catchments table 6 the explanations for this can be attributed to the region dependent quality of the imerg e see fig 8 and table 3 and the complex basin characteristics that may degrade the effectiveness of the hydrological model 6 conclusions the rapid development of satellite based observations and retrieval techniques has prompted the continuous improvement of spes providing unprecedented opportunities for potential hydrological applications quality evaluation of spes is always an indispensable step to further promote their widespread applications this study assessed the performance of latest imerg v06 products including the imerg e and imerg f and tmpa 3b42 products in the south china meanwhile the grid based xinanjiang model was employed to verify the hydrological utilities of three selected spes based on two kinds of parameter calibration scenarios in six selected catchments the main conclusions are summarized as follows 1 compared to the benchmark precipitation i e cmpa the three spes can capture well the spatial distribution of annual precipitation and precipitation frequency in terms of kg e and its components the imerg f generally presents best performance while the imerg e perform poorer in the south china and selected catchments the accuracy of tmpa 3b42 falls between the two imerg products according to pod and far the two imerg products demonstrate similar capability in detecting rain no rain and delineating the rain areas in the south china while the tmpa 3b42 shows poorer performance with lower pod values 2 the cmpa forced simulations agree well with the observed hydrographs in both calibration and validation periods highlighting the excellent performance of the grid based xinanjiang model and the cmpa precipitation over the selected catchments using the cmpa calibrated model parameters i e standard calibration of scenario i the imerg e forced simulations show poor performance in the selected catchments except the qurb much better performance is observed in the imerg f forced simulations nse greater than 0 62 except in the uhuairb 3 the input specific parameters recalibration i e scenario ii significantly improves the simulation performance of spes and such improvement is much more pronounced for the imerg e in this context the imerg e shows great application values in most of the selected catchments the imerg f forced simulations also present the best performance which however are still inferior to the simulations driven by the benchmark precipitation the tmpa 3b42 forced simulations generally illustrate comparable reliability to these of imerg f under both two calibration scenarios with a slight performance degradation this study confirms the outperformance of the imerg f over the tmpa 3b42 in the south china meanwhile the results indicate that the gauge calibrated spes i e the imerg f and tmpa 3b42 present slightly better performance than the near real time imerg e and the improvement is more remarkable when simulating the discharge highlighting the necessity of bias correction for the spes before hydrological applications in addition high application value of the imerg e is noted to facilitate hydrological modelling over most of the selected catchments especially when input specific model parameters are employed however it is still a long way for providing reliable simulations for the early warning system in any given catchment using the imerg e although the imerg f can provide relatively reliable simulations it still hard to replace gauge observations completely these findings provide a useful guidance to both the end users and the retrieval algorithm developers and thus contribute to the potential improvement of the imerg products credit authorship contribution statement jianbin su conceptualization methodology writing original draft data curation investigation writing review editing validation xin li writing review editing validation weiwei ren data curation investigation haishen lü writing review editing validation supervision donghai zheng writing review editing validation supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported in part by the national key research and development program grant no 2018yfa0605400 and the national natural science foundation of china grant nos 41988101 41830752 and 1571015 the imerg and tmpa products were downloaded from nasa goddard earth sciences data and information services center ges disc at http disc sci gsfc nasa gov the cmpa benchmark was obtained from the china meteorological date service center at http data cma cn 
4198,with increasing spatiotemporal resolution and accuracy the satellite based precipitation estimations have made their exploitation in hydrological applications possible especially in poorly gauged regions however their hydrological utilities still need to be evaluated in different catchments to provide useful knowledge to the end users this study investigates the performance of latest version v06 of integrated multi satellite retrievals for global precipitation measurement imerg products and their capacity to guide hydrological modelling in south china both near real time i e early run imerg e and post real time i e final run imerg f imerg products are evaluated and the post real time trmm multi satellite precipitation analysis tmpa 3b42 product is employed for intercomparison the hydrological utilities of abovementioned products are further explored over six different medium sized catchments in south china based on the grid based xinanjiang model the results indicate that the imerg f outperforms the imerg e in south china and the quality of tmpa 3b42 falls between them in terms of k g e and its components for the hydrological utility using gauge based model parameters the imerg e forced simulations show poor performance in most of the selected catchments whereas much better performance is observed for the imerg f forced simulations implementation of input specific calibration significantly improves the simulation performance of three spes and such improvement is much more pronounced for the imerg e the tmpa 3b42 forced simulations generally illustrate comparable reliability to these of imerg f but with a slightly degraded performance overall the imerg e shows high values in facilitating hydrological modelling over most ungauged catchments and the imerg f shows much better hydrological performance however the best performing imerg f forced simulations are still inferior to these driven by the benchmark precipitation and it s thus still hard to replace gauge observations completely with the spes this study provides a helpful knowledge about hydrological performance of satellite based precipitation estimations which potentially promote their widespread applications in hydrological modelling keywords satellite precipitation imerg tmpa hydrological validation south china 1 introduction as a fundamental component of global water cycle precipitation serves as the most crucial source of earth s freshwater kidd and huffman 2011 skofronick jackson et al 2017 accurate information of when where and how much precipitation falls is vital to the modeling of hydrological processes the analyses of climatic changes and the understanding of extreme weather events acharya et al 2019 however due to the strong spatiotemporal variations obtaining detailed precipitation information from the gauge network is always challenging especially over remote areas and developing countries maggioni et al 2016 besides the time latency for accessing to the gauge observations further limits their value in real time applications e g flood forecasting and landslide prediction zhang et al 2021 therefore additional data sources are urgently needed to serve as complements of gauge based observations the rapid development of satellite observations and retrieval techniques has prompted the continuous improvement of satellite based precipitation estimations spes providing new opportunities for obtaining accurate precipitation information since the launch of the tropical rainfall measuring mission trmm satellite in 1997 a series of spes is publicly available among them several well known quasi global spes namely trmm multi satellite precipitation analysis tmpa huffman et al 2007 climate prediction center morphing technique cmorph joyce et al 2004 precipitation estimation from remotely sensed information using artificial neural networks persiann hsu et al 1997 and global satellite mapping of precipitation gsmap kubota et al 2007 have been widely assessed and applied yielding substantial scientific and societal benefits li et al 2017b yang et al 2016 for instance a global flood retrospective simulation system http flood umd edu was constructed by employing the tmpa as precipitation inputs highlighting the great value of spes in flood simulation wu et al 2014 guo et al 2016 showed that the persiann climate data record can well capture both spatial and temporal patterns of drought over eastern china gao et al 2017 demonstrated the benefit of using the tmpa for flood frequency analysis over poorly gauged tributaries of the yangtze river basin although the trmm satellite stopped working on june 15 2015 it had achieved great success in providing mass quasi global precipitation data and promoting the continuous improvement of retrieval algorithms sun et al 2018 as the trmm s successor the global precipitation measurement gpm mission is the state of the art satellite based precipitation measurement program over the world skofronick jackson et al 2018 it is an international constellation of satellites including one core observatory co satellite and approximately ten partner satellites with the cooperation of these satellites the gpm mission is expected to provide accurate and timely precipitation estimation globally the integrated multisatellite retrievals for gpm imerg algorithm was thus developed to produce global precipitation estimation for the gpm mission at relatively fine spatiotemporal resolution 0 1 and 0 5 h huffman et al 2019 given its high spatiotemporal resolution the imerg products can provide detailed precipitation information compared to the tmpa the imerg algorithm uses a morphing scheme and improved retrieval algorithms for input passive microwave pmw sensors infrared satellite images and calibration reference hence the evaluation and application of the imerg products have gained much attention since the first version released on 2015 the chinese mainland has a complex topography and diverse climate conditions which makes it a good testbed for assessing the imerg products thus far several works have been conducted to assess the imerg products using gauge observations for instance su et al 2019a assessed the imerg and tmpa products and highlighted the higher accuracy of the near real time imerg products tang et al 2020 comprehensively analyzed the quality of the imerg products at hourly and daily scales from 2000 to 2018 and demonstrated that the quality of the imerg products have been improved significantly over time due to the increasing number of passive microwave samples until now the imerg products have been updated several times i e from original v03 to latest v06 leading to continuous improvement of data quality wang et al 2018 hence a timely assessment of the latest imerg products i e v06 is very important for both the potential data users and the retrieval algorithm developers given the global availability and increasing accuracy the imerg products have been widely used as input of hydrological models to obtain river discharge estimations jiang and wang 2019 maggioni and massari 2018 zhang et al 2020 over the chinese mainland several studies have attempted to provide guidelines for the use of imerg products in hydrological modelling for instance tang et al 2016 and li et al 2017a forced the coupled routing and excess storage model with the imerg product and showed that the imerg product is barely suitable for simulating the daily discharge in the gan river basin in the southern part of china yuan et al 2018 assessed the hydrological utility of the imerg products in the yellow river source region located in the northeast qinghai tibet plateau using the grid based xinanjiang model the results demonstrated that the daily discharge simulation forced with the imerg product shows comparable performance to that using the gauge based precipitation the hydrological utility of the imerg products was also evaluated in the beijiang river basin wang et al 2017 and the mishui basin jiang et al 2018b in south china results showed that the imerg products were inferior to the gauge based precipitation in the daily hydrological simulation besides su et al 2019b and yuan et al 2018 have suggested that the imerg products should be used with caution in flood simulation it is evident that performance of the imerg forced hydrologic simulations varies significantly in the above mentioned studies such differences are probably related to the heterogeneous quality of imerg products and the different characteristics of the study basins e g discharge area initial soil moisture conditions and so on besides it should be noted that various hydrological models were employed by abovementioned studies since different hydrological models have diverse foci in describing hydrological physical processes the model formulation and computational mechanisms of them have significantly differences chen et al 2013 therefore even forced by the same meteorological inputs the simulation performance from different models in the same basin may be different meanwhile a variety of model parameter calibration algorithms was adopted by these studies since the simulation performance is highly sensitive to the model parameters chen et al 2013 the uncertainty of model parameters further complicates the intercomparison of assessment results in this context it is of great value to establish a hydrological assessment framework which includes a widely used hydrological model and an effective automatic model parameter calibration algorithm to assess the hydrological utility of spes over different catchments with diverse landscapes topography and climatic conditions by this way one can get the general guidelines about hydrological utility of the spe besides previous explorations and studies mainly focused on the assessments of post real time spes few of them pay attention to the near real time products that have highly potential value in promoting the early warning system therefore the objectives of this study are twofold 1 statistically assess the quality of the imerg v06 products particularly for its near real time product based on gauge observed precipitation in south china and 2 comprehensively investigate the hydrological utility of the imerg products in different catchments by a hydrological assessment framework in addition the performance of the post real time tmpa product is also investigated and compared to the imerg products the aim of this work is to provide a timely helpful guidance about the suitability of the imerg products in hydrological modeling for the potential end users and the retrieval algorithm developers the paper is organized as follows the study area and data sets are briefly described in section 2 section 3 presents the methodology used in this research the results and discussion are arranged in sections 4 and 5 respectively and the conclusions are summarized in section 6 2 study area and datasets 2 1 study area this study focuses on the southern part of the chinese mainland which is located between 97 e 122 5 e and 20 n 36 5 n fig 1 it should be noted that the south china defined in this study covers 14 provinces and is much broader than some traditional definitions according to the china statistical yearbook of 2018 http www stats gov cn tjsj ndsj 2018 indexch htm the south china contains 56 14 population and contributes more than half of the gross domestic product of the whole china due to the combined impact of climate and hilly topography the rainstorm related disasters e g flash floods landslides and debris flows happened frequently over this area and caused heavy casualties and severe economic losses ma et al 2018 hence the accurate precipitation measurements especially for the real time precipitation monitoring are particularly crucial for this area to help establishing early warning system for disaster response to explore the hydrological utility of the latest imerg products as shown in fig 1b six medium sized catchments are selected namely beijiang river basin bjrb upper ganjiang river basin uganrb upper fuchun river basin ufcrb rongshui river basin rsrb qu river basin qurb and upper huaihe river basin uhuairb table 1 presents an overview of basic characteristics of these catchments for the period between january 2010 and december 2016 the bjrb and rsrb are controlled by the pearl river system and subjected to the typical subtropical humid monsoon climate wang et al 2017 the uganrb is the headwater catchment of the gangjiang river basin accounting for approximately 19 of the whole gangjiang river basin in area while contributing over 36 of the total discharge zhou et al 2020 the uganrb is one of the typical rainstorm regions in china which suffers from east asian monsoon climate whereby more than 70 of the total annual rainfall concentrates in the period from april to june the strong rainfall and complex terrain often result in frequent rainstorm triggered floods in the uganrb li et al 2017a the ufcrb is the source region of the fuchun river basin which locates in a densely populated and industrial area the ufcrb also suffers from the east asian monsoon climate which makes the ufcrb to be a typical flood vulnerable area the well drained qurb is the largest tributary of the right bank of the jialing river mainstem the qurb is also dominated by the east asian monsoon climate and most of the annual precipitation occurs between june and october resulting in high flood risk for the qurb gao et al 2017 the uhuairb is a typical mountain basin characterized by large streamflow fluctuations due to the large river slopes and rapid water conflux su et al 2019b the uhuairb situates across the transition area between the humid south and arid north climates of china the annual average precipitation is approximately 900 mm and 50 80 of which occurs between may and september the uhuairb faces high flood risk in the flood season which threatens the mengwa flood detention zone downstream 2 2 datasets three types of hydrometeorological datasets are employed in this study the satellite based precipitation products the benchmark precipitation product and other model inputs and validation datasets required in the hydrological modelling limited by availability of the above mentioned datasets this study focuses on the period from january 2010 to december 2016 2 2 1 satellite based precipitation products two types of satellite based precipitation products namely the tmpa and imerg products were selected in this study the gridded tmpa products are publicly available at the spatiotemporal resolution of 0 25 and 3 hour between 50 n 50 s the latest version of the tmpa products is version 7 including both near real time and post real time products the near real time tmpa relies only on a climatological bias adjustment while the post real time tmpa hereafter referred as tmpa 3b42 uses an actual monthly rain gauge analysis for bias adjustment detailed introduction of the tmpa products can be found in huffman et al 2007 based on the gpm mission the imerg algorithm aims to provide precipitation estimations by inter calibrating merging and interpolating all microwave satellite estimates together with microwave calibrated infrared satellite estimates gauge analyses and other potential precipitation estimators first the imerg algorithm merges the precipitation information from the gpm microwave imager and gpm constellation of passive microwave pmw sensors with precipitation retrieved using the 2017 version of the goddard profling algorithm gprof2017 then the imerg algorithm inter calibrates the gridded precipitation information with the gpm combined radar radiometer analysis product and creats half hourly 0 1 0 1 precipitation maps in order to fill the temporal and spatial gaps the quasi lagrangian time interpolation joyce et al 2004 is employed to the combined pmw product using displacement vectors derived from the merra2 and geos fp vertically integrated vapor tqv fields in this procedure the half hourly infrared ir precipitation rate irprecipitation maps are produced from ir based precipitation retrieval method persiann ccs hong et al 2004 later on the morphed estimations are updated with ir based precipitation observations using kalman filter weights if the time distance from the nearest pmw observation is longer than 30 min to create integrated half hourly precipitation maps precipitationuncal finally the multi satellite precipitation estimations produced in the previous step are adjusted by gauge observations to create research quality products precipitationcal the imerg algorithm runs three times for each observation time to accommodate different user requirements the first run i e early run imerg e gives a quick precipitation estimation the successive run i e late run imerg l provides a better precipitation estimation since more data are available both the imerg e and imerg l are adjusted by climatological coefficients which depend on the time and location of the observations while the last run i e final run imerg f benefits from actual monthly gauge observations hence the imerg e and imerg l are available about 4 h and 12 h after the observation time respectively but the imerg f is released approximately two months after the month of available observation besides the imerg e has only forward propagation of the microwave data in time unlike both imerg l and imerg f who use uses morphing in both directions forward and backward propagation for a more detailed description of the imerg products can be referred to huffman et al 2019 in this study the tmpa 3b42 and the latest version version 06b of imerg e and imerg f products are utilized the tmpa 3b42 was re gridded to a finer spatial resolution of 0 1 by the widely used linear interpolation method tang et al 2020 then the 3 hourly tmpa 3b42 and the half hourly imerg products are aggregated to daily following yuan et al 2018 2 2 2 benchmark precipitation products to statistically assess the selected spes the hourly china merge precipitation analysis product hereafter called cmpa at 0 1 resolution was selected as the validation dataset which can be publicly downloaded from the china meteorological data service center http data cma cn this dataset was generated by merging the hourly gauge observations from more than 30 000 automatic weather stations awss over the chinese mainland with the precipitation estimation from the cmorph products using the improved probability density function optimal interpolation method benefited from the high density of the awss e g fig 2 a for the study area and rigorous quality control of the observations the cmpa performs reasonably well in china and can capture varying features of hourly precipitation in heavy weather events tang et al 2017 independent assessment of the cmpa against the precipitation observations from approximately 2400 national weather stations also confirmed its excellent performance shen et al 2014 note that the awss employed in producing the cmpa were not used for the production of both imerg and tmpa products besides considering the possible overlap between the cmpa and the selected spes the cmorph algorithm is also used in the production of the imerg products this study only selects the cmpa grids that contain at least one aws to reduce this impact on the assessment results similarly as shown in fig 2b the periods when the aws observations are missing are also masked out from the assessment 2 2 3 model forcing and validation datasets in this study the grid based xinanjiang model is applied to the six selected catchments four precipitation datasets namely the cmpa tmpa 3b42 and two imerg products i e the imerg e and imerg f are used as model inputs to perform historical daily streamflow simulations respectively the rest of the model inputs are the near surface daily minimum and maximum temperatures obtained from the china meteorological assimilation driving datasets for the soil and water assessment tool model meng and wang 2017 the temperature inputs are also re gridded into the regular 0 1 grids and aggregated to daily temporal resolution to drive the grid based xinanjiang model the daily discharge observations of the six selected catchments are collected from the corresponding hydrometric stations see fig 1b which are available from the china annual hydrological report that published by the ministry of water resources 3 methodology in this study we first assess the imerg products against the cmpa over the entire south china and compare them with the tmpa 3b42 product later on the feasibility of using them as the precipitation input of the grid based xinanjiang model is explored over the six selected catchments 3 1 statistical metrics for assessing the satellite based precipitation estimations to quantify the accuracy of the selected spes the modified kling gupta efficiency k g e along with its three individual components are employed i e correlation coefficient cc bias ratio br and variability ratio vr kling et al 2012 among them the cc is a measure of the linear correlation between the selected spes and the cmpa the br is able to reflect the overestimation br 1 or underestimation br 1 of the selected spes indicating the average tendency of bias and the vr provides a relative measure of dispersion the k g e is an integrated statistic metric that balances the contributions of correlation bias and variability term providing an overall performance of the selected spes this set of metrics has been adopted and recommended by many other researches baez villanueva et al 2018 wang et al 2018 zambrano bigiarini et al 2017 in addition two categorical metrics namely the probability of detection pod and the false alarm ratio far are also employed wilks 2006 the pod gives the fraction of actual precipitation events that are correctly detected by the spes while the far calculates the fraction of precipitation evens that are incorrectly identified by spes in practice the categorical metrics far and pod are very effective in assessing rain area delineation and detecting the happen of raining or not raining according to tian et al 2009 and yong et al 2016 a small value i e 1 mm day is used as the rain no rain threshold to calculate the categorical metrics formulas and perfect values of abovementioned statistical metrics are listed in table 2 3 2 grid based xinanjiang model and hydrological simulation xinanjiang model is a famous lumped conceptual hydrological model which was first developed by zhao 1992 and widely applied to humid and semi humid catchments of china it is an operational hydrological model employed by numerous watershed management institutions in china yuan et al 2018 for improving its applicability in the medium to large sized watershed a grid based xinanjiang model was developed to accommodate the user demand li et al 2007 according to yuan and ren 2004 and yuan et al 2005 the grid based xinanjiang model inherits the three soil layer evapotranspiration and saturation excess runoff schemes of the lumped version for simulating actual evapotranspiration and total runoff meanwhile for modeling the hillslope runoff a gravitational water reservoir with bottom and side outlets is employed to divide the total runoff into its three components i e surface interflow and groundwater runoffs then three linear reservoirs are adopted to perform the slope runoff convergence respectively finally the muskingum routing method is used to obtain the discharge series at the outlet of the given catchment representing the routing effect of the river channel system in this study the grid based xinanjiang model is adopted for the daily discharge simulations over the six selected catchments the model is driven by gridded precipitation and potential evapotranspiration that is calculated by the air temperature based hargreaves method using the daily minimum and maximum near surface temperature hargreaves and samani 1982 the grid based xinanjiang model contains 15 parameters with 8 parameters used to simulate the evapotranspiration and runoff and 7 parameters for hillslope runoff concentration and streamflow routing yuan et al 2018 detailed information on the model parameters and their numerical range is referred to jiang et al 2017 and jiang et al 2018a in this study the model parameters are automatically calibrated using the effective and efficient shuffled complex evolution global optimization algorithm sce ua developed by duan et al 1994 to comprehensively characterize both high and low flow processes the maximum sum of the nash sutcliffe model efficiency coefficient nse and its log transformed value lognse are used as the objective function of the sce ua to search the optimal model parameters yuan et al 2018 the objective function is defined as 1 f m a x n s e l o g n s e n s e 1 i 1 n q s i q o i 2 i 1 n q o i q o 2 l o g n s e 1 i 1 n l o g q s i l o g q o i 2 i 1 n l o g q o i log q o 2 where n represents the sample number q o i and q s i are observed and simulated discharge m 3 s respectively q o and log q o are mean values of corresponding elements besides the abovementioned kg e is also considered for assessing model performance as it can provide a robust measure for a wider range of flow conditions camici et al 2018 in this study the grid based xinanjiang model is operated at the resolution of 0 1 and two model calibration scenarios are designed to better investigate the hydrological utility of the selected spes scenario i the grid based xinanjiang model is forced with the benchmark precipitation i e the cmpa for calibrating the model parameters and the calibrated model is then driven by the selected spes for both calibration and validation periods this scenario is a widely used and known as the standard calibration procedure which can directly investigate the hydrological utility of the selected spes in this study the calibration and validation periods are january 2010 to may 2014 and june 2014 to december 2016 respectively note that the year of 2014 is excluded from the hydrological simulation over the ufcrb since the corresponding discharge observations are not available scenario ii the grid based xinanjiang model is forced with the selected spes for calibrating and validating the model parameters in both calibration and validation periods this scenario is defined as the input specific calibration procedure that the input specific parameters are adopted to conduct streamflow simulations which is an important option for hydrological applications in ungauged basins without gauge precipitation observations 4 results this section first presents the overall evaluation results of the daily aggregated spes using the cmpa over the whole south china and the selected catchments section 4 1 subsequently the hydrological utility of the daily aggregated spes is evaluated using the grid based xinanjiang model over the six selected catchments section 4 2 4 1 statistical evaluation of satellite based precipitation estimations 4 1 1 mean precipitation and precipitation frequency figs 3 and 4 show the average annual precipitation and corresponding precipitation frequency derived from the three spes and cmpa respectively note that the value of 1 mm day same as the threshold used to calculate the categorical metrics is also employed to determine the occurrence of precipitation event for calculating the precipitation frequency as shown in fig 3d a high spatial variability of average annual precipitation is observed over the south china the regions with high precipitation are mainly in the southeastern parts that are affected by the southeast monsoon from the pacific ocean and the southwest monsoon from the bay of bengal which bring abundant water vapor and thus generate a large amount of precipitation the water vapor carried by the monsoon declines along the move to the northwestern parts leading to the decrease of precipitation accumulation accordingly the three selected spes generally capture well the spatial variability of the cmpa precipitation which however tend to produce larger values in the central parts i e hunan and jiangxi provinces besides the spatial heterogeneities of precipitation from the imerg f and tmpa 3b42 are lower than these of the imerg e and cmpa in terms of precipitation frequency interestingly two imerg products show almost identical distributions of precipitation frequency over the entire south china this means that the imerg f does not necessary outperform the imerg e in detecting precipitation events even though the imerg f employed more additional microwave sensor information and was calibrated by the monthly gpcc data although the three spes generally capture the spatial distribution of precipitation frequency obtained from the cmpa there are still some notable regional differences for instance the three spes tend to underestimate the high precipitation frequency obtained by the cmpa over the central part of sichuan province r1 in fig 4d and the juncture area of guangdong guangxi and hunan provinces r2 in fig 4d the imerg products demonstrate a high precipitation frequency over the juncture area of hubei hunan and jiangxi provinces r3 in fig 4a that is not observed in the tmpa 3b42 and cmpa the tmpa 3b42 generally presents a lower precipitation frequency than both imerg products and cmpa over most areas of the south china besides there are several patches of unusually high precipitation frequency given by the tmpa 3b42 which are not observed in the imerg and cmpa products 4 1 2 statistical evaluation over the south china in order to spatially analyze the performance of the three spes the temporal kg e and its error components as well as the categorical metrics are calculated over the cmpa grids including at least one aws the spatial distributions of these statistical metrics are shown in figs 5 and 6 respectively and the corresponding boxplots are shown in fig 7 as shown in fig 7a the kg e values of the imerg f are generally higher than these of the imerg e due to that the imerg f was calibrated by the monthly gpcc data although the tmpa 3b42 was also calibrated by the monthly gpcc data the kg e scores of the tmpa 3b42 are also lower than these of the imerg f the reasons for this phenomenon may be manifold the most important factors can be attributed to the improvement of imerg algorithm with respect to tmpa algorithm e g usage of morphing technology in imerg products secondly the coarser grid spacing is also a potential factor affecting the performance of tmpa 3b42 although the tmpa 3b42 was re gridded to a finer grid spacing of 0 1 the precipitation estimate is still resolved at the 0 25 scale besides the usage of additional passive microwave samples that also potentially increases the accuracy of the imerg f tang et al 2020 regarding to the spatial variations fig 5a c the kg e values of the three selected spes demonstrate similar spatial patterns and remarkable improvements i e the higher kg e values can be observed for the imerg f and tmpa 3b42 over the henan hubei and hunan provinces green circle area in fig 5a in comparison to the imerg e when decomposing the overall performance of kg e into its three components i e the cc br and vr the three spes also show similar spatial variations of performance metrics see fig 5 the imerg f has the best score of the cc metrics and the near real time imerg e provides the same averaged cc value as the tmpa 3b42 fig 7b with the averaged br values around 1 20 all the three spes show overestimation across most parts of south china fig 7c and 5 it can be also noted that the lower kg e values of the imerg e over the green circle area are mainly caused by the severe overestimation with the br values generally larger than 1 6 in terms of the vr metric fig 7d the box plots of the vr metric for the three spes are centered around 1 i e the perfect values of the vr and show strong symmetry meanwhile the box range for the imerg f and tmpa 3b42 are more compact than that for the imerg e indicating a better capacity in representing the temporal variation of the cmpa precipitation for the capacity of precipitation detection in terms of pod and far metrics as shown in fig 6 the imerg f shows similar spatial distributions of pod and far in compared to the imerg e the high pod values of the two imerg products are in the northeastern part of the south china while the low pod values occur in the juncture area of the guizhou sichuan and chongqing provinces meanwhile it is interesting to find that the areas with high pod values generally show high far values as well beyond that the detection performance of the imerg f is better than imerg e in absolute terms for example imerg f clearly shows lower far values in the northern part of the south china and higher pod in the guizhou the tmpa 3b42 also demonstrates similar spatial distributions of pod and far values as these of the imerg products but with lower values figs 6 and 7 4 1 3 statistical evaluation over the six selected catchments fig 8 shows the statistical metrics of the three spes in the six selected catchments which are calculated from the basin averaged daily precipitation similar as the findings found in the entire south china see section 4 1 2 the imerg f demonstrates the best overall performance in the selected catchments as indicated by the highest kg e values fig 8a it can be observed that the kg e values of the imerg f vary significantly across different catchments and the higher values are noted for the bjrb ufcrb and qurb the imerg e generally provides lower kg e values than the imerg f and the differences between them are relatively smaller in the bjrb and qurb the kg e values of the tmpa 3b42 are also lower than these of the imerg f which are even lower than these of the imerg e in the bjrb and qurb after decomposing the kg e metric into its three components the imerg f presents the highest cc values among the three spes over all six catchments fig 8b regarding to the br metric fig 8c all the three spes tend to overestimate the cmpa precipitation and the overestimation is most noticeable in the uhuairb according to the vr metric fig 8d the most outstanding feature is that the imerg e generally shows larger temporal variations i e larger vr values than the imerg f and tmpa 3b42 this indicates that the implementation of gauge calibration using the monthly gpcc data can reduce the temporal variations of the post real time spes besides the imerg f generally provides the higher pod and lower far values fig 8e and f indicating its better detection capacity in comparison to the imerg e at basin scale the monthly gauge adjustment in the imerg f may contributes most to such improvement meanwhile given that only forward morphing technology is used in the imerg e the use of the forward and backward morphing technology in the imerg f also contributes to the improved detection capacity in the imerg f similar as the abovementioned finding in the south china the imerg products generally present higher pod values than that of the tmpa 3b42 in the six selected catchments which however contain more false precipitation events i e higher far values as well moreover the detection capacity of the three spes also shows remarkable regional difference with higher far values noted in the bjrb and uhuairb and lower pod values in the rsrb to further examine the basin scale performance table 3 presents the statistical metrics of three spes calculated from the basin averaged daily precipitation in the six selected catchments since the spatial aggregation can cancel out some random errors the basin scale performance of the three spes are significantly better than their grid based performance comparing table 3 with fig 8 even so lower kg e values of the imerg e are also noted in the uganrb rsrb and uhuairb less than 0 7 the lower kg e values of the imerg e are mainly caused by the higher overestimation of precipitation i e larger br values in the uhuairb and by its higher temporal heterogeneity i e larger vr values in the uganrb and rsrb benefiting from the gauge calibration using the monthly gpcc data the overall performance of the imerg f is significantly better than that of the imerg e which however also suffers from lower kge values in the uganrb rsrb and uhuairb the performance of tmpa 3b42 generally falls between the two imerg products 4 2 evaluation of hydrological utility the statistical evaluation described in section 4 1 suggests that the imerg f shows the best performance over the six selected catchments while the imerg e has a shorter latency time that makes it more suitable for the early warning system e g flood forecasting and landslide prediction in this section the grid based xinanjiang model is first forced with the cmpa precipitation and calibrated validated over the six selected catchments then the calibrated model is driven by the three spes to investigate their hydrological utilities i e scenario i standard calibration in addition the grid based xinanjiang model is also directly forced with the selected spes and calibrated validated over the six selected catchments i e scenario ii input specific calibration to explore the potential hydrological utility of the corresponding spe in hypothetical ungauged catchments 4 2 1 calibration and validation of the xinanjiang model forced with the cmpa precipitation fig 9 shows the time series of observed and simulated daily hydrographs forced by the cmpa precipitation over the six selected catchments table 4 provides the corresponding statistical metrics i e nse lognse and kg e for both calibration and validation periods in general the simulated hydrographs forced by the cmpa precipitation agree well with the observations in both calibration and validation periods over the selected catchments except the uhuairb with the nse values larger than 0 86 this indicates the excellent performance of the grid based xinanjiang model and the cmpa precipitation over these catchments in the uhuairb the simulations cannot capture the observed peaks which also largely underestimate the observed discharge that generally follow the cmpa precipitation in the first half year of 2010 interestingly the simulations in the wetter catchments with larger annual precipitation see table 1 i e the bjrb uganrb and ufcrb are better than these of other catchments as evidenced by higher nse and k g e values this can be explained by the fact that the xinanjiang model was specifically developed for simulating hydrological processes in humid area zhao 1992 meanwhile it should be noted that the simulations in the uhuairb are largely improved in the validation period with the nse values increased from 0 73 to 0 85 this can be also attributed to the wetter climate in the validation period which is more suitable for the application of the grid based xinanjiang model lower lognse values less than 0 75 are obtained in the ufcrb and qurb indicating that the simulations cannot well capture the base flow processes in these two catchments for the ufcrb the noted deficiency can be attributed to the frequent human activities that disrupt the natural runoff processes xu et al 2013 and the complex topography steep slopes of river and rapid water conflux complicate the hydrological simulation in the qurb song et al 2020 in summary both the calibration and validation results indicate that the grid based xinanjiang model is able to capture the key features of observed hydrographs and is thus suitable for hydrological simulations in the selected catchments therefore it is reasonable to use the calibrated grid based xinanjiang model to evaluate the hydrological utility of the spes over these catchments 4 2 2 hydrological simulation based on the standard calibration scenario i the calibrated grid based xinanjiang model as described in section 4 2 1 is forced by the three spes over the selected six catchments to investigate their hydrological utilities directly time series of observed and simulated daily discharges driven by the two imerg products are shown in fig 10 and similar analyses for the tmpa 3b42 are presented in the appendix fig a1 the statistical metrics computed between the observed and simulated hydrographs forced by the three spes are given in table 5 compared to the cmpa based simulations the spes forced simulations show remarkable degradation of performance in both calibration and validation periods comparing tables 5 with 4 the main reason can be related to the errors noted for these spes see section 4 1 and the model parameters calibrated by the cmpa are not optimal for the spes can be the other important reason as shown in fig 10 the imerg e based simulations show poor performance in the selected catchments and generally overestimate the high flow of the observed hydrographs as indicated by the low nse values the imerg f forced simulations can catch the essential feature of observed hydrographs and perform much better than the imerg e over the selected catchments it can be noted that the imerg f forced simulations provide unreliable results in the uhuairb characterized by low nse lognse and kg e values in both calibration and validation periods in addition it is found that the performance difference between the imerg e and the imerg f in hydrological simulation is more pronounced than that in statistical evaluation as shown in section 4 1 in other words the gauge calibration using the monthly gpcc data performed in the imerg f provides additional benefits to hydrological simulation highlighting the necessity of bias correction before using spes as the input of the hydrological model the tpma p generally presents comparable hydrological utility to that of the imerg f with slightly lower performance metrics over most of the selected catchments 4 2 3 hydrological simulation based on the input specific calibration scenario ii to further assess the hydrological utilities of three selected spes in the ungauged basins the grid based xinanjiang model is recalibrated and validated by employing the individual spe as the precipitation input for the same period as described in the scenario i fig 11 shows the imerg based simulations against the observed hydrographs over the selected catchments and similar comparisons for the tmpa 3b42 are presented in the appendix fig a2 tabl 6 summarizes the associated diagnostic metrics computed between the observed and simulated hydrographs comparison between tables 5 and 6 shows that the performance of the spe based simulations in the scenario ii is much better than these in the scenario i as evidenced by higher nse lognse and kg e values this indicates that the input specific calibration of the model parameters can effectively improve the hydrological utilities of the spes on the other hand it also confirms that the cmpa based model parameters cannot be directly transplanted to the spe forced model in addition the diagnostic metrics of the spe forced simulations based on the cmpa calibrated model parameters vary significantly between the calibration and validation periods table 5 while the performance is more stable using the input specific model parameters table 6 compared to the spe based simulations of scenario i fig 10 the improvement of performance in the scenario ii fig 11 mainly benefits from the improved accuracy in simulating high flow as indicated by large increase of nse values however the simulation performance of the grid based xinanjiang model forced by the three spes is hard to surpass the simulation forced by the cmpa in the selected catchments this reflects that the performance improvement resulting from the model recalculation is still limited by the uncertainty of spe products similar as the findings in the scenario i the imerg f forced simulations perform best among the three spes followed by the tmpa 3b42 the imerg e performs poorest in the daily discharge simulations however the performance improvement of the imerg e in hydrological simulation due to the parameter recalibration e g the averaged nse value of the six selected catchments increased from approximately 0 12 to about 0 57 in the validation period is much higher than these of the imerg f and tmpa 3b42 highlighting the potential application value of the imerg e in the early warning system it should be noted that the imerg e forced simulations using the input specific model parameters tend to underestimate the peak flow in some selected catchments e g uganrb ufcrb and rsrb which may bring uncertainty in flood related simulation 5 discussion the wide spatial coverage and increasing spatiotemporal resolution of spes have promoted their applications this study investigated the performances of imerg and tmpa products in the south china with special focus on their hydrological utilities it s generally expected that the use of gauge calibration e g using the monthly gpcc data can significantly improve the performance of the post real time spe products however only a slight improvement is observed in this study for the imerg f compared to the imerg e in terms of kg e metric and its components fig 7 since the simple monthly gauge calibration fails to correct the rain areas delineation and rain no rain detection le et al 2020 su et al 2018 tang et al 2020 the imerg f demonstrates limited improvement in the categorical scores as well in contrast the gauge calibrated spes i e the imerg f and tmpa perform much better than the near real time spe i e the imerg e in streamflow simulation particularly in scenario i which employs the cmpa based model parameters this finding suggests that the gauge correction can provide more benefits to hydrological applications and highlights the necessity of gauge correction for hydrological application since the error of spe is characterized by evident temporal heterogeneity tang et al 2020 su et al 2017 advised that the gauge calibration performed at finer time scale is much more effective than that at monthly scale results from le et al 2020 also confirmed the necessity of incorporating daily gauge observations in the gauge calibration procedures therefore we recommend that the monthly gauge calibration implemented by the imerg f needs to be further improved on the other hand the dense distribution of gauge observations also plays an important role in the gauge adjustment of spes sun et al 2016 given that the mainstream spes are generally calibrated at the global quasi global scale with limited rain gauge observations i e only 194 rain gauge stations are adopted by the gpcc tang et al 2016 regional or national scale gauge calibration employing more available rain gauge observations is highly recommended to further improve the performance of spes in terms of the hydrological utility the simulation performances highly depend on a complex interaction among the characteristics of input data hydrological model formulation and model parameters as well as the characteristics of specific river basins camici et al 2020 precipitation is a critical input for the hydrologic models and the accuracy of precipitation data is thus essential for reliable hydrologic predictions since the hydrologic models are simplifications of highly complex and non linear hydrological processes the relation between quality of precipitation input and hydrological performance is not always straightforward qi et al 2016 in this study it is hard to judge the accuracy of hydrological performance of spes only from statistical metrics of themselves in fact there are little effective general guidelines about which performance metric s and associated range can be used to identify the best performance of rainfall product for discharge simulation camici et al 2020 qi et al 2016 hence the special attention should be paid to the error propagation from precipitation into the modeled discharge in addition the uncertainties of other model inputs also affect the simulation performance since the cmpa based simulations present excellent performance in the selected catchments as shown in fig 9 the uncertainties from other model inputs except the precipitation are not considered in this study up to now various hydrological models have been used to evaluate hydrological utility of spes in different catchments maggioni and massari 2018 however different hydrological models have diverse foci in describing hydrological processes and each of them has its own characteristics and computational mechanisms chen et al 2013 jiang et al 2017 which imply that tolerance degree to precipitation error can be distinct among different hydrological models currently the hydrological utilities of spes are mainly evaluated in individual catchments by different hydrological models jiang and wang 2019 the combined effects of model uncertainty and basin characteristics e g differences in discharge area initial soil moisture conditions land use and land cover complicate the comparability of different studies and it s thus hard to get general guidelines about which spes should be favored and how reliable are the spes facilitate hydrological modelling in this study to obtain the general guideline a hydrological modeling framework based on the grid based xinanjiang model and a global optimization algorithm i e sec ua was implemented to evaluate the hydrological utilities of spes in six medium sized catchments the results highlight the potential value of the imerg e in the early warning system and confirm the better hydrological performance of the imerg f which thus provide a helpful guidance for the hydrological utilities of spes in the south china in addition the performance of discharge simulations is also strongly sensitive to model parameters qi et al 2016 traditionally the model parameters are calibrated and validated relying on the input of gauge based precipitation observations referred as real precipitation status standard calibration of scenario i in this study however the calibrated model parameters forced by the gauge based precipitation are not optimal for the spes duo to differences noted between the spes and gauge observations in terms of temporal dynamic and spatial distribution fig 8 and table 3 which may degrade the hydrological performance of spes fig 10 and table 5 an input specific model recalibration scenario ii in this study can overcome abovementioned drawbacks and significantly improve the hydrological simulations of spes fig 11 and table 6 zeng et al 2018 also reported that the recalibrated model parameters are able to correct the rainfall time shift allowing to obtain good hydrological performances from the imperfect precipitation estimation given that higher uncertainty was noted in the imerg e the input specific model recalibration significantly increases the application value of the imerg e in the early warning system figs 11 vs 10 it should be noted that model parameters calibrated relying on the biased precipitation inputs e g spes may not be sufficient to characterize the natural basin feature and can even exceed their reasonable ranges su et al 2019a yuan et al 2018 meanwhile the inappropriate model parameters may result in lower predictive capability in internal sub basins jiang and wang 2019 to avoid parameter overflow a physical based searching space is defined for the sce ua optimization algorithm in this study following the suggestion of zhao 1992 the limitation of this study is that the spatially uniform parameters are employed for each catchment which can be unreliable for catchments with complex climate terrain and land cover condition leading to degradation of hydrological utility of spes therefore it s highly recommended to collect detailed hydrological features of basins e g topography land cover and soil texture information and then estimate the physical based model parameters for the distributed hydrological models in follow up studies it should be also noted that the diagnostic metrics of the imerg e forced simulations vary dramatically in the selected catchments table 6 the explanations for this can be attributed to the region dependent quality of the imerg e see fig 8 and table 3 and the complex basin characteristics that may degrade the effectiveness of the hydrological model 6 conclusions the rapid development of satellite based observations and retrieval techniques has prompted the continuous improvement of spes providing unprecedented opportunities for potential hydrological applications quality evaluation of spes is always an indispensable step to further promote their widespread applications this study assessed the performance of latest imerg v06 products including the imerg e and imerg f and tmpa 3b42 products in the south china meanwhile the grid based xinanjiang model was employed to verify the hydrological utilities of three selected spes based on two kinds of parameter calibration scenarios in six selected catchments the main conclusions are summarized as follows 1 compared to the benchmark precipitation i e cmpa the three spes can capture well the spatial distribution of annual precipitation and precipitation frequency in terms of kg e and its components the imerg f generally presents best performance while the imerg e perform poorer in the south china and selected catchments the accuracy of tmpa 3b42 falls between the two imerg products according to pod and far the two imerg products demonstrate similar capability in detecting rain no rain and delineating the rain areas in the south china while the tmpa 3b42 shows poorer performance with lower pod values 2 the cmpa forced simulations agree well with the observed hydrographs in both calibration and validation periods highlighting the excellent performance of the grid based xinanjiang model and the cmpa precipitation over the selected catchments using the cmpa calibrated model parameters i e standard calibration of scenario i the imerg e forced simulations show poor performance in the selected catchments except the qurb much better performance is observed in the imerg f forced simulations nse greater than 0 62 except in the uhuairb 3 the input specific parameters recalibration i e scenario ii significantly improves the simulation performance of spes and such improvement is much more pronounced for the imerg e in this context the imerg e shows great application values in most of the selected catchments the imerg f forced simulations also present the best performance which however are still inferior to the simulations driven by the benchmark precipitation the tmpa 3b42 forced simulations generally illustrate comparable reliability to these of imerg f under both two calibration scenarios with a slight performance degradation this study confirms the outperformance of the imerg f over the tmpa 3b42 in the south china meanwhile the results indicate that the gauge calibrated spes i e the imerg f and tmpa 3b42 present slightly better performance than the near real time imerg e and the improvement is more remarkable when simulating the discharge highlighting the necessity of bias correction for the spes before hydrological applications in addition high application value of the imerg e is noted to facilitate hydrological modelling over most of the selected catchments especially when input specific model parameters are employed however it is still a long way for providing reliable simulations for the early warning system in any given catchment using the imerg e although the imerg f can provide relatively reliable simulations it still hard to replace gauge observations completely these findings provide a useful guidance to both the end users and the retrieval algorithm developers and thus contribute to the potential improvement of the imerg products credit authorship contribution statement jianbin su conceptualization methodology writing original draft data curation investigation writing review editing validation xin li writing review editing validation weiwei ren data curation investigation haishen lü writing review editing validation supervision donghai zheng writing review editing validation supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported in part by the national key research and development program grant no 2018yfa0605400 and the national natural science foundation of china grant nos 41988101 41830752 and 1571015 the imerg and tmpa products were downloaded from nasa goddard earth sciences data and information services center ges disc at http disc sci gsfc nasa gov the cmpa benchmark was obtained from the china meteorological date service center at http data cma cn 
4199,the nonexistence of universal drought indicators may lead to uncertainties in the variability of drought leading to discrepancies in the frequency change in compound dry and hot events cdhes in different regions across the globe understanding the impact of the selection of drought indicators on the variability of cdhes is important for assessing their risks and pursuing mitigation measures however quantitative assessments of differences in cdhes variabilities based on different drought indicators have been lacking in this study we evaluate changes in cdhes based on three different drought indices including self calibrating palmer drought severity index scpdsi standardized precipitation index spi and standardized precipitation evapotranspiration index spei and standardized temperature index sti as the hot indicator over global maize producing areas generally the compound dry and hot event defined by spei sti shows a relatively higher magnitude of increase in the frequency and spatial extent compared with that defined by spi sti and scpdsi sti from 1949 1980 to 1981 2012 over most regions for the top ten maize producing countries the frequency of different countries affected by cdhes at the same time also increases from 1949 1980 to 1981 2012 for these three types of compound events consistent patterns are shown based on different thresholds and different base periods in defining compound events results from this study could help understand compound event variations and provide useful insights for agricultural management under global warming keywords drought index drought compound event dry and hot 1 introduction droughts and hot extremes may cause disastrous impacts on food security energy consumption and ecosystems chen et al 2020 hao et al 2018 he et al 2019 mishra and singh 2010 zhou et al 2020 the concurrence or occurrence in close succession of these two events i e compound events or extremes may lead to larger impacts than that caused by each in isolation chen et al 2018 kong et al 2020 leng et al 2016 matiu et al 2017 zscheischler et al 2018 focus on compound dry and hot events cdhes has increased due to recent occurrences of these events causing tremendous damages to crop production ecosystem function and public health such as those during summers of 2010 2018 in europe and 2006 2013 in china feng et al 2019 iizumi and ramankutty 2016 mishra et al 2020 ribeiro et al 2020 vogel et al 2019 zscheischler et al 2020 thus it is of critical importance to further our understanding of the variability of cdhes for mitigation efforts on the global scale an overall increase in temperature and associated extremes has been shown for a majority of global land areas coumou and robinson 2013 ipcc 2013 seneviratne et al 2014 however the discrepancy in the variation of droughts still exists at the regional and global scales dai 2013 hoffmann et al 2020 sheffield et al 2012 spinoni et al 2014 which is partly due to the differences in drought indicators and their capability in describing relative impacts of moisture supply and evaporative demand of the atmosphere berg and sheffield 2018 haile et al 2020 liu et al 2016a mukherjee et al 2018 vicente serrano et al 2020 for example sheffield et al 2012 highlighted that drought defined by conventional palmer drought severity index pdsi with pet estimated using the penman monteith equation showed little changes from 1948 to 2008 meanwhile an increase in drought defined by the self calibrating pdsi scpdsi from 1950 to 2010 over global land areas has been demonstrated dai 2013 the discrepancy has been explained from different evapotranspiration calculation methods based on thornthwaite or penman monteith method base periods 1950 2008 and 1950 1979 and precipitation datasets trenberth et al 2014 these results suggest that the pattern and magnitude of drought changes may vary if multiple drought indicators are used different indicators of droughts and hot extremes have been used to define cdhes and recent work suggests a consistent increase in frequency severity duration and spatial extent at regional and global scales hao et al 2013 mazdiyasni and aghakouchak 2015 sarhadi et al 2018 sharma and mujumdar 2017 wu et al 2019 for example wu et al 2019 found a consistent increase in the spatial extent of cdhes in china based on the standardized precipitation index spi and the number of hot days nhd chen et al 2019 assessed changes in the likelihood of cdhes over china based on pdsi as the drought indicator and found a substantial increase of cdhes in northern parts of china based on precipitation and temperature to define compound dry and hot conditions zscheischler and seneviratne 2017 showed an increased likelihood of cdhes in the future at the global scale these studies of cdhes are based on different drought indicators which differ in capturing different processes of moisture supply and demand for example spi is solely based on precipitation the spei includes both precipitation and temperature or potential evapotranspiration pet vicente serrano et al 2010 while scpdsi further incorporates wind speed and solar radiation in addition to precipitation and temperature if the penman monteith method is used for calculating pet wells et al 2004 it is expected that these factors may influence the variability of cdhes leading to different magnitudes of changes although consistent patterns of increased frequency of cdhes have been shown in historical periods the impact of drought indicators and their capability to capture different water and energy processes on the variability of cdhes which is important for accurate risk estimation has been lacking as such there is a pressing need to understand how the choice of drought indices may influence the detection of changes in cdhes at the global and regional scale agriculture is among the most vulnerable sectors to the concurrent drought and heatwaves or cdhes leng 2019 ribeiro et al 2020 zscheischler et al 2020 zscheischler et al 2018 previous studies have shown an increased frequency of cdhes over different cropland areas in historical periods lu et al 2018 sarhadi et al 2018 wu et al 2021 lu et al 2018 found an upward trend in the frequency of cdhes weighted by crop areas during wheat and maize growing seasons from 1980 to 2015 based on observed precipitation and temperature in china studies also show increased frequency of cdhes in future periods based on climate projections over cropland areas sarhadi et al 2018 wu et al 2021 based on the model simulations from phase 5 of the coupled model intercomparison project cmip5 sarhadi et al 2018 assessed changes in the joint probability of severe warm and dry conditions using the copula approach and found an increase in the probability from 2020 to 2050 under rcp8 5 over global cropland areas wu et al 2021 revealed that the cropland area affected by cdhes would increase to about 1 7 1 8 times by 2090 2099 compared with that during 1990 1999 based on seasonal precipitation and temperature from cmip5 to develop mitigation measures for the agricultural sectors it is critical to understand how the choice of drought indicators affects the variability of cdhes over global crop areas the objective of this study is to compare the variability of cdhes based on multiple drought indicators i e scpdsi spi and spei over global crop areas in this study we explore changing patterns of cdhes under global warming with a focus on the maize growing area monfreda et al 2008 for maize is shown to be among the most vulnerable and sensitive crops under climate change zhao et al 2017 the data and methods are introduced in section 2 results of variations based on three types of cdhes at the global and national scales are shown in section 3 followed by discussion and conclusion in section 4 and 5 2 data and methods 2 1 indicators the pdsi is developed based on the concept of water balance that incorporates the antecedent precipitation soil moisture and pet palmer 1965 its main drawback is that certain parameters are obtained in u s regions which hinders comparisons between diverse climatic areas the self calibrating pdsi scpdsi addresses this drawback by estimating parameters locally and is spatially comparable wells et al 2004 in this study the monthly scpdsi series from 1949 to 2012 were directly collected from the cru ts 4 03 dataset http www cru uea ac uk data harris et al 2014 for this scpdsi dataset pet is estimated based on the penman monteith approach the spi is computed based on the accumulation of monthly precipitation of certain time scales e g 1 month in this study mckee et al 1993 it has been recommended as the drought indicator to track the meteorological drought condition hayes et al 2011 for the accumulated precipitation a parametric or nonparametric distribution f is fitted to obtain the marginal probability p which is then transformed to the normal random variable based on the standard normal distribution φ i e spi φ 1 p in this study the empirical or nonparametric weibull plotting position formula is used to estimate the marginal probability p wu et al 2019 the motivation of using the empirical method is to avoid the parametric assumption of distributions of precipitation data the spi is calculated based on global monthly precipitation at the 0 5 0 5 resolution for 1949 2012 from cru ts 4 03 dataset to match the source of scpdsi data the spei is a multi scalar drought index that combines the effects of both precipitation and temperature vicente serrano et al 2010 it can be computed similarly to spi based on the difference between precipitation and pet which provides a measure of the water surplus or deficit in this study the pet based on the penman monteith method is taken from the cru4 03 dataset the marginal probability of the difference between precipitation and pet is also estimated based on the empirical weibull plotting position formula ahmadalipour et al 2017 the 1 month spei is calculated based on global monthly climate precipitation and pet data for the same period from the cru 4 03 dataset the standardized temperature index sti is used as the indicator of hot extremes hao et al 2019 the computation of sti follows the same procedure of computing spi based on the global temperature data from the cru 4 03 dataset 2 2 identification of compound events a compound dry and hot event is identified based on the concurrence of drought indicator values scpdsi spi and spei below the 20th percentile and hot indicator sti values above the 80th percentile of each month of the base period from 1949 to 2012 which can be expressed as a binary variable o 1 for occurrences and 0 for non occurrences based on the three drought indices and sti three types of cdhes are extracted for each month denoted as scpdsi sti spi sti and spei sti the frequency of cdhes of each period at each grid is then computed based on the number of months in which cdhes occur divided by the total number of months the annual spatial extent of compound events is defined as the number of grids within the maize growing area covered by cdhes averaged occurrences of 12 months for each year divided by the total number of grid points over the maize growing area based on the above definition and calculation we assess the global and national change of cdhes the detailed workflow of this study is shown in fig 1 3 results 3 1 changes at the global scale previous studies have shown that the global mean temperature is relatively stable for the period from the 1950s to the 1980s coumou and robinson 2013 hansen et al 2012 thus the whole study period 1949 2012 is divided into two equal periods i e 1949 1980 and 1981 2012 the changes in the frequency of cdhes for the two periods 1949 1980 and 1981 2012 over global maize areas and land areas are shown in fig 2 and fig s1 a consistent increase in the frequency of cdhes over the majority of global maize producing areas and global land areas is observed the enhanced cdhes could be related to the upward trend of the temperature for the 1949 2012 period over maize producing areas as shown in fig s2 a the increase of cdhes is also observed in the spatial extent as shown in fig 3 a based on the nonparametric mann kendall test of annual spatial extent over the past 64 year period ayantobo et al 2018 liu et al 2016b a significant increasing trend is observed with the slope of 1 01 decade 1 01 decade and 1 42 decade respectively based on the three types of compound events i e scpdsi sti spi sti and spei sti by comparing with that during the first period 1949 1980 the frequency of cdhes defined by scpdsi and sti increased in regions including western europe eastern south america africa southeast asia and eastern asia about 77 18 of maize producing areas during 1981 2012 as shown in fig 2 a results based on spi and spei show similar patterns of increased frequency over global maize producing areas around 85 39 and 89 72 respectively as shown in fig 2 b c the decrease of compound events is shown in limited areas such as parts of the usa and argentina which is consistent with previous studies feng et al 2020 this may be related to increased precipitation and pet variations over most of the above areas dai 2013 yang et al 2017 as partly shown in fig s2 b c though the consistent increase in the frequency and spatial extent is clearly shown from three different types of compound events these results highlight the discrepancy in the magnitude which results from the difference of drought indices for example the percentage of grids with increased cdhes is relatively high for the compound event defined based on spei along with the largest slope of increased spatial extent the difference between scpdsi and spei may result from the role of the atmospheric evaporation demand aed which is an essential factor for drought quantification the scpdsi is based on the soil moisture balance in which the evapotranspiration et would constrain aed in water limited regions where et is constrained by water availability the increased aed with temperature increase does not further affect pdsi or scpdsi when the soil water content is close to zero vicente serrano et al 2011 vicente serrano et al 2020 while for spei enhanced aed is expected to reduce spei values without et dependence on soil moisture limitation vicente serrano et al 2020 this is consistent with previous studies showing that spei has greater sensitivity to aed than pdsi especially in arid areas cook et al 2014 thus with the increased temperature over large regions of global maize producing areas the spei would capture more drought than the scpdsi which is expected to lead to a larger increase in cdhes in addition the difference between spi and spei is that spi is only based on precipitation without consideration of aed while spei incorporates aed in drought characterization with possibly enhanced severity of dry events this may explain the higher magnitude of increased spatial extent of cdhes based on spei than that based on spi 3 2 changes at the national scale cdhes may lead to large impacts on crop yield of different countries and pose threat to food security iizumi and ramankutty 2016 matiu et al 2017 ray et al 2015 here we select the top ten maize producing countries for further analysis including the usa china brazil argentina ukraine mexico india france canada and south africa tigchelaar et al 2018 the gridded drought indicator scpdsi spi and spei and sti within each country are spatially averaged over grids within maize areas of each country to compute changes of cdhes the change in the frequency of compound events from the period 1949 1980 to 1981 2012 for each country is shown in fig 4 increased frequency of compound dry hot events for the top ten maize producing countries is captured during the last 64 years in fig 4 the range for frequency increase of cdhes defined by scpdsi sti in ten countries is between 1 6 and 14 8 only limited regions show a slight decline like the usa and argentina the annual mean scpdsi showed an upward trend from 1950 to 2010 over these regions dai 2013 which may cause fewer compound dry hot events for spi sti the frequency of cdhes enhances in most countries ranges from 2 3 to 7 3 except for argentina for spei sti all countries show an upward trend ranges from 0 5 to 12 0 with remarkable increases in several countries such as south africa in south africa the increased temperature and pet dunn et al 2020 as shown in fig s2 may contribute to more compound events generally the magnitude of the increase of spei sti is relatively higher among three types of compound events in most countries 8 countries which is consistent with analysis at the global scale these results highlight the difference in the amplitude of changes in the frequency of cdhes in different countries based on multiple drought indicators 3 3 synchronous changes in different countries climate change may enhance the frequency of different countries experiencing extreme events synchronously which may affect food supply and further pose threats to the agricultural market sarhadi et al 2018 tigchelaar et al 2018 thus the frequency of multiple countries struck by cdhes at the same time is of particular interest here we quantify changes in the frequency of cdhes occurring simultaneously in two different countries defined as country country compound events though the synchronous occurrences of cdhes over different countries may not be physically linked i e caused by the same climate driver they have important implications for understanding the variability of global or regional risks of crop losses and food security as shown in fig 5 the frequency of synchronous occurrences of cdhes over the ten top maize producing countries in the same period tends to increase from the first period 1949 1980 to the recent period 1981 2012 for example the frequency of country country compound events defined by scpdsi sti increases for 80 0 of the country pairs the total number is 45 country pairs which changes from 0 3 to 3 1 with a decrease in very few pairs like argentina canada fig 5 a for spi sti the frequency of country country compound event occurrences enhances by 0 3 1 6 for 84 4 of all country pairs fig 5 b the increase in the frequency of country country compound events is the highest based on spei sti which shows an increase of 0 3 4 2 for all country pairs fig 5 c overall most country pairs show an upward trend in the country country compound events although the magnitude of the increase varies with different drought indicators if cdhes increase in large maize exporting countries maize yield loss may occur and global export supply is expected to face a decline which could threaten regional or global food security thus to further explore changes in synchronous cdhes in large maize exporting countries we select the top four maize exporting countries i e usa brazil argentina and ukraine tigchelaar et al 2018 and explore frequency changes of at least two exporting countries experiencing cdhes synchronously as shown in fig 6 it can be seen that the frequency of these three compound events presents an upward trend and spei sti shows the largest increase among those three types of compound events 6 51 during the 1949 2012 period 4 discussion 4 1 impact of thresholds the threshold for defining droughts or hot extremes may lead to discrepancy in variations of cdhes we perform the aforementioned analysis using another set of thresholds which is the co occurrence of drought indicator below the 30th percentile and hot indicator higher than the 70th percentile for the period 1949 2012 a similar pattern of increased frequency is captured fig 2 along with a significant increase in the spatial extent fig 3 for example the compound event based on spei sti still shows the highest increase with a slope of 2 20 decade fig 3 b followed by that based on scpdsi and spi for the threshold changed from d20 h80 to d30 h70 the frequency increases from 77 18 to 80 65 from 85 39 to 89 87 and from 89 72 to 92 53 for the three types of compound events i e scpdsi sti spi sti and spei sti respectively at the national scale the frequency in compound dry hot events also increases in most countries under three types of cdhes as shown in fig 4 b the consistent pattern of increased frequency of country country compound events for most country pairs is shown in fig 5 d f note that the increase in the frequency and slope based on 30 70 percentiles is higher than those based on 20 80 percentiles since more compound events can be extracted based on 30 70 thresholds for example the frequency of the country country compound events increases by 0 3 3 1 0 3 1 6 and 0 3 4 2 based on thresholds 20 80 percentiles and increases by 0 3 7 0 0 3 3 9 and 0 3 6 3 based on 30 70 percentiles for the three types of compound events i e scpdsi sti spi sti and spei sti in addition the change in three types of compound events in at least two exporting countries experiencing cdhes synchronously based on 30 70 percentiles i e 3 39 6 25 and 10 16 is higher than that based on 20 80 percentiles i e 2 08 2 08 and 6 51 which is shown in fig 6 4 2 impacts of base periods the base period to define the thresholds also affects the frequency of compound events we assessed the variation of compound events based on the thresholds of drought and hot indicators 20th 80th for the base period 1961 1990 overall consistent patterns of increased frequency and spatial extent of cdhes are shown over maize growing areas as shown in fig 2 g i fig 3 c fig 4 c fig 5 g i and fig 6 note that the slope of the spatial extent of cdhes with the base period 1961 1990 is generally greater than that of compound events with the base period 1949 2012 for example for the base period changed from 1949 2012 to 1961 1990 the spatial extent of compound events increases from 1 01 decade to 1 35 decade from 1 01 decade to 1 18 decade and from 1 42 decade to 1 70 decade for the three types of compound events i e scpdsi sti spi sti and spei sti respectively this is likely because positive temperature anomalies over global land areas have been shown for the past half century for the base period 1961 1990 ipcc 2013 jones et al 2012 trenberth et al 2014 overall similar patterns of increased frequency and spatial extent for cdhes are shown based on the base period 1961 1990 highlighting the robustness of increased frequency of cdhes 4 3 implications overall three types of compound events all show an upward trend for maize producing regions indicating increased risks of cdhes to crop yield meanwhile the magnitude of changes differs among three types of cdhes due to different drought indices this calls for an appropriate definition of cdhes for operational agricultural management the three drought indices have been widely used in previous studies in characterizing drought conditions in different regions and a consensus on suitable drought indicators has not been reached hao et al 2018 hayes et al 2011 mukherjee et al 2018 vicente serrano et al 2020 this also implies that there is no universal indicator for cdhes and that different ways of constructing cdhes have distinct properties for example for the cdhes based on spi the dry and hot condition is measured based on precipitation and temperature respectively i e isolated dry and hot conditions however for the cdhes based on scpdsi and spei the temperature or evapotranspiration has been incorporated in the computation of drought indicators which indicates concurrent dry and hot conditions to some extent vogel et al 2021 a potential way to reduce the uncertainty from different drought indicators would be selecting the indicator based on impacts of interest for example if the cdhes are employed for the prediction of crop yield the suitable drought indicator to define cdhes can be selected as the one corresponding to the highest correlation between cdhes and crop yield fu et al 2019 zampieri et al 2017 in addition an increase in the assessment of the impact of cdhes on different sectors e g crop yield or vegetation have been witnessed in the past decades feng et al 2019 ribeiro et al 2020 this study also highlights that multiple indicators of droughts and hot extremes should be evaluated in the impact assessments of cdhes for example for the impact of chdes on vegetation e g based on normalized difference vegetation index or ndvi hao et al 2021 multiple droughts indicators and hot extreme indicators can be employed to construct the joint and conditional distribution to evaluate the response of vegetation to cdhes finally though we only focus on the meteorological drought indicators representing other types of droughts to define cdhes also need to be evaluated the selection and validation of cdhes indicators and their applications for impact assessments will be further evaluated in our future studies 4 4 limitations there are some limitations in this study as other factors may affect the results and induce extra uncertainties in changes of compound events for example the calibration period of pdsi may affect the variability of drought van der schrier et al 2013 vicente serrano et al 2011 and compound events as well in addition the spi and spei can be computed at different time scales which may also add uncertainties in the magnitude of changes for example changes in the frequency of cdhes based on the 3 month and 6 month spi and spei are shown in figures s3 and s4 which show a consistent upward trend of cdhes with different magnitudes of change compared with that based on 1 month spi and spei moreover different datasets may also lead to uncertainties in computing drought hot indicators hoffmann et al 2020 liu et al 2016a and associated cdhes for example hoffmann et al 2020 showed that the uncertainties in the magnitude of variations in drought indices i e spi pdsi and spei to the input data choice could be larger than differences between indices themselves in many cases especially when the dry tail of distributions is considered nevertheless the main point of our study is to highlight large uncertainties in changes of cdhes from multiple drought indicators which deserves increased attention in the mitigation and adaptation of cdhes under global warming 5 conclusion we compare changes in cdhes over global maize areas based on different drought indicators the frequency of cdhes has increased from 1949 1980 to 1981 2012 in global maize areas along with a significant increase in the spatial extent large variations exist in the magnitude of the increased cdhes from multiple drought indicators the frequency of compound events defined by different drought indicators i e scpdsi spi and spei increases for 77 18 89 72 of all grids from 1949 1980 to 1981 2012 across the globe the slope of the annual spatial extent for the three compound events is 1 01 decade 1 01 decade and 1 42 decade over the past 64 year period respectively overall we found a relatively higher magnitude in the frequency and spatial extent of the compound event defined based on spei sti over most regions in addition the increased frequency of synchronous occurrences of cdhes is also shown in a majority of country pairs of the top ten maize producing countries based on the three types of compound events i e scpdsi sti spi sti and spei sti although the magnitude of the increase varies with different drought indicators results based on the different thresholds i e 30 70 percentiles and the base periods i e 1961 1990 show consistent results of increased cdhes though the magnitude differs due to exacerbated impacts of cdhes on different sectors of society it is important to understand the uncertainties in the variability of cdhes for mitigation efforts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is supported by the national natural science foundation of china nsfc no 41601014 the monthly precipitation temperature and self calibrating pdsi scpdsi from 1949 to 2012 were collected from the cru ts 4 03 dataset http www cru uea ac uk data we thank the editor and reviewers for the constructive comments and suggestions appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126728 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4199,the nonexistence of universal drought indicators may lead to uncertainties in the variability of drought leading to discrepancies in the frequency change in compound dry and hot events cdhes in different regions across the globe understanding the impact of the selection of drought indicators on the variability of cdhes is important for assessing their risks and pursuing mitigation measures however quantitative assessments of differences in cdhes variabilities based on different drought indicators have been lacking in this study we evaluate changes in cdhes based on three different drought indices including self calibrating palmer drought severity index scpdsi standardized precipitation index spi and standardized precipitation evapotranspiration index spei and standardized temperature index sti as the hot indicator over global maize producing areas generally the compound dry and hot event defined by spei sti shows a relatively higher magnitude of increase in the frequency and spatial extent compared with that defined by spi sti and scpdsi sti from 1949 1980 to 1981 2012 over most regions for the top ten maize producing countries the frequency of different countries affected by cdhes at the same time also increases from 1949 1980 to 1981 2012 for these three types of compound events consistent patterns are shown based on different thresholds and different base periods in defining compound events results from this study could help understand compound event variations and provide useful insights for agricultural management under global warming keywords drought index drought compound event dry and hot 1 introduction droughts and hot extremes may cause disastrous impacts on food security energy consumption and ecosystems chen et al 2020 hao et al 2018 he et al 2019 mishra and singh 2010 zhou et al 2020 the concurrence or occurrence in close succession of these two events i e compound events or extremes may lead to larger impacts than that caused by each in isolation chen et al 2018 kong et al 2020 leng et al 2016 matiu et al 2017 zscheischler et al 2018 focus on compound dry and hot events cdhes has increased due to recent occurrences of these events causing tremendous damages to crop production ecosystem function and public health such as those during summers of 2010 2018 in europe and 2006 2013 in china feng et al 2019 iizumi and ramankutty 2016 mishra et al 2020 ribeiro et al 2020 vogel et al 2019 zscheischler et al 2020 thus it is of critical importance to further our understanding of the variability of cdhes for mitigation efforts on the global scale an overall increase in temperature and associated extremes has been shown for a majority of global land areas coumou and robinson 2013 ipcc 2013 seneviratne et al 2014 however the discrepancy in the variation of droughts still exists at the regional and global scales dai 2013 hoffmann et al 2020 sheffield et al 2012 spinoni et al 2014 which is partly due to the differences in drought indicators and their capability in describing relative impacts of moisture supply and evaporative demand of the atmosphere berg and sheffield 2018 haile et al 2020 liu et al 2016a mukherjee et al 2018 vicente serrano et al 2020 for example sheffield et al 2012 highlighted that drought defined by conventional palmer drought severity index pdsi with pet estimated using the penman monteith equation showed little changes from 1948 to 2008 meanwhile an increase in drought defined by the self calibrating pdsi scpdsi from 1950 to 2010 over global land areas has been demonstrated dai 2013 the discrepancy has been explained from different evapotranspiration calculation methods based on thornthwaite or penman monteith method base periods 1950 2008 and 1950 1979 and precipitation datasets trenberth et al 2014 these results suggest that the pattern and magnitude of drought changes may vary if multiple drought indicators are used different indicators of droughts and hot extremes have been used to define cdhes and recent work suggests a consistent increase in frequency severity duration and spatial extent at regional and global scales hao et al 2013 mazdiyasni and aghakouchak 2015 sarhadi et al 2018 sharma and mujumdar 2017 wu et al 2019 for example wu et al 2019 found a consistent increase in the spatial extent of cdhes in china based on the standardized precipitation index spi and the number of hot days nhd chen et al 2019 assessed changes in the likelihood of cdhes over china based on pdsi as the drought indicator and found a substantial increase of cdhes in northern parts of china based on precipitation and temperature to define compound dry and hot conditions zscheischler and seneviratne 2017 showed an increased likelihood of cdhes in the future at the global scale these studies of cdhes are based on different drought indicators which differ in capturing different processes of moisture supply and demand for example spi is solely based on precipitation the spei includes both precipitation and temperature or potential evapotranspiration pet vicente serrano et al 2010 while scpdsi further incorporates wind speed and solar radiation in addition to precipitation and temperature if the penman monteith method is used for calculating pet wells et al 2004 it is expected that these factors may influence the variability of cdhes leading to different magnitudes of changes although consistent patterns of increased frequency of cdhes have been shown in historical periods the impact of drought indicators and their capability to capture different water and energy processes on the variability of cdhes which is important for accurate risk estimation has been lacking as such there is a pressing need to understand how the choice of drought indices may influence the detection of changes in cdhes at the global and regional scale agriculture is among the most vulnerable sectors to the concurrent drought and heatwaves or cdhes leng 2019 ribeiro et al 2020 zscheischler et al 2020 zscheischler et al 2018 previous studies have shown an increased frequency of cdhes over different cropland areas in historical periods lu et al 2018 sarhadi et al 2018 wu et al 2021 lu et al 2018 found an upward trend in the frequency of cdhes weighted by crop areas during wheat and maize growing seasons from 1980 to 2015 based on observed precipitation and temperature in china studies also show increased frequency of cdhes in future periods based on climate projections over cropland areas sarhadi et al 2018 wu et al 2021 based on the model simulations from phase 5 of the coupled model intercomparison project cmip5 sarhadi et al 2018 assessed changes in the joint probability of severe warm and dry conditions using the copula approach and found an increase in the probability from 2020 to 2050 under rcp8 5 over global cropland areas wu et al 2021 revealed that the cropland area affected by cdhes would increase to about 1 7 1 8 times by 2090 2099 compared with that during 1990 1999 based on seasonal precipitation and temperature from cmip5 to develop mitigation measures for the agricultural sectors it is critical to understand how the choice of drought indicators affects the variability of cdhes over global crop areas the objective of this study is to compare the variability of cdhes based on multiple drought indicators i e scpdsi spi and spei over global crop areas in this study we explore changing patterns of cdhes under global warming with a focus on the maize growing area monfreda et al 2008 for maize is shown to be among the most vulnerable and sensitive crops under climate change zhao et al 2017 the data and methods are introduced in section 2 results of variations based on three types of cdhes at the global and national scales are shown in section 3 followed by discussion and conclusion in section 4 and 5 2 data and methods 2 1 indicators the pdsi is developed based on the concept of water balance that incorporates the antecedent precipitation soil moisture and pet palmer 1965 its main drawback is that certain parameters are obtained in u s regions which hinders comparisons between diverse climatic areas the self calibrating pdsi scpdsi addresses this drawback by estimating parameters locally and is spatially comparable wells et al 2004 in this study the monthly scpdsi series from 1949 to 2012 were directly collected from the cru ts 4 03 dataset http www cru uea ac uk data harris et al 2014 for this scpdsi dataset pet is estimated based on the penman monteith approach the spi is computed based on the accumulation of monthly precipitation of certain time scales e g 1 month in this study mckee et al 1993 it has been recommended as the drought indicator to track the meteorological drought condition hayes et al 2011 for the accumulated precipitation a parametric or nonparametric distribution f is fitted to obtain the marginal probability p which is then transformed to the normal random variable based on the standard normal distribution φ i e spi φ 1 p in this study the empirical or nonparametric weibull plotting position formula is used to estimate the marginal probability p wu et al 2019 the motivation of using the empirical method is to avoid the parametric assumption of distributions of precipitation data the spi is calculated based on global monthly precipitation at the 0 5 0 5 resolution for 1949 2012 from cru ts 4 03 dataset to match the source of scpdsi data the spei is a multi scalar drought index that combines the effects of both precipitation and temperature vicente serrano et al 2010 it can be computed similarly to spi based on the difference between precipitation and pet which provides a measure of the water surplus or deficit in this study the pet based on the penman monteith method is taken from the cru4 03 dataset the marginal probability of the difference between precipitation and pet is also estimated based on the empirical weibull plotting position formula ahmadalipour et al 2017 the 1 month spei is calculated based on global monthly climate precipitation and pet data for the same period from the cru 4 03 dataset the standardized temperature index sti is used as the indicator of hot extremes hao et al 2019 the computation of sti follows the same procedure of computing spi based on the global temperature data from the cru 4 03 dataset 2 2 identification of compound events a compound dry and hot event is identified based on the concurrence of drought indicator values scpdsi spi and spei below the 20th percentile and hot indicator sti values above the 80th percentile of each month of the base period from 1949 to 2012 which can be expressed as a binary variable o 1 for occurrences and 0 for non occurrences based on the three drought indices and sti three types of cdhes are extracted for each month denoted as scpdsi sti spi sti and spei sti the frequency of cdhes of each period at each grid is then computed based on the number of months in which cdhes occur divided by the total number of months the annual spatial extent of compound events is defined as the number of grids within the maize growing area covered by cdhes averaged occurrences of 12 months for each year divided by the total number of grid points over the maize growing area based on the above definition and calculation we assess the global and national change of cdhes the detailed workflow of this study is shown in fig 1 3 results 3 1 changes at the global scale previous studies have shown that the global mean temperature is relatively stable for the period from the 1950s to the 1980s coumou and robinson 2013 hansen et al 2012 thus the whole study period 1949 2012 is divided into two equal periods i e 1949 1980 and 1981 2012 the changes in the frequency of cdhes for the two periods 1949 1980 and 1981 2012 over global maize areas and land areas are shown in fig 2 and fig s1 a consistent increase in the frequency of cdhes over the majority of global maize producing areas and global land areas is observed the enhanced cdhes could be related to the upward trend of the temperature for the 1949 2012 period over maize producing areas as shown in fig s2 a the increase of cdhes is also observed in the spatial extent as shown in fig 3 a based on the nonparametric mann kendall test of annual spatial extent over the past 64 year period ayantobo et al 2018 liu et al 2016b a significant increasing trend is observed with the slope of 1 01 decade 1 01 decade and 1 42 decade respectively based on the three types of compound events i e scpdsi sti spi sti and spei sti by comparing with that during the first period 1949 1980 the frequency of cdhes defined by scpdsi and sti increased in regions including western europe eastern south america africa southeast asia and eastern asia about 77 18 of maize producing areas during 1981 2012 as shown in fig 2 a results based on spi and spei show similar patterns of increased frequency over global maize producing areas around 85 39 and 89 72 respectively as shown in fig 2 b c the decrease of compound events is shown in limited areas such as parts of the usa and argentina which is consistent with previous studies feng et al 2020 this may be related to increased precipitation and pet variations over most of the above areas dai 2013 yang et al 2017 as partly shown in fig s2 b c though the consistent increase in the frequency and spatial extent is clearly shown from three different types of compound events these results highlight the discrepancy in the magnitude which results from the difference of drought indices for example the percentage of grids with increased cdhes is relatively high for the compound event defined based on spei along with the largest slope of increased spatial extent the difference between scpdsi and spei may result from the role of the atmospheric evaporation demand aed which is an essential factor for drought quantification the scpdsi is based on the soil moisture balance in which the evapotranspiration et would constrain aed in water limited regions where et is constrained by water availability the increased aed with temperature increase does not further affect pdsi or scpdsi when the soil water content is close to zero vicente serrano et al 2011 vicente serrano et al 2020 while for spei enhanced aed is expected to reduce spei values without et dependence on soil moisture limitation vicente serrano et al 2020 this is consistent with previous studies showing that spei has greater sensitivity to aed than pdsi especially in arid areas cook et al 2014 thus with the increased temperature over large regions of global maize producing areas the spei would capture more drought than the scpdsi which is expected to lead to a larger increase in cdhes in addition the difference between spi and spei is that spi is only based on precipitation without consideration of aed while spei incorporates aed in drought characterization with possibly enhanced severity of dry events this may explain the higher magnitude of increased spatial extent of cdhes based on spei than that based on spi 3 2 changes at the national scale cdhes may lead to large impacts on crop yield of different countries and pose threat to food security iizumi and ramankutty 2016 matiu et al 2017 ray et al 2015 here we select the top ten maize producing countries for further analysis including the usa china brazil argentina ukraine mexico india france canada and south africa tigchelaar et al 2018 the gridded drought indicator scpdsi spi and spei and sti within each country are spatially averaged over grids within maize areas of each country to compute changes of cdhes the change in the frequency of compound events from the period 1949 1980 to 1981 2012 for each country is shown in fig 4 increased frequency of compound dry hot events for the top ten maize producing countries is captured during the last 64 years in fig 4 the range for frequency increase of cdhes defined by scpdsi sti in ten countries is between 1 6 and 14 8 only limited regions show a slight decline like the usa and argentina the annual mean scpdsi showed an upward trend from 1950 to 2010 over these regions dai 2013 which may cause fewer compound dry hot events for spi sti the frequency of cdhes enhances in most countries ranges from 2 3 to 7 3 except for argentina for spei sti all countries show an upward trend ranges from 0 5 to 12 0 with remarkable increases in several countries such as south africa in south africa the increased temperature and pet dunn et al 2020 as shown in fig s2 may contribute to more compound events generally the magnitude of the increase of spei sti is relatively higher among three types of compound events in most countries 8 countries which is consistent with analysis at the global scale these results highlight the difference in the amplitude of changes in the frequency of cdhes in different countries based on multiple drought indicators 3 3 synchronous changes in different countries climate change may enhance the frequency of different countries experiencing extreme events synchronously which may affect food supply and further pose threats to the agricultural market sarhadi et al 2018 tigchelaar et al 2018 thus the frequency of multiple countries struck by cdhes at the same time is of particular interest here we quantify changes in the frequency of cdhes occurring simultaneously in two different countries defined as country country compound events though the synchronous occurrences of cdhes over different countries may not be physically linked i e caused by the same climate driver they have important implications for understanding the variability of global or regional risks of crop losses and food security as shown in fig 5 the frequency of synchronous occurrences of cdhes over the ten top maize producing countries in the same period tends to increase from the first period 1949 1980 to the recent period 1981 2012 for example the frequency of country country compound events defined by scpdsi sti increases for 80 0 of the country pairs the total number is 45 country pairs which changes from 0 3 to 3 1 with a decrease in very few pairs like argentina canada fig 5 a for spi sti the frequency of country country compound event occurrences enhances by 0 3 1 6 for 84 4 of all country pairs fig 5 b the increase in the frequency of country country compound events is the highest based on spei sti which shows an increase of 0 3 4 2 for all country pairs fig 5 c overall most country pairs show an upward trend in the country country compound events although the magnitude of the increase varies with different drought indicators if cdhes increase in large maize exporting countries maize yield loss may occur and global export supply is expected to face a decline which could threaten regional or global food security thus to further explore changes in synchronous cdhes in large maize exporting countries we select the top four maize exporting countries i e usa brazil argentina and ukraine tigchelaar et al 2018 and explore frequency changes of at least two exporting countries experiencing cdhes synchronously as shown in fig 6 it can be seen that the frequency of these three compound events presents an upward trend and spei sti shows the largest increase among those three types of compound events 6 51 during the 1949 2012 period 4 discussion 4 1 impact of thresholds the threshold for defining droughts or hot extremes may lead to discrepancy in variations of cdhes we perform the aforementioned analysis using another set of thresholds which is the co occurrence of drought indicator below the 30th percentile and hot indicator higher than the 70th percentile for the period 1949 2012 a similar pattern of increased frequency is captured fig 2 along with a significant increase in the spatial extent fig 3 for example the compound event based on spei sti still shows the highest increase with a slope of 2 20 decade fig 3 b followed by that based on scpdsi and spi for the threshold changed from d20 h80 to d30 h70 the frequency increases from 77 18 to 80 65 from 85 39 to 89 87 and from 89 72 to 92 53 for the three types of compound events i e scpdsi sti spi sti and spei sti respectively at the national scale the frequency in compound dry hot events also increases in most countries under three types of cdhes as shown in fig 4 b the consistent pattern of increased frequency of country country compound events for most country pairs is shown in fig 5 d f note that the increase in the frequency and slope based on 30 70 percentiles is higher than those based on 20 80 percentiles since more compound events can be extracted based on 30 70 thresholds for example the frequency of the country country compound events increases by 0 3 3 1 0 3 1 6 and 0 3 4 2 based on thresholds 20 80 percentiles and increases by 0 3 7 0 0 3 3 9 and 0 3 6 3 based on 30 70 percentiles for the three types of compound events i e scpdsi sti spi sti and spei sti in addition the change in three types of compound events in at least two exporting countries experiencing cdhes synchronously based on 30 70 percentiles i e 3 39 6 25 and 10 16 is higher than that based on 20 80 percentiles i e 2 08 2 08 and 6 51 which is shown in fig 6 4 2 impacts of base periods the base period to define the thresholds also affects the frequency of compound events we assessed the variation of compound events based on the thresholds of drought and hot indicators 20th 80th for the base period 1961 1990 overall consistent patterns of increased frequency and spatial extent of cdhes are shown over maize growing areas as shown in fig 2 g i fig 3 c fig 4 c fig 5 g i and fig 6 note that the slope of the spatial extent of cdhes with the base period 1961 1990 is generally greater than that of compound events with the base period 1949 2012 for example for the base period changed from 1949 2012 to 1961 1990 the spatial extent of compound events increases from 1 01 decade to 1 35 decade from 1 01 decade to 1 18 decade and from 1 42 decade to 1 70 decade for the three types of compound events i e scpdsi sti spi sti and spei sti respectively this is likely because positive temperature anomalies over global land areas have been shown for the past half century for the base period 1961 1990 ipcc 2013 jones et al 2012 trenberth et al 2014 overall similar patterns of increased frequency and spatial extent for cdhes are shown based on the base period 1961 1990 highlighting the robustness of increased frequency of cdhes 4 3 implications overall three types of compound events all show an upward trend for maize producing regions indicating increased risks of cdhes to crop yield meanwhile the magnitude of changes differs among three types of cdhes due to different drought indices this calls for an appropriate definition of cdhes for operational agricultural management the three drought indices have been widely used in previous studies in characterizing drought conditions in different regions and a consensus on suitable drought indicators has not been reached hao et al 2018 hayes et al 2011 mukherjee et al 2018 vicente serrano et al 2020 this also implies that there is no universal indicator for cdhes and that different ways of constructing cdhes have distinct properties for example for the cdhes based on spi the dry and hot condition is measured based on precipitation and temperature respectively i e isolated dry and hot conditions however for the cdhes based on scpdsi and spei the temperature or evapotranspiration has been incorporated in the computation of drought indicators which indicates concurrent dry and hot conditions to some extent vogel et al 2021 a potential way to reduce the uncertainty from different drought indicators would be selecting the indicator based on impacts of interest for example if the cdhes are employed for the prediction of crop yield the suitable drought indicator to define cdhes can be selected as the one corresponding to the highest correlation between cdhes and crop yield fu et al 2019 zampieri et al 2017 in addition an increase in the assessment of the impact of cdhes on different sectors e g crop yield or vegetation have been witnessed in the past decades feng et al 2019 ribeiro et al 2020 this study also highlights that multiple indicators of droughts and hot extremes should be evaluated in the impact assessments of cdhes for example for the impact of chdes on vegetation e g based on normalized difference vegetation index or ndvi hao et al 2021 multiple droughts indicators and hot extreme indicators can be employed to construct the joint and conditional distribution to evaluate the response of vegetation to cdhes finally though we only focus on the meteorological drought indicators representing other types of droughts to define cdhes also need to be evaluated the selection and validation of cdhes indicators and their applications for impact assessments will be further evaluated in our future studies 4 4 limitations there are some limitations in this study as other factors may affect the results and induce extra uncertainties in changes of compound events for example the calibration period of pdsi may affect the variability of drought van der schrier et al 2013 vicente serrano et al 2011 and compound events as well in addition the spi and spei can be computed at different time scales which may also add uncertainties in the magnitude of changes for example changes in the frequency of cdhes based on the 3 month and 6 month spi and spei are shown in figures s3 and s4 which show a consistent upward trend of cdhes with different magnitudes of change compared with that based on 1 month spi and spei moreover different datasets may also lead to uncertainties in computing drought hot indicators hoffmann et al 2020 liu et al 2016a and associated cdhes for example hoffmann et al 2020 showed that the uncertainties in the magnitude of variations in drought indices i e spi pdsi and spei to the input data choice could be larger than differences between indices themselves in many cases especially when the dry tail of distributions is considered nevertheless the main point of our study is to highlight large uncertainties in changes of cdhes from multiple drought indicators which deserves increased attention in the mitigation and adaptation of cdhes under global warming 5 conclusion we compare changes in cdhes over global maize areas based on different drought indicators the frequency of cdhes has increased from 1949 1980 to 1981 2012 in global maize areas along with a significant increase in the spatial extent large variations exist in the magnitude of the increased cdhes from multiple drought indicators the frequency of compound events defined by different drought indicators i e scpdsi spi and spei increases for 77 18 89 72 of all grids from 1949 1980 to 1981 2012 across the globe the slope of the annual spatial extent for the three compound events is 1 01 decade 1 01 decade and 1 42 decade over the past 64 year period respectively overall we found a relatively higher magnitude in the frequency and spatial extent of the compound event defined based on spei sti over most regions in addition the increased frequency of synchronous occurrences of cdhes is also shown in a majority of country pairs of the top ten maize producing countries based on the three types of compound events i e scpdsi sti spi sti and spei sti although the magnitude of the increase varies with different drought indicators results based on the different thresholds i e 30 70 percentiles and the base periods i e 1961 1990 show consistent results of increased cdhes though the magnitude differs due to exacerbated impacts of cdhes on different sectors of society it is important to understand the uncertainties in the variability of cdhes for mitigation efforts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is supported by the national natural science foundation of china nsfc no 41601014 the monthly precipitation temperature and self calibrating pdsi scpdsi from 1949 to 2012 were collected from the cru ts 4 03 dataset http www cru uea ac uk data we thank the editor and reviewers for the constructive comments and suggestions appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126728 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
