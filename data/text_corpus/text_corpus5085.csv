index,text
25425,the impacts of climate change especially sea level rise are an increasing threat to the world s coastal regions following recommendations made by the united nations about the preservation of mangrove environments particularly given their potential for effective natural defence against wave driven hazards a series of experiments have been conducted to quantify the ability of mangroves to counter climate change impacts to date these experiments have been limited by computational cost and inability to model multiple scenarios with improved data quality and availability machine learning has enormous potential to supplement or even replace existing numerical methods this article presents both an outline of the importance of protecting mangrove environments and a review of methods currently used to quantify the capacity of mangroves to adapt to climate change impacts in view of the limitations of existing numerical methods the article also discusses the potential of machine learning as an efficient and effective alternative keywords mangrove environments climate change hydro morphodynamic modelling adaptation policies machine learning data driven modelling data availability no data was used for the research described in the article 1 introduction climate change is a global threat due its influence on weather and climate phenomena and their subsequent impacts in all parts of the world changes in the frequency and magnitude of temperature extremes tropical cyclones storm surges and episodes of land erosion present enormous societal and environmental challenges disproportionately among some of the world s most vulnerable populations in coastal regions of the global south the most recent 6th assessment report ar6 of the intergovernmental panel on climate change ipcc revealed an unequivocal increase in ocean temperature and sea level alongside a decrease in sea ice extent since the beginning of the 20th century ipcc 2021a failure to manage the effects of climate change and slow the progress of global warming is likely to result in a 1 5 c temperature increase by 2040 and a 3 c increase by 2100 relative to pre industrial levels ipcc 2021b it is therefore of critical importance to find solutions for the mitigation of climate change impacts with the potential to be implemented globally one such effort which was endorsed by the climate change and land report of the ipcc samset et al 2020 and the global adaptation commission report estoque et al 2020 is nature based solutions pe er et al 2020 unlike engineering based alternatives such an approach has the potential to tackle climate mitigation and adaptation challenges in a more affordable way while benefiting society and nature seddon et al 2020 the world s forests have an important role to play in climate change mitigation and adaptation it is vital to observe how forests are responding to the effects of a warming climate alongside the stressors of deforestation and land use change in order to identify possible solutions and to quantify the extent to which they are appropriate to the issues faced several dynamic vegetation models anticipate that forest environments will be able to shift towards the areas with more suitable climate conditions which could result in forest expansions kirilenko and sedjo 2007 however there is a concern that this migration will be outpaced by climate and societal disturbances thus leading to significant losses of natural forests kirilenko and sedjo 2007 the lag of inland migration behind climate change effects is in part caused by the lack of connectivity to nearby habitats in addition to the presence of microclimates that differ from those in the surrounding areas and which might not be suitable for the present vegetation species pecl et al 2017 in addition there are human driven factors e g land development overexploitation species translocations and introductions and pollution that also affect the ability of such forests to migrate for example in a study by réjou méchain et al 2021 which investigated rainforest change in africa it was suggested that the areas which will be heavily affected by climate change are those vulnerable to human induced pressures such as the forests in coastal gabon the democratic republic of the congo and atlantic forests the world s mangrove environments cover 15 million ha and provide a unique habitat for many species in addition to goods and services to humans carugati et al 2018 mangroves are amongst the most productive marine ecosystems with an economic value ranging from us 3 624 98 us 26 734 61 per ha per year lee et al 2014 rizal et al 2018 however global mangrove habitats are declining at an alarming rate of 1 2 per year with 20 35 of global mangrove extent lost over the last 50 years due to deforestation and land exploitation in addition to sea level rise and erosion alongi 2015 goldberg et al 2020 according to the latest ipcc climate projections to the end of the twenty first century mangrove environments are expected to exhibit vulnerabilities in different parts of the world for example mangroves present in australia pakistan the arabian peninsula and both coasts of mexico are expected to decline as salinity increases freshwater becomes more scarce and critical temperature thresholds are reached more frequently alongi 2015 in the caribbean islands mangrove environments will decline as sea level rises and there is little or no upland space to colonise in new zealand usa and china mangrove environments will continue to expand their latitudinal range as temperature and atmospheric co 2 concentrations increase nevertheless these predictions are based on the assumptions that the decline or expansion of mangroves is related to complex changes in floral and faunal species composition morphology biodiversity biomass physiology growth and productivity alongi 2015 given the importance of having a plan to fight climate change such as forest planting restoration and protection it is imperative to have accurate assessments of the ability of these environments to mitigate climate change impacts and identify the suitable conditions for them to survive and grow there have been recent advances in the understanding of different climatic events and their effects on the natural ecosystems using both field studies and numerical modelling simulations alexander 2016 grant et al 2017 shan et al 2021 in particular the improved spatial resolution of global climate models and their capacity to be downscaled to regional scales has created new opportunities to understand how different climatic events affect the natural ecosystem functioning and the adaptation capabilities ummenhofer and meehl 2017 while higher spatial resolution is important there are several physical variables and model parameters that need to be known in order to achieve the optimal model skill lavidas and venugopal 2018 this issue becomes most apparent at the point of application to real world situations where since users are required to estimate or tune several parameters to deliver a customised model for a particular area of interest lavidas and venugopal 2018 machine learning methods are becoming increasingly recognised as powerful tools for technological progress in a variety of fields although machine learning has been widely applied in business and scientific disciplines including food identity discovery anxiety detection and speech detection delić et al 2019 erban et al 2019 rolnick et al 2019 sau and bhakta 2019 environmental application has not been fully in particular its use in assessing adaptation solutions in the face of a changing climate is rare milojevic dupont and creutzig 2021 nonetheless there are numerous applications for which machine learning has enormous potential including remote sensing smart and urban building design and transportation ladi et al 2022 avand et al 2021 fathi et al 2020 bhavsar et al 2017 perhaps most crucially the application of machine learning in climate change mitigation is particularly under explored and should be accelerated if it is to reach its potential milojevic dupont and creutzig 2021 with respect to mangrove environments only a small number of studies addressing climate change mitigation using machine learning pham et al 2019 goldberg et al 2020 maina et al 2021 with most applications limited to biomass estimation using satellite imagery jachowski et al 2013 zhu et al 2017 pham and brabyn 2017 hauser et al 2020 ghosh and behera 2021 maurya et al 2021 the aim of this article is three fold 1 to outline climate driven threats posed to mangrove environments to justify protective measure in view of their mitigatory capacity 2 to outline the latest developments and limitations in numerical modelling approaches taken to quantify climate change impacts on mangrove environments and 3 to outline the potential for machine learning methods both in terms of overcoming the limitations of numerical modelling and in understanding mangrove qualities and threats the article is structured as follows in section 2 we discuss the geographical distribution of mangrove environments in addition to the dangers facing such environments and possible adaptation strategies to help mangroves adapt to climate change effects in section 3 we explain how mangroves are able to attenuate waves and prevent erosion using different types of numerical models and experiments and emphasise on the limitations of these studies in section 4 we further discuss the limitations identified and provide possible solutions to overcome them using machine learning 2 mangrove environments the global state of play in addition to playing a vital role in conserving human sustainability and livelihoods in some of the world s poorest regions mangrove environments have ecological and economic importance lee et al 2014 mangrove environments produce oxygen dasgupta et al 2017 sequester carbon yando et al 2016 and provide breeding grounds and nursery sites for several terrestrial and marine organisms including commercial species and juvenile reef fish carugati et al 2018 most importantly and particularly so in the context of this article they provide great protection for populations of low elevation coastal zones from catastrophic events such as storm surges cyclones tsunamis and land erosion alongi 2015 there are about 70 different mangrove species spanning 19 distinct families and 28 genera with a range of different morphology and physiology characteristics duke et al 1998 global mangrove biogeography is characterised primarily by two locations which are the indo west pacific 54 mangrove species and the atlantic east pacific 17 mangrove species tomlinson 2016 with few shared species mangroves presence is largely limited to latitudes between 30 north and 30 south numbere 2018 the northern range roughly corresponds with japan 31 22 n and bermuda 32 20 n while the southern range extends to new zealand 38 03 s australia 38 45 s and on the east coast of south africa 32 59 s spalding 2010 nowhere is the vulnerability and value of mangrove environments more evident than across the sundarbans a unesco world heritage site that constitutes the world s largest single block of mangrove forest payo et al 2016 it lies on the boundary between the ganges brahmaputra delta and the bay of bengal das et al 2012 the forest consists of about 200 islands separated by about 400 interconnected tidal rivers creeks and canals roy et al 2016 fig 1 shows the extent of the sundarbans mangrove forest this region is known for frequent cyclonic storms and tidal bores that originate from the bay of bengal parvathy et al 2017 the most abundant mangrove species are e agallocha 59 69 of total mangroves followed by h fomes 30 89 c decandra 6 12 and x mekongensis 0 82 2 1 threats facing mangrove environments an estimated 64 of the world s mangrove environments are within 25 km of large urban centers and are threatened by future urban development marois and mitsch 2015 the threat of deforestation and degradation becomes apparent in low income regions who rely heavily on such environments for building material charcoal and non timber forest products lau and scales 2016 between 2000 and 2016 human factors contributed to more than 62 of mangrove losses goldberg et al 2020 however a decrease in the human related losses in recent years caused an increase in the contribution of climatic losses in fact the natural causes have become the dominant factor for the total mangrove losses compared to the human driven actions merzdorf et al 2020 fig 2 shows the geographical distribution of mangrove species and the causes and extent of mangrove loss between human driven and natural factors with respect to the effect of climate change on nature it is shown that the mean sea level has risen steadily due to the contribution of glaciers and ice sheets oppenheimer and glavovic 2019 tide gauge and altimetry observations show that mean sea level has increased from 1 4 mm yr 1 over the period 1901 1990 to 2 1 mm yr 1 over the period 1970 2015 to 3 2 mm yr 1 over the period 1993 2015 to 3 6 mm yr 1 over the period 2006 2015 oppenheimer and glavovic 2019 chapter 24 of the ipcc fifth assessment report ar5 in the working group ii wgii concluded that the mean sea level rise would continue at an accelerating rate and in return contribute to extreme coastal high water levels hijoka et al 2014 moreover the combination of sea level rise and cyclone intensification would contribute to increases in coastal flooding in addition to damages to mangrove environments and coral reefs which would aggravate wave damage in the regions lying beyond the natural defences hijoka et al 2014 besides flooding this would also lead to an increase in land erosion and saltwater intrusion hijoka et al 2014 consequently the increase in sea water level along with the intensification of cyclones would pose a key issue for coastal areas in particular those experiencing high cyclone frequency or intensity gedan et al 2011 villanoy et al 2012 since sea level rise slr is considered the most important threat to mangrove environments ghosh et al 2016 used geographical information system gis to predict the impact of slr on the mangrove under three scenarios related to slr which are 0 46 m low 0 75 m medium and 1 48 m extreme at the bangladesh sundarbans the results of the study show that the two dominant mangrove species h fomes and e agallocha will be inundated by 0 41 1 29 16 73 and 0 72 1 94 14 88 for low medium and extreme slr scenarios respectively under 2 4 mm year net subsidence rate ghosh et al 2016 with respect to the subsidence at the sundarbans it was noticed that the farakka dam in india has blocked the sediment transport from the ganges to the sea which resulted in a significant decline of mangroves particularly in the central and eastern sectors of the sundarbans godoy and de lacerda 2015 in addition to inundation the mangroves distribution would also change due to salinity intrusion more specifically in response to the increasing salinity some salt loving species such as c decandra would have the opportunity to expand into the non salt tolerant species such as h fomes sudhir et al 2022 similar results were found by dasgupta et al 2017 where there are considerable losses for heritiera fomes significant gains for excoecaria agallocha small changes for avicennia alba a marina a officinalis ceriops decandra and sonneratia apetala and mixed outcomes for species combinations according to the estimates 2 2 potential for mangrove environments to mitigate climate change impacts despite the challenges that mangrove environments face with respect to slr and salinity intrusion they are considered as effective natural defences against climate change chow 2017 dasgupta et al 2019 salgado and martinez 2017 this is especially true in wave attenuation where mangroves are able to reduce wave height by 35 whilst providing a cost effective option for restoration projects in comparison with other coastal habitats such as salt marshes coral reefs and seagrass narayan et al 2016 in addition to attenuating waves due to slr mangroves are also able to reduce the damage from extreme climatic events such as tsunamis and tropical cyclones as they act as the first line of natural defence against such events amira kamil et al 2021 krauss and osland 2020 furthermore mangrove environments are great at providing sediment stability and prevent erosion with their complex root structures marois and mitsch 2015 we conclude this section by discussing a case study in which mangrove environments have been utilised as a bio shield a restoration project at kuala selangor in malaysia was initiated with the motivation to use cheap and natural methods to protect the region however the project was reported as a failure after 70 of mangrove saplings died in the first eight months of the restoration project ghazali et al 2016 the main reason behind this failure was the incomplete knowledge of the hydrodynamic systems present at the site the study recommended the examining how the new mangroves resemble the blueprint of rehabilitation determine the efficiency of the restoration and discern the reduction of wave height and velocity due to the mangroves present ghazali et al 2016 according to this recommendation numerical models would enhance the knowledge of mangrove scientists by providing simulation results based on the proposed restoration plan schmitt et al 2013 flores de santiago et al 2017 to explain numerical modelling would simulate the new dynamics of the region based on the added mangrove trees the mangrove scientists would then compare those new dynamics with the previous one before adding the mangrove trees based on different climatic scenarios this would provide insight towards the success of the proposed restoration plan and thus increase the project success rate mangrove environments could be an important solution to fight climate change effects such as sea level rise sediment erosion and cyclones the advantage of such environments is their low cost to build in comparison to hard protection and their effectiveness in reducing wave heights and preventing erosion due to their complex root structure this stresses the need to protect mangrove environments and to focus on restoring such environments in damaged areas numerical modelling could play a crucial role in determining the success of such restoration projects as they could determine the efficiency of these projects from the simulation results as a result the next section discusses different modelling approaches used to model mangrove environments 3 hydro morphodynamic modelling in this section we discuss works that have focused on quantitatively measuring the capacity of mangrove environments to adapt to climate change effects such as sea level rise and erosion historically work in the field of hydrodynamics and measuring sediment transport for mangrove environments has fallen into two categories flume experiments or lab based experiments where the mangroves are substituted by rods and in situ or field based experiments where field observations are collected using different techniques buldakov et al 2017 maza et al 2017 2019 in recent years with the rise in commercially available design and simulation software numerical modelling which uses computer based simulations has emerged as a third approach nguyen et al 2015 sharafati et al 2019 the latter introduced the ability to expand the modelling to different configurations thus optimising the design and parameter values of the problem at hand deal et al 2018 heyrani et al 2021 3 1 flume and in situ experiments the first flume experiments were established by grove karl gilbert in geomorphology and designed specifically to study the mechanics of fluvial systems and sediment transport sherman et al 2013 other applications used flume experiments to measure the velocity of water through pools filled with gravel zimmermann et al 2020 in terms of vegetation such experiments were used to determine the ability of the former to capture sediments kretz et al 2021 with respect to mangrove environments flume experiments were conducted to determine their ability to attenuate waves abdullah et al 2019 on the other hand in situ experiments have been used to collect data at the site of the region of interest for several purposes such as to analyse the groundwater changes śliwińska et al 2019 compare it with data from numerical models ahmmed et al 2021 miramontes et al 2019 and detect the presence of dangerous bacteria teta et al 2017 in terms of mangrove environments such experiments have been conducted for a variety of reasons including measuring soil carbon storage enhancement xiong et al 2018 wastewater bioremediation theuerkauff et al 2020 and change in sediment distribution due to mangroves presence kamal et al 2017 important results derived from flume experiments about the dynamics of mangroves showed that the factors affecting these abilities were hydrodynamic such as the wave nature and depth of water botanic that constitutes the type of mangrove tree present and its density and the geological factors that affect the nature of the ground rasmeemasmuang and sasaki 2015 for mangroves specifically the distribution density of the model maza et al 2017 in addition to the root and trunk density wang et al 2022 play a critical role in the mangroves ability to reduce wave heights and velocities in addition to wave attenuation the sediment trapping capacity of mangroves has received extensive attention in recent decades this is in part due to the anticipated impacts of accelerated sea level rise as a result of global warming which would result in the increased erosion chen et al 2018 understanding the trapping mechanism in which mangroves trap sediments with their complex root structure is also important in engineering reconstruction projects li et al 2019b generally the factors affecting the trapping capacity are similar to those discussed for the wave attenuation which are mangrove density biomass as well as the inter tidal position determining submergence emergence status permatasari et al 2018 suwa et al 2021 experiments on this subject showed that the root porosity of mangroves plays a critical role in determining the erosion level and the capacity of mangroves to adapt to higher velocities kazemi et al 2021 furthermore the height of mangroves can substantially decrease the turbulence intensity and therefore reduce the erosion potential horstman et al 2018 with respect to in situ observations mangroves showed desired sediment trapping abilities with a 20 decrease in the suspended sediment concentration compared with bare mudflat chen et al 2018 a similar conclusion was reached by permatasari et al 2018 who studied the trapping capacity of mangrove environments at the lamreh estuary where a negative correlation was deduced between mangrove density and suspended sediment transport although the presented results are encouraging flume experiment are faced with reproducibility issues as the setup needs to be changed for different experiments and results cannot be extrapolated to other conditions another issue is that those laboratory experiments are a simplification of real world scenarios therefore conclusions obtained must be used with caution as there are several parameters such as true climate variabilities and land conditions that are not considered accurately represented during the experiments regarding in situ experiments they are limited by cost of equipment size of the area of interest and equipment availability 3 2 numerical modelling with respect to hydrodynamic and sediment transport applications numerical modelling has been extensively applied as it is faster and more practical in terms of repeating the simulation using different configurations compared with collecting data from flume experiments or field surveys to conduct the review of numerical models a compilation of modelling studies written in english over the last eight years 2015 2022 was performed the research was conducted on september 2022 in scopus and google scholar we searched for literature using the different combinations of the following boolean search strings for paper theme mangrove and numerical modelling or hydrodynamic or morphodynamic or hydro morphodynamic or 1d or 2d or 3d or navier stokes or shallow water equations or finite difference method or finite volume method or finite element method it is important to note that although there are other numerical models available the choice was restricted to those that modelled mangrove environments only among the reviewed numerical models they can be grouped according to two main criteria the solving dimension and the method starting with the dimensions numerical models can either be 1d 2d or 3d 1d models solve the saint venant equations and they are computationally inexpensive however such models cannot describe the full settings of the region as flow is assumed in one direction leading to inaccurate results as a result such models are restricted to 1d flow as in pipes or closed channels 2d models solve the shallow water equations and are much more used to model mangrove environments as they offer better flexibility and more applicability to model mangroves in more complex settings nevertheless they are less computationally efficient that 1d models and require better meshes judge et al 2018 finally 3d models solve the full navier stokes equations and can capture the full complexity of the region and application of mangroves this can be become very useful when investigating small effects of mangroves such as mangrove seedlings on the hydrodynamics and sediment change le minor et al 2019 nonetheless it could become computationally and time expensive thus either limiting its application to small scale problems or requiring expensive hardware to model large regions table 1 summarises the advantages and limitations of each dimension with respect to the solving methods numerical models either use finite difference method fdm finite volume method fvm or finite element method fem the finite difference method is the most simple and suitable for 1d problems and provide high stability and computational efficiency however it is not accurate for solving on irregular domains and complex structures such as modelling real mangrove environments along a coast the finite volume method is most commonly used for solving problems with mangrove environments as it provide accurate solutions for such applications especially in 2d nonetheless this method is sensitive to time steps and complex domains would require a smaller time step thus reducing its time efficiency finally the finite element method provides high accuracy solutions and is best suitable for complex regions this is currently being used in next generation coastal ocean models kärnä et al 2018 solver stability remains an issue and a balance between accuracy and time efficiency is needed table 2 summarises the differences between the solving methods 3 2 1 1d models following discussion of the different dimensions and solver methods used in the numerical models we move to the different software used to model the hydro morphodynamics of mangrove environments along with some applications and limitations starting with the tsunami flow inversion from deposits tsuflind this one dimensional finite volume software was initially developed for tsunami applications tang and weiss 2015 however soria et al 2018 used it to quantify the ability of mangroves to act as a buffer against high velocity flows based on sand deposits as the haiyan typhoon it was concluded that mangroves in extreme cases were not suitable to prevent such flows from land inundation however the study assumed no bedload change and sediment deposition to be uniform which would have reduced the velocity on land roeber and bricker 2015 moving on mike11 is a one dimensional finite difference solver that is mainly used for simulating flow in estuaries rivers irrigation systems and channels gauthier et al 2017 it was used by van dat et al 2021 to study the effect of salt intrusion due to sea level rise on mangrove environments it was found that salt intrusion greatly affects mangrove distribution and thus should be considered when developing a governance strategy for mangrove ecosystems nonetheless without the ability to model realistic complex flows in 1d it would be very difficult to quantify these effects accurately and would just be left with the general behaviour finally xbeach was used to model wave dampening by mangrove environments van rooijen et al 2015 in this study the authors used the 1d formulation of xbeach although it can also be used for 2d problems the study reported major underestimation of the calculated values when compared to laboratory data main reasons for such performance were attributed to lack of calibration data and the inability to determine proper values for the drag coefficient 3 2 2 2d models a major limitation of one dimensional models is that they cannot model real world situations accurately which limits the applicability of such models one solution to that is to use a 2d model as it could provide better applicability and flexibility the difference in wave attenuation among different mangrove species was studied by sinha et al 2019 the study used the simulating waves nearshore swan a 2d finite difference model to determine the effects of salinity on mangrove wave attenuation the results show that the mangroves present at the hyposaline have reduced the wave heights by 31 in november and 51 in july more than the wave attenuation achieved by the hypersaline estuary sinha et al 2019 no data however was used to validate the simulated results another study that modelled wave attenuation used simulating waves till shore swash 2d numerical model which also uses a finite difference numerical scheme phan et al 2019 although the simulated results agreed with the data the study used a laboratory experiment with a fixed drag coefficient this would hinder the interpretability of results to extend beyond the experiment i e for real world scenarios deb and ferreira 2018 used swan with advanced circulation adcirc to model wave and hydrodynamics respectively the model used also showed the mangrove s ability to attenuate storm surges but it was limited on validation data and with outdated historical data for the calculation of bottom roughness as a result this has lead to a large simulation error where the root mean squared error rmse was 0 3 m to overcome this limitation horstman et al 2015 used field data and numerical modelling to model the hydro morphodynamics of mangrove environments the study used delft3d flow to study the contributions of various biogeophysical mangrove settings to the observed tidal dynamics after gathering field data for calibration and verification delft3d flow modelled the hydrodynamics and change in sediment concentration simultaneously the study concluded that mangroves are greatly sensitive to climate change and anthropogenic threats which could decrease their ability to reduce wave velocities and its sediment trapping abilities the data limitation issue was also tackled by pelckmans et al 2021 through using remote sensing and geographical information systems gis techniques the study modelled wave propagation in mangroves using telemac 2d a finite volume solver although the error between measured and simulated data ranged between 0 2 7 8 the error increased to 4 4 8 73 during spring tides reaching about 0 39 m one possible issue that lead to such errors is using a relatively high friction value to model mangrove 0 14 representing a dense forest mcivor et al 2012 zhang et al 2019 used hyrdrosed2d a depth integrated 2d numerical model to study the attenuation of tsunami run up waves by mangroves hydrosed2d is a finite volume model that solves the nonlinear shallow water equations results showed that mangrove environments are able to attenuate tsunami run up waves and prevent land inundation nonetheless errors up to 10 were reported between measured and simulated data limiting the confidence of the quantified results continuing on similar climatic events mike21 fm was used to model mangrove effect on cyclones rahdarian and niksokhan 2017 mike21 fm is a 2d finite volume model which can calculate wind and wave induced currents to simulate storm surges results showed that the area with mangroves protected 90 km 2 of land from inundation and water levels were reduced between 0 11 1 25 m this implies the need to protect existing mangroves and plan restoration projects for areas with damaged or without mangrove environments as they play a critical role in reducing water inundation and protecting lives speaking of restoration projects srh 2d another 2d finite volume solver was used to asses floodplain restoration shih et al 2022 the results showed the ability of mangroves to migrate and expand their habitat extent using the spread of their propagules nevertheless the study used a propagation trajectory model which relies on a probability and has a randomness characteristic presenting a modelling uncertainty 3 2 3 3d models finally moving on to the more complex 3 dimensional models such models are able to capture more complex fluid features and their effect on mangroves such as turbulence wang et al 2020 ihfoam a 3d finite volume solver was used to model solitary wave attenuation by mangrove environments by solving the reynolds averaged navier stokes rans in addition instead of using a bulk drag coefficient the study investigated using time varying drag coefficient results showed that mangrove wave reduction abilities varied significantly depending on the density of vegetation as previously discussed the major limitation arises from the needed to have very fine meshes to simulate such accurate flow dynamics which consequently would increase the computational cost significantly thus limiting the usages of 3d modelling to numerical flume experiments would help alleviate such complexities wang et al 2020 this was also done in le minor et al 2019 who used openfoam a finite volume solver to model the hydraulics and sediment dynamics of mangrove seedlings the study used a cm scale flume experiment and it showed that the seedling ability is heavily affected by turbulences and flow magnitudes however extrapolating the results to large scale real applications is difficult as the steady did not incorporate wave forcing and a dynamic mesh to mimic real waves furthermore adding such parameters would require different model settings increasing the computational cost even further a numerical study of climate change impacts namely sea level rise on mangrove distribution was conducted by luo and chui 2022 the study used mantra a 3d finite element model to account for spatio temporal variations in water dynamics and vegetation change by solving water and mass balances at daily timescales in their study the authors modelled a large scale region in southern china unlike the previous studies which used flume experiments the study also modelled the dynamics under different climate change scenarios for the next 120 years results show that under low and moderate sea level rise scenarios mangroves are able to adapt and even increase however such conclusions are based on the semi enclosed bay site and cannot be used in other regions such as the open sea for protection or restoration projects moreover the study assumed that soil accretion would keep up with sea level rise rate which might not be realistic under a moderate and high scenarios 3 2 4 summary table 3 summarises the reviewed models and their respective applications the main limitations which can be drawn from this review are lack of calibration and validation data simplifying assumptions and using small scale settings to prevent high computational costs such weaknesses have led to high errors and inability to extrapolate or generalise conclusions from the derived simulation results furthermore numerical studies are also quite challenging since it is difficult to model the complex nature of the waves mangroves and even the topographical features of the region of study which could cause inaccurate flow and sediment dynamics guan et al 2018 3 3 model data integration as discussed in section 3 1 there exist several challenges in modelling hydro morphodynamic applications using flume and in situ experiments including the site location and complex mangrove structure this makes the deployment of sensing equipment difficult and could potentially result in inaccurate results where field measurements are critically affected by the location of the device on the other hand due to the complexity of the numerical models equations and lack of calibration and validation data as discussed in section 3 2 the simulated results would suffer from significant errors as a result a combination of in situ and numerical methods as a solution to overcome the discussed limitations of each method is suggested le minor et al 2021 the proposed approach was to employ process based numerical simulations as a complementary tool to in situ and flume measurements experimental data would then be needed to assess the built model s performance under conditions similar to those observed in real mangrove environments in addition in situ measurements would be used to calibrate the model and validate the results of the numerical model this is defined as model data integration which is the integrated and balanced use of physical and numerical models vyzikas et al 2014 this was specifically implemented by cannon et al 2020 who aimed at assessing the hydrodynamic thresholds for the persistence of mangrove environments against waves and tidal inundations it was motivated by the need to consider hydrodynamic factors in mangrove restoration projects and to evaluate wave thresholds for use in site selection which would significantly improve the likelihood of successful mangrove recruitment and retention the study used a variety of data sources to measure shoreline structures and vegetation composition and swan was used to model wave heights the derived results suggested that mangrove survival was significantly influenced by the wave climate at all recurrence rates cannon et al 2020 combining collected data and the numerical model lead to accurate results when calculating the critical wave threshold with just 3 8 error a similar conclusion was also reached in studying the thresholds of mangrove survival with rapid increase in sea level using a similar approach saintilan et al 2020 nonetheless using a logistic regression to understand the probability of mangrove survival might not represent the relationship of the variables properly on the other hand an ensemble to decision trees known as random forest could provide better predictions for mangrove survival and highlight the effect of each variable accurately maina et al 2021 in addition to studying the hydrodynamics modelling sediment change by using a data model integration approach is also as important to better investigate the effect of erosion and deposition due to climate change on mangrove environments woodroffe et al 2016 sidik et al 2021 a study by east et al 2020 aimed to model reef hydrodynamics and sediment transport under different sea level rise scenarios the authors used field surveys and satellite imagery to characterise the range of substrate types hydrodynamic settings and ecological communities furthermore the data was used to run a two dimensional depth averaged wave model by using a green naghdi free surface solver from the open source model basilisk popinet 2015 the results of the study showed that inland migration is likely to occur under sea level rise the limitation in this study however was assuming the continuous sediment production by the carbonate producing organisms this could be affected by climate change impacts such as increases in ocean acidity and sea surface temperatures east et al 2020 4 research gaps and opportunities in addition to the limitations discussed in section 3 2 about some inaccuracies noticed in numerical models due to the inability of these models to capture and explain all of the complex physical phenomena another challenge of working with numerical models is their high computational cost to solve dynamic processes xing et al 2018 such challenges become apparent when solving the hydrodynamics bhola et al 2018 and sediment transport models ţene et al 2018 thus there is a trade off between a computationally expensive yet more accurate model solving the full navier stokes equations and a simplified depth averaged navier stokes known as nonlinear shallow water equations reducing the computational cost fent et al 2018 as discussed in section 1 the applications of machine learning in climate change related topics have not been explored to its potential however with the increased attention towards addressing climate change and development in the field of machine learning recent work has investigated the application of machine learning models to aid in adaptation policies biesbroek et al 2020 li et al 2019a in particular several research studies have been exploring the hydrological changes including extreme scenarios associated with climate change using machine learning methods ardabili et al 2019 das and nanduri 2018 davenport and diffenbaugh 2021 feng et al 2019 o gorman and dwyer 2018 zhu et al 2019 given the limitations described in section 3 2 this section explores the potential for machine learning models to complement or even replace numerical models in hydro morphodynamic applications 4 1 numerical vs machine learning models the main reason for the establishment of the scientific method was the weaknesses of the human mind and the natural bias present when seeking an explanation of the physical processes montáns et al 2019 nonetheless these classical methods are still biased by the deductive thinking of the human mind this is where the importance of data driven approaches becomes apparent as they use implicit functions to learn from raw data in an unbiased fashion wei et al 2018 since iterative algorithms such as nonlinear monte carlo methods e et al 2020 sipin 2014 are usually used to solve such numerical models their computational complexity would considerably increase if too many model runs or iterations were required to reach the solution one attempt to solve such an issue is to replace the numerical solver by a machine learning based simulator known as surrogate modelling bass and bedient 2018 swischuk et al 2019 zhang et al 2020 before discussing surrogate modelling it is important to explain the differences between numerical models and machine learning models numerical models are considered as white box models which tend to solve physics equations such as navier stokes for hydro morphodynamic modelling whereas machine learning models are considered either grey or black models where they tend to learn the nonlinear relationship between the input data and output data introducing a nonlinear mapping in the learning algorithm numerical models often use a mesh to descretise the space domain and solve the physics equations on the points of the mesh whereas the machine learning models do not require any mesh as they take the input data directly furthermore numerical models require accurate parameter values and parameter calibration such as viscosity diffusivity friction and timestep values in order to produce accurate numerical solutions on the other hand machine learning models require hyperparameter tuning such as the learning rate for a neural network and the c and sigma hyperparameters for support vector machines in addition machine learning models require accurate data to train the model since it is essentially the main input else it would produce inaccurate results more importantly numerical models are time and computationally expensive due to parameter calibration and solving complex physics equation on a grid with potentially thousands of points this makes them very slow especially when doing long term simulations and could even reach numerical instabilities when they reach impossible physical solutions machine learning models conversely require much less time and computational resources as they rely on minimising the error between the predicted and actual outputs using optimisation algorithms the main danger in machine learning models is that they can produce physically impossible results in the application of surrogate modelling for numerical models as they do not take physical laws into consideration when training the model a recent machine learning model called physics informed neural networks raissi et al 2019 tends to constrain the solutions of the neural network by introducing the physics equations as an additional term in the loss function table 4 summarises the differences between the numerical models and machine learning models 4 2 data driven modelling in this section we discuss different machine learning models used and their respective applications in modelling the hydro morphodynamics of mangrove environments for modelling wave dissipation in mangrove environments malvin et al 2020 used a neural network with its training data obtained from a numerical model simulation the model was able to predict wave attenuation accurately when compared to the numerical simulation although a large number of training data was required for the neural network needing a large number of data in order to achieve good results is a common aspect for most machine learning models especially for neural networks this could be an issue when the data is coming from complex numerical solver simulations since it would need large computational time for the numerical model to generate enough simulation results remote sensing is a suitable alternative to numerical based simulation data as it does not require solving any physics equations in remote sensing data is collected from repositories such as landsat sentinel and global mangrove watch then machine learning models are trained on these input images after performing necessary pre processing and cleaning and would produce the required output according to the objective of the study remote sensing data from landsat was used to model mangrove risk against tropical cyclones and applied four different machine learning models including support vector machine random forest k nearest neighbour artificial neural networks and multiple linear regression to identify the best risk model for damage projection zhang et al 2019 the random forest model was best suited for the large scale application as it was accurate fast and it did not have many parameters nonetheless similar to malvin et al 2020 this approach required a large number of training data however obtaining more remote sensing data is much easier compared to generating more simulation data from a numerical model awty carroll et al 2019 used a continuous change detection and classification algorithm to investigate the damage extent and recovery of mangroves from cyclone sidr at the sundarbans from landsat satellite images the algorithm uses a per pixel model fitting approach wang et al 2018 to record the seasonal changes of land covered by the mangrove s inter year greening and browning trends liu et al 2020 despite achieving high accuracy results the algorithm was computationally very expensive for instance processing the whole region took around two weeks using 600 cores nevertheless using other machine learning algorithms such as artificial neural networks an approach taken by french et al 2017 showed accurate and rapid results when predicting tidal surge inundation at estuarine ports the authors of awty carroll et al 2019 tried tackling the issue of the computational time in another study in which a continuous monitoring of land disturbance algorithm was implemented awty carroll et al 2021 global mangrove watch data was used for training and landsat data was used for testing despite reporting an improvement in algorithm efficiency there was a trade off in accuracy where less than 50 accuracy was achieved at a study site maina et al 2021 used random forest regressor to investigate the threats facing mangroves the study used 19 years of satellite data and concluded that catchment erosion human pressure sea level are main drivers of current mangrove extents the study also concluded that east africa is particularly vulnerable to local and global threats and recommended adaptive management over time and space it could have been interesting to perform sensitivity analysis to the drivers to quantify their ability in affecting mangrove environments gouvêa et al 2022 used boosted regression trees to predict future mangrove extents by considering contrasting representative concentration pathway scenarios results showed that there is a possible 20 mangrove extent decline especially at the tropical atlantic and eastern pacific and the western and eastern indo pacific regions finally abdullah and barua 2022 used an artificial neural network with multilayer perceptron to monitor and predict vegetation vulnerability including mangrove environments in bangladesh results from the study showed that the modelled region could lose a large amount of its area in the next decade based on the current trends detected from satellite images the results however could have been more accurate if a more complex neural network model such as convolutional and recurrent neural networks were used as seen from the discussed works machine learning has a great potential in modelling hydro morphodynamics in mangrove environments and even expand their application towards predicting future dynamics based on different climate change conditions 4 3 uncertainty quantification the developed surrogate model or meta model emulates computer model simulations as a surrogate for new predictions while compensating for its bias relative to field measured data tang et al 2020 these predictions will ideally offer a full accounting of uncertainty for the estimated variables the proximity between predictions and observations can be characterised using a rational process known as uncertainty quantification uq zhu and zabaras 2018 this can be viewed as the process of determining appropriate uncertainties associated with model based predictions estimated by the surrogate models uncertainty in machine learning model predictions arises from a variety of sources including 1 epistemic uncertainty due to lack of enough training data 2 aleatoric uncertainty which is caused by the noise in the data 3 model discrepancy or inadequacy due to the difference between the model and the true system and 4 solution and coding errors ghanem et al 2017 such uncertainties make surrogate modelling even more challenging and limits its interpretation and accuracy thus quantifying this uncertainty would be useful to help the user understand the differences between predictions estimated by the selected surrogate model and observations most uq studies focus on understanding distributions of outputs or observables from a process as a function of uncertain or random inputs daneshkhah and bedford 2013 daneshkhah et al 2017 gramacy 2020 the uncertainty propagation up constitutes most applications the key element of up is to learn how outputs are affected by inputs when layers of fitted models are used e g gaussian processes see daneshkhah et al 2020 and donnelly et al 2022 for surrogates and additionally as models of discrepancy in the uq and calibration context probabilistic data driven modelling has the potential to provide promising solutions to the discussed challenges including reduction of computational cost and uncertainty quantification recently kernel based algorithms such as the gaussian process regression gpr have been increasing in popularity especially for retrieval applications camps valls et al 2016 generally the gpr algorithm relies on the probabilistic tackling of regression problems which leads to an analytical expression of the predictive uncertainty provided along with final estimates of functional traits berger et al 2021 thus along with the higher accuracy provided by such models the gpr is becoming important for solving earth observation regression problems such as the assessing the model s adaptability to different locations and times after quantifying the uncertainty of the model s parameterisation and input data estévez et al 2021 the main problem nonetheless with gpr is that inversion of the kernel matrix required to calculate the correlation between input variables is of the cubic order that depends on the sample size thus with increasing the size of data this method becomes intractable damianou and lawrence damianou and lawrence 2013 extended the ideas of the gp through adding several gp layers on top of each other to create a more flexible and effective model called deep gaussian process dgp this was mainly motivated by the concept of the deep neural networks that results in a several layers model each consisting of a gp the purpose of stacking several gp layers on top of each other is that they are able to produce high abstraction levels and complex correlations between the variables could be easily evaluated in terms of several smaller matrices the dgp aims at solving the issues that emerged from using the gp such as the matrix inversion using a bayesian nonlinear probabilistic and dimensionality reduction method called bayesian gaussian process latent variable model bgplvm that was introduced by titsias and lawrence 2010 in this case however up is no longer possible as it is computationally and mathematically intractable a potential alternative would be resorting to sampling techniques such as monte carlo havasi et al 2018 such models could be suitable surrogates for complex physical and numerical models as it is able to provide fast and accurate results whilst being able to quantify the uncertainty present in the problems 5 summary and outlook this article discussed the dangers that climate change impacts impose on mangrove environments and how the latter could adapt to such impacts furthermore different numerical modelling methods used to quantify climate change impacts on mangrove environments were discussed and evaluated in terms of their strengths and weaknesses finally alternative methods particularly machine learning models were evaluated in terms of their potential to complement or even replace numerical modelling approaches with respect to numerical modelling the choice of model included swan xbeach mike21 fm delft3d flow openfoam ihfoam among others these models are either 1d 2d or 3d and use either finite difference finite volume or finite element numerical schemes the numerical modelling approach can provide accurate results of the hydro morphodynamics of mangrove environments given a high resolution mesh and proper parameter settings are used however common issues faced with such models include solving non trivial physics equations which could lead to numerical instability and inaccuracy requiring high computational cost and suffering from long simulation times especially for large and complex regions machine learning models have gained significant attention especially in the climate modelling field due to their ability in providing fast and accurate surrogates to numerical models furthermore they could provide long term predictions of climate change effects on mangrove environments efficiently these models do not require solving physics equations nor having a high resolution mesh as they tend to learn the nonlinear relationships between the inputs and outputs directly from the trained data some challenges facing machine learning models include explainability as most of them are considered black box models which could lead to providing physically impossible outputs therefore such models require training on large number of datasets and remote sensing images could help mitigate this issue especially for deep neural networks in addition quantifying uncertainty could provide an insight on the model s confidence in providing predictions on unseen data and probabilistic machine learning models or sampling methods could address this issue declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to thank coventry university united kingdom for funding this phd studentship titled enhancing mangrove forest resilience against coastal degradation and climate change impacts using advanced bayesian machine learning methods through the gcrf scheme 
25425,the impacts of climate change especially sea level rise are an increasing threat to the world s coastal regions following recommendations made by the united nations about the preservation of mangrove environments particularly given their potential for effective natural defence against wave driven hazards a series of experiments have been conducted to quantify the ability of mangroves to counter climate change impacts to date these experiments have been limited by computational cost and inability to model multiple scenarios with improved data quality and availability machine learning has enormous potential to supplement or even replace existing numerical methods this article presents both an outline of the importance of protecting mangrove environments and a review of methods currently used to quantify the capacity of mangroves to adapt to climate change impacts in view of the limitations of existing numerical methods the article also discusses the potential of machine learning as an efficient and effective alternative keywords mangrove environments climate change hydro morphodynamic modelling adaptation policies machine learning data driven modelling data availability no data was used for the research described in the article 1 introduction climate change is a global threat due its influence on weather and climate phenomena and their subsequent impacts in all parts of the world changes in the frequency and magnitude of temperature extremes tropical cyclones storm surges and episodes of land erosion present enormous societal and environmental challenges disproportionately among some of the world s most vulnerable populations in coastal regions of the global south the most recent 6th assessment report ar6 of the intergovernmental panel on climate change ipcc revealed an unequivocal increase in ocean temperature and sea level alongside a decrease in sea ice extent since the beginning of the 20th century ipcc 2021a failure to manage the effects of climate change and slow the progress of global warming is likely to result in a 1 5 c temperature increase by 2040 and a 3 c increase by 2100 relative to pre industrial levels ipcc 2021b it is therefore of critical importance to find solutions for the mitigation of climate change impacts with the potential to be implemented globally one such effort which was endorsed by the climate change and land report of the ipcc samset et al 2020 and the global adaptation commission report estoque et al 2020 is nature based solutions pe er et al 2020 unlike engineering based alternatives such an approach has the potential to tackle climate mitigation and adaptation challenges in a more affordable way while benefiting society and nature seddon et al 2020 the world s forests have an important role to play in climate change mitigation and adaptation it is vital to observe how forests are responding to the effects of a warming climate alongside the stressors of deforestation and land use change in order to identify possible solutions and to quantify the extent to which they are appropriate to the issues faced several dynamic vegetation models anticipate that forest environments will be able to shift towards the areas with more suitable climate conditions which could result in forest expansions kirilenko and sedjo 2007 however there is a concern that this migration will be outpaced by climate and societal disturbances thus leading to significant losses of natural forests kirilenko and sedjo 2007 the lag of inland migration behind climate change effects is in part caused by the lack of connectivity to nearby habitats in addition to the presence of microclimates that differ from those in the surrounding areas and which might not be suitable for the present vegetation species pecl et al 2017 in addition there are human driven factors e g land development overexploitation species translocations and introductions and pollution that also affect the ability of such forests to migrate for example in a study by réjou méchain et al 2021 which investigated rainforest change in africa it was suggested that the areas which will be heavily affected by climate change are those vulnerable to human induced pressures such as the forests in coastal gabon the democratic republic of the congo and atlantic forests the world s mangrove environments cover 15 million ha and provide a unique habitat for many species in addition to goods and services to humans carugati et al 2018 mangroves are amongst the most productive marine ecosystems with an economic value ranging from us 3 624 98 us 26 734 61 per ha per year lee et al 2014 rizal et al 2018 however global mangrove habitats are declining at an alarming rate of 1 2 per year with 20 35 of global mangrove extent lost over the last 50 years due to deforestation and land exploitation in addition to sea level rise and erosion alongi 2015 goldberg et al 2020 according to the latest ipcc climate projections to the end of the twenty first century mangrove environments are expected to exhibit vulnerabilities in different parts of the world for example mangroves present in australia pakistan the arabian peninsula and both coasts of mexico are expected to decline as salinity increases freshwater becomes more scarce and critical temperature thresholds are reached more frequently alongi 2015 in the caribbean islands mangrove environments will decline as sea level rises and there is little or no upland space to colonise in new zealand usa and china mangrove environments will continue to expand their latitudinal range as temperature and atmospheric co 2 concentrations increase nevertheless these predictions are based on the assumptions that the decline or expansion of mangroves is related to complex changes in floral and faunal species composition morphology biodiversity biomass physiology growth and productivity alongi 2015 given the importance of having a plan to fight climate change such as forest planting restoration and protection it is imperative to have accurate assessments of the ability of these environments to mitigate climate change impacts and identify the suitable conditions for them to survive and grow there have been recent advances in the understanding of different climatic events and their effects on the natural ecosystems using both field studies and numerical modelling simulations alexander 2016 grant et al 2017 shan et al 2021 in particular the improved spatial resolution of global climate models and their capacity to be downscaled to regional scales has created new opportunities to understand how different climatic events affect the natural ecosystem functioning and the adaptation capabilities ummenhofer and meehl 2017 while higher spatial resolution is important there are several physical variables and model parameters that need to be known in order to achieve the optimal model skill lavidas and venugopal 2018 this issue becomes most apparent at the point of application to real world situations where since users are required to estimate or tune several parameters to deliver a customised model for a particular area of interest lavidas and venugopal 2018 machine learning methods are becoming increasingly recognised as powerful tools for technological progress in a variety of fields although machine learning has been widely applied in business and scientific disciplines including food identity discovery anxiety detection and speech detection delić et al 2019 erban et al 2019 rolnick et al 2019 sau and bhakta 2019 environmental application has not been fully in particular its use in assessing adaptation solutions in the face of a changing climate is rare milojevic dupont and creutzig 2021 nonetheless there are numerous applications for which machine learning has enormous potential including remote sensing smart and urban building design and transportation ladi et al 2022 avand et al 2021 fathi et al 2020 bhavsar et al 2017 perhaps most crucially the application of machine learning in climate change mitigation is particularly under explored and should be accelerated if it is to reach its potential milojevic dupont and creutzig 2021 with respect to mangrove environments only a small number of studies addressing climate change mitigation using machine learning pham et al 2019 goldberg et al 2020 maina et al 2021 with most applications limited to biomass estimation using satellite imagery jachowski et al 2013 zhu et al 2017 pham and brabyn 2017 hauser et al 2020 ghosh and behera 2021 maurya et al 2021 the aim of this article is three fold 1 to outline climate driven threats posed to mangrove environments to justify protective measure in view of their mitigatory capacity 2 to outline the latest developments and limitations in numerical modelling approaches taken to quantify climate change impacts on mangrove environments and 3 to outline the potential for machine learning methods both in terms of overcoming the limitations of numerical modelling and in understanding mangrove qualities and threats the article is structured as follows in section 2 we discuss the geographical distribution of mangrove environments in addition to the dangers facing such environments and possible adaptation strategies to help mangroves adapt to climate change effects in section 3 we explain how mangroves are able to attenuate waves and prevent erosion using different types of numerical models and experiments and emphasise on the limitations of these studies in section 4 we further discuss the limitations identified and provide possible solutions to overcome them using machine learning 2 mangrove environments the global state of play in addition to playing a vital role in conserving human sustainability and livelihoods in some of the world s poorest regions mangrove environments have ecological and economic importance lee et al 2014 mangrove environments produce oxygen dasgupta et al 2017 sequester carbon yando et al 2016 and provide breeding grounds and nursery sites for several terrestrial and marine organisms including commercial species and juvenile reef fish carugati et al 2018 most importantly and particularly so in the context of this article they provide great protection for populations of low elevation coastal zones from catastrophic events such as storm surges cyclones tsunamis and land erosion alongi 2015 there are about 70 different mangrove species spanning 19 distinct families and 28 genera with a range of different morphology and physiology characteristics duke et al 1998 global mangrove biogeography is characterised primarily by two locations which are the indo west pacific 54 mangrove species and the atlantic east pacific 17 mangrove species tomlinson 2016 with few shared species mangroves presence is largely limited to latitudes between 30 north and 30 south numbere 2018 the northern range roughly corresponds with japan 31 22 n and bermuda 32 20 n while the southern range extends to new zealand 38 03 s australia 38 45 s and on the east coast of south africa 32 59 s spalding 2010 nowhere is the vulnerability and value of mangrove environments more evident than across the sundarbans a unesco world heritage site that constitutes the world s largest single block of mangrove forest payo et al 2016 it lies on the boundary between the ganges brahmaputra delta and the bay of bengal das et al 2012 the forest consists of about 200 islands separated by about 400 interconnected tidal rivers creeks and canals roy et al 2016 fig 1 shows the extent of the sundarbans mangrove forest this region is known for frequent cyclonic storms and tidal bores that originate from the bay of bengal parvathy et al 2017 the most abundant mangrove species are e agallocha 59 69 of total mangroves followed by h fomes 30 89 c decandra 6 12 and x mekongensis 0 82 2 1 threats facing mangrove environments an estimated 64 of the world s mangrove environments are within 25 km of large urban centers and are threatened by future urban development marois and mitsch 2015 the threat of deforestation and degradation becomes apparent in low income regions who rely heavily on such environments for building material charcoal and non timber forest products lau and scales 2016 between 2000 and 2016 human factors contributed to more than 62 of mangrove losses goldberg et al 2020 however a decrease in the human related losses in recent years caused an increase in the contribution of climatic losses in fact the natural causes have become the dominant factor for the total mangrove losses compared to the human driven actions merzdorf et al 2020 fig 2 shows the geographical distribution of mangrove species and the causes and extent of mangrove loss between human driven and natural factors with respect to the effect of climate change on nature it is shown that the mean sea level has risen steadily due to the contribution of glaciers and ice sheets oppenheimer and glavovic 2019 tide gauge and altimetry observations show that mean sea level has increased from 1 4 mm yr 1 over the period 1901 1990 to 2 1 mm yr 1 over the period 1970 2015 to 3 2 mm yr 1 over the period 1993 2015 to 3 6 mm yr 1 over the period 2006 2015 oppenheimer and glavovic 2019 chapter 24 of the ipcc fifth assessment report ar5 in the working group ii wgii concluded that the mean sea level rise would continue at an accelerating rate and in return contribute to extreme coastal high water levels hijoka et al 2014 moreover the combination of sea level rise and cyclone intensification would contribute to increases in coastal flooding in addition to damages to mangrove environments and coral reefs which would aggravate wave damage in the regions lying beyond the natural defences hijoka et al 2014 besides flooding this would also lead to an increase in land erosion and saltwater intrusion hijoka et al 2014 consequently the increase in sea water level along with the intensification of cyclones would pose a key issue for coastal areas in particular those experiencing high cyclone frequency or intensity gedan et al 2011 villanoy et al 2012 since sea level rise slr is considered the most important threat to mangrove environments ghosh et al 2016 used geographical information system gis to predict the impact of slr on the mangrove under three scenarios related to slr which are 0 46 m low 0 75 m medium and 1 48 m extreme at the bangladesh sundarbans the results of the study show that the two dominant mangrove species h fomes and e agallocha will be inundated by 0 41 1 29 16 73 and 0 72 1 94 14 88 for low medium and extreme slr scenarios respectively under 2 4 mm year net subsidence rate ghosh et al 2016 with respect to the subsidence at the sundarbans it was noticed that the farakka dam in india has blocked the sediment transport from the ganges to the sea which resulted in a significant decline of mangroves particularly in the central and eastern sectors of the sundarbans godoy and de lacerda 2015 in addition to inundation the mangroves distribution would also change due to salinity intrusion more specifically in response to the increasing salinity some salt loving species such as c decandra would have the opportunity to expand into the non salt tolerant species such as h fomes sudhir et al 2022 similar results were found by dasgupta et al 2017 where there are considerable losses for heritiera fomes significant gains for excoecaria agallocha small changes for avicennia alba a marina a officinalis ceriops decandra and sonneratia apetala and mixed outcomes for species combinations according to the estimates 2 2 potential for mangrove environments to mitigate climate change impacts despite the challenges that mangrove environments face with respect to slr and salinity intrusion they are considered as effective natural defences against climate change chow 2017 dasgupta et al 2019 salgado and martinez 2017 this is especially true in wave attenuation where mangroves are able to reduce wave height by 35 whilst providing a cost effective option for restoration projects in comparison with other coastal habitats such as salt marshes coral reefs and seagrass narayan et al 2016 in addition to attenuating waves due to slr mangroves are also able to reduce the damage from extreme climatic events such as tsunamis and tropical cyclones as they act as the first line of natural defence against such events amira kamil et al 2021 krauss and osland 2020 furthermore mangrove environments are great at providing sediment stability and prevent erosion with their complex root structures marois and mitsch 2015 we conclude this section by discussing a case study in which mangrove environments have been utilised as a bio shield a restoration project at kuala selangor in malaysia was initiated with the motivation to use cheap and natural methods to protect the region however the project was reported as a failure after 70 of mangrove saplings died in the first eight months of the restoration project ghazali et al 2016 the main reason behind this failure was the incomplete knowledge of the hydrodynamic systems present at the site the study recommended the examining how the new mangroves resemble the blueprint of rehabilitation determine the efficiency of the restoration and discern the reduction of wave height and velocity due to the mangroves present ghazali et al 2016 according to this recommendation numerical models would enhance the knowledge of mangrove scientists by providing simulation results based on the proposed restoration plan schmitt et al 2013 flores de santiago et al 2017 to explain numerical modelling would simulate the new dynamics of the region based on the added mangrove trees the mangrove scientists would then compare those new dynamics with the previous one before adding the mangrove trees based on different climatic scenarios this would provide insight towards the success of the proposed restoration plan and thus increase the project success rate mangrove environments could be an important solution to fight climate change effects such as sea level rise sediment erosion and cyclones the advantage of such environments is their low cost to build in comparison to hard protection and their effectiveness in reducing wave heights and preventing erosion due to their complex root structure this stresses the need to protect mangrove environments and to focus on restoring such environments in damaged areas numerical modelling could play a crucial role in determining the success of such restoration projects as they could determine the efficiency of these projects from the simulation results as a result the next section discusses different modelling approaches used to model mangrove environments 3 hydro morphodynamic modelling in this section we discuss works that have focused on quantitatively measuring the capacity of mangrove environments to adapt to climate change effects such as sea level rise and erosion historically work in the field of hydrodynamics and measuring sediment transport for mangrove environments has fallen into two categories flume experiments or lab based experiments where the mangroves are substituted by rods and in situ or field based experiments where field observations are collected using different techniques buldakov et al 2017 maza et al 2017 2019 in recent years with the rise in commercially available design and simulation software numerical modelling which uses computer based simulations has emerged as a third approach nguyen et al 2015 sharafati et al 2019 the latter introduced the ability to expand the modelling to different configurations thus optimising the design and parameter values of the problem at hand deal et al 2018 heyrani et al 2021 3 1 flume and in situ experiments the first flume experiments were established by grove karl gilbert in geomorphology and designed specifically to study the mechanics of fluvial systems and sediment transport sherman et al 2013 other applications used flume experiments to measure the velocity of water through pools filled with gravel zimmermann et al 2020 in terms of vegetation such experiments were used to determine the ability of the former to capture sediments kretz et al 2021 with respect to mangrove environments flume experiments were conducted to determine their ability to attenuate waves abdullah et al 2019 on the other hand in situ experiments have been used to collect data at the site of the region of interest for several purposes such as to analyse the groundwater changes śliwińska et al 2019 compare it with data from numerical models ahmmed et al 2021 miramontes et al 2019 and detect the presence of dangerous bacteria teta et al 2017 in terms of mangrove environments such experiments have been conducted for a variety of reasons including measuring soil carbon storage enhancement xiong et al 2018 wastewater bioremediation theuerkauff et al 2020 and change in sediment distribution due to mangroves presence kamal et al 2017 important results derived from flume experiments about the dynamics of mangroves showed that the factors affecting these abilities were hydrodynamic such as the wave nature and depth of water botanic that constitutes the type of mangrove tree present and its density and the geological factors that affect the nature of the ground rasmeemasmuang and sasaki 2015 for mangroves specifically the distribution density of the model maza et al 2017 in addition to the root and trunk density wang et al 2022 play a critical role in the mangroves ability to reduce wave heights and velocities in addition to wave attenuation the sediment trapping capacity of mangroves has received extensive attention in recent decades this is in part due to the anticipated impacts of accelerated sea level rise as a result of global warming which would result in the increased erosion chen et al 2018 understanding the trapping mechanism in which mangroves trap sediments with their complex root structure is also important in engineering reconstruction projects li et al 2019b generally the factors affecting the trapping capacity are similar to those discussed for the wave attenuation which are mangrove density biomass as well as the inter tidal position determining submergence emergence status permatasari et al 2018 suwa et al 2021 experiments on this subject showed that the root porosity of mangroves plays a critical role in determining the erosion level and the capacity of mangroves to adapt to higher velocities kazemi et al 2021 furthermore the height of mangroves can substantially decrease the turbulence intensity and therefore reduce the erosion potential horstman et al 2018 with respect to in situ observations mangroves showed desired sediment trapping abilities with a 20 decrease in the suspended sediment concentration compared with bare mudflat chen et al 2018 a similar conclusion was reached by permatasari et al 2018 who studied the trapping capacity of mangrove environments at the lamreh estuary where a negative correlation was deduced between mangrove density and suspended sediment transport although the presented results are encouraging flume experiment are faced with reproducibility issues as the setup needs to be changed for different experiments and results cannot be extrapolated to other conditions another issue is that those laboratory experiments are a simplification of real world scenarios therefore conclusions obtained must be used with caution as there are several parameters such as true climate variabilities and land conditions that are not considered accurately represented during the experiments regarding in situ experiments they are limited by cost of equipment size of the area of interest and equipment availability 3 2 numerical modelling with respect to hydrodynamic and sediment transport applications numerical modelling has been extensively applied as it is faster and more practical in terms of repeating the simulation using different configurations compared with collecting data from flume experiments or field surveys to conduct the review of numerical models a compilation of modelling studies written in english over the last eight years 2015 2022 was performed the research was conducted on september 2022 in scopus and google scholar we searched for literature using the different combinations of the following boolean search strings for paper theme mangrove and numerical modelling or hydrodynamic or morphodynamic or hydro morphodynamic or 1d or 2d or 3d or navier stokes or shallow water equations or finite difference method or finite volume method or finite element method it is important to note that although there are other numerical models available the choice was restricted to those that modelled mangrove environments only among the reviewed numerical models they can be grouped according to two main criteria the solving dimension and the method starting with the dimensions numerical models can either be 1d 2d or 3d 1d models solve the saint venant equations and they are computationally inexpensive however such models cannot describe the full settings of the region as flow is assumed in one direction leading to inaccurate results as a result such models are restricted to 1d flow as in pipes or closed channels 2d models solve the shallow water equations and are much more used to model mangrove environments as they offer better flexibility and more applicability to model mangroves in more complex settings nevertheless they are less computationally efficient that 1d models and require better meshes judge et al 2018 finally 3d models solve the full navier stokes equations and can capture the full complexity of the region and application of mangroves this can be become very useful when investigating small effects of mangroves such as mangrove seedlings on the hydrodynamics and sediment change le minor et al 2019 nonetheless it could become computationally and time expensive thus either limiting its application to small scale problems or requiring expensive hardware to model large regions table 1 summarises the advantages and limitations of each dimension with respect to the solving methods numerical models either use finite difference method fdm finite volume method fvm or finite element method fem the finite difference method is the most simple and suitable for 1d problems and provide high stability and computational efficiency however it is not accurate for solving on irregular domains and complex structures such as modelling real mangrove environments along a coast the finite volume method is most commonly used for solving problems with mangrove environments as it provide accurate solutions for such applications especially in 2d nonetheless this method is sensitive to time steps and complex domains would require a smaller time step thus reducing its time efficiency finally the finite element method provides high accuracy solutions and is best suitable for complex regions this is currently being used in next generation coastal ocean models kärnä et al 2018 solver stability remains an issue and a balance between accuracy and time efficiency is needed table 2 summarises the differences between the solving methods 3 2 1 1d models following discussion of the different dimensions and solver methods used in the numerical models we move to the different software used to model the hydro morphodynamics of mangrove environments along with some applications and limitations starting with the tsunami flow inversion from deposits tsuflind this one dimensional finite volume software was initially developed for tsunami applications tang and weiss 2015 however soria et al 2018 used it to quantify the ability of mangroves to act as a buffer against high velocity flows based on sand deposits as the haiyan typhoon it was concluded that mangroves in extreme cases were not suitable to prevent such flows from land inundation however the study assumed no bedload change and sediment deposition to be uniform which would have reduced the velocity on land roeber and bricker 2015 moving on mike11 is a one dimensional finite difference solver that is mainly used for simulating flow in estuaries rivers irrigation systems and channels gauthier et al 2017 it was used by van dat et al 2021 to study the effect of salt intrusion due to sea level rise on mangrove environments it was found that salt intrusion greatly affects mangrove distribution and thus should be considered when developing a governance strategy for mangrove ecosystems nonetheless without the ability to model realistic complex flows in 1d it would be very difficult to quantify these effects accurately and would just be left with the general behaviour finally xbeach was used to model wave dampening by mangrove environments van rooijen et al 2015 in this study the authors used the 1d formulation of xbeach although it can also be used for 2d problems the study reported major underestimation of the calculated values when compared to laboratory data main reasons for such performance were attributed to lack of calibration data and the inability to determine proper values for the drag coefficient 3 2 2 2d models a major limitation of one dimensional models is that they cannot model real world situations accurately which limits the applicability of such models one solution to that is to use a 2d model as it could provide better applicability and flexibility the difference in wave attenuation among different mangrove species was studied by sinha et al 2019 the study used the simulating waves nearshore swan a 2d finite difference model to determine the effects of salinity on mangrove wave attenuation the results show that the mangroves present at the hyposaline have reduced the wave heights by 31 in november and 51 in july more than the wave attenuation achieved by the hypersaline estuary sinha et al 2019 no data however was used to validate the simulated results another study that modelled wave attenuation used simulating waves till shore swash 2d numerical model which also uses a finite difference numerical scheme phan et al 2019 although the simulated results agreed with the data the study used a laboratory experiment with a fixed drag coefficient this would hinder the interpretability of results to extend beyond the experiment i e for real world scenarios deb and ferreira 2018 used swan with advanced circulation adcirc to model wave and hydrodynamics respectively the model used also showed the mangrove s ability to attenuate storm surges but it was limited on validation data and with outdated historical data for the calculation of bottom roughness as a result this has lead to a large simulation error where the root mean squared error rmse was 0 3 m to overcome this limitation horstman et al 2015 used field data and numerical modelling to model the hydro morphodynamics of mangrove environments the study used delft3d flow to study the contributions of various biogeophysical mangrove settings to the observed tidal dynamics after gathering field data for calibration and verification delft3d flow modelled the hydrodynamics and change in sediment concentration simultaneously the study concluded that mangroves are greatly sensitive to climate change and anthropogenic threats which could decrease their ability to reduce wave velocities and its sediment trapping abilities the data limitation issue was also tackled by pelckmans et al 2021 through using remote sensing and geographical information systems gis techniques the study modelled wave propagation in mangroves using telemac 2d a finite volume solver although the error between measured and simulated data ranged between 0 2 7 8 the error increased to 4 4 8 73 during spring tides reaching about 0 39 m one possible issue that lead to such errors is using a relatively high friction value to model mangrove 0 14 representing a dense forest mcivor et al 2012 zhang et al 2019 used hyrdrosed2d a depth integrated 2d numerical model to study the attenuation of tsunami run up waves by mangroves hydrosed2d is a finite volume model that solves the nonlinear shallow water equations results showed that mangrove environments are able to attenuate tsunami run up waves and prevent land inundation nonetheless errors up to 10 were reported between measured and simulated data limiting the confidence of the quantified results continuing on similar climatic events mike21 fm was used to model mangrove effect on cyclones rahdarian and niksokhan 2017 mike21 fm is a 2d finite volume model which can calculate wind and wave induced currents to simulate storm surges results showed that the area with mangroves protected 90 km 2 of land from inundation and water levels were reduced between 0 11 1 25 m this implies the need to protect existing mangroves and plan restoration projects for areas with damaged or without mangrove environments as they play a critical role in reducing water inundation and protecting lives speaking of restoration projects srh 2d another 2d finite volume solver was used to asses floodplain restoration shih et al 2022 the results showed the ability of mangroves to migrate and expand their habitat extent using the spread of their propagules nevertheless the study used a propagation trajectory model which relies on a probability and has a randomness characteristic presenting a modelling uncertainty 3 2 3 3d models finally moving on to the more complex 3 dimensional models such models are able to capture more complex fluid features and their effect on mangroves such as turbulence wang et al 2020 ihfoam a 3d finite volume solver was used to model solitary wave attenuation by mangrove environments by solving the reynolds averaged navier stokes rans in addition instead of using a bulk drag coefficient the study investigated using time varying drag coefficient results showed that mangrove wave reduction abilities varied significantly depending on the density of vegetation as previously discussed the major limitation arises from the needed to have very fine meshes to simulate such accurate flow dynamics which consequently would increase the computational cost significantly thus limiting the usages of 3d modelling to numerical flume experiments would help alleviate such complexities wang et al 2020 this was also done in le minor et al 2019 who used openfoam a finite volume solver to model the hydraulics and sediment dynamics of mangrove seedlings the study used a cm scale flume experiment and it showed that the seedling ability is heavily affected by turbulences and flow magnitudes however extrapolating the results to large scale real applications is difficult as the steady did not incorporate wave forcing and a dynamic mesh to mimic real waves furthermore adding such parameters would require different model settings increasing the computational cost even further a numerical study of climate change impacts namely sea level rise on mangrove distribution was conducted by luo and chui 2022 the study used mantra a 3d finite element model to account for spatio temporal variations in water dynamics and vegetation change by solving water and mass balances at daily timescales in their study the authors modelled a large scale region in southern china unlike the previous studies which used flume experiments the study also modelled the dynamics under different climate change scenarios for the next 120 years results show that under low and moderate sea level rise scenarios mangroves are able to adapt and even increase however such conclusions are based on the semi enclosed bay site and cannot be used in other regions such as the open sea for protection or restoration projects moreover the study assumed that soil accretion would keep up with sea level rise rate which might not be realistic under a moderate and high scenarios 3 2 4 summary table 3 summarises the reviewed models and their respective applications the main limitations which can be drawn from this review are lack of calibration and validation data simplifying assumptions and using small scale settings to prevent high computational costs such weaknesses have led to high errors and inability to extrapolate or generalise conclusions from the derived simulation results furthermore numerical studies are also quite challenging since it is difficult to model the complex nature of the waves mangroves and even the topographical features of the region of study which could cause inaccurate flow and sediment dynamics guan et al 2018 3 3 model data integration as discussed in section 3 1 there exist several challenges in modelling hydro morphodynamic applications using flume and in situ experiments including the site location and complex mangrove structure this makes the deployment of sensing equipment difficult and could potentially result in inaccurate results where field measurements are critically affected by the location of the device on the other hand due to the complexity of the numerical models equations and lack of calibration and validation data as discussed in section 3 2 the simulated results would suffer from significant errors as a result a combination of in situ and numerical methods as a solution to overcome the discussed limitations of each method is suggested le minor et al 2021 the proposed approach was to employ process based numerical simulations as a complementary tool to in situ and flume measurements experimental data would then be needed to assess the built model s performance under conditions similar to those observed in real mangrove environments in addition in situ measurements would be used to calibrate the model and validate the results of the numerical model this is defined as model data integration which is the integrated and balanced use of physical and numerical models vyzikas et al 2014 this was specifically implemented by cannon et al 2020 who aimed at assessing the hydrodynamic thresholds for the persistence of mangrove environments against waves and tidal inundations it was motivated by the need to consider hydrodynamic factors in mangrove restoration projects and to evaluate wave thresholds for use in site selection which would significantly improve the likelihood of successful mangrove recruitment and retention the study used a variety of data sources to measure shoreline structures and vegetation composition and swan was used to model wave heights the derived results suggested that mangrove survival was significantly influenced by the wave climate at all recurrence rates cannon et al 2020 combining collected data and the numerical model lead to accurate results when calculating the critical wave threshold with just 3 8 error a similar conclusion was also reached in studying the thresholds of mangrove survival with rapid increase in sea level using a similar approach saintilan et al 2020 nonetheless using a logistic regression to understand the probability of mangrove survival might not represent the relationship of the variables properly on the other hand an ensemble to decision trees known as random forest could provide better predictions for mangrove survival and highlight the effect of each variable accurately maina et al 2021 in addition to studying the hydrodynamics modelling sediment change by using a data model integration approach is also as important to better investigate the effect of erosion and deposition due to climate change on mangrove environments woodroffe et al 2016 sidik et al 2021 a study by east et al 2020 aimed to model reef hydrodynamics and sediment transport under different sea level rise scenarios the authors used field surveys and satellite imagery to characterise the range of substrate types hydrodynamic settings and ecological communities furthermore the data was used to run a two dimensional depth averaged wave model by using a green naghdi free surface solver from the open source model basilisk popinet 2015 the results of the study showed that inland migration is likely to occur under sea level rise the limitation in this study however was assuming the continuous sediment production by the carbonate producing organisms this could be affected by climate change impacts such as increases in ocean acidity and sea surface temperatures east et al 2020 4 research gaps and opportunities in addition to the limitations discussed in section 3 2 about some inaccuracies noticed in numerical models due to the inability of these models to capture and explain all of the complex physical phenomena another challenge of working with numerical models is their high computational cost to solve dynamic processes xing et al 2018 such challenges become apparent when solving the hydrodynamics bhola et al 2018 and sediment transport models ţene et al 2018 thus there is a trade off between a computationally expensive yet more accurate model solving the full navier stokes equations and a simplified depth averaged navier stokes known as nonlinear shallow water equations reducing the computational cost fent et al 2018 as discussed in section 1 the applications of machine learning in climate change related topics have not been explored to its potential however with the increased attention towards addressing climate change and development in the field of machine learning recent work has investigated the application of machine learning models to aid in adaptation policies biesbroek et al 2020 li et al 2019a in particular several research studies have been exploring the hydrological changes including extreme scenarios associated with climate change using machine learning methods ardabili et al 2019 das and nanduri 2018 davenport and diffenbaugh 2021 feng et al 2019 o gorman and dwyer 2018 zhu et al 2019 given the limitations described in section 3 2 this section explores the potential for machine learning models to complement or even replace numerical models in hydro morphodynamic applications 4 1 numerical vs machine learning models the main reason for the establishment of the scientific method was the weaknesses of the human mind and the natural bias present when seeking an explanation of the physical processes montáns et al 2019 nonetheless these classical methods are still biased by the deductive thinking of the human mind this is where the importance of data driven approaches becomes apparent as they use implicit functions to learn from raw data in an unbiased fashion wei et al 2018 since iterative algorithms such as nonlinear monte carlo methods e et al 2020 sipin 2014 are usually used to solve such numerical models their computational complexity would considerably increase if too many model runs or iterations were required to reach the solution one attempt to solve such an issue is to replace the numerical solver by a machine learning based simulator known as surrogate modelling bass and bedient 2018 swischuk et al 2019 zhang et al 2020 before discussing surrogate modelling it is important to explain the differences between numerical models and machine learning models numerical models are considered as white box models which tend to solve physics equations such as navier stokes for hydro morphodynamic modelling whereas machine learning models are considered either grey or black models where they tend to learn the nonlinear relationship between the input data and output data introducing a nonlinear mapping in the learning algorithm numerical models often use a mesh to descretise the space domain and solve the physics equations on the points of the mesh whereas the machine learning models do not require any mesh as they take the input data directly furthermore numerical models require accurate parameter values and parameter calibration such as viscosity diffusivity friction and timestep values in order to produce accurate numerical solutions on the other hand machine learning models require hyperparameter tuning such as the learning rate for a neural network and the c and sigma hyperparameters for support vector machines in addition machine learning models require accurate data to train the model since it is essentially the main input else it would produce inaccurate results more importantly numerical models are time and computationally expensive due to parameter calibration and solving complex physics equation on a grid with potentially thousands of points this makes them very slow especially when doing long term simulations and could even reach numerical instabilities when they reach impossible physical solutions machine learning models conversely require much less time and computational resources as they rely on minimising the error between the predicted and actual outputs using optimisation algorithms the main danger in machine learning models is that they can produce physically impossible results in the application of surrogate modelling for numerical models as they do not take physical laws into consideration when training the model a recent machine learning model called physics informed neural networks raissi et al 2019 tends to constrain the solutions of the neural network by introducing the physics equations as an additional term in the loss function table 4 summarises the differences between the numerical models and machine learning models 4 2 data driven modelling in this section we discuss different machine learning models used and their respective applications in modelling the hydro morphodynamics of mangrove environments for modelling wave dissipation in mangrove environments malvin et al 2020 used a neural network with its training data obtained from a numerical model simulation the model was able to predict wave attenuation accurately when compared to the numerical simulation although a large number of training data was required for the neural network needing a large number of data in order to achieve good results is a common aspect for most machine learning models especially for neural networks this could be an issue when the data is coming from complex numerical solver simulations since it would need large computational time for the numerical model to generate enough simulation results remote sensing is a suitable alternative to numerical based simulation data as it does not require solving any physics equations in remote sensing data is collected from repositories such as landsat sentinel and global mangrove watch then machine learning models are trained on these input images after performing necessary pre processing and cleaning and would produce the required output according to the objective of the study remote sensing data from landsat was used to model mangrove risk against tropical cyclones and applied four different machine learning models including support vector machine random forest k nearest neighbour artificial neural networks and multiple linear regression to identify the best risk model for damage projection zhang et al 2019 the random forest model was best suited for the large scale application as it was accurate fast and it did not have many parameters nonetheless similar to malvin et al 2020 this approach required a large number of training data however obtaining more remote sensing data is much easier compared to generating more simulation data from a numerical model awty carroll et al 2019 used a continuous change detection and classification algorithm to investigate the damage extent and recovery of mangroves from cyclone sidr at the sundarbans from landsat satellite images the algorithm uses a per pixel model fitting approach wang et al 2018 to record the seasonal changes of land covered by the mangrove s inter year greening and browning trends liu et al 2020 despite achieving high accuracy results the algorithm was computationally very expensive for instance processing the whole region took around two weeks using 600 cores nevertheless using other machine learning algorithms such as artificial neural networks an approach taken by french et al 2017 showed accurate and rapid results when predicting tidal surge inundation at estuarine ports the authors of awty carroll et al 2019 tried tackling the issue of the computational time in another study in which a continuous monitoring of land disturbance algorithm was implemented awty carroll et al 2021 global mangrove watch data was used for training and landsat data was used for testing despite reporting an improvement in algorithm efficiency there was a trade off in accuracy where less than 50 accuracy was achieved at a study site maina et al 2021 used random forest regressor to investigate the threats facing mangroves the study used 19 years of satellite data and concluded that catchment erosion human pressure sea level are main drivers of current mangrove extents the study also concluded that east africa is particularly vulnerable to local and global threats and recommended adaptive management over time and space it could have been interesting to perform sensitivity analysis to the drivers to quantify their ability in affecting mangrove environments gouvêa et al 2022 used boosted regression trees to predict future mangrove extents by considering contrasting representative concentration pathway scenarios results showed that there is a possible 20 mangrove extent decline especially at the tropical atlantic and eastern pacific and the western and eastern indo pacific regions finally abdullah and barua 2022 used an artificial neural network with multilayer perceptron to monitor and predict vegetation vulnerability including mangrove environments in bangladesh results from the study showed that the modelled region could lose a large amount of its area in the next decade based on the current trends detected from satellite images the results however could have been more accurate if a more complex neural network model such as convolutional and recurrent neural networks were used as seen from the discussed works machine learning has a great potential in modelling hydro morphodynamics in mangrove environments and even expand their application towards predicting future dynamics based on different climate change conditions 4 3 uncertainty quantification the developed surrogate model or meta model emulates computer model simulations as a surrogate for new predictions while compensating for its bias relative to field measured data tang et al 2020 these predictions will ideally offer a full accounting of uncertainty for the estimated variables the proximity between predictions and observations can be characterised using a rational process known as uncertainty quantification uq zhu and zabaras 2018 this can be viewed as the process of determining appropriate uncertainties associated with model based predictions estimated by the surrogate models uncertainty in machine learning model predictions arises from a variety of sources including 1 epistemic uncertainty due to lack of enough training data 2 aleatoric uncertainty which is caused by the noise in the data 3 model discrepancy or inadequacy due to the difference between the model and the true system and 4 solution and coding errors ghanem et al 2017 such uncertainties make surrogate modelling even more challenging and limits its interpretation and accuracy thus quantifying this uncertainty would be useful to help the user understand the differences between predictions estimated by the selected surrogate model and observations most uq studies focus on understanding distributions of outputs or observables from a process as a function of uncertain or random inputs daneshkhah and bedford 2013 daneshkhah et al 2017 gramacy 2020 the uncertainty propagation up constitutes most applications the key element of up is to learn how outputs are affected by inputs when layers of fitted models are used e g gaussian processes see daneshkhah et al 2020 and donnelly et al 2022 for surrogates and additionally as models of discrepancy in the uq and calibration context probabilistic data driven modelling has the potential to provide promising solutions to the discussed challenges including reduction of computational cost and uncertainty quantification recently kernel based algorithms such as the gaussian process regression gpr have been increasing in popularity especially for retrieval applications camps valls et al 2016 generally the gpr algorithm relies on the probabilistic tackling of regression problems which leads to an analytical expression of the predictive uncertainty provided along with final estimates of functional traits berger et al 2021 thus along with the higher accuracy provided by such models the gpr is becoming important for solving earth observation regression problems such as the assessing the model s adaptability to different locations and times after quantifying the uncertainty of the model s parameterisation and input data estévez et al 2021 the main problem nonetheless with gpr is that inversion of the kernel matrix required to calculate the correlation between input variables is of the cubic order that depends on the sample size thus with increasing the size of data this method becomes intractable damianou and lawrence damianou and lawrence 2013 extended the ideas of the gp through adding several gp layers on top of each other to create a more flexible and effective model called deep gaussian process dgp this was mainly motivated by the concept of the deep neural networks that results in a several layers model each consisting of a gp the purpose of stacking several gp layers on top of each other is that they are able to produce high abstraction levels and complex correlations between the variables could be easily evaluated in terms of several smaller matrices the dgp aims at solving the issues that emerged from using the gp such as the matrix inversion using a bayesian nonlinear probabilistic and dimensionality reduction method called bayesian gaussian process latent variable model bgplvm that was introduced by titsias and lawrence 2010 in this case however up is no longer possible as it is computationally and mathematically intractable a potential alternative would be resorting to sampling techniques such as monte carlo havasi et al 2018 such models could be suitable surrogates for complex physical and numerical models as it is able to provide fast and accurate results whilst being able to quantify the uncertainty present in the problems 5 summary and outlook this article discussed the dangers that climate change impacts impose on mangrove environments and how the latter could adapt to such impacts furthermore different numerical modelling methods used to quantify climate change impacts on mangrove environments were discussed and evaluated in terms of their strengths and weaknesses finally alternative methods particularly machine learning models were evaluated in terms of their potential to complement or even replace numerical modelling approaches with respect to numerical modelling the choice of model included swan xbeach mike21 fm delft3d flow openfoam ihfoam among others these models are either 1d 2d or 3d and use either finite difference finite volume or finite element numerical schemes the numerical modelling approach can provide accurate results of the hydro morphodynamics of mangrove environments given a high resolution mesh and proper parameter settings are used however common issues faced with such models include solving non trivial physics equations which could lead to numerical instability and inaccuracy requiring high computational cost and suffering from long simulation times especially for large and complex regions machine learning models have gained significant attention especially in the climate modelling field due to their ability in providing fast and accurate surrogates to numerical models furthermore they could provide long term predictions of climate change effects on mangrove environments efficiently these models do not require solving physics equations nor having a high resolution mesh as they tend to learn the nonlinear relationships between the inputs and outputs directly from the trained data some challenges facing machine learning models include explainability as most of them are considered black box models which could lead to providing physically impossible outputs therefore such models require training on large number of datasets and remote sensing images could help mitigate this issue especially for deep neural networks in addition quantifying uncertainty could provide an insight on the model s confidence in providing predictions on unseen data and probabilistic machine learning models or sampling methods could address this issue declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to thank coventry university united kingdom for funding this phd studentship titled enhancing mangrove forest resilience against coastal degradation and climate change impacts using advanced bayesian machine learning methods through the gcrf scheme 
25426,green infrastructure gi is cost effective for managing urban runoff however inspection and maintenance of gi are an increasingly common burden for stormwater managers for instance bioretention cells a popular type of gi may clog sometimes unexpectedly and detection can be challenging due to their dispersed placement current inspection programs nationwide largely rely on time intensive manual qualitative inspections this study develops an approach for real time monitoring and prediction of column performance first we conduct laboratory experiments to continuously collect soil moisture data using sensors at two different depths in bioretention column testbeds four design configurations are used that allow the water to drain differently through the column hence acting as different environmental climates next we develop machine learning models i e long short term memory lstm models to accurately predict current and future soil moisture levels our results suggest that the quality of predictions is overall high but they vary across the configurations keywords machine learning sensor data time series green infrastructure bioretention columns real time monitoring data availability data will be made available on request 1 introduction across the united states urban stormwater causes surface water degradation leading watershed managers to turn to green infrastructure gi as a way to clean runoff and bring more natural hydrologic regimes to streams gi improves existing or builds new infrastructure adding resiliency to aged systems and helping manage extreme weather caused by climate change research has shown that besides climate change benefits gi provides many health benefits for residents by improving not only stormwater quality and flood defense but also wildlife habitat urban heat island mitigation and public green space kaluarachchi 2020 gi is considered a distributed management technique whereby interventions are placed throughout the watershed to provide local scale improvements one such gi approach that has been increasingly utilized over the past decade is bioretention there are many design variations of bioretention that are used bioretention has been shown to reduce the quantity and improve the quality of urban runoff by utilizing natural processes despite these benefits bioretention cells are prone to clogging from anthropogenic factors such as debris accumulation within the contributing watershed plant die off and seasonal weather changes benedict et al 2002 additionally these environmental variable vary from site to site thus there are ongoing maintenance and inspection needs for these systems to ensure their functionality gi inspection maintenance has increasingly received attention as a major component of many stormwater management programs current strategies for these programs heavily rely on manual inspection and maintenance with largely qualitative approaches this entails performing hands on inspections on each gi installation one by one across the entire city benedict et al 2002 angelstam et al 2017 the distributed approach to gi implementation noted above complicates this task and increases the financial burden ryan 2019 as an example metro water services in nashville tn has multiple staff solely dedicated to this task any changes in a given practice function between these inspections are difficult to anticipate or even identify recently interest has grown for real time environmental monitoring in urban watersheds such activities may better quantify the performance of gi and allow more quantitative assessments as to when maintenance is needed further by minimizing hands on labor required by staff and saving maintenance time within the system the stormwater management program can be more efficient and effective as an example soil moisture data can be used to understand the timing and pattern of runoff movement through the soil profile tracking changes in these patterns over time can aid in understanding how system function is shifting changes in soil moisture patterns that include slower time to peak and a recession with a flatter slope may indicate that clogging of the system is causing runoff to infiltrate more slowly and thus the need for maintenance these observations can be relayed back to inspectors to trigger additional actions potentially complicating this approach is the wide array of gi designs utilized in practice for bioretention alone design variations include 1 traditional which are allowed to freely drain at the bottom of the soil profile 2 internal water storage where water ponds in the bottom of the cell before filling to the level of the drain if a given storm is large enough and 3 real time control a new design approach that allows drainage to be instigated or stopped based on a set of control rules real time control has recently become of interest due to the adaptability that it affords kerkez et al 2016 studies such as persaud et al 2019 have shown that real time control can instigate favorable conditions for biogeochemical processes to optimize runoff treatment however these systems may not have soil moisture processes as regimented as standard designs as opening and closing of the valve can influence infiltration processes in addition to the onset of rainfall runoff technological advancements now allow access to low cost sensors which can be installed within each gi structure allowing for real time monitoring further these real time monitoring systems are increasingly becoming user friendly tools paving the way for use in real world applications glasgow et al 2004 feuer 1995 a study by jim gao 2014 showed that rtrm enables many opportunities for system improvements in this study neural networks nn learn the relationships from the actual operations data to model its function the model is tested and validated using google s data center the results suggest that machine learning specifically nn can effectively and efficiently use existing sensor data to model data centers performance gao 2014 other studies test methods such as boolean logic based multi criteria analysis and suggest they are not as accurate in predicting future gi states they study artificial neural network ann and network based fuzzy inference system anfis to determine that ann is 72 percent accurate where anfis is only 65 percent accurate labib 2019 internet of things iot has allowed for many water industry advancements intelligent water is one of these advancements that is already operational in multiple cities south bend indiana and ann arbor michigan are examples within the united states of where such systems are implemented and show promise of improving the drainage system s function atzori et al 2010 globally there have been other countries that have shown promise of implementation of improved drainage systems as well for example melbourne australia which is a forest city is planning on implementing gi to help combat the worsening urban heat island effect uhi and could benefit from a useful gi monitoring tool fuentes et al 2021 although advancements are being made this is still a new field and many applications have yet to be explored for instance can gi managers be informed of maintenance and inspection needs using these new technology applications this application represents how innovative and valuable iot can be for the water industry bumblauskas et al 2017 as noted above the application of iot to the water industry has matured which makes successful field deployment possible low cost sensor nodes may allow maintenance programs and gi inspections more efficiency by measuring performance quantitatively and allowing year long assessments however determining what tools can be used to interpret large amounts of data while keeping an overall low cost has yet to be determined this study conducts a proof of concept investigation of how data from sensors installed in bioretention can be analyzed and interpreted the objective of this study is to 1 understand how the soil moisture in different bioretention columns relate to each other when under the same conditions 2 if the number of sensors used in a given installation can be optimized 3 predict the future state of the bioretention column using historical and future soil moisture and rainfall sensor data these objectives will build knowledge on using sensors to inform gi maintenance and pave the way for future research in this study we develop time series models using long short term memory lstm networks hochreiter and schmidhuber 1997 wojciech zaremba and vinyals 2014 in the literature various types of time series modeling exist this includes traditional statistical models such as autoregressive integrated moving average arima or more advanced nn based models such as lstm networks arima is simpler and generally works well when there is a clear trend in the time series however in many complex cases lstm has shown to outperform arima this has been shown in forecasting wind speed shivani et al 2019 web traffic shelatkar et al 2020 and financial data siami namini et al 2018 among others in this study we first perform some preliminary analysis to compare the performance of arima and lstm in our use case based on these results lstm outperforms arima partly because our data trends are rather complex as the data are collected from sensors capturing the soil moisture in response to precipitation at different depths hence we perform our main analysis using lstms to showcase the model performance we exploit a column study notably these columns represent a simplification of field conditions based upon more complex hydrologic processes that may vary spatially across a bioretention however even if patterns are slightly different in a field installation the methods herein are robust enough to be trained on those data further this work represents a proof of concept to see how well soil moisture patterns can be modeled and predicted at an individual location within a bioretention which is relatively well represented by the column this is highly valuable as it allows an assessment as to how patterns change over time due to perturbations such as clogging of soil media 2 data to carefully control test conditions data collection for this case study took place inside a greenhouse bioretention cell function is mimicked using pvc columns constructed with materials per typical design guidance specifically all columns use a 30 centimeter cm diameter column with a drain at the bottom biological fouling is a concern when building the columns so drains are frequently utilized and monitored column construction and operation included sanding the column s interior walls to minimize preferential flow lastly columns are filled by layers of 57 stone pea gravel sand bioretention media mulch and a ponding zone each column is planted in a climate controlled greenhouse of 15 27 c the columns are continuously monitored for soil moisture using two meter group s teros 10 sensors two sensors are placed within each column at depths of 30 and 60 cm from the top of the bioretention media the sensors then gather the water volume content namely soil moisture in units of volume volume v v every minute for 43 days anon 2020 this time period was selected because 1 the time period was not too long as to make the time consuming methodology of the column study impossible and 2 a reasonable number of rainfall events occurred during this period allowing a realistic data set of soil moisture patterns while more data are obviously better we believe this data set contained variable storm sizes and variable dry periods between events and thus does give a reasonable data set to test the machine learning approaches herein following data collection we pre process and cleanse the data by removing the missing data points after verifying that they are caused by sensor failure and or sensor anomalies to provide a robust analysis there are four types of column configurations representing four potential bioretention design configurations there are five columns of each configuration for a total of 20 columns fig 1 conveys how each configuration is constructed free draining fd the drain on the bottom of the column is completely unobstructed i e drains via gravity internal water storage iws an upturned elbow in the pipe is attached to the drain on the bottom which creates a 45 cm submerged zone at the bottom of the column soil moisture sm uses a remote level controller with real time monitoring of water depth to control the soil moisture level at a depth of 30 cm to maintain field capacity volume control vc actively managed remotely in real time by opening and closing the valves within the column to maintain a storage depth of 30 cm from the bottom of the bioretention media as measured by a pressure transducer to create artificial stormwater for use in the study tap water was supplemented with chemicals to achieve typical stormwater runoff concentrations per bratieres et al 2008 sediment collected from a local detention pond was sieved and added to the mixture the artificial stormwater was continuously mixed while being added to the columns to ensure even distribution of constituents among the columns bratieres et al 2008a the detailed breakdown of the chemical concentrations is provided in table 7 in the appendix application of the stormwater is performed to mimic the size and frequency of real historical rainfall events across a roughly six week period july 31 2019 to september 1 2019 per data collected from the mcghee tyson airport located in knoxville tn the correct volume of water used for each application is calculated using these rainfall data and an assumed 20 1 watershed to practice area ratio overall this resulted in 14 events brown et al 2009 to ensure that similar concentrations of stormwater pollutants were observed in all columns applications were made in three passes that is 1 3 of the application volume was applied to each column followed by the next 1 3 etc this ensures that any settling that occurred in the tank used to make the stormwater mixture would not be weighted unevenly to one group of columns this process took 45 min per column for the sm and vc design configurations which relied on preemptive control of the system based on rainfall predictions predictive data are obtained from the national oceanic and atmospheric administration if a rain event is expected for a given day the predicted rainfall depth is sent to the columns the day prior via wireless communication the columns would identify current sensor readings and decide if water should be released to allow space for the predicted incoming rainfall a stevens pressure transducer is used to measure the water storage levels for the vc configuration the valves are triggered to drain or retain water in order to maintain water storage levels at 30 cm for more details on the experimental method and column operation refer to persaud et al 2019 3 methods in this study we first use statistical techniques to establish the differences between various time series collected and then create three experiments to examine the extent to which these data streams can be predicted accurately from historical temporal data and or other independent sources i e other sensors to test for similarities among the data streams we conduct analysis of variance anova this test particularly allows us to check whether the data streams in aggregate statistically follow the same or different distributions more specifically the null hypothesis is that the means of the data streams considered are statistically the same hence rejecting the null hypothesis suggests that there are at least two group means that are statistically significantly different from each other we use two different modeling approaches in this study namely arima and lstm the arima model is defined as follows 1 y ˆ t μ ϕ 1 y t 1 ϕ p y t p θ 1 e t 1 θ q e t q where p denotes the amount of the autoregressive ar term q is the order of the moving average ma term and d is the number of nonseasonal differences required to make it stationary ho and xie 1998 furthermore lstm which is a special type of recurrent neural networks rnns hochreiter and schmidhuber 1997 wojciech zaremba and vinyals 2014 is defined as follows forget gate f t σ g w f x t u f h t 1 b f input gate i t σ g w i x t u i h t 1 b i output gate o t σ g w o x t u o h t 1 b o cell state c t f t c t 1 i t σ c w c x t u c h t 1 b c hidden state h t o t σ c c t where x t r d is the input vector to the lstm unit where d refers to the number of input features and w r h d u r h h and b r h are weight matrices and bias vector parameters where h refers to the number of hidden units the forget gate decides what information can move to the next layers by disregarding any information not needed to create predictions the input gate goes through the information received and determines the importance of that information for creating future predictions the output gate is responsible for deciding the next hidden state both the hidden and cell states are types of data states used in the lstm model note that weights and biases need to be learned during model training through sequential lstm cells inputs and outputs to evaluate the models we use mean absolute error mae as our main metric i e 2 m a e 1 n j 1 n y j y ˆ j where y j and y ˆ j are the response variable the predicted value for sample j respectively willmott and matsuura 2005 note that metric mae was particularly chosen because it produces a value representing the average error which is in the same units as the response variable of interest mae has repeatedly used in the literature for similar model evaluation purposes and provides a good measure for evaluating the average model performance error diouf et al 2015 first we conduct a comparison between arima and lstm to do so we conduct an experiment where we predict future soil moisture using past temporal data with the data split of 70 30 for this analysis we report the mae as well as the nash sutcliffe efficiency nse nse is used for time series models to calculate one minus the ratio of the error variance divided by the variance of the actual nse ranges from to 1 where the higher the nse the better i e 3 n s e 1 t 1 t q o t q m t 2 t 1 t q o t q o 2 where q o is the mean of observed discharges q m is modeled discharge and q o t is observed discharge at time t zeybek 2018 based on the preliminary results obtained from the comparison we opt to use lstm in the remainder of the study specifically we conduct the following three sets of experiments using the lstms and report the mae based on preliminary results and a grid search on lookback period length from 1 50 min in 5 minute increments we fix the lookback period to one minute for our lstm models experiment i the model predicts the 30 cm soil moisture sensor data stream in column x using the 30 cm soil moisture sensor data stream from column y and the previous rainfall data for any given configuration where x y 1 2 3 4 5 x y the goal of this experiment is to show how different bioretention columns in the same conditions relate to each other when it comes to soil moisture experiment ii the model predicts the 60 cm soil moisture sensor data stream in column x using the 30 cm soil moisture sensor data stream from column y and the previous rainfall data for any given configuration where x y 1 2 3 4 5 x y the goal for this experiment is to show if we can safely eliminate the need for any extra sensors within the bioretention columns experiment iii the model predicts the 30 cm soil moisture sensor data stream at t time points in the future in column y using the 30 cm soil moisture sensor data stream from column x the previous rainfall data and the future rainfall data up to time t for any given configuration where x y 1 2 3 4 5 note that in this experiment different from past experiments x can be set equal to y as t 0 the goal of this is to predict the future state of the bioretention to see if interventions or actions need to be taken note that in some of the evaluations particularly in experiments i iii we use temporal k fold cross validation with k 10 specifically we split the data streams into ten segments each segment containing data corresponding to consecutive time points in each fold we use the data from nine segments for training and the data in the one remaining segment for testing barari 2019 fig 2 visualizes the approach for modeling and evaluating for experiment i 4 results and discussion 4 1 descriptive statistics first we conduct an anova test with the null hypothesis that all data streams collected from the sensors at the same depth from the five columns with the same configuration are statistically the same hence if this hypothesis is rejected it suggests that although the columns are cosmetically similar in configuration and treatment the data collected are inherently different the detailed results of the anova test are presented in table 8 in the appendix across all tests conducted the p values corresponding are extremely small hence the null hypotheses are rejected for all tests therefore despite the similarities in the sensor readings collected from the columns within the same configuration any predictions across the columns are not trivial potential reasons for these differences are discussed below the internal soil moisture within each of the four configurations differ slightly in pattern but are comparable fig 3 shows example patterns across the four configurations at 30 cm and 60 cm depths in this figure the peaks depict the rapid increases in the soil moisture a k a a rainfall event the peaks are similar in time and magnitude which indicates all columns received the same amount of water and responded in a similar manner however the rate at which they return to the original baseline differs as indicated by variations in the slope of the declining lines after the peaks vc returns the quickest steepest slope whereas the iws is the slowest flattest slope note that fd and sm are relatively similar over time as seen in fig 3 figs 4 7 provide comparisons of the columns under the same treatment for the complete breakdown of each column within all four configurations over the 43 day period see table 9 in the appendix for instance the iws column 4 at 60 cm has a fairly low mean and column 5 has a relatively high mean in comparison to the first three columns similarly columns 1 3 in the volume control configuration at 60 cm are all similar to each other but columns 4 and 5 are different these outliers should be considered when interpreting the models results because it would explain whether or not the predicted soil moisture is worse when using these columns for training these outliers can be caused by random discontinuous sensor faults random noise or calibration errors baljak et al 2012 further differences in column construction could lead to these results such as variable amounts of compaction of the bioretention media which could affect infiltration dynamics differences in capillary action among the columns and or the presence of unintended preferential flow paths fig 4 compares each of the five fd columns at 30 cm versus 60 cm the plots when compared to each other can show the water flow throughout the column for fd the two plots maintain similar patterns for each column meaning the water is passing through each column at a controlled rate with minimal restrictions due to clogging furthermore it drains continuously over time solely due to gravity just as we would expect it to fd columns act as one extreme where water flows through the column relatively quick fig 5 shows iws which is the configuration that acts as the other extreme in comparison to fd it purposefully slows and even stops drainage so that ponding occurs at the top of the columns iws columns perform as expected for columns 1 3 by having around a 0 1 v v higher value at the 60 cm sensor than at the 30 cm sensor however column 4 has lower soil moisture readings at 60 cm versus 30 cm which is rather unexpected on the other hand column 5 has very high readings compared to the values expected given the first few columns these irregularities could indicate differences in column construction and since smooth variations are easier to predict explain why the predictions for columns 4 and 5 perform worse than those for columns 1 3 fig 6 compares all five columns in the sm configuration at 30 cm versus 60 cm the sm functions in between fd and iws so that drainage is slowed but not to the extreme that ponding occurs columns 1 3 all perform as the fd configuration did the columns drain without signs of blockage and in a controlled manner column 4 shows that the soil moisture is 0 05 v v higher than the average mean of the first three columns and 0 08 v v higher than the average maximum value at the 60 cm sensor column 5 is 0 082 v v higher than the average mean of the first three columns and 0 120 v v higher than the average maximum value at the 60 cm sensor these could be signs of incorrect sensor readings or poor drainage due to higher compaction in this column low in the soil profile overall the averages of the data collected from columns 4 and 5 are 22 6 and 37 percent higher respectively when compared with the averages from columns 1 3 this means that these columns show many data inconsistencies which could cause the results produced by the lstm to not be as accurate when specifically trained and or tested with columns 4 and 5 fig 7 shows the vc configuration which also functions in between the two extremes fd and iws it varies from the sm configuration by opening and closing the solenoid valve which controls how the water drains at a different rate the vc configuration opens and closes it much more frequently then the sm configuration on average nearly 14 times as frequently this is because vc decides when the valve opens on a target water storage depth within the column at 30 cm and 60 cm none of the columns in the vc configuration show a large difference in their data pattern when compared to the other columns however close observations show that all the means at 30 cm are very similar approximately to 0 23 but at 60 cm columns 1 3 stay close to 0 23 and columns 4 and 5 rise to about 0 32 this means columns 1 3 are draining the water at an approximately equal rate throughout the column whereas columns 4 and 5 are not columns 4 and 5 have a larger soil moisture in the bottom of the column compared to the top meaning the flow of water is decreasing 4 2 model selection arima vs lstm we perform an analysis where we compare arima and lstm models using the sensor data at both 30 cm and 60 cm for the fd configuration we conduct a grid search to determine the best parameter values for both arima and lstm models for arima we use p 1 8 q 1 8 and d 1 for lstm we modify the number of neurons for the lstm layer ranging from 10 to 150 in increments of 10 and the dropout rate for the dropout layer ranging from 0 to 0 5 in increments of 0 1 the best parameters for arima and lstm are provided in tables 10 and 11 in the appendix table 1 presents the results of the comparison between the arima and lstm models in terms of mae for each column under the fd configuration similarly table 2 presents the results of the comparison in terms of nse overall the results show that predictions are more accurate when using lstm lstm generates substantially lower mae values paired t test p value 1 0 4 and higher nse values note that arima produces negative nse values indicating that arima is not an acceptable model for these data fig 9 in the appendix presents an example of this prediction for one of the columns based on these results we only use lstm in the remainder of this study 4 3 model performance for experiments i iii table 3 presents the results for experiment i where we predict the 30 cm soil moisture sensor data stream in column x using the 30 cm soil moisture sensor data stream in column y and rainfall under all four configurations row one shows which column is used to train the model and column 1 shows the column on which the model is tested for instance for fd configuration an mae of 0 024 0 012 is the result of experiment i when column 2 is used for training and column 1 is used for testing overall the maes are relatively small in every case suggesting that relationships between soil moisture levels at the same depth can somewhat be captured using the lstm model between columns with similar designs furthermore this means it is possible to use sensor data to monitor bioretention columns in the same environment meaning they are composed of the same media have a surrounding area with the same type of soil and are receiving similar precipitation and show how they are functioning in real time this can range several blocks to a few miles within a town or a city depending on how it is built table 4 shows the results for experiment ii where the model predicts the 60 cm soil moisture sensor data stream in column x using the 30 cm soil moisture sensor data stream from column y and rainfall under all four configurations for example similar to table 3 row one under fd presents the results when the data from column 1 are used to predict the response variable from column x x 2 5 however care must be taken when comparing the results of experiments i and ii recall that experiment i predicts the 30 cm soil moisture data stream in one column using the 30 cm soil moisture in another column whereas experiment ii predicts the moisture data stream at two different depths hence overall the prediction task in experiment i may be easier than the one in experiment ii and that may be why the results are slightly better in experiment i however the results from the two experiments are not inherently comparable as one uses cross validation and the other not this changes the amount of data used in training and the evaluation process hence a direct comparison between results may not be meaningful table 5 shows the mae of experiment iii with the fd configuration without future rainfall and table 6 shows the mae of experiment iii with the fd configuration with future rainfall the results show that predictions are more accurate when future rainfall data is used in the model the results also suggest that this model can accurately predict the soil moisture sensor data stream in the future up to 60 min with the mae averaging 0 020 beyond 1 h the model looses the desired relationship and we can no longer accurately predict the soil moisture sensor data stream however as one can see in fig 8 even though the model can accurately predict the soil moisture sensor data stream up to 60 min in the future it is most accurate between zero and 30 min 5 conclusion and future work this study investigates how accurately soil moisture levels can be predicted in the future the results suggest that if the dispersed bioretention cells are close enough to have similar climate conditions you can use one cell to predict the state of multiple others up to one hour in the future to clarify a column is similar to having a core of a sandbox a bioretention practice is an entire sandbox and dispersed practices are like having sandboxes all over a playground hence only one column needs to have sensors at both 30 and 60 cm and all the surrounding columns only need one this being said stormwater managers can view the status of all the bioretention gi practices in real time with the use of monitors instead of performing hands on inspections this will save time money and resources for the given companies in the future we aim to create machine learning algorithms that use data streams from low cost sensors to enable predicting the future state of gis these predictions will pave the way for a number of capabilities for future infrastructure design and maintenance first if soil moisture can be accurately predicted based on historical data changes in soil moisture patterns can also be identified these changes can be linked to the need for maintenance as clogging of filter media will be identifiable by a longer time to peak and more flat receding limb identifying changes in these patterns can instigate further inspection by notifying municipal staff second these predictions can be used to understand how bioretention and other gi will respond to incoming rain events this can potentially allow for real time control based on these predictions finally soil moisture measurements in bioretention are only one of many potential applications for these approaches sensors measuring water depth temperature vegetation health and other biogeochemical attributes can be leveraged to track performance of gi to aid in optimized management of these systems the methods herein show great promise in characterizing and predicting such data streams software and data availability developer and contact information kalina scarbrough kalinascarbrough gmail com year first available 2021 operating system osx windows or linux software required python 3 6 0 numpy 1 21 0 scipy 1 7 0 keras 2 4 0 availability and online documentation https github com kscarbr3 real time sensor based prediction of soil moisture in green infrastructure a case study git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is partially supported by the university of tennessee s institute for a secure and sustainable environment isse united states and the national science foundation united states grant cns 1737432 appendix see tables 7 8 and fig 9 
25426,green infrastructure gi is cost effective for managing urban runoff however inspection and maintenance of gi are an increasingly common burden for stormwater managers for instance bioretention cells a popular type of gi may clog sometimes unexpectedly and detection can be challenging due to their dispersed placement current inspection programs nationwide largely rely on time intensive manual qualitative inspections this study develops an approach for real time monitoring and prediction of column performance first we conduct laboratory experiments to continuously collect soil moisture data using sensors at two different depths in bioretention column testbeds four design configurations are used that allow the water to drain differently through the column hence acting as different environmental climates next we develop machine learning models i e long short term memory lstm models to accurately predict current and future soil moisture levels our results suggest that the quality of predictions is overall high but they vary across the configurations keywords machine learning sensor data time series green infrastructure bioretention columns real time monitoring data availability data will be made available on request 1 introduction across the united states urban stormwater causes surface water degradation leading watershed managers to turn to green infrastructure gi as a way to clean runoff and bring more natural hydrologic regimes to streams gi improves existing or builds new infrastructure adding resiliency to aged systems and helping manage extreme weather caused by climate change research has shown that besides climate change benefits gi provides many health benefits for residents by improving not only stormwater quality and flood defense but also wildlife habitat urban heat island mitigation and public green space kaluarachchi 2020 gi is considered a distributed management technique whereby interventions are placed throughout the watershed to provide local scale improvements one such gi approach that has been increasingly utilized over the past decade is bioretention there are many design variations of bioretention that are used bioretention has been shown to reduce the quantity and improve the quality of urban runoff by utilizing natural processes despite these benefits bioretention cells are prone to clogging from anthropogenic factors such as debris accumulation within the contributing watershed plant die off and seasonal weather changes benedict et al 2002 additionally these environmental variable vary from site to site thus there are ongoing maintenance and inspection needs for these systems to ensure their functionality gi inspection maintenance has increasingly received attention as a major component of many stormwater management programs current strategies for these programs heavily rely on manual inspection and maintenance with largely qualitative approaches this entails performing hands on inspections on each gi installation one by one across the entire city benedict et al 2002 angelstam et al 2017 the distributed approach to gi implementation noted above complicates this task and increases the financial burden ryan 2019 as an example metro water services in nashville tn has multiple staff solely dedicated to this task any changes in a given practice function between these inspections are difficult to anticipate or even identify recently interest has grown for real time environmental monitoring in urban watersheds such activities may better quantify the performance of gi and allow more quantitative assessments as to when maintenance is needed further by minimizing hands on labor required by staff and saving maintenance time within the system the stormwater management program can be more efficient and effective as an example soil moisture data can be used to understand the timing and pattern of runoff movement through the soil profile tracking changes in these patterns over time can aid in understanding how system function is shifting changes in soil moisture patterns that include slower time to peak and a recession with a flatter slope may indicate that clogging of the system is causing runoff to infiltrate more slowly and thus the need for maintenance these observations can be relayed back to inspectors to trigger additional actions potentially complicating this approach is the wide array of gi designs utilized in practice for bioretention alone design variations include 1 traditional which are allowed to freely drain at the bottom of the soil profile 2 internal water storage where water ponds in the bottom of the cell before filling to the level of the drain if a given storm is large enough and 3 real time control a new design approach that allows drainage to be instigated or stopped based on a set of control rules real time control has recently become of interest due to the adaptability that it affords kerkez et al 2016 studies such as persaud et al 2019 have shown that real time control can instigate favorable conditions for biogeochemical processes to optimize runoff treatment however these systems may not have soil moisture processes as regimented as standard designs as opening and closing of the valve can influence infiltration processes in addition to the onset of rainfall runoff technological advancements now allow access to low cost sensors which can be installed within each gi structure allowing for real time monitoring further these real time monitoring systems are increasingly becoming user friendly tools paving the way for use in real world applications glasgow et al 2004 feuer 1995 a study by jim gao 2014 showed that rtrm enables many opportunities for system improvements in this study neural networks nn learn the relationships from the actual operations data to model its function the model is tested and validated using google s data center the results suggest that machine learning specifically nn can effectively and efficiently use existing sensor data to model data centers performance gao 2014 other studies test methods such as boolean logic based multi criteria analysis and suggest they are not as accurate in predicting future gi states they study artificial neural network ann and network based fuzzy inference system anfis to determine that ann is 72 percent accurate where anfis is only 65 percent accurate labib 2019 internet of things iot has allowed for many water industry advancements intelligent water is one of these advancements that is already operational in multiple cities south bend indiana and ann arbor michigan are examples within the united states of where such systems are implemented and show promise of improving the drainage system s function atzori et al 2010 globally there have been other countries that have shown promise of implementation of improved drainage systems as well for example melbourne australia which is a forest city is planning on implementing gi to help combat the worsening urban heat island effect uhi and could benefit from a useful gi monitoring tool fuentes et al 2021 although advancements are being made this is still a new field and many applications have yet to be explored for instance can gi managers be informed of maintenance and inspection needs using these new technology applications this application represents how innovative and valuable iot can be for the water industry bumblauskas et al 2017 as noted above the application of iot to the water industry has matured which makes successful field deployment possible low cost sensor nodes may allow maintenance programs and gi inspections more efficiency by measuring performance quantitatively and allowing year long assessments however determining what tools can be used to interpret large amounts of data while keeping an overall low cost has yet to be determined this study conducts a proof of concept investigation of how data from sensors installed in bioretention can be analyzed and interpreted the objective of this study is to 1 understand how the soil moisture in different bioretention columns relate to each other when under the same conditions 2 if the number of sensors used in a given installation can be optimized 3 predict the future state of the bioretention column using historical and future soil moisture and rainfall sensor data these objectives will build knowledge on using sensors to inform gi maintenance and pave the way for future research in this study we develop time series models using long short term memory lstm networks hochreiter and schmidhuber 1997 wojciech zaremba and vinyals 2014 in the literature various types of time series modeling exist this includes traditional statistical models such as autoregressive integrated moving average arima or more advanced nn based models such as lstm networks arima is simpler and generally works well when there is a clear trend in the time series however in many complex cases lstm has shown to outperform arima this has been shown in forecasting wind speed shivani et al 2019 web traffic shelatkar et al 2020 and financial data siami namini et al 2018 among others in this study we first perform some preliminary analysis to compare the performance of arima and lstm in our use case based on these results lstm outperforms arima partly because our data trends are rather complex as the data are collected from sensors capturing the soil moisture in response to precipitation at different depths hence we perform our main analysis using lstms to showcase the model performance we exploit a column study notably these columns represent a simplification of field conditions based upon more complex hydrologic processes that may vary spatially across a bioretention however even if patterns are slightly different in a field installation the methods herein are robust enough to be trained on those data further this work represents a proof of concept to see how well soil moisture patterns can be modeled and predicted at an individual location within a bioretention which is relatively well represented by the column this is highly valuable as it allows an assessment as to how patterns change over time due to perturbations such as clogging of soil media 2 data to carefully control test conditions data collection for this case study took place inside a greenhouse bioretention cell function is mimicked using pvc columns constructed with materials per typical design guidance specifically all columns use a 30 centimeter cm diameter column with a drain at the bottom biological fouling is a concern when building the columns so drains are frequently utilized and monitored column construction and operation included sanding the column s interior walls to minimize preferential flow lastly columns are filled by layers of 57 stone pea gravel sand bioretention media mulch and a ponding zone each column is planted in a climate controlled greenhouse of 15 27 c the columns are continuously monitored for soil moisture using two meter group s teros 10 sensors two sensors are placed within each column at depths of 30 and 60 cm from the top of the bioretention media the sensors then gather the water volume content namely soil moisture in units of volume volume v v every minute for 43 days anon 2020 this time period was selected because 1 the time period was not too long as to make the time consuming methodology of the column study impossible and 2 a reasonable number of rainfall events occurred during this period allowing a realistic data set of soil moisture patterns while more data are obviously better we believe this data set contained variable storm sizes and variable dry periods between events and thus does give a reasonable data set to test the machine learning approaches herein following data collection we pre process and cleanse the data by removing the missing data points after verifying that they are caused by sensor failure and or sensor anomalies to provide a robust analysis there are four types of column configurations representing four potential bioretention design configurations there are five columns of each configuration for a total of 20 columns fig 1 conveys how each configuration is constructed free draining fd the drain on the bottom of the column is completely unobstructed i e drains via gravity internal water storage iws an upturned elbow in the pipe is attached to the drain on the bottom which creates a 45 cm submerged zone at the bottom of the column soil moisture sm uses a remote level controller with real time monitoring of water depth to control the soil moisture level at a depth of 30 cm to maintain field capacity volume control vc actively managed remotely in real time by opening and closing the valves within the column to maintain a storage depth of 30 cm from the bottom of the bioretention media as measured by a pressure transducer to create artificial stormwater for use in the study tap water was supplemented with chemicals to achieve typical stormwater runoff concentrations per bratieres et al 2008 sediment collected from a local detention pond was sieved and added to the mixture the artificial stormwater was continuously mixed while being added to the columns to ensure even distribution of constituents among the columns bratieres et al 2008a the detailed breakdown of the chemical concentrations is provided in table 7 in the appendix application of the stormwater is performed to mimic the size and frequency of real historical rainfall events across a roughly six week period july 31 2019 to september 1 2019 per data collected from the mcghee tyson airport located in knoxville tn the correct volume of water used for each application is calculated using these rainfall data and an assumed 20 1 watershed to practice area ratio overall this resulted in 14 events brown et al 2009 to ensure that similar concentrations of stormwater pollutants were observed in all columns applications were made in three passes that is 1 3 of the application volume was applied to each column followed by the next 1 3 etc this ensures that any settling that occurred in the tank used to make the stormwater mixture would not be weighted unevenly to one group of columns this process took 45 min per column for the sm and vc design configurations which relied on preemptive control of the system based on rainfall predictions predictive data are obtained from the national oceanic and atmospheric administration if a rain event is expected for a given day the predicted rainfall depth is sent to the columns the day prior via wireless communication the columns would identify current sensor readings and decide if water should be released to allow space for the predicted incoming rainfall a stevens pressure transducer is used to measure the water storage levels for the vc configuration the valves are triggered to drain or retain water in order to maintain water storage levels at 30 cm for more details on the experimental method and column operation refer to persaud et al 2019 3 methods in this study we first use statistical techniques to establish the differences between various time series collected and then create three experiments to examine the extent to which these data streams can be predicted accurately from historical temporal data and or other independent sources i e other sensors to test for similarities among the data streams we conduct analysis of variance anova this test particularly allows us to check whether the data streams in aggregate statistically follow the same or different distributions more specifically the null hypothesis is that the means of the data streams considered are statistically the same hence rejecting the null hypothesis suggests that there are at least two group means that are statistically significantly different from each other we use two different modeling approaches in this study namely arima and lstm the arima model is defined as follows 1 y ˆ t μ ϕ 1 y t 1 ϕ p y t p θ 1 e t 1 θ q e t q where p denotes the amount of the autoregressive ar term q is the order of the moving average ma term and d is the number of nonseasonal differences required to make it stationary ho and xie 1998 furthermore lstm which is a special type of recurrent neural networks rnns hochreiter and schmidhuber 1997 wojciech zaremba and vinyals 2014 is defined as follows forget gate f t σ g w f x t u f h t 1 b f input gate i t σ g w i x t u i h t 1 b i output gate o t σ g w o x t u o h t 1 b o cell state c t f t c t 1 i t σ c w c x t u c h t 1 b c hidden state h t o t σ c c t where x t r d is the input vector to the lstm unit where d refers to the number of input features and w r h d u r h h and b r h are weight matrices and bias vector parameters where h refers to the number of hidden units the forget gate decides what information can move to the next layers by disregarding any information not needed to create predictions the input gate goes through the information received and determines the importance of that information for creating future predictions the output gate is responsible for deciding the next hidden state both the hidden and cell states are types of data states used in the lstm model note that weights and biases need to be learned during model training through sequential lstm cells inputs and outputs to evaluate the models we use mean absolute error mae as our main metric i e 2 m a e 1 n j 1 n y j y ˆ j where y j and y ˆ j are the response variable the predicted value for sample j respectively willmott and matsuura 2005 note that metric mae was particularly chosen because it produces a value representing the average error which is in the same units as the response variable of interest mae has repeatedly used in the literature for similar model evaluation purposes and provides a good measure for evaluating the average model performance error diouf et al 2015 first we conduct a comparison between arima and lstm to do so we conduct an experiment where we predict future soil moisture using past temporal data with the data split of 70 30 for this analysis we report the mae as well as the nash sutcliffe efficiency nse nse is used for time series models to calculate one minus the ratio of the error variance divided by the variance of the actual nse ranges from to 1 where the higher the nse the better i e 3 n s e 1 t 1 t q o t q m t 2 t 1 t q o t q o 2 where q o is the mean of observed discharges q m is modeled discharge and q o t is observed discharge at time t zeybek 2018 based on the preliminary results obtained from the comparison we opt to use lstm in the remainder of the study specifically we conduct the following three sets of experiments using the lstms and report the mae based on preliminary results and a grid search on lookback period length from 1 50 min in 5 minute increments we fix the lookback period to one minute for our lstm models experiment i the model predicts the 30 cm soil moisture sensor data stream in column x using the 30 cm soil moisture sensor data stream from column y and the previous rainfall data for any given configuration where x y 1 2 3 4 5 x y the goal of this experiment is to show how different bioretention columns in the same conditions relate to each other when it comes to soil moisture experiment ii the model predicts the 60 cm soil moisture sensor data stream in column x using the 30 cm soil moisture sensor data stream from column y and the previous rainfall data for any given configuration where x y 1 2 3 4 5 x y the goal for this experiment is to show if we can safely eliminate the need for any extra sensors within the bioretention columns experiment iii the model predicts the 30 cm soil moisture sensor data stream at t time points in the future in column y using the 30 cm soil moisture sensor data stream from column x the previous rainfall data and the future rainfall data up to time t for any given configuration where x y 1 2 3 4 5 note that in this experiment different from past experiments x can be set equal to y as t 0 the goal of this is to predict the future state of the bioretention to see if interventions or actions need to be taken note that in some of the evaluations particularly in experiments i iii we use temporal k fold cross validation with k 10 specifically we split the data streams into ten segments each segment containing data corresponding to consecutive time points in each fold we use the data from nine segments for training and the data in the one remaining segment for testing barari 2019 fig 2 visualizes the approach for modeling and evaluating for experiment i 4 results and discussion 4 1 descriptive statistics first we conduct an anova test with the null hypothesis that all data streams collected from the sensors at the same depth from the five columns with the same configuration are statistically the same hence if this hypothesis is rejected it suggests that although the columns are cosmetically similar in configuration and treatment the data collected are inherently different the detailed results of the anova test are presented in table 8 in the appendix across all tests conducted the p values corresponding are extremely small hence the null hypotheses are rejected for all tests therefore despite the similarities in the sensor readings collected from the columns within the same configuration any predictions across the columns are not trivial potential reasons for these differences are discussed below the internal soil moisture within each of the four configurations differ slightly in pattern but are comparable fig 3 shows example patterns across the four configurations at 30 cm and 60 cm depths in this figure the peaks depict the rapid increases in the soil moisture a k a a rainfall event the peaks are similar in time and magnitude which indicates all columns received the same amount of water and responded in a similar manner however the rate at which they return to the original baseline differs as indicated by variations in the slope of the declining lines after the peaks vc returns the quickest steepest slope whereas the iws is the slowest flattest slope note that fd and sm are relatively similar over time as seen in fig 3 figs 4 7 provide comparisons of the columns under the same treatment for the complete breakdown of each column within all four configurations over the 43 day period see table 9 in the appendix for instance the iws column 4 at 60 cm has a fairly low mean and column 5 has a relatively high mean in comparison to the first three columns similarly columns 1 3 in the volume control configuration at 60 cm are all similar to each other but columns 4 and 5 are different these outliers should be considered when interpreting the models results because it would explain whether or not the predicted soil moisture is worse when using these columns for training these outliers can be caused by random discontinuous sensor faults random noise or calibration errors baljak et al 2012 further differences in column construction could lead to these results such as variable amounts of compaction of the bioretention media which could affect infiltration dynamics differences in capillary action among the columns and or the presence of unintended preferential flow paths fig 4 compares each of the five fd columns at 30 cm versus 60 cm the plots when compared to each other can show the water flow throughout the column for fd the two plots maintain similar patterns for each column meaning the water is passing through each column at a controlled rate with minimal restrictions due to clogging furthermore it drains continuously over time solely due to gravity just as we would expect it to fd columns act as one extreme where water flows through the column relatively quick fig 5 shows iws which is the configuration that acts as the other extreme in comparison to fd it purposefully slows and even stops drainage so that ponding occurs at the top of the columns iws columns perform as expected for columns 1 3 by having around a 0 1 v v higher value at the 60 cm sensor than at the 30 cm sensor however column 4 has lower soil moisture readings at 60 cm versus 30 cm which is rather unexpected on the other hand column 5 has very high readings compared to the values expected given the first few columns these irregularities could indicate differences in column construction and since smooth variations are easier to predict explain why the predictions for columns 4 and 5 perform worse than those for columns 1 3 fig 6 compares all five columns in the sm configuration at 30 cm versus 60 cm the sm functions in between fd and iws so that drainage is slowed but not to the extreme that ponding occurs columns 1 3 all perform as the fd configuration did the columns drain without signs of blockage and in a controlled manner column 4 shows that the soil moisture is 0 05 v v higher than the average mean of the first three columns and 0 08 v v higher than the average maximum value at the 60 cm sensor column 5 is 0 082 v v higher than the average mean of the first three columns and 0 120 v v higher than the average maximum value at the 60 cm sensor these could be signs of incorrect sensor readings or poor drainage due to higher compaction in this column low in the soil profile overall the averages of the data collected from columns 4 and 5 are 22 6 and 37 percent higher respectively when compared with the averages from columns 1 3 this means that these columns show many data inconsistencies which could cause the results produced by the lstm to not be as accurate when specifically trained and or tested with columns 4 and 5 fig 7 shows the vc configuration which also functions in between the two extremes fd and iws it varies from the sm configuration by opening and closing the solenoid valve which controls how the water drains at a different rate the vc configuration opens and closes it much more frequently then the sm configuration on average nearly 14 times as frequently this is because vc decides when the valve opens on a target water storage depth within the column at 30 cm and 60 cm none of the columns in the vc configuration show a large difference in their data pattern when compared to the other columns however close observations show that all the means at 30 cm are very similar approximately to 0 23 but at 60 cm columns 1 3 stay close to 0 23 and columns 4 and 5 rise to about 0 32 this means columns 1 3 are draining the water at an approximately equal rate throughout the column whereas columns 4 and 5 are not columns 4 and 5 have a larger soil moisture in the bottom of the column compared to the top meaning the flow of water is decreasing 4 2 model selection arima vs lstm we perform an analysis where we compare arima and lstm models using the sensor data at both 30 cm and 60 cm for the fd configuration we conduct a grid search to determine the best parameter values for both arima and lstm models for arima we use p 1 8 q 1 8 and d 1 for lstm we modify the number of neurons for the lstm layer ranging from 10 to 150 in increments of 10 and the dropout rate for the dropout layer ranging from 0 to 0 5 in increments of 0 1 the best parameters for arima and lstm are provided in tables 10 and 11 in the appendix table 1 presents the results of the comparison between the arima and lstm models in terms of mae for each column under the fd configuration similarly table 2 presents the results of the comparison in terms of nse overall the results show that predictions are more accurate when using lstm lstm generates substantially lower mae values paired t test p value 1 0 4 and higher nse values note that arima produces negative nse values indicating that arima is not an acceptable model for these data fig 9 in the appendix presents an example of this prediction for one of the columns based on these results we only use lstm in the remainder of this study 4 3 model performance for experiments i iii table 3 presents the results for experiment i where we predict the 30 cm soil moisture sensor data stream in column x using the 30 cm soil moisture sensor data stream in column y and rainfall under all four configurations row one shows which column is used to train the model and column 1 shows the column on which the model is tested for instance for fd configuration an mae of 0 024 0 012 is the result of experiment i when column 2 is used for training and column 1 is used for testing overall the maes are relatively small in every case suggesting that relationships between soil moisture levels at the same depth can somewhat be captured using the lstm model between columns with similar designs furthermore this means it is possible to use sensor data to monitor bioretention columns in the same environment meaning they are composed of the same media have a surrounding area with the same type of soil and are receiving similar precipitation and show how they are functioning in real time this can range several blocks to a few miles within a town or a city depending on how it is built table 4 shows the results for experiment ii where the model predicts the 60 cm soil moisture sensor data stream in column x using the 30 cm soil moisture sensor data stream from column y and rainfall under all four configurations for example similar to table 3 row one under fd presents the results when the data from column 1 are used to predict the response variable from column x x 2 5 however care must be taken when comparing the results of experiments i and ii recall that experiment i predicts the 30 cm soil moisture data stream in one column using the 30 cm soil moisture in another column whereas experiment ii predicts the moisture data stream at two different depths hence overall the prediction task in experiment i may be easier than the one in experiment ii and that may be why the results are slightly better in experiment i however the results from the two experiments are not inherently comparable as one uses cross validation and the other not this changes the amount of data used in training and the evaluation process hence a direct comparison between results may not be meaningful table 5 shows the mae of experiment iii with the fd configuration without future rainfall and table 6 shows the mae of experiment iii with the fd configuration with future rainfall the results show that predictions are more accurate when future rainfall data is used in the model the results also suggest that this model can accurately predict the soil moisture sensor data stream in the future up to 60 min with the mae averaging 0 020 beyond 1 h the model looses the desired relationship and we can no longer accurately predict the soil moisture sensor data stream however as one can see in fig 8 even though the model can accurately predict the soil moisture sensor data stream up to 60 min in the future it is most accurate between zero and 30 min 5 conclusion and future work this study investigates how accurately soil moisture levels can be predicted in the future the results suggest that if the dispersed bioretention cells are close enough to have similar climate conditions you can use one cell to predict the state of multiple others up to one hour in the future to clarify a column is similar to having a core of a sandbox a bioretention practice is an entire sandbox and dispersed practices are like having sandboxes all over a playground hence only one column needs to have sensors at both 30 and 60 cm and all the surrounding columns only need one this being said stormwater managers can view the status of all the bioretention gi practices in real time with the use of monitors instead of performing hands on inspections this will save time money and resources for the given companies in the future we aim to create machine learning algorithms that use data streams from low cost sensors to enable predicting the future state of gis these predictions will pave the way for a number of capabilities for future infrastructure design and maintenance first if soil moisture can be accurately predicted based on historical data changes in soil moisture patterns can also be identified these changes can be linked to the need for maintenance as clogging of filter media will be identifiable by a longer time to peak and more flat receding limb identifying changes in these patterns can instigate further inspection by notifying municipal staff second these predictions can be used to understand how bioretention and other gi will respond to incoming rain events this can potentially allow for real time control based on these predictions finally soil moisture measurements in bioretention are only one of many potential applications for these approaches sensors measuring water depth temperature vegetation health and other biogeochemical attributes can be leveraged to track performance of gi to aid in optimized management of these systems the methods herein show great promise in characterizing and predicting such data streams software and data availability developer and contact information kalina scarbrough kalinascarbrough gmail com year first available 2021 operating system osx windows or linux software required python 3 6 0 numpy 1 21 0 scipy 1 7 0 keras 2 4 0 availability and online documentation https github com kscarbr3 real time sensor based prediction of soil moisture in green infrastructure a case study git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is partially supported by the university of tennessee s institute for a secure and sustainable environment isse united states and the national science foundation united states grant cns 1737432 appendix see tables 7 8 and fig 9 
25427,lake ecosystem models are essential for capturing ecosystem response to climate change and for studying eutrophication model ability to capture ecosystem dynamics is limited for various reasons aiming to increase confidence in simulations of future scenarios the current study adapts a recently established four level validation framework named csps conceptual state process and system validation lake ecosystem model wet was calibrated for lake kinneret and validated using the csps framework inspecting 18 state variables 11 processes and 15 emergent properties validation showed that most processes and emergent properties were correctly simulated with some weaknesses revealed such as light attenuation and particle dynamics modifying the model to overcome those weaknesses will result in increased model reliability this conclusion would have been impossible without the process validation we conclude that higher validation levels are necessary to receive reliable results when running process based lake models on future scenarios keywords lake ecosystem modeling model evaluation csps wet lake kinneret abbreviations csps conceptual state process and system validation data availability data will be made available on request software and data availability the 1d hydrodynamic ecological model wet v1 0 was used wet is a public domain model that is freely available at https projects au dk wet the complete fortran source code for wet is also available through gitlab https gitlab com wet last accessed may 8 2022 the parallel sensitivity and auto calibration parsac tool was developed by bolding bruggeman as a designated python package parsac version 0 5 5 bruggeman and bolding 2020 was applied in this study and is freely available via the python package manager pip install parsac user from pypi org 1 introduction ecosystem models are an essential tool for capturing ecosystem transformation in response to climate change thuiller et al 2008 soares and calijuri 2021 ecosystem models provide us with a deeper scientific understanding of ecosystem behavior trolle et al 2012 and therefore can be a pivotal tool for identifying and supporting management actions lake models have been used to support restoration actions from eutrophication brett et al 2016 cui et al 2016 allan 2018 ladwig et al 2018 and for maintaining fisheries kumar et al 2016 ofir et al 2017 natugonza et al 2019 lake models can provide information on the potential impacts of climate change and how to best mitigate those impacts koenigstein et al 2016 rolighed et al 2016 ladwig et al 2018 bucak et al 2018 gal et al 2020 heneghan et al 2021 and for assessing management actions to maintain the lake ecosystem services gal et al 2009 ofir et al 2017 lewis et al 2020 lake ecosystem models can also be used for near real time prediction to assist in immediate management decisions such as lake bathing suspension halt drinking water consumption and actions to mitigate pollution events janssen et al 2015 robson et al 2017 prediction is very difficult especially if it s about the future danish proverb highlights the fact that large benefits associated with model predictions come with considerable challenges many researchers use diminishing expressions like all models are wrong works of fiction or based on properties of convenience to describe the problematic nature of models although these are extreme viewpoints they emphasize the difficulties that characterizes modeling efforts box g e p 1979 arhonditsis and brett 2004 and subsequent inaccuracy in model results model inaccuracy is a consequence of a number of factors including gaps and errors in forcing and calibration data parameters uncertainty numerical approximation and most importantly models are but a simplification of natural processes refsgaard et al 2007 yet even simplified hydrodynamic ecological lake models are complex containing hundreds of parameters that require calibration for the specific lake in question vinçon leite and casenave 2019 calibration is conducted by running the model on existing forcing data and minimizing the result deviation from existing observations measured by various fit measures bennett et al 2013 our confidence is built on model calibration in reference to historical time periods for which meteorological conditions basin inflows and outflows are available along with lake based observations and measurements however can we expect models to correctly simulate the system dynamic response by subjecting it to external drivers beyond the range of data for which the model was calibrated our confidence in the model ability to capture ecosystem dynamics and shifts is limited yet it is at the heart of our expectation from models hipsey et al 2015 to extend the confidence in model performance validation is conducted by testing model behavior most commonly the validation process consists of extending the calibration period by running the model with independent forcing data and testing whether model performance holds using the same parameter values and evaluated based on same fit measures arhonditsis and brett 2004 this type of validation is termed state validation as simulated state variables are compared to the lake based measurements of the same state variables hipsey et al 2020 however models can be right for the wrong reasons robson et al 2017 and if that s the case it might fail under conditions other than those for which it was calibrated and validated bennett et al 2013 brett et al 2016 vinçon leite and casenave 2019 mesman et al 2020 this is particularly important when using lake ecosystem models to evaluate long term climate change scenarios that often exceed historical conditions in addition can the models simulate processes and variables for which they were not calibrated and still maintain reasonable accuracy given the increasing dependency on models as part of the decision making process we need means to better evaluate models minimize model inaccuracy and uncertainty thereby providing more reliable models for decision makers in order to approach the issue of increased reliability and accuracy of models hipsey et al 2020 established a framework for the hierarchical assessment of models named csps conceptual state process and system validation the csps framework expands the more traditional evaluation of state variable predictions by evaluating model predictions at four levels 0 conceptual validation ensures that the model is consistent with ecological theory 1 state validation compares simulated state variables with observations 2 process validation compares fluxes with measured process rates and 3 system validation evaluates system level emergent properties patterns and relationships the csps framework can reveal model weaknesses that would be otherwise overlooked level 1 is the widely used validation conducted when first applying a model to a lake ecosystem level 2 asks if the biogeochemical processes are correctly modeled finally level 3 concerns general system properties that the model does not address explicitly successful validation of the higher levels produces a model that is more likely to follow the processes and system properties even when subjected to long term scenarios these include scenarios beyond the range of forcing conditions for which it was developed and trained for these additional perspectives disclose instances in which model is right for the wrong reasons another objective of the csps is setting a common framework for model assessment to facilitate benchmarking and synthesis of aquatic ecosystem models the csps approach was implemented for the first time by robson et al 2020 their study highlights both strengths of processes and system properties that the model was able to simulate and weaknesses that prompt additional model parameter modification climate change may result in scenarios outside the conditions that the model was calibrated for implementing the csps validation conclusions accounting for the weaknesses revealed will enhance the reliability of model predictions and enable running the model on future scenarios with greater confidence lake kinneret plays a pivotal role in israel s water supply system and is also a unique site for recreational religious and cultural services as a result an extensive monitoring program has been in place for over 50 years zohary et al 2014 this unique database provides means for applying and testing the csps validation framework with the intention of predicting climate change impacts on lake kinneret a gotm wet model umlauf et al 2012 schnedler meyer et al 2022 for lake kinneret was constructed aiming to increase confidence in future scenario simulations the current study continues the adoption of the csps framework suggested by hipsey et al 2020 strengths and weaknesses of the wet lake kinneret ecosystem model was identified by the csps validation consequently model adjustments should be made to overcome model shortcomings the end result will be a model that can be used to predict future trends of the lake ecosystem with higher confidence when subjected to long term climate change scenarios 2 methods 2 1 study site lake kinneret sea of galilee is a warm monomictic lake situated in northern israel it is the only natural freshwater lake in israel and as such is a unique ecosystem that provides a series of critical ecosystem services including drinking water fisheries and recreation as well as cultural and religious significance the lake s maximum depth is 44 m its mean depth is 24 m and its surface area is 168 km2 the lake water level has a natural annual fluctuation of approximately 1 8 m but fluctuations of over 6 2 m were recorded since the 1990 s around a mean level of 211 2 i e below sea level with increases in water level in the winter spring reaching up to 4 m gal et al 2013 the lake is stratified about 10 months a year march january with an anoxic hypolimnion the kinneret limnological laboratory kll has been conducting an extensive routine monitoring program since 1969 comprising all major physical biological and chemical variables we refer the reader to zohary et al 2014 for additional information on the lake ecosystem 2 2 model description the lake ecosystem model gotm wet schnedler meyer et al 2022 is comprised of a hydrodynamical model gotm coupled to a biogeochemical model wet the general ocean turbulence model gotm is a one dimensional water column model covering the key hydrodynamic and thermodynamic processes related to vertical mixing in natural waters umlauf et al 2012 the main controlling equations of gotm are transport of momentum and heat the gotm version applied within the gotm wet is adapted to lakes where all layers include not only the pelagic volume but also the sediment area drawn from the lake specific hypsography chou et al 2021 the water ecosystem tool wet schnedler meyer et al 2022 is a re development on fabm pclake hu et al 2016 which is a one dimensional biogeochemical model originally based on a zero dimensional shallow lake model pclake janse 1990 the model is based on closed nitrogen and phosphorus cycles across the biotic and a biotic lake components wet is coupled to gotm by the framework of biogeochemical models fabm bruggeman and bolding 2014 and is referred simply as wet hereafter as with other lake ecosystem models the application of wet to a new lake ecosystem requires intensive calibration of parameters vinçon leite and casenave 2019 2 3 model setup the setup and construction of wet requires several configuration predefinitions the model was constructed with an hourly time step and included 50 horizontal depth layers varying in thickness according to lake level lake level was provided as input to the model the biotic structure included five phytoplankton and three zooplankton groups similar to those used in previous model set up for the lake gal et al 2009 the phytoplankton groups were configured according to their different nature and seasonal growth 1 dinoflagellates dino mainly representing peridinium gatunense the most dominant taxa until the mid 1990 s 2 nitrogen fixing cyanobacteria nfixcyano mainly representing aphanizomenon oval and cylindrospermopsis spp 3 microcystis spp cyanomc 4 other cyanobacteria cyano and 5 all the rest of the phytoplankton green largely representing aulacoseira granulata and mougeotia spp among others zooplankton were configured by functional groups according to gal et al 2009 1 herbivorous zooplankton herbzoo 2 microzooplankton microzoo and 3 predatory zooplankton predzoo special attention in the model configuration was given to the dinoflagellate p gatunense because it was the lake s most abundant phytoplankton but has an irregular cycle since 1995 it s irregular annual appearance has been linked to the lack of a yet unknown unmonitored micronutrient vital for the peridinium growth that appears only in high precipitation years zohary et al 2012 also see appendix a we therefore configured the dino group with a requirement for a micronutrient that was introduced into the jordan river with a positive correlation to streamflow 2 4 model input simulations require meteorological inflow and outflow input forcing data as well as initial conditions for simulated variables hourly meteorological measurements were taken mostly from a meteorological station located at deepest point in the lake stn a and included wind speed air temperature relative humidity barometric pressure and short wave radiation to complete missing measurements from the station data were extracted from land based stations and reanalysis from the era5 land hourly data climatic model muñoz sabater 2019 daily inflows of six streams the jordan river being the main one and one outflow were taken from the israel hydrological service measurements and adjusted to match the water balance provided by the mekorot water company assouline 1993 berger et al 2019 inflow temperature salinity and nutrient concentrations are also measured and were used as daily forcing data daily precipitation measurements were taken from stations located near the lake shore and verified to match the monthly water balance hypsographic data and daily lake levels were taken from mekorot measurements some data imputation and adjustment of forcing data were done to address missing data and different data sources described in appendix b table s2 in appendix b contains a list of forcing data boundary conditions the model initial conditions were extracted from the end of a 60 year spin up period ji 2017 this was done to satisfy the non uniform depth distribution of sediment inorganic and organic materials that reach a steady state only after about a 60 year model run as determined from preliminary simulations we conducted see appendix c 2 5 data for model calibration and validation in order to evaluate the results of the lake kinneret wet model we conducted a time series comparison of data collected as part of the kll monitoring program sukenik et al 2014 the time series for model calibration was set to the period of feb 1 2003 to jan 31 2014 with an additional 1 year spin up period to acclimate for the specific 2003 conditions while the validation period was feb 1 2014 to jan 31 2020 we start the simulations in february as the annual lake overturn typically starts during january the time series data used for calibration of state variables measurements at stn a included temperature o2 no3 nh4 po4 3 and plankton phytoplankton and zooplankton monitoring data are provided as biomass and density respectively zooplankton density was converted to biomass according to gophen and azoulay 2002 hambright 2008 blankenbach 2015 and is detailed in appendix d the measurements were collected from various depths at stn a at a sampling frequency ranging from daily temperature weekly chemical variables to 14 days biological variables the model was calibrated using the state variables mentioned above while maintaining a residual stream close to zero where residual stream is the method gotm uses to maintain a correct water balance an auto calibration tool parsac parallel sensitivity and auto calibration tool bruggeman and bolding 2020 was used for the calibration process in tandem with manual calibration about 80 of the parameters were changed from default values as a result of the calibration process appendix e 2 6 conceptual and state validation levels 0 1 the conceptual validation of the wet model ensuring the model is consistent with theory was based on existing literature by answering questions set forth by hipsey et al 2020 in addition to a water balance check the calibration and state validation compared simulated state variables with observations based on several fit measures fit measures for temperature and abiotic variables were analyzed separately for the epilimnion depth 10 and hypolimnion depth 20 while plankton were analyzed for the epilimnion only the fit measures included the following a nmae normalized mean absolute error bennett et al 2013 absolute error relative to the mean ideal value is 0 and range 0 ꝏ sometimes referred to as mare mean absolute relative error b r2 coefficient of determination ideal value is 1 and range 0 1 c nse nash sutcliffe efficiency nash and sutcliffe 1970 also called mef model efficiency normalizes the error to the variance values ranges between negative values to 1 where 1 is a perfect fit 0 is equivalent to the mean benchmark and negative is worse than the mean d kge kling gupta efficiency gupta et al 2009 knoben et al 2019 incorporates 3 fit measures correlation variability bias and mean bias values range between negative values to 1 where 1 is a perfect fit 0 41 is equivalent to the mean benchmark and lower values are worse than the mean for result description simplicity and clarity kge fit measure was used to classify the results into the following classes excellent kge 0 8 good kge 0 3 fair kge 0 41 unsatisfactory kge 0 41 2 7 process validation level 2 in order to conduct the process validation we compared fluxes rates and additional ecosystem characteristics for which the model was not calibrated with observed and calculated corresponding information from lake based measurements eleven such characteristics were analyzed for the entire period feb 2004 jan 2019 table 1 1 stratification days start day end day period we defined a stratified water column when the temperature profile had a stdev average ratio 3 or the vertical temperature decline 1 c per 1 m interval 2 5 nutrient s depletion and accumulation rates in the hypolimnion were calculated as monthly rates when nutrient depletion accumulation took place 6 the period in which the epilimnion ammonium concentration was above 0 14 mg l 7 the n fixation calculated annually for the entire lake and compared to the kll lake nitrogen balance calculation ninio et al 2020 8 light attenuation in observations 400 700 nm measured every 2 weeks by the kinneret monitoring program in μein m2 sec and in the model short wave radiation provided in w m2 were calculated as radiation relative to surface radiation 9 sedimentation rate in the model was compared to the measured rates based on four sediment traps in the lake ostrovsky et al 2014 10 community respiration cr and 11 primary production compared to fortnightly measurements in the lake berman et al 2014 see appendix f for further details regarding process validation variable calculations 2 8 system validation level 3 system patterns were identified by finding significant relationships in observed data fifteen such relationships expressed as linear models that had an r2 0 25 were considered emergent properties of the system and were used for system validation table 2 these emergent properties were validated by comparing the simulated results to the lake based data using their r2 and slope 3 results model calibration and validation results are presented according to the four validation level csps framework hipsey et al 2020 3 1 conceptual validation level 0 conceptual validation can mainly be attributed to janse 1990 umlauf et al 2012 hu et al 2016 which detail in their studies the mathematics of gotm and of fabm pclake and provide some testing these studies answer most of the questions set forth by hipsey et al 2020 table 3 3 2 state validation level 1 following calibration the model was run for the validation period feb 2014 to jan 2020 the model successfully reproduced the stratification cycle yielding high fit measures of water column temperature in both calibration and validation periods in accordance cycles of do hypolimnion po4 3 nh4 and no3 were reproduced successfully as well as the seasonal phytoplankton groups dinoflagellate and n fixing cyanobacteria dino nfixcyano table 4 fig 1 fig 4 apart from a few exceptions the calibration period yielded better results than the validation period for example kge values for microcytes cyanomc were 0 02 for calibration and 1 27 for validation table 4 model accuracy declines with model level i e accuracy of hydrodynamic abiotic biotic table 5 there was an excellent agreement between observed and simulated epilimnetic water temperature fig 1a relative errors were higher for the hypolimnion where the model overestimated the temperature rise during summer however errors were less than 1 c hypolimnion dissolved oxygen was modeled with excellent accuracy fig 1d which is important since it controls p release from sediment and the nitrification rate golterman 2004 epilimnion oxygen was underestimated particularly since 2013 the accumulation of po4 3 in low oxygen conditions was simulated with good accuracy fig 2 b epilimnion po4 3 concentration is one order of magnitude lower than hypolimnion and the model simulates this well it was not expected to simulate the fluctuations in epilimnion po4 3 accurately since the values are close to detection limit and p from dust is a significant source while it was modeled as a uniform source the fact that epilimnion po4 3 is kept close to a constant minimum suggests that p is a limiting factor for the phytoplankton the model successfully captured the seasonality of the eplimnetic and hypolimnetic concentrations of nh4 however the model underestimated nh4 values in the epilimnion and overestimated them in the hypolimnion fig 2c and d nitrate simulation was good despite the simplified modeling of nitrification and denitrification processes in addition to the inaccuracy in nh4 values fig 2e and f predicted summer no3 concentration was 0 1 mg l higher than observation this might affect phytoplankton simulation as nitrogen can be a limiting nutrient during summer the model successfully simulated the biomass and seasonal dynamics of the planktonic groups the dinoflagellate group simulation was good and captured nicely the year to year variation fig 3 a the model had good detection of the n fixing cyanobacteria summer bloom however the model was unable to follow the year to year variability and the summer abrupt growth of the cyanobacteria the overall biomass and variability of the green phytoplankton group were simulated correctly but fit measures were fair only slightly better than the mean benchmark the microcystis group has a winter bloom which the model simulated only occasionally so calibration fit was fair and validation was unsatisfactory the microzooplankton hindcast was unsatisfactory during calibration and fair during validation period fig 4a it s simulated pattern follows the dinoflagellate biomass as dead phytoplankton becomes particulate organic matter pom which the microzooplankton feed on this pattern does not exist in the observations implying that the process is more complex or feeding preference is different than defined herbivore zooplankton biomass fluctuated significantly the simulations followed the overall biomass and fluctuations but underestimated observations peaks herbivore zooplankton feed on the fairly simulated green phytoplankton which may explain the fair model fit of this group the predatory zooplankton had clear peaks in the spring the simulation followed this pattern but the peak occurred one month earlier than in the observation data which resulted in fair fit a possible cause for the fair fit is that predatory zooplankton feed on micro zooplankton and herbivore zooplankton which were both simulated only fairly to evaluate overall model performance fit measures were compared to 33 studies of non polymictic lakes that published model fit measures independent of unit fit measures namely nmae and r2 and occasionally nse were used for the comparison fig 5 fig 6 results showed that model fit of this study was usually within the range that other studies achieved although nmae values of temperature and dissolved oxygen were significantly higher than equivalent results in other studies r2 and nse for these variables were at least as good as other results many studies used chlorophyll a variable which was not used in this study but is presented here for additional comparison to the phytoplankton biomass fig 6 appendix g provides details and references of the comparison 3 3 process validation level 2 comparison between model and observations was made for fluxes and characteristics for which the model was not calibrated model rates were generally within the observation distribution showing that the model simulated the processes correctly comparing paired results per date fit results were poor r2 0 19 suggesting that year to year variations were poorly simulated eleven characteristics were analyzed by non paired results table 1 model results fall within the observed distribution of turnover day and beginning of stratification fig 7 however the model predicted that turnover day is about a week earlier than observed hypolimnion oxygen depletion rate was well simulated fig 8 b model hypolimnion po4 3 accumulation rate had the same mean rate as the observed rate fig 8c but data distribution was much larger in the observations than the model because po4 3 observations were variable the simulated hypolimnion nh4 accumulation rate fig 8d was higher than observed in contrast to model epilimnion high nh4 period which was on average 17 days shorter than the observations fig 8f model hypolimnion no3 depletion rate was slightly slower than the observations fig 8e these results may suggest that the nitrogen cycle processes are not well simulated yet the nitrogen fixation rate was successfully simulated fig 8g model community respiration and primary production were very similar to observations fig 8h and i relative light attenuation results implies that lake kinneret is more turbid than the model suggests fig 8a a hint for the source of discrepancy was found in the model sedimentation rates that were usually lower than observations sedimentation rate at stn a had similar values to the model at 45 m depth an average of 4 g m 2 day 1 however sedimentation traps at shallower depths indicated higher sedimentation rates than simulated the model showed overall negative sedimentation and specifically at depths shallower than 33 m in contrast to observations missing particulate matter in the model is a possible cause for its low performance regarding light attenuation and sedimentation 3 4 system validation level 3 fifteen relations expressed as linear models that had r2 0 25 were considered emergent properties of the system and were used for the system validation table 6 the relationship linear model s p value both for the observations and for model results were all 0 001 the results show that the model captured 14 emergent properties correctly and one disagreement slope in the opposite sign as observation slope since there are almost no precedent to such rigorous validation of lake models we have no basis for comparison of our results to other cases 4 discussion we have applied the csps validation framework hipsey et al 2020 to a wet lake kinneret ecosystem model this is the second application of the csps framework aiming to gain improved confidence in model predictions inspecting 11 processes and 15 emergent properties compared to 2 3 by robson et al 2020 taken together the four tiers of validation of the csps framework conceptual state process and system validation successfully identified model properties that were not detected during calibration and the conventional state validation the use of csps also pointed to model modifications resulting in increased model reliability and accuracy process validation showed that most processes 9 out of 11 are correctly simulated with some weaknesses revealed discussed hereafter the system validation revealed that most of the emergent properties inspected were adequately simulated by the model 14 out of 15 the results of the 4 level validation translate to high confidence in the model ability to simulate lake kinneret ecosystem however special attention should be given to the model weaknesses in particular the shortage of particles in the water column 4 1 data issue data needed for modeling can be divided into three classes i forcing data ii parameter values and iii state variables csps validation can indicate issues in forcing data e g missing particles in the jordan river and of issues with parameters values e g light attenuation parameters however data for state variables are used as the minimizing function for both calibration and validation and therefore cannot directly point to doubts with these data this includes the higher level validation as data for process and system validation will typically be derived from state variables state variable observations can suffer from reliability accuracy errors and availability these two factors impact model verification at all levels li and wu 2006 yates et al 2018 reliability of observed state variables is very difficult to assess and thus affect model reliability availability of observed state variables is imperative for validation coreau et al 2010 yates et al 2018 validation over a long period can minimize data reliability issues for that reason observation availability is of great importance limited state variable data makes validation difficult however bennett et al 2013 suggest some guidelines to cope with low data availability while csps level 1 validation requires an independent dataset relative to calibration levels 2 3 can be applied on the calibration dataset and hence csps does not inherently require more data with poor data calibration is questionable and validation cannot be carried out resulting in low confidence in model results refsgaard et al 2006 houlahan et al 2017 poor csps validation results can however indicate where monitoring programs should be improved 4 2 prediction ability declines from abiotic to biotic variables reduced prediction accuracy of the biotic components relative to the abiotic variables as shown in table 5 is anticipated arhonditsis and brett 2004 soares and calijuri 2021 and caused by three factors 1 model error propagates through the causality chain rigosi and rueda 2012 the hydrodynamic lake process controls the stratification which in turn controls the dissolved oxygen ladwig et al 2021 oxygen controls the n and p cycles and those control the phytoplankton which in turn control the zooplankton 2 uncertainty increases with model complexity i e increased number of parameters puy et al 2022 and 3 higher process complexity requires increased simplification of the process description in the model which results in model errors simulated biotic processes are more complex than abiotic processes robson 2014 have more parameters and most probably contain more simplifications one notable simplification is the phytoplankton grouping that contains a number of different species with different properties shimoda and arhonditsis 2016 phytoplankton groups that contain one main species with a clear seasonal bloom is simulated much better than a group that contains many species for example the dinoflagellate group simulation was good thanks to the micronutrient attribute used and also because it consists mainly of one species peridinium gatunense the green phytoplankton group on the other hand is combined of various species and hence modeling cannot capture specific species characteristics such as temperature preference splitting groups to contain one main species may improve accuracy but involves the high cost of parameter calibration and the uncertainty involved as each phytoplankton group contains over 20 parameters 4 3 performance of the wet lake kinneret ecosystem model 4 3 1 model structure conceptual validation revisits the model structure and configuration used ensuring that the model components are consistent with ecological theory the extended use of wet and its components gotm fabm pclake in lake ecosystems modeling reinforces the choice in wet and highlights domains and parameters where attention should be focused such as the n cycle 4 3 2 model strength process validation shows that the model simulates the main lake processes very well stratification start and end day po4 3 accumulation rate o2 depletion rate n fixation and cr all had mean values withing the 75 quantile of the observed values moreover 12 out of 15 emergent properties were in good agreement with observations this demonstrates that system properties are simulated adequately despite not being directly coded to simulate many of the emergent properties possibly emanate from seasonal relations between variables indicating that seasonal processes were also nicely depicted by the model comparing wet lake kinneret model calibration to other studies of lake model calibrations showed similar results state validation showed somewhat poorer results compared to calibration period having 60 of the variable fit measures displaying better calibration results that might suggest over fitting of the calibration period janssen et al 2015 comparing calibration and state validation results across studies is difficult since model fit measures vary considerably soares and calijuri 2021 we encourage researchers to use several fit measures rather than a single measure in order to facilitate future benchmarking in particular we recommend the model efficiency fit measures nse and kge mizukami et al 2019 and newer developed efficiency fit measures frequently used in hydrology lee and choi 2022 4 3 3 model weakness 4 3 3 1 nitrogen cycle the mean value of nh4 accumulation rate no3 depletion rate and high nh4 period were outside the 75 quantile of the observed values suggesting that the modeling of nitrogen processes is imperfect this corresponds to the conceptual validation stating that the complex n cycle is simplified some studies found that pclake was overestimating epilimnion nh4 bucak et al 2018 andersen et al 2020 chou et al 2021 in contrast in our study calibration of hypolimnion nh4 to match observations resulted in underestimation of epilimnion nh4 result not shown poorly simulated n cycle might produce poor results for phytoplankton for example simulated summer no3 concentration is higher than the observed value resulting in modeled n fixing cyanobacteria losing their advantage of thriving during low n concentration periods damaging its correct simulation 4 3 3 2 light attenuation and particles validation results of both light attenuation and sedimentation rate suggest that the model lacks a considerable mass of pom that obstruct light and then settle at the lake bottom underestimation of microzooplankton feeding on pom and low community respiration pom mineralization support this suggestion the early turnover day prediction caused by high hypolimnion temperatures might also be attributed the light attenuation discrepancy possible explanations of this gap between model and observations could be a high decomposition rate of pom b underestimation of particle flow from two sources the jordan stream rom et al 2014 and dust deposition on the lake gross et al 2020 c low phytoplankton growth rate and low mortality may maintain low phytoplankton biomass which translates into low pom when phytoplankton dies d underestimation of resuspension due to internal waves caused by the afternoon strong winds and e spatial processes that are not reflected in the one dimensional model the source for this discrepancy should be further investigated 4 4 model hidden flaws knowledge of model parameter values is sparse and in most cases the values are difficult to measure and quantify resulting in parameter uncertainty which is a major challenge when apply multi parameter models robson et al 2018 changing parameter values during calibration may result in high fit to observation but not necessarily for the right reasons oreskes et al 1994 this has driven the employment of a number of methods to minimize the impact such as sensitivity and uncertainty analyses makler pick et al 2011 razavi et al 2021 ensemble modeling trolle et al 2014 spence et al 2020 single model ensemble modeling gal et al 2014 the csps validation process also allow identification of such occurrences for example temperature profiles were simulated with high accuracy despite the light attenuation discrepancy epilimnion nh4 had a good fit despite of hypolimnion nh4 disagreement in contrast agreement in the positive relationship between micro zooplankton and predatory zooplankton despite their unsatisfactory state validation results demonstrate the opposite notion that even if prediction is poor processes and trends can be simulated correctly predator prey relations in this example identifying these hidden flaws e g temperature profile being right for the wrong reason was facilitated by the csps validation resulting in increased confidence that most model results are right for the right reasons and some need further attention 5 conclusions here we demonstrated that the use of the csps validation is superior to the commonly used state validation as it includes and expands upon the state validation and enables identification of hidden flaws and weaknesses it can strengthen the confidence in the model since it tests whether most of the processes and emergent properties are captured correctly the csps validation also revealed some deficiencies and limitations of the model not detectable otherwise it is not expected that models are 100 accurate since they are simplifications of a complex reality yet models are tools to explore future scenarios for specific objectives and thus we must increase their reliability as we have shown csps validation approach increases model reliability the answer to the question should the model be modified because of the validation results depends on the research objectives can the model answer the questions at hand given the simulation ability revealed by the validation in our case study model uncertainty was particularly large in the biotic variables the results suggest that the model had low particle concentration that might affect a number of model properties and further calibration should be done this conclusion would have been impossible without the process validation we conclude that higher validation levels are necessary in order to receive reliable results when running process based lake models on future scenarios declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors gratefully thank the wet team and specifically tobias k andersen for his support and helpful comments to the manuscript and to anders nielsen for his support we thank kinneret limnological laboratory kll staff for their continues effort running the monitoring program this research was supported by grants from the israeli ministry of science and technology appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105637 
25427,lake ecosystem models are essential for capturing ecosystem response to climate change and for studying eutrophication model ability to capture ecosystem dynamics is limited for various reasons aiming to increase confidence in simulations of future scenarios the current study adapts a recently established four level validation framework named csps conceptual state process and system validation lake ecosystem model wet was calibrated for lake kinneret and validated using the csps framework inspecting 18 state variables 11 processes and 15 emergent properties validation showed that most processes and emergent properties were correctly simulated with some weaknesses revealed such as light attenuation and particle dynamics modifying the model to overcome those weaknesses will result in increased model reliability this conclusion would have been impossible without the process validation we conclude that higher validation levels are necessary to receive reliable results when running process based lake models on future scenarios keywords lake ecosystem modeling model evaluation csps wet lake kinneret abbreviations csps conceptual state process and system validation data availability data will be made available on request software and data availability the 1d hydrodynamic ecological model wet v1 0 was used wet is a public domain model that is freely available at https projects au dk wet the complete fortran source code for wet is also available through gitlab https gitlab com wet last accessed may 8 2022 the parallel sensitivity and auto calibration parsac tool was developed by bolding bruggeman as a designated python package parsac version 0 5 5 bruggeman and bolding 2020 was applied in this study and is freely available via the python package manager pip install parsac user from pypi org 1 introduction ecosystem models are an essential tool for capturing ecosystem transformation in response to climate change thuiller et al 2008 soares and calijuri 2021 ecosystem models provide us with a deeper scientific understanding of ecosystem behavior trolle et al 2012 and therefore can be a pivotal tool for identifying and supporting management actions lake models have been used to support restoration actions from eutrophication brett et al 2016 cui et al 2016 allan 2018 ladwig et al 2018 and for maintaining fisheries kumar et al 2016 ofir et al 2017 natugonza et al 2019 lake models can provide information on the potential impacts of climate change and how to best mitigate those impacts koenigstein et al 2016 rolighed et al 2016 ladwig et al 2018 bucak et al 2018 gal et al 2020 heneghan et al 2021 and for assessing management actions to maintain the lake ecosystem services gal et al 2009 ofir et al 2017 lewis et al 2020 lake ecosystem models can also be used for near real time prediction to assist in immediate management decisions such as lake bathing suspension halt drinking water consumption and actions to mitigate pollution events janssen et al 2015 robson et al 2017 prediction is very difficult especially if it s about the future danish proverb highlights the fact that large benefits associated with model predictions come with considerable challenges many researchers use diminishing expressions like all models are wrong works of fiction or based on properties of convenience to describe the problematic nature of models although these are extreme viewpoints they emphasize the difficulties that characterizes modeling efforts box g e p 1979 arhonditsis and brett 2004 and subsequent inaccuracy in model results model inaccuracy is a consequence of a number of factors including gaps and errors in forcing and calibration data parameters uncertainty numerical approximation and most importantly models are but a simplification of natural processes refsgaard et al 2007 yet even simplified hydrodynamic ecological lake models are complex containing hundreds of parameters that require calibration for the specific lake in question vinçon leite and casenave 2019 calibration is conducted by running the model on existing forcing data and minimizing the result deviation from existing observations measured by various fit measures bennett et al 2013 our confidence is built on model calibration in reference to historical time periods for which meteorological conditions basin inflows and outflows are available along with lake based observations and measurements however can we expect models to correctly simulate the system dynamic response by subjecting it to external drivers beyond the range of data for which the model was calibrated our confidence in the model ability to capture ecosystem dynamics and shifts is limited yet it is at the heart of our expectation from models hipsey et al 2015 to extend the confidence in model performance validation is conducted by testing model behavior most commonly the validation process consists of extending the calibration period by running the model with independent forcing data and testing whether model performance holds using the same parameter values and evaluated based on same fit measures arhonditsis and brett 2004 this type of validation is termed state validation as simulated state variables are compared to the lake based measurements of the same state variables hipsey et al 2020 however models can be right for the wrong reasons robson et al 2017 and if that s the case it might fail under conditions other than those for which it was calibrated and validated bennett et al 2013 brett et al 2016 vinçon leite and casenave 2019 mesman et al 2020 this is particularly important when using lake ecosystem models to evaluate long term climate change scenarios that often exceed historical conditions in addition can the models simulate processes and variables for which they were not calibrated and still maintain reasonable accuracy given the increasing dependency on models as part of the decision making process we need means to better evaluate models minimize model inaccuracy and uncertainty thereby providing more reliable models for decision makers in order to approach the issue of increased reliability and accuracy of models hipsey et al 2020 established a framework for the hierarchical assessment of models named csps conceptual state process and system validation the csps framework expands the more traditional evaluation of state variable predictions by evaluating model predictions at four levels 0 conceptual validation ensures that the model is consistent with ecological theory 1 state validation compares simulated state variables with observations 2 process validation compares fluxes with measured process rates and 3 system validation evaluates system level emergent properties patterns and relationships the csps framework can reveal model weaknesses that would be otherwise overlooked level 1 is the widely used validation conducted when first applying a model to a lake ecosystem level 2 asks if the biogeochemical processes are correctly modeled finally level 3 concerns general system properties that the model does not address explicitly successful validation of the higher levels produces a model that is more likely to follow the processes and system properties even when subjected to long term scenarios these include scenarios beyond the range of forcing conditions for which it was developed and trained for these additional perspectives disclose instances in which model is right for the wrong reasons another objective of the csps is setting a common framework for model assessment to facilitate benchmarking and synthesis of aquatic ecosystem models the csps approach was implemented for the first time by robson et al 2020 their study highlights both strengths of processes and system properties that the model was able to simulate and weaknesses that prompt additional model parameter modification climate change may result in scenarios outside the conditions that the model was calibrated for implementing the csps validation conclusions accounting for the weaknesses revealed will enhance the reliability of model predictions and enable running the model on future scenarios with greater confidence lake kinneret plays a pivotal role in israel s water supply system and is also a unique site for recreational religious and cultural services as a result an extensive monitoring program has been in place for over 50 years zohary et al 2014 this unique database provides means for applying and testing the csps validation framework with the intention of predicting climate change impacts on lake kinneret a gotm wet model umlauf et al 2012 schnedler meyer et al 2022 for lake kinneret was constructed aiming to increase confidence in future scenario simulations the current study continues the adoption of the csps framework suggested by hipsey et al 2020 strengths and weaknesses of the wet lake kinneret ecosystem model was identified by the csps validation consequently model adjustments should be made to overcome model shortcomings the end result will be a model that can be used to predict future trends of the lake ecosystem with higher confidence when subjected to long term climate change scenarios 2 methods 2 1 study site lake kinneret sea of galilee is a warm monomictic lake situated in northern israel it is the only natural freshwater lake in israel and as such is a unique ecosystem that provides a series of critical ecosystem services including drinking water fisheries and recreation as well as cultural and religious significance the lake s maximum depth is 44 m its mean depth is 24 m and its surface area is 168 km2 the lake water level has a natural annual fluctuation of approximately 1 8 m but fluctuations of over 6 2 m were recorded since the 1990 s around a mean level of 211 2 i e below sea level with increases in water level in the winter spring reaching up to 4 m gal et al 2013 the lake is stratified about 10 months a year march january with an anoxic hypolimnion the kinneret limnological laboratory kll has been conducting an extensive routine monitoring program since 1969 comprising all major physical biological and chemical variables we refer the reader to zohary et al 2014 for additional information on the lake ecosystem 2 2 model description the lake ecosystem model gotm wet schnedler meyer et al 2022 is comprised of a hydrodynamical model gotm coupled to a biogeochemical model wet the general ocean turbulence model gotm is a one dimensional water column model covering the key hydrodynamic and thermodynamic processes related to vertical mixing in natural waters umlauf et al 2012 the main controlling equations of gotm are transport of momentum and heat the gotm version applied within the gotm wet is adapted to lakes where all layers include not only the pelagic volume but also the sediment area drawn from the lake specific hypsography chou et al 2021 the water ecosystem tool wet schnedler meyer et al 2022 is a re development on fabm pclake hu et al 2016 which is a one dimensional biogeochemical model originally based on a zero dimensional shallow lake model pclake janse 1990 the model is based on closed nitrogen and phosphorus cycles across the biotic and a biotic lake components wet is coupled to gotm by the framework of biogeochemical models fabm bruggeman and bolding 2014 and is referred simply as wet hereafter as with other lake ecosystem models the application of wet to a new lake ecosystem requires intensive calibration of parameters vinçon leite and casenave 2019 2 3 model setup the setup and construction of wet requires several configuration predefinitions the model was constructed with an hourly time step and included 50 horizontal depth layers varying in thickness according to lake level lake level was provided as input to the model the biotic structure included five phytoplankton and three zooplankton groups similar to those used in previous model set up for the lake gal et al 2009 the phytoplankton groups were configured according to their different nature and seasonal growth 1 dinoflagellates dino mainly representing peridinium gatunense the most dominant taxa until the mid 1990 s 2 nitrogen fixing cyanobacteria nfixcyano mainly representing aphanizomenon oval and cylindrospermopsis spp 3 microcystis spp cyanomc 4 other cyanobacteria cyano and 5 all the rest of the phytoplankton green largely representing aulacoseira granulata and mougeotia spp among others zooplankton were configured by functional groups according to gal et al 2009 1 herbivorous zooplankton herbzoo 2 microzooplankton microzoo and 3 predatory zooplankton predzoo special attention in the model configuration was given to the dinoflagellate p gatunense because it was the lake s most abundant phytoplankton but has an irregular cycle since 1995 it s irregular annual appearance has been linked to the lack of a yet unknown unmonitored micronutrient vital for the peridinium growth that appears only in high precipitation years zohary et al 2012 also see appendix a we therefore configured the dino group with a requirement for a micronutrient that was introduced into the jordan river with a positive correlation to streamflow 2 4 model input simulations require meteorological inflow and outflow input forcing data as well as initial conditions for simulated variables hourly meteorological measurements were taken mostly from a meteorological station located at deepest point in the lake stn a and included wind speed air temperature relative humidity barometric pressure and short wave radiation to complete missing measurements from the station data were extracted from land based stations and reanalysis from the era5 land hourly data climatic model muñoz sabater 2019 daily inflows of six streams the jordan river being the main one and one outflow were taken from the israel hydrological service measurements and adjusted to match the water balance provided by the mekorot water company assouline 1993 berger et al 2019 inflow temperature salinity and nutrient concentrations are also measured and were used as daily forcing data daily precipitation measurements were taken from stations located near the lake shore and verified to match the monthly water balance hypsographic data and daily lake levels were taken from mekorot measurements some data imputation and adjustment of forcing data were done to address missing data and different data sources described in appendix b table s2 in appendix b contains a list of forcing data boundary conditions the model initial conditions were extracted from the end of a 60 year spin up period ji 2017 this was done to satisfy the non uniform depth distribution of sediment inorganic and organic materials that reach a steady state only after about a 60 year model run as determined from preliminary simulations we conducted see appendix c 2 5 data for model calibration and validation in order to evaluate the results of the lake kinneret wet model we conducted a time series comparison of data collected as part of the kll monitoring program sukenik et al 2014 the time series for model calibration was set to the period of feb 1 2003 to jan 31 2014 with an additional 1 year spin up period to acclimate for the specific 2003 conditions while the validation period was feb 1 2014 to jan 31 2020 we start the simulations in february as the annual lake overturn typically starts during january the time series data used for calibration of state variables measurements at stn a included temperature o2 no3 nh4 po4 3 and plankton phytoplankton and zooplankton monitoring data are provided as biomass and density respectively zooplankton density was converted to biomass according to gophen and azoulay 2002 hambright 2008 blankenbach 2015 and is detailed in appendix d the measurements were collected from various depths at stn a at a sampling frequency ranging from daily temperature weekly chemical variables to 14 days biological variables the model was calibrated using the state variables mentioned above while maintaining a residual stream close to zero where residual stream is the method gotm uses to maintain a correct water balance an auto calibration tool parsac parallel sensitivity and auto calibration tool bruggeman and bolding 2020 was used for the calibration process in tandem with manual calibration about 80 of the parameters were changed from default values as a result of the calibration process appendix e 2 6 conceptual and state validation levels 0 1 the conceptual validation of the wet model ensuring the model is consistent with theory was based on existing literature by answering questions set forth by hipsey et al 2020 in addition to a water balance check the calibration and state validation compared simulated state variables with observations based on several fit measures fit measures for temperature and abiotic variables were analyzed separately for the epilimnion depth 10 and hypolimnion depth 20 while plankton were analyzed for the epilimnion only the fit measures included the following a nmae normalized mean absolute error bennett et al 2013 absolute error relative to the mean ideal value is 0 and range 0 ꝏ sometimes referred to as mare mean absolute relative error b r2 coefficient of determination ideal value is 1 and range 0 1 c nse nash sutcliffe efficiency nash and sutcliffe 1970 also called mef model efficiency normalizes the error to the variance values ranges between negative values to 1 where 1 is a perfect fit 0 is equivalent to the mean benchmark and negative is worse than the mean d kge kling gupta efficiency gupta et al 2009 knoben et al 2019 incorporates 3 fit measures correlation variability bias and mean bias values range between negative values to 1 where 1 is a perfect fit 0 41 is equivalent to the mean benchmark and lower values are worse than the mean for result description simplicity and clarity kge fit measure was used to classify the results into the following classes excellent kge 0 8 good kge 0 3 fair kge 0 41 unsatisfactory kge 0 41 2 7 process validation level 2 in order to conduct the process validation we compared fluxes rates and additional ecosystem characteristics for which the model was not calibrated with observed and calculated corresponding information from lake based measurements eleven such characteristics were analyzed for the entire period feb 2004 jan 2019 table 1 1 stratification days start day end day period we defined a stratified water column when the temperature profile had a stdev average ratio 3 or the vertical temperature decline 1 c per 1 m interval 2 5 nutrient s depletion and accumulation rates in the hypolimnion were calculated as monthly rates when nutrient depletion accumulation took place 6 the period in which the epilimnion ammonium concentration was above 0 14 mg l 7 the n fixation calculated annually for the entire lake and compared to the kll lake nitrogen balance calculation ninio et al 2020 8 light attenuation in observations 400 700 nm measured every 2 weeks by the kinneret monitoring program in μein m2 sec and in the model short wave radiation provided in w m2 were calculated as radiation relative to surface radiation 9 sedimentation rate in the model was compared to the measured rates based on four sediment traps in the lake ostrovsky et al 2014 10 community respiration cr and 11 primary production compared to fortnightly measurements in the lake berman et al 2014 see appendix f for further details regarding process validation variable calculations 2 8 system validation level 3 system patterns were identified by finding significant relationships in observed data fifteen such relationships expressed as linear models that had an r2 0 25 were considered emergent properties of the system and were used for system validation table 2 these emergent properties were validated by comparing the simulated results to the lake based data using their r2 and slope 3 results model calibration and validation results are presented according to the four validation level csps framework hipsey et al 2020 3 1 conceptual validation level 0 conceptual validation can mainly be attributed to janse 1990 umlauf et al 2012 hu et al 2016 which detail in their studies the mathematics of gotm and of fabm pclake and provide some testing these studies answer most of the questions set forth by hipsey et al 2020 table 3 3 2 state validation level 1 following calibration the model was run for the validation period feb 2014 to jan 2020 the model successfully reproduced the stratification cycle yielding high fit measures of water column temperature in both calibration and validation periods in accordance cycles of do hypolimnion po4 3 nh4 and no3 were reproduced successfully as well as the seasonal phytoplankton groups dinoflagellate and n fixing cyanobacteria dino nfixcyano table 4 fig 1 fig 4 apart from a few exceptions the calibration period yielded better results than the validation period for example kge values for microcytes cyanomc were 0 02 for calibration and 1 27 for validation table 4 model accuracy declines with model level i e accuracy of hydrodynamic abiotic biotic table 5 there was an excellent agreement between observed and simulated epilimnetic water temperature fig 1a relative errors were higher for the hypolimnion where the model overestimated the temperature rise during summer however errors were less than 1 c hypolimnion dissolved oxygen was modeled with excellent accuracy fig 1d which is important since it controls p release from sediment and the nitrification rate golterman 2004 epilimnion oxygen was underestimated particularly since 2013 the accumulation of po4 3 in low oxygen conditions was simulated with good accuracy fig 2 b epilimnion po4 3 concentration is one order of magnitude lower than hypolimnion and the model simulates this well it was not expected to simulate the fluctuations in epilimnion po4 3 accurately since the values are close to detection limit and p from dust is a significant source while it was modeled as a uniform source the fact that epilimnion po4 3 is kept close to a constant minimum suggests that p is a limiting factor for the phytoplankton the model successfully captured the seasonality of the eplimnetic and hypolimnetic concentrations of nh4 however the model underestimated nh4 values in the epilimnion and overestimated them in the hypolimnion fig 2c and d nitrate simulation was good despite the simplified modeling of nitrification and denitrification processes in addition to the inaccuracy in nh4 values fig 2e and f predicted summer no3 concentration was 0 1 mg l higher than observation this might affect phytoplankton simulation as nitrogen can be a limiting nutrient during summer the model successfully simulated the biomass and seasonal dynamics of the planktonic groups the dinoflagellate group simulation was good and captured nicely the year to year variation fig 3 a the model had good detection of the n fixing cyanobacteria summer bloom however the model was unable to follow the year to year variability and the summer abrupt growth of the cyanobacteria the overall biomass and variability of the green phytoplankton group were simulated correctly but fit measures were fair only slightly better than the mean benchmark the microcystis group has a winter bloom which the model simulated only occasionally so calibration fit was fair and validation was unsatisfactory the microzooplankton hindcast was unsatisfactory during calibration and fair during validation period fig 4a it s simulated pattern follows the dinoflagellate biomass as dead phytoplankton becomes particulate organic matter pom which the microzooplankton feed on this pattern does not exist in the observations implying that the process is more complex or feeding preference is different than defined herbivore zooplankton biomass fluctuated significantly the simulations followed the overall biomass and fluctuations but underestimated observations peaks herbivore zooplankton feed on the fairly simulated green phytoplankton which may explain the fair model fit of this group the predatory zooplankton had clear peaks in the spring the simulation followed this pattern but the peak occurred one month earlier than in the observation data which resulted in fair fit a possible cause for the fair fit is that predatory zooplankton feed on micro zooplankton and herbivore zooplankton which were both simulated only fairly to evaluate overall model performance fit measures were compared to 33 studies of non polymictic lakes that published model fit measures independent of unit fit measures namely nmae and r2 and occasionally nse were used for the comparison fig 5 fig 6 results showed that model fit of this study was usually within the range that other studies achieved although nmae values of temperature and dissolved oxygen were significantly higher than equivalent results in other studies r2 and nse for these variables were at least as good as other results many studies used chlorophyll a variable which was not used in this study but is presented here for additional comparison to the phytoplankton biomass fig 6 appendix g provides details and references of the comparison 3 3 process validation level 2 comparison between model and observations was made for fluxes and characteristics for which the model was not calibrated model rates were generally within the observation distribution showing that the model simulated the processes correctly comparing paired results per date fit results were poor r2 0 19 suggesting that year to year variations were poorly simulated eleven characteristics were analyzed by non paired results table 1 model results fall within the observed distribution of turnover day and beginning of stratification fig 7 however the model predicted that turnover day is about a week earlier than observed hypolimnion oxygen depletion rate was well simulated fig 8 b model hypolimnion po4 3 accumulation rate had the same mean rate as the observed rate fig 8c but data distribution was much larger in the observations than the model because po4 3 observations were variable the simulated hypolimnion nh4 accumulation rate fig 8d was higher than observed in contrast to model epilimnion high nh4 period which was on average 17 days shorter than the observations fig 8f model hypolimnion no3 depletion rate was slightly slower than the observations fig 8e these results may suggest that the nitrogen cycle processes are not well simulated yet the nitrogen fixation rate was successfully simulated fig 8g model community respiration and primary production were very similar to observations fig 8h and i relative light attenuation results implies that lake kinneret is more turbid than the model suggests fig 8a a hint for the source of discrepancy was found in the model sedimentation rates that were usually lower than observations sedimentation rate at stn a had similar values to the model at 45 m depth an average of 4 g m 2 day 1 however sedimentation traps at shallower depths indicated higher sedimentation rates than simulated the model showed overall negative sedimentation and specifically at depths shallower than 33 m in contrast to observations missing particulate matter in the model is a possible cause for its low performance regarding light attenuation and sedimentation 3 4 system validation level 3 fifteen relations expressed as linear models that had r2 0 25 were considered emergent properties of the system and were used for the system validation table 6 the relationship linear model s p value both for the observations and for model results were all 0 001 the results show that the model captured 14 emergent properties correctly and one disagreement slope in the opposite sign as observation slope since there are almost no precedent to such rigorous validation of lake models we have no basis for comparison of our results to other cases 4 discussion we have applied the csps validation framework hipsey et al 2020 to a wet lake kinneret ecosystem model this is the second application of the csps framework aiming to gain improved confidence in model predictions inspecting 11 processes and 15 emergent properties compared to 2 3 by robson et al 2020 taken together the four tiers of validation of the csps framework conceptual state process and system validation successfully identified model properties that were not detected during calibration and the conventional state validation the use of csps also pointed to model modifications resulting in increased model reliability and accuracy process validation showed that most processes 9 out of 11 are correctly simulated with some weaknesses revealed discussed hereafter the system validation revealed that most of the emergent properties inspected were adequately simulated by the model 14 out of 15 the results of the 4 level validation translate to high confidence in the model ability to simulate lake kinneret ecosystem however special attention should be given to the model weaknesses in particular the shortage of particles in the water column 4 1 data issue data needed for modeling can be divided into three classes i forcing data ii parameter values and iii state variables csps validation can indicate issues in forcing data e g missing particles in the jordan river and of issues with parameters values e g light attenuation parameters however data for state variables are used as the minimizing function for both calibration and validation and therefore cannot directly point to doubts with these data this includes the higher level validation as data for process and system validation will typically be derived from state variables state variable observations can suffer from reliability accuracy errors and availability these two factors impact model verification at all levels li and wu 2006 yates et al 2018 reliability of observed state variables is very difficult to assess and thus affect model reliability availability of observed state variables is imperative for validation coreau et al 2010 yates et al 2018 validation over a long period can minimize data reliability issues for that reason observation availability is of great importance limited state variable data makes validation difficult however bennett et al 2013 suggest some guidelines to cope with low data availability while csps level 1 validation requires an independent dataset relative to calibration levels 2 3 can be applied on the calibration dataset and hence csps does not inherently require more data with poor data calibration is questionable and validation cannot be carried out resulting in low confidence in model results refsgaard et al 2006 houlahan et al 2017 poor csps validation results can however indicate where monitoring programs should be improved 4 2 prediction ability declines from abiotic to biotic variables reduced prediction accuracy of the biotic components relative to the abiotic variables as shown in table 5 is anticipated arhonditsis and brett 2004 soares and calijuri 2021 and caused by three factors 1 model error propagates through the causality chain rigosi and rueda 2012 the hydrodynamic lake process controls the stratification which in turn controls the dissolved oxygen ladwig et al 2021 oxygen controls the n and p cycles and those control the phytoplankton which in turn control the zooplankton 2 uncertainty increases with model complexity i e increased number of parameters puy et al 2022 and 3 higher process complexity requires increased simplification of the process description in the model which results in model errors simulated biotic processes are more complex than abiotic processes robson 2014 have more parameters and most probably contain more simplifications one notable simplification is the phytoplankton grouping that contains a number of different species with different properties shimoda and arhonditsis 2016 phytoplankton groups that contain one main species with a clear seasonal bloom is simulated much better than a group that contains many species for example the dinoflagellate group simulation was good thanks to the micronutrient attribute used and also because it consists mainly of one species peridinium gatunense the green phytoplankton group on the other hand is combined of various species and hence modeling cannot capture specific species characteristics such as temperature preference splitting groups to contain one main species may improve accuracy but involves the high cost of parameter calibration and the uncertainty involved as each phytoplankton group contains over 20 parameters 4 3 performance of the wet lake kinneret ecosystem model 4 3 1 model structure conceptual validation revisits the model structure and configuration used ensuring that the model components are consistent with ecological theory the extended use of wet and its components gotm fabm pclake in lake ecosystems modeling reinforces the choice in wet and highlights domains and parameters where attention should be focused such as the n cycle 4 3 2 model strength process validation shows that the model simulates the main lake processes very well stratification start and end day po4 3 accumulation rate o2 depletion rate n fixation and cr all had mean values withing the 75 quantile of the observed values moreover 12 out of 15 emergent properties were in good agreement with observations this demonstrates that system properties are simulated adequately despite not being directly coded to simulate many of the emergent properties possibly emanate from seasonal relations between variables indicating that seasonal processes were also nicely depicted by the model comparing wet lake kinneret model calibration to other studies of lake model calibrations showed similar results state validation showed somewhat poorer results compared to calibration period having 60 of the variable fit measures displaying better calibration results that might suggest over fitting of the calibration period janssen et al 2015 comparing calibration and state validation results across studies is difficult since model fit measures vary considerably soares and calijuri 2021 we encourage researchers to use several fit measures rather than a single measure in order to facilitate future benchmarking in particular we recommend the model efficiency fit measures nse and kge mizukami et al 2019 and newer developed efficiency fit measures frequently used in hydrology lee and choi 2022 4 3 3 model weakness 4 3 3 1 nitrogen cycle the mean value of nh4 accumulation rate no3 depletion rate and high nh4 period were outside the 75 quantile of the observed values suggesting that the modeling of nitrogen processes is imperfect this corresponds to the conceptual validation stating that the complex n cycle is simplified some studies found that pclake was overestimating epilimnion nh4 bucak et al 2018 andersen et al 2020 chou et al 2021 in contrast in our study calibration of hypolimnion nh4 to match observations resulted in underestimation of epilimnion nh4 result not shown poorly simulated n cycle might produce poor results for phytoplankton for example simulated summer no3 concentration is higher than the observed value resulting in modeled n fixing cyanobacteria losing their advantage of thriving during low n concentration periods damaging its correct simulation 4 3 3 2 light attenuation and particles validation results of both light attenuation and sedimentation rate suggest that the model lacks a considerable mass of pom that obstruct light and then settle at the lake bottom underestimation of microzooplankton feeding on pom and low community respiration pom mineralization support this suggestion the early turnover day prediction caused by high hypolimnion temperatures might also be attributed the light attenuation discrepancy possible explanations of this gap between model and observations could be a high decomposition rate of pom b underestimation of particle flow from two sources the jordan stream rom et al 2014 and dust deposition on the lake gross et al 2020 c low phytoplankton growth rate and low mortality may maintain low phytoplankton biomass which translates into low pom when phytoplankton dies d underestimation of resuspension due to internal waves caused by the afternoon strong winds and e spatial processes that are not reflected in the one dimensional model the source for this discrepancy should be further investigated 4 4 model hidden flaws knowledge of model parameter values is sparse and in most cases the values are difficult to measure and quantify resulting in parameter uncertainty which is a major challenge when apply multi parameter models robson et al 2018 changing parameter values during calibration may result in high fit to observation but not necessarily for the right reasons oreskes et al 1994 this has driven the employment of a number of methods to minimize the impact such as sensitivity and uncertainty analyses makler pick et al 2011 razavi et al 2021 ensemble modeling trolle et al 2014 spence et al 2020 single model ensemble modeling gal et al 2014 the csps validation process also allow identification of such occurrences for example temperature profiles were simulated with high accuracy despite the light attenuation discrepancy epilimnion nh4 had a good fit despite of hypolimnion nh4 disagreement in contrast agreement in the positive relationship between micro zooplankton and predatory zooplankton despite their unsatisfactory state validation results demonstrate the opposite notion that even if prediction is poor processes and trends can be simulated correctly predator prey relations in this example identifying these hidden flaws e g temperature profile being right for the wrong reason was facilitated by the csps validation resulting in increased confidence that most model results are right for the right reasons and some need further attention 5 conclusions here we demonstrated that the use of the csps validation is superior to the commonly used state validation as it includes and expands upon the state validation and enables identification of hidden flaws and weaknesses it can strengthen the confidence in the model since it tests whether most of the processes and emergent properties are captured correctly the csps validation also revealed some deficiencies and limitations of the model not detectable otherwise it is not expected that models are 100 accurate since they are simplifications of a complex reality yet models are tools to explore future scenarios for specific objectives and thus we must increase their reliability as we have shown csps validation approach increases model reliability the answer to the question should the model be modified because of the validation results depends on the research objectives can the model answer the questions at hand given the simulation ability revealed by the validation in our case study model uncertainty was particularly large in the biotic variables the results suggest that the model had low particle concentration that might affect a number of model properties and further calibration should be done this conclusion would have been impossible without the process validation we conclude that higher validation levels are necessary in order to receive reliable results when running process based lake models on future scenarios declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors gratefully thank the wet team and specifically tobias k andersen for his support and helpful comments to the manuscript and to anders nielsen for his support we thank kinneret limnological laboratory kll staff for their continues effort running the monitoring program this research was supported by grants from the israeli ministry of science and technology appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105637 
25428,complex environmental optimization problems often require computationally expensive simulation models to assess candidate solutions however the complexity of response surfaces necessitates multiple such assessments and thus renders the search procedure too laborious surrogate based optimization is a powerful approach for accelerating convergence towards promising solutions here we introduce the adaptive multi surrogate enhanced evolutionary annealing simplex amseeas algorithm as an extension of its precursor seeas which is a single surrogate based optimization method amseeas exploits the strengths of multiple surrogate models that are combined via a roulette type mechanism for selecting a specific metamodel to be activated in every iteration amseeas proves its robustness and efficiency via extensive benchmarking against seeas and other state of the art surrogate based global optimization methods in both theoretical mathematical problems and in a computationally demanding real world hydraulic design application the latter seeks for cost effective sizing of levees along a drainage channel to minimize flood inundation calculated by the time expensive hydrodynamic model hec ras keywords surrogate modeling machine learning high dimensional expensive black box heb problems evolutionary annealing simplex test functions hydraulic design software and data availability a python implementation of eas seeas and amseeas and the rest of our work are available online at https github com spyrostsat amseeas 1 introduction simulation models of detailed spatial and temporal resolution have a pivotal role in environmental sciences also gaining increasing popularity in engineering practice such models provide the capability to represent complex physical phenomena accounting for the spatiotemporal dynamics of all processes of interest as well to describe their interactions with infrastructures and societal factors their utility is further enhanced when these are coupled with optimization methods maier et al 2014 at a conceptual level combined simulation optimization schemes tsoukalas et al 2016 can be employed to address both decision making applications e g optimal design planning management and real time control of environmental systems and inverse modeling problems as well aiming to identify optimal model configurations so that the observed responses are faithfully represented as their name suggests these use simulation models to evaluate the system s performance expressed in terms of an objective function of a nonlinear global optimization problem with no analytical solution or derivative information the literature is particularly rich in such efforts and advances which have been summarized in the review works of labadie 2004 fowler et al 2008 nicklow et al 2010 reed et al 2013 ahmad et al 2014 and kumar and yadav 2022 which emphasize on water resources management as well as by duan 2003 and efstratiadis and koutsoyiannis 2010 which focus on hydrological calibration the major obstacle encountered in model based optimization problems is the required computational workload which is dictated by the computational cost e g time of the underlying simulation model state of the art simulation models have the ability to describe the peculiarities of environmental systems with great accuracy and detail e g in terms of geometry boundary conditions and spatiotemporal dynamics yet this comes at a price another common category of time demanding models involves stochastic simulation schemes that are driven by long synthetic data in order to quantify their probabilistic performance with satisfactory accuracy in all these cases the computational time for a single model execution may require from a few minutes up to several hours on the other hand depending on the problem dimensionality and the irregularity of the response surface a typical global optimization algorithm may need to evaluate the objective function and hence call the simulation model thousands of times in order to converge to a satisfactory solution these two issues combined result in termination of the optimization procedure after days or even weeks which makes the optimization task infeasible and sometimes even prohibitive among the different strategies proposed by the research community e g razavi et al 2010 to deal with the time issue including parallel computing computationally efficient search algorithms opportunistic avoidance of model evaluations a particularly interesting one with proven effectiveness is the use of surrogate modeling techniques surrogate models also known as meta models offer an elegant software based solution where low computational cost approximation models are used to represent the actual and time expensive simulation model s response surface and thus guide the optimization procedure in particular the surrogates aim to aid the search procedure by replacing the actual simulation model to some extent and proposing promising solutions that will possibly lead to a much faster convergence of the optimization under this premise the simulation procedure is called in limited cases in order to evaluate the objective function wherever suggested by the surrogate model from a historical perspective one of the first works that popularized such approaches is attributed to jones et al 1998 who introduced the so called efficient global optimization ego ego embeds a kriging i e gaussian process method as a surrogate model to the core of the optimization procedure among others this has the role of locating potentially good solutions that are worth evaluating through the actual simulation model after the seminal publication of ego several optimization schemes incorporated the idea of using surrogate modeling techniques thus resulting to a vast list of algorithms where numerous alternative machine learning models are employed as surrogate models such as polynomials radial basis functions rbfs random forests rfs support vector machines svms artificial neural networks anns etc e g müller et al 2013 liu et al 2014 golzari et al 2015 mallipeddi and lee 2015 awad et al 2018 the use of surrogate assisted approaches spans over multiple types of optimization problems also including discrete constrained and multiobjective optimization emphasis was also given to time demanding problems with many control variables also referred to as high dimensional expensive black box heb problems running advances for heb problems include the so called knowledge transfer assisted efficient global optimization kt ego algorithm which extends the classical version of ego to handle high i e 20 variables dimensions wang et al 2022 other recently published methods aim at providing hybrid schemes that combine different search and or surrogate strategies for instance dong et al 2018a introduced the multi surrogate based differential evolution with multi start exploration mdeme algorithm which uses differential evolution enhanced by three surrogate models i e kriging radial basis function quadratic polynomial response while pan et al 2021 proposed the so called surrogate assisted hybrid optimization saho that employs two different optimization techniques i e teaching learning based optimization differential evolution combined with rbf metamodels the idea of surrogate based optimization has also found fertile ground in the domain of water resources and the environment e g wang et al 2014 tsoukalas and makropoulos 2015a tsoukalas and makropoulos 2015b shaw et al 2017 xia et al 2021 lu et al 2022 starting from the pioneering work by regis and shoemaker 2004 since then this approach managed to address real world optimization problems of significant complexity for instance yazdi and salehi neyshabouri 2014 introduced a framework to solve high dimensional problems for optimizing flood control detention dams wu et al 2015 proposed the so called surrogate based optimization for integrated surface water groundwater modeling soim algorithm for water management optimization problems xi et al 2017 proposed a surrogate assisted approach to efficiently calibrate agricultural hydrological models on a limited budget recently sun et al 2022 developed the so called multi objective adaptive surrogate modelling based optimization for constrained hybrid problems mo asmoch which is designed to handle problems consisting of both continuous and discrete control variables the focus of this research is to provide improved surrogate based solutions for handling time demanding global optimization problems where term global is used to denote nonlinear single objective unconstrained optimization problems with continuous variables this kind of problems is very common in the domain of water resources and the environment where the objective function is typically defined through a computationally expensive simulation model and the resulting response surface is in general multimodal for convenience we also consider that the problem is configured in single objective terms meaning that the system s performance is expressed via an overall scalar metric and more specifically by means of a cost function to minimize we highlight that this overall metric may aggregate several criteria potentially conflicting and embed as well few external constraints by means of penalties on the other hand the vast majority of constraints that are associated with internal modeling procedures e g description of physical processes are exclusively handled through the underlying simulation model thus the optimization problem is by definition formalized as unconstrained regarding the search space also known as decision or feasible space it is formalized as a hypervolume by assigning lower and upper boundaries to the problem s variables according to the problem type these may be referred to as control variables decision variables design variables or parameters table 1 contains a list of state of the art i e published during the last decade surrogate based algorithms for the case of our interest i e global optimization it is interesting to notice that very few of them are publicly available while only three utilize more than one surrogates across the exploration exploitation procedure in an attempt to fill this gap we introduce and provide as open source software in python environment the so called adaptive multi surrogate enhanced evolutionary annealing simplex amseeas algorithm its key novelty is the use of multiple surrogates that cooperate to enable significant improvements across the search space as the search evolves the most effective surrogates are applied more frequently on the basis of a self adaptive probabilistic selection scheme amseeas builds upon two existing optimization schemes i e the surrogate enhanced evolutionary annealing simplex seeas by tsoukalas et al 2016 and the evolutionary annealing simplex eas by efstratiadis and koutsoyiannis 2002 a brief overview of eas and seeas and a more detailed description of amseeas are given in section 2 to evaluate its effectiveness and efficiency the proposed algorithm is thoroughly benchmarked against its forerunner i e seeas as well as other state of the art surrogate based global optimization algorithms the comparison is realized initially via testing all algorithms on a set of mathematical test functions with complex response surfaces and multiple local optima section 3 furthermore to evaluate the proposed algorithm s performance on a real world problem of significant difficulty amseeas is tested on a hydraulic design problem where the evaluation of the cost function requires the use of a time expensive hydrodynamic model section 4 the overall analyses illustrate the advantages of amseeas in terms of providing systematically better solutions under a limited computational budget 2 optimization methodology 2 1 evolutionary annealing simplex eas is a heuristic global optimization algorithm developed by efstratiadis and koutsoyiannis 2002 its main rationale is finding an effective way to combine the strengths of the downhill simplex local optimization method nelder and meadf 1965 with simulated annealing kirkpatrick et al 1983 also incorporating fundamentals of evolutionary algorithms namely the concepts of an evolving population and the genetic operators ryan 2003 in this respect it combines the flexibility of simulated annealing to escape from local optima with the ability of the nelder mead method to locate areas of attraction quickly and accurately this is accomplished through the introduction of a temperature variable t which determines the randomness assigned to the search procedure at early stages temperature is desired to have large values thus making the system warm so that randomness can play a major role to favor the exploration across the entire feasible space in contrast as the search evolves the algorithm is capable of finding areas of attraction and the system gets colder since its temperature decreases thus the search becomes more deterministic and exploitation can begin at each iteration cycle the generation of new solutions is realized by randomly selecting from the population so far sub sets of n 1 points in the n dimensional search space thus each sub set defines a simplex and employing appropriate geometrical transformations in order to determine the simplex vertex to be replaced the associated population members are not being compared exclusively by their objective function value but a randomness term related to the current system temperature is added in early iterations randomness is often the most crucial component in the comparison and thus the solution being chosen for replacement may not be the actually worst simplex vertex nonetheless the best vertex i e the solution with the lowest objective function value is not part of this comparison in order not to accidently discard a good solution next the algorithm seeks for improved solutions based on a stochastic formulation of the standard nelder mead sequence reflection expansion contraction shrinkage also introducing additional transformations if these movements fail to detect improved points then a mutation mechanism is activated which ensures diversity among the population members and eventually helps the algorithm escape from possible local optima 2 2 surrogate enhanced evolutionary annealing simplex seeas is a heuristic population based global optimization method originally developed by tsoukalas et al 2016 and is essentially an extension of eas in a sense that a surrogate model sm is incorporated to assist the search procedure in particular a cubic rbf with linear polynomial tail which is an interpolation method seeas also uses and maintains an external archive which includes all population members so far for which the value of the real objective function is computed every time a new function value is obtained through the simulation model the associated point enters the archive too all data in the archive are used whenever the interpolation is applied so that the metamodel can approximate the response surface of the real model more accurately and make useful predictions that will help the convergence towards the global optimum as the number of objective function evaluations increases the archive enlarges as well and the updated surrogate becomes progressively more accurate in its predictions in fact the sm in seeas has a double role namely locating autonomously new promising points where the objective function will be evaluated and providing guidance on the execution of the simplex transformations as employed in eas by indicating promising directions the rbf comes with an acquisition function af which is a well known technique used in surrogate based optimization sbo in order to balance exploration and exploitation the af in seeas uses self adjusting weights that are updated in every iteration according to the current number of objective function evaluations and the maximum allowed one the generation of the initial population is employed via the latin hypercube sampling lhs technique giunta et al 2003 which is an established statistical method that ensures sufficient sampling across the search space a typical iteration step is carried out as follows initially the sm is fitted to the data of the current external archive next an internal global optimization algorithm i e the original version of eas is used to optimize the updated sm by using as objective function the af emerging from the fitted rbf the arising global minimum is a candidate solution to enter the population if this point is better than the worst solution in the current population then it replaces it and enters the population otherwise it is rejected in any case it enters the external archive thus improving the available information about the geometry of the search space afterwards the search procedure is based on the genetic operators of eas in this respect a simplex is randomly created from the existing population and executes the standard simplex movements a key difference with eas is that all movements except for shrinkage i e reflection expansion contraction are supported by the metamodel for instance after defining the direction of reflection multiple new candidate points are produced along this and the sm is applied to dictate which one should be chosen and evaluated through the actual simulation model the rest elements of eas are maintained as in the original algorithm in particular the mutation operator and the self adjusting annealing schedule which controls the system s temperature and eventually the randomness of the search procedure at the end of the iteration there is at least one new point obtained that enters the population replacing one of its preexisting members the search is terminated either by reaching a maximum allowed number of function evaluations or by fulfilling a given convergence criterion for a detailed description of seeas regarding the mathematical equations of the rbf af and the analysis of the surrogate enhanced eas operators interested readers are encouraged to refer to tsoukalas et al 2016 2 3 adaptive multi surrogate enhanced evolutionary annealing simplex the proposed algorithm is an improved version of seeas the notable difference between the two algorithms is that amseeas is not limited to the incorporation of a single metamodel namely the rbf surrogate its rationale stands on the fact that there is a wide variety of metamodels listed in the literature that are able to be embedded within optimization algorithms in order to assist the search procedure and that there is no specific metamodel clearly superior to the rest ones in fact the performance of a specific sm depends on the available data i e points in which the real objective function value is computed and the peculiarities of the response surface of the underlying optimization problem dimensionality complexity multimodality in cases where the metamodel does not fit well to the existing data thus providing poor predictions there is consequently an increased risk to sacrifice a substantial number of function evaluations practically without any improvement that being said it is possible in some occasions to apply a surrogate based approach but actually have the opposite effect from the desirable one which is the drastic decrease of the computational effort induced by the objective function calls in this vein a more effective policy would be detecting whether a surrogate exhibits a good behavior or not and if so completely discarding it from the search procedure in the amseeas setting multiple metamodels coexist and operate as a group as the optimization evolves however the implementation of many sms does not necessarily guarantee a better convergence behavior the key challenge is ensuring that all surrogates cooperate effectively support each other and exploit each other s advantages in this mindset amseeas incorporates five sms in the core of eas namely i a cubic rbf with linear polynomial tail ii a random forest iii a support vector machine iv a gaussian process with a rational quadratic kernel and v a gaussian process with a matérn kernel these five metamodels emerged among many others after extensive analysis and experimentation with different optimization problems as mentioned in the previous section the surrogate model in seeas has a double role the first one is providing on its own new promising points for real objective function evaluation and the second one is supporting the simplex movements regarding the second role amseeas maintains the same principles and uses the same surrogate as seeas i e cubic rbf with linear polynomial tail exclusively which seems to co operate well with the downhill simplex method regarding the first role the novelty introduced in amseeas involves the creation of a virtual roulette which is responsible for deciding which out of the five surrogates is activated in every iteration this is implemented by assigning different probabilities to the five sms at the beginning of each iteration in particular the metamodels that are more likely to make better predictions get a higher probability of being chosen by the roulette for activation on the contrary metamodels that seem to have poor fitting on the data are excluded from the roulette spinning only for this iteration to avoid wasting an objective function evaluation finally metamodels with marginally satisfactory fitting get a low probability of being selected under this premise either one specific surrogate will be activated i e as chosen by the roulette or none when all five metamodels demonstrate too poor fitting on the existing data and hence are all excluded from the roulette the assignment of probabilities is determined by a well known goodness of fit criterion namely the nash sutcliffe efficiency nse 1 n s e 1 i 1 n y m i y o i 2 i 1 n y o i y o 2 where y o is the mean actual value y m i and y o i are the ith modeled and actual values respectively and n is the size of data the nse values range between and 1 when n s e 1 the model perfectly fits the observations whereas if n s e 0 the model has the same predictive skill with the mean actual value negative nse values indicate a very bad fitting with worse predictive skill than the mean y o based on the aforementioned analysis the threshold determining whether a surrogate will participate in the roulette spinning or not in a given iteration is the value of n s e 0 this is reasonable since the fact that a metamodel s fitting corresponds to a negative nse value is a strong indicator that the particular sm will probably waste an objective function evaluation if chosen by the roulette on the other hand metamodels with positive nse values will be part of the roulette wheel and have a non zero probability of being selected which is proportional to their nse value in particular considering m total surrogates 1 m 5 with positive nse values i e n s e 1 n s e 2 n s e m we compute their sum n s e s u m n s e 1 n s e 2 n s e m and their corresponding probabilities are calculated as p 1 n s e 1 n s e s u m p 2 n s e 2 n s e s u m p m n s e m n s e s u m in order to estimate the nse values at each iteration the total data so far in the archive are split into a training and a test set the training set contains a randomly determined 80 of points and the test set contains the remaining 20 the five surrogates are fitted to the same training set and then make their predictions in the same test set including the rest available points provided that at least one of the five surrogates has a positive nse value the roulette mechanism is activated to select the surrogate to be next used for predicting and providing a new promising point for evaluating the real objective function contrarily if all metamodels have a negative nse value the roulette mechanism remains deactivated and the iteration continues by jumping to the surrogate assisted genetic operators of seeas the roulette mechanism contains an additional functionality i e permanently eliminating surrogates once they reach a specific threshold more specifically once a metamodel is chosen by the roulette it indicates the next point where the real objective function value will be calculated if this point is better than the worst point in the current population it replaces it otherwise it is ignored so if a surrogate makes a bad prediction i e it generates a worse point than the whole current population then a penalty counter is initialized when this counter reaches a given maximum value then the associated metamodel is considered unable for providing any more assistance hence it is permanently removed from the system allowing for the rest of the metamodels to continue enhancing the optimization procedure however if all metamodels reach that maximum penalty value then the roulette mechanism is discarded and the search evolves by only employing the surrogate assisted genetic operators of seeas a demonstration of the incorporation of multiple metamodels and the corresponding virtual roulette mechanism is illustrated in fig 1 since the cubic rbf with linear polynomial tail has a negative nse value i e n s e 0 050 it will abstain from the roulette for this particular iteration whereas the rest of surrogate models will participate in it since the random forest metamodel exhibits the highest nse value i e n s e 0 916 it also gains the highest probability i e 46 97 of being chosen for prediction the proposed algorithm includes two final extensions the first one refers to the sampling method used to generate the initial population the default method is the lhs technique however our extensive investigations with test functions see section 3 showed that in some cases the algorithm performs much better when the symmetric latin hypercube design ye et al 2000 is employed the second extension refers to a local search strategy in an attempt to further improve the best solution emerging at the end of each iteration cycle more explicitly after finalizing the surrogate assisted genetic operators the current best population solution is acquired and a search across its neighborhood is conducted in this respect a large number of points around the optimal one is generated and evaluated through the cubic rbf with linear polynomial tail in order to indicate the most promising one for employing a real objective function evaluation if this is better than the current worst point in the population it replaces it otherwise it is rejected nevertheless the new point enters the external archive flowcharts of both seeas and amseeas are presented in fig 2 3 benchmarking of optimization algorithms with mathematical test functions 3 1 problem setup in order to assess the performance of amseeas with respect to other well established surrogate based global optimization methods we initially evaluate them against a number of standardized optimization tests involving mathematical functions that exhibit different complexities amseeas is benchmarked against five state of the art surrogate based global optimization algorithms that are listed in table 1 specifically the ones that are publicly available namely pods xia et al 2021 turbo eriksson et al 2019 sop krityakierne et al 2016 seeas tsoukalas et al 2016 dycors regis and shoemaker 2013 in our tests we also include the classical mlmsrbf method regis and shoemaker 2007 since it has gained significant popularity over the time as indicated by the associated number of citations more than 400 regarding turbo we consider two different configurations of it depending on the number of trust regions used which is a critical input argument of the algorithm thus we evaluate two versions herein referred to as turbo 1 and turbo m with 1 and m 5 trust regions respectively under this premise eight algorithms are eventually participating in the benchmarking in order to ensure fair comparisons we use the same population size for all algorithms equal to m 2 n 1 where n is the number of control variables as proposed by regis and shoemaker 2006 moreover in all cases the generation of the initial population also referred to as design of experiment doe is employed via the lhs technique and the default values for all algorithmic inputs and hyperparameters are set as suggested in the associated publications as far as the computational workload of the core optimization procedures e g random sampling geometrical transformations building of surrogates check of termination criteria is minimal we consider that the simulation is by far the most time consuming stage and thus the total computational time of the search procedure is mainly determined by the total number of function evaluations the benchmarking suite consists of six well recognized mathematical functions the global minimum of which is known a priori and is equal to zero the six functions are listed in table 2 for each optimization problem shown in table 2 we consider two different dimensions by setting the number of control variables equal to n 15 and n 30 as well as two different computational budgets in terms of maximum allowable function evaluations mfe which are set equal to m f e 500 and m f e 1000 in 15 d and 30 d problems the population size is set equal to m 32 and m 62 respectively these assumptions are realized in order to have a much more detailed overall view of the algorithms capabilities and assess their performance not only in cases where the complexity of the problem increases but also in cases where the simulation phase is so time consuming that only a few hundred objective function evaluations can be executed for obtaining a satisfactory solution in a reasonable time as a result a total of 6 2 2 24 different optimization problems arise each problem is executed 30 times by starting from independent populations so that sufficient samples can be collected and then assessed in every algorithm run the best approximation to the global minimum is retrieved i e closest convergence to zero afterwards we compute the median value across the sample of 30 optimized sets in all 24 optimization problems and across the eight competing algorithmic schemes in order to draw valid conclusions about all algorithms capabilities and compare their performances we apply the concept of stochastic dominance levy 1992 in this respect for each algorithm and each test problem that runs 30 times from different initial populations we collect the associated optimal values and plot their empirically derived cumulative distribution function cdf to contrast the performance of two algorithms a and b in a given problem we compare their cdfs symbolized φ a and φ b respectively algorithm a is considered stochastically dominant over b if φ a q φ b q for all q and vice versa where q is a random quantity to minimize if however the two cdfs are intersected then we compare their median values and thus consider as dominant the algorithm with the better performance at this point given that the difference at their medians is statistically significant to ensure statistical significance we apply the non parametric wilcoxon signed rank test woolson 2008 the null hypothesis of the test is that the data in φ a and φ b are samples from continuous distributions with equal medians while the confidence level is set equal to 95 if the null hypothesis is not rejected then the two algorithms are considered equally good 3 2 results table 3 and table 4 depict the performance of the eight competing algorithms in terms of median of the optimized function values found for 30 independent runs over the six test problems in the 15 d and 30 d space respectively and under the two budgets i e 500 and 1000 maximum function evaluations for each optimization problem the empirical cdfs of all algorithms are presented in appendix a figure a1 to figure a6 whenever required we also employ the wilcoxon signed rank test between the algorithms providing the best i e lowest median values in order to help us draw clear conclusions about the algorithm exhibiting stochastic dominance in the particular problem the summary results of these tests are presented in table 5 where n is the number of control variables and h indicates the rejection or not of the null hypothesis i e if h 0 the null hypothesis is not rejected and thus both algorithms are considered equally good the rejection or not of the null hypothesis on a particular problem depends on the value of w which is the outcome of the wilcoxon test if w w c r i t the null hypothesis is rejected w c r i t is equal to 137 and denotes the critical value of w for a two tailed test with a sample size of 30 and 95 confidence level as an example in fig 3 we demonstrate one case where the cdf plots are sufficient for evaluating which algorithm is dominant in the particular problem i e 15 d rastrigin case with m f e 500 and one case where the wilcoxon signed rank test is essential to detect the dominating method i e 30 d rastrigin case with m f e 500 the aforementioned analyses clearly indicate the superiority of amseeas in most of examined problems in 15 d configurations the proposed algorithm dominates in five o f 1 o f 2 o f 4 o f 5 o f 6 and two o f 1 o f 5 problems for m f e 500 and m f e 1000 respectively as the number of control variables and thus the complexity of the optimization problem increases amseeas performs even more efficiently in particular it achieves best performance in five o f 1 o f 2 o f 4 o f 5 o f 6 problems for both computational budgets overall amseeas is stochastically dominant in 17 out of 24 problems pods in 5 whereas seeas in 3 the rest algorithms exhibit less satisfactory performance that is turbo 1 and dycors are optimal in only one case each while turbo m sop and mlmsrbf remain sub optimal across all problems it is important to highlight that all algorithms perform poorly against of4 zakharov and of5 rastrigin as none of them exhibits satisfactory convergence to the global minima especially in the 30 d space this is not surprising as these functions produce complicated response surfaces thus making it extremely difficult for the metamodels to fit the data and offer useful predictions 3 3 convergence behavior analysis in order to extend our analysis regarding the proposed algorithm s effectiveness we examined one additional issue referred to the convergence speed towards the global optimum in terms of objective function evaluations briefly in every run out of 30 in total of each out of 24 optimization problems we retrieved the best approximations to the associated global optimum as the objective function evaluations increase towards their maximum allowed limit i e m f e 500 or 1000 subsequently we computed the median values of the best solutions so far in order to plot and evaluate how fast the optimization evolves in each case the resulting convergence curves are presented in appendix b figure b1 to figure b6 in fig 4 we present for illustration purposes the resulting convergence curves of all competing algorithms in a particular optimization problem as the figures indicate in some optimization problems i e sphere zakharov amseeas exhibits faster convergence towards the global optimum even from the first few hundreds of function evaluations thus clearly outperforming the rest optimization methods besides that even when amseeas is initially evolving relatively slowly i e ackley rastrigin lévy afterwards its behavior is remarkably improved for instance in the 30 d lévy cases while seeas outperforms amseeas at the early stages of the optimization procedures after approximately four hundred evaluations the latter converges much faster another noteworthy case is the zakharov function for which the performance of seeas and other surrogate based approaches is rather poor see discussion by tsoukalas et al 2016 however the multi model approach coupled with the roulette and penalty mechanisms that are introduced in the amseeas version made the algorithm clearly more effective 4 real world benchmarking in a computationally demanding hydraulic design problem since the application of surrogate assisted methods in practice involves heb problems in order to obtain a more comprehensive picture of the proposed algorithm s capabilities we also test how amseeas performs in such a real world problem by comparing it to its precursor i e seeas 4 1 study area and optimization problem definition the optimization problem involves a hydraulic design study in the context of a broader flood risk assessment analysis which is described in detail in the recent work by efstratiadis et al 2022 the area of application is the lower course of trachones stream which crosses highly urbanized suburbs of athens greece its total drainage area is approximately 24 k m 2 and it extends in the south of athens between the foothills of mount hymettus and the coast the design optimization task refers to the sizing of levees along the open parts of the lower drainage network which is conceptually configured by means of 27 lateral weirs that are represented in the hec ras environment we remark that the three out of 27 lateral structures are internal and act as side channel spillways by transferring flow from one channel section to another these levees do not produce overflow and thus are not part of the design variables set the control variables of the design optimization problem are the elevations h i of 24 out of 27 individual levees which are allowed to receive non negative values up to 1 0 m two conflicting criteria are considered namely the total overflow occurring from the major channel segments over their associated levees and the total construction cost in this respect as the elevation increases the expected overflow decreases however the construction cost increases accordingly as explained latter for a given configuration of the drainage system the overflow over the levees is estimated through a hydrodynamic model driven with a specific design flood event on the other hand the construction cost of each levee is estimated by 2 c i c l i h i d where l i is the length of each individual levee d is the crest width same for all levees which is set equal to 2 m and c is an indicative unit construction cost which is for convenience set equal to 30 m 3 the objective function to minimize is expressed in dimensionless terms as follows 3 f h 1 h 2 h n t o v t o v max t c t c max 2 where t o v i 1 n v i and t c c i 1 n l i h i d specifically t o v denotes the total overflow t c stands for the overall construction cost of levees v i is the overflow of each individual weir i and n 24 is the number of control variables similarly t o v max is the maximum potential total overflow which refers to the do nothing solution i e zero increase of elevation in all 24 lateral structures and hence zero cost and is equal to 3272 6 10 3 m 3 while t c max denotes the maximum overall construction cost by assuming that all 24 individual levees are elevated by h max 1 0 m and is equal to 515 9 10 3 the above formulation of the objective function makes the optimization task independent of the uncertainty induced by the subjective assignment of input arguments d and c the hydrodynamic simulations are performed with the hec ras 6 1 software under one dimensional analysis with 5 m spatial resolution and a computational time step of 30 s the coupling of hec ras with the optimization algorithms is implemented in a python environment by utilizing the hec ras controller goodell 2014 which is part of the hec ras application programming interface api the controller incorporates a wealth of procedures that allow the manipulation of hec ras externally by setting input data retrieving input or output data and performing common functions such as opening and closing hec ras changing plans model running and plotting output data similar works employing the aforementioned api e g for automation purposes are those of siqueira et al 2016 leon and goodell 2016 and dysarz 2018 regarding the hydrodynamic modeling procedure since the longitudinal slopes of the stream branches are quite significant and the flow velocities are high the so called local partial inertia lpi technique fread et al 1996 is used for improving the stability of the numerical solution by setting a froude number threshold equal to 0 01 for the manning s coefficient parametrization the computational domain is classified into three specific friction zones i e cross sections constructed by concrete gabions and natural terrain for which we apply 0 016 0 025 and 0 030 s m 1 3 respectively the input flood event corresponding to the one dimensional hydrodynamic model boundary conditions is selected from the set of stochastic weather scenarios produced by efstratiadis et al 2022 and refers to a return period of 500 years and a 90 confidence level this challenging problem reveals the actual value of sbo techniques since to evaluate the objective function a time demanding hec ras simulation needs to be executed first specifically by using a 3 ghz intel core i9 processor with a 32 gb of ram a single run lasts 2 min consequently to obtain an optimized design within a reasonable time period only a few hundreds of function evaluations are allowed 4 2 performance comparison of the algorithms as the computational workload is quite heavy we consider a maximum allowed number of objective function evaluations equal to m f e 500 while to extract sufficient statistical outcomes we repeat the optimization procedure with seeas and amseeas for a total of ten times we remark that the computational time is almost fully dictated by the simulation stage which in turn mainly depends on the execution of hec ras provided that both algorithms are allowed to call the simulation 500 times each optimization run requires approximately 17 hours in contrast to theoretical test functions the global minimum of this highly complex engineering design problem cannot be known a priori on the other hand by setting all design variables equal to zero it is easy to detect that the do nothing solution results in a total overflow equal to 3272 6 10 3 m 3 and a zero cost thus an objective function value equal to 0 50 the key results obtained with seeas and amseeas are shown in table 6 these indicate that amseeas ensures much better performance in comparison to seeas while the outcomes of the latter are actually worse than the do nothing scenario the superiority of amseeas over seeas is shown even more clearly if we plot the cdfs of the associated optimal objective function values as shown in fig 5 amseeas is considered stochastically dominant over seeas in fig 6 we also plot the convergence curves of the two algorithms to evaluate how fast in terms of number of objective function evaluations does the convergence procedure evolve once again it is clear that amseeas outperforms seeas during the entire search procedure finally in table 7 we compare the optimized control variables in terms of elevation increase of the overall best solution found by each algorithm out of the ten optimization trials the layout of the best design solution of each algorithm is demonstrated in fig 7 a colorized scale ranging from blue to red is used to highlight the amount of additional elevation among the levees of the area in each case with the blue values referring to the lowest possible elevations 0 0 m 0 2 m and the red values to the highest ones 0 8 m 1 0 m it is evident that the best solution found by seeas requires higher elevation of the individual levees in comparison to the amseeas one which justifies the huge difference in the construction costs in particular the solution proposed by amseeas is cheaper by 75 51 while resulting in only 0 26 more total overflow than the seeas solution 5 conclusions global optimization problems are usually handled through objective functions the values of which are available after the execution of a black box simulation model however several of the models used in environmental sciences require high computational effort thus introducing significant barriers to the optimization procedure the conflicting aspects of model accuracy and computational hardware requirements led to the search for new ideas and tools to achieve satisfactory solutions within reasonable time in this respect sbo techniques are a well established approach for such challenging problems this study introduces the adaptive multi surrogate enhanced evolutionary annealing simplex amseeas method key novelty of which is the effective co operation of multiple surrogate models to ensure flexibility against objective functions and associated response surfaces of different characteristics a virtual roulette is introduced to decide which sm should be activated in every iteration the probability of each metamodel being selected by the roulette for prediction depends on how accurately it fits existing data each metamodel comes with a penalty counter which increases whenever the metamodel makes a bad prediction these counters can permanently discard surrogates when reaching a specific threshold to assess the efficiency and effectiveness of the proposed algorithm we initially benchmarked it against six state of the art surrogate based global optimization methods in six characteristic theoretical mathematical problems with alternative settings i e two alternative dimensions and two computational budgets thus resulting to 24 optimization problems in total the results emerging from this analysis are encouraging as amseeas proves its robustness by outperforming the other algorithms in most of problem settings in particular amseeas is considered stochastically dominant over its competitors in 17 out of 24 problems next we contrasted amseeas against seeas i e the original method on which the proposed algorithm is based in a highly challenging real world problem from the field of hydraulic design of flood protection works a key barrier to such problems is the computational burden induced by the use of detailed hydrodynamic models in this case hec ras to assess the performance of a specific design in terms of flood hazard and eventually flood risk while also keeping the cost of the proposed hydraulic infrastructures to a minimum additional challenges are induced by the problem dimension since in the examined case we were looking for optimizing 24 design variables that represent the levee heights across the open channel network within our analysis we detected the optimal solutions found by amseeas and seeas from a set of ten independent runs and under a limited computational budget of only 500 function evaluations the design proposed by amseeas ensures a substantially decreased construction cost with respect to seeas and at the same time a low flood inundation risk marginally only exceeding the estimated inundation hazard that results from the optimal design by seeas our extended analyses with the mathematical problems and the hydraulic design application as well indicate that amseeas is a robust and efficient optimization algorithm able to handle computationally challenging heb problems in practice without compromising neither on simulation model sophistication nor on proper probabilistic treatment of complex environmental problems this is due to the fact that the proposed method does not simply incorporate multiple surrogate models to support the optimization procedure the introduced mechanisms behave in a completely stochastic manner and result in the adjustment of the exploited metamodels on the given objective function to minimize depending on the characteristics and irregularities of the response surface some metamodels might be of actual assistance while others might have the opposite behaviour and undermine the convergence process by pointing towards directions other than the region of interest however the inclusion of the virtual roulette and penalty mechanisms ensures that all inappropriate surrogates for a given problem will abstain from the search procedure while the more appropriate ones will be given the chance to assist it up until they stop producing useful information this strategy assures that the number of wasted objective function evaluations is limited to a minimum extent and that the genetic operators of seeas get the most proper assistance on any given optimization problem eventually this strategy seems to be a very promising one towards ensuring adaptation of the search procedure to response surfaces of varying characteristics acknowledgements the research leading to these results has received funding from the european union s horizon 2020 research and innovation programme from the eu horizon 2020 green deal call under grant agreement no 101037084 for the research project impetus dynamic information management approach for the implementation of climate resilient adaptation packages in european regions the research and its conclusions reflect only the views of the authors and the european union is not liable for any use that may be made of the information contained herein the authors would like to thank the editor dr saman razavi and the three reviewers dr jeremy white and two anonymous ones for their constructive comments suggestions and critique which helped to provide a significantly improved manuscript appendix a empirical cdfs for mathematical test functions fig a1 empirical cdfs for test function of1 sphere with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a1 fig a2 empirical cdfs for test function of2 ackley with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a2 fig a3 empirical cdfs for test function of3 griewank with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a3 fig a4 empirical cdfs for test function of4 zakharov with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a4 fig a5 empirical cdfs for test function of5 rastrigin with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a5 fig a6 empirical cdfs for test function of6 lévy with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a6 appendix b convergence curves for mathematical test functions fig b1 convergence curves for test function of1 sphere with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b1 fig b2 convergence curves for test function of2 ackley with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b2 fig b3 convergence curves for test function of3 griewank with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b3 fig b4 convergence curves for test function of4 zakharov with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b4 fig b5 convergence curves for test function of5 rastrigin with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b5 fig b6 convergence curves for test function of6 lévy with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b6 
25428,complex environmental optimization problems often require computationally expensive simulation models to assess candidate solutions however the complexity of response surfaces necessitates multiple such assessments and thus renders the search procedure too laborious surrogate based optimization is a powerful approach for accelerating convergence towards promising solutions here we introduce the adaptive multi surrogate enhanced evolutionary annealing simplex amseeas algorithm as an extension of its precursor seeas which is a single surrogate based optimization method amseeas exploits the strengths of multiple surrogate models that are combined via a roulette type mechanism for selecting a specific metamodel to be activated in every iteration amseeas proves its robustness and efficiency via extensive benchmarking against seeas and other state of the art surrogate based global optimization methods in both theoretical mathematical problems and in a computationally demanding real world hydraulic design application the latter seeks for cost effective sizing of levees along a drainage channel to minimize flood inundation calculated by the time expensive hydrodynamic model hec ras keywords surrogate modeling machine learning high dimensional expensive black box heb problems evolutionary annealing simplex test functions hydraulic design software and data availability a python implementation of eas seeas and amseeas and the rest of our work are available online at https github com spyrostsat amseeas 1 introduction simulation models of detailed spatial and temporal resolution have a pivotal role in environmental sciences also gaining increasing popularity in engineering practice such models provide the capability to represent complex physical phenomena accounting for the spatiotemporal dynamics of all processes of interest as well to describe their interactions with infrastructures and societal factors their utility is further enhanced when these are coupled with optimization methods maier et al 2014 at a conceptual level combined simulation optimization schemes tsoukalas et al 2016 can be employed to address both decision making applications e g optimal design planning management and real time control of environmental systems and inverse modeling problems as well aiming to identify optimal model configurations so that the observed responses are faithfully represented as their name suggests these use simulation models to evaluate the system s performance expressed in terms of an objective function of a nonlinear global optimization problem with no analytical solution or derivative information the literature is particularly rich in such efforts and advances which have been summarized in the review works of labadie 2004 fowler et al 2008 nicklow et al 2010 reed et al 2013 ahmad et al 2014 and kumar and yadav 2022 which emphasize on water resources management as well as by duan 2003 and efstratiadis and koutsoyiannis 2010 which focus on hydrological calibration the major obstacle encountered in model based optimization problems is the required computational workload which is dictated by the computational cost e g time of the underlying simulation model state of the art simulation models have the ability to describe the peculiarities of environmental systems with great accuracy and detail e g in terms of geometry boundary conditions and spatiotemporal dynamics yet this comes at a price another common category of time demanding models involves stochastic simulation schemes that are driven by long synthetic data in order to quantify their probabilistic performance with satisfactory accuracy in all these cases the computational time for a single model execution may require from a few minutes up to several hours on the other hand depending on the problem dimensionality and the irregularity of the response surface a typical global optimization algorithm may need to evaluate the objective function and hence call the simulation model thousands of times in order to converge to a satisfactory solution these two issues combined result in termination of the optimization procedure after days or even weeks which makes the optimization task infeasible and sometimes even prohibitive among the different strategies proposed by the research community e g razavi et al 2010 to deal with the time issue including parallel computing computationally efficient search algorithms opportunistic avoidance of model evaluations a particularly interesting one with proven effectiveness is the use of surrogate modeling techniques surrogate models also known as meta models offer an elegant software based solution where low computational cost approximation models are used to represent the actual and time expensive simulation model s response surface and thus guide the optimization procedure in particular the surrogates aim to aid the search procedure by replacing the actual simulation model to some extent and proposing promising solutions that will possibly lead to a much faster convergence of the optimization under this premise the simulation procedure is called in limited cases in order to evaluate the objective function wherever suggested by the surrogate model from a historical perspective one of the first works that popularized such approaches is attributed to jones et al 1998 who introduced the so called efficient global optimization ego ego embeds a kriging i e gaussian process method as a surrogate model to the core of the optimization procedure among others this has the role of locating potentially good solutions that are worth evaluating through the actual simulation model after the seminal publication of ego several optimization schemes incorporated the idea of using surrogate modeling techniques thus resulting to a vast list of algorithms where numerous alternative machine learning models are employed as surrogate models such as polynomials radial basis functions rbfs random forests rfs support vector machines svms artificial neural networks anns etc e g müller et al 2013 liu et al 2014 golzari et al 2015 mallipeddi and lee 2015 awad et al 2018 the use of surrogate assisted approaches spans over multiple types of optimization problems also including discrete constrained and multiobjective optimization emphasis was also given to time demanding problems with many control variables also referred to as high dimensional expensive black box heb problems running advances for heb problems include the so called knowledge transfer assisted efficient global optimization kt ego algorithm which extends the classical version of ego to handle high i e 20 variables dimensions wang et al 2022 other recently published methods aim at providing hybrid schemes that combine different search and or surrogate strategies for instance dong et al 2018a introduced the multi surrogate based differential evolution with multi start exploration mdeme algorithm which uses differential evolution enhanced by three surrogate models i e kriging radial basis function quadratic polynomial response while pan et al 2021 proposed the so called surrogate assisted hybrid optimization saho that employs two different optimization techniques i e teaching learning based optimization differential evolution combined with rbf metamodels the idea of surrogate based optimization has also found fertile ground in the domain of water resources and the environment e g wang et al 2014 tsoukalas and makropoulos 2015a tsoukalas and makropoulos 2015b shaw et al 2017 xia et al 2021 lu et al 2022 starting from the pioneering work by regis and shoemaker 2004 since then this approach managed to address real world optimization problems of significant complexity for instance yazdi and salehi neyshabouri 2014 introduced a framework to solve high dimensional problems for optimizing flood control detention dams wu et al 2015 proposed the so called surrogate based optimization for integrated surface water groundwater modeling soim algorithm for water management optimization problems xi et al 2017 proposed a surrogate assisted approach to efficiently calibrate agricultural hydrological models on a limited budget recently sun et al 2022 developed the so called multi objective adaptive surrogate modelling based optimization for constrained hybrid problems mo asmoch which is designed to handle problems consisting of both continuous and discrete control variables the focus of this research is to provide improved surrogate based solutions for handling time demanding global optimization problems where term global is used to denote nonlinear single objective unconstrained optimization problems with continuous variables this kind of problems is very common in the domain of water resources and the environment where the objective function is typically defined through a computationally expensive simulation model and the resulting response surface is in general multimodal for convenience we also consider that the problem is configured in single objective terms meaning that the system s performance is expressed via an overall scalar metric and more specifically by means of a cost function to minimize we highlight that this overall metric may aggregate several criteria potentially conflicting and embed as well few external constraints by means of penalties on the other hand the vast majority of constraints that are associated with internal modeling procedures e g description of physical processes are exclusively handled through the underlying simulation model thus the optimization problem is by definition formalized as unconstrained regarding the search space also known as decision or feasible space it is formalized as a hypervolume by assigning lower and upper boundaries to the problem s variables according to the problem type these may be referred to as control variables decision variables design variables or parameters table 1 contains a list of state of the art i e published during the last decade surrogate based algorithms for the case of our interest i e global optimization it is interesting to notice that very few of them are publicly available while only three utilize more than one surrogates across the exploration exploitation procedure in an attempt to fill this gap we introduce and provide as open source software in python environment the so called adaptive multi surrogate enhanced evolutionary annealing simplex amseeas algorithm its key novelty is the use of multiple surrogates that cooperate to enable significant improvements across the search space as the search evolves the most effective surrogates are applied more frequently on the basis of a self adaptive probabilistic selection scheme amseeas builds upon two existing optimization schemes i e the surrogate enhanced evolutionary annealing simplex seeas by tsoukalas et al 2016 and the evolutionary annealing simplex eas by efstratiadis and koutsoyiannis 2002 a brief overview of eas and seeas and a more detailed description of amseeas are given in section 2 to evaluate its effectiveness and efficiency the proposed algorithm is thoroughly benchmarked against its forerunner i e seeas as well as other state of the art surrogate based global optimization algorithms the comparison is realized initially via testing all algorithms on a set of mathematical test functions with complex response surfaces and multiple local optima section 3 furthermore to evaluate the proposed algorithm s performance on a real world problem of significant difficulty amseeas is tested on a hydraulic design problem where the evaluation of the cost function requires the use of a time expensive hydrodynamic model section 4 the overall analyses illustrate the advantages of amseeas in terms of providing systematically better solutions under a limited computational budget 2 optimization methodology 2 1 evolutionary annealing simplex eas is a heuristic global optimization algorithm developed by efstratiadis and koutsoyiannis 2002 its main rationale is finding an effective way to combine the strengths of the downhill simplex local optimization method nelder and meadf 1965 with simulated annealing kirkpatrick et al 1983 also incorporating fundamentals of evolutionary algorithms namely the concepts of an evolving population and the genetic operators ryan 2003 in this respect it combines the flexibility of simulated annealing to escape from local optima with the ability of the nelder mead method to locate areas of attraction quickly and accurately this is accomplished through the introduction of a temperature variable t which determines the randomness assigned to the search procedure at early stages temperature is desired to have large values thus making the system warm so that randomness can play a major role to favor the exploration across the entire feasible space in contrast as the search evolves the algorithm is capable of finding areas of attraction and the system gets colder since its temperature decreases thus the search becomes more deterministic and exploitation can begin at each iteration cycle the generation of new solutions is realized by randomly selecting from the population so far sub sets of n 1 points in the n dimensional search space thus each sub set defines a simplex and employing appropriate geometrical transformations in order to determine the simplex vertex to be replaced the associated population members are not being compared exclusively by their objective function value but a randomness term related to the current system temperature is added in early iterations randomness is often the most crucial component in the comparison and thus the solution being chosen for replacement may not be the actually worst simplex vertex nonetheless the best vertex i e the solution with the lowest objective function value is not part of this comparison in order not to accidently discard a good solution next the algorithm seeks for improved solutions based on a stochastic formulation of the standard nelder mead sequence reflection expansion contraction shrinkage also introducing additional transformations if these movements fail to detect improved points then a mutation mechanism is activated which ensures diversity among the population members and eventually helps the algorithm escape from possible local optima 2 2 surrogate enhanced evolutionary annealing simplex seeas is a heuristic population based global optimization method originally developed by tsoukalas et al 2016 and is essentially an extension of eas in a sense that a surrogate model sm is incorporated to assist the search procedure in particular a cubic rbf with linear polynomial tail which is an interpolation method seeas also uses and maintains an external archive which includes all population members so far for which the value of the real objective function is computed every time a new function value is obtained through the simulation model the associated point enters the archive too all data in the archive are used whenever the interpolation is applied so that the metamodel can approximate the response surface of the real model more accurately and make useful predictions that will help the convergence towards the global optimum as the number of objective function evaluations increases the archive enlarges as well and the updated surrogate becomes progressively more accurate in its predictions in fact the sm in seeas has a double role namely locating autonomously new promising points where the objective function will be evaluated and providing guidance on the execution of the simplex transformations as employed in eas by indicating promising directions the rbf comes with an acquisition function af which is a well known technique used in surrogate based optimization sbo in order to balance exploration and exploitation the af in seeas uses self adjusting weights that are updated in every iteration according to the current number of objective function evaluations and the maximum allowed one the generation of the initial population is employed via the latin hypercube sampling lhs technique giunta et al 2003 which is an established statistical method that ensures sufficient sampling across the search space a typical iteration step is carried out as follows initially the sm is fitted to the data of the current external archive next an internal global optimization algorithm i e the original version of eas is used to optimize the updated sm by using as objective function the af emerging from the fitted rbf the arising global minimum is a candidate solution to enter the population if this point is better than the worst solution in the current population then it replaces it and enters the population otherwise it is rejected in any case it enters the external archive thus improving the available information about the geometry of the search space afterwards the search procedure is based on the genetic operators of eas in this respect a simplex is randomly created from the existing population and executes the standard simplex movements a key difference with eas is that all movements except for shrinkage i e reflection expansion contraction are supported by the metamodel for instance after defining the direction of reflection multiple new candidate points are produced along this and the sm is applied to dictate which one should be chosen and evaluated through the actual simulation model the rest elements of eas are maintained as in the original algorithm in particular the mutation operator and the self adjusting annealing schedule which controls the system s temperature and eventually the randomness of the search procedure at the end of the iteration there is at least one new point obtained that enters the population replacing one of its preexisting members the search is terminated either by reaching a maximum allowed number of function evaluations or by fulfilling a given convergence criterion for a detailed description of seeas regarding the mathematical equations of the rbf af and the analysis of the surrogate enhanced eas operators interested readers are encouraged to refer to tsoukalas et al 2016 2 3 adaptive multi surrogate enhanced evolutionary annealing simplex the proposed algorithm is an improved version of seeas the notable difference between the two algorithms is that amseeas is not limited to the incorporation of a single metamodel namely the rbf surrogate its rationale stands on the fact that there is a wide variety of metamodels listed in the literature that are able to be embedded within optimization algorithms in order to assist the search procedure and that there is no specific metamodel clearly superior to the rest ones in fact the performance of a specific sm depends on the available data i e points in which the real objective function value is computed and the peculiarities of the response surface of the underlying optimization problem dimensionality complexity multimodality in cases where the metamodel does not fit well to the existing data thus providing poor predictions there is consequently an increased risk to sacrifice a substantial number of function evaluations practically without any improvement that being said it is possible in some occasions to apply a surrogate based approach but actually have the opposite effect from the desirable one which is the drastic decrease of the computational effort induced by the objective function calls in this vein a more effective policy would be detecting whether a surrogate exhibits a good behavior or not and if so completely discarding it from the search procedure in the amseeas setting multiple metamodels coexist and operate as a group as the optimization evolves however the implementation of many sms does not necessarily guarantee a better convergence behavior the key challenge is ensuring that all surrogates cooperate effectively support each other and exploit each other s advantages in this mindset amseeas incorporates five sms in the core of eas namely i a cubic rbf with linear polynomial tail ii a random forest iii a support vector machine iv a gaussian process with a rational quadratic kernel and v a gaussian process with a matérn kernel these five metamodels emerged among many others after extensive analysis and experimentation with different optimization problems as mentioned in the previous section the surrogate model in seeas has a double role the first one is providing on its own new promising points for real objective function evaluation and the second one is supporting the simplex movements regarding the second role amseeas maintains the same principles and uses the same surrogate as seeas i e cubic rbf with linear polynomial tail exclusively which seems to co operate well with the downhill simplex method regarding the first role the novelty introduced in amseeas involves the creation of a virtual roulette which is responsible for deciding which out of the five surrogates is activated in every iteration this is implemented by assigning different probabilities to the five sms at the beginning of each iteration in particular the metamodels that are more likely to make better predictions get a higher probability of being chosen by the roulette for activation on the contrary metamodels that seem to have poor fitting on the data are excluded from the roulette spinning only for this iteration to avoid wasting an objective function evaluation finally metamodels with marginally satisfactory fitting get a low probability of being selected under this premise either one specific surrogate will be activated i e as chosen by the roulette or none when all five metamodels demonstrate too poor fitting on the existing data and hence are all excluded from the roulette the assignment of probabilities is determined by a well known goodness of fit criterion namely the nash sutcliffe efficiency nse 1 n s e 1 i 1 n y m i y o i 2 i 1 n y o i y o 2 where y o is the mean actual value y m i and y o i are the ith modeled and actual values respectively and n is the size of data the nse values range between and 1 when n s e 1 the model perfectly fits the observations whereas if n s e 0 the model has the same predictive skill with the mean actual value negative nse values indicate a very bad fitting with worse predictive skill than the mean y o based on the aforementioned analysis the threshold determining whether a surrogate will participate in the roulette spinning or not in a given iteration is the value of n s e 0 this is reasonable since the fact that a metamodel s fitting corresponds to a negative nse value is a strong indicator that the particular sm will probably waste an objective function evaluation if chosen by the roulette on the other hand metamodels with positive nse values will be part of the roulette wheel and have a non zero probability of being selected which is proportional to their nse value in particular considering m total surrogates 1 m 5 with positive nse values i e n s e 1 n s e 2 n s e m we compute their sum n s e s u m n s e 1 n s e 2 n s e m and their corresponding probabilities are calculated as p 1 n s e 1 n s e s u m p 2 n s e 2 n s e s u m p m n s e m n s e s u m in order to estimate the nse values at each iteration the total data so far in the archive are split into a training and a test set the training set contains a randomly determined 80 of points and the test set contains the remaining 20 the five surrogates are fitted to the same training set and then make their predictions in the same test set including the rest available points provided that at least one of the five surrogates has a positive nse value the roulette mechanism is activated to select the surrogate to be next used for predicting and providing a new promising point for evaluating the real objective function contrarily if all metamodels have a negative nse value the roulette mechanism remains deactivated and the iteration continues by jumping to the surrogate assisted genetic operators of seeas the roulette mechanism contains an additional functionality i e permanently eliminating surrogates once they reach a specific threshold more specifically once a metamodel is chosen by the roulette it indicates the next point where the real objective function value will be calculated if this point is better than the worst point in the current population it replaces it otherwise it is ignored so if a surrogate makes a bad prediction i e it generates a worse point than the whole current population then a penalty counter is initialized when this counter reaches a given maximum value then the associated metamodel is considered unable for providing any more assistance hence it is permanently removed from the system allowing for the rest of the metamodels to continue enhancing the optimization procedure however if all metamodels reach that maximum penalty value then the roulette mechanism is discarded and the search evolves by only employing the surrogate assisted genetic operators of seeas a demonstration of the incorporation of multiple metamodels and the corresponding virtual roulette mechanism is illustrated in fig 1 since the cubic rbf with linear polynomial tail has a negative nse value i e n s e 0 050 it will abstain from the roulette for this particular iteration whereas the rest of surrogate models will participate in it since the random forest metamodel exhibits the highest nse value i e n s e 0 916 it also gains the highest probability i e 46 97 of being chosen for prediction the proposed algorithm includes two final extensions the first one refers to the sampling method used to generate the initial population the default method is the lhs technique however our extensive investigations with test functions see section 3 showed that in some cases the algorithm performs much better when the symmetric latin hypercube design ye et al 2000 is employed the second extension refers to a local search strategy in an attempt to further improve the best solution emerging at the end of each iteration cycle more explicitly after finalizing the surrogate assisted genetic operators the current best population solution is acquired and a search across its neighborhood is conducted in this respect a large number of points around the optimal one is generated and evaluated through the cubic rbf with linear polynomial tail in order to indicate the most promising one for employing a real objective function evaluation if this is better than the current worst point in the population it replaces it otherwise it is rejected nevertheless the new point enters the external archive flowcharts of both seeas and amseeas are presented in fig 2 3 benchmarking of optimization algorithms with mathematical test functions 3 1 problem setup in order to assess the performance of amseeas with respect to other well established surrogate based global optimization methods we initially evaluate them against a number of standardized optimization tests involving mathematical functions that exhibit different complexities amseeas is benchmarked against five state of the art surrogate based global optimization algorithms that are listed in table 1 specifically the ones that are publicly available namely pods xia et al 2021 turbo eriksson et al 2019 sop krityakierne et al 2016 seeas tsoukalas et al 2016 dycors regis and shoemaker 2013 in our tests we also include the classical mlmsrbf method regis and shoemaker 2007 since it has gained significant popularity over the time as indicated by the associated number of citations more than 400 regarding turbo we consider two different configurations of it depending on the number of trust regions used which is a critical input argument of the algorithm thus we evaluate two versions herein referred to as turbo 1 and turbo m with 1 and m 5 trust regions respectively under this premise eight algorithms are eventually participating in the benchmarking in order to ensure fair comparisons we use the same population size for all algorithms equal to m 2 n 1 where n is the number of control variables as proposed by regis and shoemaker 2006 moreover in all cases the generation of the initial population also referred to as design of experiment doe is employed via the lhs technique and the default values for all algorithmic inputs and hyperparameters are set as suggested in the associated publications as far as the computational workload of the core optimization procedures e g random sampling geometrical transformations building of surrogates check of termination criteria is minimal we consider that the simulation is by far the most time consuming stage and thus the total computational time of the search procedure is mainly determined by the total number of function evaluations the benchmarking suite consists of six well recognized mathematical functions the global minimum of which is known a priori and is equal to zero the six functions are listed in table 2 for each optimization problem shown in table 2 we consider two different dimensions by setting the number of control variables equal to n 15 and n 30 as well as two different computational budgets in terms of maximum allowable function evaluations mfe which are set equal to m f e 500 and m f e 1000 in 15 d and 30 d problems the population size is set equal to m 32 and m 62 respectively these assumptions are realized in order to have a much more detailed overall view of the algorithms capabilities and assess their performance not only in cases where the complexity of the problem increases but also in cases where the simulation phase is so time consuming that only a few hundred objective function evaluations can be executed for obtaining a satisfactory solution in a reasonable time as a result a total of 6 2 2 24 different optimization problems arise each problem is executed 30 times by starting from independent populations so that sufficient samples can be collected and then assessed in every algorithm run the best approximation to the global minimum is retrieved i e closest convergence to zero afterwards we compute the median value across the sample of 30 optimized sets in all 24 optimization problems and across the eight competing algorithmic schemes in order to draw valid conclusions about all algorithms capabilities and compare their performances we apply the concept of stochastic dominance levy 1992 in this respect for each algorithm and each test problem that runs 30 times from different initial populations we collect the associated optimal values and plot their empirically derived cumulative distribution function cdf to contrast the performance of two algorithms a and b in a given problem we compare their cdfs symbolized φ a and φ b respectively algorithm a is considered stochastically dominant over b if φ a q φ b q for all q and vice versa where q is a random quantity to minimize if however the two cdfs are intersected then we compare their median values and thus consider as dominant the algorithm with the better performance at this point given that the difference at their medians is statistically significant to ensure statistical significance we apply the non parametric wilcoxon signed rank test woolson 2008 the null hypothesis of the test is that the data in φ a and φ b are samples from continuous distributions with equal medians while the confidence level is set equal to 95 if the null hypothesis is not rejected then the two algorithms are considered equally good 3 2 results table 3 and table 4 depict the performance of the eight competing algorithms in terms of median of the optimized function values found for 30 independent runs over the six test problems in the 15 d and 30 d space respectively and under the two budgets i e 500 and 1000 maximum function evaluations for each optimization problem the empirical cdfs of all algorithms are presented in appendix a figure a1 to figure a6 whenever required we also employ the wilcoxon signed rank test between the algorithms providing the best i e lowest median values in order to help us draw clear conclusions about the algorithm exhibiting stochastic dominance in the particular problem the summary results of these tests are presented in table 5 where n is the number of control variables and h indicates the rejection or not of the null hypothesis i e if h 0 the null hypothesis is not rejected and thus both algorithms are considered equally good the rejection or not of the null hypothesis on a particular problem depends on the value of w which is the outcome of the wilcoxon test if w w c r i t the null hypothesis is rejected w c r i t is equal to 137 and denotes the critical value of w for a two tailed test with a sample size of 30 and 95 confidence level as an example in fig 3 we demonstrate one case where the cdf plots are sufficient for evaluating which algorithm is dominant in the particular problem i e 15 d rastrigin case with m f e 500 and one case where the wilcoxon signed rank test is essential to detect the dominating method i e 30 d rastrigin case with m f e 500 the aforementioned analyses clearly indicate the superiority of amseeas in most of examined problems in 15 d configurations the proposed algorithm dominates in five o f 1 o f 2 o f 4 o f 5 o f 6 and two o f 1 o f 5 problems for m f e 500 and m f e 1000 respectively as the number of control variables and thus the complexity of the optimization problem increases amseeas performs even more efficiently in particular it achieves best performance in five o f 1 o f 2 o f 4 o f 5 o f 6 problems for both computational budgets overall amseeas is stochastically dominant in 17 out of 24 problems pods in 5 whereas seeas in 3 the rest algorithms exhibit less satisfactory performance that is turbo 1 and dycors are optimal in only one case each while turbo m sop and mlmsrbf remain sub optimal across all problems it is important to highlight that all algorithms perform poorly against of4 zakharov and of5 rastrigin as none of them exhibits satisfactory convergence to the global minima especially in the 30 d space this is not surprising as these functions produce complicated response surfaces thus making it extremely difficult for the metamodels to fit the data and offer useful predictions 3 3 convergence behavior analysis in order to extend our analysis regarding the proposed algorithm s effectiveness we examined one additional issue referred to the convergence speed towards the global optimum in terms of objective function evaluations briefly in every run out of 30 in total of each out of 24 optimization problems we retrieved the best approximations to the associated global optimum as the objective function evaluations increase towards their maximum allowed limit i e m f e 500 or 1000 subsequently we computed the median values of the best solutions so far in order to plot and evaluate how fast the optimization evolves in each case the resulting convergence curves are presented in appendix b figure b1 to figure b6 in fig 4 we present for illustration purposes the resulting convergence curves of all competing algorithms in a particular optimization problem as the figures indicate in some optimization problems i e sphere zakharov amseeas exhibits faster convergence towards the global optimum even from the first few hundreds of function evaluations thus clearly outperforming the rest optimization methods besides that even when amseeas is initially evolving relatively slowly i e ackley rastrigin lévy afterwards its behavior is remarkably improved for instance in the 30 d lévy cases while seeas outperforms amseeas at the early stages of the optimization procedures after approximately four hundred evaluations the latter converges much faster another noteworthy case is the zakharov function for which the performance of seeas and other surrogate based approaches is rather poor see discussion by tsoukalas et al 2016 however the multi model approach coupled with the roulette and penalty mechanisms that are introduced in the amseeas version made the algorithm clearly more effective 4 real world benchmarking in a computationally demanding hydraulic design problem since the application of surrogate assisted methods in practice involves heb problems in order to obtain a more comprehensive picture of the proposed algorithm s capabilities we also test how amseeas performs in such a real world problem by comparing it to its precursor i e seeas 4 1 study area and optimization problem definition the optimization problem involves a hydraulic design study in the context of a broader flood risk assessment analysis which is described in detail in the recent work by efstratiadis et al 2022 the area of application is the lower course of trachones stream which crosses highly urbanized suburbs of athens greece its total drainage area is approximately 24 k m 2 and it extends in the south of athens between the foothills of mount hymettus and the coast the design optimization task refers to the sizing of levees along the open parts of the lower drainage network which is conceptually configured by means of 27 lateral weirs that are represented in the hec ras environment we remark that the three out of 27 lateral structures are internal and act as side channel spillways by transferring flow from one channel section to another these levees do not produce overflow and thus are not part of the design variables set the control variables of the design optimization problem are the elevations h i of 24 out of 27 individual levees which are allowed to receive non negative values up to 1 0 m two conflicting criteria are considered namely the total overflow occurring from the major channel segments over their associated levees and the total construction cost in this respect as the elevation increases the expected overflow decreases however the construction cost increases accordingly as explained latter for a given configuration of the drainage system the overflow over the levees is estimated through a hydrodynamic model driven with a specific design flood event on the other hand the construction cost of each levee is estimated by 2 c i c l i h i d where l i is the length of each individual levee d is the crest width same for all levees which is set equal to 2 m and c is an indicative unit construction cost which is for convenience set equal to 30 m 3 the objective function to minimize is expressed in dimensionless terms as follows 3 f h 1 h 2 h n t o v t o v max t c t c max 2 where t o v i 1 n v i and t c c i 1 n l i h i d specifically t o v denotes the total overflow t c stands for the overall construction cost of levees v i is the overflow of each individual weir i and n 24 is the number of control variables similarly t o v max is the maximum potential total overflow which refers to the do nothing solution i e zero increase of elevation in all 24 lateral structures and hence zero cost and is equal to 3272 6 10 3 m 3 while t c max denotes the maximum overall construction cost by assuming that all 24 individual levees are elevated by h max 1 0 m and is equal to 515 9 10 3 the above formulation of the objective function makes the optimization task independent of the uncertainty induced by the subjective assignment of input arguments d and c the hydrodynamic simulations are performed with the hec ras 6 1 software under one dimensional analysis with 5 m spatial resolution and a computational time step of 30 s the coupling of hec ras with the optimization algorithms is implemented in a python environment by utilizing the hec ras controller goodell 2014 which is part of the hec ras application programming interface api the controller incorporates a wealth of procedures that allow the manipulation of hec ras externally by setting input data retrieving input or output data and performing common functions such as opening and closing hec ras changing plans model running and plotting output data similar works employing the aforementioned api e g for automation purposes are those of siqueira et al 2016 leon and goodell 2016 and dysarz 2018 regarding the hydrodynamic modeling procedure since the longitudinal slopes of the stream branches are quite significant and the flow velocities are high the so called local partial inertia lpi technique fread et al 1996 is used for improving the stability of the numerical solution by setting a froude number threshold equal to 0 01 for the manning s coefficient parametrization the computational domain is classified into three specific friction zones i e cross sections constructed by concrete gabions and natural terrain for which we apply 0 016 0 025 and 0 030 s m 1 3 respectively the input flood event corresponding to the one dimensional hydrodynamic model boundary conditions is selected from the set of stochastic weather scenarios produced by efstratiadis et al 2022 and refers to a return period of 500 years and a 90 confidence level this challenging problem reveals the actual value of sbo techniques since to evaluate the objective function a time demanding hec ras simulation needs to be executed first specifically by using a 3 ghz intel core i9 processor with a 32 gb of ram a single run lasts 2 min consequently to obtain an optimized design within a reasonable time period only a few hundreds of function evaluations are allowed 4 2 performance comparison of the algorithms as the computational workload is quite heavy we consider a maximum allowed number of objective function evaluations equal to m f e 500 while to extract sufficient statistical outcomes we repeat the optimization procedure with seeas and amseeas for a total of ten times we remark that the computational time is almost fully dictated by the simulation stage which in turn mainly depends on the execution of hec ras provided that both algorithms are allowed to call the simulation 500 times each optimization run requires approximately 17 hours in contrast to theoretical test functions the global minimum of this highly complex engineering design problem cannot be known a priori on the other hand by setting all design variables equal to zero it is easy to detect that the do nothing solution results in a total overflow equal to 3272 6 10 3 m 3 and a zero cost thus an objective function value equal to 0 50 the key results obtained with seeas and amseeas are shown in table 6 these indicate that amseeas ensures much better performance in comparison to seeas while the outcomes of the latter are actually worse than the do nothing scenario the superiority of amseeas over seeas is shown even more clearly if we plot the cdfs of the associated optimal objective function values as shown in fig 5 amseeas is considered stochastically dominant over seeas in fig 6 we also plot the convergence curves of the two algorithms to evaluate how fast in terms of number of objective function evaluations does the convergence procedure evolve once again it is clear that amseeas outperforms seeas during the entire search procedure finally in table 7 we compare the optimized control variables in terms of elevation increase of the overall best solution found by each algorithm out of the ten optimization trials the layout of the best design solution of each algorithm is demonstrated in fig 7 a colorized scale ranging from blue to red is used to highlight the amount of additional elevation among the levees of the area in each case with the blue values referring to the lowest possible elevations 0 0 m 0 2 m and the red values to the highest ones 0 8 m 1 0 m it is evident that the best solution found by seeas requires higher elevation of the individual levees in comparison to the amseeas one which justifies the huge difference in the construction costs in particular the solution proposed by amseeas is cheaper by 75 51 while resulting in only 0 26 more total overflow than the seeas solution 5 conclusions global optimization problems are usually handled through objective functions the values of which are available after the execution of a black box simulation model however several of the models used in environmental sciences require high computational effort thus introducing significant barriers to the optimization procedure the conflicting aspects of model accuracy and computational hardware requirements led to the search for new ideas and tools to achieve satisfactory solutions within reasonable time in this respect sbo techniques are a well established approach for such challenging problems this study introduces the adaptive multi surrogate enhanced evolutionary annealing simplex amseeas method key novelty of which is the effective co operation of multiple surrogate models to ensure flexibility against objective functions and associated response surfaces of different characteristics a virtual roulette is introduced to decide which sm should be activated in every iteration the probability of each metamodel being selected by the roulette for prediction depends on how accurately it fits existing data each metamodel comes with a penalty counter which increases whenever the metamodel makes a bad prediction these counters can permanently discard surrogates when reaching a specific threshold to assess the efficiency and effectiveness of the proposed algorithm we initially benchmarked it against six state of the art surrogate based global optimization methods in six characteristic theoretical mathematical problems with alternative settings i e two alternative dimensions and two computational budgets thus resulting to 24 optimization problems in total the results emerging from this analysis are encouraging as amseeas proves its robustness by outperforming the other algorithms in most of problem settings in particular amseeas is considered stochastically dominant over its competitors in 17 out of 24 problems next we contrasted amseeas against seeas i e the original method on which the proposed algorithm is based in a highly challenging real world problem from the field of hydraulic design of flood protection works a key barrier to such problems is the computational burden induced by the use of detailed hydrodynamic models in this case hec ras to assess the performance of a specific design in terms of flood hazard and eventually flood risk while also keeping the cost of the proposed hydraulic infrastructures to a minimum additional challenges are induced by the problem dimension since in the examined case we were looking for optimizing 24 design variables that represent the levee heights across the open channel network within our analysis we detected the optimal solutions found by amseeas and seeas from a set of ten independent runs and under a limited computational budget of only 500 function evaluations the design proposed by amseeas ensures a substantially decreased construction cost with respect to seeas and at the same time a low flood inundation risk marginally only exceeding the estimated inundation hazard that results from the optimal design by seeas our extended analyses with the mathematical problems and the hydraulic design application as well indicate that amseeas is a robust and efficient optimization algorithm able to handle computationally challenging heb problems in practice without compromising neither on simulation model sophistication nor on proper probabilistic treatment of complex environmental problems this is due to the fact that the proposed method does not simply incorporate multiple surrogate models to support the optimization procedure the introduced mechanisms behave in a completely stochastic manner and result in the adjustment of the exploited metamodels on the given objective function to minimize depending on the characteristics and irregularities of the response surface some metamodels might be of actual assistance while others might have the opposite behaviour and undermine the convergence process by pointing towards directions other than the region of interest however the inclusion of the virtual roulette and penalty mechanisms ensures that all inappropriate surrogates for a given problem will abstain from the search procedure while the more appropriate ones will be given the chance to assist it up until they stop producing useful information this strategy assures that the number of wasted objective function evaluations is limited to a minimum extent and that the genetic operators of seeas get the most proper assistance on any given optimization problem eventually this strategy seems to be a very promising one towards ensuring adaptation of the search procedure to response surfaces of varying characteristics acknowledgements the research leading to these results has received funding from the european union s horizon 2020 research and innovation programme from the eu horizon 2020 green deal call under grant agreement no 101037084 for the research project impetus dynamic information management approach for the implementation of climate resilient adaptation packages in european regions the research and its conclusions reflect only the views of the authors and the european union is not liable for any use that may be made of the information contained herein the authors would like to thank the editor dr saman razavi and the three reviewers dr jeremy white and two anonymous ones for their constructive comments suggestions and critique which helped to provide a significantly improved manuscript appendix a empirical cdfs for mathematical test functions fig a1 empirical cdfs for test function of1 sphere with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a1 fig a2 empirical cdfs for test function of2 ackley with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a2 fig a3 empirical cdfs for test function of3 griewank with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a3 fig a4 empirical cdfs for test function of4 zakharov with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a4 fig a5 empirical cdfs for test function of5 rastrigin with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a5 fig a6 empirical cdfs for test function of6 lévy with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig a6 appendix b convergence curves for mathematical test functions fig b1 convergence curves for test function of1 sphere with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b1 fig b2 convergence curves for test function of2 ackley with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b2 fig b3 convergence curves for test function of3 griewank with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b3 fig b4 convergence curves for test function of4 zakharov with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b4 fig b5 convergence curves for test function of5 rastrigin with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b5 fig b6 convergence curves for test function of6 lévy with 15 a b and 30 variables c d with m f e 500 a c and m f e 1000 b d fig b6 
25429,in the lower mekong river basin floodplains rice cultivation is highly crucial for regional and global food security however prolonged flooding can pose damage to rice cultivation and other socio economic aspects yet there is no rapid operational inundation forecasting system that can help decision makers proactively mitigate flood damages here we integrated the so called forecasting inundation extents using rotated empirical orthogonal function analysis fier framework with an altimetry based operational mekong river level forecasting system and built an operational web application fier mekong https fier mekong streamlit app that generates daily skillful forecasted inundation extents 70 of critical success index and depths in about 3 and 30 s respectively with up to 18 day lead times one of its applications predicting flood induced rice economic losses is also presented had fier mekong being adopted we estimated that the rice damages up to 87 and 53 million us dollars during the 2020 and 2021 harvest time respectively could have been avoided keywords operational inundation forecasting sar imagery flood risk prediction mekong river basin data availability data will be made available on request 1 introduction in the lower mekong river basin lmrb floodplains including the tonle sap lake floodplains tslf cambodian floodplains cf and vietnamese mekong delta vmd inhabitants largely rely on the resources gifted by the mekong river mr where flood patterns with strong seasonality nourish the floodplain agriculture and freshwater fisheries that are major supporters of the livelihoods of local inhabitants mekong river commission mrc 2011 agriculture especially rice cultivation is the main foundation of national economies and sustains not only regional but also global food supply maitah et al 2020 matsubara et al 2020 okazumi et al 2014 triet et al 2018 the cambodian population relies on rice production in the tslf and cf for both domestic consumption and commercial export cramb et al 2020 vietnam is the fifth largest rice producer and the third largest rice exporter in the world maitah et al 2020 with more than half of production coming from the vmd kien et al 2020 bich tho and umetsu 2022 however rice paddies are vulnerable to prolonged floodings and can die after being continuously submerged in water for days mrc 2009 failure of rice crops that are supposed to feed local and global populations will consequently lead to larger regional or even global scale impacts prolonged floodings can also pose threats to other socio economic aspects such as infrastructure and soil fertility and might even directly threaten human lives horton et al 2022 oddo et al 2018 moreover recent studies have reported an increasing flood occurrence and intensity in the lmrb floodplains due to climate change chen et al 2020 try et al 2020a 2020b therefore it is vital to rapidly predict flood inundation extents to help decision makers develop proactive damage prevention measures in a timely manner that can mitigate flood induced socio economic damages however the hydrological system in the lmrb floodplains is highly complex with multiple processes including inflow from the mr mainstem the unique tonle sap lake tsl flow reversal and tidal intrusions see section 2 for details in addition the region has largely flat terrains and densely distributed man made hydraulic structures these conditions impede the implementation of a rapid operational inundation extents forecasting system using conventional approaches including the hydrodynamic modelling approach and the non modelling terrain based approach the hydrodynamic modelling approach has been widely used to transform discharge outputs from rainfall runoff models to distributed inundation extents by solving one dimensional saint venant equations or two dimensional shallow water equations however these models suffer from several sources of error bates et al 2014 including model structural errors and uncertainties in 1 the model input data including the rainfall runoff data to set the boundary and initial conditions 2 input digital elevation model dem and channel bathymetry data 3 friction coefficients to represent energy loss mechanisms and 4 information about hydraulic structures in the reach all these uncertainties in hydrodynamic model calibration boundary conditions and topographic data can significantly influence flood inundation predictions teng et al 2017 bates et al 2014 furthermore the required spatial parametric inputs may not always be available chen et al 2019 leandro et al 2014 teng et al 2017 indeed dung et al 2011 and triet et al 2020 2018 who simulated inundation extents in the vmd by a one dimensional hydrodynamic model mike11 pointed out the need for detailed dike survey data and light detection and ranging lidar dem for the model to generate inundation extents with satisfactory skills in the vmd triet et al 2018 also mentioned that it is challenging for a full two dimensional model to be implemented in the vmd due to the need for highly accurate topographic and hydraulic data in the region in addition the models require a heavy computational burden especially for a high resolution large scale forecasting framework predicting inundation extents for the upcoming 3 days in a small area less than 300 km2 using 100 m and 30 m resolution mike 21 models was found to take 396 min and 1012 min respectively fraehr et al 2022 therefore its high computational cost makes many operational applications such as ensemble and real time forecasting unfeasible particularly in developing countries due to their lack of adequate technology infrastructure fraehr et al 2022 zhou et al 2021 biswas and hossain 2018 on the other hand the non modelling terrain based approach such as height above nearest drainage hand nobre et al 2016 zheng et al 2018 estimates the inundation extents based on topography data and water depths it first normalizes the topography according to local relative heights along the drainage network to generate hand values then a rating curve is used to transform streamflow forecasts to depths for a given river cell finally based on the planar water surface approximation flood inundation extents are determined by selecting surrounding land cells from which hand values are less than the given water depths in the stream nobre et al 2011 teng et al 2017 the hand approach requires significantly less computational power than hydrodynamic models and may work well on confined floodplains with steep valleys and straight river reaches bates and de roo 2000 wing et al 2019 however a recent study by johnson et al 2019 demonstrated that the method severely overpredicts inundation extents in regions of low relief in addition the hand approach does not account for overland water flow and backwater effects caused by infrastructure and coastal flooding with the flat terrains in the lmrb floodplains especially over the cf and vmd balica et al 2014 and a dense network of flood protection infrastructure in the vmd the hand approach may not be applicable recently space borne remote sensing especially using the synthetic aperture radar sar and visible and near infrared sensors has emerged as a powerful tool to depict areal inundation dynamics with repeated views over a wide range of locations e g ahamed and bolten 2017 kim et al 2017 lee et al 2015 oddo et al 2018 smith 1997 chang et al 2020 using the tslf as a test bed proposed a satellite imagery based inundation extent forecasting framework that addresses the need for computationally efficient areal inundation estimation and forecasts with high temporal resolution this framework is named forecasting inundation extents using rotated empirical orthogonal function analysis fier simply speaking fier forecasts inundation extents based on a correlation between historical inundation extents and hydrological data water levels or streamflow once the correlation is identified inundation extents can be forecasted with forecasted hydrological data available from an external forecasting system specifically fier decomposes multi temporal historical satellite images into significant spatiotemporal patterns using the rotated empirical orthogonal function reof analysis kaiser 1958 lorenz 1956 the patterns are then coupled with the hydrological data by regression models then the forecasted hydrological data when available can be used as inputs to synthesize forecasted satellite like images from which the corresponding forecasted inundation extents can be delineated consequently fier becomes much more computationally efficient and scalable than the conventional approaches fier has been first implemented in the tslf fier tslf for daily hindcast and forecast of inundation extents to test its feasibility without operationalization using a multi temporal stack of sentinel 1a images daily interpolated jason altimetry derived tsl levels and the climate index chang et al 2020 in this study we took a step further to implement for the first time an operational fier system that can rapidly forecast skillful inundation extents over not only tslf but the entire lmrb floodplains fier mekong toward its operationalization first of all we have operationalized our in house satellite altimetry based mr water level forecasting system chang et al 2019 see section 3 3 and the supplementary information secondly we automated the regression modeling by adopting the neural network regression so that the regression models can be built without pre assuming a function to be fit to the data imani et al 2014 such automatic model building can help us scale up the implementation of fier to other areas of interest thirdly we adopted a water classification technique using z score statistics for faster and more efficient inundation mapping devries et al 2020 lastly we developed an operational fier mekong web application that can rapidly generate and visualize daily forecasted inundation extents in about 3 s and depths in about 30 s with up to 18 day lead time on a daily basis the forecasted inundation depths were generated by utilizing the publicly available google earth engine gee based floodwater depth estimation tool fwdet gee peter et al 2020 the web application also allows users to export the forecasted inundation extents and depths as geocoded tiffs geotiffs a widely used georeferenced raster data format for further geospatial analysis of their interests the web based platform has been recognized as a powerful tool for disseminating flood risk information knight et al 2015 hearn 2009 many web based tools have been developed for either real time flood forecasting or flood risk assessment under different scenarios for example mourato et al 2021 presented an operational fluvial flood forecast and alert system in águeda municipality portugal with up to 3 days of lead time knight et al 2015 developed a web based geospatial decision support tool for two coastal cities in the united kingdom and assessed their potential flood risk under different scenarios of sea level rise storm intensity wave height and river flow mohanty and karmakar 2021 presented a web based flood risk information system for the flood prone jagatsinghpur district in eastern india and analyzed the flood risks under different scenarios of rainfall and storm tides however to the best of our knowledge the web based operational inundation extent and depth forecasting system is the first of its kind that covers the entire lmrb floodplains the forecasted inundation extents and depths can be used for further geospatial analysis such as conducting spatial and temporal prediction of flood induced socio economic damages with relevant land cover data the spatio temporal flood damage prediction can help decision makers take proactive actions to manage and prevent flood damages such as disseminating timely and effective early warnings in this paper we presented the forecasted inundation extents in 2020 and 2021 generated by fier mekong and evaluated the forecasting skills considering the importance of rice cultivation in the lmrb floodplains we also demonstrated the application of fier forecasted inundation extents on spatio temporal prediction of flood induced rice damages the structure of this paper is as follows section 2 describes the characteristics of the lmrb floodplains section 3 introduces the data that were used for the fier mekong section 4 explains the fier mekong process skill evaluation indices fwdet gee and how the early risk assessment was conducted section 5 describes the fier mekong web application section 6 presents the results of fier forecasted inundation extents the skill evaluations and demonstrates the potential application of the fier mekong in early assessment of flood induced economic losses of rice production the scalability of the fier framework is also discussed finally section 7 concludes the paper and discusses future scopes 2 study area the mr the largest in southeast asia is a transboundary river flowing through six countries china myanmar lao pdr thailand cambodia and vietnam with headwater in the tibetan plateau and the river mouth in the vmd this paper focuses on forecasting the inundation extents in the downstream floodplains of the mrb called the lmrb floodplains in cambodia and vietnam the area is about 66 589 km2 and encompasses the tslf cf and vmd the flood pulse here has a distinct seasonal cycle governed by the southwest monsoon with the wet season from june to october pagano 2014 the geographical locations of these floodplains are shown in fig 1 along with a sentinel 1a vv polarized intensity image that illustrates the spatial coverage of the images used see section 3 1 for a detailed description of the sentinel 1a images historical maximum inundated areas are also delineated in blue based on the joint research center jrc global surface water data pekel et al 2016 tsl is located in cambodia and is the largest natural freshwater lake in southeast asia its seasonal flow reversal makes it unique in the world in the wet season the high water level in the mr mainstem causes the water flows reversely toward the tsl due to the inflow of water from the mr mainstem inundation extents in the tslf can be increased sixfold in the wet season from around 2500 km2 15 000 km2 during the dry season the water level in the mr recedes and the water in tslf flows toward the cf and vmd mrc 2021 the cf is located south of the tslf and extends from kratie to the cambodia vietnam national border vmd spans from the cambodia vietnam national border toward the mouth of the mr where ocean tides can influence the flow variations our study years 2020 and 2021 are considered dry years with lower than average 2008 2017 wet season inflow from the upstream during these years rainfall from june to august was lower than the long term average but then increased in september and october reflecting the delayed onset of southwest monsoons this has caused the tsl flow reversals during the wet seasons of these years to be the lowest since 2008 mrc 2022 3 data 3 1 sentinel 1 sar imagery sentinel 1a equipped with a c band 5 405 ghz sar imaging sensor is a satellite mission under the copernicus earth observation program of the european space agency esa it was launched on april 3rd 2014 and has been consistently providing freely available imagery with a 12 day revisiting cycle we used sentinel 1a vv polarized ground range detection high resolution grdh intensity images from which spatio temporal patterns have been extracted using the reof analysis the vv polarization was chosen considering its superior surface water mapping capability markert et al 2020 twele et al 2016 to cover the lmrb floodplains four frames including frames 23 29 34 and 39 of path 26 were used see fig 1 we also used sentinel 1a images for cross comparison with the fier forecasted inundation extents table 1 summarizes the numbers of sentinel 1a images used with their acquisition periods we leveraged the publicly available cloud based gee data catalog and gee python based application programming interface api to alleviate the computational burden of sentinel 1a imagery retrieval and preprocessing a gee api based open source python application called hydrologic remote sensing analysis for floods hydrafloods https servir mekong github io hydra floods was used for preprocessing including image mosaicking slope correction vollrath et al 2020 using the multi error removed improved terrain dem merit dem yamazaki et al 2017 and the gamma map speckle filtering lopes et al 1990 the jrc s global surface water data was used to mask out the permanent water bodies in the sentinel 1a images to mitigate the influence of surface roughness changes on sar intensities which are often caused by winds another gee api based python package called restee https github com kmarkert restee was used to convert the preprocessed gee server side sentinel 1a image collection to the client side python gridded dataset with user defined spatial coverage and resampling resolution the converted dataset can then be processed using non gee based apis and python libraries to build the prototype of an operational inundation forecasting system a spatial resolution of 0 005 which is equivalent to approximately 552 m was simply set accordingly the fier estimated inundation extents have the same spatial resolution both the hydrafloods and restee were developed by the nasa servir coordination office sco the sentinel 1a images merit dem and jrc data were all retrieved from the gee data catalog and preprocessed with the gee a scheme of the data preprocessing is shown in fig 3 3 2 merit dem the merit dem yamazaki et al 2017 was used for the slope correction of multi temporal sentinel 1a images and as the topographic reference to derive the inundation depth maps from the inundation extents it is a 3 arc second resolution global dem with the earth gravitational model 1996 egm96 geoid as a datum it uses the srtm3 dem and the advanced land observing satellite world 3d 30 m dem aw3d 30m dem as the baseline where the unobserved gaps were filled by the viewfinder panoramas dem it has improved accuracy than the srtm dem since its bias and errors were mitigated using the additional dataset including the ice cloud and land elevation satellite icesat laser altimetry land surface elevation data landsat derived forest cover dataset hansen et al 2013 and nasa global forest height data simard et al 2011 3 3 mekong river water levels observations and forecasts the historical in situ water levels provided by the mrc https portal mrcmekong org home were used to examine their correlation with the temporal patterns extracted from the multi temporal sentinel 1a images fig 1 shows the locations of the in situ gauges used which are kratie koh khel chau doc and can tho from upstream to downstream shown with red dots since fier requires forecasted water levels at these gauges to generate forecasted inundation extents the operational variable infiltration capacity vic model aided satellite altimetry based water level forecasting system chang et al 2019 was used details in supplementary information essentially the water level forecasting system uses upstream altimetry derived water levels to forecast the downstream water levels at the gauges by assuming a linear relationship between them this approach was adopted because 1 it is computationally efficient and thus suitable for the operational purpose and 2 it forecasts water levels with promising skills at can tho in the vmd which mrc does not routinely issue chang et al 2019 pagano 2014 although the mr water level forecast system has up to 20 day lead times the operational maximal lead time of fier mekong becomes 18 day due to the latency of input forcings of the vic model hossain et al 2017 3 4 rice cover market price and yield calculating the wet season flood induced rice economic losses requires spatial distribution market price and the yield of rice the rice harvest times need to be also considered the wet season harvest time in the cf is around late august to early september okazumi et al 2014 on the other hand in the vmd the wet season harvest time spatially varies for double cropping areas the harvest time is from july to early august for triple cropping areas there is an additional harvest time depending on whether the areas are fully protected or not triet et al 2018 we only considered the additional harvest time of the triple cropping areas that are not fully protected and can be vulnerable to flooding which is from late august to early september triet et al 2018 for the spatial distribution of rice paddies in the cf the latest landcover data for the year of 2018 from the servir mekong regional land cover monitoring system https www landcovermapping org en landcover servir mekong accessed on mar 24th 2022 was used in the vmd since the wet season harvest time spatially varies depending on local annual cropping times the latest land cover data for the year of 2020 of vu et al 2022 https data mendeley com datasets kpftzmsyyz 2 was used both landcover data were spatially interpolated to be aligned with the inundation extents from fier mekong using the nearest neighbor method see fig 2 the rice market price in us dollar usd ton was retrieved from food and agriculture organization fao https www fao org markets and trade commodities rice fao rice price update en fao accessed on mar 24th 2022 the rice yield ton ha was retrieved from the u s department of agriculture usda https ipad fas usda gov countrysummary usda accessed on mar 24th 2022 these data are summarized in table 2 for the rice market price we used the average price of all rice varieties for the rice yield we used the average yield of the market year 2019 2020 and 2020 2021 for 2020 and the average yield of 2020 2021 and 2021 2022 for 2021 4 methods this section explains how fier was implemented in the lmrb floodplains fier mekong fig 3 shows the flowchart of the entire process it requires the multi temporal sentinel 1a intensity images and historic in situ mr water levels from mrc to construct the regression models with the forecasted mr water levels from section 3 3 the forecasted inundation extents can be produced then the fier forecasted inundation extents were combined with the merit dem to determine the forecasted two dimensional inundation depths using fwdet gee finally using the forecasted inundation depths the flood induced rice economic losses were estimated to demonstrate the applicability of fier mekong for preventing or mitigating the anticipated flood damages on rice paddies all the processes are python based and were run locally except for the gee apis which use the gee server as the backend yellow boxes in fig 3 4 1 backbone of fier mekong 4 1 1 overview of fier essentially fier forecasts the inundation extents based on the relationships between the observed historical inundation extents with hydrological data technical details of fier can be found in chang et al 2020 and a summary of the fier framework is given here for the readability of this paper firstly the reof analysis kaiser 1958 lorenz 1956 was applied to obtain the significant spatio temporal patterns of multi temporal sentinel 1a images it starts with the conventional empirical orthogonal function eof analysis which is statistically identical to the principal component analysis followed by a varimax rotation to enhance the physical interpretability of the decomposed spatio temporal patterns the outputs include static spatial patterns called the rotated spatial modes rsms and their corresponding temporal patterns called the rotated temporal principal components rtpcs next flood related rtpcs and their corresponding hydrological drivers such as the mr water levels used in fier mekong were identified based on correlation analysis then regression models that couple the flood related rtpcs and their corresponding best correlated mr water levels were built by feeding forecasted mr water levels into the regression models forecasted rtpcs can be obtained next the forecasted sentinel 1 like images can be synthesized by summing products of the forecasted rtpcs and the static rsms finally forecasted inundation extents can be obtained by applying a water classification method to the forecasted sentinel 1 like images as previously mentioned different from chang et al 2020 this study automated the regression modeling with the neural network regression and improved the efficiency of water classification with the z score statistic based approach devries et al 2020 which will be explained in section 4 1 2 and 4 1 3 respectively 4 1 2 neural network regression the optimal regression models were determined by the grid search with k fold cross validation approach this approach is an automatic trial and error process where the training and testing are conducted using different neural network architectures and different combinations of data subsets the data are first divided into k subsets one of the subsets is used as a test set while others are used as a training set the neural network architectures in the searching space are trained until the training scores have converged and then test scores are recorded finally once every subset has been used as the test set the process ends the optimal neural network architecture is the one that provides the best mean test score over different test sets such a process helps prevent the models from being overfitted kim et al 2021 the number of k is generally from 2 to 10 which could be determined based on data availability zhou et al 2017 considering the data size was 85 acquired within three years see table 1 we employed k 3 three fold which ensured the test set encompassed a full hydrologic cycle a year of data both the rtpcs and mr in situ water levels were normalized before the training which is a common measure to expedite the training process the rectified linear unit relu activation function was adopted in the hidden layers and the adam gradient descent algorithm with a default learning rate 0 001 was adopted as an optimizer the mean squared error was used as the loss function to evaluate the scores the keras a python based api with tensorflow libraries as the backend was used for the neural network regression process see fig 4 for illustration the flood related spatio temporal patterns of multi temporal sentinel 1a images as well as the neural network regression models are shown in section 6 1 4 1 3 water classification using z score statistics we applied a rapid and robust water classification method which is based on the z score statistics of the wet season images devries et al 2020 the method compares the change between the wet season and baseline images representing the dry season with the change among the baseline images using the z score statistics and then performs water classification the z score statistics can be derived as 1 z σ 0 σ 0 b a s e l i n e s t d σ 0 b a s e l i n e where z is the z score image σ 0 is the wet season image to be classified fier synthesized or observed sentinel 1a image σ 0 b a s e l i n e and s t d σ 0 b a s e l i n e are the mean and standard deviation std of the baseline images respectively note that the fier synthesized forecasted images are the anomalies with respect to the mean of the multi temporal sentinel 1a images from which the spatiotemporal patterns were extracted by the reof analysis since the multi temporal sentinel 1a images encompass the wet season images their mean also contains some water related signals and therefore was added back to the fier synthesized forecasted images before water classification the baseline period was considered as the months when the monthly average in situ water levels from 2017 to 2019 corresponding to the acquisition period of the multi temporal sentinel 1a images at the four identified gauges were the lowest then the sentinel 1a images acquired in these months were used to calculate σ 0 b a s e l i n e and s t d σ 0 b a s e l i n e the z score image was then generated and the pixels that have low z scores were considered to be flooded here we classified the pixels whose z scores are lower than 3 as flooded following devries et al 2020 details about the operational system will be explained in section 5 the forecasted inundation extents generated by fier mekong were cross compared with the inundation extents derived from the real sentinel 1a images for skill evaluation which will be shown in section 6 2 4 2 skill evaluation of fier mekong the skill of forecasted inundation extents in the lmrb floodplains estimation was evaluated with pixel count based indices including the overall accuracy and critical success index csi derived from the confusion matrix kohavi and provost 1998 the inundation extents derived from real sentinel 1a images were considered as observations as fig 5 illustrates a and d are the numbers of pixels that fier correctly estimates true positive and true negative respectively conversely b and c are the numbers of misestimated pixels indicating false positive and false negative respectively the overall accuracy is the percentage of pixels that fier correctly estimated either true positive or true negative over the total number of pixels 2 o v e r a l l a c c u r a c y a d a b c d 100 the csi gilbert 1884 also called threat score is the number of pixels that fier correctly estimated as inundated over the number of pixels that were either estimated or observed as inundated 3 c s i a a b c 100 the csi avoids possible bias caused by true negative d wing et al 2017 both overall accuracy and csi can have a value from 0 to 100 where 0 means there is no match between the estimation and observation while 100 means perfect estimation 4 3 fwdet gee gee based inundation depth estimation the inundation depths were determined by using the fwdet gee peter et al 2020 the tool overlays the inundation extents on a dem and then calculates the differences between the elevations of inundated pixels and their nearest inundated boundary pixels to estimate the inundation depths firstly the inundation boundary pixels are identified and their elevations are retrieved secondly elevations of the nearest inundation boundary pixels are assigned to the non boundary inundated pixels finally the floodwater depths at inundated pixels are estimated by subtracting the actual dem value from the assigned boundary elevations compared with the previous arcgis and qgis versions cohen et al 2019 the major advantage of the gee version is its significantly reduced processing time the gee script is freely accessible online https dataverse harvard edu dataset xhtml persistentid doi 10 7910 dvn jq4bcn in this study fier forecasted inundation extents and merit dem were used to obtain the fier forecasted inundation depths 4 4 flood induced rice economic loss estimation to assess the potential economic impact of fier mekong we estimated the flood induced rice economic losses based on the fier forecasted inundation depth maps the process starts with a rice relative damage curve which is a function of consecutive flood duration days and inundation depths m mrc 2009 this approach has been widely used in the literature okazumi et al 2014 chung et al 2019 in mrc 2009 the flood damages to rice paddies are categorized with respect to different inundation depths and durations as 1 rice paddies will not be affected with inundation depth 0 5 m 2 rice crops will be damaged by flood with a duration longer than 5 days 3 rice will be completely destroyed with inundation depth 1 5 m and duration 13 days with these okazumi et al 2014 derived the damage curve with up to 13 days of flood durations and up to 1 5 m of inundation depths as 4 r d h w 0 5 86 875 22 5 d 0 625 d 2 where r d is rice relative damage h w is inundation depth m 0 5 h w 1 5 if h w 1 5 then h w 1 5 and d is inundation duration day if d 13 then d 13 fig 6 shows examples of the relative damage curves both h w and d can be derived from the daily fier forecasted inundation depth maps the rice relative damages r d were then used to quantify the corresponding rice economic losses as 5 d m p y a i 1 w r d i where d is flood induced rice damage usd of the country inside the sentinel 1a image frames we used m p is rice market price usd ton y is rice yield ton ha a and r d are area ha and aforementioned relative damage of a pixel and w is the total number of pixels that represent rice under flood risk based on the aforementioned criteria in mrc 2009 inside the image frame for each country table 2 has summarized the rice market price and yield the spatial distribution of rice paddies has been shown in fig 2 5 operational fier mekong web application 5 1 configuration the operational fier mekong web application was constructed using streamlit an open source python framework that helps create deploy and host web applications in streamlit framework a main frontend python script that depicts the frontend elements e g button select box calendar along with other required libraries and data are stored in the github repository the web application and repository are deployed and hosted by streamlit cloud platform any changes on the github repository will be automatically updated the operational fier mekong web application requires the following data 1 automatically updated mr water level forecasts 2 the rsms and neural network regression models which are used to routinely synthesize the forecasted sar like images and 3 the mean and std of historical sentinel 1a intensities for the water classification using z score statistics the water level forecasts are automatically generated and then managed stored and updated to the google sheet database by using google sheets api and a python library called pygsheets the automatic updating process of forecasted water levels is executed by the linux crontab on a daily basis the rsms regression models and mean and std of historical sentinel 1a intensities were pre obtained locally see section 4 1 and uploaded to the github repository as fig 7 shows the operation of the web application begins with updating the water level forecasts then the streamlit cloud platform will access the updated forecasted water levels and their dates the forecast date can be selected from the calendar in the frontend then the corresponding forecasted water levels will be fed into the fier regression models to estimate the forecasted rtpcs next the forecasted rtpcs and the rsms are multiplied and linearly summed up to synthesize the forecasted sentinel 1 like images from which the forecasted inundation extents can be finally obtained these processes are all done by the streamlit cloud platform users can request the web application to generate the forecasted inundation depths as well note that fier mekong can also provide hindcasts of inundation extents and depths by using historical in situ water levels as inputs 5 2 graphic user interface gui the user friendly frontend gui is shown in fig 8 with the selected date of interest with up to 18 days of lead times the corresponding forecasted water levels in the google sheet database will be retrieved and the forecasted inundation extents will be visualized there is also an option to generate and visualize the inundation depth end users can also choose to export the maps as geotiff for further geospatial analysis fier mekong also allows users to generate hindcasted inundation extents and depths by selecting hindcast as the run type 6 results and discussions 6 1 flood related spatio temporal patterns and fier regression models here the first four modes of spatio temporal patterns accounting for about 64 of the total variance of the multi temporal sentinel 1a images were identified as significant based on the monte carlo simulation and were rotated to obtain the rsms and the corresponding rtpcs hereafter the first mode of rsm rtpc will be called rsm 01 rtpc 01 and the second mode of rsm rtpc will be called rsm 02 rtpc 02 and so forth a correlation analysis between the four significant modes of rtpcs and the in situ water levels was conducted the four gauges from which water levels are best correlated with rtpc 01 to rtpc 04 were then identified which are chau doc koh khel can tho and kratie respectively the top panel of fig 9 shows the rtpcs along with their best correlated in situ water levels and correlation coefficients corr which are all higher than 0 7 to 0 8 it means the changes in sentinel 1a sar intensities are temporally relevant to the water level changes at these gauges which can be further interpreted together with the corresponding rsms shown in the bottom panel of fig 9 the positive correlations between the rtpcs and water levels indicate that when water levels are higher the rtpcs will also be higher and lead to lower synthesized intensity values after being multiplied with the negative value pixels blue pixels in the rsms similar to the case of sar the lower synthesized intensity values indicate flooding by comparing with the blue shaded areas in fig 1 we can see that the negative value pixels in rsms are mostly observed inside the floodplains regression models between the flood related rtpcs and corresponding best correlated mr in situ water levels were then built using the neural network regression optimized by the grid search with k fold cross validation approach table 3 shows the final neural network architecture mean training score mean test score and the corresponding r2 values of each mode the r2 values were all higher than 0 6 to 0 7 considered to be satisfactory moriasi et al 2007 fig 10 shows the scatter plots of rtpcs and their corresponding water levels along with the regression models 6 2 fier forecasted inundation extents and skill evaluations to evaluate the skill of fier mekong we generated fier forecasted inundation extents with up to 18 day lead time in the wet season of 2020 and 2021 estimation which were cross compared with the inundation extents directly delineated from 26 sentinel 1a images acquired during the same period observation see table 1 fig 11 shows the mean and std of overall accuracies and csis from different lead times 1 day to 18 day as can be expected the skills of fier forecasted inundation extents generally degrade with longer lead times however the mean accuracies are near 98 and mean csis are about 72 74 which are considered to be fairly good bernhofen et al 2018 the degradation of fier inundation forecasting skills with longer lead times may be due to errors in the forecasted water levels see fig 8 of supplementary information which are used as an input of inundation forecasts since the forecasted water levels were generated based on vic model simulated discharges and altimetry derived water levels at upstream virtual stations vss errors from both can influence the fier inundation forecasting skills the potential error sources include 1 the errors of input forcing data of the vic model 2 the errors in the altimetry derived water levels at upstream vss and 3 the assumption that there is a linear relationship between upstream and downstream water levels ignoring the impact of tributaries and direct rainfall in between in addition the lmrb floodplains specifically in the vmd have a strong human intervention that alters the natural flood regime irregular human operation of water resources infrastructure e g sluice gates water pumping in the region could alter the water levels hung et al 2012 2014 that cannot be predicted and consequently may lead to errors in the fier forecasted inundation extents fig 12 shows the wet season monthly inundation occurrences days representing the inundation dynamics generated using fier mekong the top and middle panels of fig 12 illustrate the occurrences using the fier forecasted inundation extents with 18 day lead time for the wet seasons of 2020 and 2021 we start to see some inundations in august and much higher occurrences in september in october the inundation occurrences in the lower vmd slightly increased the much higher inundation occurrences in september and october 2020 and 2021 generated based on the fier mekong forecasts agree with the onset of increased rainfall see section 2 the fier forecasted inundation extents for 2020 and 2021 were not extensive as they were both dry years mrc 2022 to demonstrate the skill of fier mekong for a wet year fier hindcasted inundation extents for the wet season of 2018 the most recent wet year were generated as mentioned earlier fier mekong can also generate hindcasts of inundation using the historical water levels for a date of interest obtained at the four in situ stations see fig 1 the corresponding monthly inundation occurrences for the wet season of 2018 are shown at the bottom panel of fig 12 by cross comparing with real inundation extents derived from 11 sentinel 1a images acquired in the 2018 wet season the accuracy and csi of the hindcasted inundation were 96 6 1 9 and 75 6 14 9 respectively in 2018 inundations spread out earlier than in 2020 and 2021 and were much more extensive some inundations started to occur in july in august the inundation occurrences became much greater all over the tslf and cf according to the mrc report 2020 the inundation in the wet season of 2018 reached nearly the maximum extent compared to satellite images obtained from 2006 to 2017 mrc 2020 the spread of inundation in july and august of 2018 happened earlier than usual caused by an early onset of high rainfall mrc 2020 then in october the inundation occurrences significantly increased and widely spread over the lower vmd these results show that fier mekong is capable of simulating inundation extents even for an extreme case in other words if the input forecasted water levels for this upcoming wet season are much higher than the ones in 2020 and 2021 fier mekong can be used to obtain more widespread and extreme inundations to show the locations where users can have higher confidence in the fier forecasted inundation extents pixel wise temporal correctness rates were also calculated the temporal correctness rates represent the percentage of times that each pixel was correctly estimated either true positive or true negative over the total number of sentinel 1a images used for the cross comparison 26 images as table 1 shows to summarize the temporal correctness rates over different lead times the mean and std of the rates with 1 day to 18 day daily lead times were calculated as shown in the top panel of fig 13 respectively their cumulative histograms over the pixels that have ever been estimated or observed as inundated were also generated showing approximately 83 of pixels have mean temporal correctness rates 80 bottom panel of fig 13a and about 93 of pixels have std of temporal correctness rates 4 bottom panel of fig 13b the cumulative histograms indicate that most of the pixels were correctly estimated most of the time and consistently with different lead times 6 3 application flood induced rice economic losses here we attempted to assess the potential economic impact of fier mekong by demonstrating how it can be used to estimate flood induced rice economic losses since rice paddies become destroyed when they are continuously submerged under water for days fier mekong can be used to forecast prolonged inundation toward spatial pre event damage prediction here the fier forecasted inundation extents fwdet gee tool peter et al 2020 and the flood induced rice relative damage curves mrc 2009 along with the rice data in table 2 were integrated to calculate the flood induced rice economic losses as a result the spatial prediction of the maximal flood induced rice economic losses for the next 13 days was conducted for 2020 and 2021 for example since 2020 and 2021 were both dry years a post event damage assessment for 2018 the most recent wet year was also conducted using the fier hindcasted inundation depths and rice data of 2018 we focused on the wet season harvest time see section 3 4 to calculate the losses assuming the rice has not been reaped the rapid spatial predictions of flood induced losses can give decision makers timely information about when and where rice would be damaged along with the severity fig 14 a shows an example of such spatial predictions of flood induced rice economic losses in usds for the periods of august 29th to september 10th of 2020 arbitrarily chosen for the wet season harvest time using the forecasts performed on august 28th of 2020 accordingly such information can help decision makers prioritize the proactive damage prevention measures for instance advising farmers when and where to harvest the crops before they become damaged or destroyed due to the forthcoming flooding fig 15 shows the time series of fier forecasted flood induced rice economic losses during the wet season harvest time of 2020 and 2021 in the cf and vmd indicating how much rice paddies under flood risk could have been saved in million usd if the farmers had taken early actions using fier mekong the maximal flood induced rice economic losses that could have been saved with fier mekong were about 86 and 52 million usd in cambodia and 1 5 and 1 2 million usd in vietnam during the 2020 and 2021 wet season harvest time respectively the estimated losses in vietnam were much lower than those in cambodia probably because the vietnamese farmers have adjusted their rice harvesting calendar to the time before the arrival of the main floodings triet et al 2018 in addition the vmd has a dense network of flood protection infrastructure hung et al 2012 2014 fig 14b shows the fier based post event estimation of maximal flood induced rice economic losses during the wet season harvest time of 2018 we can observe that the largest loss occurred in the cf near the cambodia vietnam national border this post event flood induced loss estimation can be used as a reference for flood risk mapping and future land use planning by public administrators or insurance companies oddo et al 2018 for example the public administrators may consider reinforcing the flood prevention infrastructures e g dikes or levees adjusting the rice planting and harvesting calendar or planting more flood resistant rice varieties in the areas the insurance companies can also prioritize to whom they should provide flood insurance the maximal total flood induced rice economic losses were about 316 and 46 million usd in cambodia and vietnam respectively during the wet season harvest time of 2018 however it is challenging to numerically compare the estimated losses with government or agency statistics as they are often reported from different data sources with ambiguous loss estimation formulas that can be based on insufficient or inaccurate information central region urban environmental improvement project crueip 2003 chen 2007 oddo et al 2018 hence in this study we attempt to demonstrate that a standardized loss estimation method can still provide valuable information for flood damage mitigation the quality of the estimated flood induced economic impact is influenced by the skill of fier mekong that has been discussed in section 6 2 another factor is the accuracy of the dem as the calculation of inundation depths relies on it as a topographic reference the spatial resolution of fier estimated inundation extents can also play a role especially over the floodplains with extremely low relief since the calculation of the inundation depths starts from extracting the elevations of inundation boundaries a finer resolution inundation map can be helpful for extracting more accurate elevations along the inundation boundary the spatial resolution of fier forecasted inundation extents can be tuned to be finer during the sentinel 1 imagery retrieval from the gee data catalog using restee section 3 1 finally more detailed rice cultivation related data such as geospatial information of different rice varieties and corresponding planting and harvesting calendars if available can further improve the accuracy of the economic loss estimation overall this section demonstrates the potential of fier mekong for timely forecasting of flood induced rice economic losses in space and time while the post event flood damage estimation has been done in many recent studies chung et al 2019 kwak et al 2015 oddo et al 2018 okazumi et al 2014 triet et al 2018 fier can provide prompt pre event forecasts of flood damages that decision makers can use to take timely proactive measures assessing or forecasting other flood induced socio economic damages such as on infrastructure and households can be done if corresponding damage curves are available chen 2007 oddo et al 2018 6 4 framework scalability fier is an innovative data driven approach that produces inundation forecasts based on the correlation between the spatiotemporal patterns extracted from satellite imagery and external hydrological data it is mathematically simple and highly scalable enabling us to easily build and operate the framework for any flood prone region in the world using the cloud based gee data catalog and python based gee apis moreover a python implementation of fier called fierpy which has been co developed with the nasa sco https github com satellitehydrology fierpy can be run on python based cloud computing platforms such as google colab hence end users that are interested in building their regional fier framework with rather limited computational resources can implement it on cloud computing platforms in addition fier can be built by leveraging existing operational flood forecasting systems in the regions such as the water level forecasting system operated by the bangladesh water development board bwdb http www ffwc gov bd and the national water model nwm for u s https water noaa gov map chang et al 2022 on the other hand fier mekong itself can be easily integrated with different existing flood forecasting systems the current version of fier mekong adopted our in house vic model aided satellite altimetry based water level forecasting system chang et al 2019 since the required water level forecasts at can tho are not provided by the operational water level forecasting system that mrc maintains https portal mrcmekong org monitoring flood forecasting however mrc s water level forecasting system provides more accurate forecasts at the other three stations kratie koh khel and chau doc and therefore it is possible to use both of the water level forecasting systems if such needs are identified from the end users moreover not only water levels but streamflow data can be used to build the fier framework accordingly fier mekong can be integrated with natural streamflow forecasts obtained from a hydrologic model such as hype du et al 2020 or vic hossain et al 2017 and or regulated outflow from a reservoir obtained from for example the reservoir assessment tool rat mekong das et al 2022 or the integrated reservoir operation scheme iros du et al 2022 7 conclusions and future scopes this study has implemented and operationalized fier a data driven satellite imagery based inundation extent forecasting framework for the lmrb floodplains where the implementation of the conventional approach is challenging due to its highly complex hydrological system in addition a freely available user friendly operational web application hosted by a cloud platform have been built as well the system can operationally generate daily continuous forecasts of inundation extents in 3 s and depths in 30 s with lead times up to 18 days daily hindcasts since 2008 can also be generated the inundation forecasts can be exported as geotiff files for further geospatial analysis such as combing with different land cover data for spatial prediction of flood induced damages and economic losses here we generated fier forecasted inundation extents for the wet season of 2020 and 2021 and cross compared them with inundation extents from the real sentinel 1a images to demonstrate its skills 70 of csis the error sources of forecasted inundation extents include the error in the input forecasted water levels as well as the uncertainties in the fier regression models that were possibly caused by human intervention activities e g with the dense network of dikes sluice gate operation water pumping we also demonstrated an application of fier mekong for spatial prediction of flood induced rice economic losses such spatial loss prediction can help decision makers prioritize when and where proactive flood damage prevention measures should be taken for instance decision makers can disseminate flood risk warnings to the inhabitants in the areas where rice paddies may be damaged by floods in the following days through short message service sms on mobile phones via local providers after being informed the local inhabitants can then evaluate the necessity to adjust the rice reaping schedule we estimated that the flood induced rice damages up to 87 and 53 million us dollars during the harvest time in 2020 and 2021 respectively could have been avoided with the implementation of fier mekong for future work the spatial flood induced loss prediction will be made available on the web application so decision makers or any end users that have an internet connection can directly obtain the results without processing the data on their own such an open web application can also help address the delays in flood warning dissemination caused by bureaucratic inefficiency or inadequate infrastructure investment in developing countries since local inhabitants with internet access can get information directly from the web application on their own keoduangsine and goodwin 2012 note that fier mekong can also generate daily hindcasted inundation extents with historical water levels which are also important for land use planning and risk mapping oddo et al 2018 the fier hindcasted inundation extents and rice economic losses for the year 2018 the most recent wet year were also presented more importantly with cloud based databases such as the gee data catalog that archives the sentinel 1 imagery over the globe fier has the potential to be easily implemented in other flood prone regions in the world fier also has the flexibility to use forecasted water levels from different models ensembling their strengths to generate more accurate forecasted inundation extents an example is to combine the vic model aided satellite altimetry based water level forecasting system used in this study with the mrc s operational forecasting system as section 6 4 described on the other hand historical and forecasted natural and regulated streamflow obtained from hydrologic models and a reservoir operation scheme can be used as well to forecast inundation extents due to not only natural but dam induced floods it is also worth mentioning that the types of satellite imagery that can be used for the fier process are not restricted to sar imagery in fact the use of both sar and optical imagery through data fusion can help address the limitation of fier since fier forecasts the inundation extents based on spatiotemporal patterns of historical satellite images the maximal inundation extents it can forecast are limited by the historical inundation extents that have been observed as sentinel 1a itself was launched in 2014 and has a 12 day revisiting cycle some historical extreme flood events may be missing thus the sole use of sentinel 1a imagery may limit the forecasting capability of fier the optical imagery on the other hand have temporally longer and denser observations therefore it is expected that combining sar and optical imagery through data fusion techniques can help overcome the limitation of using a single source of satellite imagery 8 software availability name of the web application fier mekong developer and contact information chi hung chang cchang37 cougarnet uh edu hyongki lee hlee45 central uh edu son k do skdo3 cougarnet uh edu kel markert kel markert nasa gov year first available 2022 requirements a device with a web browser and internet connection source code availability https github com satellitehydrology fier mekong cost free program language python declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by nasa s applied sciences program for servir 80nssc20k0152 80nssc23k0184 university of houston s gear program and vietnam national foundation for science and technology development ne s002847 1 this study was also partially supported by the korea ministry of environment under the demand responsive water supply service program 2019002650004 and the national research foundation of korea nrf grant funded by the korean government msit no 2021r1a2c100578011 we also acknowledge the publicly available fwdet gee tool developed by peter et al 2020 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105643 
25429,in the lower mekong river basin floodplains rice cultivation is highly crucial for regional and global food security however prolonged flooding can pose damage to rice cultivation and other socio economic aspects yet there is no rapid operational inundation forecasting system that can help decision makers proactively mitigate flood damages here we integrated the so called forecasting inundation extents using rotated empirical orthogonal function analysis fier framework with an altimetry based operational mekong river level forecasting system and built an operational web application fier mekong https fier mekong streamlit app that generates daily skillful forecasted inundation extents 70 of critical success index and depths in about 3 and 30 s respectively with up to 18 day lead times one of its applications predicting flood induced rice economic losses is also presented had fier mekong being adopted we estimated that the rice damages up to 87 and 53 million us dollars during the 2020 and 2021 harvest time respectively could have been avoided keywords operational inundation forecasting sar imagery flood risk prediction mekong river basin data availability data will be made available on request 1 introduction in the lower mekong river basin lmrb floodplains including the tonle sap lake floodplains tslf cambodian floodplains cf and vietnamese mekong delta vmd inhabitants largely rely on the resources gifted by the mekong river mr where flood patterns with strong seasonality nourish the floodplain agriculture and freshwater fisheries that are major supporters of the livelihoods of local inhabitants mekong river commission mrc 2011 agriculture especially rice cultivation is the main foundation of national economies and sustains not only regional but also global food supply maitah et al 2020 matsubara et al 2020 okazumi et al 2014 triet et al 2018 the cambodian population relies on rice production in the tslf and cf for both domestic consumption and commercial export cramb et al 2020 vietnam is the fifth largest rice producer and the third largest rice exporter in the world maitah et al 2020 with more than half of production coming from the vmd kien et al 2020 bich tho and umetsu 2022 however rice paddies are vulnerable to prolonged floodings and can die after being continuously submerged in water for days mrc 2009 failure of rice crops that are supposed to feed local and global populations will consequently lead to larger regional or even global scale impacts prolonged floodings can also pose threats to other socio economic aspects such as infrastructure and soil fertility and might even directly threaten human lives horton et al 2022 oddo et al 2018 moreover recent studies have reported an increasing flood occurrence and intensity in the lmrb floodplains due to climate change chen et al 2020 try et al 2020a 2020b therefore it is vital to rapidly predict flood inundation extents to help decision makers develop proactive damage prevention measures in a timely manner that can mitigate flood induced socio economic damages however the hydrological system in the lmrb floodplains is highly complex with multiple processes including inflow from the mr mainstem the unique tonle sap lake tsl flow reversal and tidal intrusions see section 2 for details in addition the region has largely flat terrains and densely distributed man made hydraulic structures these conditions impede the implementation of a rapid operational inundation extents forecasting system using conventional approaches including the hydrodynamic modelling approach and the non modelling terrain based approach the hydrodynamic modelling approach has been widely used to transform discharge outputs from rainfall runoff models to distributed inundation extents by solving one dimensional saint venant equations or two dimensional shallow water equations however these models suffer from several sources of error bates et al 2014 including model structural errors and uncertainties in 1 the model input data including the rainfall runoff data to set the boundary and initial conditions 2 input digital elevation model dem and channel bathymetry data 3 friction coefficients to represent energy loss mechanisms and 4 information about hydraulic structures in the reach all these uncertainties in hydrodynamic model calibration boundary conditions and topographic data can significantly influence flood inundation predictions teng et al 2017 bates et al 2014 furthermore the required spatial parametric inputs may not always be available chen et al 2019 leandro et al 2014 teng et al 2017 indeed dung et al 2011 and triet et al 2020 2018 who simulated inundation extents in the vmd by a one dimensional hydrodynamic model mike11 pointed out the need for detailed dike survey data and light detection and ranging lidar dem for the model to generate inundation extents with satisfactory skills in the vmd triet et al 2018 also mentioned that it is challenging for a full two dimensional model to be implemented in the vmd due to the need for highly accurate topographic and hydraulic data in the region in addition the models require a heavy computational burden especially for a high resolution large scale forecasting framework predicting inundation extents for the upcoming 3 days in a small area less than 300 km2 using 100 m and 30 m resolution mike 21 models was found to take 396 min and 1012 min respectively fraehr et al 2022 therefore its high computational cost makes many operational applications such as ensemble and real time forecasting unfeasible particularly in developing countries due to their lack of adequate technology infrastructure fraehr et al 2022 zhou et al 2021 biswas and hossain 2018 on the other hand the non modelling terrain based approach such as height above nearest drainage hand nobre et al 2016 zheng et al 2018 estimates the inundation extents based on topography data and water depths it first normalizes the topography according to local relative heights along the drainage network to generate hand values then a rating curve is used to transform streamflow forecasts to depths for a given river cell finally based on the planar water surface approximation flood inundation extents are determined by selecting surrounding land cells from which hand values are less than the given water depths in the stream nobre et al 2011 teng et al 2017 the hand approach requires significantly less computational power than hydrodynamic models and may work well on confined floodplains with steep valleys and straight river reaches bates and de roo 2000 wing et al 2019 however a recent study by johnson et al 2019 demonstrated that the method severely overpredicts inundation extents in regions of low relief in addition the hand approach does not account for overland water flow and backwater effects caused by infrastructure and coastal flooding with the flat terrains in the lmrb floodplains especially over the cf and vmd balica et al 2014 and a dense network of flood protection infrastructure in the vmd the hand approach may not be applicable recently space borne remote sensing especially using the synthetic aperture radar sar and visible and near infrared sensors has emerged as a powerful tool to depict areal inundation dynamics with repeated views over a wide range of locations e g ahamed and bolten 2017 kim et al 2017 lee et al 2015 oddo et al 2018 smith 1997 chang et al 2020 using the tslf as a test bed proposed a satellite imagery based inundation extent forecasting framework that addresses the need for computationally efficient areal inundation estimation and forecasts with high temporal resolution this framework is named forecasting inundation extents using rotated empirical orthogonal function analysis fier simply speaking fier forecasts inundation extents based on a correlation between historical inundation extents and hydrological data water levels or streamflow once the correlation is identified inundation extents can be forecasted with forecasted hydrological data available from an external forecasting system specifically fier decomposes multi temporal historical satellite images into significant spatiotemporal patterns using the rotated empirical orthogonal function reof analysis kaiser 1958 lorenz 1956 the patterns are then coupled with the hydrological data by regression models then the forecasted hydrological data when available can be used as inputs to synthesize forecasted satellite like images from which the corresponding forecasted inundation extents can be delineated consequently fier becomes much more computationally efficient and scalable than the conventional approaches fier has been first implemented in the tslf fier tslf for daily hindcast and forecast of inundation extents to test its feasibility without operationalization using a multi temporal stack of sentinel 1a images daily interpolated jason altimetry derived tsl levels and the climate index chang et al 2020 in this study we took a step further to implement for the first time an operational fier system that can rapidly forecast skillful inundation extents over not only tslf but the entire lmrb floodplains fier mekong toward its operationalization first of all we have operationalized our in house satellite altimetry based mr water level forecasting system chang et al 2019 see section 3 3 and the supplementary information secondly we automated the regression modeling by adopting the neural network regression so that the regression models can be built without pre assuming a function to be fit to the data imani et al 2014 such automatic model building can help us scale up the implementation of fier to other areas of interest thirdly we adopted a water classification technique using z score statistics for faster and more efficient inundation mapping devries et al 2020 lastly we developed an operational fier mekong web application that can rapidly generate and visualize daily forecasted inundation extents in about 3 s and depths in about 30 s with up to 18 day lead time on a daily basis the forecasted inundation depths were generated by utilizing the publicly available google earth engine gee based floodwater depth estimation tool fwdet gee peter et al 2020 the web application also allows users to export the forecasted inundation extents and depths as geocoded tiffs geotiffs a widely used georeferenced raster data format for further geospatial analysis of their interests the web based platform has been recognized as a powerful tool for disseminating flood risk information knight et al 2015 hearn 2009 many web based tools have been developed for either real time flood forecasting or flood risk assessment under different scenarios for example mourato et al 2021 presented an operational fluvial flood forecast and alert system in águeda municipality portugal with up to 3 days of lead time knight et al 2015 developed a web based geospatial decision support tool for two coastal cities in the united kingdom and assessed their potential flood risk under different scenarios of sea level rise storm intensity wave height and river flow mohanty and karmakar 2021 presented a web based flood risk information system for the flood prone jagatsinghpur district in eastern india and analyzed the flood risks under different scenarios of rainfall and storm tides however to the best of our knowledge the web based operational inundation extent and depth forecasting system is the first of its kind that covers the entire lmrb floodplains the forecasted inundation extents and depths can be used for further geospatial analysis such as conducting spatial and temporal prediction of flood induced socio economic damages with relevant land cover data the spatio temporal flood damage prediction can help decision makers take proactive actions to manage and prevent flood damages such as disseminating timely and effective early warnings in this paper we presented the forecasted inundation extents in 2020 and 2021 generated by fier mekong and evaluated the forecasting skills considering the importance of rice cultivation in the lmrb floodplains we also demonstrated the application of fier forecasted inundation extents on spatio temporal prediction of flood induced rice damages the structure of this paper is as follows section 2 describes the characteristics of the lmrb floodplains section 3 introduces the data that were used for the fier mekong section 4 explains the fier mekong process skill evaluation indices fwdet gee and how the early risk assessment was conducted section 5 describes the fier mekong web application section 6 presents the results of fier forecasted inundation extents the skill evaluations and demonstrates the potential application of the fier mekong in early assessment of flood induced economic losses of rice production the scalability of the fier framework is also discussed finally section 7 concludes the paper and discusses future scopes 2 study area the mr the largest in southeast asia is a transboundary river flowing through six countries china myanmar lao pdr thailand cambodia and vietnam with headwater in the tibetan plateau and the river mouth in the vmd this paper focuses on forecasting the inundation extents in the downstream floodplains of the mrb called the lmrb floodplains in cambodia and vietnam the area is about 66 589 km2 and encompasses the tslf cf and vmd the flood pulse here has a distinct seasonal cycle governed by the southwest monsoon with the wet season from june to october pagano 2014 the geographical locations of these floodplains are shown in fig 1 along with a sentinel 1a vv polarized intensity image that illustrates the spatial coverage of the images used see section 3 1 for a detailed description of the sentinel 1a images historical maximum inundated areas are also delineated in blue based on the joint research center jrc global surface water data pekel et al 2016 tsl is located in cambodia and is the largest natural freshwater lake in southeast asia its seasonal flow reversal makes it unique in the world in the wet season the high water level in the mr mainstem causes the water flows reversely toward the tsl due to the inflow of water from the mr mainstem inundation extents in the tslf can be increased sixfold in the wet season from around 2500 km2 15 000 km2 during the dry season the water level in the mr recedes and the water in tslf flows toward the cf and vmd mrc 2021 the cf is located south of the tslf and extends from kratie to the cambodia vietnam national border vmd spans from the cambodia vietnam national border toward the mouth of the mr where ocean tides can influence the flow variations our study years 2020 and 2021 are considered dry years with lower than average 2008 2017 wet season inflow from the upstream during these years rainfall from june to august was lower than the long term average but then increased in september and october reflecting the delayed onset of southwest monsoons this has caused the tsl flow reversals during the wet seasons of these years to be the lowest since 2008 mrc 2022 3 data 3 1 sentinel 1 sar imagery sentinel 1a equipped with a c band 5 405 ghz sar imaging sensor is a satellite mission under the copernicus earth observation program of the european space agency esa it was launched on april 3rd 2014 and has been consistently providing freely available imagery with a 12 day revisiting cycle we used sentinel 1a vv polarized ground range detection high resolution grdh intensity images from which spatio temporal patterns have been extracted using the reof analysis the vv polarization was chosen considering its superior surface water mapping capability markert et al 2020 twele et al 2016 to cover the lmrb floodplains four frames including frames 23 29 34 and 39 of path 26 were used see fig 1 we also used sentinel 1a images for cross comparison with the fier forecasted inundation extents table 1 summarizes the numbers of sentinel 1a images used with their acquisition periods we leveraged the publicly available cloud based gee data catalog and gee python based application programming interface api to alleviate the computational burden of sentinel 1a imagery retrieval and preprocessing a gee api based open source python application called hydrologic remote sensing analysis for floods hydrafloods https servir mekong github io hydra floods was used for preprocessing including image mosaicking slope correction vollrath et al 2020 using the multi error removed improved terrain dem merit dem yamazaki et al 2017 and the gamma map speckle filtering lopes et al 1990 the jrc s global surface water data was used to mask out the permanent water bodies in the sentinel 1a images to mitigate the influence of surface roughness changes on sar intensities which are often caused by winds another gee api based python package called restee https github com kmarkert restee was used to convert the preprocessed gee server side sentinel 1a image collection to the client side python gridded dataset with user defined spatial coverage and resampling resolution the converted dataset can then be processed using non gee based apis and python libraries to build the prototype of an operational inundation forecasting system a spatial resolution of 0 005 which is equivalent to approximately 552 m was simply set accordingly the fier estimated inundation extents have the same spatial resolution both the hydrafloods and restee were developed by the nasa servir coordination office sco the sentinel 1a images merit dem and jrc data were all retrieved from the gee data catalog and preprocessed with the gee a scheme of the data preprocessing is shown in fig 3 3 2 merit dem the merit dem yamazaki et al 2017 was used for the slope correction of multi temporal sentinel 1a images and as the topographic reference to derive the inundation depth maps from the inundation extents it is a 3 arc second resolution global dem with the earth gravitational model 1996 egm96 geoid as a datum it uses the srtm3 dem and the advanced land observing satellite world 3d 30 m dem aw3d 30m dem as the baseline where the unobserved gaps were filled by the viewfinder panoramas dem it has improved accuracy than the srtm dem since its bias and errors were mitigated using the additional dataset including the ice cloud and land elevation satellite icesat laser altimetry land surface elevation data landsat derived forest cover dataset hansen et al 2013 and nasa global forest height data simard et al 2011 3 3 mekong river water levels observations and forecasts the historical in situ water levels provided by the mrc https portal mrcmekong org home were used to examine their correlation with the temporal patterns extracted from the multi temporal sentinel 1a images fig 1 shows the locations of the in situ gauges used which are kratie koh khel chau doc and can tho from upstream to downstream shown with red dots since fier requires forecasted water levels at these gauges to generate forecasted inundation extents the operational variable infiltration capacity vic model aided satellite altimetry based water level forecasting system chang et al 2019 was used details in supplementary information essentially the water level forecasting system uses upstream altimetry derived water levels to forecast the downstream water levels at the gauges by assuming a linear relationship between them this approach was adopted because 1 it is computationally efficient and thus suitable for the operational purpose and 2 it forecasts water levels with promising skills at can tho in the vmd which mrc does not routinely issue chang et al 2019 pagano 2014 although the mr water level forecast system has up to 20 day lead times the operational maximal lead time of fier mekong becomes 18 day due to the latency of input forcings of the vic model hossain et al 2017 3 4 rice cover market price and yield calculating the wet season flood induced rice economic losses requires spatial distribution market price and the yield of rice the rice harvest times need to be also considered the wet season harvest time in the cf is around late august to early september okazumi et al 2014 on the other hand in the vmd the wet season harvest time spatially varies for double cropping areas the harvest time is from july to early august for triple cropping areas there is an additional harvest time depending on whether the areas are fully protected or not triet et al 2018 we only considered the additional harvest time of the triple cropping areas that are not fully protected and can be vulnerable to flooding which is from late august to early september triet et al 2018 for the spatial distribution of rice paddies in the cf the latest landcover data for the year of 2018 from the servir mekong regional land cover monitoring system https www landcovermapping org en landcover servir mekong accessed on mar 24th 2022 was used in the vmd since the wet season harvest time spatially varies depending on local annual cropping times the latest land cover data for the year of 2020 of vu et al 2022 https data mendeley com datasets kpftzmsyyz 2 was used both landcover data were spatially interpolated to be aligned with the inundation extents from fier mekong using the nearest neighbor method see fig 2 the rice market price in us dollar usd ton was retrieved from food and agriculture organization fao https www fao org markets and trade commodities rice fao rice price update en fao accessed on mar 24th 2022 the rice yield ton ha was retrieved from the u s department of agriculture usda https ipad fas usda gov countrysummary usda accessed on mar 24th 2022 these data are summarized in table 2 for the rice market price we used the average price of all rice varieties for the rice yield we used the average yield of the market year 2019 2020 and 2020 2021 for 2020 and the average yield of 2020 2021 and 2021 2022 for 2021 4 methods this section explains how fier was implemented in the lmrb floodplains fier mekong fig 3 shows the flowchart of the entire process it requires the multi temporal sentinel 1a intensity images and historic in situ mr water levels from mrc to construct the regression models with the forecasted mr water levels from section 3 3 the forecasted inundation extents can be produced then the fier forecasted inundation extents were combined with the merit dem to determine the forecasted two dimensional inundation depths using fwdet gee finally using the forecasted inundation depths the flood induced rice economic losses were estimated to demonstrate the applicability of fier mekong for preventing or mitigating the anticipated flood damages on rice paddies all the processes are python based and were run locally except for the gee apis which use the gee server as the backend yellow boxes in fig 3 4 1 backbone of fier mekong 4 1 1 overview of fier essentially fier forecasts the inundation extents based on the relationships between the observed historical inundation extents with hydrological data technical details of fier can be found in chang et al 2020 and a summary of the fier framework is given here for the readability of this paper firstly the reof analysis kaiser 1958 lorenz 1956 was applied to obtain the significant spatio temporal patterns of multi temporal sentinel 1a images it starts with the conventional empirical orthogonal function eof analysis which is statistically identical to the principal component analysis followed by a varimax rotation to enhance the physical interpretability of the decomposed spatio temporal patterns the outputs include static spatial patterns called the rotated spatial modes rsms and their corresponding temporal patterns called the rotated temporal principal components rtpcs next flood related rtpcs and their corresponding hydrological drivers such as the mr water levels used in fier mekong were identified based on correlation analysis then regression models that couple the flood related rtpcs and their corresponding best correlated mr water levels were built by feeding forecasted mr water levels into the regression models forecasted rtpcs can be obtained next the forecasted sentinel 1 like images can be synthesized by summing products of the forecasted rtpcs and the static rsms finally forecasted inundation extents can be obtained by applying a water classification method to the forecasted sentinel 1 like images as previously mentioned different from chang et al 2020 this study automated the regression modeling with the neural network regression and improved the efficiency of water classification with the z score statistic based approach devries et al 2020 which will be explained in section 4 1 2 and 4 1 3 respectively 4 1 2 neural network regression the optimal regression models were determined by the grid search with k fold cross validation approach this approach is an automatic trial and error process where the training and testing are conducted using different neural network architectures and different combinations of data subsets the data are first divided into k subsets one of the subsets is used as a test set while others are used as a training set the neural network architectures in the searching space are trained until the training scores have converged and then test scores are recorded finally once every subset has been used as the test set the process ends the optimal neural network architecture is the one that provides the best mean test score over different test sets such a process helps prevent the models from being overfitted kim et al 2021 the number of k is generally from 2 to 10 which could be determined based on data availability zhou et al 2017 considering the data size was 85 acquired within three years see table 1 we employed k 3 three fold which ensured the test set encompassed a full hydrologic cycle a year of data both the rtpcs and mr in situ water levels were normalized before the training which is a common measure to expedite the training process the rectified linear unit relu activation function was adopted in the hidden layers and the adam gradient descent algorithm with a default learning rate 0 001 was adopted as an optimizer the mean squared error was used as the loss function to evaluate the scores the keras a python based api with tensorflow libraries as the backend was used for the neural network regression process see fig 4 for illustration the flood related spatio temporal patterns of multi temporal sentinel 1a images as well as the neural network regression models are shown in section 6 1 4 1 3 water classification using z score statistics we applied a rapid and robust water classification method which is based on the z score statistics of the wet season images devries et al 2020 the method compares the change between the wet season and baseline images representing the dry season with the change among the baseline images using the z score statistics and then performs water classification the z score statistics can be derived as 1 z σ 0 σ 0 b a s e l i n e s t d σ 0 b a s e l i n e where z is the z score image σ 0 is the wet season image to be classified fier synthesized or observed sentinel 1a image σ 0 b a s e l i n e and s t d σ 0 b a s e l i n e are the mean and standard deviation std of the baseline images respectively note that the fier synthesized forecasted images are the anomalies with respect to the mean of the multi temporal sentinel 1a images from which the spatiotemporal patterns were extracted by the reof analysis since the multi temporal sentinel 1a images encompass the wet season images their mean also contains some water related signals and therefore was added back to the fier synthesized forecasted images before water classification the baseline period was considered as the months when the monthly average in situ water levels from 2017 to 2019 corresponding to the acquisition period of the multi temporal sentinel 1a images at the four identified gauges were the lowest then the sentinel 1a images acquired in these months were used to calculate σ 0 b a s e l i n e and s t d σ 0 b a s e l i n e the z score image was then generated and the pixels that have low z scores were considered to be flooded here we classified the pixels whose z scores are lower than 3 as flooded following devries et al 2020 details about the operational system will be explained in section 5 the forecasted inundation extents generated by fier mekong were cross compared with the inundation extents derived from the real sentinel 1a images for skill evaluation which will be shown in section 6 2 4 2 skill evaluation of fier mekong the skill of forecasted inundation extents in the lmrb floodplains estimation was evaluated with pixel count based indices including the overall accuracy and critical success index csi derived from the confusion matrix kohavi and provost 1998 the inundation extents derived from real sentinel 1a images were considered as observations as fig 5 illustrates a and d are the numbers of pixels that fier correctly estimates true positive and true negative respectively conversely b and c are the numbers of misestimated pixels indicating false positive and false negative respectively the overall accuracy is the percentage of pixels that fier correctly estimated either true positive or true negative over the total number of pixels 2 o v e r a l l a c c u r a c y a d a b c d 100 the csi gilbert 1884 also called threat score is the number of pixels that fier correctly estimated as inundated over the number of pixels that were either estimated or observed as inundated 3 c s i a a b c 100 the csi avoids possible bias caused by true negative d wing et al 2017 both overall accuracy and csi can have a value from 0 to 100 where 0 means there is no match between the estimation and observation while 100 means perfect estimation 4 3 fwdet gee gee based inundation depth estimation the inundation depths were determined by using the fwdet gee peter et al 2020 the tool overlays the inundation extents on a dem and then calculates the differences between the elevations of inundated pixels and their nearest inundated boundary pixels to estimate the inundation depths firstly the inundation boundary pixels are identified and their elevations are retrieved secondly elevations of the nearest inundation boundary pixels are assigned to the non boundary inundated pixels finally the floodwater depths at inundated pixels are estimated by subtracting the actual dem value from the assigned boundary elevations compared with the previous arcgis and qgis versions cohen et al 2019 the major advantage of the gee version is its significantly reduced processing time the gee script is freely accessible online https dataverse harvard edu dataset xhtml persistentid doi 10 7910 dvn jq4bcn in this study fier forecasted inundation extents and merit dem were used to obtain the fier forecasted inundation depths 4 4 flood induced rice economic loss estimation to assess the potential economic impact of fier mekong we estimated the flood induced rice economic losses based on the fier forecasted inundation depth maps the process starts with a rice relative damage curve which is a function of consecutive flood duration days and inundation depths m mrc 2009 this approach has been widely used in the literature okazumi et al 2014 chung et al 2019 in mrc 2009 the flood damages to rice paddies are categorized with respect to different inundation depths and durations as 1 rice paddies will not be affected with inundation depth 0 5 m 2 rice crops will be damaged by flood with a duration longer than 5 days 3 rice will be completely destroyed with inundation depth 1 5 m and duration 13 days with these okazumi et al 2014 derived the damage curve with up to 13 days of flood durations and up to 1 5 m of inundation depths as 4 r d h w 0 5 86 875 22 5 d 0 625 d 2 where r d is rice relative damage h w is inundation depth m 0 5 h w 1 5 if h w 1 5 then h w 1 5 and d is inundation duration day if d 13 then d 13 fig 6 shows examples of the relative damage curves both h w and d can be derived from the daily fier forecasted inundation depth maps the rice relative damages r d were then used to quantify the corresponding rice economic losses as 5 d m p y a i 1 w r d i where d is flood induced rice damage usd of the country inside the sentinel 1a image frames we used m p is rice market price usd ton y is rice yield ton ha a and r d are area ha and aforementioned relative damage of a pixel and w is the total number of pixels that represent rice under flood risk based on the aforementioned criteria in mrc 2009 inside the image frame for each country table 2 has summarized the rice market price and yield the spatial distribution of rice paddies has been shown in fig 2 5 operational fier mekong web application 5 1 configuration the operational fier mekong web application was constructed using streamlit an open source python framework that helps create deploy and host web applications in streamlit framework a main frontend python script that depicts the frontend elements e g button select box calendar along with other required libraries and data are stored in the github repository the web application and repository are deployed and hosted by streamlit cloud platform any changes on the github repository will be automatically updated the operational fier mekong web application requires the following data 1 automatically updated mr water level forecasts 2 the rsms and neural network regression models which are used to routinely synthesize the forecasted sar like images and 3 the mean and std of historical sentinel 1a intensities for the water classification using z score statistics the water level forecasts are automatically generated and then managed stored and updated to the google sheet database by using google sheets api and a python library called pygsheets the automatic updating process of forecasted water levels is executed by the linux crontab on a daily basis the rsms regression models and mean and std of historical sentinel 1a intensities were pre obtained locally see section 4 1 and uploaded to the github repository as fig 7 shows the operation of the web application begins with updating the water level forecasts then the streamlit cloud platform will access the updated forecasted water levels and their dates the forecast date can be selected from the calendar in the frontend then the corresponding forecasted water levels will be fed into the fier regression models to estimate the forecasted rtpcs next the forecasted rtpcs and the rsms are multiplied and linearly summed up to synthesize the forecasted sentinel 1 like images from which the forecasted inundation extents can be finally obtained these processes are all done by the streamlit cloud platform users can request the web application to generate the forecasted inundation depths as well note that fier mekong can also provide hindcasts of inundation extents and depths by using historical in situ water levels as inputs 5 2 graphic user interface gui the user friendly frontend gui is shown in fig 8 with the selected date of interest with up to 18 days of lead times the corresponding forecasted water levels in the google sheet database will be retrieved and the forecasted inundation extents will be visualized there is also an option to generate and visualize the inundation depth end users can also choose to export the maps as geotiff for further geospatial analysis fier mekong also allows users to generate hindcasted inundation extents and depths by selecting hindcast as the run type 6 results and discussions 6 1 flood related spatio temporal patterns and fier regression models here the first four modes of spatio temporal patterns accounting for about 64 of the total variance of the multi temporal sentinel 1a images were identified as significant based on the monte carlo simulation and were rotated to obtain the rsms and the corresponding rtpcs hereafter the first mode of rsm rtpc will be called rsm 01 rtpc 01 and the second mode of rsm rtpc will be called rsm 02 rtpc 02 and so forth a correlation analysis between the four significant modes of rtpcs and the in situ water levels was conducted the four gauges from which water levels are best correlated with rtpc 01 to rtpc 04 were then identified which are chau doc koh khel can tho and kratie respectively the top panel of fig 9 shows the rtpcs along with their best correlated in situ water levels and correlation coefficients corr which are all higher than 0 7 to 0 8 it means the changes in sentinel 1a sar intensities are temporally relevant to the water level changes at these gauges which can be further interpreted together with the corresponding rsms shown in the bottom panel of fig 9 the positive correlations between the rtpcs and water levels indicate that when water levels are higher the rtpcs will also be higher and lead to lower synthesized intensity values after being multiplied with the negative value pixels blue pixels in the rsms similar to the case of sar the lower synthesized intensity values indicate flooding by comparing with the blue shaded areas in fig 1 we can see that the negative value pixels in rsms are mostly observed inside the floodplains regression models between the flood related rtpcs and corresponding best correlated mr in situ water levels were then built using the neural network regression optimized by the grid search with k fold cross validation approach table 3 shows the final neural network architecture mean training score mean test score and the corresponding r2 values of each mode the r2 values were all higher than 0 6 to 0 7 considered to be satisfactory moriasi et al 2007 fig 10 shows the scatter plots of rtpcs and their corresponding water levels along with the regression models 6 2 fier forecasted inundation extents and skill evaluations to evaluate the skill of fier mekong we generated fier forecasted inundation extents with up to 18 day lead time in the wet season of 2020 and 2021 estimation which were cross compared with the inundation extents directly delineated from 26 sentinel 1a images acquired during the same period observation see table 1 fig 11 shows the mean and std of overall accuracies and csis from different lead times 1 day to 18 day as can be expected the skills of fier forecasted inundation extents generally degrade with longer lead times however the mean accuracies are near 98 and mean csis are about 72 74 which are considered to be fairly good bernhofen et al 2018 the degradation of fier inundation forecasting skills with longer lead times may be due to errors in the forecasted water levels see fig 8 of supplementary information which are used as an input of inundation forecasts since the forecasted water levels were generated based on vic model simulated discharges and altimetry derived water levels at upstream virtual stations vss errors from both can influence the fier inundation forecasting skills the potential error sources include 1 the errors of input forcing data of the vic model 2 the errors in the altimetry derived water levels at upstream vss and 3 the assumption that there is a linear relationship between upstream and downstream water levels ignoring the impact of tributaries and direct rainfall in between in addition the lmrb floodplains specifically in the vmd have a strong human intervention that alters the natural flood regime irregular human operation of water resources infrastructure e g sluice gates water pumping in the region could alter the water levels hung et al 2012 2014 that cannot be predicted and consequently may lead to errors in the fier forecasted inundation extents fig 12 shows the wet season monthly inundation occurrences days representing the inundation dynamics generated using fier mekong the top and middle panels of fig 12 illustrate the occurrences using the fier forecasted inundation extents with 18 day lead time for the wet seasons of 2020 and 2021 we start to see some inundations in august and much higher occurrences in september in october the inundation occurrences in the lower vmd slightly increased the much higher inundation occurrences in september and october 2020 and 2021 generated based on the fier mekong forecasts agree with the onset of increased rainfall see section 2 the fier forecasted inundation extents for 2020 and 2021 were not extensive as they were both dry years mrc 2022 to demonstrate the skill of fier mekong for a wet year fier hindcasted inundation extents for the wet season of 2018 the most recent wet year were generated as mentioned earlier fier mekong can also generate hindcasts of inundation using the historical water levels for a date of interest obtained at the four in situ stations see fig 1 the corresponding monthly inundation occurrences for the wet season of 2018 are shown at the bottom panel of fig 12 by cross comparing with real inundation extents derived from 11 sentinel 1a images acquired in the 2018 wet season the accuracy and csi of the hindcasted inundation were 96 6 1 9 and 75 6 14 9 respectively in 2018 inundations spread out earlier than in 2020 and 2021 and were much more extensive some inundations started to occur in july in august the inundation occurrences became much greater all over the tslf and cf according to the mrc report 2020 the inundation in the wet season of 2018 reached nearly the maximum extent compared to satellite images obtained from 2006 to 2017 mrc 2020 the spread of inundation in july and august of 2018 happened earlier than usual caused by an early onset of high rainfall mrc 2020 then in october the inundation occurrences significantly increased and widely spread over the lower vmd these results show that fier mekong is capable of simulating inundation extents even for an extreme case in other words if the input forecasted water levels for this upcoming wet season are much higher than the ones in 2020 and 2021 fier mekong can be used to obtain more widespread and extreme inundations to show the locations where users can have higher confidence in the fier forecasted inundation extents pixel wise temporal correctness rates were also calculated the temporal correctness rates represent the percentage of times that each pixel was correctly estimated either true positive or true negative over the total number of sentinel 1a images used for the cross comparison 26 images as table 1 shows to summarize the temporal correctness rates over different lead times the mean and std of the rates with 1 day to 18 day daily lead times were calculated as shown in the top panel of fig 13 respectively their cumulative histograms over the pixels that have ever been estimated or observed as inundated were also generated showing approximately 83 of pixels have mean temporal correctness rates 80 bottom panel of fig 13a and about 93 of pixels have std of temporal correctness rates 4 bottom panel of fig 13b the cumulative histograms indicate that most of the pixels were correctly estimated most of the time and consistently with different lead times 6 3 application flood induced rice economic losses here we attempted to assess the potential economic impact of fier mekong by demonstrating how it can be used to estimate flood induced rice economic losses since rice paddies become destroyed when they are continuously submerged under water for days fier mekong can be used to forecast prolonged inundation toward spatial pre event damage prediction here the fier forecasted inundation extents fwdet gee tool peter et al 2020 and the flood induced rice relative damage curves mrc 2009 along with the rice data in table 2 were integrated to calculate the flood induced rice economic losses as a result the spatial prediction of the maximal flood induced rice economic losses for the next 13 days was conducted for 2020 and 2021 for example since 2020 and 2021 were both dry years a post event damage assessment for 2018 the most recent wet year was also conducted using the fier hindcasted inundation depths and rice data of 2018 we focused on the wet season harvest time see section 3 4 to calculate the losses assuming the rice has not been reaped the rapid spatial predictions of flood induced losses can give decision makers timely information about when and where rice would be damaged along with the severity fig 14 a shows an example of such spatial predictions of flood induced rice economic losses in usds for the periods of august 29th to september 10th of 2020 arbitrarily chosen for the wet season harvest time using the forecasts performed on august 28th of 2020 accordingly such information can help decision makers prioritize the proactive damage prevention measures for instance advising farmers when and where to harvest the crops before they become damaged or destroyed due to the forthcoming flooding fig 15 shows the time series of fier forecasted flood induced rice economic losses during the wet season harvest time of 2020 and 2021 in the cf and vmd indicating how much rice paddies under flood risk could have been saved in million usd if the farmers had taken early actions using fier mekong the maximal flood induced rice economic losses that could have been saved with fier mekong were about 86 and 52 million usd in cambodia and 1 5 and 1 2 million usd in vietnam during the 2020 and 2021 wet season harvest time respectively the estimated losses in vietnam were much lower than those in cambodia probably because the vietnamese farmers have adjusted their rice harvesting calendar to the time before the arrival of the main floodings triet et al 2018 in addition the vmd has a dense network of flood protection infrastructure hung et al 2012 2014 fig 14b shows the fier based post event estimation of maximal flood induced rice economic losses during the wet season harvest time of 2018 we can observe that the largest loss occurred in the cf near the cambodia vietnam national border this post event flood induced loss estimation can be used as a reference for flood risk mapping and future land use planning by public administrators or insurance companies oddo et al 2018 for example the public administrators may consider reinforcing the flood prevention infrastructures e g dikes or levees adjusting the rice planting and harvesting calendar or planting more flood resistant rice varieties in the areas the insurance companies can also prioritize to whom they should provide flood insurance the maximal total flood induced rice economic losses were about 316 and 46 million usd in cambodia and vietnam respectively during the wet season harvest time of 2018 however it is challenging to numerically compare the estimated losses with government or agency statistics as they are often reported from different data sources with ambiguous loss estimation formulas that can be based on insufficient or inaccurate information central region urban environmental improvement project crueip 2003 chen 2007 oddo et al 2018 hence in this study we attempt to demonstrate that a standardized loss estimation method can still provide valuable information for flood damage mitigation the quality of the estimated flood induced economic impact is influenced by the skill of fier mekong that has been discussed in section 6 2 another factor is the accuracy of the dem as the calculation of inundation depths relies on it as a topographic reference the spatial resolution of fier estimated inundation extents can also play a role especially over the floodplains with extremely low relief since the calculation of the inundation depths starts from extracting the elevations of inundation boundaries a finer resolution inundation map can be helpful for extracting more accurate elevations along the inundation boundary the spatial resolution of fier forecasted inundation extents can be tuned to be finer during the sentinel 1 imagery retrieval from the gee data catalog using restee section 3 1 finally more detailed rice cultivation related data such as geospatial information of different rice varieties and corresponding planting and harvesting calendars if available can further improve the accuracy of the economic loss estimation overall this section demonstrates the potential of fier mekong for timely forecasting of flood induced rice economic losses in space and time while the post event flood damage estimation has been done in many recent studies chung et al 2019 kwak et al 2015 oddo et al 2018 okazumi et al 2014 triet et al 2018 fier can provide prompt pre event forecasts of flood damages that decision makers can use to take timely proactive measures assessing or forecasting other flood induced socio economic damages such as on infrastructure and households can be done if corresponding damage curves are available chen 2007 oddo et al 2018 6 4 framework scalability fier is an innovative data driven approach that produces inundation forecasts based on the correlation between the spatiotemporal patterns extracted from satellite imagery and external hydrological data it is mathematically simple and highly scalable enabling us to easily build and operate the framework for any flood prone region in the world using the cloud based gee data catalog and python based gee apis moreover a python implementation of fier called fierpy which has been co developed with the nasa sco https github com satellitehydrology fierpy can be run on python based cloud computing platforms such as google colab hence end users that are interested in building their regional fier framework with rather limited computational resources can implement it on cloud computing platforms in addition fier can be built by leveraging existing operational flood forecasting systems in the regions such as the water level forecasting system operated by the bangladesh water development board bwdb http www ffwc gov bd and the national water model nwm for u s https water noaa gov map chang et al 2022 on the other hand fier mekong itself can be easily integrated with different existing flood forecasting systems the current version of fier mekong adopted our in house vic model aided satellite altimetry based water level forecasting system chang et al 2019 since the required water level forecasts at can tho are not provided by the operational water level forecasting system that mrc maintains https portal mrcmekong org monitoring flood forecasting however mrc s water level forecasting system provides more accurate forecasts at the other three stations kratie koh khel and chau doc and therefore it is possible to use both of the water level forecasting systems if such needs are identified from the end users moreover not only water levels but streamflow data can be used to build the fier framework accordingly fier mekong can be integrated with natural streamflow forecasts obtained from a hydrologic model such as hype du et al 2020 or vic hossain et al 2017 and or regulated outflow from a reservoir obtained from for example the reservoir assessment tool rat mekong das et al 2022 or the integrated reservoir operation scheme iros du et al 2022 7 conclusions and future scopes this study has implemented and operationalized fier a data driven satellite imagery based inundation extent forecasting framework for the lmrb floodplains where the implementation of the conventional approach is challenging due to its highly complex hydrological system in addition a freely available user friendly operational web application hosted by a cloud platform have been built as well the system can operationally generate daily continuous forecasts of inundation extents in 3 s and depths in 30 s with lead times up to 18 days daily hindcasts since 2008 can also be generated the inundation forecasts can be exported as geotiff files for further geospatial analysis such as combing with different land cover data for spatial prediction of flood induced damages and economic losses here we generated fier forecasted inundation extents for the wet season of 2020 and 2021 and cross compared them with inundation extents from the real sentinel 1a images to demonstrate its skills 70 of csis the error sources of forecasted inundation extents include the error in the input forecasted water levels as well as the uncertainties in the fier regression models that were possibly caused by human intervention activities e g with the dense network of dikes sluice gate operation water pumping we also demonstrated an application of fier mekong for spatial prediction of flood induced rice economic losses such spatial loss prediction can help decision makers prioritize when and where proactive flood damage prevention measures should be taken for instance decision makers can disseminate flood risk warnings to the inhabitants in the areas where rice paddies may be damaged by floods in the following days through short message service sms on mobile phones via local providers after being informed the local inhabitants can then evaluate the necessity to adjust the rice reaping schedule we estimated that the flood induced rice damages up to 87 and 53 million us dollars during the harvest time in 2020 and 2021 respectively could have been avoided with the implementation of fier mekong for future work the spatial flood induced loss prediction will be made available on the web application so decision makers or any end users that have an internet connection can directly obtain the results without processing the data on their own such an open web application can also help address the delays in flood warning dissemination caused by bureaucratic inefficiency or inadequate infrastructure investment in developing countries since local inhabitants with internet access can get information directly from the web application on their own keoduangsine and goodwin 2012 note that fier mekong can also generate daily hindcasted inundation extents with historical water levels which are also important for land use planning and risk mapping oddo et al 2018 the fier hindcasted inundation extents and rice economic losses for the year 2018 the most recent wet year were also presented more importantly with cloud based databases such as the gee data catalog that archives the sentinel 1 imagery over the globe fier has the potential to be easily implemented in other flood prone regions in the world fier also has the flexibility to use forecasted water levels from different models ensembling their strengths to generate more accurate forecasted inundation extents an example is to combine the vic model aided satellite altimetry based water level forecasting system used in this study with the mrc s operational forecasting system as section 6 4 described on the other hand historical and forecasted natural and regulated streamflow obtained from hydrologic models and a reservoir operation scheme can be used as well to forecast inundation extents due to not only natural but dam induced floods it is also worth mentioning that the types of satellite imagery that can be used for the fier process are not restricted to sar imagery in fact the use of both sar and optical imagery through data fusion can help address the limitation of fier since fier forecasts the inundation extents based on spatiotemporal patterns of historical satellite images the maximal inundation extents it can forecast are limited by the historical inundation extents that have been observed as sentinel 1a itself was launched in 2014 and has a 12 day revisiting cycle some historical extreme flood events may be missing thus the sole use of sentinel 1a imagery may limit the forecasting capability of fier the optical imagery on the other hand have temporally longer and denser observations therefore it is expected that combining sar and optical imagery through data fusion techniques can help overcome the limitation of using a single source of satellite imagery 8 software availability name of the web application fier mekong developer and contact information chi hung chang cchang37 cougarnet uh edu hyongki lee hlee45 central uh edu son k do skdo3 cougarnet uh edu kel markert kel markert nasa gov year first available 2022 requirements a device with a web browser and internet connection source code availability https github com satellitehydrology fier mekong cost free program language python declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by nasa s applied sciences program for servir 80nssc20k0152 80nssc23k0184 university of houston s gear program and vietnam national foundation for science and technology development ne s002847 1 this study was also partially supported by the korea ministry of environment under the demand responsive water supply service program 2019002650004 and the national research foundation of korea nrf grant funded by the korean government msit no 2021r1a2c100578011 we also acknowledge the publicly available fwdet gee tool developed by peter et al 2020 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105643 
