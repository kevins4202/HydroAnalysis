index,text
25620,large areas of nitrogen sensitive habitats are currently estimated to be in exceedance of their critical loads cls as indicators for protection from nitrogen deposition in the uk deposition estimates from the semi empirical concentration based estimated deposition cbed model are used for official reporting of current exceedances the uk integrated assessment model ukiam framework is designed to provide future projections of concentrations and deposition due to projected changes in emissions ukiam has been extended to provide alternative deposition estimates aligned with those of cbed and the results combined with the range in habitat cl values to create an exceedance score leading to a probabilistic evaluation of cl exceedances the utility of the method is demonstrated by analysing a series of hypothetical scenarios it is shown that nh3 mitigation is likely to be four times more effective in reducing cl exceedances in the uk than the mitigation of nox emissions keywords nitrogen deposition uncertainty critical loads eutrophication integrated assessment modelling 1 introduction by 2030 the uk government plans to reduce ammonia nh3 and nitrogen oxide nox emissions by 16 and 73 respectively relative to emissions in 2005 as outlined in the 2019 clean air strategy defra 2019 in addition to the well documented human health impacts of these pollutants they have a significant impact on the health of ecosystems through direct effects from atmospheric concentrations and when deposited in wet no3 nh4 and dry nh3 hno3 nox forms this impact includes the eutrophication of soils and freshwater leading to the loss of species that become less competitive when nitrogen n availability is increased and acidification leading to effects such as reduced fertility and nutrient deficiencies and the loss of acid sensitive species sulphur emissions also contribute significantly to acidification however the deposition of sulphur oxides sox is no longer the main cause of acidification in the uk following a 94 decrease in the uk so2 emissions between 1970 and 2010 rotap 2012 as of 2017 39 of uk ecosystems were considered to be in exceedance of their critical loads cls for acidity while 58 were considered to be in excess of their eutrophication cl rowe et al 2020 the exceedance of cls has long been used as a method to evaluate the harm caused to specific habitats by the deposition of reactive nitrogen and has proven particularly useful for target setting and policy development e g the unece s gothenburg protocol cls are estimates of the deposition rate below which a habitat is not considered to be significantly harmed according to current knowledge nilsson and grennfelt 1988 for n deposition cls are evaluated empirically based on experiment and field observations and agreed at expert workshops at the unece level under the convention on long range transboundary air pollution given the empirical nature of their derivation there is a degree of subjectivity in their evaluation in addition to uncertainties underlying the observations on which these judgements are made further some studies e g armitage et al 2014 payne et al 2013 have suggested that vegetation changes incrementally with n deposition with no threshold below which no effects are seen given these factors there is a question as to how meaningful the exceedance of a single limit value i e the cl value is in assessing the harm caused to habitats through eutrophication despite this cl values do provide a useful measure of the varying degrees of resilience of different habitats to excess nitrogen exceedances of cl values also continue to be commonly used for policy development in many european countries including the uk e g trends report rowe et al 2020 in the uk the concentration based estimated deposition cbed model is used for official reporting of cl exceedances of deposition for sensitive habitats hall et al 2015 the cbed model estimates dry deposition rates using a big leaf model smith et al 2000 combining gas and particulate concentration maps constrained to measurements with maps of vegetation cover the model accounts for vegetation specific deposition velocities and includes a simple model of the complex bi directional exchange of ammonia due to stomatal emission wet deposition is estimated by combining spatially distributed measurements of concentrations in precipitation with annual precipitation maps and a two dimensional seeder feeder rainfall seeder rain from high level cloud falls through lower hill feeder clouds model dore and choularton 1992 to estimate wet deposition rates smith and fowler 2000 considerable uncertainty is associated with these deposition estimates particularly in areas of higher altitude and precipitation where deposition rates are greatest this is due to a number of reasons including a shortage of measurements at high altitude and the added complexity of orographic enhancement the direct use of concentrations in precipitation measurements as input to the model also means that there is a shortage of measurements for model validation smith and fowler 2000 showed that uncertainty in the feeder rain enhancement factor specified as a parameter of the seeder feeder model has a significant effect on the model output this factor which is used to scale the estimated deposition due to feeder rain is assumed to be equal to 2 based on a single set of experiments conducted on the great dun fell over the course of a few days choularton et al 1988 while reasonable agreement has been shown with subsequent measurements dore et al 2001 beswick et al 2003 such validation datasets are scarce further measurements of wet deposition from bulk collectors are known to overestimate wet deposition as they can also capture a degree of dry deposition e g fowler and cape 1984 gonzález benítez et al 2009 cape et al 2011 in contrast atmospheric chemical transport models actms are generally found to underestimate deposition of reactive n when compared to measurements in the uk dore et al 2015 and across europe fagerli et al 2021 to what extent this discrepancy is due to bias in the models or the measurements is currently not well understood one cause could be an underestimation of nh3 emissions the uk national atmospheric emission inventory estimates were recently shown to give lower uk nh3 emissions when compared to estimates derived from satellite observations marais et al 2021 further research is required before these differences are fully understood and a substantial increase in measurement sites is likely required the uk integrated assessment model ukiam is a model framework consisting of a family of physically based models used to investigate the impact of future emissions scenarios on uk air quality and ecosystem health oxley et al 2013 apsimon et al 2021 a scenario can be run within an hour and possible outputs include national concentration and deposition maps estimates of exceedances of cls and a full breakdown of source apportionment allowing the identification of the most harmful sources by habitat and by region ukiam also underestimates deposition relative to measurements and the cbed model the degree of underestimation by ukiam relative to cbed is of the same order as the mean underestimation of the actms considered by dore et al 2015 relative to measurements the areas of greatest disagreement between ukiam and cbed correspond to the areas of greatest uncertainty in deposition which tend to be areas of higher altitude and precipitation where complex wet deposition processes occur such as occult and seeder feeder deposition in this paper we outline a new methodology implemented within the ukiam framework for the assessment of the impact of n deposition on sensitive habitats in the uk on a national and regional level the approach is based on the joint nature conservation committee s jncc nitrogen decision framework s method designed for national level evaluation while accounting for the uncertainty in both the n deposition and cl estimates jones et al 2016 we make use of cls as a measure of the varying sensitivities of habitats to eutrophication but remove the dependence on the exceedance of a single limit value by introducing an exceedance score based on two cl values a minimum and a maximum these two cl values are combined with two deposition estimates based on two fundamentally different models ukiam and cbed providing an upper and lower value in each grid square and used to create an exceedance scale as the two deposition models used are based on inherently different methodologies they are unlikely to share the same biases therefore combining the two provides a more robust analysis than using a single model prediction further the exceedance scale removes the dependency of the analysis on a single imprecise limit value and avoids step changes in exceedance which can occur when a single limit value is used the method is used to assess the state of habitat exceedance of nitrogen deposition for the base year 2016 in addition to two sets of hypothetical scenarios designed to explore an effective strategy for reducing exceedances in the uk and the degree of abatement required to reach uk government targets 2 method 2 1 the ukiam framework a brief overview of the components of the ukiam framework relevant to ecosystem assessment is provided here for a detailed description of a recent version of the framework see apsimon et al 2021 the framework brings together inputs from several independent models these include asam apsimon et al 1994 used to calculate the imported pollutant contribution from other countries brutal oxley et al 2009 a bottom up traffic model which estimates the emissions from traffic and ukiam5 a 5 km resolution sub model used to estimate concentrations and deposition due to uk sources in addition to international shipping nox emissions from shipping are substantial and have a significant impact both on ecosystem health and on air quality in the uk apsimon et al 2019 ukiam5 estimates deposition for future scenarios by scaling source receptor s r footprints of deposition generated by an actm to reflect the change in emissions relative to a base case the s r footprints are generated by reducing the emissions from each source individually relative to a base case before calculating the deposition rate using the actm the difference between the calculated deposition and the base case is then used to calculate a map of the change in deposition per unit change in emission i e the s r footprint this is done separately for each pollutant for the work presented in this study the frame fine resolution atmospheric multi pollutant exchange model singles et al 1998 is used to generate the uk s r footprints however any atmospheric dispersion model could be used to generate the s r footprints and implemented within the ukiam framework this scaling of s r footprints depends on the assumption that concentration and deposition estimates vary linearly with emissions this linear assumption has been shown to be acceptable for variations in emissions of 40 aleksankina et al 2018 i e within this range the effect of non linearity is acceptable relative to other uncertainties the framework also includes the ukiam1 sub model used to generate 1 km resolution maps of concentrations for primary pollutants pm and nox no2 for example used to develop policy aimed at meeting the uk s targets for pm2 5 concentrations set out in the uk s clean air strategy defra 2019 however in this paper we focus on the utility of the ukiam framework in evaluating n deposition and corresponding cl exceedances at a national level in total ukiam considers 94 uk sources these are divided between ten snap selected nomenclature for air pollution sectors as defined in the uk s national atmospheric emissions inventory naei table 1 shows the sectors and the associated emissions of nox and nh3 international shipping in the sea areas surrounding the uk is not included in the naei but is also considered the nh3 emissions are dominated by snap 10 agriculture these nh3 emissions are mainly due to livestock but fertiliser also contributes a significant amount 56 ktonnes nox emissions are distributed between several snap sectors mainly snap 1 3 7 and 8 in addition to international shipping the importance of including international shipping is evident from the very high emission of nox surrounding the uk from this sector apsimon et al 2019 the imported contribution from other countries is also significant and the magnitude is estimated using asam which uses s r footprints derived from the european scale model emep simpson et al 2012 these emep s r footprints are used to calculate the total deposited n from european emissions this contribution is then spatially distributed across the uk using frame s r deposition footprints emep is used to provide the magnitude of the contribution from other countries reaching the uk because frame is known to underestimate long range transport of ions such as no3 and nh4 due to an assumption of constant drizzle which overestimates washout of these pollutants however frame then gives a more detailed spatial mapping of the deposition reflecting important orographic enhancement effects on wet removal over land at higher altitudes the use of both emep and frame therefore allows both a reasonable estimate of the contribution from other countries and the enhanced washout in areas of higher precipitation the maps for the imported contribution from other countries and for shipping is provided in the supplementary material figs s4 and s5 different deposition velocities are assumed for short habitats such as grasses and dwarf shrub heath than for taller habitats such as woodlands two separate maps are used the first for short habitats which is referred to as the moorland deposition map and a second for woodland referred to as the woodland deposition map these deposition values are calculated for all grid squares regardless of whether a given habitat exists within the grid square and therefore do not represent the actual deposition which depends on the area of moorland and woodland habitats within a grid square the moorland and woodland maps for deposition derived from beef production in 2016 are provided in the supplementary material as an example fig s2 the sources given in table 1 are broken down further into 94 individual sources the breakdown for agriculture is shown the latest version of ukiam version 6r allows the emissions from different regions of the uk england wales scotland northern ireland and london to be varied independently although the distribution within these regions remain fixed 2 2 nitrogen sensitive habitats in total 13 nitrogen sensitive habitats are considered for analysis these are given in table 2 along with the range of cl values these habitats match those used for official reporting in the uk rowe et al 2020 the ukiam includes a library of 1 km 1 km maps indicating the area of land covered by each habitat in each grid square along with the appropriate recommended cl value for those habitats for which this value is not constant across the country therefore while depositions are estimated at 5 km resolution all exceedance statistics and maps are generated at 1 km resolution due to the higher resolution of the ecosystems data along with minimum and maximum cls a recommended value cl rec within this range is also given for each habitat although increasingly use of the minimum cl value is recommended for this here the recommended cl value is used to calculate the exceedance unless otherwise stated while the minimum and maximum values are used to derive the exceedance score section 2 4 2 3 accumulated exceedance the accumulated exceedance ae is used as a metric for the level of exceedance of the recommended cl for a particular habitat at the national or regional level the ae for a habitat is calculated as follows ae kg year exceedance kg ha year x exceeded area ha the average accumulated exceedance aae is used as a metric of the exceedance across all n sensitive habitats the aae is calculated by dividing the total ae for all habitats by the total area of all habitats hall et al 2015 2 4 exceedance score the exceedance score is derived in order to provide a more stable and reliable indicator of ecosystem protection than provided by the exceedance of a single cl value estimated using a single deposition value the method is based on that designed for national scale evaluation factor 1 score outlined in the jncc s nitrogen decision framework jones et al 2016 we use the minimum and maximum deposition values for a given habitat in each grid square to provide an indicator of the uncertainty rather than the 95 confidence interval used in the nitrogen decision framework as illustrated in fig 1 the two deposition estimates consist of the ukiam estimate and the ukiam scaled estimate the ukiam scaled estimate is generated by multiplying the ukiam deposition in each grid square by the ratio of the cbed and ukiam values for the 2016 base year n u k i a m s c a l e d i n u k i a m i n c b e d 2016 n u k i a m 2016 i for each grid square i the resulting ukiam scaled map of deposition combines important spatial information based on the empirical cbed model for example areas where measured deposition is greater than the modelled ukiam deposition with estimates of the relative reduction in deposition estimated by ukiam this allows us to incorporate the range in deposition estimates into our analysis of future scenarios which is otherwise not possible for cbed due to its dependence on measurements we then define our minimum and maximum deposition values for each grid square as 1 n m i n i min n u k i a m i n u k i a m s c a l e d i n m a x i max n u k i a m i n u k i a m s c a l e d i this is done separately for the woodland and moorland deposition estimates the maps of n m i n and n m a x for moorland and woodland are shown in fig s6 the difference between the deposition estimates provided by the two models varies in magnitude across the uk fig s7 maps of the ratio of deposition given by cbed and ukiam are shown in fig s8 a statistical comparison of the ukiam deposition estimates with those of cbed is provided in the supplementary material ukiam generally predicts lower values of deposition particularly in areas of higher precipitation and higher altitude such as much of scotland wales and the lake district where the uncertainty in deposition estimates and measurements are greatest for the base year the ukiam scaled estimate is equal to the cbed estimate since it is the base year values which are used to derive the ratio maps however the method can also be used to provide a second estimate of deposition for future scenarios which are not available directly from cbed cl values provide a useful measure of the varying degrees of resilience of different habitats to excess nitrogen for example montane is a particularly sensitive habitat and therefore has a lower recommended cl value 7 kg n ha yaer than a more resilient yet still sensitive habitat calcareous grassland which has a higher recommended cl value 15 kg n ha yaer the damage per unit n deposited is likely to be higher for montane than calcareous grassland and cls provide a measure of this difference each habitat is assigned a recommended critical load clrec in addition to a minimum clmin and a maximum clmax reflecting the imprecise derivation of cls we make use of this range as an indicator of the habitat s resilience to n deposition and therefore the likelihood with which a habitat will survive estimated rates of deposited n the higher and lower deposition values within each grid square are identified equation 1 six scores p0 p1 p4 and p5 are defined ranging from highly unlikely to be in exceedance to highly likely to be in exceedance and p2 and p3 which are defined as marginal due to cl estimates and deposition estimates respectively fig 1 the score is assigned to each habitat grid square i as follows 2 p i p 0 i f n m a x i c l m i n p 1 i f n m i n i c l m i n a n d n m a x i c l m i n a n d n m a x i c l m a x p 2 i f n m i n i c l m i n a n d n m a x i c l m a x p 3 i f n m i n i c l m i n a n d n m a x i c l m a x p 4 i f n m i n i c l m i n a n d n m i n i c l m a x a n d n m a x i c l m a x p 5 i f n m i n i c l m a x 3 results 3 1 base year assessment 3 1 1 ukiam deposition estimates and source apportionment maps of the total deposition of n nox and nhx estimated by ukiam for 2016 are given in fig 2 a c note the different scale used for the nox and nhx components as compared with the total n map the deposition of nhx greatly exceeds that of nox the greatest deposition occurs in areas of higher altitude and precipitation that are downwind of many emissions sources such as wales and north west england here orographic enhancement increases the rate of deposition through processes such as occult deposition crossley et al 1992 and seeder feeder effect fowler et al 1988 the deposition given by ukiam in scotland is much lower than the other regions of the uk due to the distance of much of scotland from major pollution sources the distribution of nhx deposition correlates closely with the distribution of agriculture emissions fig s3 since a large proportion of nhx is often deposited locally some nox deposition occurs in urban areas which are large sources of nox emissions but most of the nox deposition occurs after conversion to no3 during long range transport with removal in precipitation enhanced over areas of higher altitude table 3 provides the contribution of uk sources international shipping and the imported contribution from europe for each uk region as calculated by ukiam this source apportionment is not available for cbed since only the total nhx and nox depositions are available see fig 2d f maps of the nhx and nox deposition due to these sources are provided in the supplementary material figs s4 and s5 uk emissions of nhx make by far the greatest contribution to the deposition budget in the uk 97 8 ktonnes twice that of uk nox sources 49 7 ktonnes a large proportion of the uk nhx contribution is due to agriculture emissions 81 0 ktonnes while the largest contributor to nox deposition within the uk is road transport 21 0 ktonnes the relative contribution of agriculture is particularly high in northern ireland where it constitutes 93 of the deposited nhx the imported contribution of total nitrogen is also provided in table 3 for international shipping and other european countries the international shipping contribution is entirely nox since nhx emissions from shipping are negligible this contribution is mainly concentrated in the south of england and wales near the major shipping lanes see fig s4 the contribution from other countries is highest in wales northern ireland and the south east of england due to their proximity to other countries the nhx contribution from across the border in ireland is particularly high fig s5 3 1 2 cbed deposition estimates and model comparison we refer to the base case as 2016 because ukiam uses naei emission estimates for this year however it should be noted that the cbed deposition values are in fact derived from a three year average spanning 2015 2017 fig 2 d f shows this three year average n nox and nhx deposition in the uk it is immediately evident that cbed gives significantly higher deposition than ukiam fig 2 a c this is particularly true in areas of higher altitude such as much of scotland wales and the north west of england conversely ukiam gives higher deposition values in urban areas where london in particular stands out despite these differences the areas of highest deposition rates are generally in agreement between the two models maps of the difference between the two models and the ratio of deposition values are shown in fig 3 and fig 4 respectively table 4 shows the deposition budget in each region of the uk as predicted by the two models with the exception of scotland ukiam estimates the total deposited nhx and nox to be 83 and 80 respectively of that estimated by cbed however in scotland the deposition is 46 and 60 of that given by cbed for nox and nhx respectively a statistical comparison of the two models is given in the supplementary material 3 1 3 critical load exceedances fig 5 shows the aae for all ecosystems calculated using the recommended critical loads table 2 given by a ukiam and b cbed due to the higher deposition predicted by cbed the exceedance tends to be higher than for ukiam across much of the uk this is particularly true in wales and the north of england in scotland the exceedance predicted by cbed is also higher than that by ukiam however it is considerably lower than that for the other regions of the uk table 5 over half of the n sensitive habitat area considered is in scotland therefore the uk wide statistics are heavily dependent on the exceedances in this region as the situation in scotland is not reflective of that in the rest of the uk it is worth considering the average exceedance across england wales and northern ireland separately these are also given in table 5 and can be seen to be significantly higher than the uk wide average the significance of agriculture is evident from both models with both predicting high exceedance in areas near high agricultural nh3 emissions shown in fig s3 a 3 2 exceedance score table 6 shows the percentage area of habitat attributed to each exceedance score for 2016 for the uk 46 5 of the n sensitive habitat area is either highly unlikely or unlikely in exceedance this lies between the area not in exceedance of the recommended cl estimated by cbed and ukiam independently in section 3 1 3 of 42 and 57 respectively similarly for scotland 71 3 of the n sensitive habitat area is either highly unlikely or unlikely in exceedance which lies between the independently estimated range of 65 87 for wales and n ireland the percentage area in these lowest two exceedance scores is lower than the range derived from the two models however for these two regions the percentage area assigned to p2 and p3 is much greater indicating a greater proportion of habitat area near the exceedance limit therefore the estimated proportion of protected habitat area in these two regions is less certain than that for england and scotland which have a lower proportion of habitat area assigned to the p2 and p3 scores the regional differences in habitat protection are evident with a much more positive outlook in scotland than the remainder of the uk for each region the percentage area of habitats which lie within the p2 score is much greater than that for p3 the percentage area of habitats given the p3 score is very low including in scotland despite the large differences in deposition here between the two models while the difference between the deposition estimates of the two models often exceeds the range in habitat cls this tends to occur for grid squares for which both deposition estimates are greater than the minimum cl value leading to a p4 or p5 score whether a habitat area is deemed to be in exceedance or not is therefore considerably more sensitive to the range in critical load values than the range in deposition estimates fig 6 and table s4 in supplementary material shows the exceedance scores for each habitat across the uk in 2016 it is immediately evident that woodland habitats are at greatest risk other than scots pine which is entirely within scotland with other unmanaged woodlands in particular danger a large variation in exceedance is estimated for bog with 35 8 in category p0 and 34 5 in category p5 this indicates a large variation in the habitat s exceedance depending on location this variation is not captured by the area in exceedance and aae statistics presented in table s2 supplementary material while the area in exceedance given by ukiam and cbed are fairly consistent with the highly likely in exceedance percentage at 36 and 41 respectively the variation in exceedance of the bog areas is not clear from the aae values which are relatively low 1 7 kg ha and 3 2 kg ha ukiam can also provide a regional breakdown of these statistics tables s5 s8 which reveals that this variation is due to low exceedances in scotland and high exceedances elsewhere the scale provided by this approach also highlights the uncertain picture for calcareous grassland scots pine and dune grass where large proportions of the habitats lie within the p2 category 37 3 38 7 and 53 1 respectively indicating that for a large proportion of grid squares the deposition estimates lie within the cl range 3 3 scenario analysis we first consider three scenarios as an initial investigation into the most effective strategy to reduce cl exceedance in the uk these selective scenarios include a 40 reduction in all imported emissions imported from other countries and sea areas including international shipping a 40 reduction in all uk nox emissions and a 40 reduction in all uk nh3 emissions the absolute contribution of each of these sources to the total n deposition in addition to the spatial distribution of the deposition due to each of these sources varies significantly as seen in table 3 and figs s4 and s5 due to these differences their relative impacts on cl exceedances are likely to vary significantly by individually reducing each of these components by an equal proportion we are able to explore their relative impacts on exceedances we then consider four blanket scenarios abating all nh3 and nox emissions both domestic and imported by 20 30 40 and 50 in order to investigate the degree of improvement that can be expected from varying degrees of abatement fig 7 shows a map of the reduction in aae for each scenario as given by ukiam it is clear that the 40 reduction in uk nh3 emissions is the scenario which most effectively reduces the aae for this scenario both ukiam and ukiam scaled see fig s9 predict significant reductions in aae across much of england wales northern ireland and southern scotland the reductions predicted for the abated imported contribution and abated uk nox emissions are much lower for the abated imported contribution scenario the reductions are highest in wales the north west of england and the south east of england which correspond to the areas of higher deposition of imported emissions the reduction in aae due to uk nox abatement is low across the entire uk with slightly higher values in the north west of england table 7 provides the reduction in n deposition for each scenario relative to the 2016 base case in addition to the change in area of habitat in the lower two and higher two exceedance categories representing highly unlikely or unlikely in exceedance p0 p1 and likely or highly likely in exceedance p4 p5 the upper and lower values shown for the deposition represent the changes predicted by ukiam and ukiam scaled the uk nh3 abatement scenario leads to a greater reduction in deposition by roughly a factor of 2 than the other two selective scenarios from the selective scenarios only the uk nh3 abatement scenario achieves the 2030 target of 17 reduction in deposited n outlined in uk s clean air strategy defra 2019 the greater reduction in deposition by a factor of 2 from the abatement of nh3 as compared to the other two selective abatement scenarios is unsurprising given that nh3 emissions contribute twice the amount to the total n deposition budget table 3 however the increase in the area of habitats in categories p0 p1 for the uk nh3 abatement scenario is a factor four greater than that for the nox abatement scenario and three times greater than the imported abated scenario a similar comparison is seen for the reduction in the area of habitats in categories p4 p5 this increased factor is due to the different spatial distributions of the deposition from these sources with a greater proportion of the deposited n from nh3 affecting sensitive habitats therefore the abatement of uk nh3 emissions is likely to be a much more effective strategy for reducing cl exceedances than the abatement of uk nox emissions and is likely to provide much greater benefits for sensitive habitats than those gained from emission reductions outside the uk the uk nox abatement scenario leads to the lowest improvement in habitat area in exceedance lower than the reduced imported contribution scenario this is perhaps unsurprising since much of the deposited nox is from long range transport and the total reduction in european nox emissions for the imported scenario is considerably greater than that for the uk nox scenario for the imported scenario all european nox emissions outside the uk is reduced by 40 including international shipping however the reduction in deposited n for the two scenarios is of a similar order this suggests that n deposited due to imported emissions has a proportionally greater impact on cl exceedances than that deposited due to uk nox emissions as it is more likely to deposit in areas containing sensitive habitats for the blanket reduction scenarios the rate of decrease in deposited n with each 10 increment reduction of reactive n emitted is nearly linear the small non linearity is due to cross pollutant effects so2 emissions are kept constant for all scenarios the change in habitat area in the p0 p1 categories also decreases fairly linearly at the national level however this is not the case by region as seen in fig 8 in wales there is an increasing rate of improvement as abatement is increased suggesting that in wales high abatement policies are required in order to see the most significant improvements it is also clear from fig 8 that there remains a significant variation in habitat exceedance between each region even for the 50 abated scenario in this case the national area of habitats in the p0 p1 categories is up to 78 such an improvement would contribute significantly to the achievement of the target set by the uk government in the 25 year environment plan defra 2018 of restoring 75 of protected sites to a favourable condition however given that each devolved administration in the uk sets their own environmental targets in reality this target applies only to england for which the area is significantly lower at 55 this is still a very large improvement on the base year value of 8 and there is a large proportion of habitat area near the exceedance limit with 32 of habitat area assigned to the p2 or p3 score it should also be noted that there remains a high degree of variation in exceedance between habitats for all considered scenarios see fig s10 in the supplementary material the proportion of woodland habitats unlikely to be in exceedance remains low across the scenarios with the area of unmanaged woodland given the p0 p1 scores as low as 8 for the 50 abated scenario however there is again a large proportion of habitat area near the exceedance limit reflected by the prominence of the p2 score at 50 4 discussion the analysis of the 2016 base year given here highlights the urgent need for action within the uk to protect nitrogen sensitive habitats from eutrophication outside scotland the vast majority of woodland habitats are highly likely to be in exceedance of their eutrophication cls with high levels of exceedance predicted by both ukiam and cbed models large areas of short habitats such as grasslands and bogs are also likely to be in exceedance with saltmarsh being the only habitat which is currently almost entirely below its cl the 50 abatement scenario gives an indication of what could be achieved were european nations to meet the highly ambitious target proposed by the unep s colombo declaration united nations 2019 of a 50 reduction in all n waste by 2030 it is clear from the rather modest gains that are achieved by the lower abatement scenarios that reductions of the order targeted by the colombo directive are required if these n sensitive habitats in the uk are to be protected the significantly greater reductions in exceedances achieved through the abatement of nh3 as compared to nox also highlights that significant abatement of nh3 emissions is key to achieve the uk government s target of restoring 75 of protected sites to a favourable condition 25 year environment plan defra 2018 it is unlikely that the uk necd targets of a 16 reduction in nh3 and a 73 reduction in nox emissions will be sufficient it is important to note that here we consider all sensitive habitat areas not only those assigned for protection for example sssi sites of special scientific interest sites while the modelling suggests that a considerable decrease in national emissions is required this alone will not be enough to protect these sites and therefore additional local measures will also likely be necessary as suggested in the nitrogen futures report dragosits et al 2020 the outlook in scotland is more positive than the other regions of the uk despite considerable uncertainty in the deposition estimates there this uncertainty is reflected in the large discrepancy between the two model predictions in scotland the causes of the disagreement are not yet fully understood however both models result in lower exceedances in scotland than the remainder of the uk table 5 the exceedance score shows that most of the habitat area in scotland lies within the lower p0 and p1 categories providing confidence in this regional outlook despite the uncertainty in deposition estimates for each scenario the p2 category is much more prominent than the p3 category this does not mean that the ranges in the cl estimates are necessarily greater than the ranges seen in deposition estimates however the areas where the absolute difference between the deposition estimates is large correspond to the areas where deposition is high and therefore both deposition estimates tend to be in exceedance of the minimum cl value leading to a p4 or p5 category in areas where the deposition estimates are of a similar order to the cl range the absolute difference between the two deposition estimates tend to be smaller than the cl range leading to a p2 category this remains true for the abatement scenarios for which the area in the p3 category remain low despite areas of very high deposition being much reduced marginal evaluations of cl exceedance are therefore in the most part due to the range in cl values rather than the range in deposition estimates finally it should be noted that restoring a habitat area to n deposition values below their cls does not guarantee protection or restoration of the habitat to a past state first there is evidence that there is no threshold of n deposition below which no effects are seen e g armitage et al 2014 payne et al 2013 secondly a particular habitat could disappear from the area before n deposition levels are reduced below the cl despite this the cl values do provide useful indicators of habitat resilience to n deposition 5 conclusions the ukiam framework has been extended to provide a second set of nitrogen deposition estimates based on those given by the cbed model this second set of deposition values is used to provide higher and lower cl exceedance estimates at a national and regional level for nitrogen sensitive habitats the difference between the two models is used as an indicator for the uncertainty which is in turn projected to future scenarios by scaling the ukiam projections by the 2016 ukiam cbed ratio it is unlikely that both models share the same biases and as a result a more robust analysis is provided further an exceedance score has been developed using the two deposition values and the minimum and maximum cl estimates removing the dependency of the analysis on a single imprecise limit value and avoiding step changes in exceedance which can occur when a single limit value is used applying the method to the base year 2016 it is shown that the vast majority of woodland habitats in the uk are likely to be in considerable exceedance of their cls outside of scotland where deposition is lower in the most part large areas of short habitats are also likely in exceedance three hypothetical scenarios are considered in order to explore the varying impacts of n emissions from different sources on cl exceedances in the uk it is shown that uk nh3 emissions contribute a disproportionate amount to exceedances as compared to uk nox and imported emissions with a 40 reduction in nh3 emissions leading to a factor four greater decrease in habitat exceedance relative to the 2016 base year than the other two scenarios four further scenarios are used to explore what level of protection is feasible given increasing abatement levels it is shown that a reduction in the order of 50 of all nox and nh3 sources a level consistent with the colombo declaration target is likely required to reach the uk government s target of restoring 75 of protected sites to a favourable condition defra 2018 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work reflects the personal views of the authors although the modelling work has been supported by the uk department of environment food and rural affairs the findings and recommendations discussed here are those of the authors and do not necessarily represent the views of defra appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105355 
25620,large areas of nitrogen sensitive habitats are currently estimated to be in exceedance of their critical loads cls as indicators for protection from nitrogen deposition in the uk deposition estimates from the semi empirical concentration based estimated deposition cbed model are used for official reporting of current exceedances the uk integrated assessment model ukiam framework is designed to provide future projections of concentrations and deposition due to projected changes in emissions ukiam has been extended to provide alternative deposition estimates aligned with those of cbed and the results combined with the range in habitat cl values to create an exceedance score leading to a probabilistic evaluation of cl exceedances the utility of the method is demonstrated by analysing a series of hypothetical scenarios it is shown that nh3 mitigation is likely to be four times more effective in reducing cl exceedances in the uk than the mitigation of nox emissions keywords nitrogen deposition uncertainty critical loads eutrophication integrated assessment modelling 1 introduction by 2030 the uk government plans to reduce ammonia nh3 and nitrogen oxide nox emissions by 16 and 73 respectively relative to emissions in 2005 as outlined in the 2019 clean air strategy defra 2019 in addition to the well documented human health impacts of these pollutants they have a significant impact on the health of ecosystems through direct effects from atmospheric concentrations and when deposited in wet no3 nh4 and dry nh3 hno3 nox forms this impact includes the eutrophication of soils and freshwater leading to the loss of species that become less competitive when nitrogen n availability is increased and acidification leading to effects such as reduced fertility and nutrient deficiencies and the loss of acid sensitive species sulphur emissions also contribute significantly to acidification however the deposition of sulphur oxides sox is no longer the main cause of acidification in the uk following a 94 decrease in the uk so2 emissions between 1970 and 2010 rotap 2012 as of 2017 39 of uk ecosystems were considered to be in exceedance of their critical loads cls for acidity while 58 were considered to be in excess of their eutrophication cl rowe et al 2020 the exceedance of cls has long been used as a method to evaluate the harm caused to specific habitats by the deposition of reactive nitrogen and has proven particularly useful for target setting and policy development e g the unece s gothenburg protocol cls are estimates of the deposition rate below which a habitat is not considered to be significantly harmed according to current knowledge nilsson and grennfelt 1988 for n deposition cls are evaluated empirically based on experiment and field observations and agreed at expert workshops at the unece level under the convention on long range transboundary air pollution given the empirical nature of their derivation there is a degree of subjectivity in their evaluation in addition to uncertainties underlying the observations on which these judgements are made further some studies e g armitage et al 2014 payne et al 2013 have suggested that vegetation changes incrementally with n deposition with no threshold below which no effects are seen given these factors there is a question as to how meaningful the exceedance of a single limit value i e the cl value is in assessing the harm caused to habitats through eutrophication despite this cl values do provide a useful measure of the varying degrees of resilience of different habitats to excess nitrogen exceedances of cl values also continue to be commonly used for policy development in many european countries including the uk e g trends report rowe et al 2020 in the uk the concentration based estimated deposition cbed model is used for official reporting of cl exceedances of deposition for sensitive habitats hall et al 2015 the cbed model estimates dry deposition rates using a big leaf model smith et al 2000 combining gas and particulate concentration maps constrained to measurements with maps of vegetation cover the model accounts for vegetation specific deposition velocities and includes a simple model of the complex bi directional exchange of ammonia due to stomatal emission wet deposition is estimated by combining spatially distributed measurements of concentrations in precipitation with annual precipitation maps and a two dimensional seeder feeder rainfall seeder rain from high level cloud falls through lower hill feeder clouds model dore and choularton 1992 to estimate wet deposition rates smith and fowler 2000 considerable uncertainty is associated with these deposition estimates particularly in areas of higher altitude and precipitation where deposition rates are greatest this is due to a number of reasons including a shortage of measurements at high altitude and the added complexity of orographic enhancement the direct use of concentrations in precipitation measurements as input to the model also means that there is a shortage of measurements for model validation smith and fowler 2000 showed that uncertainty in the feeder rain enhancement factor specified as a parameter of the seeder feeder model has a significant effect on the model output this factor which is used to scale the estimated deposition due to feeder rain is assumed to be equal to 2 based on a single set of experiments conducted on the great dun fell over the course of a few days choularton et al 1988 while reasonable agreement has been shown with subsequent measurements dore et al 2001 beswick et al 2003 such validation datasets are scarce further measurements of wet deposition from bulk collectors are known to overestimate wet deposition as they can also capture a degree of dry deposition e g fowler and cape 1984 gonzález benítez et al 2009 cape et al 2011 in contrast atmospheric chemical transport models actms are generally found to underestimate deposition of reactive n when compared to measurements in the uk dore et al 2015 and across europe fagerli et al 2021 to what extent this discrepancy is due to bias in the models or the measurements is currently not well understood one cause could be an underestimation of nh3 emissions the uk national atmospheric emission inventory estimates were recently shown to give lower uk nh3 emissions when compared to estimates derived from satellite observations marais et al 2021 further research is required before these differences are fully understood and a substantial increase in measurement sites is likely required the uk integrated assessment model ukiam is a model framework consisting of a family of physically based models used to investigate the impact of future emissions scenarios on uk air quality and ecosystem health oxley et al 2013 apsimon et al 2021 a scenario can be run within an hour and possible outputs include national concentration and deposition maps estimates of exceedances of cls and a full breakdown of source apportionment allowing the identification of the most harmful sources by habitat and by region ukiam also underestimates deposition relative to measurements and the cbed model the degree of underestimation by ukiam relative to cbed is of the same order as the mean underestimation of the actms considered by dore et al 2015 relative to measurements the areas of greatest disagreement between ukiam and cbed correspond to the areas of greatest uncertainty in deposition which tend to be areas of higher altitude and precipitation where complex wet deposition processes occur such as occult and seeder feeder deposition in this paper we outline a new methodology implemented within the ukiam framework for the assessment of the impact of n deposition on sensitive habitats in the uk on a national and regional level the approach is based on the joint nature conservation committee s jncc nitrogen decision framework s method designed for national level evaluation while accounting for the uncertainty in both the n deposition and cl estimates jones et al 2016 we make use of cls as a measure of the varying sensitivities of habitats to eutrophication but remove the dependence on the exceedance of a single limit value by introducing an exceedance score based on two cl values a minimum and a maximum these two cl values are combined with two deposition estimates based on two fundamentally different models ukiam and cbed providing an upper and lower value in each grid square and used to create an exceedance scale as the two deposition models used are based on inherently different methodologies they are unlikely to share the same biases therefore combining the two provides a more robust analysis than using a single model prediction further the exceedance scale removes the dependency of the analysis on a single imprecise limit value and avoids step changes in exceedance which can occur when a single limit value is used the method is used to assess the state of habitat exceedance of nitrogen deposition for the base year 2016 in addition to two sets of hypothetical scenarios designed to explore an effective strategy for reducing exceedances in the uk and the degree of abatement required to reach uk government targets 2 method 2 1 the ukiam framework a brief overview of the components of the ukiam framework relevant to ecosystem assessment is provided here for a detailed description of a recent version of the framework see apsimon et al 2021 the framework brings together inputs from several independent models these include asam apsimon et al 1994 used to calculate the imported pollutant contribution from other countries brutal oxley et al 2009 a bottom up traffic model which estimates the emissions from traffic and ukiam5 a 5 km resolution sub model used to estimate concentrations and deposition due to uk sources in addition to international shipping nox emissions from shipping are substantial and have a significant impact both on ecosystem health and on air quality in the uk apsimon et al 2019 ukiam5 estimates deposition for future scenarios by scaling source receptor s r footprints of deposition generated by an actm to reflect the change in emissions relative to a base case the s r footprints are generated by reducing the emissions from each source individually relative to a base case before calculating the deposition rate using the actm the difference between the calculated deposition and the base case is then used to calculate a map of the change in deposition per unit change in emission i e the s r footprint this is done separately for each pollutant for the work presented in this study the frame fine resolution atmospheric multi pollutant exchange model singles et al 1998 is used to generate the uk s r footprints however any atmospheric dispersion model could be used to generate the s r footprints and implemented within the ukiam framework this scaling of s r footprints depends on the assumption that concentration and deposition estimates vary linearly with emissions this linear assumption has been shown to be acceptable for variations in emissions of 40 aleksankina et al 2018 i e within this range the effect of non linearity is acceptable relative to other uncertainties the framework also includes the ukiam1 sub model used to generate 1 km resolution maps of concentrations for primary pollutants pm and nox no2 for example used to develop policy aimed at meeting the uk s targets for pm2 5 concentrations set out in the uk s clean air strategy defra 2019 however in this paper we focus on the utility of the ukiam framework in evaluating n deposition and corresponding cl exceedances at a national level in total ukiam considers 94 uk sources these are divided between ten snap selected nomenclature for air pollution sectors as defined in the uk s national atmospheric emissions inventory naei table 1 shows the sectors and the associated emissions of nox and nh3 international shipping in the sea areas surrounding the uk is not included in the naei but is also considered the nh3 emissions are dominated by snap 10 agriculture these nh3 emissions are mainly due to livestock but fertiliser also contributes a significant amount 56 ktonnes nox emissions are distributed between several snap sectors mainly snap 1 3 7 and 8 in addition to international shipping the importance of including international shipping is evident from the very high emission of nox surrounding the uk from this sector apsimon et al 2019 the imported contribution from other countries is also significant and the magnitude is estimated using asam which uses s r footprints derived from the european scale model emep simpson et al 2012 these emep s r footprints are used to calculate the total deposited n from european emissions this contribution is then spatially distributed across the uk using frame s r deposition footprints emep is used to provide the magnitude of the contribution from other countries reaching the uk because frame is known to underestimate long range transport of ions such as no3 and nh4 due to an assumption of constant drizzle which overestimates washout of these pollutants however frame then gives a more detailed spatial mapping of the deposition reflecting important orographic enhancement effects on wet removal over land at higher altitudes the use of both emep and frame therefore allows both a reasonable estimate of the contribution from other countries and the enhanced washout in areas of higher precipitation the maps for the imported contribution from other countries and for shipping is provided in the supplementary material figs s4 and s5 different deposition velocities are assumed for short habitats such as grasses and dwarf shrub heath than for taller habitats such as woodlands two separate maps are used the first for short habitats which is referred to as the moorland deposition map and a second for woodland referred to as the woodland deposition map these deposition values are calculated for all grid squares regardless of whether a given habitat exists within the grid square and therefore do not represent the actual deposition which depends on the area of moorland and woodland habitats within a grid square the moorland and woodland maps for deposition derived from beef production in 2016 are provided in the supplementary material as an example fig s2 the sources given in table 1 are broken down further into 94 individual sources the breakdown for agriculture is shown the latest version of ukiam version 6r allows the emissions from different regions of the uk england wales scotland northern ireland and london to be varied independently although the distribution within these regions remain fixed 2 2 nitrogen sensitive habitats in total 13 nitrogen sensitive habitats are considered for analysis these are given in table 2 along with the range of cl values these habitats match those used for official reporting in the uk rowe et al 2020 the ukiam includes a library of 1 km 1 km maps indicating the area of land covered by each habitat in each grid square along with the appropriate recommended cl value for those habitats for which this value is not constant across the country therefore while depositions are estimated at 5 km resolution all exceedance statistics and maps are generated at 1 km resolution due to the higher resolution of the ecosystems data along with minimum and maximum cls a recommended value cl rec within this range is also given for each habitat although increasingly use of the minimum cl value is recommended for this here the recommended cl value is used to calculate the exceedance unless otherwise stated while the minimum and maximum values are used to derive the exceedance score section 2 4 2 3 accumulated exceedance the accumulated exceedance ae is used as a metric for the level of exceedance of the recommended cl for a particular habitat at the national or regional level the ae for a habitat is calculated as follows ae kg year exceedance kg ha year x exceeded area ha the average accumulated exceedance aae is used as a metric of the exceedance across all n sensitive habitats the aae is calculated by dividing the total ae for all habitats by the total area of all habitats hall et al 2015 2 4 exceedance score the exceedance score is derived in order to provide a more stable and reliable indicator of ecosystem protection than provided by the exceedance of a single cl value estimated using a single deposition value the method is based on that designed for national scale evaluation factor 1 score outlined in the jncc s nitrogen decision framework jones et al 2016 we use the minimum and maximum deposition values for a given habitat in each grid square to provide an indicator of the uncertainty rather than the 95 confidence interval used in the nitrogen decision framework as illustrated in fig 1 the two deposition estimates consist of the ukiam estimate and the ukiam scaled estimate the ukiam scaled estimate is generated by multiplying the ukiam deposition in each grid square by the ratio of the cbed and ukiam values for the 2016 base year n u k i a m s c a l e d i n u k i a m i n c b e d 2016 n u k i a m 2016 i for each grid square i the resulting ukiam scaled map of deposition combines important spatial information based on the empirical cbed model for example areas where measured deposition is greater than the modelled ukiam deposition with estimates of the relative reduction in deposition estimated by ukiam this allows us to incorporate the range in deposition estimates into our analysis of future scenarios which is otherwise not possible for cbed due to its dependence on measurements we then define our minimum and maximum deposition values for each grid square as 1 n m i n i min n u k i a m i n u k i a m s c a l e d i n m a x i max n u k i a m i n u k i a m s c a l e d i this is done separately for the woodland and moorland deposition estimates the maps of n m i n and n m a x for moorland and woodland are shown in fig s6 the difference between the deposition estimates provided by the two models varies in magnitude across the uk fig s7 maps of the ratio of deposition given by cbed and ukiam are shown in fig s8 a statistical comparison of the ukiam deposition estimates with those of cbed is provided in the supplementary material ukiam generally predicts lower values of deposition particularly in areas of higher precipitation and higher altitude such as much of scotland wales and the lake district where the uncertainty in deposition estimates and measurements are greatest for the base year the ukiam scaled estimate is equal to the cbed estimate since it is the base year values which are used to derive the ratio maps however the method can also be used to provide a second estimate of deposition for future scenarios which are not available directly from cbed cl values provide a useful measure of the varying degrees of resilience of different habitats to excess nitrogen for example montane is a particularly sensitive habitat and therefore has a lower recommended cl value 7 kg n ha yaer than a more resilient yet still sensitive habitat calcareous grassland which has a higher recommended cl value 15 kg n ha yaer the damage per unit n deposited is likely to be higher for montane than calcareous grassland and cls provide a measure of this difference each habitat is assigned a recommended critical load clrec in addition to a minimum clmin and a maximum clmax reflecting the imprecise derivation of cls we make use of this range as an indicator of the habitat s resilience to n deposition and therefore the likelihood with which a habitat will survive estimated rates of deposited n the higher and lower deposition values within each grid square are identified equation 1 six scores p0 p1 p4 and p5 are defined ranging from highly unlikely to be in exceedance to highly likely to be in exceedance and p2 and p3 which are defined as marginal due to cl estimates and deposition estimates respectively fig 1 the score is assigned to each habitat grid square i as follows 2 p i p 0 i f n m a x i c l m i n p 1 i f n m i n i c l m i n a n d n m a x i c l m i n a n d n m a x i c l m a x p 2 i f n m i n i c l m i n a n d n m a x i c l m a x p 3 i f n m i n i c l m i n a n d n m a x i c l m a x p 4 i f n m i n i c l m i n a n d n m i n i c l m a x a n d n m a x i c l m a x p 5 i f n m i n i c l m a x 3 results 3 1 base year assessment 3 1 1 ukiam deposition estimates and source apportionment maps of the total deposition of n nox and nhx estimated by ukiam for 2016 are given in fig 2 a c note the different scale used for the nox and nhx components as compared with the total n map the deposition of nhx greatly exceeds that of nox the greatest deposition occurs in areas of higher altitude and precipitation that are downwind of many emissions sources such as wales and north west england here orographic enhancement increases the rate of deposition through processes such as occult deposition crossley et al 1992 and seeder feeder effect fowler et al 1988 the deposition given by ukiam in scotland is much lower than the other regions of the uk due to the distance of much of scotland from major pollution sources the distribution of nhx deposition correlates closely with the distribution of agriculture emissions fig s3 since a large proportion of nhx is often deposited locally some nox deposition occurs in urban areas which are large sources of nox emissions but most of the nox deposition occurs after conversion to no3 during long range transport with removal in precipitation enhanced over areas of higher altitude table 3 provides the contribution of uk sources international shipping and the imported contribution from europe for each uk region as calculated by ukiam this source apportionment is not available for cbed since only the total nhx and nox depositions are available see fig 2d f maps of the nhx and nox deposition due to these sources are provided in the supplementary material figs s4 and s5 uk emissions of nhx make by far the greatest contribution to the deposition budget in the uk 97 8 ktonnes twice that of uk nox sources 49 7 ktonnes a large proportion of the uk nhx contribution is due to agriculture emissions 81 0 ktonnes while the largest contributor to nox deposition within the uk is road transport 21 0 ktonnes the relative contribution of agriculture is particularly high in northern ireland where it constitutes 93 of the deposited nhx the imported contribution of total nitrogen is also provided in table 3 for international shipping and other european countries the international shipping contribution is entirely nox since nhx emissions from shipping are negligible this contribution is mainly concentrated in the south of england and wales near the major shipping lanes see fig s4 the contribution from other countries is highest in wales northern ireland and the south east of england due to their proximity to other countries the nhx contribution from across the border in ireland is particularly high fig s5 3 1 2 cbed deposition estimates and model comparison we refer to the base case as 2016 because ukiam uses naei emission estimates for this year however it should be noted that the cbed deposition values are in fact derived from a three year average spanning 2015 2017 fig 2 d f shows this three year average n nox and nhx deposition in the uk it is immediately evident that cbed gives significantly higher deposition than ukiam fig 2 a c this is particularly true in areas of higher altitude such as much of scotland wales and the north west of england conversely ukiam gives higher deposition values in urban areas where london in particular stands out despite these differences the areas of highest deposition rates are generally in agreement between the two models maps of the difference between the two models and the ratio of deposition values are shown in fig 3 and fig 4 respectively table 4 shows the deposition budget in each region of the uk as predicted by the two models with the exception of scotland ukiam estimates the total deposited nhx and nox to be 83 and 80 respectively of that estimated by cbed however in scotland the deposition is 46 and 60 of that given by cbed for nox and nhx respectively a statistical comparison of the two models is given in the supplementary material 3 1 3 critical load exceedances fig 5 shows the aae for all ecosystems calculated using the recommended critical loads table 2 given by a ukiam and b cbed due to the higher deposition predicted by cbed the exceedance tends to be higher than for ukiam across much of the uk this is particularly true in wales and the north of england in scotland the exceedance predicted by cbed is also higher than that by ukiam however it is considerably lower than that for the other regions of the uk table 5 over half of the n sensitive habitat area considered is in scotland therefore the uk wide statistics are heavily dependent on the exceedances in this region as the situation in scotland is not reflective of that in the rest of the uk it is worth considering the average exceedance across england wales and northern ireland separately these are also given in table 5 and can be seen to be significantly higher than the uk wide average the significance of agriculture is evident from both models with both predicting high exceedance in areas near high agricultural nh3 emissions shown in fig s3 a 3 2 exceedance score table 6 shows the percentage area of habitat attributed to each exceedance score for 2016 for the uk 46 5 of the n sensitive habitat area is either highly unlikely or unlikely in exceedance this lies between the area not in exceedance of the recommended cl estimated by cbed and ukiam independently in section 3 1 3 of 42 and 57 respectively similarly for scotland 71 3 of the n sensitive habitat area is either highly unlikely or unlikely in exceedance which lies between the independently estimated range of 65 87 for wales and n ireland the percentage area in these lowest two exceedance scores is lower than the range derived from the two models however for these two regions the percentage area assigned to p2 and p3 is much greater indicating a greater proportion of habitat area near the exceedance limit therefore the estimated proportion of protected habitat area in these two regions is less certain than that for england and scotland which have a lower proportion of habitat area assigned to the p2 and p3 scores the regional differences in habitat protection are evident with a much more positive outlook in scotland than the remainder of the uk for each region the percentage area of habitats which lie within the p2 score is much greater than that for p3 the percentage area of habitats given the p3 score is very low including in scotland despite the large differences in deposition here between the two models while the difference between the deposition estimates of the two models often exceeds the range in habitat cls this tends to occur for grid squares for which both deposition estimates are greater than the minimum cl value leading to a p4 or p5 score whether a habitat area is deemed to be in exceedance or not is therefore considerably more sensitive to the range in critical load values than the range in deposition estimates fig 6 and table s4 in supplementary material shows the exceedance scores for each habitat across the uk in 2016 it is immediately evident that woodland habitats are at greatest risk other than scots pine which is entirely within scotland with other unmanaged woodlands in particular danger a large variation in exceedance is estimated for bog with 35 8 in category p0 and 34 5 in category p5 this indicates a large variation in the habitat s exceedance depending on location this variation is not captured by the area in exceedance and aae statistics presented in table s2 supplementary material while the area in exceedance given by ukiam and cbed are fairly consistent with the highly likely in exceedance percentage at 36 and 41 respectively the variation in exceedance of the bog areas is not clear from the aae values which are relatively low 1 7 kg ha and 3 2 kg ha ukiam can also provide a regional breakdown of these statistics tables s5 s8 which reveals that this variation is due to low exceedances in scotland and high exceedances elsewhere the scale provided by this approach also highlights the uncertain picture for calcareous grassland scots pine and dune grass where large proportions of the habitats lie within the p2 category 37 3 38 7 and 53 1 respectively indicating that for a large proportion of grid squares the deposition estimates lie within the cl range 3 3 scenario analysis we first consider three scenarios as an initial investigation into the most effective strategy to reduce cl exceedance in the uk these selective scenarios include a 40 reduction in all imported emissions imported from other countries and sea areas including international shipping a 40 reduction in all uk nox emissions and a 40 reduction in all uk nh3 emissions the absolute contribution of each of these sources to the total n deposition in addition to the spatial distribution of the deposition due to each of these sources varies significantly as seen in table 3 and figs s4 and s5 due to these differences their relative impacts on cl exceedances are likely to vary significantly by individually reducing each of these components by an equal proportion we are able to explore their relative impacts on exceedances we then consider four blanket scenarios abating all nh3 and nox emissions both domestic and imported by 20 30 40 and 50 in order to investigate the degree of improvement that can be expected from varying degrees of abatement fig 7 shows a map of the reduction in aae for each scenario as given by ukiam it is clear that the 40 reduction in uk nh3 emissions is the scenario which most effectively reduces the aae for this scenario both ukiam and ukiam scaled see fig s9 predict significant reductions in aae across much of england wales northern ireland and southern scotland the reductions predicted for the abated imported contribution and abated uk nox emissions are much lower for the abated imported contribution scenario the reductions are highest in wales the north west of england and the south east of england which correspond to the areas of higher deposition of imported emissions the reduction in aae due to uk nox abatement is low across the entire uk with slightly higher values in the north west of england table 7 provides the reduction in n deposition for each scenario relative to the 2016 base case in addition to the change in area of habitat in the lower two and higher two exceedance categories representing highly unlikely or unlikely in exceedance p0 p1 and likely or highly likely in exceedance p4 p5 the upper and lower values shown for the deposition represent the changes predicted by ukiam and ukiam scaled the uk nh3 abatement scenario leads to a greater reduction in deposition by roughly a factor of 2 than the other two selective scenarios from the selective scenarios only the uk nh3 abatement scenario achieves the 2030 target of 17 reduction in deposited n outlined in uk s clean air strategy defra 2019 the greater reduction in deposition by a factor of 2 from the abatement of nh3 as compared to the other two selective abatement scenarios is unsurprising given that nh3 emissions contribute twice the amount to the total n deposition budget table 3 however the increase in the area of habitats in categories p0 p1 for the uk nh3 abatement scenario is a factor four greater than that for the nox abatement scenario and three times greater than the imported abated scenario a similar comparison is seen for the reduction in the area of habitats in categories p4 p5 this increased factor is due to the different spatial distributions of the deposition from these sources with a greater proportion of the deposited n from nh3 affecting sensitive habitats therefore the abatement of uk nh3 emissions is likely to be a much more effective strategy for reducing cl exceedances than the abatement of uk nox emissions and is likely to provide much greater benefits for sensitive habitats than those gained from emission reductions outside the uk the uk nox abatement scenario leads to the lowest improvement in habitat area in exceedance lower than the reduced imported contribution scenario this is perhaps unsurprising since much of the deposited nox is from long range transport and the total reduction in european nox emissions for the imported scenario is considerably greater than that for the uk nox scenario for the imported scenario all european nox emissions outside the uk is reduced by 40 including international shipping however the reduction in deposited n for the two scenarios is of a similar order this suggests that n deposited due to imported emissions has a proportionally greater impact on cl exceedances than that deposited due to uk nox emissions as it is more likely to deposit in areas containing sensitive habitats for the blanket reduction scenarios the rate of decrease in deposited n with each 10 increment reduction of reactive n emitted is nearly linear the small non linearity is due to cross pollutant effects so2 emissions are kept constant for all scenarios the change in habitat area in the p0 p1 categories also decreases fairly linearly at the national level however this is not the case by region as seen in fig 8 in wales there is an increasing rate of improvement as abatement is increased suggesting that in wales high abatement policies are required in order to see the most significant improvements it is also clear from fig 8 that there remains a significant variation in habitat exceedance between each region even for the 50 abated scenario in this case the national area of habitats in the p0 p1 categories is up to 78 such an improvement would contribute significantly to the achievement of the target set by the uk government in the 25 year environment plan defra 2018 of restoring 75 of protected sites to a favourable condition however given that each devolved administration in the uk sets their own environmental targets in reality this target applies only to england for which the area is significantly lower at 55 this is still a very large improvement on the base year value of 8 and there is a large proportion of habitat area near the exceedance limit with 32 of habitat area assigned to the p2 or p3 score it should also be noted that there remains a high degree of variation in exceedance between habitats for all considered scenarios see fig s10 in the supplementary material the proportion of woodland habitats unlikely to be in exceedance remains low across the scenarios with the area of unmanaged woodland given the p0 p1 scores as low as 8 for the 50 abated scenario however there is again a large proportion of habitat area near the exceedance limit reflected by the prominence of the p2 score at 50 4 discussion the analysis of the 2016 base year given here highlights the urgent need for action within the uk to protect nitrogen sensitive habitats from eutrophication outside scotland the vast majority of woodland habitats are highly likely to be in exceedance of their eutrophication cls with high levels of exceedance predicted by both ukiam and cbed models large areas of short habitats such as grasslands and bogs are also likely to be in exceedance with saltmarsh being the only habitat which is currently almost entirely below its cl the 50 abatement scenario gives an indication of what could be achieved were european nations to meet the highly ambitious target proposed by the unep s colombo declaration united nations 2019 of a 50 reduction in all n waste by 2030 it is clear from the rather modest gains that are achieved by the lower abatement scenarios that reductions of the order targeted by the colombo directive are required if these n sensitive habitats in the uk are to be protected the significantly greater reductions in exceedances achieved through the abatement of nh3 as compared to nox also highlights that significant abatement of nh3 emissions is key to achieve the uk government s target of restoring 75 of protected sites to a favourable condition 25 year environment plan defra 2018 it is unlikely that the uk necd targets of a 16 reduction in nh3 and a 73 reduction in nox emissions will be sufficient it is important to note that here we consider all sensitive habitat areas not only those assigned for protection for example sssi sites of special scientific interest sites while the modelling suggests that a considerable decrease in national emissions is required this alone will not be enough to protect these sites and therefore additional local measures will also likely be necessary as suggested in the nitrogen futures report dragosits et al 2020 the outlook in scotland is more positive than the other regions of the uk despite considerable uncertainty in the deposition estimates there this uncertainty is reflected in the large discrepancy between the two model predictions in scotland the causes of the disagreement are not yet fully understood however both models result in lower exceedances in scotland than the remainder of the uk table 5 the exceedance score shows that most of the habitat area in scotland lies within the lower p0 and p1 categories providing confidence in this regional outlook despite the uncertainty in deposition estimates for each scenario the p2 category is much more prominent than the p3 category this does not mean that the ranges in the cl estimates are necessarily greater than the ranges seen in deposition estimates however the areas where the absolute difference between the deposition estimates is large correspond to the areas where deposition is high and therefore both deposition estimates tend to be in exceedance of the minimum cl value leading to a p4 or p5 category in areas where the deposition estimates are of a similar order to the cl range the absolute difference between the two deposition estimates tend to be smaller than the cl range leading to a p2 category this remains true for the abatement scenarios for which the area in the p3 category remain low despite areas of very high deposition being much reduced marginal evaluations of cl exceedance are therefore in the most part due to the range in cl values rather than the range in deposition estimates finally it should be noted that restoring a habitat area to n deposition values below their cls does not guarantee protection or restoration of the habitat to a past state first there is evidence that there is no threshold of n deposition below which no effects are seen e g armitage et al 2014 payne et al 2013 secondly a particular habitat could disappear from the area before n deposition levels are reduced below the cl despite this the cl values do provide useful indicators of habitat resilience to n deposition 5 conclusions the ukiam framework has been extended to provide a second set of nitrogen deposition estimates based on those given by the cbed model this second set of deposition values is used to provide higher and lower cl exceedance estimates at a national and regional level for nitrogen sensitive habitats the difference between the two models is used as an indicator for the uncertainty which is in turn projected to future scenarios by scaling the ukiam projections by the 2016 ukiam cbed ratio it is unlikely that both models share the same biases and as a result a more robust analysis is provided further an exceedance score has been developed using the two deposition values and the minimum and maximum cl estimates removing the dependency of the analysis on a single imprecise limit value and avoiding step changes in exceedance which can occur when a single limit value is used applying the method to the base year 2016 it is shown that the vast majority of woodland habitats in the uk are likely to be in considerable exceedance of their cls outside of scotland where deposition is lower in the most part large areas of short habitats are also likely in exceedance three hypothetical scenarios are considered in order to explore the varying impacts of n emissions from different sources on cl exceedances in the uk it is shown that uk nh3 emissions contribute a disproportionate amount to exceedances as compared to uk nox and imported emissions with a 40 reduction in nh3 emissions leading to a factor four greater decrease in habitat exceedance relative to the 2016 base year than the other two scenarios four further scenarios are used to explore what level of protection is feasible given increasing abatement levels it is shown that a reduction in the order of 50 of all nox and nh3 sources a level consistent with the colombo declaration target is likely required to reach the uk government s target of restoring 75 of protected sites to a favourable condition defra 2018 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work reflects the personal views of the authors although the modelling work has been supported by the uk department of environment food and rural affairs the findings and recommendations discussed here are those of the authors and do not necessarily represent the views of defra appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105355 
25621,shallow slope failures triggered by rainfall commonly pose considerable risks in mountainous areas in order to delineate areas where landslides are more prone to occur within a region we have designed and developed a python qgis plugin named fast shallow landslide assessment model fslam the plugin integrates a simplified hydrological model and a geotechnical model based on the infinite slope theory and contains two principal modules runoff and slope stability modelling it can output up to 15 raster maps describing the hydrological and stability conditions in a short computational time firstly we explain the design of graphical user interface and the elements of the plugin then the berguedà area in ne spain is used as case study to present the procedure of the plugin application the results show that the accuracy of landslide susceptibility assessment performed by fslam plugin is high and the computing time is only a few minutes keywords landslide susceptibility python qgis plugin rainfall runoff 1 introduction shallow landslides induced by heavy rainfall are a global issue which are posing severe implications for mountain environments baum and godt 2010 froude and petley 2018 for scientists and stakeholders dealing with shallow slope failures accurate landslide susceptibility maps at regional scale are necessary information they can identify the areas that are spatially prone to landslides guzzetti et al 2005 hence landslide susceptibility assessment is considered as a key tool for the understanding of the spatial distribution of landslides and can help decision makers on the design of landslide risk reduction strategy guzzetti et al 1999 fell et al 2008 landslide susceptibility models can roughly be divided into heuristic models physically based models statistical models and machine learning models fell et al 2008 zêzere et al 2017 broeckx et al 2018 reichenbach et al 2018 merghadi et al 2020 among them physically based models incorporate soil properties into the susceptibility analysis as they are normally based on geotechnical slope stability equations driven by hydrologic inputs wang et al 2019 on the contrary the other models are typically independent from the physical processes of landslides initiation and runout strauch et al 2019 physically based models therefore have advantages in helping to understand the mechanisms of slope failures especially regarding the effects of rainfall although physically based models for landslide susceptibility assessment are popular e g montgomery and dietrich 1994a rigon et al 2006 baum et al 2008 lehmann and or 2012 mergili et al 2014 each of them has its particular drawbacks which can be grouped in two main groups i large uncertainty associated with soil properties it may strongly influence the model performance due to the inherent variability of input parameters tofani et al 2017 ii high computational cost as a result of the incorporation of a comprehensive approach for the rainfall infiltration in unsaturated soil iverson 2000 model calculations can last from several hours to even several days e g rossi et al 2013 to overcome these limitations an open source code named fast shallow landslide assessment model fslam has been proposed medina et al 2021 it does not only allow using soil properties as stochastic parameters but it can also obtain landslide susceptibility maps of large regions within a few minutes however the fslam code was initially not implemented into a plugin and did not have a graphical user interface gui which made it difficult to use plugins are an essential component of software which can extend the capabilities of an already existing software as well as improve the user experience by providing more user friendly gui without affecting the source code sela et al 2019 plugins are available for many commonly used web software such as web browsers google chrome 2018 free and commercial engineering software like autocad autodeskinc 2015 quantum gis qgis qgis development team 2021 as well as for word processors and video editing software although plugins are not required for the use of the original application third party users are highly encouraged to develop new features which can reduce the burden of the main developers within the framework above many plugins have been developed especially the ones for geosciences modelling based on qgis platform nielsen et al 2017 criollo et al 2019 ellsäßer et al 2020 among them some plugins can be used for natural hazards spatial analysis and modelling such as qvast qgis for volcanic susceptibility bartolini et al 2013 susceptibility zoning plugin titti and sarretta 2020 and lagrisu landslide grid and slope units althuwaynee 2021 however these plugins are mainly used for pre processing of landslide mapping e g lagrisu is used for landslide samples extraction in qgis or apply statistically based methods to perform landslide susceptibility mapping e g the susceptibility zoning plugin applies the frequency ratio and weight of evidence methods a plugin based on physical model for regional landslide susceptibility assessment is not available so far hence to contribute to filling this gap we developed a python qgis plugin for the newest version of the fslam code the main objectives of the present work are i presenting and describing in detail a plugin developed in pyqgis based on the fslam code medina et al 2021 ii showing and describing all the outputs associated with the newest version of fslam including 15 raster maps and 6 text files iii applying the fslam plugin to the study area berguedà spain to assess the landslide susceptibility and present the whole application of the plugin in a scientific context 2 methods 2 1 fslam model description there are two main parts included in the fslam model one is the stability modelling where slope stability is computed using two hydrological approaches and the infinite slope theory the other one is the runoff modelling which calculates the peak water discharge in each cell of the study area the main aspects of both of them are explained in the sections below while further details on the previous version of the stability calculation can be found in medina et al 2021 2 1 1 stability modelling the stability modelling includes two different sub models namely the geotechnical model and the hydrological model medina et al 2021 the former employs the infinite slope theory to calculate the slope stability lambe and whitman 1979 pack et al 1998 where the factor of safety fs can be computed by 1 f s c r c s g ρ s z cos θ sin θ 1 h z ρ w ρ s tan φ tan θ where c r kpa is the root cohesion from the vegetation c s kpa is the effective cohesion of soil g m s2 is the gravity ρ s kg m3 is the density of the saturated soil z m is the soil depth h m is the position of the water table θ is the terrain slope ρ w kg m3 is the density of water φ is internal friction angle in the next step the hydrological model is used to compute the rainfall infiltration into the soil layer and the position of the water table the fslam model integrates lateral flow and vertical flow to achieve this task first the lateral flow method is used to determine the increase of the water table h a unit m related to the effective antecedent water recharge q a unit mm d associated to the rainfall occurred in the period preceding the landslides the q a is defined as a reduced percentage of the precipitation due to the runoff and evapotranspiration and can also be considered as the effective water infiltration into the soil layer medina et al 2021 it provides the initial condition for landsliding it should be noted that in medina et al 2021 the antecedent water recharge was called antecedent rainfall p a unit mm however this term was confusing and thus we renamed it both in the present paper and the qgis plugin then the vertical flow method is applied to determine the increase of the water table h e unit m associated with the event rainfall hence the final position of the water table is the sum of the two increases of the water table the equation to obtain h a is expressed by 2 h a a b q a k s i n θ c o s θ ρ w ρ s where a m2 is the drainage area b m is the cell size q a mm d is the effective antecedent water recharge k m s is the horizontal hydraulic conductivity montgomery and dietrich 1994a the equation to calculate h e m is as following 3 h e p e n p e 5080 c n 51 2 n p e 4 5080 c n 51 where p e mm is the event rainfall which represents the boundary condition for landsliding n is the soil porosity and cn the curve number proposed in the event oriented scs cn model usda 1986 scs cn is a hydrological model developed by the united states department of agriculture usda and it is one of the simplest and most successful models in computing event oriented runoff cn is an empirical parameter used in the scs cn model for predicting direct runoff or infiltration from rainfall excess and it only relates the runoff threshold and the soil water storage usda 1986 to remedy the issue regarding the uncertainty of soil property values stochastic models simoni et al 2008 have been proposed for the fslam which allow the input parameters to have a statistical distribution within a range of possible values hence when the inputs are in the form of stochastic parameters within a specific range it is possible to compute the probability of failure pof at each cell which corresponds to the probability of having a fs value lower than 1 herein the parameters that can have stochastic inputs include two soil properties c s and φ and root cohesion c r related with land use and land cover lulc medina et al 2021 it must be stated that one of the limitations in the used stochastic model is using a normal distribution for the soil mechanical properties however beyond this limitation another crucial assumption in simoni et al 2008 is that the variables are independent this permits the problem to be analytically tractable which is important for a simplified model 2 1 2 runoff modelling the runoff module can be used to calculate the peak discharge at each cell following a rainfall event for the plugin this variable is not yet used to compute slope stability but just to calibrate the parameters by comparing its value to the observed hydrographs during the event on the other hand there are several mass movements related to in channel processes where the discharge is one of the main triggering mechanisms hence in a near future it is possible to include these processes into the model the curve number which has been widely applied in hydrology is the only parameter that is required in this phase yu 1998 woodward et al 2002 mishra and singh 2013 the approach to compute the peak discharge is the rational method which requires the tributary area the runoff coefficient and the rainfall intensity chow et al 1988 the tributary area means the total area upstream of a specified point including all overland flow that directly or indirectly connects down slope to this point and it is calculated by applying a d8 cumflow algorithm o callaghan and mark 1984 the runoff coefficient c is computed using the following equation témez 1991 4 c p e i a p e 23 i a p e 11 i a 2 where i a mm is the initial abstraction computed from the cn then the rainfall intensity is computed using the spanish intensity duration frequency idf curves témez 1978 5 i i d 11 28 0 1 t c 0 1 28 0 1 1 where i is the rainfall intensity mm d i d is the daily rainfall intensity mm h measured as millimeters per hour and t c is the concentration time h which is expressed by 6 t c 0 3 l j 0 25 0 76 where l km is the longest distance from the watershed divide to the outlet and j m m is the average slope by using the so called cn method above the fslam plugin simplifies the event rainfall into two parts namely the runoff and infiltration the runoff calculation can be used for the calibration of the cn whereas the infiltration is finally incorporated in the stability calculation 2 2 software design 2 2 1 general overview a graphical user interface gui was developed for the newest version of fslam code in order to avoid the manual execution of the software to facilitate this task the well known open source software qgis was selected to implement fslam as a plugin qgis development team 2021 because geographic information systems gis are the most common tools for the analysis of spatial data like landslide assessment the plugin can be downloaded by the users and even be improved including new functions beyond the current abilities an application named qt creator https www qt io based on python language https www python org was selected to design and develop the interface of the fslam plugin qt is a software development framework that can be used to develop applications on various operating systems the gui connects the user with the different parts of the fslam plugin fig 1 the three principal parts are i input data ii fslam model and iii output data details on the input and output data will be given later but summarising the input data include five raster and two text files while the outcomes consist of 15 raster maps and 6 text files in addition the users can subsequently perform additional external analyses e g the performance analysis by an existing landslide inventory regarding the part of the fslam model there are two main calculations namely runoff and stability in the runoff modelling fslam starts with morphologic standard geoprocesses fill sinks slope calculation and flow accumulation among others and then calculates the infiltration and runoff regarding the stability modelling the code mainly determines the following three conditions i preliminary stability ii initial stability considering the effective antecedent recharge and iii final stability after event rainfall it should be stated that new output files were added in the stability modelling of the plugin in respect to the original model presented by medina et al 2021 these additional outputs improve the understanding of the infiltration and stability processes 2 2 2 computational requirements of fslam the code of the fslam model is developed in fortran 90 language focusing on code performance it is proposed as a model able to work at a regional scale but using small cell size the requirements in terms of ram memory are high by the code being the flow accumulation geoprocess the most demanding algorithm taking the case with 30 million cells as an example the raster of the digital elevation model dem is stored in a 64 bits float number hence a 30 million dem requires 458 mb of ram the flow accumulation algorithm multiplies this requirement eight times resulting in up to 4 gb ram additional storage is required for other float inputs such as the rainfall raster slopes etc which raises this value by 8 gb of ram land use and soils properties are stored as unsigned bytes in order to reduce the memory space but limiting the number of classes to 256 the 32 bits applications are limited to 2 gb of ram hence the code is developed in 64 bits none os specific functions are used therefore the code could be compiled in any platform most of the algebraic operations take advantage of the vector parallelization provided by the compilers at instruction level for iterative subroutines openmp has been selected to explicitly parallelize algorithms no mpi instructions have been used hence it is not possible to take advantage of the hpc clusters 2 3 application and testing the berguedà region of spain was selected as a test area in this study in order to show the application of the fslam plugin this area has received an important interest after the catastrophic 1982 rainfall episode which caused the most notable multiple occurrence of regional landslide events morle in eastern pyrenees this fact together with the availability of a landslide inventory and rainfall data make this case ideal for our purpose 2 3 1 study area and 1982 rainfall episode the berguedà study area is located in the southern side of the oriental pre pyrenees ne spain fig 2 and covers an area of 504 8 km2 the region comprises the upper llobregat river ulr basin down to the la baells water reservoir which was inaugurated in 1976 geologically the bedrock of the ulr basin consists of sedimentary rocks including limestones conglomerates mudstones sandstones and turbidites corominas and moya 1999 bathurst et al 2006 besides superficial alluvial and colluvial deposits of variable thickness covering bedrock formations are also found from a morphological point of view the catchment elevation ranges from 622 m a s l up to 2500 m a s l at its highest point the slope angles are mainly between 20 and 40 being the inclination where more mass movements took place during the 1982 rainfall episode corominas and alonso 1990 the berguedà region has a mediterranean climate which is characterized by mild and wet winters along with hot and dry summers millán et al 1995 giorgi and lionello 2008 however as other mountainous regions in the world the orography and the south facing location of the study area introduce changes in the typical seasonal precipitation regimes trapero et al 2013 in the pyrenees the collision between warm air masses coming from the south and cold air fronts located beyond the mountain ranges can cause heavy precipitation especially in the autumn when the mediterranean sea has warm temperature corominas and alonso 1990 if the total accumulated rainfall is equal to or higher than 80 mm they are considered torrential rains millán et al 1995 and they can also be related to the occurrence of landslides corominas and moya 1999 from november 6 to 7 1982 one of the most catastrophic and exceptional rainfall episode for the eastern pyrenees in the 20th century was recorded a strong storm affected spain andorra and france reaching more than 400 mm of accumulative rainfall in a period of 24 h the consequences of this extreme weather episode were devastating it triggered many slope instabilities and flash floods causing 44 fatalities and considerable economic losses corominas and alonso 1990 trapero et al 2013 the berguedà region was severely affected by this rainfall episode and a large number of slope failures were triggered in particular in a study area of 1250 km2 more than 1800 mass movements were reported clotet and gallart 1984 including shallow slides debris flows slumps and rock falls this number corresponds to a density of 1 5 movements per square kilometre the area with a very high density of shallow landslides and debris flows was around vallcebre and la pobla de lillet where a total of 340 mm in 48 h was registered corominas and moya 1999 bathurst et al 2006 the inventory that we used in this study includes shallow slides and debris flows associated with this catastrophic rainfall event we merged inventories of previous studies clotet and gallart 1984 baeza 1994 santacana 2001 where the employed methods were mainly photointerpretation with aerial photographs and fieldwork the total number of landslides in the final inventory was 998 fig 2 among which 11 points were inventoried by baeza 1994 157 by clotet and gallart 1984 595 by santacana 2001 and 235 by the cartographic and geological institute of catalonia icgc 2021 2 3 2 available geospatial data the fslam code applies the infinite slope theory and thus needs in every cell to solve the equations described in section 2 1 these parameters which are principally related to soil and lulc properties are included in the five raster and two csv files that are required as the input data of the plugin fig 1 the first input raster is the dem fig 3 a which has continuous data type it was downloaded from icgc 2013 with a spatial resolution of 5 m in the next step the information on soil properties was approximated by the geological map at 1 50000 scale as no geotechnical map was available the geological map was downloaded from icgc 2016 as a vector shapefile then it was reclassified into thirteen lithological classes and transformed into a raster file with a 5 m resolution fig 3b given that the rainfall episode occurred in 1982 we used the oldest version of lulc map that exists from the study area corresponding to the year 1987 a raster map was downloaded from the department of territory and sustainability with a spatial resolution of 30 m icgc 2018 the original map had twelve classes that were reclassified into eight categories fig 3c by combining some similar categories and the resolution was resampled from 30 m into 5 m it should be noted that the values in lulc and soil rasters are discrete data types which is different from the dem raster different lulc and soil maps may have various categories and users are allowed to determine the exact categories in lulc and soil rasters according to the situation of their study areas and data availability for all the parameters regarding soil properties and root cohesion required by the plugin their initial values were selected by using standard literature site specific data and expert criteria usda 1986 baeza 1994 ecorisq 2021 geotechdata 2021 then multiple fslam simulations were iteratively performed to fit the landslide episode that occurred in the berguedà area in 1982 this step focussed mainly on the three most significant parameters of the fslam model c r c s and φ medina et al 2021 and calibrated them in addition the cn values were initially determined according to the ranges proposed by the catalan water agency and the usda usda 1986 montalbán et al 2013 subsequently these values were calibrated by the runoff modelling where an iterative approach was also applied therefore it is strongly recommended having an available landslide inventory and a known peak discharge data during the calibration procedure is also very useful the calibrated parameters will be the best fit values after the iterative fslam simulations it must be stated that the plugin can also be used when no peak discharge data is available since the runoff module is separated from the stability modelling the final values of all the parameters obtained after the calibration phase for the berguedà region are listed in table 1 and table 2 in terms of precipitation two raster files were used one for the effective antecedent water recharge and another for the event rainfall in general the former can be determined by the regional average antecedent e g monthly rainfall and water balance data the latter indicates the boundary condition for landsliding so it can be obtained from the storm event that triggers the landslide episode in this study the effective antecedent recharge map fig 3d was determined using the official monthly precipitation maps published in the catalan climatic atlas gencat 2008 the first five days of november until the beginning of the triggering rainfall episode on november 6 no precipitation was measured in the berguedà area therefore the average monthly precipitation amount for october was used to determine the effective recharge this map was adjusted by the precipitation observed during october 1982 at meteorological stations of the study area on the other side the event precipitation map was obtained by combining the isohyetal maps published by hürlimann et al 2003 and bathurst et al 2006 for the november 1982 event also the simulation of a precipitation map of the 1982 rainfall episode carried out by trapero et al 2013 was included then the isohyetal map was adjusted using daily precipitation data that was recorded by the spanish meteorological agency inm of the meteorological stations within the study area finally the two maps were converted into the two necessary raster files with 5 m resolution fig 3e it is important to stress that we are trying to reproduce a real case study but the plugin could be easily used to try hypothetical scenarios e g different rainfall events hence it is possible to figure out the robustness of the model in the future since it can be expected that users would like to apply the plugin using a variety of different cases 2 3 3 analysis of the landslide inventory in order to reveal the main factors affecting slope failures in the study area the relationship between landslide and different governing factors was analysed including elevation slope angle lithological class and lulc the slope map was derived from the dem raster with 5 m resolution we counted the total number of landslides in each category and also computed the density of landslides number of landslide inventory points per square kilometre fig 4 the analysis of the morphologic factors showed that the landslides mainly occurred at low medium elevations fig 4a the elevations between 800 and 1600 m asl identify most landslides in the region and also have the peak of density of landslides a majority of landslides started at the slope angles ranging from 25 to 40 whereas the areas with slope angles between 35 and 45 have the peak of density fig 4b the similar finding has also been revealed in other previous studies hürlimann et al 2016 shu et al 2019 namely slope angle plays the most important role in landslides in the pyrenees regarding the lithological class fig 4c the category of mudstone garumnian eocene and keuper contributes most to the landslide occurrence followed by limestone and mudstone conglomerate sandstone and shale alluvium and limestone except these classes the numbers of landslides in the other classes were much smaller finally the effect of lulc on landslide initiation fig 4d indicated that nearly 90 of the landslides started in forest shrubs and grassland additionally the category of bedrock is another significant lulc category with a large value of density overall all these factors mentioned above have contribution to landslide initiation in the berguedà region because the spatial landslide distributions in different categories of each factor show evident difference therefore it is necessary to incorporate this information into the model 2 4 performance analysis of landslide susceptibility mapping performance analysis is not a built in function of the fslam plugin but can provide insights into the results of the landslide susceptibility mapping in this study three methods were applied to evaluate the performance of the landslide susceptibility map generated by the fslam plugin first 5000 points were randomly selected within the study area and their pof values were compared with the landslide inventory points in the region three rainfall scenarios were considered namely no rainfall only q a and q a and p e the comparison of pof values between inventory and random points can reveal the effects of rainfall conditions on the slope stability second the receiver operating characteristic roc analysis fawcett 2006 was used to evaluate the model accuracy the roc curve is a widely used approach which can display the results of a binary classification between the outcomes of a predictive model and the actual occurrence of the forecasted event corsini and mulas 2017 here the area under the curve auc is used to indicate the model performance and a higher auc value represents a better result when a physically based model is applied to evaluate the slope stability it is necessary to appropriately determine a threshold for the safety level of the slope hence in the last step the confusion matrix under a specific classification threshold was provided to understand the model performance according to previous literature silva et al 2008 park et al 2013 lee and park 2016 the pof of 0 1 was selected as the threshold for safety level of natural slopes in probabilistic analysis of this study different indexes were counted including true positive tp true negative tn false positive fp false negative fn true positive rate tpr false positive rate fpr and accuracy acc the relative trade off between tpr and fpr was also plotted into the roc curve to calculate the distance from the model result to the perfect classification the so called perfect classification would be located at the upper left of the roc curve with the coordinate of 0 1 park et al 2013 3 results 3 1 plugin implementation results 3 1 1 plugin overview after downloading the plugin the whole plugin folder should be copied to the installation directory of qgis plugins before it can be used the installation process is swift followed by the manual activation by using the manage and install plugins window under the plugins menu in qgis then the fslam plugin can be launched from its toolbar within qgis or it can be directly opened by the icon on the menu once the plugin is executed the graphical user interface gui of the fslam plugin appears fig 5 which includes four tab widgets named inputs outputs runoff outputs stability and info the input and output data are determined in the first three tabs and the last one shows the plugin information the model description logo and references used in the plugin more details on the requirements and limitations related to these tabs are described in continuation 3 1 2 input data in the inputs tab it is required to load all the input files for the fslam model including five ascii raster files and two csv text files fig 1 the raster files are the digital elevation model the soil properties the land use and land cover the effective antecedent recharge and the event rainfall three of them are continuous raster files and have units namely dem in m effective recharge in mm d and event rainfall in mm the other two raster files soil and lulc are categorical and theirs positive integer values are linked to the two csv text files that include the parameter values of each specific soil or lulc category specifically for each identified category in lulc and soil rasters the algorithm built in the plugin seeks the parameters that are used for the model calculation in the corresponding text files next these parameters will be linked into the rasters to carry out modelling the first text file includes the six soil properties soil cohesion friction angle soil depth hydraulic conductivity porosity and density and the hydrologic soil group hsg usda 1986 2007 for each soil class the second file describes the hydrological mechanical terrain unit hmtu and provides the values of root cohesion and curve number cn for each lulc category all the parameters in these two text files can be stochastic when more than two stochastic parameters are involved in the analysis the monte carlo simulation may be a better option for the probabilistic analysis since it can consider many other probability distribution functions but it would strongly increase the computational time simoni et al 2008 medina et al 2021 additionally it should be noted that all the raster maps must have the same format spatial extent pixel size coordinate system and no data value hence it is highly recommended to align all the raster files by using the align raster tool in qgis before using them as inputs based on our experiences no data should be determined as a specific value which must be different from the other values in the input rasters e g 9999 qgis tool can be used to reproject to a different coordinate system change the no data value or to resample pixel size however the resolution of the original raster will impact on the results as well as the uncertainty of the input data will change with pixel size 3 1 3 output data the output data of the plugin can be selected in the outputs runoff and outputs stability tabs only one raster file can be obtained from the tab of outputs runoff named runoff asc which shows the peak discharge in each cell the stability modelling offers 14 raster files regarding multiple data for analysis for the raster files in these two tabs every item is optional but at least one file should be selected as the output otherwise the model runs but no output data are given additionally for the first six checkboxes in the outputs stability tab an extra csv file will be generated for each of the selected option these text files contain data on the cumulative distribution function cdf of the resulting raster files the name of the csv files is the same as the corresponding raster file followed by the suffix cdf all the output files raster and text files related to the tab of outputs stability are listed in table 3 users can assign any existing folder in their computer as the path to store the output files all the selected output raster files as well as the corresponding csv files will show up in this output folder additionally a text file named filenames txt that contains the path of all input files is created which can be used for future consultations of the simulations performed in the following the 14 outputs regarding the stability calculations will be described the first two outputs are related to the probability of failure pof if the first checkbox pof after effective antecedent recharge and event rainfall is selected the plugin computes the raster map called prob failure final cond asc which includes the pof value at each cell after applying q a and p e whereas the pof value after q a can be generated by using the checkbox of pof after effective antecedent recharge both maps have a range from 0 to 1 the effect of the event rainfall on slope stability can be identified by comparing the two maps meanwhile the pof map after q a can indicate the impact of the effective antecedent recharge on slope stability when compared with the pof map under dry conditions the following two outputs calculate the factor of safety fs under the same conditions as for pof i applying q a and p e and ii applying only q a the values in both maps are larger than 0 and the two maps can be used to check unstable areas with fs values less than 1 the next two checkboxes related to definitions proposed by montgomery and dietrich 1994b the checkbox unconditionally stable cells shows the stability condition of each cell when the soil is completely saturated specifically it indicates the probability of each cell of being unconditionally stable pous hence mathematically the value equals to the difference between 1 and the pof under saturated condition if some landslide points with high pous values are located in the area it means that the soil properties must be checked because even with fully saturated soil the soil will be stable the checkbox of unconditionally unstable cells indicates the pof at each cell when soil is completely dry it can also be explained by the probability of being unconditionally unstable pouu of each cell if no landslide points in the area with high pouu values the areas where the landslide points start may have too large soil property values thus the soil properties must be checked in this condition in areas where slopes are very steep it could happen that values of pouu are inevitably very high this could only be avoided by giving soil properties of bedrock instead of soil the following two outputs are associated with the saturation and infiltration the checkbox saturation degree after effective antecedent recharge ha z computes the raster file named h z ant cond asc which shows the position of the water table in relation to the depth of the soil layer after the effective antecedent recharge it equals to the ratio between the increase of the water table associated with the effective antecedent recharge h a and the soil depth z the raster file named infiltration asc is generated from the checkbox part of pe that infiltrates and it shows the amount of event rainfall which infiltrates into the soil layer the remaining part of the event rainfall is surface runoff which results in the peak water discharge the fslam plugin also provides topographic outputs by the checkbox 3 additional topographic rasters fillsinks slope flow accumulation these outputs include i the dem with the sinks filled fill asc ii slope angle map derived from the dem slope asc and iii the map showing the drainage area at each cell cumflow asc finally three additional outputs can be obtained by selecting the checkbox extra raster files of the 3 terms defined in eq 9 of medina et al 2021 these three raster files are containing data on the initial stability conditions e g slope cohesion thickness etc and the influence of q a and p e on the slope stability the contributions of both effective recharge and event rainfall to fs are not larger than 0 while that of the parameters under dry conditions is not less than 0 the equations of these three contributions are given in eq 9 of medina et al 2021 the path for the outputs can be set by clicking the combo box above the run button 3 2 testing results 3 2 1 computing time all the raster files were prepared at a 5 m resolution which gives a total of about 30 million cells for the entire berguedà study area simulations showed that the stability calculation of the fslam plugin only lasted approximately 4 min for the entire region however a comparison of computing time showed that the writing of rasters files lasted much longer than the stability calculations hence the entire run of the plugin including the writing of all the raster files lasted approximately 80 min this leads us to conclude that it is important to determine which outputs are required for the analysis before applying the plugin since this choice can strongly reduce the running time of the plugin all the calculations above were performed by a computer with one 8 cores 1 8 ghz cpu and 8 gb of ram 3 2 2 output results when selecting all the checkboxes in the gui the plugin will write fifteen raster maps and six csv text files in the specified result folder in the following the raster maps are divided into three groups namely hydrological maps fs related maps and pof related maps the only two maps that will not be shown are the filled dem fill asc and slope map slope asc since the slope map has already been presented in fig 2 as stated above the runoff map fig 6 a was used to calibrate the curve number by comparing the observed and computed peak discharge the observed peak discharge into the la baells reservoir was estimated as 1300 m3 s c barbero catalan water agency personal communication march 15 2021 whereas the simulated runoff by the plugin was 1394 m3 s fig 6b the error between simulated and actual values of runoff was only approximately 7 which is satisfactory for such a simplified approach and the existing uncertainties of p e and cn however this runoff modelling indicates that the selected cn values table 1 fit well for hydrologic conditions of the 1982 rainfall episode in addition to the runoff map the first group of outputs of the fslam plugin includes the flow accumulation map infiltration map and relative saturation degree map they are all hydrology related maps and are useful to understand the infiltration processes during a rainfall episode the flow accumulation map cumflow asc fig 7 a is not only directly related to runoff but also to h a the flow accumulation values range up to 107 m2 in the berguedà area we can see that ridgelines mostly have smallest values and maximum values are mainly located at valley floors where main river are situated the infiltration map infiltration asc fig 7b shows the potential amount of the event rainfall which may infiltrate into the soil in the berguedà area it ranges from 0 to 255 mm relatively large values of infiltration more than 170 mm mainly occur in the lithological classes with high porosity on the contrary the lithological classes with low porosity for example mudstones have small infiltration the map showing the relative saturation degree applying the effective antecedent recharge h z ant cond asc is related to the flow accumulation and the hydraulic conductivity fig 7c because the hydraulic conductivity is linked to the lithology high percentages of relative saturation degree more than 0 75 mainly occur in mudstones where k values are low on the contrary conglomerate sandstone and alluvial lithologies are less saturated due to their relatively high hydraulic conductivity the second group of outputs is composed of five maps related to the factor of safety fs and generated from the stability modelling they reflect the stability conditions in the study area under different rainfall scenarios using the mean parameter value in the range of the stochastic model the raster maps named fs ant cond asc fig 8 a and fs final cond asc fig 8b show a clear correlation between the slope angle and the stability since in those areas with smooth slopes the safety factor is evidently higher when only the effective antecedent recharge is considered most zones in the study area are stable especially in the areas with a low saturation degree of the soil layer see fig 7c the specific effects of the two rainfalls and the soil properties under dry conditions can be identified and compared using the other three maps in this group the contribution of the effective recharge fig 8c is closely related to the hydraulic conductivity and thus with the lithological classes in a similar way to the saturation degree map those areas with mudstone type lithology are mostly affected by the effective antecedent recharge in other areas the decrease of fs is associated with the infiltration of the event rainfall fig 8d rainfall conditions can decrease the slope stability so the contributions of both q a fig 8c and p e fig 8c to fs are negative values the third group of output files are pof related maps and show the probability of failure for different rainfall and soil saturation scenarios they are key outputs of the fslam plugin as they show those areas most likely to generate landslides and therefore they allow the users to anticipate and perform a good risk management overall the study area is stable when only the effective antecedent recharge is applied fig 9 a most areas with pof values larger than 0 75 occur in the garumnian and eocene mudstones after the event rainfall the stability condition in many areas greatly decreases fig 9b this indicates that the input of the event rainfall makes these cells to reach the critical stability conditions the pous values under totally saturated condition have a high relationship with slope angle in the study area fig 9c in comparison to steep slopes it is evident that relatively flat areas have larger probability to be unconditionally stable for example the valley floors generally have high pous values which indicates that the pof at these areas is low or even negligible on the contrary the value of 0 i e pof 1 reveals those areas that have the highest landslide susceptibility under totally saturated condition many landslide inventory points are located in such areas indicating that the soil properties used are appropriate regarding the pouu map under dry conditions fig 9d it may have minor importance because most of the areas in berguedà have low values to properly distinguish these two maps it is important to understand that an equal value on both maps represents totally opposite meanings for example pous 1 stands for a cell that is always stable even in totally saturated condition while pouu 1 means that the cell is always unstable even without any rainfall in addition to the maps showed above the fslam plugin also calculates csv text files outputs they can be simply presented as curves plots which should help the interpretation of the maps fig 10 we have grouped the curves with the same topic into one plot in order to be able to better compare them considering that the range of fs at regional scale may be very large the csv files only lists the cumulative distribution function cdf when fs is not larger than 1 the comparison under different rainfall conditions fig 10a reveals the negative effect of the event rainfall on slope stability for instance after the q a only 17 cells are unstable fs 1 but the percentage increases to 27 after applying q a and p e additionally regarding the cdf curves of the pof values under different rainfall conditions fig 10b and c the overall stability condition can be revealed and compared by computing the area under the cdf curve auc cdf the larger the auc cdf is the more stable the study area is from an overall point of view finally it should be mentioned that the probability density function pdf can be computed from the cdf which can show the percentage of cells corresponding to different pof values 3 2 3 performance analysis and scientific interpretation in this section we switch to a supplementary external analysis of the plugin see flowchart in fig 1 using the existing landslide inventory and present the example of a performance analysis for the landslide susceptibility map such a performance analysis is a standard part of landslide susceptibility assessment the percentage of points versus the pof fig 11 a showed the evident negative effects of q a and p e on landslide points before applying rainfall most of the landslide points had a pof less than 0 2 after q a and p e more than 45 points had a pof larger than 0 8 on the contrary the pof change of the random points was very slight before the rainfall 9 points had pof larger than 0 8 and this ratio only increased by 10 after q a and p e the model accuracy was also clarified by the roc analysis results fig 11b the area under the curve was 0 69 when using the landslide points for analysis after adding a buffer zone around the landslide points due to the uncertainty on the landslide inventory points location the auc value improved evidently when the buffer zone had a radius of 25 m the auc reached to 0 89 at its highest value which was satisfactory considering that the landslide inventory of the 1982 episode has a rather low precision applying a buffer is justified and the resulting accuracy is acceptable the results regarding the confusion matrix table 4 showed that by using the 0 1 as the pof threshold the tpr was 0 797 when no buffer zone is applied around the landslide points the distance to the perfect classification was 0 647 in this condition when the radius of the buffer zone increased the tpr increased whereas the distance to the perfect classification decreased similarly the accuracy increased from 0 647 no buffer to 0 737 25 m buffer all these indexes indicate that the landslide susceptibility map captures most of the landslide occurrences in the region and the result performance is good however it should be also noted that the fpr values didn t change in different conditions so false positives should be also paid attention in the final application of the plugin especially employing it for early warning systems 4 conclusions in the present study a python qgis plugin named fslam is introduced it allows to compute regional shallow landslide susceptibility combining the effect of antecedent and event rainfall conditions through gui based workflow in qgis an additional runoff module can also be used to obtain the peak discharge the plugin outputs involve multiple data including the raster maps regarding topography rainfall infiltration saturated degree of soil layer factor of safety and probability of failure which can be selected individually by the users the plugin was tested in the area of berguedà spain by analysing the landslide events triggered by 1982 rainfall episode the results showed that the computational time for the area containing in total 30 million pixels only lasted 5 min the accuracy of the landslide susceptibility map reached 89 when a 25 m radius buffer was added around the landslide initiation points indicating a satisfactory performance of the plugin the current version of the plugin has been released in github repository allowing automatic downloading and further extension and adaption future possible incorporation into the fslam plugin may focus on auto calibration routines and an additional module which would directly show some results about the performance e g comparison between the landslide inventory and random points and the calculation of roc scores 5 software availability name of software fslam software version 1 0 developers zizheng guo vicente medina software license the gnu general public license v3 0 contact address division of geotechnical engineering and geosciences department of civil and environmental engineering barcelonatech upc jordi girona 1 3 d2 08034 barcelona spain email cuggzz cug edu cn availability https github com engeomodels fslam plugin declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was partly funded by the spanish national projects smucphy and eroslop bia 2015 67500 r and pid2019 104266rb i00 aei 10 13039 501100011033 zizheng guo acknowledges the financial support of china scholarship council for his research at upc barcelonatech fundamental research funds for national universities china university of geosciences wuhan and china shaanxi province key research program no 2019zdlsf07 07 02 
25621,shallow slope failures triggered by rainfall commonly pose considerable risks in mountainous areas in order to delineate areas where landslides are more prone to occur within a region we have designed and developed a python qgis plugin named fast shallow landslide assessment model fslam the plugin integrates a simplified hydrological model and a geotechnical model based on the infinite slope theory and contains two principal modules runoff and slope stability modelling it can output up to 15 raster maps describing the hydrological and stability conditions in a short computational time firstly we explain the design of graphical user interface and the elements of the plugin then the berguedà area in ne spain is used as case study to present the procedure of the plugin application the results show that the accuracy of landslide susceptibility assessment performed by fslam plugin is high and the computing time is only a few minutes keywords landslide susceptibility python qgis plugin rainfall runoff 1 introduction shallow landslides induced by heavy rainfall are a global issue which are posing severe implications for mountain environments baum and godt 2010 froude and petley 2018 for scientists and stakeholders dealing with shallow slope failures accurate landslide susceptibility maps at regional scale are necessary information they can identify the areas that are spatially prone to landslides guzzetti et al 2005 hence landslide susceptibility assessment is considered as a key tool for the understanding of the spatial distribution of landslides and can help decision makers on the design of landslide risk reduction strategy guzzetti et al 1999 fell et al 2008 landslide susceptibility models can roughly be divided into heuristic models physically based models statistical models and machine learning models fell et al 2008 zêzere et al 2017 broeckx et al 2018 reichenbach et al 2018 merghadi et al 2020 among them physically based models incorporate soil properties into the susceptibility analysis as they are normally based on geotechnical slope stability equations driven by hydrologic inputs wang et al 2019 on the contrary the other models are typically independent from the physical processes of landslides initiation and runout strauch et al 2019 physically based models therefore have advantages in helping to understand the mechanisms of slope failures especially regarding the effects of rainfall although physically based models for landslide susceptibility assessment are popular e g montgomery and dietrich 1994a rigon et al 2006 baum et al 2008 lehmann and or 2012 mergili et al 2014 each of them has its particular drawbacks which can be grouped in two main groups i large uncertainty associated with soil properties it may strongly influence the model performance due to the inherent variability of input parameters tofani et al 2017 ii high computational cost as a result of the incorporation of a comprehensive approach for the rainfall infiltration in unsaturated soil iverson 2000 model calculations can last from several hours to even several days e g rossi et al 2013 to overcome these limitations an open source code named fast shallow landslide assessment model fslam has been proposed medina et al 2021 it does not only allow using soil properties as stochastic parameters but it can also obtain landslide susceptibility maps of large regions within a few minutes however the fslam code was initially not implemented into a plugin and did not have a graphical user interface gui which made it difficult to use plugins are an essential component of software which can extend the capabilities of an already existing software as well as improve the user experience by providing more user friendly gui without affecting the source code sela et al 2019 plugins are available for many commonly used web software such as web browsers google chrome 2018 free and commercial engineering software like autocad autodeskinc 2015 quantum gis qgis qgis development team 2021 as well as for word processors and video editing software although plugins are not required for the use of the original application third party users are highly encouraged to develop new features which can reduce the burden of the main developers within the framework above many plugins have been developed especially the ones for geosciences modelling based on qgis platform nielsen et al 2017 criollo et al 2019 ellsäßer et al 2020 among them some plugins can be used for natural hazards spatial analysis and modelling such as qvast qgis for volcanic susceptibility bartolini et al 2013 susceptibility zoning plugin titti and sarretta 2020 and lagrisu landslide grid and slope units althuwaynee 2021 however these plugins are mainly used for pre processing of landslide mapping e g lagrisu is used for landslide samples extraction in qgis or apply statistically based methods to perform landslide susceptibility mapping e g the susceptibility zoning plugin applies the frequency ratio and weight of evidence methods a plugin based on physical model for regional landslide susceptibility assessment is not available so far hence to contribute to filling this gap we developed a python qgis plugin for the newest version of the fslam code the main objectives of the present work are i presenting and describing in detail a plugin developed in pyqgis based on the fslam code medina et al 2021 ii showing and describing all the outputs associated with the newest version of fslam including 15 raster maps and 6 text files iii applying the fslam plugin to the study area berguedà spain to assess the landslide susceptibility and present the whole application of the plugin in a scientific context 2 methods 2 1 fslam model description there are two main parts included in the fslam model one is the stability modelling where slope stability is computed using two hydrological approaches and the infinite slope theory the other one is the runoff modelling which calculates the peak water discharge in each cell of the study area the main aspects of both of them are explained in the sections below while further details on the previous version of the stability calculation can be found in medina et al 2021 2 1 1 stability modelling the stability modelling includes two different sub models namely the geotechnical model and the hydrological model medina et al 2021 the former employs the infinite slope theory to calculate the slope stability lambe and whitman 1979 pack et al 1998 where the factor of safety fs can be computed by 1 f s c r c s g ρ s z cos θ sin θ 1 h z ρ w ρ s tan φ tan θ where c r kpa is the root cohesion from the vegetation c s kpa is the effective cohesion of soil g m s2 is the gravity ρ s kg m3 is the density of the saturated soil z m is the soil depth h m is the position of the water table θ is the terrain slope ρ w kg m3 is the density of water φ is internal friction angle in the next step the hydrological model is used to compute the rainfall infiltration into the soil layer and the position of the water table the fslam model integrates lateral flow and vertical flow to achieve this task first the lateral flow method is used to determine the increase of the water table h a unit m related to the effective antecedent water recharge q a unit mm d associated to the rainfall occurred in the period preceding the landslides the q a is defined as a reduced percentage of the precipitation due to the runoff and evapotranspiration and can also be considered as the effective water infiltration into the soil layer medina et al 2021 it provides the initial condition for landsliding it should be noted that in medina et al 2021 the antecedent water recharge was called antecedent rainfall p a unit mm however this term was confusing and thus we renamed it both in the present paper and the qgis plugin then the vertical flow method is applied to determine the increase of the water table h e unit m associated with the event rainfall hence the final position of the water table is the sum of the two increases of the water table the equation to obtain h a is expressed by 2 h a a b q a k s i n θ c o s θ ρ w ρ s where a m2 is the drainage area b m is the cell size q a mm d is the effective antecedent water recharge k m s is the horizontal hydraulic conductivity montgomery and dietrich 1994a the equation to calculate h e m is as following 3 h e p e n p e 5080 c n 51 2 n p e 4 5080 c n 51 where p e mm is the event rainfall which represents the boundary condition for landsliding n is the soil porosity and cn the curve number proposed in the event oriented scs cn model usda 1986 scs cn is a hydrological model developed by the united states department of agriculture usda and it is one of the simplest and most successful models in computing event oriented runoff cn is an empirical parameter used in the scs cn model for predicting direct runoff or infiltration from rainfall excess and it only relates the runoff threshold and the soil water storage usda 1986 to remedy the issue regarding the uncertainty of soil property values stochastic models simoni et al 2008 have been proposed for the fslam which allow the input parameters to have a statistical distribution within a range of possible values hence when the inputs are in the form of stochastic parameters within a specific range it is possible to compute the probability of failure pof at each cell which corresponds to the probability of having a fs value lower than 1 herein the parameters that can have stochastic inputs include two soil properties c s and φ and root cohesion c r related with land use and land cover lulc medina et al 2021 it must be stated that one of the limitations in the used stochastic model is using a normal distribution for the soil mechanical properties however beyond this limitation another crucial assumption in simoni et al 2008 is that the variables are independent this permits the problem to be analytically tractable which is important for a simplified model 2 1 2 runoff modelling the runoff module can be used to calculate the peak discharge at each cell following a rainfall event for the plugin this variable is not yet used to compute slope stability but just to calibrate the parameters by comparing its value to the observed hydrographs during the event on the other hand there are several mass movements related to in channel processes where the discharge is one of the main triggering mechanisms hence in a near future it is possible to include these processes into the model the curve number which has been widely applied in hydrology is the only parameter that is required in this phase yu 1998 woodward et al 2002 mishra and singh 2013 the approach to compute the peak discharge is the rational method which requires the tributary area the runoff coefficient and the rainfall intensity chow et al 1988 the tributary area means the total area upstream of a specified point including all overland flow that directly or indirectly connects down slope to this point and it is calculated by applying a d8 cumflow algorithm o callaghan and mark 1984 the runoff coefficient c is computed using the following equation témez 1991 4 c p e i a p e 23 i a p e 11 i a 2 where i a mm is the initial abstraction computed from the cn then the rainfall intensity is computed using the spanish intensity duration frequency idf curves témez 1978 5 i i d 11 28 0 1 t c 0 1 28 0 1 1 where i is the rainfall intensity mm d i d is the daily rainfall intensity mm h measured as millimeters per hour and t c is the concentration time h which is expressed by 6 t c 0 3 l j 0 25 0 76 where l km is the longest distance from the watershed divide to the outlet and j m m is the average slope by using the so called cn method above the fslam plugin simplifies the event rainfall into two parts namely the runoff and infiltration the runoff calculation can be used for the calibration of the cn whereas the infiltration is finally incorporated in the stability calculation 2 2 software design 2 2 1 general overview a graphical user interface gui was developed for the newest version of fslam code in order to avoid the manual execution of the software to facilitate this task the well known open source software qgis was selected to implement fslam as a plugin qgis development team 2021 because geographic information systems gis are the most common tools for the analysis of spatial data like landslide assessment the plugin can be downloaded by the users and even be improved including new functions beyond the current abilities an application named qt creator https www qt io based on python language https www python org was selected to design and develop the interface of the fslam plugin qt is a software development framework that can be used to develop applications on various operating systems the gui connects the user with the different parts of the fslam plugin fig 1 the three principal parts are i input data ii fslam model and iii output data details on the input and output data will be given later but summarising the input data include five raster and two text files while the outcomes consist of 15 raster maps and 6 text files in addition the users can subsequently perform additional external analyses e g the performance analysis by an existing landslide inventory regarding the part of the fslam model there are two main calculations namely runoff and stability in the runoff modelling fslam starts with morphologic standard geoprocesses fill sinks slope calculation and flow accumulation among others and then calculates the infiltration and runoff regarding the stability modelling the code mainly determines the following three conditions i preliminary stability ii initial stability considering the effective antecedent recharge and iii final stability after event rainfall it should be stated that new output files were added in the stability modelling of the plugin in respect to the original model presented by medina et al 2021 these additional outputs improve the understanding of the infiltration and stability processes 2 2 2 computational requirements of fslam the code of the fslam model is developed in fortran 90 language focusing on code performance it is proposed as a model able to work at a regional scale but using small cell size the requirements in terms of ram memory are high by the code being the flow accumulation geoprocess the most demanding algorithm taking the case with 30 million cells as an example the raster of the digital elevation model dem is stored in a 64 bits float number hence a 30 million dem requires 458 mb of ram the flow accumulation algorithm multiplies this requirement eight times resulting in up to 4 gb ram additional storage is required for other float inputs such as the rainfall raster slopes etc which raises this value by 8 gb of ram land use and soils properties are stored as unsigned bytes in order to reduce the memory space but limiting the number of classes to 256 the 32 bits applications are limited to 2 gb of ram hence the code is developed in 64 bits none os specific functions are used therefore the code could be compiled in any platform most of the algebraic operations take advantage of the vector parallelization provided by the compilers at instruction level for iterative subroutines openmp has been selected to explicitly parallelize algorithms no mpi instructions have been used hence it is not possible to take advantage of the hpc clusters 2 3 application and testing the berguedà region of spain was selected as a test area in this study in order to show the application of the fslam plugin this area has received an important interest after the catastrophic 1982 rainfall episode which caused the most notable multiple occurrence of regional landslide events morle in eastern pyrenees this fact together with the availability of a landslide inventory and rainfall data make this case ideal for our purpose 2 3 1 study area and 1982 rainfall episode the berguedà study area is located in the southern side of the oriental pre pyrenees ne spain fig 2 and covers an area of 504 8 km2 the region comprises the upper llobregat river ulr basin down to the la baells water reservoir which was inaugurated in 1976 geologically the bedrock of the ulr basin consists of sedimentary rocks including limestones conglomerates mudstones sandstones and turbidites corominas and moya 1999 bathurst et al 2006 besides superficial alluvial and colluvial deposits of variable thickness covering bedrock formations are also found from a morphological point of view the catchment elevation ranges from 622 m a s l up to 2500 m a s l at its highest point the slope angles are mainly between 20 and 40 being the inclination where more mass movements took place during the 1982 rainfall episode corominas and alonso 1990 the berguedà region has a mediterranean climate which is characterized by mild and wet winters along with hot and dry summers millán et al 1995 giorgi and lionello 2008 however as other mountainous regions in the world the orography and the south facing location of the study area introduce changes in the typical seasonal precipitation regimes trapero et al 2013 in the pyrenees the collision between warm air masses coming from the south and cold air fronts located beyond the mountain ranges can cause heavy precipitation especially in the autumn when the mediterranean sea has warm temperature corominas and alonso 1990 if the total accumulated rainfall is equal to or higher than 80 mm they are considered torrential rains millán et al 1995 and they can also be related to the occurrence of landslides corominas and moya 1999 from november 6 to 7 1982 one of the most catastrophic and exceptional rainfall episode for the eastern pyrenees in the 20th century was recorded a strong storm affected spain andorra and france reaching more than 400 mm of accumulative rainfall in a period of 24 h the consequences of this extreme weather episode were devastating it triggered many slope instabilities and flash floods causing 44 fatalities and considerable economic losses corominas and alonso 1990 trapero et al 2013 the berguedà region was severely affected by this rainfall episode and a large number of slope failures were triggered in particular in a study area of 1250 km2 more than 1800 mass movements were reported clotet and gallart 1984 including shallow slides debris flows slumps and rock falls this number corresponds to a density of 1 5 movements per square kilometre the area with a very high density of shallow landslides and debris flows was around vallcebre and la pobla de lillet where a total of 340 mm in 48 h was registered corominas and moya 1999 bathurst et al 2006 the inventory that we used in this study includes shallow slides and debris flows associated with this catastrophic rainfall event we merged inventories of previous studies clotet and gallart 1984 baeza 1994 santacana 2001 where the employed methods were mainly photointerpretation with aerial photographs and fieldwork the total number of landslides in the final inventory was 998 fig 2 among which 11 points were inventoried by baeza 1994 157 by clotet and gallart 1984 595 by santacana 2001 and 235 by the cartographic and geological institute of catalonia icgc 2021 2 3 2 available geospatial data the fslam code applies the infinite slope theory and thus needs in every cell to solve the equations described in section 2 1 these parameters which are principally related to soil and lulc properties are included in the five raster and two csv files that are required as the input data of the plugin fig 1 the first input raster is the dem fig 3 a which has continuous data type it was downloaded from icgc 2013 with a spatial resolution of 5 m in the next step the information on soil properties was approximated by the geological map at 1 50000 scale as no geotechnical map was available the geological map was downloaded from icgc 2016 as a vector shapefile then it was reclassified into thirteen lithological classes and transformed into a raster file with a 5 m resolution fig 3b given that the rainfall episode occurred in 1982 we used the oldest version of lulc map that exists from the study area corresponding to the year 1987 a raster map was downloaded from the department of territory and sustainability with a spatial resolution of 30 m icgc 2018 the original map had twelve classes that were reclassified into eight categories fig 3c by combining some similar categories and the resolution was resampled from 30 m into 5 m it should be noted that the values in lulc and soil rasters are discrete data types which is different from the dem raster different lulc and soil maps may have various categories and users are allowed to determine the exact categories in lulc and soil rasters according to the situation of their study areas and data availability for all the parameters regarding soil properties and root cohesion required by the plugin their initial values were selected by using standard literature site specific data and expert criteria usda 1986 baeza 1994 ecorisq 2021 geotechdata 2021 then multiple fslam simulations were iteratively performed to fit the landslide episode that occurred in the berguedà area in 1982 this step focussed mainly on the three most significant parameters of the fslam model c r c s and φ medina et al 2021 and calibrated them in addition the cn values were initially determined according to the ranges proposed by the catalan water agency and the usda usda 1986 montalbán et al 2013 subsequently these values were calibrated by the runoff modelling where an iterative approach was also applied therefore it is strongly recommended having an available landslide inventory and a known peak discharge data during the calibration procedure is also very useful the calibrated parameters will be the best fit values after the iterative fslam simulations it must be stated that the plugin can also be used when no peak discharge data is available since the runoff module is separated from the stability modelling the final values of all the parameters obtained after the calibration phase for the berguedà region are listed in table 1 and table 2 in terms of precipitation two raster files were used one for the effective antecedent water recharge and another for the event rainfall in general the former can be determined by the regional average antecedent e g monthly rainfall and water balance data the latter indicates the boundary condition for landsliding so it can be obtained from the storm event that triggers the landslide episode in this study the effective antecedent recharge map fig 3d was determined using the official monthly precipitation maps published in the catalan climatic atlas gencat 2008 the first five days of november until the beginning of the triggering rainfall episode on november 6 no precipitation was measured in the berguedà area therefore the average monthly precipitation amount for october was used to determine the effective recharge this map was adjusted by the precipitation observed during october 1982 at meteorological stations of the study area on the other side the event precipitation map was obtained by combining the isohyetal maps published by hürlimann et al 2003 and bathurst et al 2006 for the november 1982 event also the simulation of a precipitation map of the 1982 rainfall episode carried out by trapero et al 2013 was included then the isohyetal map was adjusted using daily precipitation data that was recorded by the spanish meteorological agency inm of the meteorological stations within the study area finally the two maps were converted into the two necessary raster files with 5 m resolution fig 3e it is important to stress that we are trying to reproduce a real case study but the plugin could be easily used to try hypothetical scenarios e g different rainfall events hence it is possible to figure out the robustness of the model in the future since it can be expected that users would like to apply the plugin using a variety of different cases 2 3 3 analysis of the landslide inventory in order to reveal the main factors affecting slope failures in the study area the relationship between landslide and different governing factors was analysed including elevation slope angle lithological class and lulc the slope map was derived from the dem raster with 5 m resolution we counted the total number of landslides in each category and also computed the density of landslides number of landslide inventory points per square kilometre fig 4 the analysis of the morphologic factors showed that the landslides mainly occurred at low medium elevations fig 4a the elevations between 800 and 1600 m asl identify most landslides in the region and also have the peak of density of landslides a majority of landslides started at the slope angles ranging from 25 to 40 whereas the areas with slope angles between 35 and 45 have the peak of density fig 4b the similar finding has also been revealed in other previous studies hürlimann et al 2016 shu et al 2019 namely slope angle plays the most important role in landslides in the pyrenees regarding the lithological class fig 4c the category of mudstone garumnian eocene and keuper contributes most to the landslide occurrence followed by limestone and mudstone conglomerate sandstone and shale alluvium and limestone except these classes the numbers of landslides in the other classes were much smaller finally the effect of lulc on landslide initiation fig 4d indicated that nearly 90 of the landslides started in forest shrubs and grassland additionally the category of bedrock is another significant lulc category with a large value of density overall all these factors mentioned above have contribution to landslide initiation in the berguedà region because the spatial landslide distributions in different categories of each factor show evident difference therefore it is necessary to incorporate this information into the model 2 4 performance analysis of landslide susceptibility mapping performance analysis is not a built in function of the fslam plugin but can provide insights into the results of the landslide susceptibility mapping in this study three methods were applied to evaluate the performance of the landslide susceptibility map generated by the fslam plugin first 5000 points were randomly selected within the study area and their pof values were compared with the landslide inventory points in the region three rainfall scenarios were considered namely no rainfall only q a and q a and p e the comparison of pof values between inventory and random points can reveal the effects of rainfall conditions on the slope stability second the receiver operating characteristic roc analysis fawcett 2006 was used to evaluate the model accuracy the roc curve is a widely used approach which can display the results of a binary classification between the outcomes of a predictive model and the actual occurrence of the forecasted event corsini and mulas 2017 here the area under the curve auc is used to indicate the model performance and a higher auc value represents a better result when a physically based model is applied to evaluate the slope stability it is necessary to appropriately determine a threshold for the safety level of the slope hence in the last step the confusion matrix under a specific classification threshold was provided to understand the model performance according to previous literature silva et al 2008 park et al 2013 lee and park 2016 the pof of 0 1 was selected as the threshold for safety level of natural slopes in probabilistic analysis of this study different indexes were counted including true positive tp true negative tn false positive fp false negative fn true positive rate tpr false positive rate fpr and accuracy acc the relative trade off between tpr and fpr was also plotted into the roc curve to calculate the distance from the model result to the perfect classification the so called perfect classification would be located at the upper left of the roc curve with the coordinate of 0 1 park et al 2013 3 results 3 1 plugin implementation results 3 1 1 plugin overview after downloading the plugin the whole plugin folder should be copied to the installation directory of qgis plugins before it can be used the installation process is swift followed by the manual activation by using the manage and install plugins window under the plugins menu in qgis then the fslam plugin can be launched from its toolbar within qgis or it can be directly opened by the icon on the menu once the plugin is executed the graphical user interface gui of the fslam plugin appears fig 5 which includes four tab widgets named inputs outputs runoff outputs stability and info the input and output data are determined in the first three tabs and the last one shows the plugin information the model description logo and references used in the plugin more details on the requirements and limitations related to these tabs are described in continuation 3 1 2 input data in the inputs tab it is required to load all the input files for the fslam model including five ascii raster files and two csv text files fig 1 the raster files are the digital elevation model the soil properties the land use and land cover the effective antecedent recharge and the event rainfall three of them are continuous raster files and have units namely dem in m effective recharge in mm d and event rainfall in mm the other two raster files soil and lulc are categorical and theirs positive integer values are linked to the two csv text files that include the parameter values of each specific soil or lulc category specifically for each identified category in lulc and soil rasters the algorithm built in the plugin seeks the parameters that are used for the model calculation in the corresponding text files next these parameters will be linked into the rasters to carry out modelling the first text file includes the six soil properties soil cohesion friction angle soil depth hydraulic conductivity porosity and density and the hydrologic soil group hsg usda 1986 2007 for each soil class the second file describes the hydrological mechanical terrain unit hmtu and provides the values of root cohesion and curve number cn for each lulc category all the parameters in these two text files can be stochastic when more than two stochastic parameters are involved in the analysis the monte carlo simulation may be a better option for the probabilistic analysis since it can consider many other probability distribution functions but it would strongly increase the computational time simoni et al 2008 medina et al 2021 additionally it should be noted that all the raster maps must have the same format spatial extent pixel size coordinate system and no data value hence it is highly recommended to align all the raster files by using the align raster tool in qgis before using them as inputs based on our experiences no data should be determined as a specific value which must be different from the other values in the input rasters e g 9999 qgis tool can be used to reproject to a different coordinate system change the no data value or to resample pixel size however the resolution of the original raster will impact on the results as well as the uncertainty of the input data will change with pixel size 3 1 3 output data the output data of the plugin can be selected in the outputs runoff and outputs stability tabs only one raster file can be obtained from the tab of outputs runoff named runoff asc which shows the peak discharge in each cell the stability modelling offers 14 raster files regarding multiple data for analysis for the raster files in these two tabs every item is optional but at least one file should be selected as the output otherwise the model runs but no output data are given additionally for the first six checkboxes in the outputs stability tab an extra csv file will be generated for each of the selected option these text files contain data on the cumulative distribution function cdf of the resulting raster files the name of the csv files is the same as the corresponding raster file followed by the suffix cdf all the output files raster and text files related to the tab of outputs stability are listed in table 3 users can assign any existing folder in their computer as the path to store the output files all the selected output raster files as well as the corresponding csv files will show up in this output folder additionally a text file named filenames txt that contains the path of all input files is created which can be used for future consultations of the simulations performed in the following the 14 outputs regarding the stability calculations will be described the first two outputs are related to the probability of failure pof if the first checkbox pof after effective antecedent recharge and event rainfall is selected the plugin computes the raster map called prob failure final cond asc which includes the pof value at each cell after applying q a and p e whereas the pof value after q a can be generated by using the checkbox of pof after effective antecedent recharge both maps have a range from 0 to 1 the effect of the event rainfall on slope stability can be identified by comparing the two maps meanwhile the pof map after q a can indicate the impact of the effective antecedent recharge on slope stability when compared with the pof map under dry conditions the following two outputs calculate the factor of safety fs under the same conditions as for pof i applying q a and p e and ii applying only q a the values in both maps are larger than 0 and the two maps can be used to check unstable areas with fs values less than 1 the next two checkboxes related to definitions proposed by montgomery and dietrich 1994b the checkbox unconditionally stable cells shows the stability condition of each cell when the soil is completely saturated specifically it indicates the probability of each cell of being unconditionally stable pous hence mathematically the value equals to the difference between 1 and the pof under saturated condition if some landslide points with high pous values are located in the area it means that the soil properties must be checked because even with fully saturated soil the soil will be stable the checkbox of unconditionally unstable cells indicates the pof at each cell when soil is completely dry it can also be explained by the probability of being unconditionally unstable pouu of each cell if no landslide points in the area with high pouu values the areas where the landslide points start may have too large soil property values thus the soil properties must be checked in this condition in areas where slopes are very steep it could happen that values of pouu are inevitably very high this could only be avoided by giving soil properties of bedrock instead of soil the following two outputs are associated with the saturation and infiltration the checkbox saturation degree after effective antecedent recharge ha z computes the raster file named h z ant cond asc which shows the position of the water table in relation to the depth of the soil layer after the effective antecedent recharge it equals to the ratio between the increase of the water table associated with the effective antecedent recharge h a and the soil depth z the raster file named infiltration asc is generated from the checkbox part of pe that infiltrates and it shows the amount of event rainfall which infiltrates into the soil layer the remaining part of the event rainfall is surface runoff which results in the peak water discharge the fslam plugin also provides topographic outputs by the checkbox 3 additional topographic rasters fillsinks slope flow accumulation these outputs include i the dem with the sinks filled fill asc ii slope angle map derived from the dem slope asc and iii the map showing the drainage area at each cell cumflow asc finally three additional outputs can be obtained by selecting the checkbox extra raster files of the 3 terms defined in eq 9 of medina et al 2021 these three raster files are containing data on the initial stability conditions e g slope cohesion thickness etc and the influence of q a and p e on the slope stability the contributions of both effective recharge and event rainfall to fs are not larger than 0 while that of the parameters under dry conditions is not less than 0 the equations of these three contributions are given in eq 9 of medina et al 2021 the path for the outputs can be set by clicking the combo box above the run button 3 2 testing results 3 2 1 computing time all the raster files were prepared at a 5 m resolution which gives a total of about 30 million cells for the entire berguedà study area simulations showed that the stability calculation of the fslam plugin only lasted approximately 4 min for the entire region however a comparison of computing time showed that the writing of rasters files lasted much longer than the stability calculations hence the entire run of the plugin including the writing of all the raster files lasted approximately 80 min this leads us to conclude that it is important to determine which outputs are required for the analysis before applying the plugin since this choice can strongly reduce the running time of the plugin all the calculations above were performed by a computer with one 8 cores 1 8 ghz cpu and 8 gb of ram 3 2 2 output results when selecting all the checkboxes in the gui the plugin will write fifteen raster maps and six csv text files in the specified result folder in the following the raster maps are divided into three groups namely hydrological maps fs related maps and pof related maps the only two maps that will not be shown are the filled dem fill asc and slope map slope asc since the slope map has already been presented in fig 2 as stated above the runoff map fig 6 a was used to calibrate the curve number by comparing the observed and computed peak discharge the observed peak discharge into the la baells reservoir was estimated as 1300 m3 s c barbero catalan water agency personal communication march 15 2021 whereas the simulated runoff by the plugin was 1394 m3 s fig 6b the error between simulated and actual values of runoff was only approximately 7 which is satisfactory for such a simplified approach and the existing uncertainties of p e and cn however this runoff modelling indicates that the selected cn values table 1 fit well for hydrologic conditions of the 1982 rainfall episode in addition to the runoff map the first group of outputs of the fslam plugin includes the flow accumulation map infiltration map and relative saturation degree map they are all hydrology related maps and are useful to understand the infiltration processes during a rainfall episode the flow accumulation map cumflow asc fig 7 a is not only directly related to runoff but also to h a the flow accumulation values range up to 107 m2 in the berguedà area we can see that ridgelines mostly have smallest values and maximum values are mainly located at valley floors where main river are situated the infiltration map infiltration asc fig 7b shows the potential amount of the event rainfall which may infiltrate into the soil in the berguedà area it ranges from 0 to 255 mm relatively large values of infiltration more than 170 mm mainly occur in the lithological classes with high porosity on the contrary the lithological classes with low porosity for example mudstones have small infiltration the map showing the relative saturation degree applying the effective antecedent recharge h z ant cond asc is related to the flow accumulation and the hydraulic conductivity fig 7c because the hydraulic conductivity is linked to the lithology high percentages of relative saturation degree more than 0 75 mainly occur in mudstones where k values are low on the contrary conglomerate sandstone and alluvial lithologies are less saturated due to their relatively high hydraulic conductivity the second group of outputs is composed of five maps related to the factor of safety fs and generated from the stability modelling they reflect the stability conditions in the study area under different rainfall scenarios using the mean parameter value in the range of the stochastic model the raster maps named fs ant cond asc fig 8 a and fs final cond asc fig 8b show a clear correlation between the slope angle and the stability since in those areas with smooth slopes the safety factor is evidently higher when only the effective antecedent recharge is considered most zones in the study area are stable especially in the areas with a low saturation degree of the soil layer see fig 7c the specific effects of the two rainfalls and the soil properties under dry conditions can be identified and compared using the other three maps in this group the contribution of the effective recharge fig 8c is closely related to the hydraulic conductivity and thus with the lithological classes in a similar way to the saturation degree map those areas with mudstone type lithology are mostly affected by the effective antecedent recharge in other areas the decrease of fs is associated with the infiltration of the event rainfall fig 8d rainfall conditions can decrease the slope stability so the contributions of both q a fig 8c and p e fig 8c to fs are negative values the third group of output files are pof related maps and show the probability of failure for different rainfall and soil saturation scenarios they are key outputs of the fslam plugin as they show those areas most likely to generate landslides and therefore they allow the users to anticipate and perform a good risk management overall the study area is stable when only the effective antecedent recharge is applied fig 9 a most areas with pof values larger than 0 75 occur in the garumnian and eocene mudstones after the event rainfall the stability condition in many areas greatly decreases fig 9b this indicates that the input of the event rainfall makes these cells to reach the critical stability conditions the pous values under totally saturated condition have a high relationship with slope angle in the study area fig 9c in comparison to steep slopes it is evident that relatively flat areas have larger probability to be unconditionally stable for example the valley floors generally have high pous values which indicates that the pof at these areas is low or even negligible on the contrary the value of 0 i e pof 1 reveals those areas that have the highest landslide susceptibility under totally saturated condition many landslide inventory points are located in such areas indicating that the soil properties used are appropriate regarding the pouu map under dry conditions fig 9d it may have minor importance because most of the areas in berguedà have low values to properly distinguish these two maps it is important to understand that an equal value on both maps represents totally opposite meanings for example pous 1 stands for a cell that is always stable even in totally saturated condition while pouu 1 means that the cell is always unstable even without any rainfall in addition to the maps showed above the fslam plugin also calculates csv text files outputs they can be simply presented as curves plots which should help the interpretation of the maps fig 10 we have grouped the curves with the same topic into one plot in order to be able to better compare them considering that the range of fs at regional scale may be very large the csv files only lists the cumulative distribution function cdf when fs is not larger than 1 the comparison under different rainfall conditions fig 10a reveals the negative effect of the event rainfall on slope stability for instance after the q a only 17 cells are unstable fs 1 but the percentage increases to 27 after applying q a and p e additionally regarding the cdf curves of the pof values under different rainfall conditions fig 10b and c the overall stability condition can be revealed and compared by computing the area under the cdf curve auc cdf the larger the auc cdf is the more stable the study area is from an overall point of view finally it should be mentioned that the probability density function pdf can be computed from the cdf which can show the percentage of cells corresponding to different pof values 3 2 3 performance analysis and scientific interpretation in this section we switch to a supplementary external analysis of the plugin see flowchart in fig 1 using the existing landslide inventory and present the example of a performance analysis for the landslide susceptibility map such a performance analysis is a standard part of landslide susceptibility assessment the percentage of points versus the pof fig 11 a showed the evident negative effects of q a and p e on landslide points before applying rainfall most of the landslide points had a pof less than 0 2 after q a and p e more than 45 points had a pof larger than 0 8 on the contrary the pof change of the random points was very slight before the rainfall 9 points had pof larger than 0 8 and this ratio only increased by 10 after q a and p e the model accuracy was also clarified by the roc analysis results fig 11b the area under the curve was 0 69 when using the landslide points for analysis after adding a buffer zone around the landslide points due to the uncertainty on the landslide inventory points location the auc value improved evidently when the buffer zone had a radius of 25 m the auc reached to 0 89 at its highest value which was satisfactory considering that the landslide inventory of the 1982 episode has a rather low precision applying a buffer is justified and the resulting accuracy is acceptable the results regarding the confusion matrix table 4 showed that by using the 0 1 as the pof threshold the tpr was 0 797 when no buffer zone is applied around the landslide points the distance to the perfect classification was 0 647 in this condition when the radius of the buffer zone increased the tpr increased whereas the distance to the perfect classification decreased similarly the accuracy increased from 0 647 no buffer to 0 737 25 m buffer all these indexes indicate that the landslide susceptibility map captures most of the landslide occurrences in the region and the result performance is good however it should be also noted that the fpr values didn t change in different conditions so false positives should be also paid attention in the final application of the plugin especially employing it for early warning systems 4 conclusions in the present study a python qgis plugin named fslam is introduced it allows to compute regional shallow landslide susceptibility combining the effect of antecedent and event rainfall conditions through gui based workflow in qgis an additional runoff module can also be used to obtain the peak discharge the plugin outputs involve multiple data including the raster maps regarding topography rainfall infiltration saturated degree of soil layer factor of safety and probability of failure which can be selected individually by the users the plugin was tested in the area of berguedà spain by analysing the landslide events triggered by 1982 rainfall episode the results showed that the computational time for the area containing in total 30 million pixels only lasted 5 min the accuracy of the landslide susceptibility map reached 89 when a 25 m radius buffer was added around the landslide initiation points indicating a satisfactory performance of the plugin the current version of the plugin has been released in github repository allowing automatic downloading and further extension and adaption future possible incorporation into the fslam plugin may focus on auto calibration routines and an additional module which would directly show some results about the performance e g comparison between the landslide inventory and random points and the calculation of roc scores 5 software availability name of software fslam software version 1 0 developers zizheng guo vicente medina software license the gnu general public license v3 0 contact address division of geotechnical engineering and geosciences department of civil and environmental engineering barcelonatech upc jordi girona 1 3 d2 08034 barcelona spain email cuggzz cug edu cn availability https github com engeomodels fslam plugin declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was partly funded by the spanish national projects smucphy and eroslop bia 2015 67500 r and pid2019 104266rb i00 aei 10 13039 501100011033 zizheng guo acknowledges the financial support of china scholarship council for his research at upc barcelonatech fundamental research funds for national universities china university of geosciences wuhan and china shaanxi province key research program no 2019zdlsf07 07 02 
25622,optimization is underlying technique of many graph theoretic algorithms including shortest paths and minimum spanning tree and hence hidden component of landscape connectivity methods optimization solutions have different selectivity but it remains unexplored indeterminate solutions happen without warning and may be spatially biased errors are hidden these issues may undermine connectivity measures and conclusions two optimization targets may be distinguished mobile agent and connectivity provider with different requirements review of how are they handled in literature finds them entangled and often misspecified owing to inconclusive results especially pronounced when conservation and explanatory goals are mixed i categorize graph structures against targets and show how initial choice of structure introduces implicit optimization allowing for stochasticity of mobile agent resolves omniscience problem lastly i advocate against creating landscape graphs with thresholds in favor of complete planar graphs thresholding is impractical biased by design and yields non optimal solutions due to simplistic partitioning keywords landscape connectivity graph theory networks optimization least cost distance wildlife corridor 1 introduction landscape connectivity research embraced innovative ideas and methods so quickly that several mathematical and philosophical issues were left behind the goal of this paper is to step back and look through well established methods and enormous inventory of empirical studies to see one common aspect optimization its inner workings and ontological consequences to this aim i use no empirical material but formal analysis and observation of graph algorithms my direct focus is on graph theoretic approach as introduced by urban and keitt 2001 but some issues also influence circuit theoretic approach mcrae et al 2008 in many places i recall new graphscape software pomianowski and solon 2020 to show alternative approach to the problems 2 optimization a primer shortest paths are the cornerstone of landscape connectivity analysis via measures of network distances they became ubiquitous component of more complex tools and workflows their ecological limitations have been recognized early e g fahrig 2007 sawyer et al 2011 but their mathematical merits and reliability are rarely discussed it may come as a surprise to ecologists that by using least cost paths lcp they are actually involved in optimization procedure this happens not only when they openly seek for best conservation solution but also when they use some long established connectivity metrics in case of graph algorithms the result of optimization is twofold 1 optimal structure and 2 goodness value 1 1 matrix algorithms engaged to the same problem may return only goodness and this is their disadvantage widely used network distance is just special case of goodness value of underlying shortest path structure the first issue we face is that it is not easy to find some natural benchmark against which goodness can be judged the same way as for instance object s mass may be tied to all known masses around it may be sought inside graph for instance by comparing with graph diameter or average edge length but these comparisons are only valid locally more universal benchmark or yardstick must be sought outside of graph realm and sole goodness value is of no help in more complicated cases of optimization covered in section 3 goodness is detached from any real world measure the only way to proceed is to judge optimal solution by other non graph measure for instance by cartesian length area or ecological capacity to do this of course optimal structure must be preserved and not only goodness this theme will reappear throughout this text many times algorithms behind shortest paths minimum spanning trees and many other graph concepts belong to the class of discrete optimization algorithms although their principle of operation is simple their inner workings are intricate and counterintuitive they were originally conceived for technical solutions rather than scientific explanation similarity with machine learning is apparent here similar caution is necessary to avoid being a victim of a black box inherent problem with optimization is that errors originating from the wrong use of basic concepts are hard to detect and their impact on results is difficult to assess first optimization result is always right in a sense that it does utterly match designed criteria whatever they are therefore knowing precisely what we ask for is essential for proper optimization wrong goal or mismatch between declared goal and input data cannot be detected by conventional means the concept of goodness in optimization is completely different from concept of good or reliable result as understood by statistics departure from best solution in optimization is not measured by an error similar to statistical error thus we cannot relate this departure to variance in data and impose confidence interval on the result we also cannot increase the size of input data to increase the reliability of results without a proper error it is impossible to relate the quality of results to any known parameter of input graph or controllable feature of the model essentially optimal solution is take or leave it proposal from this come two conclusions models should be built with greater emphasis on formal correctness and robust assumptions workflows should be designed in such a way that raw graph structures coming out of optimization are available common practice in landscape connectivity research is that raw structures are treated as temporary and too quickly collapsed to summary measures any irregularities in optimization results then are masked by complicated final formulas and impossible to detect it is however possible to design an intermediate step in software where paths trees etc are exposed and may be subject of visualization and analysis even if statistical tools are missing structures may be confronted with certain expectations or common sense it is especially important in experimental phase this must be done with care because intuitions about proper layout of graph structures may be misleading which i experienced many times developing graph software satisfactory visualization is necessary and general purpose graph software is not enough because graphs must be seen coupled with underlying patch mosaic to address these issues graphscape software 1 provides diagnostics for gis graph transition 2 makes structures available in raw listings and exports 3 provides a number of ways to visualize paths and minimum spanning tree among them a transit density measure generalized betweenness centrality of freeman 1977 suitable for observation of arbitrary graph structure within landscape mosaic graph optimization delivers strictly one out of a set of possible solutions i will now look into this set solution space to see what is inside and how it can be penetrated solution space is interesting because it is where second third and all n th best solutions sit except for similar principal characteristic they may differ greatly in terms of other properties for instance second best shortest path may run far away from the optimal one solution space is huge for any non trivial problem it has combinatorial character so it is growing very quickly with a number of elements concerned for instance number of edges composing a path or number of trees spanning a graph when trying to imagine algorithms operation or novel algorithm it is crucial to understand that 1 algorithms do not search the whole space 2 alternatives cannot be easily ordered to make search easier or more organized for these reasons common algorithms do not provide insight into the number structure and spatial location of alternatives the space around the solution is impenetrable this is in contrast to statistical analysis where alternative solutions can be compared or juxtaposed with the source data e g fitting different types of regression curves 3 optimization selectivity and indetermination seeing single optimal solution on a background of whole solution space is a beginning of thinking about the selectivity of solution that is a question how much better it is from alternatives we will be primarily interested in close by competitors as having the biggest impact on our valuation optimization outcome may fall in three broad categories resembling possible outcome of a race a competitors are far away from the winner b competitors are close c competitors are equal to the winner tie case a is selective and needs not to be further investigated unfortunately we do not know if this is what happened behind the scenes so some attention must go to less successful cases case b presents a spectrum of growing concern that optimal solution delivered does not fully describe actual optimum as seen by mobile agents or as sought for conservation purposes with it there is growing interest in properties of alternatives it might be that goodness of alternatives decreases quickly or not uniformly or not spatial distribution also matters they may have specific spatial distribution by themselves or in relation to the optimal one for instance follow the optimum very closely or be dispersed at the extreme they may form a substantial group clustered in space together outweighing the optimal solution this unexplored world is less of a problem for technical applications but it is a shortcoming for explanatory process there have been many proposals to attack the problem in landscape connectivity research see loro et al 2015 for review but their common weakness is and lack of formal proof of correctness and underestimation of sheer size of solution space they can be pictured as attempts to exhaustively explore an ocean with a canoe in this infeasible challenge discovery of any island is considered a proof that no islands exist in more remote parts an example is generator of alternative corridors op cit and an attempt to improve dijkstra algorithm by randomized removal of edges pinto and keitt 2009 proposed solutions are mistaken for similar reasons i give against thresholding in section 5 proper algorithmic solutions exist for alternative structures for instance k shortest paths eppstein 1994 but have not been used in any case those seeking for novel methods should be prepared that multiple paths problem is more demanding kind optimization it cannot be solved without supplying additional constraints and specifying them in line with ecological concepts will be major challenge case c shares above problems with b but additionally introduces indetermination of optimal solution it might be imagined as a discrete analogue of under determination of system of linear equations but does not end with clear no solution message common algorithms still pick the winner and it is a problem which i will address soon 2 2 this is an example of corner case of textbook algorithms a situation considered untypical excluded for clarity and supposed to be solved in real life implementations it is not easy to estimate the severity of this error probability of two paths clash it must depend on the combination of three factors 1 particular graph structure mostly on density of links measured by average node degree 2 length of paths considered and 3 graph weights system numeric values attached to graph edges in practice factors 1 and 2 cannot be controlled without disturbing the whole model i will focus on factor 3 and on discrete weights system but the same reasoning will work for continuous weights discrete weights system made of several bins groups of equal values with known probabilities can be described by entropy the best guess is that a chance of drawing two sets of equal sums from this system is positively related to number of repeating elementary values and thus negatively related to entropy the advice is then to use weights system of high entropy to decrease paths indetermination this can be achieved either by equalizing bins probabilities again difficult in practice or increasing the number of bins the next question is where within a graph these errors occur faced with equal paths or other structures algorithms will pick the first one available in the order of processing and this is dependent on the order of nodes and edges as provided by the user it is easy to foresee that this order will not be random but driven by data preparation phase in case of landscape data most likely spatially biased see fig 1 this bias will drive winning paths towards certain locations of the graph at the expense of other locations which is sketched on fig 2 in most connectivity studies this effect will go unnoticed because lengths of winners and losers are the same however transit densities including betweenness centrality observed at individual patch level may reveal this issue a gross test of indetermination could be then iterating over several permutations of input data and checking for different density patterns in literature or existing software i could not find a single case of exercising a control over indetermination problem one of the ways of mitigation would be to control uniqueness of weights on data input in graphscape software this problem was partially addressed with additional edge property border factor which significantly raises uniqueness of weights generally it seems that safe and least disturbing strategy would be to avoid small closed pools of weights however these may emerge in many practical applications for instance when weights are 1 mono valued as in unweighted graphs 2 drawn from a pool of expert estimates of resistance 3 based on too simple discrete distance metric 4 derived from categorized properties of patches or especially raster cells this way indetermination is dependent on many other methodological choices and in my opinion should be included in decision process optimization selectivity may be viewed as an analogue of confidence in statistical inference good or poor selectivity of solution depends on data and not the method of optimization i distinguished three levels of selectivity with extreme case of indetermination at the end both issues cannot be solved until a new generation of algorithms appears but some measures of mitigation are available today indetermination problem will remain under cover until connectivity research shifts focus away from shrink wrapped indicators and workflows a methodology for modelling multiple paths within graph oriented approach is still in infancy but before new solutions are tested more important question should be answered first how they can be consumed one way would be to replace current shortest paths with compound solution which would require serious changes in theoretic framework and thrust graph based methods in direction of circuit based methods additionally difficult for other structures like trees less disruptive option would be to stay behind primary solution and use results as supplementary information for instance to quantify solution selectivity or spatial pattern against alternatives 4 optimization target problem minimum spanning trees were devised to provide an optimum route system in wiring of electric components and infrastructure networks the solution seemed to be unquestionable because everybody benefits from more efficient networking but this perspective may be no longer true in more complicated context like human transportation networks and landscape connectivity and conservation in transportation networks it has been observed that the joint effect of individual optimizations done by drivers may not lead to system wide optimum braess paradox braess et al 2005 this suggests that seemingly straightforward and singular optimization problem may actually hide many different and unaligned sub problems the key question before optimization is performed or analyzed is then who is the beneficiary connectivity optimization may be tailored toward one of two parties with different interests the first one is a mobile agent an entity using network connectivity for its own purposes or responding unwittingly to network conditions its interests are covered by the mobility model a set of mathematical concepts designed to mirror elementary mobility behavior the second one is a provider of a network connectivity that is the external body with obligation to create and maintain connectivity for instance road authority or conservation authority while mobile agent has egoistic attitude in many models it seeks only to minimize cost of traversal provider attitude is altruistic and objective in a sense that it cares for well being of entire system well being of multiple species necessarily belongs to this category it is important to note that provider must also optimize for non connectivity goals mainly habitat quality which is considered dilemma e g moilanen 2011 additionally it must abide to specific statutory goal balance benefits with limited resources and take other external considerations the concept of provider is similar to stakeholder of opdam et al 2008 but focused on connectivity in landscape connectivity research different optimization targets are rarely delineated and systematically investigated this may be a problem because the difference in interests always leads to different and possibly opposing optimization criteria this is why sharp and open specification of mobility model pomianowski and solon 2020 is desirable i have noticed several patterns of entanglement of optimization targets clear and safe situation is when single mobile agent is investigated or the study just iterates over many agents without synthesis however when multiple agents optimum is investigated then a method of conciliation of different targets is often not satisfactory the silent assumption may be that adding connectivities automatically yields best for all results e g koen et al 2014 miranda et al 2021 in pereira et al 2017 the problem was properly recognized and then converted into finding good connectivity for different sets of habitat patches one set per agent but solution of this problem was only heuristic kp algorithm similar approach was in brás et al 2013 optimization by seeking among sets of patches combinatorial is more difficult than by graph weight function in my opinion the only way to optimize for multispecies target especially without reliable tools at hand is to group mobile agents and leave results not synthetized caveat is that group members must share similar mobility model and not other features even those ecologically important part of this thinking was behind grouping of bird species mimet et al 2013 and in very promising concept of surrogate species meurant et al 2018 my doubt is however that in both cases groups were based on factors not directly related to species mobility in first case it was habitat preference forest farmland specialists and generalists in the second case different combinations of factors were tested but out of 6 only 2 were somehow related to mobility very rigorous and focused on mobility approach was shown by lechner et al 2016 in dispersal guilds of species formed by cluster analysis gurrutxaga et al 2010 modelled connectivity based on mobile agents perspective only but demonstrated how it can be cleverly combined with overall goal of provider in this case regional government without explicitly including the latter in the model it was a good match of core areas scale and simple but reasonable mobility model at last there exists a strain of studies e g wimberly et al 2018 using generic target so in fact no particular mobile agent target at all the implied presence of provider target if any is by underlying graph structure explanatory value of these is only in investigating topological properties of patch mosaic i think these studies can only claim applicability for conservation purposes if they operate on structures prescribed by the provider for instance for a given corridor or set of patches finding a dispersal distance and then finding beneficiary species among those in neighborhood other justified case of use is undetermined or unpredictable mobility of agent for instance of plant pathogens margosian et al 2009 conscious recognition of different targets is best observed in studies directed towards immediate conservation applications when these are performed with specialized optimization tools like linear programming enforcing strictness and transparency possingham et al 2000 maximized biodiversity against the number of protected sites so indirectly against conservation expenses it was later extended to include boundary length of sites mcdonnell et al 2002 and gave inspiring example of how different criteria can be added to optimization target in controlled manner however connectivity was only approximated by boundary compactness dilkina et al 2016 investigated optimum corridors for two species separately and jointly with constant size raster cells corridor length easily translated to area and then to potential provider expenses two strategies of connectivity preservation were tested against limited budget in yemshanov et al 2019 existing connectivity software does not openly recognize the difference between provider and mobile agent perspective it is scattered between many parts of tooling and initial assumptions our graphscape system is no better but whose who recognize the problem and wish to specifically model provider perspective in it or any other graph based tool the best way to do is to put provider criteria in graph weight function in graphscape this is accomplished by using mobility model in a non standard way path resistance may be substituted by variables related to provider target a simplest example in the context of environmental protection could be the area the cost of establishing a protection over ecological corridor or excluding a swath of land from social activity is always related to the area obtained this way resulting shortest paths and minimum spanning tree reflect the least expensive way of connecting two or several target patches without taking animal mobility needs for consideration another variable may be the total length of border like mcdonnell et al 2002 more complex measures like those used in dilkina et al 2016 or other policy related indicators such as population size density land ownership or zoning categories can be used too provided they are casted into continuous variable optimization done with specialized tools share common factor with optimization done on graphs ultimately there remains a single target function driving the process and tested towards extreme point in this setting the only way to include multiple criteria is by more or less sophisticated weighing them before entry to target function in dilkina et al 2016 it is accomplished with α parameter distributing two resistances according to desired proportion general solution is possible for more than two targets with similar equations especially random utility models could be relevant for future developments see manski 1977 simplest version of weighing can start with just equalizing provider and mobile agent optimization this can be accomplished by careful re scaling both resistances so they fall into the same numeric range or otherwise reflect equal sensitivity from both perspectives then a function w r a would reflect balanced interests of mobile agent r patch resistance and provider a patch area expense from this point changing proportion and other transformations may be tried as long as they produce values appropriate for graph weights additive weighing schemes were investigated by rayfield et al 2016 who claim they can be extended up to tens of thousands of targets however how to weigh other priorities of provider against thousands of species is still an open question i have shown how multiple targets can be embraced even with simple graph models however even if technically feasible this kind of all encompassing approach has its limitations first the claim that joint solution is immediately useful and ready for application is questionable conservation or land use decisions are rarely implemented in a single step rather they are a part of prolonged policy and must be implemented gradually also uncertain future and additional operational factors may be hard to cast into one simple criterion if remaining considerations cannot be brought into optimization then the rationale for joint optimization is weakened next disadvantage of weighing is that it does not solve the problem of multiple sets of terminal patches usually required for multiple species related issue is to what degree conservation oriented research needs to be married with explanatory research in single study the paradox of mixed provider agent optimization is that graph weight values are meaningful to the algorithm but have no ecological meaning in themselves the same may be told of structures obtained from such weights and their summary measures they are just optimal and nothing else gippoliti and battisti 2017 complain that ecological network concept and connectivity results have been too easily translated into conservation concepts i think the opposite is also true and it takes special caution to mix both approaches under one methodological umbrella perhaps it is more informative to give decision makers separate diagnosis which has explanatory value itself and separate conservation solutions i will once again refer to surrogate species study meurant et al 2018 it s declared goal was to find species groups for connectivity conservation while connectivity was modelled by standard resistances mobile agent perspective grouping was strongly driven by provider perspective the authors did not show it and readers have all reasons suspect that both optimization targets worked in orthogonal or even opposite way for instance it is unrealistic to expect the grouping category diversity of habitat needs or potential as an umbrella species drives mobile agent s pattern of movement in systematic way connectivity part of the study was lost altogether and at the end we are not able verify the original claim that it can be protected by conservation efforts this otherwise competent work falls very short of declared goal due to inattentive approach to optimization targets 5 target oriented structures previous section described optimization done by means of computational techniques which may be dubbed explicit however any graph based connectivity study is influenced by the choice of underlying graph structure even before computations start and before mobility model is specified this may be called implicit optimization by using this or that graph structure we commit to certain efficiency principle behind it efficiency can be attributed either to system wide or more focused optimum so the choice introduces either the perspective of provider or mobile agent and there is no way to avoid it although investigations of walks mechanism by animals have been attempted palmer et al 2011 i am not aware of this being done for more complex structures i will illustrate this on minimum spanning tree mst by only reviewing its formal properties mst efficiency principle is minimum cost of moving between a set of terminal nodes but nothing is told about pairs of nodes from the viewpoint of individual mobile agent the movement between any two nodes within mst may be not optimal also the whole structure cannot be effectively utilized by individual because trees cannot be traversed in a single pass without repeating certain parts except for trees with linear arrangement of nodes therefore plain mst is strongly geared towards provider target similar is steiner tree e g lai et al 2011 but rarely used because of high computational expense 3 3 jungnickel 2013 p 122 reports that improvement over mst cannot exceed 14 for planar graphs due to complexity heuristic algorithms must be used and this minute improvement may be not realized in practice slightly different effect is when mst is computed by a two stage algorithm graphscape and is composed of shortest paths sp then at least the movement between two nodes adjacent in mst actually distanced many patches away is guaranteed to be most effective so this structure may partly reflect mobile agent target of all structures shortest paths sp or a set of quasi shortest paths set of k paths are best for this purpose i would like to note that great deal of connectivity indices indirectly use a set of shortest paths connecting all nodes even if not exactly a textbook structure i include it here to simulate thinking about what is the optimization target in this case table 1 lists a number of more common graph structures classified into three groups according to their ability to reflect provider or mobile agent target the middle category may be coerced to work with both targets depending on context however i am convinced that both should be clearly separated if addressed in single study please note that corridor concept is absent because it has no direct equivalent in graph theory at the end i want to point out ontological aspect of above division structures which are not effective for individual mobile agent may be still effective or meaningful for stochastic mobile agent in trees some agents will traverse parts of the structure optimally and others will traverse other parts optimally as long as they belong to the same category species metapopulation they can collectively achieve optimum and benefit from connectivity for instance migratory patterns were proven to be optimal at the level of whole species by creative use of bi partite graphs somveille et al 2021 this thinking can be also applied to activity over time for sufficiently long time span momentary decisions will turn into stochastic pattern stochastic mobile agent modelling may thus enjoy a wider range of structures to form realistic mobility model connectivity research needs backing from animal movement studies and one of the strains of these is focused on modelling individual behavior and decisions individual based model of heinz and strand 2006 fahrig 2007 palmer et al 2011 lapoint et al 2013 here most attention goes to momentary perception of risks opportunities and gains during the movement of single individual naturally it is difficult to explain more intricate spatial behaviors and uncover optimal decisions in them i argue that this approach cannot advance beyond certain level but much can be done at the next level by matching individual movement patterns with appropriate structures and leveraging wider view of optimality similar thinking may be applied to the problem of omniscience of mobile agents coulon et al 2015 that is alleged and questioned ability to comprehend broad fragments of landscape and find best routes indeed any systematic exploration of graph requires memory and even more structured memory which can be recalled orderly and selectively graph algorithms breadth first depth first searches shortest paths minimum spanning tree maintain and constantly update lists of visited nodes and their currently best solution however we do not have to attribute exactly the same mechanism to animal agents to explain their optimal solutions explorations can be done in unsystematic way and still lead to optimum provided they are performed consecutively over individual s lifetime three mechanisms non oriented oriented and memory based mueller and fagan 2008 may be relevant also for an individual of single species and may play together in sequence so random walks get upgraded to oriented traversals improving navigation has been recognized for many animals op cit additionally individual mobile agent may benefit from interspecific communication also unintentional like olfactory cues and observation to improve its solution after arriving at optimum by any means memory mechanism can be switched on the cost of memorizing already discovered structure will be much lower than memory requirements for full exploration done in algorithmic way and it may be easily within individual s abilities so contrary to omniscience hypothesis there is no need to have complete knowledge to perform successfully at stochastic level i believe it is safe to assume that in stable conditions solutions worked out by animal mobile agents are optimal because all three factors time number of individuals and target function work in the same direction the critique of least cost paths grounded in mechanistic understanding of animal decisions fahrig 2007 misses the stochastic kind of wisdom further reaching conclusion is that connectivity models which pursue optimality with right structures are well fit for the problems 6 graph assemblage with thresholding graph assemblage is a critical step of transforming spatially referenced ecological data into graph structure in pomianowski and solon 2020 we have raised doubts about graph creation based on thresholds that is when graph edges are created whenever distance is lower than certain threshold value which is often based on dispersion distance specific for species this approach originating from urban and keitt 2001 has been followed by a multitude of studies e g fall et al 2007 garcía feced et al 2011 and more examples in dale and fortin 2010 it has been authoritatively proclaimed the only way to introduce graph measures to landscape connectivity by calabrese and fagan 2004 and recent review keeley et al 2021 confirms that it still stands behind many widely used ecological indicators as well as new ones e g saura et al 2017 it is built in the software and even mandatory in models implemented in conefor sensinode consen 2007 apart from analytical applications urban and keitt 2001 p 1210 proposed threshold based components as ready made structures for conservation activities management prioritizing site acquisition or protection the options pro and against thresholding on full and planar graphs were considered by foltête and giraudoux 2012 moilanen 2011 rightfully criticized thresholding as leading to information loss focusing on deficiency in handling the distance super steep distance kernel and inability to serve multiple species with different dispersal kernels it may be helpful to add that unnecessary loss comes also from wasting algorithms power because they were designed to work on continuous data usually distance since their origin the old argument of lower computational demands compared to full graphs seems to be less and less valid over the years to this i add another objection a mismatch with graph theoretic algorithms optimizing nature of shortest paths and minimum spanning tree algorithms calls for unconstrained choice of input edges the algorithms test the usability of any particular edge against other edges in the context of entire graph structure and not single property weight no edge is considered weak until its weakness has been proven this is algorithms wit and strength that cannot be outperformed by hand picking thresholding is kind of pre emptive choice made outside of the algorithm and made without consideration for edge placement within graph structure it cannot be smarter than the algorithm and it has detrimental effect on emerging graph if for instance a threshold is set at median dispersion distance then thresholding excludes roughly half of the connectivity potential from algorithm operation this excluded portion consists of a finite and unknown number of missing edges which could have been be used to conduct movement the overall effect is changing the layout and placement of paths and their artificial elongation graphs treated this way cannot be reliably used to build shortest paths minimum spanning trees and all derived measures this general reservation concerns all graphs altered by manual intervention but how does it work specifically in habitat matrix context thresholding produces components detached graphs considered well connected out of some initial graph fully connected or planar landscape graph this is accomplished be a series of accept reject decisions which are in fact equivalent to link removal made on initial graph depicted on fig 3 is component made from two candidate parts of graph by including the shorter link and rejecting the longer link above threshold it can be seen that if shortest path algorithm is run later on this structure non optimal path will be produced in more complete setting there will be many alternative links removed influencing connectivity for different parts of the component components produced by thresholding are therefore defective incomplete and not delivering the promise of maximum inner connectivity this is why they are not suitable for many commonly used computations including shortest paths structures path enumerations maps shortest paths lengths network distances graph diameter node degree betweenness centrality connectivity indices involving shortest paths iic pc the error originates from shortest paths their lengths and derived measures but is more far reaching because eventually also the number of links is affected and so component patches and their areas the list may be continued up to the final conservation prioritization rankings in studies of scale dependent connectivity a series of thresholds is used to simulate a graduate build up of components which are analyzed by some parameter for instance expected cluster size o brien et al 2006 or graph diameter bunn et al 2000 brooks 2006 neel 2008 resulting plots with distinctive plateaus are interpreted as an evidence of distinct levels of connectivity the idea of systematic thresholding is an improvement over single threshold applied to whole problem but ultimately it also suffers from wrong components with more faithful component construction plateaus could occur elsewhere and it is likely that some would vanish altogether because the buildup of components would be more gradual the same way it does when binary measures change to continuous laita et al 2011 no doubt thresholding exposes some kind of sensitivity of the graph but no proof was provided so far that this sensitivity is in pair with ecological connectivity for this kind of analysis be successful buildup process should be controlled is such a way that components remain optimal all the way along x axis or at least have some well defined ecological property while thresholding sensitivity is an object of interest for some researchers it is practical problem for the others many landscape mosaics will respond abruptly to minimal changes of threshold forming either overly large or too fine components see foltête et al 2016 difficult to process further only one side of this problem mega patches was addressed by cavanaugh et al 2014 it looks like sometimes threshold cannot be stabilized either way resulting in a mix of components of disproportionate sizes exemplary is the resulting graph of minor et al 2009 such troublesome graphs are inappropriate for distance based approach no reasonable metric and resistance based approach too heterogeneous however they are surprisingly often accepted relative success of thresholding to produce a graph is a matter of luck because it depends on actual terrain configuration therefore it is risky to plan it in more extensive workflow where it could undermine further steps when two goals of thresholding are put together strange finding appears so much investigated sensitivity to scale is precisely the reason why thresholding is dependent on the resolution of mosaic raster input in context of graph assemblage this is an artifact which makes resulting graph biased commanding idea of habitat matrix connectivity framework that is to extract meaningful graph components out of surrounding background fails because of too simplistic approach to the problem of graph partitioning there are two solutions one is to abandon the idea that the landscape is divided into disjoint subgraphs and proceed with complete landscape graph pomianowski and solon 2020 seeking for embedded structures the other is to find better methods of partitioning some attempts have been done for instance compartments bodin and norberg 2007 gao et al 2013 and graph theory has more appropriate tools strongly connected components graph cuts still waiting the problem is however that generalized partitioning is computationally tough and it has to resort to heuristic methods see buluç et al 2021 for review so advancing this way within habitat matrix paradigm will be a challenge 7 closing remarks i have shown a common trait in landscape connectivity methodology understanding how optimization works is important for our choice of methods spotting out false assumptions and avoiding intractable models unfortunately for future developments many barriers exist ahead and these cannot be overcome without more obedient approach to graph theory conclusions were presented along the way so at the end i have only two general remarks landscape connectivity research suffers from inconclusiveness i think some of it can be attributed to wrongly defined or operationalized optimization problems many subtleties are hidden in proper formulation and separation of different optimization targets and this paper s value is only in encouraging further discussion the second one is that common attitude to cover conservation and explanation under one umbrella which was sufficient at the early stage should be re evaluated considering differences in optimization targets and methods required declaration of competing interest the author declares that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements i thank professor jerzy solon for reviewing the manuscript 
25622,optimization is underlying technique of many graph theoretic algorithms including shortest paths and minimum spanning tree and hence hidden component of landscape connectivity methods optimization solutions have different selectivity but it remains unexplored indeterminate solutions happen without warning and may be spatially biased errors are hidden these issues may undermine connectivity measures and conclusions two optimization targets may be distinguished mobile agent and connectivity provider with different requirements review of how are they handled in literature finds them entangled and often misspecified owing to inconclusive results especially pronounced when conservation and explanatory goals are mixed i categorize graph structures against targets and show how initial choice of structure introduces implicit optimization allowing for stochasticity of mobile agent resolves omniscience problem lastly i advocate against creating landscape graphs with thresholds in favor of complete planar graphs thresholding is impractical biased by design and yields non optimal solutions due to simplistic partitioning keywords landscape connectivity graph theory networks optimization least cost distance wildlife corridor 1 introduction landscape connectivity research embraced innovative ideas and methods so quickly that several mathematical and philosophical issues were left behind the goal of this paper is to step back and look through well established methods and enormous inventory of empirical studies to see one common aspect optimization its inner workings and ontological consequences to this aim i use no empirical material but formal analysis and observation of graph algorithms my direct focus is on graph theoretic approach as introduced by urban and keitt 2001 but some issues also influence circuit theoretic approach mcrae et al 2008 in many places i recall new graphscape software pomianowski and solon 2020 to show alternative approach to the problems 2 optimization a primer shortest paths are the cornerstone of landscape connectivity analysis via measures of network distances they became ubiquitous component of more complex tools and workflows their ecological limitations have been recognized early e g fahrig 2007 sawyer et al 2011 but their mathematical merits and reliability are rarely discussed it may come as a surprise to ecologists that by using least cost paths lcp they are actually involved in optimization procedure this happens not only when they openly seek for best conservation solution but also when they use some long established connectivity metrics in case of graph algorithms the result of optimization is twofold 1 optimal structure and 2 goodness value 1 1 matrix algorithms engaged to the same problem may return only goodness and this is their disadvantage widely used network distance is just special case of goodness value of underlying shortest path structure the first issue we face is that it is not easy to find some natural benchmark against which goodness can be judged the same way as for instance object s mass may be tied to all known masses around it may be sought inside graph for instance by comparing with graph diameter or average edge length but these comparisons are only valid locally more universal benchmark or yardstick must be sought outside of graph realm and sole goodness value is of no help in more complicated cases of optimization covered in section 3 goodness is detached from any real world measure the only way to proceed is to judge optimal solution by other non graph measure for instance by cartesian length area or ecological capacity to do this of course optimal structure must be preserved and not only goodness this theme will reappear throughout this text many times algorithms behind shortest paths minimum spanning trees and many other graph concepts belong to the class of discrete optimization algorithms although their principle of operation is simple their inner workings are intricate and counterintuitive they were originally conceived for technical solutions rather than scientific explanation similarity with machine learning is apparent here similar caution is necessary to avoid being a victim of a black box inherent problem with optimization is that errors originating from the wrong use of basic concepts are hard to detect and their impact on results is difficult to assess first optimization result is always right in a sense that it does utterly match designed criteria whatever they are therefore knowing precisely what we ask for is essential for proper optimization wrong goal or mismatch between declared goal and input data cannot be detected by conventional means the concept of goodness in optimization is completely different from concept of good or reliable result as understood by statistics departure from best solution in optimization is not measured by an error similar to statistical error thus we cannot relate this departure to variance in data and impose confidence interval on the result we also cannot increase the size of input data to increase the reliability of results without a proper error it is impossible to relate the quality of results to any known parameter of input graph or controllable feature of the model essentially optimal solution is take or leave it proposal from this come two conclusions models should be built with greater emphasis on formal correctness and robust assumptions workflows should be designed in such a way that raw graph structures coming out of optimization are available common practice in landscape connectivity research is that raw structures are treated as temporary and too quickly collapsed to summary measures any irregularities in optimization results then are masked by complicated final formulas and impossible to detect it is however possible to design an intermediate step in software where paths trees etc are exposed and may be subject of visualization and analysis even if statistical tools are missing structures may be confronted with certain expectations or common sense it is especially important in experimental phase this must be done with care because intuitions about proper layout of graph structures may be misleading which i experienced many times developing graph software satisfactory visualization is necessary and general purpose graph software is not enough because graphs must be seen coupled with underlying patch mosaic to address these issues graphscape software 1 provides diagnostics for gis graph transition 2 makes structures available in raw listings and exports 3 provides a number of ways to visualize paths and minimum spanning tree among them a transit density measure generalized betweenness centrality of freeman 1977 suitable for observation of arbitrary graph structure within landscape mosaic graph optimization delivers strictly one out of a set of possible solutions i will now look into this set solution space to see what is inside and how it can be penetrated solution space is interesting because it is where second third and all n th best solutions sit except for similar principal characteristic they may differ greatly in terms of other properties for instance second best shortest path may run far away from the optimal one solution space is huge for any non trivial problem it has combinatorial character so it is growing very quickly with a number of elements concerned for instance number of edges composing a path or number of trees spanning a graph when trying to imagine algorithms operation or novel algorithm it is crucial to understand that 1 algorithms do not search the whole space 2 alternatives cannot be easily ordered to make search easier or more organized for these reasons common algorithms do not provide insight into the number structure and spatial location of alternatives the space around the solution is impenetrable this is in contrast to statistical analysis where alternative solutions can be compared or juxtaposed with the source data e g fitting different types of regression curves 3 optimization selectivity and indetermination seeing single optimal solution on a background of whole solution space is a beginning of thinking about the selectivity of solution that is a question how much better it is from alternatives we will be primarily interested in close by competitors as having the biggest impact on our valuation optimization outcome may fall in three broad categories resembling possible outcome of a race a competitors are far away from the winner b competitors are close c competitors are equal to the winner tie case a is selective and needs not to be further investigated unfortunately we do not know if this is what happened behind the scenes so some attention must go to less successful cases case b presents a spectrum of growing concern that optimal solution delivered does not fully describe actual optimum as seen by mobile agents or as sought for conservation purposes with it there is growing interest in properties of alternatives it might be that goodness of alternatives decreases quickly or not uniformly or not spatial distribution also matters they may have specific spatial distribution by themselves or in relation to the optimal one for instance follow the optimum very closely or be dispersed at the extreme they may form a substantial group clustered in space together outweighing the optimal solution this unexplored world is less of a problem for technical applications but it is a shortcoming for explanatory process there have been many proposals to attack the problem in landscape connectivity research see loro et al 2015 for review but their common weakness is and lack of formal proof of correctness and underestimation of sheer size of solution space they can be pictured as attempts to exhaustively explore an ocean with a canoe in this infeasible challenge discovery of any island is considered a proof that no islands exist in more remote parts an example is generator of alternative corridors op cit and an attempt to improve dijkstra algorithm by randomized removal of edges pinto and keitt 2009 proposed solutions are mistaken for similar reasons i give against thresholding in section 5 proper algorithmic solutions exist for alternative structures for instance k shortest paths eppstein 1994 but have not been used in any case those seeking for novel methods should be prepared that multiple paths problem is more demanding kind optimization it cannot be solved without supplying additional constraints and specifying them in line with ecological concepts will be major challenge case c shares above problems with b but additionally introduces indetermination of optimal solution it might be imagined as a discrete analogue of under determination of system of linear equations but does not end with clear no solution message common algorithms still pick the winner and it is a problem which i will address soon 2 2 this is an example of corner case of textbook algorithms a situation considered untypical excluded for clarity and supposed to be solved in real life implementations it is not easy to estimate the severity of this error probability of two paths clash it must depend on the combination of three factors 1 particular graph structure mostly on density of links measured by average node degree 2 length of paths considered and 3 graph weights system numeric values attached to graph edges in practice factors 1 and 2 cannot be controlled without disturbing the whole model i will focus on factor 3 and on discrete weights system but the same reasoning will work for continuous weights discrete weights system made of several bins groups of equal values with known probabilities can be described by entropy the best guess is that a chance of drawing two sets of equal sums from this system is positively related to number of repeating elementary values and thus negatively related to entropy the advice is then to use weights system of high entropy to decrease paths indetermination this can be achieved either by equalizing bins probabilities again difficult in practice or increasing the number of bins the next question is where within a graph these errors occur faced with equal paths or other structures algorithms will pick the first one available in the order of processing and this is dependent on the order of nodes and edges as provided by the user it is easy to foresee that this order will not be random but driven by data preparation phase in case of landscape data most likely spatially biased see fig 1 this bias will drive winning paths towards certain locations of the graph at the expense of other locations which is sketched on fig 2 in most connectivity studies this effect will go unnoticed because lengths of winners and losers are the same however transit densities including betweenness centrality observed at individual patch level may reveal this issue a gross test of indetermination could be then iterating over several permutations of input data and checking for different density patterns in literature or existing software i could not find a single case of exercising a control over indetermination problem one of the ways of mitigation would be to control uniqueness of weights on data input in graphscape software this problem was partially addressed with additional edge property border factor which significantly raises uniqueness of weights generally it seems that safe and least disturbing strategy would be to avoid small closed pools of weights however these may emerge in many practical applications for instance when weights are 1 mono valued as in unweighted graphs 2 drawn from a pool of expert estimates of resistance 3 based on too simple discrete distance metric 4 derived from categorized properties of patches or especially raster cells this way indetermination is dependent on many other methodological choices and in my opinion should be included in decision process optimization selectivity may be viewed as an analogue of confidence in statistical inference good or poor selectivity of solution depends on data and not the method of optimization i distinguished three levels of selectivity with extreme case of indetermination at the end both issues cannot be solved until a new generation of algorithms appears but some measures of mitigation are available today indetermination problem will remain under cover until connectivity research shifts focus away from shrink wrapped indicators and workflows a methodology for modelling multiple paths within graph oriented approach is still in infancy but before new solutions are tested more important question should be answered first how they can be consumed one way would be to replace current shortest paths with compound solution which would require serious changes in theoretic framework and thrust graph based methods in direction of circuit based methods additionally difficult for other structures like trees less disruptive option would be to stay behind primary solution and use results as supplementary information for instance to quantify solution selectivity or spatial pattern against alternatives 4 optimization target problem minimum spanning trees were devised to provide an optimum route system in wiring of electric components and infrastructure networks the solution seemed to be unquestionable because everybody benefits from more efficient networking but this perspective may be no longer true in more complicated context like human transportation networks and landscape connectivity and conservation in transportation networks it has been observed that the joint effect of individual optimizations done by drivers may not lead to system wide optimum braess paradox braess et al 2005 this suggests that seemingly straightforward and singular optimization problem may actually hide many different and unaligned sub problems the key question before optimization is performed or analyzed is then who is the beneficiary connectivity optimization may be tailored toward one of two parties with different interests the first one is a mobile agent an entity using network connectivity for its own purposes or responding unwittingly to network conditions its interests are covered by the mobility model a set of mathematical concepts designed to mirror elementary mobility behavior the second one is a provider of a network connectivity that is the external body with obligation to create and maintain connectivity for instance road authority or conservation authority while mobile agent has egoistic attitude in many models it seeks only to minimize cost of traversal provider attitude is altruistic and objective in a sense that it cares for well being of entire system well being of multiple species necessarily belongs to this category it is important to note that provider must also optimize for non connectivity goals mainly habitat quality which is considered dilemma e g moilanen 2011 additionally it must abide to specific statutory goal balance benefits with limited resources and take other external considerations the concept of provider is similar to stakeholder of opdam et al 2008 but focused on connectivity in landscape connectivity research different optimization targets are rarely delineated and systematically investigated this may be a problem because the difference in interests always leads to different and possibly opposing optimization criteria this is why sharp and open specification of mobility model pomianowski and solon 2020 is desirable i have noticed several patterns of entanglement of optimization targets clear and safe situation is when single mobile agent is investigated or the study just iterates over many agents without synthesis however when multiple agents optimum is investigated then a method of conciliation of different targets is often not satisfactory the silent assumption may be that adding connectivities automatically yields best for all results e g koen et al 2014 miranda et al 2021 in pereira et al 2017 the problem was properly recognized and then converted into finding good connectivity for different sets of habitat patches one set per agent but solution of this problem was only heuristic kp algorithm similar approach was in brás et al 2013 optimization by seeking among sets of patches combinatorial is more difficult than by graph weight function in my opinion the only way to optimize for multispecies target especially without reliable tools at hand is to group mobile agents and leave results not synthetized caveat is that group members must share similar mobility model and not other features even those ecologically important part of this thinking was behind grouping of bird species mimet et al 2013 and in very promising concept of surrogate species meurant et al 2018 my doubt is however that in both cases groups were based on factors not directly related to species mobility in first case it was habitat preference forest farmland specialists and generalists in the second case different combinations of factors were tested but out of 6 only 2 were somehow related to mobility very rigorous and focused on mobility approach was shown by lechner et al 2016 in dispersal guilds of species formed by cluster analysis gurrutxaga et al 2010 modelled connectivity based on mobile agents perspective only but demonstrated how it can be cleverly combined with overall goal of provider in this case regional government without explicitly including the latter in the model it was a good match of core areas scale and simple but reasonable mobility model at last there exists a strain of studies e g wimberly et al 2018 using generic target so in fact no particular mobile agent target at all the implied presence of provider target if any is by underlying graph structure explanatory value of these is only in investigating topological properties of patch mosaic i think these studies can only claim applicability for conservation purposes if they operate on structures prescribed by the provider for instance for a given corridor or set of patches finding a dispersal distance and then finding beneficiary species among those in neighborhood other justified case of use is undetermined or unpredictable mobility of agent for instance of plant pathogens margosian et al 2009 conscious recognition of different targets is best observed in studies directed towards immediate conservation applications when these are performed with specialized optimization tools like linear programming enforcing strictness and transparency possingham et al 2000 maximized biodiversity against the number of protected sites so indirectly against conservation expenses it was later extended to include boundary length of sites mcdonnell et al 2002 and gave inspiring example of how different criteria can be added to optimization target in controlled manner however connectivity was only approximated by boundary compactness dilkina et al 2016 investigated optimum corridors for two species separately and jointly with constant size raster cells corridor length easily translated to area and then to potential provider expenses two strategies of connectivity preservation were tested against limited budget in yemshanov et al 2019 existing connectivity software does not openly recognize the difference between provider and mobile agent perspective it is scattered between many parts of tooling and initial assumptions our graphscape system is no better but whose who recognize the problem and wish to specifically model provider perspective in it or any other graph based tool the best way to do is to put provider criteria in graph weight function in graphscape this is accomplished by using mobility model in a non standard way path resistance may be substituted by variables related to provider target a simplest example in the context of environmental protection could be the area the cost of establishing a protection over ecological corridor or excluding a swath of land from social activity is always related to the area obtained this way resulting shortest paths and minimum spanning tree reflect the least expensive way of connecting two or several target patches without taking animal mobility needs for consideration another variable may be the total length of border like mcdonnell et al 2002 more complex measures like those used in dilkina et al 2016 or other policy related indicators such as population size density land ownership or zoning categories can be used too provided they are casted into continuous variable optimization done with specialized tools share common factor with optimization done on graphs ultimately there remains a single target function driving the process and tested towards extreme point in this setting the only way to include multiple criteria is by more or less sophisticated weighing them before entry to target function in dilkina et al 2016 it is accomplished with α parameter distributing two resistances according to desired proportion general solution is possible for more than two targets with similar equations especially random utility models could be relevant for future developments see manski 1977 simplest version of weighing can start with just equalizing provider and mobile agent optimization this can be accomplished by careful re scaling both resistances so they fall into the same numeric range or otherwise reflect equal sensitivity from both perspectives then a function w r a would reflect balanced interests of mobile agent r patch resistance and provider a patch area expense from this point changing proportion and other transformations may be tried as long as they produce values appropriate for graph weights additive weighing schemes were investigated by rayfield et al 2016 who claim they can be extended up to tens of thousands of targets however how to weigh other priorities of provider against thousands of species is still an open question i have shown how multiple targets can be embraced even with simple graph models however even if technically feasible this kind of all encompassing approach has its limitations first the claim that joint solution is immediately useful and ready for application is questionable conservation or land use decisions are rarely implemented in a single step rather they are a part of prolonged policy and must be implemented gradually also uncertain future and additional operational factors may be hard to cast into one simple criterion if remaining considerations cannot be brought into optimization then the rationale for joint optimization is weakened next disadvantage of weighing is that it does not solve the problem of multiple sets of terminal patches usually required for multiple species related issue is to what degree conservation oriented research needs to be married with explanatory research in single study the paradox of mixed provider agent optimization is that graph weight values are meaningful to the algorithm but have no ecological meaning in themselves the same may be told of structures obtained from such weights and their summary measures they are just optimal and nothing else gippoliti and battisti 2017 complain that ecological network concept and connectivity results have been too easily translated into conservation concepts i think the opposite is also true and it takes special caution to mix both approaches under one methodological umbrella perhaps it is more informative to give decision makers separate diagnosis which has explanatory value itself and separate conservation solutions i will once again refer to surrogate species study meurant et al 2018 it s declared goal was to find species groups for connectivity conservation while connectivity was modelled by standard resistances mobile agent perspective grouping was strongly driven by provider perspective the authors did not show it and readers have all reasons suspect that both optimization targets worked in orthogonal or even opposite way for instance it is unrealistic to expect the grouping category diversity of habitat needs or potential as an umbrella species drives mobile agent s pattern of movement in systematic way connectivity part of the study was lost altogether and at the end we are not able verify the original claim that it can be protected by conservation efforts this otherwise competent work falls very short of declared goal due to inattentive approach to optimization targets 5 target oriented structures previous section described optimization done by means of computational techniques which may be dubbed explicit however any graph based connectivity study is influenced by the choice of underlying graph structure even before computations start and before mobility model is specified this may be called implicit optimization by using this or that graph structure we commit to certain efficiency principle behind it efficiency can be attributed either to system wide or more focused optimum so the choice introduces either the perspective of provider or mobile agent and there is no way to avoid it although investigations of walks mechanism by animals have been attempted palmer et al 2011 i am not aware of this being done for more complex structures i will illustrate this on minimum spanning tree mst by only reviewing its formal properties mst efficiency principle is minimum cost of moving between a set of terminal nodes but nothing is told about pairs of nodes from the viewpoint of individual mobile agent the movement between any two nodes within mst may be not optimal also the whole structure cannot be effectively utilized by individual because trees cannot be traversed in a single pass without repeating certain parts except for trees with linear arrangement of nodes therefore plain mst is strongly geared towards provider target similar is steiner tree e g lai et al 2011 but rarely used because of high computational expense 3 3 jungnickel 2013 p 122 reports that improvement over mst cannot exceed 14 for planar graphs due to complexity heuristic algorithms must be used and this minute improvement may be not realized in practice slightly different effect is when mst is computed by a two stage algorithm graphscape and is composed of shortest paths sp then at least the movement between two nodes adjacent in mst actually distanced many patches away is guaranteed to be most effective so this structure may partly reflect mobile agent target of all structures shortest paths sp or a set of quasi shortest paths set of k paths are best for this purpose i would like to note that great deal of connectivity indices indirectly use a set of shortest paths connecting all nodes even if not exactly a textbook structure i include it here to simulate thinking about what is the optimization target in this case table 1 lists a number of more common graph structures classified into three groups according to their ability to reflect provider or mobile agent target the middle category may be coerced to work with both targets depending on context however i am convinced that both should be clearly separated if addressed in single study please note that corridor concept is absent because it has no direct equivalent in graph theory at the end i want to point out ontological aspect of above division structures which are not effective for individual mobile agent may be still effective or meaningful for stochastic mobile agent in trees some agents will traverse parts of the structure optimally and others will traverse other parts optimally as long as they belong to the same category species metapopulation they can collectively achieve optimum and benefit from connectivity for instance migratory patterns were proven to be optimal at the level of whole species by creative use of bi partite graphs somveille et al 2021 this thinking can be also applied to activity over time for sufficiently long time span momentary decisions will turn into stochastic pattern stochastic mobile agent modelling may thus enjoy a wider range of structures to form realistic mobility model connectivity research needs backing from animal movement studies and one of the strains of these is focused on modelling individual behavior and decisions individual based model of heinz and strand 2006 fahrig 2007 palmer et al 2011 lapoint et al 2013 here most attention goes to momentary perception of risks opportunities and gains during the movement of single individual naturally it is difficult to explain more intricate spatial behaviors and uncover optimal decisions in them i argue that this approach cannot advance beyond certain level but much can be done at the next level by matching individual movement patterns with appropriate structures and leveraging wider view of optimality similar thinking may be applied to the problem of omniscience of mobile agents coulon et al 2015 that is alleged and questioned ability to comprehend broad fragments of landscape and find best routes indeed any systematic exploration of graph requires memory and even more structured memory which can be recalled orderly and selectively graph algorithms breadth first depth first searches shortest paths minimum spanning tree maintain and constantly update lists of visited nodes and their currently best solution however we do not have to attribute exactly the same mechanism to animal agents to explain their optimal solutions explorations can be done in unsystematic way and still lead to optimum provided they are performed consecutively over individual s lifetime three mechanisms non oriented oriented and memory based mueller and fagan 2008 may be relevant also for an individual of single species and may play together in sequence so random walks get upgraded to oriented traversals improving navigation has been recognized for many animals op cit additionally individual mobile agent may benefit from interspecific communication also unintentional like olfactory cues and observation to improve its solution after arriving at optimum by any means memory mechanism can be switched on the cost of memorizing already discovered structure will be much lower than memory requirements for full exploration done in algorithmic way and it may be easily within individual s abilities so contrary to omniscience hypothesis there is no need to have complete knowledge to perform successfully at stochastic level i believe it is safe to assume that in stable conditions solutions worked out by animal mobile agents are optimal because all three factors time number of individuals and target function work in the same direction the critique of least cost paths grounded in mechanistic understanding of animal decisions fahrig 2007 misses the stochastic kind of wisdom further reaching conclusion is that connectivity models which pursue optimality with right structures are well fit for the problems 6 graph assemblage with thresholding graph assemblage is a critical step of transforming spatially referenced ecological data into graph structure in pomianowski and solon 2020 we have raised doubts about graph creation based on thresholds that is when graph edges are created whenever distance is lower than certain threshold value which is often based on dispersion distance specific for species this approach originating from urban and keitt 2001 has been followed by a multitude of studies e g fall et al 2007 garcía feced et al 2011 and more examples in dale and fortin 2010 it has been authoritatively proclaimed the only way to introduce graph measures to landscape connectivity by calabrese and fagan 2004 and recent review keeley et al 2021 confirms that it still stands behind many widely used ecological indicators as well as new ones e g saura et al 2017 it is built in the software and even mandatory in models implemented in conefor sensinode consen 2007 apart from analytical applications urban and keitt 2001 p 1210 proposed threshold based components as ready made structures for conservation activities management prioritizing site acquisition or protection the options pro and against thresholding on full and planar graphs were considered by foltête and giraudoux 2012 moilanen 2011 rightfully criticized thresholding as leading to information loss focusing on deficiency in handling the distance super steep distance kernel and inability to serve multiple species with different dispersal kernels it may be helpful to add that unnecessary loss comes also from wasting algorithms power because they were designed to work on continuous data usually distance since their origin the old argument of lower computational demands compared to full graphs seems to be less and less valid over the years to this i add another objection a mismatch with graph theoretic algorithms optimizing nature of shortest paths and minimum spanning tree algorithms calls for unconstrained choice of input edges the algorithms test the usability of any particular edge against other edges in the context of entire graph structure and not single property weight no edge is considered weak until its weakness has been proven this is algorithms wit and strength that cannot be outperformed by hand picking thresholding is kind of pre emptive choice made outside of the algorithm and made without consideration for edge placement within graph structure it cannot be smarter than the algorithm and it has detrimental effect on emerging graph if for instance a threshold is set at median dispersion distance then thresholding excludes roughly half of the connectivity potential from algorithm operation this excluded portion consists of a finite and unknown number of missing edges which could have been be used to conduct movement the overall effect is changing the layout and placement of paths and their artificial elongation graphs treated this way cannot be reliably used to build shortest paths minimum spanning trees and all derived measures this general reservation concerns all graphs altered by manual intervention but how does it work specifically in habitat matrix context thresholding produces components detached graphs considered well connected out of some initial graph fully connected or planar landscape graph this is accomplished be a series of accept reject decisions which are in fact equivalent to link removal made on initial graph depicted on fig 3 is component made from two candidate parts of graph by including the shorter link and rejecting the longer link above threshold it can be seen that if shortest path algorithm is run later on this structure non optimal path will be produced in more complete setting there will be many alternative links removed influencing connectivity for different parts of the component components produced by thresholding are therefore defective incomplete and not delivering the promise of maximum inner connectivity this is why they are not suitable for many commonly used computations including shortest paths structures path enumerations maps shortest paths lengths network distances graph diameter node degree betweenness centrality connectivity indices involving shortest paths iic pc the error originates from shortest paths their lengths and derived measures but is more far reaching because eventually also the number of links is affected and so component patches and their areas the list may be continued up to the final conservation prioritization rankings in studies of scale dependent connectivity a series of thresholds is used to simulate a graduate build up of components which are analyzed by some parameter for instance expected cluster size o brien et al 2006 or graph diameter bunn et al 2000 brooks 2006 neel 2008 resulting plots with distinctive plateaus are interpreted as an evidence of distinct levels of connectivity the idea of systematic thresholding is an improvement over single threshold applied to whole problem but ultimately it also suffers from wrong components with more faithful component construction plateaus could occur elsewhere and it is likely that some would vanish altogether because the buildup of components would be more gradual the same way it does when binary measures change to continuous laita et al 2011 no doubt thresholding exposes some kind of sensitivity of the graph but no proof was provided so far that this sensitivity is in pair with ecological connectivity for this kind of analysis be successful buildup process should be controlled is such a way that components remain optimal all the way along x axis or at least have some well defined ecological property while thresholding sensitivity is an object of interest for some researchers it is practical problem for the others many landscape mosaics will respond abruptly to minimal changes of threshold forming either overly large or too fine components see foltête et al 2016 difficult to process further only one side of this problem mega patches was addressed by cavanaugh et al 2014 it looks like sometimes threshold cannot be stabilized either way resulting in a mix of components of disproportionate sizes exemplary is the resulting graph of minor et al 2009 such troublesome graphs are inappropriate for distance based approach no reasonable metric and resistance based approach too heterogeneous however they are surprisingly often accepted relative success of thresholding to produce a graph is a matter of luck because it depends on actual terrain configuration therefore it is risky to plan it in more extensive workflow where it could undermine further steps when two goals of thresholding are put together strange finding appears so much investigated sensitivity to scale is precisely the reason why thresholding is dependent on the resolution of mosaic raster input in context of graph assemblage this is an artifact which makes resulting graph biased commanding idea of habitat matrix connectivity framework that is to extract meaningful graph components out of surrounding background fails because of too simplistic approach to the problem of graph partitioning there are two solutions one is to abandon the idea that the landscape is divided into disjoint subgraphs and proceed with complete landscape graph pomianowski and solon 2020 seeking for embedded structures the other is to find better methods of partitioning some attempts have been done for instance compartments bodin and norberg 2007 gao et al 2013 and graph theory has more appropriate tools strongly connected components graph cuts still waiting the problem is however that generalized partitioning is computationally tough and it has to resort to heuristic methods see buluç et al 2021 for review so advancing this way within habitat matrix paradigm will be a challenge 7 closing remarks i have shown a common trait in landscape connectivity methodology understanding how optimization works is important for our choice of methods spotting out false assumptions and avoiding intractable models unfortunately for future developments many barriers exist ahead and these cannot be overcome without more obedient approach to graph theory conclusions were presented along the way so at the end i have only two general remarks landscape connectivity research suffers from inconclusiveness i think some of it can be attributed to wrongly defined or operationalized optimization problems many subtleties are hidden in proper formulation and separation of different optimization targets and this paper s value is only in encouraging further discussion the second one is that common attitude to cover conservation and explanation under one umbrella which was sufficient at the early stage should be re evaluated considering differences in optimization targets and methods required declaration of competing interest the author declares that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements i thank professor jerzy solon for reviewing the manuscript 
25623,a river contamination risk rank framework is developed to demonstrate the application of a computational fluid dynamics model in risk assessment of contamination exposure in surface waters the ultimate goal is to identify the factors responsible for potential future river contamination emergencies and the use of that insight to inform strategic investments in resilience enhancing infrastructure and policies the rank model is applied to preliminary assessment of a historical contamination event in the ohio river the results prove that with higher river velocity plume passage becomes faster with earlier peak time and shorter duration of plume at the point of interest for the case under study with increasing the initial spill duration by 100 the plume duration at the point of interest may increase 85 or 65 depending on the toxicity level of the contaminant the sensitivity analysis on hydraulic inputs implies that rank can be utilized for climate informed decision analysis in water quality applications keywords river water quality chemical spills computational fluid dynamics risk assessment ohio river 1 introduction approximately 50 of the world s people live within 3 km of a river kummu et al 2011 and are vulnerable to streamflow extremes flood drought and poor water quality these two problems are linked and likely to worsen with climate change schiedek et al 2007 hrdinka et al 2012 however the impact of streamflow extremes on water quality has not been given adequate attention in the scientific literature to date khan et al 2015 floods mobilize contaminants stored on the floodplain ryberg et al 2014 and overwhelm containment and treatment works droughts lower water levels elevating pollutant concentrations and stagnating flows decreasing river re oxygenation in combination with warming waters droughts increase the likelihood and severity of harmful algal blooms habs dodds et al 2009 among its many current effects on the united states climate change is raising mean temperature and increasing heavy precipitation events wuebbles et al 2017 these changes in combination with evolving flow regulation strategies and watershed development are magnifying floods throughout the united states vogel et al 2011 and europe fischer and knutti 2016 and likely elsewhere ipcc 2013 ipcc 2015 behzadi et al 2020 droughts on the other hand concentrate contaminants desiccate crops and increase competition for water projections for mid century indicate a high likelihood of increased precipitation extremes ipcc 2012 with shifting seasonality and reductions in winter snowpack ipcc 2013 altering the timing and magnitude of peak streamflow outside of the design range of existing infrastructure and their operating rules therefore in the process of reducing risks to riparian communities the compounding effects of streamflow extremes on water quality require careful exploration most of the work done in risk management and adaptive capacity hallegatte 2009 engle 2011 has focused on issues of available water miller et al 2003 arthington et al 2010 lettenmaier et al 1999 or flood risk winsemius et al 2013 kreibich et al 2015 salas et al 2018 karamouz and nazif 2013 zhou et al 2012 in isolation with promising nascent work in tradeoffs between ecological water needs and anthropocentric water supply poff et al 2016 horne et al 2019 john et al 2021 the state of the art in climate change risk management is less mature in other aspects of water resource systems though much progress has been made for example in the fields of computational fluid dynamics cfd and water supply risk assessment including climate change risk independently surprisingly little work has been done to combine the tools of bottom up water resource system risk assessment e g weather generators hydrologic models demographic and land use models economic trade off analysis bayesian networks and contaminant transport advection dispersion and reaction of contaminants in flowing water angevine et al 2014 van griensven and meixner 2006 as has been done in coupled human hydrologic system modeling several studies have investigated the effect of climate change on riverine water quality rostami et al 2018 tu 2009 whitehead et al 2009 tariq et al 2017 wang et al 2018 these studies mostly focused on questions of contaminant or nutrient loading as opposed to transport and give particular attention to temperature effects benitez gilabert et al 2010 these studies examined the influence on water quality of changes to climatic variables by monitoring water quality parameters during a specific time period but were not able to simulate the pollutant fate under changing climate other studies related to climate change impacts on water quality have mostly focused on non point source pollution in agricultural watersheds chang et al 2001 panagopoulos et al 2015 parker et al 2008 struyf et al 2004 wu et al 2012 although a few studies marshall and randhir 2008 nordam et al 2017 have been conducted to assess the impacts of climate change on pollutant transport they have not investigated the effect of climate change and extreme events on the fate of point sources contamination and chemical spills in rivers as such contaminant transport models have not been included in water system modeling chains and water quality concerns have therefore not been directly incorporated into bottom up assessments of broader water system resilience previous approaches to risk assessment or uncertainty analysis in problems of surface water contaminant transport have been almost exclusively of the one dimensional 1d type gimeno et al 2017 hou et al 2014 mcintyre et al 2003 bahadur and samuels 2015 cox et al 2015 rehana and mujumdar 2012 and have not been able to adequately reproduce contaminant plume duration contaminant plume exposure is typically the issue of greatest concern to riparian water utilities and the inability to accurately model it diminishes the usefulness of 1d water quality risk assessment tools higher dimensional cfd models are powerful tools to simulate the advection and dispersion of pollutants in surface waters many researchers have employed numerical models to simulate riverine pollutant transport ani et al 2010 benkhaldoun et al 2007 kachiashvili et al 2007 but have done so without evaluation of water contamination model response to hydro climatological inputs van griensven and meixner van griensven and meixner 2006 found that the major uncertainty in contaminant transport models relates to the form of the model i e how the processes are represented which indicates the importance of the two dimensional 2d cfd formulation and a well parameterized coupled human hydrologic model for generation of cfd inputs additionally previous approaches to climate change risk assessment in contaminant transport problems have tended to evaluate uncertainty in single inputs e g meteorological angevine et al 2014 but a systematic exploration of a multidimensional risk space is needed ray et al 2018 2019 risk assessment in contaminant transport using 2d or three dimensional 3d model formulations have predominated in applications relating to the subsurface gharamti et al 2015 but do not have obvious carry over to riverine problems the standard tools available for 2d or 3d contaminant transport in surface water such as the usepa s water quality analysis simulation program wasp di toro et al 1983 ambrose and wool 2017 and environmental fluid dynamics code efdc tetra tech inc 2002 hamrick 1992 are not appropriate for use in a risk assessment framework though excellent for their purposes they are complex slow and data intensive furthermore they do not easily support batch runs batch runs allow multiple runs of a model with one model call in contrast risk assessment requires many simulations to explore a decision space hence more parsimonious and customizable models than efdc wasp or others like them are needed for large watersheds innovation in contaminant transport modeling is required much progress has been made in the simulation of riverine sediment transport which is largely now conducted in 2d and 3d e g cao et al 2017 hu et al 2012 specific achievements of the sediment modeling community include lock and dam sediment entrainment and release he et al 2018 estimation of bed erosion effects liu and beljadid 2017 and improvements in calculation speed hu et al 2019 these experiments in sediment transport tend to be conducted as historical retrospectives as opposed to risk assessment exercises with the exception of studies such as taner et al 2019 and wild and loucks 2014 that factor sediment accumulation into overall vulnerability assessments of reservoir performance lessons learned from their methods are applied to the risk assessment approach developed in this work in summary the scarcity of studies including contaminant transport models in planning oriented scenario exploration modeling chains brown et al 2012 behzadi et al 2018 has three primary causes 1 existing contaminant transport models have mostly been developed for simulation of singular historical events and are ill fit for inclusion in a risk assessment framework requiring exploration of many scenarios in which variable inputs and even endogenous model process parameters are uncertain 2 most candidate contaminate transport models are of the 1d type but 2d or 3d contaminant behavior is often needed for simulation accuracy and 3 guidance for systematic evaluation of plausible variability in hydraulic and spill characteristics has not been available the present study demonstrates the ability of a generalizable 2d cfd model to provide insight into the risk of contamination plume exposure in excess of a threshold the sensitivity analysis on the hydraulic inputs demonstrates the beginnings of a path forward for climate informed decision analysis brown et al 2012 in water quality applications the ultimate aim is to identify the specific factors responsible for compound events of streamflow extremes flood or drought and river contamination events and the use of that insight to inform strategic staged early planning in resilience enhancing infrastructure and policies this work provides an opportunity for utilities to plan for potential contamination emergencies associated with climate change induced variations in streamflow stress tests should be conducted for water utility systems not only for changes in water quantity but also for the impact those water volume changes might have on riverine contamination the present analytical process presents utilities with a pathway to identification of the conditions likely to result in water system failures due to river contamination so that system vulnerabilities might be reduced the research uses a case study of a 48 km section of the ohio river upstream of cincinnati the developed framework is applied to preliminary assessment of the risk of various ohio river flow velocities and plume attributes on a historical water contamination event 2 methodology 2 1 framework for point of interest pi risk assessment based on uncertainties the river contamination risk rank workflow is a chain of data models and visualizations capable of identifying riverine water quality risks as functions of hydrological and contaminant characteristic indicators fig 1 hydrodynamics and pollutant fate and transport components of rank are configured using a cfd model developed by behzadi et al behzadi et al 2018 behzadi and ray 2018 which routes both the water and contaminants downstream the accuracy and robustness of the model was verified and validated in their work using various analytical and experimental cases digital elevation models dems are the sources of topographical information of the river of interest geometry and bathymetry and are used to generate the computational mesh as an input to the rank model the underlying distribution which governs the historical river flow data is identified through standard statistical procedures and is the first input of the cfd model contaminant characteristics e g plume duration and peak concentration are altered as direct input to the cfd model infrastructure operation policies as well as forecasted data can also be input to the rank algorithm to explore and predict the current and future water quality risks post processing visualizations illustrate pollutant spread with river flow breakthrough curves of pollutants at points of interest pi defined by the user show the time of travel duration and peak concentration of the plume these are the key parameters in health impact evaluations whelton et al 2017 risk assessments and stress tests are conducted through running various scenarios of uncertain parameters and infrastructure management policies the rank model aims to address the limitations of current water quality models by establishing a two dimensional finite volume based model flow and transport equations are time dependent and fully coupled to develop a robust and accurate model the rank model allows for real world uncertainty including climate land use non point source runoff risks and human infrastructure operation the following standard and widely used water quality models are listed in table 1 and compared to the present rank model 1 the riverine spill modeling system rsms was developed to predict the transport of a constituent as a result of a spill of known quantity and duration at a known point on a river or first order tributary of that river the rsms uses the one dimensional branched lagrangian transport model bltm and usace cascade model to estimate the pollutant concentration and to predict plume time of travel leading edge peak and trailing edge grayman et al 2001 2 the hydrologic engineering center s river analysis system hec ras software developed by the us army corps of engineers usace allows for one dimensional sediment transport computations and water quality analyses brunner 2016 3 the water quality analysis simulation program wasp is a dynamic fate and transport model developed by us environmental protection agency usepa which simulates concentration of environmental contaminants in surface waters in one two or three dimensions wasp should be linked with hydrodynamic models to provide flows depths and velocities wasp 2019 4 the environmental fluid dynamics code efdc is a surface water modeling system developed by us environmental protection agency usepa which includes hydrodynamic and contaminant components united states environmental protection agency 2007 see table 1 river water mixes with groundwater as it is diverted along subsurface flow paths bringing with it a myriad of chemical solutes that are transported throughout the shallow streambed by advective and dispersive processes behzadi et al 2021 while the groundwater may affect the pollutant transport for low flow condition of the river in the current study the focus is on the mass transport in surface waters and showing the accuracy and risk assessment capability of the rank framework compared to widely used water quality models 2 2 computational fluid dynamics cfd model the three components of the applied computational algorithm are mathematical modeling grid generation and numerical discretization each component is described in the following sections the riemann flux approximation toro 2009 and a source term balancing method behzadi and newman 2020 behzadi 2016 is applied to develop a well behaved and well balanced numerical scheme 2 2 1 governing system of equations the shallow water equations swes are a set of nonlinear hyperbolic equations well established in water resources management to mathematically describe long wave hydrodynamics of free surface flows when the vertical acceleration of the water particles has a negligible effect on the pressure toro 1992 in this study the two dimensional shallow water equations coupled with the depth averaged scalar transport equation presented in behzadi et al 2018 are applied in the computational algorithm to simulate the flow field and plume passage simultaneously the non dimensional form of the swes system may be written in conservative form as 1 q t f q x g q y s q 0 2 q h h u h v 3 s 0 h h s h s x τ b x h h s h s y τ b y 4 f h u h u 2 1 2 h 2 h s 2 h u v 5 g h v h u v h v 2 1 2 h 2 h s 2 where h and h s are the water depth and the bed level respectively and u and v are velocity components τ bx and τ by denote frictional stresses on the bottom which are defined as the chezy model weiyan 1992 6 c r 1 6 n 7 τ b x g u u 2 v 2 c 2 8 τ b y g v u 2 v 2 c 2 here c is the chezy number n is the manning coefficient r is the hydraulic radius approximated based on the flow depth h and g is the gravitational acceleration constant 2 2 2 grid generation in order to apply the conservation laws and implement the mathematical formulation the pre processing step in computational modeling is the discretization of the geometry of the river of interest into discrete volumes a well constructed mesh significantly improves the accuracy of the solution in the present study an unstructured mesh is generated to determine nodes and triangular elements connectivity unstructured meshes offer flexibility to conform to complex geometries convenient refinement and de refinement and rapid change from small to large elements 2 2 3 numerical discretization among options for numerical scheme finite volume methods have been extensively applied to simulate flow and mass transport mainly due to the mass conservation property and lower memory requirements zhao et al 1996 the finite volume method discretizes directly the integral form of equations mass momentum and transport equations in the present study over an arbitrary fixed domain in this study the spatial domain is divided into triangular cells and a node centered finite volume scheme is applied based on the median dual control volume to evaluate a conservative and well behaved flux at each control volume interface the primitive variable roe scheme is applied using the roe averaged values second order accuracy is used in space and time with a nonlinear implicit scheme based on a newton iterative algorithm for the time integration for an implicit scheme the system of nonlinear equations must be linearized using a newton iterative algorithm dennis and schnabel 1996 and the resulting sparse linear system is solved at each newton iteration using the gauss seidel stationary iterative method saad 2003 the mathematical formulation and the source term balancing scheme utilized in this study satisfies still water equilibrium on the arbitrary bed topography and allows for possible wet and dry interfaces within the solution domain which are essential components in river flow simulations 3 ohio river basin case study after the mississippi river the ohio river is the largest by flow in the united states and it suffers from contamination low water navigation restrictions and flooding the river supports centers of american agriculture energy and industry transports approximately 40 billion worth of goods each year and provides cooling or turbine water for approximately 450 power plants located within the watershed mainstem drum et al 2017 more than 27 million people live within the ohio river watershed 5 million of whom obtain their drinking water from the mainstem drum et al 2017 and in 2016 it was described by the united states environmental protection agency usepa as the most contaminated surface water body in the united states united states environmental protection agency 2016a the contaminants of greatest concern are nutrients and mercury orsanco 2017 though the river also suffers from high levels of legacy organochlorines e g pcbs emerging compounds e g pbdes and pfoa herrick et al 2017 the people who obtain their drinking water directly from the ohio river are subject to frequent contamination events ohio river flow velocity ranges more than an order of magnitude from approximately 0 09 to 3 2 m s climate change effects such as increased evapotranspiration seasonal precipitation shift and concentration of precipitation in already wet months may change the factors dictating river flows and velocities in the future united states environmental protection agency 2016b water quality monitoring and modeling in the ohio river basin is a collaborative effort involving the national weather service hydrologic modeling stream routing and operational river forecasting zhu et al 2021 the us army corps of engineers reservoir and infrastructure modeling and the ohio river valley water sanitation commission contaminant transport modeling in cooperation with the us geological survey and the us environmental protection agency local utilities such as the greater cincinnati water works gcww participate as well and are stakeholders in the operation of a tool called the ohio river community model adams et al 2010 the model is based on one dimensional hec ras routing supplemented with a bltm see previous description of the rsms model requires disjoint stitched model runs at short time horizons and performs poorly in the estimation of contaminant plume duration 3 1 case study of cincinnati gcww cincinnati s public water supply utility provides 416 million liters per day to approximately 1 1 million residents of southwest ohio and northern kentucky eighty percent of gcww water supply is taken directly from the ohio river whitteberry 2019 with the remaining twenty percent extracted from groundwater within the hydrologically distinct miami river basin 50 60 km away gcww maintains offline storage equivalent to approximately 2 days of total demand meaning that in an emergency it could close its ohio river intake and draw from storage for up to approximately 48 h three recent events have demonstrated the risks facing gcww supply integrity which represent the kinds of events that must be expected in the future and against which the water system must be made resilient 1 the freedom industries west virginia 4 methylcyclohexane methanol mchm spill of january 2014 whelton et al 2017 2 a diesel oil spill at a new richmond ohio duke energy generating facility in august 2014 united states environmental protection agency 2014 and 3 the southern towing ammonium nitrate spill caused by a barge hull failure on the benchmark river cincinnati ohio in december 2017 united states environmental protection agency 2017 other spills occur with at least annual regularity with evidence of linkages between historical flood events and microbial and chemical contamination of the river yard et al 2014 while none of these events resulted in water supply interruptions for cincinnati cincinnati has on these and other occasions narrowly avoided disastrous water supply disruptions because of favorable hydrologic conditions these conditions are subject to changing likelihoods in the future it is reasonable to expect that water supply interruptions might have occurred had hydrological and or contaminant conditions been slightly different furthermore cincinnati s relative invulnerability may not apply to other cities drawing water from the ohio river with less offline storage or less robust water treatment infrastructure 4 model validation freedom industries spill of 2014 on january 9 2014 an estimated 37 854 l of crude mchm an organic solvent used in coal processing was released from a freedom industries facility into the elk river a tributary of the kanawha river near charleston west virginia fig 2 a the chemical spill occurred just 1 61 km upstream from an intake to the kanawha valley water treatment plant which left almost 300 000 residents in nine west virginia counties without access to potable water bahadur and samuels 2015 the developed rank model is configured and applied for a 48 km section of the ohio river from meldahl dam to cincinnati including the gcww water intake richard miller treatment plant rmtp the goal is to simulate and reproduce the freedom industries spill of mchm into the ohio river in january 2014 and conduct a stress test on cincinnati s drinking water vulnerability following the spill dems are used to extract the geometry and bathymetry of the river portion of interest which are later employed to generate the computational mesh the required data e g river velocity at meldahl mchm measurements at meldahl beckjord and rmtp river stage and discharge per availability were collected from u s geological survey usgs website u s geological survey 2018 and the water quality and treatment division of gcww fig 2b presents the geometry and the computational grid for the portion of the ohio river under study the two dimensional unstructured mesh is generated once before starting the cfd simulation for the current study a uniform mesh is preferred to obtain the same accuracy throughout the domain however if a higher solution accuracy is required at the point of interest the mesh may be refined near its location the average grid spacing of the computational domain is 40 m resulting in 31 015 computational grid cells and the average total cpu time for simulation runs is approximately 24 h cpu with turbo intel xeon e5 2637 v4 16 3 700 ghz a no slip condition is enforced at side boundaries an inlet boundary condition is imposed at the location of meldahl dam and a free outlet boundary condition is imposed at the cincinnati location the gaussian function squires 2001 is used to estimate the temporal distribution of measured mchm level at meldahl the shape of the gaussian function is determined by two parameters mean μ and standard deviation σ thus the initial concentration at meldahl can be estimated as 9 φ t α σ 2 π e t μ 2 2 σ 2 where φ is the mchm concentration ppb parts per billion t is time in hours since time zero in this case hours after the elk river spill started in charleston west virginia α 220 μ 139 5 and σ 4 5 model settings and parameters are summarized in table 2 the initial condition applied at meldahl and the simulated breakthrough curves at beckjord and rmtp are visually compared to the sample data in fig 3 fig 4 displays three snapshots of the simulated contaminant dispersion in the ohio river from meldahl to cincinnati it shows that the plume arrives at beckjord and later at rmtp after 135 h and 140 h respectively plume arrival is calculated with the threshold of 1 ppb of mchm concentration previous studies of this spill have presented one dimensional models of flow and contaminant transport bahadur and samuels 2015 stolze and volpin 2015 these one dimensional models do not consider the width and stage variations along the river reach thomann and mueller 1987 which limits their application to reaches with constant width and steady water depth moreover the model in bahadur and samuels 2015 is designed to evaluate the contaminant concentration only without simulating the river hydrodynamics such as transient water stage and directional velocities it uses mean flows and velocities from the existing datasets and updates these flows and velocities based on nearby real time gauge readings whereas the present rank model prototyped in fig 2 is two dimensional and capable of simulating longitudinal and transverse velocity components based on variable time of arrival time of peak and duration of the plume at a pi as governed by the transient cfd equations fig 3c is a demonstration of improvements achieved on previous estimation of contaminant transport reproduction at the gcww intake to quantify the agreement between the present model and the observations personal communication wi 2018 plume passage characteristics evaluated at each location are presented in table 3 and the related percent errors are compared in table 4 the rank model estimates the time of peak concentration with errors less than 2 3 at all three locations according to the aforementioned differences in applying the velocity field the error in peak concentration at rmtp is 32 in bahadur and samuels 2015 while it is estimated with 0 95 error using the rank model it is reasonable to assume based on visual inspection of the measured samples at beckjord in fig 3b that the peak concentration may have occurred between two sampling moments and thus the real peak was missed this assumption would lead to the reported error at beckjord calculated in table 4 being interpretable as an overestimate the duration of plume is reproduced using rank at beckjord and rmtp locations with 6 5 and 1 8 errors respectively while bahadur and samuels 2015 shows 152 error in reproduction of the plume duration at rmtp 5 risk assessment for water quality risk assessment a stress test can usefully be conducted on key factors of a contamination event such as flow characteristics and spill duration in this section several scenarios of flow and contaminant characteristics are applied in the simulation of the mchm spill in january 2014 and the results of the stress test are interpreted for their impact on cincinnati s drinking water intake 5 1 flow characteristics one of the principal drivers of pollutant transport in rivers is the flow velocity fischer et al 1979 the annual maximum and minimum stages were obtained from the national weather service national weather service 2018 the annual peak discharge was obtained from hec ssp hec statistical software package which automatically downloads the data from the usgs website for the river portion shown in fig 2 the historical stream velocity data is available for usgs station 03 255 000 located at cincinnati oh a stage discharge velocity relationship is established with the data retrieved from u s geological survey 2018 and the annual maximum velocities and annual minimum stages are presented in fig 5 given that the flow velocity during the historical event in january 2014 was not the peak for that year see fig 5a and the maximum annual velocity in many other historical years was higher than 2014 e g the largest flood in the history of cincinnati occurred in january 1937 the flow velocity in january 2014 could have been much larger with consequences to contaminant transport unlike for high velocity conditions the historical record is a less useful reference for low velocity conditions of the ohio river according to the ohio river valley water sanitation commission orsanco there are twenty one locks and dams constructed on the ohio river mostly after 1959 ohio river valley water s 2021 locks and dams raise minimum stages during the low flow period at the end of each summer see fig 5b and slow river velocities sometimes to near stagnation this is of great benefit to navigation during summer and fall but makes translation of river stage measurements into velocity values during dry periods nearly impossible to investigate the impact of flow velocities that can reasonably be expected to occur along this reach of the ohio river three low flow scenarios as well as three historical floods of cincinnati presented in table 5 are selected for the stress test these specific flows were chosen to include a broad range of velocity magnitude severity level and probability of exceedance the research question is what would happen to contaminant plume duration at the gcww intake if the january 2014 mchm spill had occurred instead during a low flow or high flow period such as the july 1914 and march 1997 periods low velocity values were sampled from pre 1960 records before the impacts of locks and dams on the ohio river became dominant during hot dry summers such as occurred in the ohio river basin in 2015 river velocities could decrease below the 0 31 m s year 1914 minimum value used for this study and exploration of the impact of lock and dam operation on contaminant plume passage is therefore an important subject for future study beyond the scope of the current work the highlighted row of table 5 shows the velocity during the historical mchm spill of january 2014 5 2 contaminant characteristics spill characteristics also indicate variations in the level of risk at the point of interest particularly for chemical spills upstream of water treatment plants plume duration influences the time period during which the treatment plant should be shut down this may result in drinking water shortage to the affected residents therefore to evaluate the potential influences of spill perturbations on water supply interruption the initial plume duration of spill 2014 τ at meldahl is increased by 50 1 5τ and 100 2τ 5 3 bathymetry another factor which may impact the risk level at the point of interest is the bathymetry of the river i e the river bed topology to evaluate the sensitivity of the drinking water supply to the bathymetry the 2014 spill is reproduced with the actual and flat bed topology and the results are compared in the following section 6 results and discussion 6 1 flow and contaminant stress test the selected flow characteristics table 5 combined with the plume characteristics of actual and extended mchm spill of 2014 fig 6 were input into the rank model and the output results are presented in table 6 and fig 7 plume duration is evaluated at two threshold levels of contaminant concentration 0 01 ppb and 1 ppb the concentration of 1 ppb is selected because it represents the average detectable level of mchm concentration according to sampling results personal communication wi 2018 and west virginia testing assessment project wv tap west virginia testing assessment project wvtap 2014 the concentration of 0 01 ppb is chosen solely for the purpose of risk assessment to show that the toxicity threshold and detectable level of contaminant could also influence spill response strategies the stress test results demonstrate that with higher river velocity plume passage becomes faster with earlier peak concentration time and shorter duration of plume at rmtp table 6 for instance when the river velocity is increased by 60 from january 2014 with velocity of 1 6 m s to the flood of 1997 with velocity of 2 57 m s the plume arrives at its peak level at the rmtp location 6 4 h earlier when the river velocity is decreased by 81 from january 2014 with velocity of 1 6 m s to the flow of 1914 with velocity of 0 31 m s the time to peak concentration is delayed by 106 6 h these calculations presume a free flowing if slow river note that the ability of locks and dams to retain river flows during dry periods retarding river flow to the point of stagnation has potential to fundamentally alter these calculations and must be included in future improved versions of contaminant transport hydraulic modeling moreover with the flow of january 2014 and increasing the initial spill duration by 100 the plume duration at rmtp may increase 85 or 65 depending on the toxicity level of the contaminant though the higher velocity results in a shorter shut down period for the treatment plant it leaves the water utility managers with less time to make an initial assessment of the spill and to decide on the emergency response on the other hand lower flow velocity imposes a higher risk on drinking water supply due to the longer period of the plant shut down and limited off line storage of drinking water the simulation results shown in fig 7 indicate that lower river velocity and prolonged initial spill duration impose higher risks on the drinking water supply if contaminant plumes at levels greater than human health standards persist for more than two days gcww s offline storage is exhausted and municipal water supply interruptions to cincinnati are likely to result in particular if the toxicity level of spilled chemical is determined at 0 01 ppb fig 7a and c the drinking water storage is more at risk than at the toxicity level of 1 ppb fig 7b and d fig 7c and d demonstrate the combination of river contaminant conditions with two toxicity levels that may create failure at cincinnati risks to water supply in cincinnati are presented relative to critical threshold of plume passage duration two days at rmtp as a function of the initial spill duration at meldahl and the river velocity in the solid region of the plot the off line storage is sufficient to cover a shutdown of the ohio river intake until the plume passes the water intake whereas the hatched zone indicates conditions leading to potential water supply disruptions if the upstream meldahl contaminant plume passage lasted longer than 1 5τ and the river velocity were lower than 1 6 m s for example then a failure would occur at the threshold level of 0 01 ppb 6 2 bathymetry stress test the previous simulations were carried out with the real bathymetry of the river in this section the sensitivity of the drinking water supply to the river bathymetry is evaluated by comparing the rank results for the real bathymetry and a uniform flat river bed the flow and plume characteristics of mchm spill of 2014 were input to the rank model and the results are presented in table 7 with a flat river bed mchm reaches its peak concentration at rmtp location approximately 8 h earlier compared to the case with the real bathymetry lower plume duration is also detected in the case with a flat topography these results confirm that the accuracy of the model in reproducing spill cases is crucial in assessing the risk of water supply interruption as the plume peak time and duration are the key factors in the water intake shutdown in case of a chemical spill 7 conclusion this study demonstrated an application of a fluid dynamic model rank to water quality risk assessment and its ability in evaluating risks of hydrologic hydraulic change the contribution of the developed model is twofold 1 it allows for variable channel width water depth and velocity which makes it more accurate and flexible than existing tools in application to complex study areas the accuracy of the rank model in reproduction of contaminant plume duration makes the water quality stress test possible 2 it can be used for scenario analysis and stress testing by changing uncertain inputs as initial and boundary conditions including flow velocity non point source pollution infrastructure operating rules biological activity e g harmful algal blooms and temperature effects the model is also capable of applying time dependent velocity variations at every grid cell to address floods and runoff points the rank model is expected to be useful in a wide range of climate change risk assessments to evaluate the water quality impacts of flood and low flow the sensitivity analysis on hydraulic inputs implies that the model can be utilized for climate informed decision analysis brown et al 2012 2019 in water quality applications an initial prototype application of the developed framework was presented in this paper and applied for evaluating the risks of water supply disruption in cincinnati due to a historical chemical spill on the ohio river the next step in advancing rank is to automate the process of the computational mesh generation based on existing digital elevation models for the river of interest future research will use the rank model to evaluate risks to water quality for riparian cities from potential future changes in climate land use and infrastructure operation this will be accomplished by linking rank into a workflow of simulation models including a weather generator preferably one with direct linkages to physical climate processes such as steinschneider et al 2019 a hydrologic model and a reservoir operation model this chain of models will allow careful attention to harmful algal blooms and better understanding of the relative impacts of navigational agricultural and industrial sector impacts on water quality evaluation of the ecological responses to the river pollution could be another extension of the rank model for future studies in addition future research should consider 1 the groundwater contribution in driving the pollutant transport and 2 a potentially evolving bathymetry at its present execution speed the rank model is not easy to apply to long rivers or to many simulations the present study applies rank to a 48 km river reach and to a handful of scenarios of possible future conditions the aim for future applications is to apply rank for long rivers under all scenarios resulting from full factorial combinations of many climate related and non climate uncertainties that may affect river contamination for that purpose rank needs to respond faster by involving multiple computational cores which could be done through updating the existing model using parallel programming at sufficiently fast computational speeds the rank tool might also be suitable for real time operations and disaster response but that purpose is beyond the current scope of study this study has taken a basic approach to climate change stress testing in the interest of demonstrating a proof of concept the impact on water quality at the location of a riverine water intake was explored from delta shifts in long term average precipitation and temperature the magnitude and direction of these delta shifts were informed by the full ensemble of the current generation of general circulation model gcm output produced by the intergovernmental panel on climate change ipcc the delta shifts in long term average conditions were applied to alter the relatively short duration spill event evaluated in this case as new generations of ipcc model output become available the delta shifts should be updated to accommodate evolving understanding of climate change science however exploration of the climate change impacts e g increased evapotranspiration seasonal precipitation shift exacerbation of precipitation extremes on water quality was outside of the scope of the present study but is recommended as a subject for future research such studies are accomplishable with the rank model presented here declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests faranak behzadi reports financial support was provided by ohio water resources center acknowledgements this research was supported by the ohio water resources center ohio state university pte federal award 8293 subaward 60 070 994 the fund was awarded to the first fb and last par authors 
25623,a river contamination risk rank framework is developed to demonstrate the application of a computational fluid dynamics model in risk assessment of contamination exposure in surface waters the ultimate goal is to identify the factors responsible for potential future river contamination emergencies and the use of that insight to inform strategic investments in resilience enhancing infrastructure and policies the rank model is applied to preliminary assessment of a historical contamination event in the ohio river the results prove that with higher river velocity plume passage becomes faster with earlier peak time and shorter duration of plume at the point of interest for the case under study with increasing the initial spill duration by 100 the plume duration at the point of interest may increase 85 or 65 depending on the toxicity level of the contaminant the sensitivity analysis on hydraulic inputs implies that rank can be utilized for climate informed decision analysis in water quality applications keywords river water quality chemical spills computational fluid dynamics risk assessment ohio river 1 introduction approximately 50 of the world s people live within 3 km of a river kummu et al 2011 and are vulnerable to streamflow extremes flood drought and poor water quality these two problems are linked and likely to worsen with climate change schiedek et al 2007 hrdinka et al 2012 however the impact of streamflow extremes on water quality has not been given adequate attention in the scientific literature to date khan et al 2015 floods mobilize contaminants stored on the floodplain ryberg et al 2014 and overwhelm containment and treatment works droughts lower water levels elevating pollutant concentrations and stagnating flows decreasing river re oxygenation in combination with warming waters droughts increase the likelihood and severity of harmful algal blooms habs dodds et al 2009 among its many current effects on the united states climate change is raising mean temperature and increasing heavy precipitation events wuebbles et al 2017 these changes in combination with evolving flow regulation strategies and watershed development are magnifying floods throughout the united states vogel et al 2011 and europe fischer and knutti 2016 and likely elsewhere ipcc 2013 ipcc 2015 behzadi et al 2020 droughts on the other hand concentrate contaminants desiccate crops and increase competition for water projections for mid century indicate a high likelihood of increased precipitation extremes ipcc 2012 with shifting seasonality and reductions in winter snowpack ipcc 2013 altering the timing and magnitude of peak streamflow outside of the design range of existing infrastructure and their operating rules therefore in the process of reducing risks to riparian communities the compounding effects of streamflow extremes on water quality require careful exploration most of the work done in risk management and adaptive capacity hallegatte 2009 engle 2011 has focused on issues of available water miller et al 2003 arthington et al 2010 lettenmaier et al 1999 or flood risk winsemius et al 2013 kreibich et al 2015 salas et al 2018 karamouz and nazif 2013 zhou et al 2012 in isolation with promising nascent work in tradeoffs between ecological water needs and anthropocentric water supply poff et al 2016 horne et al 2019 john et al 2021 the state of the art in climate change risk management is less mature in other aspects of water resource systems though much progress has been made for example in the fields of computational fluid dynamics cfd and water supply risk assessment including climate change risk independently surprisingly little work has been done to combine the tools of bottom up water resource system risk assessment e g weather generators hydrologic models demographic and land use models economic trade off analysis bayesian networks and contaminant transport advection dispersion and reaction of contaminants in flowing water angevine et al 2014 van griensven and meixner 2006 as has been done in coupled human hydrologic system modeling several studies have investigated the effect of climate change on riverine water quality rostami et al 2018 tu 2009 whitehead et al 2009 tariq et al 2017 wang et al 2018 these studies mostly focused on questions of contaminant or nutrient loading as opposed to transport and give particular attention to temperature effects benitez gilabert et al 2010 these studies examined the influence on water quality of changes to climatic variables by monitoring water quality parameters during a specific time period but were not able to simulate the pollutant fate under changing climate other studies related to climate change impacts on water quality have mostly focused on non point source pollution in agricultural watersheds chang et al 2001 panagopoulos et al 2015 parker et al 2008 struyf et al 2004 wu et al 2012 although a few studies marshall and randhir 2008 nordam et al 2017 have been conducted to assess the impacts of climate change on pollutant transport they have not investigated the effect of climate change and extreme events on the fate of point sources contamination and chemical spills in rivers as such contaminant transport models have not been included in water system modeling chains and water quality concerns have therefore not been directly incorporated into bottom up assessments of broader water system resilience previous approaches to risk assessment or uncertainty analysis in problems of surface water contaminant transport have been almost exclusively of the one dimensional 1d type gimeno et al 2017 hou et al 2014 mcintyre et al 2003 bahadur and samuels 2015 cox et al 2015 rehana and mujumdar 2012 and have not been able to adequately reproduce contaminant plume duration contaminant plume exposure is typically the issue of greatest concern to riparian water utilities and the inability to accurately model it diminishes the usefulness of 1d water quality risk assessment tools higher dimensional cfd models are powerful tools to simulate the advection and dispersion of pollutants in surface waters many researchers have employed numerical models to simulate riverine pollutant transport ani et al 2010 benkhaldoun et al 2007 kachiashvili et al 2007 but have done so without evaluation of water contamination model response to hydro climatological inputs van griensven and meixner van griensven and meixner 2006 found that the major uncertainty in contaminant transport models relates to the form of the model i e how the processes are represented which indicates the importance of the two dimensional 2d cfd formulation and a well parameterized coupled human hydrologic model for generation of cfd inputs additionally previous approaches to climate change risk assessment in contaminant transport problems have tended to evaluate uncertainty in single inputs e g meteorological angevine et al 2014 but a systematic exploration of a multidimensional risk space is needed ray et al 2018 2019 risk assessment in contaminant transport using 2d or three dimensional 3d model formulations have predominated in applications relating to the subsurface gharamti et al 2015 but do not have obvious carry over to riverine problems the standard tools available for 2d or 3d contaminant transport in surface water such as the usepa s water quality analysis simulation program wasp di toro et al 1983 ambrose and wool 2017 and environmental fluid dynamics code efdc tetra tech inc 2002 hamrick 1992 are not appropriate for use in a risk assessment framework though excellent for their purposes they are complex slow and data intensive furthermore they do not easily support batch runs batch runs allow multiple runs of a model with one model call in contrast risk assessment requires many simulations to explore a decision space hence more parsimonious and customizable models than efdc wasp or others like them are needed for large watersheds innovation in contaminant transport modeling is required much progress has been made in the simulation of riverine sediment transport which is largely now conducted in 2d and 3d e g cao et al 2017 hu et al 2012 specific achievements of the sediment modeling community include lock and dam sediment entrainment and release he et al 2018 estimation of bed erosion effects liu and beljadid 2017 and improvements in calculation speed hu et al 2019 these experiments in sediment transport tend to be conducted as historical retrospectives as opposed to risk assessment exercises with the exception of studies such as taner et al 2019 and wild and loucks 2014 that factor sediment accumulation into overall vulnerability assessments of reservoir performance lessons learned from their methods are applied to the risk assessment approach developed in this work in summary the scarcity of studies including contaminant transport models in planning oriented scenario exploration modeling chains brown et al 2012 behzadi et al 2018 has three primary causes 1 existing contaminant transport models have mostly been developed for simulation of singular historical events and are ill fit for inclusion in a risk assessment framework requiring exploration of many scenarios in which variable inputs and even endogenous model process parameters are uncertain 2 most candidate contaminate transport models are of the 1d type but 2d or 3d contaminant behavior is often needed for simulation accuracy and 3 guidance for systematic evaluation of plausible variability in hydraulic and spill characteristics has not been available the present study demonstrates the ability of a generalizable 2d cfd model to provide insight into the risk of contamination plume exposure in excess of a threshold the sensitivity analysis on the hydraulic inputs demonstrates the beginnings of a path forward for climate informed decision analysis brown et al 2012 in water quality applications the ultimate aim is to identify the specific factors responsible for compound events of streamflow extremes flood or drought and river contamination events and the use of that insight to inform strategic staged early planning in resilience enhancing infrastructure and policies this work provides an opportunity for utilities to plan for potential contamination emergencies associated with climate change induced variations in streamflow stress tests should be conducted for water utility systems not only for changes in water quantity but also for the impact those water volume changes might have on riverine contamination the present analytical process presents utilities with a pathway to identification of the conditions likely to result in water system failures due to river contamination so that system vulnerabilities might be reduced the research uses a case study of a 48 km section of the ohio river upstream of cincinnati the developed framework is applied to preliminary assessment of the risk of various ohio river flow velocities and plume attributes on a historical water contamination event 2 methodology 2 1 framework for point of interest pi risk assessment based on uncertainties the river contamination risk rank workflow is a chain of data models and visualizations capable of identifying riverine water quality risks as functions of hydrological and contaminant characteristic indicators fig 1 hydrodynamics and pollutant fate and transport components of rank are configured using a cfd model developed by behzadi et al behzadi et al 2018 behzadi and ray 2018 which routes both the water and contaminants downstream the accuracy and robustness of the model was verified and validated in their work using various analytical and experimental cases digital elevation models dems are the sources of topographical information of the river of interest geometry and bathymetry and are used to generate the computational mesh as an input to the rank model the underlying distribution which governs the historical river flow data is identified through standard statistical procedures and is the first input of the cfd model contaminant characteristics e g plume duration and peak concentration are altered as direct input to the cfd model infrastructure operation policies as well as forecasted data can also be input to the rank algorithm to explore and predict the current and future water quality risks post processing visualizations illustrate pollutant spread with river flow breakthrough curves of pollutants at points of interest pi defined by the user show the time of travel duration and peak concentration of the plume these are the key parameters in health impact evaluations whelton et al 2017 risk assessments and stress tests are conducted through running various scenarios of uncertain parameters and infrastructure management policies the rank model aims to address the limitations of current water quality models by establishing a two dimensional finite volume based model flow and transport equations are time dependent and fully coupled to develop a robust and accurate model the rank model allows for real world uncertainty including climate land use non point source runoff risks and human infrastructure operation the following standard and widely used water quality models are listed in table 1 and compared to the present rank model 1 the riverine spill modeling system rsms was developed to predict the transport of a constituent as a result of a spill of known quantity and duration at a known point on a river or first order tributary of that river the rsms uses the one dimensional branched lagrangian transport model bltm and usace cascade model to estimate the pollutant concentration and to predict plume time of travel leading edge peak and trailing edge grayman et al 2001 2 the hydrologic engineering center s river analysis system hec ras software developed by the us army corps of engineers usace allows for one dimensional sediment transport computations and water quality analyses brunner 2016 3 the water quality analysis simulation program wasp is a dynamic fate and transport model developed by us environmental protection agency usepa which simulates concentration of environmental contaminants in surface waters in one two or three dimensions wasp should be linked with hydrodynamic models to provide flows depths and velocities wasp 2019 4 the environmental fluid dynamics code efdc is a surface water modeling system developed by us environmental protection agency usepa which includes hydrodynamic and contaminant components united states environmental protection agency 2007 see table 1 river water mixes with groundwater as it is diverted along subsurface flow paths bringing with it a myriad of chemical solutes that are transported throughout the shallow streambed by advective and dispersive processes behzadi et al 2021 while the groundwater may affect the pollutant transport for low flow condition of the river in the current study the focus is on the mass transport in surface waters and showing the accuracy and risk assessment capability of the rank framework compared to widely used water quality models 2 2 computational fluid dynamics cfd model the three components of the applied computational algorithm are mathematical modeling grid generation and numerical discretization each component is described in the following sections the riemann flux approximation toro 2009 and a source term balancing method behzadi and newman 2020 behzadi 2016 is applied to develop a well behaved and well balanced numerical scheme 2 2 1 governing system of equations the shallow water equations swes are a set of nonlinear hyperbolic equations well established in water resources management to mathematically describe long wave hydrodynamics of free surface flows when the vertical acceleration of the water particles has a negligible effect on the pressure toro 1992 in this study the two dimensional shallow water equations coupled with the depth averaged scalar transport equation presented in behzadi et al 2018 are applied in the computational algorithm to simulate the flow field and plume passage simultaneously the non dimensional form of the swes system may be written in conservative form as 1 q t f q x g q y s q 0 2 q h h u h v 3 s 0 h h s h s x τ b x h h s h s y τ b y 4 f h u h u 2 1 2 h 2 h s 2 h u v 5 g h v h u v h v 2 1 2 h 2 h s 2 where h and h s are the water depth and the bed level respectively and u and v are velocity components τ bx and τ by denote frictional stresses on the bottom which are defined as the chezy model weiyan 1992 6 c r 1 6 n 7 τ b x g u u 2 v 2 c 2 8 τ b y g v u 2 v 2 c 2 here c is the chezy number n is the manning coefficient r is the hydraulic radius approximated based on the flow depth h and g is the gravitational acceleration constant 2 2 2 grid generation in order to apply the conservation laws and implement the mathematical formulation the pre processing step in computational modeling is the discretization of the geometry of the river of interest into discrete volumes a well constructed mesh significantly improves the accuracy of the solution in the present study an unstructured mesh is generated to determine nodes and triangular elements connectivity unstructured meshes offer flexibility to conform to complex geometries convenient refinement and de refinement and rapid change from small to large elements 2 2 3 numerical discretization among options for numerical scheme finite volume methods have been extensively applied to simulate flow and mass transport mainly due to the mass conservation property and lower memory requirements zhao et al 1996 the finite volume method discretizes directly the integral form of equations mass momentum and transport equations in the present study over an arbitrary fixed domain in this study the spatial domain is divided into triangular cells and a node centered finite volume scheme is applied based on the median dual control volume to evaluate a conservative and well behaved flux at each control volume interface the primitive variable roe scheme is applied using the roe averaged values second order accuracy is used in space and time with a nonlinear implicit scheme based on a newton iterative algorithm for the time integration for an implicit scheme the system of nonlinear equations must be linearized using a newton iterative algorithm dennis and schnabel 1996 and the resulting sparse linear system is solved at each newton iteration using the gauss seidel stationary iterative method saad 2003 the mathematical formulation and the source term balancing scheme utilized in this study satisfies still water equilibrium on the arbitrary bed topography and allows for possible wet and dry interfaces within the solution domain which are essential components in river flow simulations 3 ohio river basin case study after the mississippi river the ohio river is the largest by flow in the united states and it suffers from contamination low water navigation restrictions and flooding the river supports centers of american agriculture energy and industry transports approximately 40 billion worth of goods each year and provides cooling or turbine water for approximately 450 power plants located within the watershed mainstem drum et al 2017 more than 27 million people live within the ohio river watershed 5 million of whom obtain their drinking water from the mainstem drum et al 2017 and in 2016 it was described by the united states environmental protection agency usepa as the most contaminated surface water body in the united states united states environmental protection agency 2016a the contaminants of greatest concern are nutrients and mercury orsanco 2017 though the river also suffers from high levels of legacy organochlorines e g pcbs emerging compounds e g pbdes and pfoa herrick et al 2017 the people who obtain their drinking water directly from the ohio river are subject to frequent contamination events ohio river flow velocity ranges more than an order of magnitude from approximately 0 09 to 3 2 m s climate change effects such as increased evapotranspiration seasonal precipitation shift and concentration of precipitation in already wet months may change the factors dictating river flows and velocities in the future united states environmental protection agency 2016b water quality monitoring and modeling in the ohio river basin is a collaborative effort involving the national weather service hydrologic modeling stream routing and operational river forecasting zhu et al 2021 the us army corps of engineers reservoir and infrastructure modeling and the ohio river valley water sanitation commission contaminant transport modeling in cooperation with the us geological survey and the us environmental protection agency local utilities such as the greater cincinnati water works gcww participate as well and are stakeholders in the operation of a tool called the ohio river community model adams et al 2010 the model is based on one dimensional hec ras routing supplemented with a bltm see previous description of the rsms model requires disjoint stitched model runs at short time horizons and performs poorly in the estimation of contaminant plume duration 3 1 case study of cincinnati gcww cincinnati s public water supply utility provides 416 million liters per day to approximately 1 1 million residents of southwest ohio and northern kentucky eighty percent of gcww water supply is taken directly from the ohio river whitteberry 2019 with the remaining twenty percent extracted from groundwater within the hydrologically distinct miami river basin 50 60 km away gcww maintains offline storage equivalent to approximately 2 days of total demand meaning that in an emergency it could close its ohio river intake and draw from storage for up to approximately 48 h three recent events have demonstrated the risks facing gcww supply integrity which represent the kinds of events that must be expected in the future and against which the water system must be made resilient 1 the freedom industries west virginia 4 methylcyclohexane methanol mchm spill of january 2014 whelton et al 2017 2 a diesel oil spill at a new richmond ohio duke energy generating facility in august 2014 united states environmental protection agency 2014 and 3 the southern towing ammonium nitrate spill caused by a barge hull failure on the benchmark river cincinnati ohio in december 2017 united states environmental protection agency 2017 other spills occur with at least annual regularity with evidence of linkages between historical flood events and microbial and chemical contamination of the river yard et al 2014 while none of these events resulted in water supply interruptions for cincinnati cincinnati has on these and other occasions narrowly avoided disastrous water supply disruptions because of favorable hydrologic conditions these conditions are subject to changing likelihoods in the future it is reasonable to expect that water supply interruptions might have occurred had hydrological and or contaminant conditions been slightly different furthermore cincinnati s relative invulnerability may not apply to other cities drawing water from the ohio river with less offline storage or less robust water treatment infrastructure 4 model validation freedom industries spill of 2014 on january 9 2014 an estimated 37 854 l of crude mchm an organic solvent used in coal processing was released from a freedom industries facility into the elk river a tributary of the kanawha river near charleston west virginia fig 2 a the chemical spill occurred just 1 61 km upstream from an intake to the kanawha valley water treatment plant which left almost 300 000 residents in nine west virginia counties without access to potable water bahadur and samuels 2015 the developed rank model is configured and applied for a 48 km section of the ohio river from meldahl dam to cincinnati including the gcww water intake richard miller treatment plant rmtp the goal is to simulate and reproduce the freedom industries spill of mchm into the ohio river in january 2014 and conduct a stress test on cincinnati s drinking water vulnerability following the spill dems are used to extract the geometry and bathymetry of the river portion of interest which are later employed to generate the computational mesh the required data e g river velocity at meldahl mchm measurements at meldahl beckjord and rmtp river stage and discharge per availability were collected from u s geological survey usgs website u s geological survey 2018 and the water quality and treatment division of gcww fig 2b presents the geometry and the computational grid for the portion of the ohio river under study the two dimensional unstructured mesh is generated once before starting the cfd simulation for the current study a uniform mesh is preferred to obtain the same accuracy throughout the domain however if a higher solution accuracy is required at the point of interest the mesh may be refined near its location the average grid spacing of the computational domain is 40 m resulting in 31 015 computational grid cells and the average total cpu time for simulation runs is approximately 24 h cpu with turbo intel xeon e5 2637 v4 16 3 700 ghz a no slip condition is enforced at side boundaries an inlet boundary condition is imposed at the location of meldahl dam and a free outlet boundary condition is imposed at the cincinnati location the gaussian function squires 2001 is used to estimate the temporal distribution of measured mchm level at meldahl the shape of the gaussian function is determined by two parameters mean μ and standard deviation σ thus the initial concentration at meldahl can be estimated as 9 φ t α σ 2 π e t μ 2 2 σ 2 where φ is the mchm concentration ppb parts per billion t is time in hours since time zero in this case hours after the elk river spill started in charleston west virginia α 220 μ 139 5 and σ 4 5 model settings and parameters are summarized in table 2 the initial condition applied at meldahl and the simulated breakthrough curves at beckjord and rmtp are visually compared to the sample data in fig 3 fig 4 displays three snapshots of the simulated contaminant dispersion in the ohio river from meldahl to cincinnati it shows that the plume arrives at beckjord and later at rmtp after 135 h and 140 h respectively plume arrival is calculated with the threshold of 1 ppb of mchm concentration previous studies of this spill have presented one dimensional models of flow and contaminant transport bahadur and samuels 2015 stolze and volpin 2015 these one dimensional models do not consider the width and stage variations along the river reach thomann and mueller 1987 which limits their application to reaches with constant width and steady water depth moreover the model in bahadur and samuels 2015 is designed to evaluate the contaminant concentration only without simulating the river hydrodynamics such as transient water stage and directional velocities it uses mean flows and velocities from the existing datasets and updates these flows and velocities based on nearby real time gauge readings whereas the present rank model prototyped in fig 2 is two dimensional and capable of simulating longitudinal and transverse velocity components based on variable time of arrival time of peak and duration of the plume at a pi as governed by the transient cfd equations fig 3c is a demonstration of improvements achieved on previous estimation of contaminant transport reproduction at the gcww intake to quantify the agreement between the present model and the observations personal communication wi 2018 plume passage characteristics evaluated at each location are presented in table 3 and the related percent errors are compared in table 4 the rank model estimates the time of peak concentration with errors less than 2 3 at all three locations according to the aforementioned differences in applying the velocity field the error in peak concentration at rmtp is 32 in bahadur and samuels 2015 while it is estimated with 0 95 error using the rank model it is reasonable to assume based on visual inspection of the measured samples at beckjord in fig 3b that the peak concentration may have occurred between two sampling moments and thus the real peak was missed this assumption would lead to the reported error at beckjord calculated in table 4 being interpretable as an overestimate the duration of plume is reproduced using rank at beckjord and rmtp locations with 6 5 and 1 8 errors respectively while bahadur and samuels 2015 shows 152 error in reproduction of the plume duration at rmtp 5 risk assessment for water quality risk assessment a stress test can usefully be conducted on key factors of a contamination event such as flow characteristics and spill duration in this section several scenarios of flow and contaminant characteristics are applied in the simulation of the mchm spill in january 2014 and the results of the stress test are interpreted for their impact on cincinnati s drinking water intake 5 1 flow characteristics one of the principal drivers of pollutant transport in rivers is the flow velocity fischer et al 1979 the annual maximum and minimum stages were obtained from the national weather service national weather service 2018 the annual peak discharge was obtained from hec ssp hec statistical software package which automatically downloads the data from the usgs website for the river portion shown in fig 2 the historical stream velocity data is available for usgs station 03 255 000 located at cincinnati oh a stage discharge velocity relationship is established with the data retrieved from u s geological survey 2018 and the annual maximum velocities and annual minimum stages are presented in fig 5 given that the flow velocity during the historical event in january 2014 was not the peak for that year see fig 5a and the maximum annual velocity in many other historical years was higher than 2014 e g the largest flood in the history of cincinnati occurred in january 1937 the flow velocity in january 2014 could have been much larger with consequences to contaminant transport unlike for high velocity conditions the historical record is a less useful reference for low velocity conditions of the ohio river according to the ohio river valley water sanitation commission orsanco there are twenty one locks and dams constructed on the ohio river mostly after 1959 ohio river valley water s 2021 locks and dams raise minimum stages during the low flow period at the end of each summer see fig 5b and slow river velocities sometimes to near stagnation this is of great benefit to navigation during summer and fall but makes translation of river stage measurements into velocity values during dry periods nearly impossible to investigate the impact of flow velocities that can reasonably be expected to occur along this reach of the ohio river three low flow scenarios as well as three historical floods of cincinnati presented in table 5 are selected for the stress test these specific flows were chosen to include a broad range of velocity magnitude severity level and probability of exceedance the research question is what would happen to contaminant plume duration at the gcww intake if the january 2014 mchm spill had occurred instead during a low flow or high flow period such as the july 1914 and march 1997 periods low velocity values were sampled from pre 1960 records before the impacts of locks and dams on the ohio river became dominant during hot dry summers such as occurred in the ohio river basin in 2015 river velocities could decrease below the 0 31 m s year 1914 minimum value used for this study and exploration of the impact of lock and dam operation on contaminant plume passage is therefore an important subject for future study beyond the scope of the current work the highlighted row of table 5 shows the velocity during the historical mchm spill of january 2014 5 2 contaminant characteristics spill characteristics also indicate variations in the level of risk at the point of interest particularly for chemical spills upstream of water treatment plants plume duration influences the time period during which the treatment plant should be shut down this may result in drinking water shortage to the affected residents therefore to evaluate the potential influences of spill perturbations on water supply interruption the initial plume duration of spill 2014 τ at meldahl is increased by 50 1 5τ and 100 2τ 5 3 bathymetry another factor which may impact the risk level at the point of interest is the bathymetry of the river i e the river bed topology to evaluate the sensitivity of the drinking water supply to the bathymetry the 2014 spill is reproduced with the actual and flat bed topology and the results are compared in the following section 6 results and discussion 6 1 flow and contaminant stress test the selected flow characteristics table 5 combined with the plume characteristics of actual and extended mchm spill of 2014 fig 6 were input into the rank model and the output results are presented in table 6 and fig 7 plume duration is evaluated at two threshold levels of contaminant concentration 0 01 ppb and 1 ppb the concentration of 1 ppb is selected because it represents the average detectable level of mchm concentration according to sampling results personal communication wi 2018 and west virginia testing assessment project wv tap west virginia testing assessment project wvtap 2014 the concentration of 0 01 ppb is chosen solely for the purpose of risk assessment to show that the toxicity threshold and detectable level of contaminant could also influence spill response strategies the stress test results demonstrate that with higher river velocity plume passage becomes faster with earlier peak concentration time and shorter duration of plume at rmtp table 6 for instance when the river velocity is increased by 60 from january 2014 with velocity of 1 6 m s to the flood of 1997 with velocity of 2 57 m s the plume arrives at its peak level at the rmtp location 6 4 h earlier when the river velocity is decreased by 81 from january 2014 with velocity of 1 6 m s to the flow of 1914 with velocity of 0 31 m s the time to peak concentration is delayed by 106 6 h these calculations presume a free flowing if slow river note that the ability of locks and dams to retain river flows during dry periods retarding river flow to the point of stagnation has potential to fundamentally alter these calculations and must be included in future improved versions of contaminant transport hydraulic modeling moreover with the flow of january 2014 and increasing the initial spill duration by 100 the plume duration at rmtp may increase 85 or 65 depending on the toxicity level of the contaminant though the higher velocity results in a shorter shut down period for the treatment plant it leaves the water utility managers with less time to make an initial assessment of the spill and to decide on the emergency response on the other hand lower flow velocity imposes a higher risk on drinking water supply due to the longer period of the plant shut down and limited off line storage of drinking water the simulation results shown in fig 7 indicate that lower river velocity and prolonged initial spill duration impose higher risks on the drinking water supply if contaminant plumes at levels greater than human health standards persist for more than two days gcww s offline storage is exhausted and municipal water supply interruptions to cincinnati are likely to result in particular if the toxicity level of spilled chemical is determined at 0 01 ppb fig 7a and c the drinking water storage is more at risk than at the toxicity level of 1 ppb fig 7b and d fig 7c and d demonstrate the combination of river contaminant conditions with two toxicity levels that may create failure at cincinnati risks to water supply in cincinnati are presented relative to critical threshold of plume passage duration two days at rmtp as a function of the initial spill duration at meldahl and the river velocity in the solid region of the plot the off line storage is sufficient to cover a shutdown of the ohio river intake until the plume passes the water intake whereas the hatched zone indicates conditions leading to potential water supply disruptions if the upstream meldahl contaminant plume passage lasted longer than 1 5τ and the river velocity were lower than 1 6 m s for example then a failure would occur at the threshold level of 0 01 ppb 6 2 bathymetry stress test the previous simulations were carried out with the real bathymetry of the river in this section the sensitivity of the drinking water supply to the river bathymetry is evaluated by comparing the rank results for the real bathymetry and a uniform flat river bed the flow and plume characteristics of mchm spill of 2014 were input to the rank model and the results are presented in table 7 with a flat river bed mchm reaches its peak concentration at rmtp location approximately 8 h earlier compared to the case with the real bathymetry lower plume duration is also detected in the case with a flat topography these results confirm that the accuracy of the model in reproducing spill cases is crucial in assessing the risk of water supply interruption as the plume peak time and duration are the key factors in the water intake shutdown in case of a chemical spill 7 conclusion this study demonstrated an application of a fluid dynamic model rank to water quality risk assessment and its ability in evaluating risks of hydrologic hydraulic change the contribution of the developed model is twofold 1 it allows for variable channel width water depth and velocity which makes it more accurate and flexible than existing tools in application to complex study areas the accuracy of the rank model in reproduction of contaminant plume duration makes the water quality stress test possible 2 it can be used for scenario analysis and stress testing by changing uncertain inputs as initial and boundary conditions including flow velocity non point source pollution infrastructure operating rules biological activity e g harmful algal blooms and temperature effects the model is also capable of applying time dependent velocity variations at every grid cell to address floods and runoff points the rank model is expected to be useful in a wide range of climate change risk assessments to evaluate the water quality impacts of flood and low flow the sensitivity analysis on hydraulic inputs implies that the model can be utilized for climate informed decision analysis brown et al 2012 2019 in water quality applications an initial prototype application of the developed framework was presented in this paper and applied for evaluating the risks of water supply disruption in cincinnati due to a historical chemical spill on the ohio river the next step in advancing rank is to automate the process of the computational mesh generation based on existing digital elevation models for the river of interest future research will use the rank model to evaluate risks to water quality for riparian cities from potential future changes in climate land use and infrastructure operation this will be accomplished by linking rank into a workflow of simulation models including a weather generator preferably one with direct linkages to physical climate processes such as steinschneider et al 2019 a hydrologic model and a reservoir operation model this chain of models will allow careful attention to harmful algal blooms and better understanding of the relative impacts of navigational agricultural and industrial sector impacts on water quality evaluation of the ecological responses to the river pollution could be another extension of the rank model for future studies in addition future research should consider 1 the groundwater contribution in driving the pollutant transport and 2 a potentially evolving bathymetry at its present execution speed the rank model is not easy to apply to long rivers or to many simulations the present study applies rank to a 48 km river reach and to a handful of scenarios of possible future conditions the aim for future applications is to apply rank for long rivers under all scenarios resulting from full factorial combinations of many climate related and non climate uncertainties that may affect river contamination for that purpose rank needs to respond faster by involving multiple computational cores which could be done through updating the existing model using parallel programming at sufficiently fast computational speeds the rank tool might also be suitable for real time operations and disaster response but that purpose is beyond the current scope of study this study has taken a basic approach to climate change stress testing in the interest of demonstrating a proof of concept the impact on water quality at the location of a riverine water intake was explored from delta shifts in long term average precipitation and temperature the magnitude and direction of these delta shifts were informed by the full ensemble of the current generation of general circulation model gcm output produced by the intergovernmental panel on climate change ipcc the delta shifts in long term average conditions were applied to alter the relatively short duration spill event evaluated in this case as new generations of ipcc model output become available the delta shifts should be updated to accommodate evolving understanding of climate change science however exploration of the climate change impacts e g increased evapotranspiration seasonal precipitation shift exacerbation of precipitation extremes on water quality was outside of the scope of the present study but is recommended as a subject for future research such studies are accomplishable with the rank model presented here declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests faranak behzadi reports financial support was provided by ohio water resources center acknowledgements this research was supported by the ohio water resources center ohio state university pte federal award 8293 subaward 60 070 994 the fund was awarded to the first fb and last par authors 
25624,this study investigated whether a simple model could scale across watersheds and effectively predict runoff driven nutrient loading as compared to a model with more complex process representation a lumped model the spreadsheet tool for estimating pollutant load stepl was adapted to use gridded data steplgrid and applied to 112 coastal watersheds across the atlantic gulf and pacific coasts of the contiguous united states u s to estimate annual runoff driven total nitrogen tn and total phosphorus tp loads steplgrid outputs were compared to those of the spatially referenced regression on watershed attributes sparrow model relative to sparrow steplgrid produced comparable estimates of tn and tp loads for most watersheds studied and its predicted loads were more similar to sparrow for tn than tp steplgrid was particularly effective at rank ordering watersheds by tn and tp loads as compared to sparrow indicating that steplgrid was useful for relative comparisons across diverse watersheds keywords nonpoint source pollution nitrogen phosphorus water quality modeling eutrophication sparrow stepl 1 introduction estuaries are among the most productive ecosystems globally and provide a wide range of ecosystem services barbier et al 2011 caffrey 2004 cloern et al 2014 freshwater inputs from upstream lands drive estuarine health and functioning and result in estuarine systems integrating the conditions of their contributing watersheds benson 1981 palmer et al 2011 watersheds that drain to estuaries are among the most densely populated and the stress generated by anthropogenic activities in these watersheds has caused severe disturbances to estuarine ecosystems including increased nutrient loads habitat loss and changes in sediment delivery carpenter et al 1998 dauer et al 2000 foley et al 2005 mallin et al 2000 vitousek et al 1997 coastal areas are particularly vulnerable to eutrophication national research council 2000 and previous national estuarine eutrophication assessments in the united states u s have shown that over two thirds of the nation s estuarine systems are moderately to highly eutrophic bricker 1999 bricker et al 2008 point sources e g wastewater treatment significantly contribute to nutrient pollution but nonpoint sources e g urban and agricultural runoff fossil fuel combustion are considered the primary drivers of estuarine eutrophication carpenter et al 1998 with riverine inputs being one of the main transport mechanisms delivering nutrients to the coasts beusen et al 2016 national research council 2000 seitzinger et al 2005 2010 sharples et al 2017 in addition to local pressure global environmental change is likely to alter coastal systems by increasing nutrient inputs aggravated by changes in hydrological regimes and precipitation patterns murdoch et al 2000 whitehead et al 2009 a wide range of surface water quality models have been developed and can be used to assess the impact of environmental change on water resources these models range in structure assumptions and complexity of process representation wang et al 2013 the choice of water quality model is dictated not only by a model s suitability for a given application but also by data availability modeler expertise and training and computational constraints a previous study wellen et al 2015 reviewed 257 modelling studies and highlighted that only 5 of 41 water quality models reviewed were among the most commonly employed such as the soil and water assessment tool swat gassman et al 2014 integrated catchment model inca wade et al 2002 whitehead et al 1998 agricultural nonpoint source pollution model annual agricultural nonpoint source pollution model angps annagnps binger and theurer 2001 young et al 1989 hydrological simulation program fortran hspf bicknell et al 2001 donigian et al 1995 and hydrologiska byråns vattenbalansavdelning hbv andersson et al 2005 a recent review also confirmed the predominance of these water quality models and showed that 44 of articles of 3 282 reviewed used swat fu et al 2019 these commonly used mechanistic and highly specified models are generally viewed as being state of the art given that they represent many processes known to drive water quality dynamics robson 2014 robson et al 2018 complex process based models offer insight into the roles of key environmental processes in driving nutrient pollution and help to evaluate the effectiveness of proposed conservation management strategies and policies national research council 2007 however these models demand data intensive calibration and computational resources schwarz et al 2006 and commonly require the selection of over 50 parameters jackson blake et al 2017 that are often unmeasurable without sufficient data and an in depth understanding of a study system there is limited potential to constrain parameters effectively during calibration and follow best practices in modelling accordingly a review led by wellen et al 2015 revealed that few modelling studies reported a validation assessment i e 57 sensitivity analysis i e 25 and optimization i e 17 during model development although complex models provide useful information of a system s dynamics and drivers their limitations have led to the development of user friendly parsimonious models that are minimally parameterized and computationally efficient several parsimonious water quality models have been developed recently such as spatially referenced regression on watershed attributes sparrow schwarz et al 2006 generalized watershed loading function gwlf lehning et al 2002 gis pollutant load application pload usepa 2001 long term hydrologic impact assessment l thia engel 2003 spreadsheet tool for estimating pollutant load stepl tetra tech inc 2011 and waquoit bay land margin ecosystems research nitrogen loading model wblmer nlm valiela et al 1997 here we define parsimonious models as models that accomplish a desired level of explanation or prediction with a minimum number of parameters or predictor variables these models have a lower number of parameters compared to more complex models e g 148 parameters for the complex model inca p vs 28 parameters for the parsimonious model simplep jackson blake et al 2017 therefore parsimonious models require fewer resources than complex models nevertheless not all models described as parsimonious have the same level of complexity some models are export coefficient based models e g stepl pload l thia while others combine statistical approaches and or mechanistic processes e g sparrow gwlf a few models have been applied at large spatial scales such as sparrow preston et al 2011 and global nutrient export from watersheds news mccrackin et al 2013 however water quality modeling challenges are exacerbated when applied at large spatial scales spatial heterogeneity sparse measurements for calibration and limited human and computational resources all limit large scale modeling fu et al 2019 therefore most existing research on the use of parsimonious nutrient loading models has focused on applications at small spatial scales jackson blake et al 2017 liu et al 2017 nejadhashemi et al 2011 leaving gaps in our understanding of the conditions in which simpler and less computationally expensive approaches may be effective for estimating watershed scale nutrient loads in freshwater runoff and evaluating the impact of different modes of change e g policies global to regional environmental drivers climate change across multiple watersheds elliott et al 2016 while some studies only require assessment of a single or few watersheds the ability to evaluate many watersheds simultaneously is increasingly becoming important in the context of climate change assessments determining nutrient load inputs at large spatial scales is necessary for developing or adapting management strategies that extend across regional or national boundaries especially to predict impacts of future change and identify systems at a higher risk of degradation due to local change e g land use conversion and global climate change however the use of future projections as inputs to water quality models increases the difficulty of applying these models at large spatial scales modeling future nutrient loads requires using several projections that correspond to different scenarios e g representative concentration pathways 4 5 and 8 5 and general circulation models e g those from the coupled model intercomparison project and these projections are often gridded the use of a model with simplified process representation could allow for water quality models to be run for multiple watersheds across many different scenarios thus allowing researchers and natural resource managers to identify systems at higher risk of degradation assess potential outcomes and prioritize further analysis to date guidance on the use of parsimonious models for assessing nutrient loading across multiple watersheds under uncertain conditions remains limited this study sought to answer the question to what extent can a model with simplified process representation and minimal parameter and data inputs effectively predict runoff driven nutrient loading to estuarine watersheds across multiple regions the objectives of our study were to 1 compare performance between nutrient loading models with varying levels of detailed process representation specifically focusing on total nitrogen tn and total phosphorus tp 2 evaluate whether a simplified approach is effective at predicting absolute or relative load magnitudes as compared to a more complex model and 3 assess which watersheds were associated with high low comparative performance of the simplified model in the present work the spreadsheet tool for estimating pollutant load stepl was selected as a representative nutrient load model with simplified process representation and data inputs i e lulc precipitation and was used as the test model in this study stepl was adapted to support gridded data steplgrid steplgrid s efficacy in predicting nonpoint source nutrient loading in estuarine watersheds was compared to that of the spatially referenced regression on watershed attributes sparrow model a parsimonious hybrid process statistical model with relatively more complex model structure as compared to steplgrid steplgrid and sparrow were applied to 112 estuarine watersheds across all coastal regions of the contiguous u s which vary in terms of climate land use and land cover our hypothesis is that a simplified model is capable of estimating estuarine nutrient loads and providing insight for large scale management decisions furthermore the study presented here also advances the use of national scale gridded products for estimating and comparing nutrient loads across large spatial extents 2 methods 2 1 study area 2 1 1 study watersheds the study encompassed 112 estuarine watersheds across five large regions of the united states fig 1 the north atlantic n 38 area 373 137 km2 south atlantic n 19 area 323 059 km2 gulf of mexico n 24 area 788 078 km2 north pacific n 20 area 79 811 km2 and south pacific n 11 area 183 367 km2 fig 1 their land use and land cover lulc compositions sizes rainfall amounts and elevation distributions are presented in figs 2 and s1 respectively north and south atlantic fig 1a and b the climate in these regions is predominantly temperate forested and urban lands dominate the north atlantic and south atlantic regions cropland is also extensive in the south atlantic further watersheds in the south atlantic i e median of 12 444 km2 are larger than those in the north atlantic i e median of 2 500 km2 gulf of mexico fig 1c the temperature gradient does not vary considerably across the gulf of mexico region while rainfall patterns do in the western part of the gulf of mexico i e west of the mississippi river fairly arid conditions are observed while subtropical and humid conditions are observed in the eastern part i e east of the mississippi river lulc patterns follow the rainfall patterns in the western and drier areas range and pasture dominate while cropland dominates in the eastern and humid areas the elevation is fairly low across the gulf of mexico s watersheds and their size is the largest compared to the four other regions north and south pacific fig 1d and e the west coast of the u s has a wide range of climatic conditions from very wet and cool i e north pacific to dry and warm i e south pacific the north pacific region is dominated by forest while the south pacific has less forest and more rangeland the west coast has been strongly structured by tectonism and volcanism that shaped the coastline as a result the north pacific region contains numerous inlets and estuaries while the south pacific region is characterized by a relatively straight coastline interrupted sometimes by estuaries and headlands the coastal watersheds located in the north and south pacific regions have the highest average elevation of all the study regions except for a few the sizes of the watersheds are fairly small along the west coast around 2 000 km2 2 1 2 estuarine watershed delineation the u s geological survey usgs previously delineated estuarine watersheds across the five study regions preston et al 2011 however vector based shapefiles of these watersheds were not available therefore the watersheds and their associated names were identified and delineated according to the noaa coastal assessment framework noaa 2007 the following watersheds were combined to correspond to estuarine watersheds as previously defined by the usgs 1 the upper and lower laguna madre 2 the north santee south santee river estuary and charleston harbor and 3 the massachusetts bay and boston harbor the differences between the area of the watersheds delineated in this study versus those defined by the usgs were computed to ensure that the delineations matched watersheds that had an absolute area difference i e between the delineation of the usgs and ours greater than 15 were not considered in the analysis i e wells bay hampton harbor estuary south maryland coastal bays indian river lagoon sarasota bay tampa bay st andrew bay additionally the mississippi river and columbia river watersheds were removed from the analysis the mississippi river watershed spans all or part of 32 states and consists of hundreds of sub watersheds that share dissimilar climates e g continental subtropical semi arid and lulc patterns therefore a direct comparison of this watershed with the other u s estuarine watersheds was deemed inappropriate further the columbia river watershed is also very large and drains a portion of canada data products used as inputs in the study presented here span only the u s resulting in data for the columbia river watershed being incomplete therefore the columbia river watershed was excluded from the analysis in total 112 estuarine systems were delineated fig 1 2 2 stepl the spreadsheet tool for estimating pollutant load stepl is a lumped parameter pollutant load model developed by tetra tech inc for the u s environmental protection agency epa the use of stepl has primarily been limited to watersheds less than 1 000 km2 in area liu et al 2017 nejadhashemi et al 2011 ohio epa 2005 with its efficacy not yet tested at larger spatial scales stepl calculates annual nutrient loads at the watershed scale as a product of surface runoff and event mean concentrations emcs emcs are empirical concentrations of a specific pollutant e g tn tp associated with a unit volume of runoff following an average storm emcs are reported by lulc type stepl calculates direct runoff from the average rainfall per event with the scs cn method then the direct runoff from the annual average event scale rainfall depth is multiplied by the average number of rain days in a year as well as the percentage of rain day events that generate runoff tetra tech inc 2011 the amount of direct annual runoff generated from each lulc class in a watershed is tabulated and multiplied by the associated emcs which are defined by the modeler typically by identifying relevant values in the literature these lulc specific loads are then summed across the watershed to estimate annual watershed scale runoff driven pollutant loads the model is currently shared as a microsoft excel spreadsheet with a user friendly visual basic interface stepl has been used by practitioners to develop total maximum daily loads tmdls for nutrients and identify strategies to reduce nutrient loads e g ohio epa 2005 because of its relatively simple structure and use of publicly available data layers as inputs stepl is easy to interpret and apply making it favored by many practitioners who seek model based decision support when screening management strategies in the absence of extensive computational resources in particular stepl allows for the potential nutrient reduction effects of best management practices bmps and low impact development lid to be estimated for urban areas tetra tech inc 2011 pollutant load reduction resulting from the implementation of bmps is computed using empirical efficiency factors stepl allows the user to choose among a list of bmp names and efficiencies for different land uses and pollutant types e g bioreactor buffer controlled drainage conservation tillage tetra tech inc 2011 furthermore the first flush phenomenon has been studied in the design of bmps bach et al 2010 kang et al 2006 lee et al 2002 smith 2001 in this study we did not consider bmps and the first flush phenomenon the stepl model computes nutrient loads at the annual scale and previous research has found that first flush phenomenon is uncertain for large watersheds due to the dilution and transport of the pollutant mamun et al 2020 additional assumptions were made first we neglected surface water groundwater interactions for the study area over long time horizons groundwater is considered to usually be a small or negligible fraction of the coastal nutrient and water budgets loicz n d spruill and bratton 2008 furthermore we did not consider nutrient sinks e g denitrification in wetlands instream processing lake attenuation and water residence time notably because the capacity of ecosystems to assimilate and remove n and p is often largely exceeded by human nutrient inputs and recovery from nutrient saturation could take decades to millennia chen et al 2018 dupas et al 2018 frei et al 2020 goyette et al 2018 haas et al 2019 randall et al 2019 van meter et al 2016 2 2 1 implementing stepl in r for spatial use the stepl model could not be used in its current visual basic based spreadsheet form because the model was applied spatially using gridded input data i e lulc soils precipitation the original spreadsheet form of stepl uses unique values entered in an excel spreadsheet instead of gridded inputs specifically the epa stepl model computes average annual direct runoff from a model database that provides average annual rainfall rain days and correction factors for rainfall and rain day totals for the counties in which the national climate data center stations are located to facilitate use of gridded inputs and reduce the coarseness of the model a distributed version of the model in which input parameters are specified for each grid cell was developed in r r core team 2020 a distributed model can account for spatial variability in physical and chemical characteristics on a cell by cell basis therefore distributed models are considered more spatially accurate in representing nonpoint source pollution the stepl distributed model was constructed and run by computing the annual runoff at the grid scale 30 m using lulc soil and precipitation data and then calculating runoff driven nutrient loads in each cell fig 3 this modified version of stepl is hereafter called steplgrid to account for its ability to use gridded spatial data although the equations of stepl remain the same steplgrid allowed for the integration of spatial data inputs and for q and cn to be estimated in a more precise and distributed way across the watershed instead of being aggregated through a spatial weighting approach in the standard lumped version of stepl 2 2 2 input data the distributed steplgrid model implemented in r used the scs cn method developed by the u s natural resources conservation service to estimate runoff produced from an average storm for each 30 m grid cell the scs cn approach requires lulc and hydrologic soil group data lulc data were obtained from the 2001 national land cover database nlcd homer et al 2012 hydrologic soil group data were obtained from gssurgo soil survey staff n d and accessed through arcgis esri 2018 gaps in hydrologic soil group values were filled with the resampled hysogs250m dataset ross et al 2018 scs curve numbers from nrcs tr 55 nrcs 1986 were assigned to each grid cell based on the cell s combination of lulc and hydrologic soil group table 1 to estimate annual direct runoff the average runoff generated from a storm was multiplied by the number of rainy days observed within the year precipitation was obtained and resampled from the gridmet dataset abatzoglou 2013 gridmet is a spatial dataset of surface meteorological variables with spatial resolution of 1 24 approximately 4 km to determine the nutrient loads associated with annual runoff produced in a watershed annual direct runoff was multiplied by lulc specific emcs emcs are an essential component in the nutrient load estimation procedures however emcs can be uncertain due to differences in how storm events are defined water quality sampling methods and variation in how emcs are reported across areas of different watershed lulc composition or size fu et al 2019 therefore a review was conducted to identify and summarize published emc values for tn and tp in total emcs from 22 publications including both peer reviewed articles and reports from the grey literature were tabulated tables s2 6 and fig s7 to ensure the emcs were representative of our area of study we excluded all emc values that were calculated outside the contiguous u s therefore the emc values were tabulated across the states included in our analysis e g north carolina texas washington and outside our study area e g minnesota kansas michigan furthermore to account for uncertainty in emc values a permutation analysis was performed with the emcs identified from the literature see section 2 4 2 to generate different parameter scenarios that encompassed the range of reported emc values permutations of the 0 1 0 5 median and 0 9 quantiles of the emc distributions were created for each of the lulc classes to generate different sets of emcs that spanned the range of emc uncertainty from our literature review then steplgrid was run for each watershed for each of the emc sets obtained from the permutation analysis the lulc classes were based on the nlcd lulc classification definitions for this study several nlcd classes were grouped together to create five broader classes used in the analysis 1 forest nlcd classes 41 deciduous forest 42 coniferous forest 43 mixed forest 2 agriculture nlcd class 82 cultivated crops 3 pasture nlcd class 81 pasture hay 4 range nlcd classes 51 shrubland 71 grasslands herbaceous and 5 urban nlcd classes 21 open urban 22 low density urban 23 medium density urban 24 high density urban 2 3 sparrow the spatially referenced regression on watershed attributes sparrow model was used as a reference model in this study sparrow developed by the us geological survey usgs is widely used by scientists and decision makers to predict long term average annual nutrient loads values of water quality constituents delivered to downstream receiving water bodies e g estuaries lakes preston et al 2009 sparrow is a hybrid process based and statistical model as it shares some attributes of both mechanistic and statistical models that are used to explain in stream loads of water quality constituents in relation to upstream sources and watershed properties e g precipitation lulc and soil characteristics the model uses statistical functions to estimate the sources fluxes and transformations of water quality constituents in a watershed water quality and streamflow measured at monitoring sites are used to estimate water quality constituent mass furthermore sparrow uses a spatially explicit structure defined by a river reach network through which upstream and downstream monitoring sites are connected within the model contributing drainage areas are determined for each stream reach allowing for watershed characteristics e g lulc soil to be used as explanatory variables in the model and assessed as water quality drivers sparrow requires expertise in hydrologic and water quality modelling for processing and assembling input data calibrating models and calculating model uncertainty therefore sparrow is relatively more structurally complex and challenging to use as compared to stepl the national water quality assessment nawqa program used sparrow to assess nutrient conditions across six regions of the u s for the year 2002 the nawqa regions are consistent with the five regions considered in this study but the nawqa assessment included the great lakes as an additional region garcia et al 2011 moore et al 2011 moorman et al 2014 preston et al 2009 rebich et al 2011 schwarz et al 2011 smith et al 1997 the regional models were calibrated and validated over several years to estimate nutrient load transport to u s estuaries and facilitate comparisons between watersheds regarding differences in dominant sources of nutrient pollution these regional sparrow models determined the proportion of watershed outlet tn and tp loads associated with different point and nonpoint sources e g nutrient loads resulting from fertilizer application urban runoff wastewater etc contrary to stepl sparrow has been used across the conterminous us and is designed to be used across spatial scales ranging from small watersheds i e tens of km2 to large watersheds i e several million km2 preston et al 2011 smith et al 1997 2 4 comparing steplgrid and sparrow outputs steplgrid and sparrow were compared based on their tn and tp load estimates from nonpoint sources modeled tn and tp loads from point sources were excluded from the analysis here steplgrid outputs were evaluated against sparrow outputs instead of observational in stream nutrient load data given that the tn and tp load estimates produced from steplgrid do not comprise all potential sources of nutrients e g nitrogen inputs through atmospheric deposition or nutrient sinks accordingly steplgrid is not designed to estimate in stream concentrations and loads but is instead a planning tool that allows for prediction of total runoff driven nutrient loads sparrow outputs also report total estimated nutrient loads generated in a watershed but while accounting for hydrologic connectivity and river network structure thus by comparing steplgrid and sparrow outputs this analysis tested whether additional model complexity i e hydrologic connectivity was needed to effectively estimate nonpoint tn and tp loads across watersheds from a range of environmental and climatic conditions both steplgrid and sparrow encounter uncertainties therefore uncertainties remain in the interpretation of steplgrid and its ability to predict real nutrient loads however the intercomparison between sparrow and steplgrid still improves understanding of water quality model differences and uncertainties related to model parameterization and structure van vliet et al 2019 further model intercomparison is useful for highlighting model disagreements and providing a more comprehensive assessment of dominant pollution processes and sources van vliet et al 2019 2 4 1 aligning model outputs the final outputs of the steplgrid model are annual tn and tp loads at the watershed scale in contrast sparrow nutrient load outputs are calibrated to the total load and as a function of distinct source types moreover the five regional sparrow models produced by the usgs divided nonpoint sources into different categories garcia et al 2011 moore et al 2011 moorman et al 2014 preston et al 2009 rebich et al 2011 schwarz et al 2011 smith et al 1997 to facilitate comparisons between sparrow and steplgrid model outputs the nutrient loads attributed to nonpoint sources in sparrow were aggregated for tn and tp 2 4 2 steplgrid uncertainty analysis an uncertainty analysis was performed with the steplgrid model to evaluate the variation in modeled tn and tp loads as a function of emc value uncertainty to evaluate the propagation of the uncertainty in the emc values we used the best available literature to select emc values across our study area see section 2 2 2 permutations of the 0 1 0 5 median and 0 9 quantiles of the emc distributions were created for each of the five lulc classes i e urban crop forest pasture range table 2 as a result 243 different emc sets n 35 were obtained from the permutation analysis steplgrid was run for each watershed for each of the 243 emc sets 2 4 3 sparrow output uncertainty the regional sparrow models reported output uncertainty with different metrics the gulf of mexico sparrow model reported output uncertainty as the 10th and 90th percentile confidence interval for the total delivered loads by watershed i e the total tn and tp from nonpoint and point sources using a parametric bootstrapping approach with hundreds of model iterations therefore for the gulf of mexico region the 10th and 90th percentile confidence interval was applied proportionally to the nonpoint source contribution for the north pacific region the uncertainty was considered to be the reported 10th and 90th percentile confidence interval of the individual nonpoint source contribution in contrast output uncertainty was not reported in the regional sparrow models for the south pacific north atlantic and south atlantic for these three regions an uncertainty interval of 35 of the mean nonpoint source tn and tp loads was assumed the range of 35 was selected as a conservative uncertainty estimate the minimum sparrow uncertainty associated with modeled nutrient loads reported from the sparrow models for the north pacific and gulf of mexico regions was 35 thus the actual uncertainty in modeled nutrient loads from sparrow for these two regions could either be greater or less than 35 2 4 4 model comparison metrics to quantify steplgrid s efficiency in predicting absolute tn and tp loads relative to sparrow several performance statistics were calculated table 3 these metrics included the nash sutcliffe efficiency nse the modified nash sutcliffe efficiency coefficient e that is less sensitive to extreme values krause et al 2005 the percent bias pbias and the ratio of the root mean square error to the standard deviation of observed data rsr of note rsr is not an independent measure of the model performance as nse 1 rsr2 the metrics were calculated using load estimates from all watersheds in each of the five study regions the steplgrid outputs used in these calculations were those that corresponded to the 0 5 quantile i e median emc set to evaluate whether the performance statistics produced satisfactory or unsatisfactory results the values were compared to benchmarks proposed by moriasi et al 2007 2015 table 4 additionally model outputs were compared to evaluate whether steplgrid was consistent with sparrow in terms of ranking watersheds by their tn and tp loads the signed rank test also called the wilcoxon signed rank test was used to determine whether the median difference between paired observations i e nutrient loads computed with steplgrid and sparrow was equal to zero the null hypothesis of the signed rank test was that the sparrow and steplgrid model outputs had the same distribution an alpha level of 0 05 was used to evaluate significance furthermore the relative ranking of nutrient loads across the watersheds was assessed to determine whether steplgrid model outputs were consistent with sparrow in terms of the ranked nutrient loads for watersheds across the study regions nutrient loads were sorted and replaced by their rank for both sparrow and steplgrid steplgrid and sparrow ranks were compared to the one to one line and the nse was computed hereafter nse values calculated from ranks are referred to as nserank furthermore the spearman s rank order correlation spearman s ρ was computed to quantify the strength and the direction of the monotonic association between the two models outputs 3 results 3 1 comparative performance statistics to compare steplgrid outputs to sparrow using common statistical performance tests steplgrid was run with the median of the emc distributions created for each of the lulc classes see section 2 4 2 the following section compared steplgrid results produced with the median emc to the mean sparrow outputs in order to quantity steplgrid s efficiency relative to sparrow 3 1 1 tn compared to sparrow steplgrid was effective at relatively ranking tn loads nserank 0 78 spearman s ρ 0 89 table 5 and fig 4 a but the tn load magnitudes differed nse 0 25 rsr 0 87 pbias 3 4 table 5 and fig 5 a when aggregating the total tn loads across all study watersheds from all regions the totals were comparable between sparrow 377 million kg and steplgrid 390 million kg with a 3 absolute difference pbias 25 nserank 0 60 spearman s coefficient ρ 0 89 p value 0 05 showed agreement between steplgrid and sparrow results while r2 0 39 nse 0 25 e 0 33 and rsr 0 87 showed poor agreement table 5 based on the nserank and spearman s ρ values steplgrid was similar to sparrow in rank ordering the watersheds in terms of tn loads across all regions table 5 and fig 4a however the remaining performance metrics reveal that steplgrid did not consistently produce comparable absolute load estimates for tn when considering all study watersheds in the five study regions table 5 although model efficiency statistics demonstrated that steplgrid estimates of tn loads were generally unsatisfactory compared to sparrow when all study watersheds were considered in aggregate steplgrid had comparable performance to sparrow within certain regions for all of the regions steplgrid captured the relative ranking of tn loads nserank 0 60 and spearman s ρ 0 80 when comparing absolute load estimates to sparrow steplgrid had very good performance for the south atlantic and satisfactory performance for the north atlantic and south pacific table 5 however the signed rank test p value 0 05 revealed that the tn load distributions of steplgrid and sparrow were not identical for the north atlantic region even though model performance metrics i e nse e rsr pbias nserank were satisfactory table 5 for both the gulf of mexico and the north pacific all statistical tests were judged unsatisfactory for estimating absolute tn loads compared to sparrow and the steplgrid and sparrow distributions for tn load estimates were not identical signed rank test p value 0 05 table 5 although there were regional trends in steplgrid model performance relative to sparrow no clear relationships between model performance and watershed characteristics were identified r2 for model prediction differences and watershed size 0 11 fig 5a similarly no trends were observed between topographic climatic and lulc patterns and tn load estimate differences between sparrow and steplgrid fig 6 a therefore given the nserank value and significant results of the spearman s correlation steplgrid equivalently estimated relative tn loads compared to sparrow at the national scale while model efficiency statistics revealed that estimates of absolute tn loads across all coastal watersheds were not consistently equivalent to sparrow however regional scale efficiency statistics showed that steplgrid predicted similarly to sparrow absolute and relative tn loads for the north atlantic south atlantic and south pacific but predicted poorly for the gulf of mexico and north pacific regions 3 1 2 tp in contrast to tn model performance metrics showed that both the magnitude nse 0 84 and the relative ranking nserank 0 85 spearman s ρ 0 93 of tp loads were equivalently captured by steplgrid compared to sparrow at the national scale table 5 figs 4b and 5b as for tn steplgrid and sparrow tp loads aggregated across all coastal areas of the continental us were comparable i e 3 absolute difference in tp load between the models with the aggregate steplgrid estimate being 49 74 million kg and sparrow being 48 03 million kg model performance evaluation metrics applied for all watersheds across the five regions confirmed that steplgrid and sparrow predicted similar tp loads at the national scale pbias 10 nse 0 84 e 0 67 rsr 0 40 nserank 0 85 spearman s ρ 0 93 and r2 0 88 indicated very good agreement between the two models table 5 thus based on the performance evaluation criteria table 4 steplgrid parameterized with the median emc set was judged very good in predicting tp loads using sparrow outputs as a benchmark at the regional scale the ranking analysis revealed that steplgrid was able to capture the relative ranking of tp loads across watersheds but performance statistics showed that steplgrid model efficiency as compared to sparrow was mixed across the regions steplgrid had very good performance for the north atlantic and gulf of mexico regions good performance for the south pacific region and unsatisfactory performance for the south atlantic and north pacific moreover for the three regions with very good performance north atlantic gulf of mexico south pacific steplgrid also captured the relative ranking of tp loads table 5 however results were mixed for the south atlantic region where all statistical performance tests indicated unsatisfactory results nse 0 07 e 0 32 rsr 0 94 except r2 0 88 pbias 41 2 and nserank 0 82 table 5 also the signed rank test for the south atlantic p value 0 026 revealed that the steplgrid tp load distribution was not equivalent to that of sparrow for the north pacific region statistical performance tests revealed that steplgrid was unsuccessful in capturing the magnitude of tp loads and had also different tp load distributions than sparrow signed rank test p value 0 05 table 5 although low agreement was observed between steplgrid and sparrow in predicting tp loads of watersheds in the north pacific and south atlantic regions steplgrid predicted more similarly tp at the national and regional levels than for tn furthermore as was seen for tn there were regional trends in steplgrid model outputs relative to sparrow however no clear relationships between model results and watershed characteristics were identified there was no clear relationship in model prediction differences and watershed size r2 0 0021 fig 5b moreover clear trends were not seen between topographic climatic and lulc patterns and tp load differences between sparrow and steplgrid fig 6b therefore given the model efficiency statistics and significant results of the spearman s correlation steplgrid was successful in estimating tp load as compared to sparrow for most of the study regions regional scale efficiency statistics showed that steplgrid predicted similarly to sparrow in the absolute and relative tp loads for the north atlantic gulf of mexico and south pacific regions but poorly for the south atlantic and north atlantic regions overall steplgrid was more efficient in predicting tp loads than tn loads across the 112 estuarine systems 3 2 comparison of steplgrid and sparrow load estimates sparrow and steplgrid outputs from the uncertainty analysis were compared when comparing steplgrid and sparrow outputs three cases were observed with regards to their load estimate uncertainties 1 no intersection i e steplgrid and sparrow do not share common values in their uncertainty intervals 2 partial intersection i e steplgrid and sparrow share some common values in their uncertainty intervals and 3 full intersection i e steplgrid uncertainty encompasses the entire range of sparrow values these three cases are referenced throughout the results 3 2 1 tn of the 112 study watersheds 89 79 had steplgrid and sparrow tn load uncertainties that fully n 26 or partially n 63 intersected fig 7a s8 s12 and table 6 the regions associated with the most comparable tn load estimate uncertainty ranges between sparrow and steplgrid were the north and south atlantic in the north atlantic 37 of 38 watersheds had intersecting uncertainty intervals between steplgrid and sparrow of the 37 watersheds with intersecting uncertainty ranges for tn 24 had partial intersection and 13 had full intersection in the north atlantic only the delaware inland bays watershed had no intersection between steplgrid and sparrow uncertainties all of the watersheds in the south atlantic n 19 had intersecting uncertainty ranges for tn in which 11 were partially intersecting and 8 were fully intersecting i e neuse river estuary bogue sound new river estuary winyah bay st helena sound savannah river estuary st marys river cumberland sound and st johns river estuary outside of the north and south atlantic tn estimates from steplgrid were not always consistent with those of sparrow of the 24 watersheds in the gulf of mexico 16 were partially intersecting 3 were fully intersecting i e charlotte harbor aransas bay and laguna madre and 5 had no intersecting uncertainty range i e apalachee bay apalachicola bay mobile bay mermentau bay and corpus christi bay for watersheds in the south pacific n 11 six had intersecting uncertainty ranges for tn in which five were partially intersecting i e san pedro bay san francisco bay north eel river humboldt bay and klamath river and one fully intersecting i e san francisco bay south the north pacific region where elevation and precipitation are the highest had 12 of 20 watersheds that did not have intersecting sparrow and steplgrid uncertainty ranges for tn fig 7 a i only one watershed in the north pacific was associated with a full intersection i e rogue river at the national scale half of the watersheds shared between 50 and 100 of the stepl and sparrow uncertainty ranges for tn load estimates and most watersheds with partial intersection had over 50 overlap between steplgrid and sparrow tn load estimates i e mean 56 69 and median 61 17 table 6 the extent to which uncertainty ranges overlapped between steplgrid and sparrow varied across watersheds and regions for example the upper chesapeake bay had only 2 30 overlap between the tn load uncertainty range for steplgrid and sparrow while san antonio bay had 95 78 overlap the north atlantic and south atlantic are the regions with the greatest overlap in tn load uncertainty ranges followed by the south pacific the gulf of mexico and the north pacific regions table 6 although steplgrid and sparrow uncertainties intersected for 79 of the watersheds studied trends were observed across regions in terms of whether steplgrid was likely to under or over predict tn loads compared to sparrow fig 7 s8 s12 steplgrid underpredicted tn loads relative to sparrow for most watersheds in the north atlantic n 22 fig 7a ii on the other hand steplgrid tended to overpredict tn loads in watersheds of the south atlantic n 14 and for almost all of the watersheds in the gulf of mexico n 23 as compared to sparrow fig 7a ii in the north and south pacific regions steplgrid underpredicted tn loads though steplgrid was more likely to overpredict tn loads in the south pacific for larger watersheds i e san francisco bay north eel river and klamath river for the gulf of mexico a similar trend was observed in that steplgrid generally overpredicted tn loads for large watersheds fig 7a ii 3 2 2 tp fully or partially intersecting uncertainty intervals were observed in tp load estimates between steplgrid and sparrow for 90 81 of the 112 study watersheds fig 7b s8 s12 and table 6 of the 90 watersheds with intersecting uncertainty ranges for tp 42 were partially intersecting and 48 were fully intersecting the majority of the watersheds with intersecting uncertainty intervals for tp load estimates were in the north atlantic south atlantic and gulf of mexico regions of the 29 watersheds with intersecting uncertainty ranges for tp in the north atlantic 11 had partial intersection and 18 had full intersection table 6 only nine of the watersheds in the north atlantic did not have common intersection intervals between steplgrid and sparrow uncertainties i e cobscook bay blue hill bay buzzards bay englishman machias bay damariscotta river estuary great south bay cape cod bay massachusetts bay boston harbor waquoit bay fig 7b i all of the watersheds with no intersection were relatively small in area ranging from 53 km2 i e waquoit bay to 2 461 km2 i e englishman machias bay as was observed for tn almost all of the watersheds in the south atlantic region n 17 had intersecting uncertainty ranges for tp in which 9 were partially intersecting and 8 were fully intersecting i e neuse river estuary bogue sound new river estuary winyah bay st helena sound savannah river estuary st marys river cumberland sound and st johns river estuary in the south atlantic the pamlico and st catherines sapelo sounds had no intersection between steplgrid and sparrow uncertainty intervals similar to the south atlantic region almost all of the gulf of mexico watersheds had intersecting uncertainty ranges for tp n 23 and most of them were fully intersecting n 18 in this region only the corpus christi watershed did not have intersecting steplgrid and sparrow uncertainty intervals for tp estimates fig 7b i the south pacific region had 7 watersheds with intersecting uncertainty ranges for tp in which 3 were partially intersecting i e elkhorn slough klamath river humboldt bay and 4 were fully intersecting i e san pedro bay san francisco bay south san francisco bay north eel river two thirds of the study watersheds had between 50 and 100 overlap between the steplgrid and sparrow uncertainty ranges with the north atlantic and gulf of mexico regions having the highest percent overlap followed by the south atlantic table 6 as with tn load estimates a wide range was observed in terms of the percent overlap of the uncertainty intervals between steplgrid and sparrow of the watersheds with partially intersecting uncertainty intervals willapa bay had the lowest percent overlap with only 0 17 while chester river estuary had the greatest overlap at 99 25 table 6 however most of the watersheds with partial uncertainty interval intersection shared over 50 overlap i e mean 61 24 and median 65 60 the north and the south pacific regions had the lowest percentage of common intervals between sparrow and steplgrid as observed for tn some regions were associated with steplgrid regularly under or over predicting tp loads as compared to sparrow fig 7 however whereas there were clear trends in under and over prediction of tn across the different study regions the results were more mixed for tp overall no clear trends were observed for the north atlantic south pacific gulf of mexico for example of the 8 watersheds with partial intersection in the gulf of mexico steplgrid overpredicted tp loads with steplgrid as compared to sparrow in approximately half of these watersheds and the other half were underpredicted fig 7b ii moreover of the 18 watersheds that had full intersection between steplgrid and sparrow uncertainty intervals in the gulf of mexico 50 had steplgrid uncertainty ranges that were centered within sparrow uncertainty ranges fig 7b iii the south pacific and the north pacific are the regions where predominant overprediction and underprediction trends were more readily observed 68 n 13 of the watersheds in the south pacific tended to have overpredicted tp loads computed with steplgrid as compared to sparrow while 70 n 14 of those in the north pacific tended to underpredict sparrow tp loads 4 discussion in the present work we compared runoff driven tn and tp load estimates from the steplgrid and sparrow models for 112 coastal watersheds across the contiguous u s although there is sometimes a perception that simple water quality models are limited in their capacity to effectively simulate biogeochemical responses our results show that the steplgrid model produced reasonable load estimates across regions of the u s that span different climates and have varied lulc compositions in particular the rank order of watersheds based on their tn and tp load estimates were very similar between steplgrid and sparrow with nserank and spearman s ρ values ranging from 0 63 to 0 88 and 0 81 to 0 94 respectively across the five study regions moreover when considering all 112 study watersheds the nserank and spearman s ρ values ranged from 0 78 to 0 88 and 0 89 to 0 93 respectively these results indicate that in terms of relatively ranking watersheds by nutrient load steplgrid is effective despite its simplicity however when estimating absolute magnitudes of watershed nutrient loads the results were mixed across the study regions overall when predicting load magnitudes steplgrid estimates of tp were more comparable to those of sparrow than for tn when evaluating the outputs of steplgrid as applied with the median emc set steplgrid and sparrow similarly predicted absolute tn load magnitudes for the north atlantic nse 0 52 south atlantic nse 0 78 and south pacific nse 0 57 but more dissimilarly for the gulf of mexico nse 2 33 and north pacific nse 0 06 regions better predictions were observed for tp for which steplgrid successfully estimated comparable tp loads to sparrow for the north atlantic nse 0 95 gulf of mexico nse 0 78 and south pacific nse 0 91 regions but predicted poorly for the south atlantic nse 0 07 and north pacific nse 0 23 regions however the uncertainty analysis in which steplgrid was run with 243 combinations of emc values provided a more comprehensive view of steplgrid s predictive ability as compared to sparrow for example although the nse for tp loads from watersheds in the south atlantic was low nse 0 07 the uncertainty analysis revealed that over three fourths of these watersheds had overlapping uncertainty intervals for tp which indicates that steplgrid provided reasonable load estimates for tp despite not predicting consistently equivalent values to those of sparrow had only the estimates produced with steplgrid parameterized with the median emc set been analyzed without considering uncertainty steplgrid could have been judged inefficient in predicting tp loads in the south atlantic thus when evaluating steplgrid s performance for individual watersheds and across regions it is essential to consider the uncertainty ranges along with the performance metrics when considering both uncertainty and performance metrics we conclude that steplgrid s tn and tp load estimates were comparable to those of sparrow for the north atlantic south atlantic gulf of mexico and south pacific regions but not the north pacific region for the north and south atlantic and south pacific regions results were interpreted with the conservative uncertainty range of 35 for sparrow load estimates due to the uncertainty ranges for the calibrated model estimates not being reported a lower assumed range in sparrow uncertainty could have resulted in different outcomes for some watersheds i e humboldt bay south puget sound for which stepl and sparrow share few common values in their uncertainty intervals for these watersheds a lower uncertainty range could have meant no intersection between stepl and sparrow uncertainty intervals nevertheless the uncertainty estimates of 35 appeared to be a conservative assumption for the majority of the watersheds studied for which a lower uncertainty range would have not changed the outcomes of the results figs s8 12 regional trends were also observed with regards to steplgrid s tendency to over or under predict tn and tp loads relative to sparrow for tn steplgrid tended to underpredict loads relative to sparrow for most watersheds in the north atlantic north pacific south pacific and overpredict tn loads for watersheds in the gulf of mexico south atlantic and large watersheds in the south pacific for tp steplgrid tended to overpredict phosphorus loads for watersheds in the south pacific and underpredict in the north pacific while no clear trends were observed for the north atlantic south atlantic and gulf of mexico regions trends in over and under prediction can likely be attributed to the way steplgrid and sparrow represent pollutant transport processes for example steplgrid uses emcs and curve numbers to represent nutrient loading and runoff processes and both emcs and curve numbers are empirical in contrast sparrow uses mass balance approaches that consider different n and p sources and retention on land and in river networks moreover sparrow uses a first order decay process as a function of water travel time and water depth preston et al 2009 while steplgrid assumes no losses or sinks and does not consider in stream n processing and travel time in its equations consequently we would expect steplgrid to produce larger tn and tp estimates as compared to sparrow especially for large watersheds thus steplgrid overpredicting loads as compared to sparrow was unsurprising however the frequent underprediction of tn loads for the north atlantic north pacific and south pacific was unexpected in particular for the north pacific region steplgrid underpredicted tn loads as compared to sparrow by a factor of 2 18 two commonalities were observed among the watersheds in the three regions with tn underprediction by steplgrid 1 forested areas were extensive and or 2 elevations and therefore slopes were relatively high especially as compared to watersheds in the gulf of mexico and south atlantic regions forested land cover accounted for a median of 71 4 31 5 and 59 9 of watershed area for the north pacific south pacific and north atlantic respectively and average elevations were 474 m 377 m and 64 m respectively emc values have been reported extensively for anthropogenic land uses e g urban crop pasture but fewer have been reported for natural lands e g forest range tables s2 6 fig s7 therefore the emcs for natural lands e g forest rangeland might not be fully representative of the real range of emcs for these land use types because natural lands can be major contributors of nitrogen to waterbodies the relative paucity of emc values for these land covers as compared to other human intensive land uses may be an important source of uncertainty in the steplgrid model additionally steplgrid estimates nutrient loads using the scs cn approach and the average annual runoff volumes tetra tech inc 2011 while the scs cn method is commonly used to simulate the amount of runoff from a specific rainfall event as a result steplgrid s process representation could mask the effects of individual high precipitation events that deliver large nutrient loads in watersheds with steeper slopes we expect that runoff peaks would be magnified so disparities between individual storm runoff dynamics and average annual runoff calculations might explain why regions with higher slopes were associated with underestimation of tn loads with steplgrid as compared to sparrow the more consistent agreement between steplgrid and sparrow when estimating tp loads may be explained by the greater importance of surface water runoff as a driver of tp as compared to tn erosion and sedimentation are the main mechanisms controlling phosphorus export and surface runoff is the dominant driver of phosphorus loading hart et al 2004 sharpley and syers 1979 because of its chemical physical properties phosphorus binds to most soil and sediments for this reason surface water primarily receives phosphorus through surface flows rather than groundwater resulting in runoff events being associated with large tp loads sharpley 1995a in contrast previous research has revealed that nitrogen is often sourced through legacy stores in groundwater reservoirs or thick unsaturated zones van meter et al 2016 making these zones an important source of nitrogen because steplgrid simulates nutrient loading solely as a function of surface runoff it is unsurprising that steplgrid was more effective in estimating tp loads than tn interestingly although there was greater agreement between steplgrid and sparrow for tp loads steplgrid tended to underpredict tp loads as compared to sparrow in the north pacific region one explanation of the discrepancy in prediction for this region is that steplgrid cannot integrate regional differences in natural nutrient background prior studies estimate that background tp accounted for over 50 of tp loads for multiple watersheds in the north and south pacific regions due to the presence of large natural soil phosphorus levels and phosphate rock deposits with background tp levels being especially high in washington and oregon sharpley 1995b wise and johnson 2011 thus as seen in the results presented here steplgrid appears to underpredict tp loads in areas with elevated background p although emcs should ideally account for background nutrient levels the literature currently does not include enough emc estimates to allow for emcs to be specified by both lulc class and region though modelers could choose to adjust emcs to account for background concentrations 5 conclusions our results show that steplgrid is particularly effective at producing rank ordered tn and tp load estimates as compared to sparrow making steplgrid a useful tool for performing inter watershed comparisons because steplgrid requires limited input data and computational power its utility would be particularly pronounced when applied to evaluate many watersheds under multiple scenarios e g climate land use policy and results presented here support the use of steplgrid for such purposes moreover data inputs required by steplgrid are publicly available for the contiguous u s making its application across multiple watersheds feasible and practical we compared steplgrid outputs to those of sparrow but it is important to acknowledge that sparrow is also an imperfect representation of reality so our findings should be interpreted within the context of existing uncertainty associated with sparrow although the approach of comparing steplgrid to sparrow is imperfect model intercomparison studies as pursued in the present work offer many advantages by building confidence using familiar benchmarks i e sparrow and outlining tradeoffs associated with different model approaches van vliet et al 2019 additionally whereas steplgrid was consistently effective at predicting relative nutrient loads steplgrid s ability to estimate the absolute magnitudes of tn and tp loads as compared to sparrow varied by watershed and region as a result steplgrid should only be used to estimate absolute nutrient load magnitudes for watersheds where its uncertainties are minimized such as in coastal plain regions with extensive human land use models with more comprehensive process representations than steplgrid should be used when analyzing nutrient loading dynamics in individual watersheds particularly if a user s interest is to estimate a single watershed s outlet loads or concentrations future applications of steplgrid should consider additional key findings from this study steplgrid appears to be effective at rank ordering watersheds by tn and tp loads as compared to sparrow regardless of the climate lulc composition or ecoregion of the watershed thus steplgrid is particularly useful as a tool for relative comparisons across multiple watersheds the minimum data and input parameter requirements and the ability to handle gridded data make steplgrid a potentially useful screening tool for modelling the impact of climate and lulc change on water quality at large spatial scales when used as a screening tool steplgrid could be used to identify coastal systems at higher risk of suffering from climate and lulc change which could then be studied through further detailed analysis steplgrid is relatively more effective at predicting tp loads than tn when compared to sparrow a trend that may be explained by steplgrid only considering surface water transport of nutrients steplgrid is also more likely to overpredict nutrient loads given that it does not account for nutrient losses as compared to sparrow steplgrid was most effective at estimating nutrient load magnitudes for watersheds in the gulf of mexico and south atlantic regions of the u s and least effective at estimating nutrient loads for north pacific watersheds the literature includes more emc values for land uses with intensive human use i e urban agricultural production and less for natural lands i e forest range consequently the relative lack of information on emcs for natural lands likely contributes large uncertainty to steplgrid outputs when the model is applied to watersheds with extensive natural areas in this study we observed that steplgrid was more likely to underpredict nutrient loads in areas with extensive forested land for studies aiming to compare or rank nutrients loads across several watersheds the use of regional or national emcs values might be suitable however for studies that require accurate pollutant estimates for a particular area local emc values may be necessary local emcs will better consider local characteristics that influence nutrient loading such as the presence of natural nutrient deposits steplgrid was only tested against sparrow for coastal watersheds across the u s additional research is needed to evaluate steplgrid s ability to estimate nutrient loads of interior watersheds where the effects of snowfall and steep elevations may be more pronounced relative to the watersheds studied here software and data availability all software and data used in this analysis are publicly available and outlined in the methods section of the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors report no conflict of interest the authors thank daniel wise from the u s geological survey oregon water science center who shared the uncertainty information for the west coast as well as lisa lowe and andrew petersen for facilitating the use of north carolina state university s high performance computing resources lrm performed all analyses interpreted results and prepared the manuscript ngn contributed to study design interpretation of results and manuscript preparation this work is supported by a u s geological survey southeast climate adaptation science center graduate fellowship awarded to lrm and the usda national institute of food and agriculture hatch project 1016068 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105344 
25624,this study investigated whether a simple model could scale across watersheds and effectively predict runoff driven nutrient loading as compared to a model with more complex process representation a lumped model the spreadsheet tool for estimating pollutant load stepl was adapted to use gridded data steplgrid and applied to 112 coastal watersheds across the atlantic gulf and pacific coasts of the contiguous united states u s to estimate annual runoff driven total nitrogen tn and total phosphorus tp loads steplgrid outputs were compared to those of the spatially referenced regression on watershed attributes sparrow model relative to sparrow steplgrid produced comparable estimates of tn and tp loads for most watersheds studied and its predicted loads were more similar to sparrow for tn than tp steplgrid was particularly effective at rank ordering watersheds by tn and tp loads as compared to sparrow indicating that steplgrid was useful for relative comparisons across diverse watersheds keywords nonpoint source pollution nitrogen phosphorus water quality modeling eutrophication sparrow stepl 1 introduction estuaries are among the most productive ecosystems globally and provide a wide range of ecosystem services barbier et al 2011 caffrey 2004 cloern et al 2014 freshwater inputs from upstream lands drive estuarine health and functioning and result in estuarine systems integrating the conditions of their contributing watersheds benson 1981 palmer et al 2011 watersheds that drain to estuaries are among the most densely populated and the stress generated by anthropogenic activities in these watersheds has caused severe disturbances to estuarine ecosystems including increased nutrient loads habitat loss and changes in sediment delivery carpenter et al 1998 dauer et al 2000 foley et al 2005 mallin et al 2000 vitousek et al 1997 coastal areas are particularly vulnerable to eutrophication national research council 2000 and previous national estuarine eutrophication assessments in the united states u s have shown that over two thirds of the nation s estuarine systems are moderately to highly eutrophic bricker 1999 bricker et al 2008 point sources e g wastewater treatment significantly contribute to nutrient pollution but nonpoint sources e g urban and agricultural runoff fossil fuel combustion are considered the primary drivers of estuarine eutrophication carpenter et al 1998 with riverine inputs being one of the main transport mechanisms delivering nutrients to the coasts beusen et al 2016 national research council 2000 seitzinger et al 2005 2010 sharples et al 2017 in addition to local pressure global environmental change is likely to alter coastal systems by increasing nutrient inputs aggravated by changes in hydrological regimes and precipitation patterns murdoch et al 2000 whitehead et al 2009 a wide range of surface water quality models have been developed and can be used to assess the impact of environmental change on water resources these models range in structure assumptions and complexity of process representation wang et al 2013 the choice of water quality model is dictated not only by a model s suitability for a given application but also by data availability modeler expertise and training and computational constraints a previous study wellen et al 2015 reviewed 257 modelling studies and highlighted that only 5 of 41 water quality models reviewed were among the most commonly employed such as the soil and water assessment tool swat gassman et al 2014 integrated catchment model inca wade et al 2002 whitehead et al 1998 agricultural nonpoint source pollution model annual agricultural nonpoint source pollution model angps annagnps binger and theurer 2001 young et al 1989 hydrological simulation program fortran hspf bicknell et al 2001 donigian et al 1995 and hydrologiska byråns vattenbalansavdelning hbv andersson et al 2005 a recent review also confirmed the predominance of these water quality models and showed that 44 of articles of 3 282 reviewed used swat fu et al 2019 these commonly used mechanistic and highly specified models are generally viewed as being state of the art given that they represent many processes known to drive water quality dynamics robson 2014 robson et al 2018 complex process based models offer insight into the roles of key environmental processes in driving nutrient pollution and help to evaluate the effectiveness of proposed conservation management strategies and policies national research council 2007 however these models demand data intensive calibration and computational resources schwarz et al 2006 and commonly require the selection of over 50 parameters jackson blake et al 2017 that are often unmeasurable without sufficient data and an in depth understanding of a study system there is limited potential to constrain parameters effectively during calibration and follow best practices in modelling accordingly a review led by wellen et al 2015 revealed that few modelling studies reported a validation assessment i e 57 sensitivity analysis i e 25 and optimization i e 17 during model development although complex models provide useful information of a system s dynamics and drivers their limitations have led to the development of user friendly parsimonious models that are minimally parameterized and computationally efficient several parsimonious water quality models have been developed recently such as spatially referenced regression on watershed attributes sparrow schwarz et al 2006 generalized watershed loading function gwlf lehning et al 2002 gis pollutant load application pload usepa 2001 long term hydrologic impact assessment l thia engel 2003 spreadsheet tool for estimating pollutant load stepl tetra tech inc 2011 and waquoit bay land margin ecosystems research nitrogen loading model wblmer nlm valiela et al 1997 here we define parsimonious models as models that accomplish a desired level of explanation or prediction with a minimum number of parameters or predictor variables these models have a lower number of parameters compared to more complex models e g 148 parameters for the complex model inca p vs 28 parameters for the parsimonious model simplep jackson blake et al 2017 therefore parsimonious models require fewer resources than complex models nevertheless not all models described as parsimonious have the same level of complexity some models are export coefficient based models e g stepl pload l thia while others combine statistical approaches and or mechanistic processes e g sparrow gwlf a few models have been applied at large spatial scales such as sparrow preston et al 2011 and global nutrient export from watersheds news mccrackin et al 2013 however water quality modeling challenges are exacerbated when applied at large spatial scales spatial heterogeneity sparse measurements for calibration and limited human and computational resources all limit large scale modeling fu et al 2019 therefore most existing research on the use of parsimonious nutrient loading models has focused on applications at small spatial scales jackson blake et al 2017 liu et al 2017 nejadhashemi et al 2011 leaving gaps in our understanding of the conditions in which simpler and less computationally expensive approaches may be effective for estimating watershed scale nutrient loads in freshwater runoff and evaluating the impact of different modes of change e g policies global to regional environmental drivers climate change across multiple watersheds elliott et al 2016 while some studies only require assessment of a single or few watersheds the ability to evaluate many watersheds simultaneously is increasingly becoming important in the context of climate change assessments determining nutrient load inputs at large spatial scales is necessary for developing or adapting management strategies that extend across regional or national boundaries especially to predict impacts of future change and identify systems at a higher risk of degradation due to local change e g land use conversion and global climate change however the use of future projections as inputs to water quality models increases the difficulty of applying these models at large spatial scales modeling future nutrient loads requires using several projections that correspond to different scenarios e g representative concentration pathways 4 5 and 8 5 and general circulation models e g those from the coupled model intercomparison project and these projections are often gridded the use of a model with simplified process representation could allow for water quality models to be run for multiple watersheds across many different scenarios thus allowing researchers and natural resource managers to identify systems at higher risk of degradation assess potential outcomes and prioritize further analysis to date guidance on the use of parsimonious models for assessing nutrient loading across multiple watersheds under uncertain conditions remains limited this study sought to answer the question to what extent can a model with simplified process representation and minimal parameter and data inputs effectively predict runoff driven nutrient loading to estuarine watersheds across multiple regions the objectives of our study were to 1 compare performance between nutrient loading models with varying levels of detailed process representation specifically focusing on total nitrogen tn and total phosphorus tp 2 evaluate whether a simplified approach is effective at predicting absolute or relative load magnitudes as compared to a more complex model and 3 assess which watersheds were associated with high low comparative performance of the simplified model in the present work the spreadsheet tool for estimating pollutant load stepl was selected as a representative nutrient load model with simplified process representation and data inputs i e lulc precipitation and was used as the test model in this study stepl was adapted to support gridded data steplgrid steplgrid s efficacy in predicting nonpoint source nutrient loading in estuarine watersheds was compared to that of the spatially referenced regression on watershed attributes sparrow model a parsimonious hybrid process statistical model with relatively more complex model structure as compared to steplgrid steplgrid and sparrow were applied to 112 estuarine watersheds across all coastal regions of the contiguous u s which vary in terms of climate land use and land cover our hypothesis is that a simplified model is capable of estimating estuarine nutrient loads and providing insight for large scale management decisions furthermore the study presented here also advances the use of national scale gridded products for estimating and comparing nutrient loads across large spatial extents 2 methods 2 1 study area 2 1 1 study watersheds the study encompassed 112 estuarine watersheds across five large regions of the united states fig 1 the north atlantic n 38 area 373 137 km2 south atlantic n 19 area 323 059 km2 gulf of mexico n 24 area 788 078 km2 north pacific n 20 area 79 811 km2 and south pacific n 11 area 183 367 km2 fig 1 their land use and land cover lulc compositions sizes rainfall amounts and elevation distributions are presented in figs 2 and s1 respectively north and south atlantic fig 1a and b the climate in these regions is predominantly temperate forested and urban lands dominate the north atlantic and south atlantic regions cropland is also extensive in the south atlantic further watersheds in the south atlantic i e median of 12 444 km2 are larger than those in the north atlantic i e median of 2 500 km2 gulf of mexico fig 1c the temperature gradient does not vary considerably across the gulf of mexico region while rainfall patterns do in the western part of the gulf of mexico i e west of the mississippi river fairly arid conditions are observed while subtropical and humid conditions are observed in the eastern part i e east of the mississippi river lulc patterns follow the rainfall patterns in the western and drier areas range and pasture dominate while cropland dominates in the eastern and humid areas the elevation is fairly low across the gulf of mexico s watersheds and their size is the largest compared to the four other regions north and south pacific fig 1d and e the west coast of the u s has a wide range of climatic conditions from very wet and cool i e north pacific to dry and warm i e south pacific the north pacific region is dominated by forest while the south pacific has less forest and more rangeland the west coast has been strongly structured by tectonism and volcanism that shaped the coastline as a result the north pacific region contains numerous inlets and estuaries while the south pacific region is characterized by a relatively straight coastline interrupted sometimes by estuaries and headlands the coastal watersheds located in the north and south pacific regions have the highest average elevation of all the study regions except for a few the sizes of the watersheds are fairly small along the west coast around 2 000 km2 2 1 2 estuarine watershed delineation the u s geological survey usgs previously delineated estuarine watersheds across the five study regions preston et al 2011 however vector based shapefiles of these watersheds were not available therefore the watersheds and their associated names were identified and delineated according to the noaa coastal assessment framework noaa 2007 the following watersheds were combined to correspond to estuarine watersheds as previously defined by the usgs 1 the upper and lower laguna madre 2 the north santee south santee river estuary and charleston harbor and 3 the massachusetts bay and boston harbor the differences between the area of the watersheds delineated in this study versus those defined by the usgs were computed to ensure that the delineations matched watersheds that had an absolute area difference i e between the delineation of the usgs and ours greater than 15 were not considered in the analysis i e wells bay hampton harbor estuary south maryland coastal bays indian river lagoon sarasota bay tampa bay st andrew bay additionally the mississippi river and columbia river watersheds were removed from the analysis the mississippi river watershed spans all or part of 32 states and consists of hundreds of sub watersheds that share dissimilar climates e g continental subtropical semi arid and lulc patterns therefore a direct comparison of this watershed with the other u s estuarine watersheds was deemed inappropriate further the columbia river watershed is also very large and drains a portion of canada data products used as inputs in the study presented here span only the u s resulting in data for the columbia river watershed being incomplete therefore the columbia river watershed was excluded from the analysis in total 112 estuarine systems were delineated fig 1 2 2 stepl the spreadsheet tool for estimating pollutant load stepl is a lumped parameter pollutant load model developed by tetra tech inc for the u s environmental protection agency epa the use of stepl has primarily been limited to watersheds less than 1 000 km2 in area liu et al 2017 nejadhashemi et al 2011 ohio epa 2005 with its efficacy not yet tested at larger spatial scales stepl calculates annual nutrient loads at the watershed scale as a product of surface runoff and event mean concentrations emcs emcs are empirical concentrations of a specific pollutant e g tn tp associated with a unit volume of runoff following an average storm emcs are reported by lulc type stepl calculates direct runoff from the average rainfall per event with the scs cn method then the direct runoff from the annual average event scale rainfall depth is multiplied by the average number of rain days in a year as well as the percentage of rain day events that generate runoff tetra tech inc 2011 the amount of direct annual runoff generated from each lulc class in a watershed is tabulated and multiplied by the associated emcs which are defined by the modeler typically by identifying relevant values in the literature these lulc specific loads are then summed across the watershed to estimate annual watershed scale runoff driven pollutant loads the model is currently shared as a microsoft excel spreadsheet with a user friendly visual basic interface stepl has been used by practitioners to develop total maximum daily loads tmdls for nutrients and identify strategies to reduce nutrient loads e g ohio epa 2005 because of its relatively simple structure and use of publicly available data layers as inputs stepl is easy to interpret and apply making it favored by many practitioners who seek model based decision support when screening management strategies in the absence of extensive computational resources in particular stepl allows for the potential nutrient reduction effects of best management practices bmps and low impact development lid to be estimated for urban areas tetra tech inc 2011 pollutant load reduction resulting from the implementation of bmps is computed using empirical efficiency factors stepl allows the user to choose among a list of bmp names and efficiencies for different land uses and pollutant types e g bioreactor buffer controlled drainage conservation tillage tetra tech inc 2011 furthermore the first flush phenomenon has been studied in the design of bmps bach et al 2010 kang et al 2006 lee et al 2002 smith 2001 in this study we did not consider bmps and the first flush phenomenon the stepl model computes nutrient loads at the annual scale and previous research has found that first flush phenomenon is uncertain for large watersheds due to the dilution and transport of the pollutant mamun et al 2020 additional assumptions were made first we neglected surface water groundwater interactions for the study area over long time horizons groundwater is considered to usually be a small or negligible fraction of the coastal nutrient and water budgets loicz n d spruill and bratton 2008 furthermore we did not consider nutrient sinks e g denitrification in wetlands instream processing lake attenuation and water residence time notably because the capacity of ecosystems to assimilate and remove n and p is often largely exceeded by human nutrient inputs and recovery from nutrient saturation could take decades to millennia chen et al 2018 dupas et al 2018 frei et al 2020 goyette et al 2018 haas et al 2019 randall et al 2019 van meter et al 2016 2 2 1 implementing stepl in r for spatial use the stepl model could not be used in its current visual basic based spreadsheet form because the model was applied spatially using gridded input data i e lulc soils precipitation the original spreadsheet form of stepl uses unique values entered in an excel spreadsheet instead of gridded inputs specifically the epa stepl model computes average annual direct runoff from a model database that provides average annual rainfall rain days and correction factors for rainfall and rain day totals for the counties in which the national climate data center stations are located to facilitate use of gridded inputs and reduce the coarseness of the model a distributed version of the model in which input parameters are specified for each grid cell was developed in r r core team 2020 a distributed model can account for spatial variability in physical and chemical characteristics on a cell by cell basis therefore distributed models are considered more spatially accurate in representing nonpoint source pollution the stepl distributed model was constructed and run by computing the annual runoff at the grid scale 30 m using lulc soil and precipitation data and then calculating runoff driven nutrient loads in each cell fig 3 this modified version of stepl is hereafter called steplgrid to account for its ability to use gridded spatial data although the equations of stepl remain the same steplgrid allowed for the integration of spatial data inputs and for q and cn to be estimated in a more precise and distributed way across the watershed instead of being aggregated through a spatial weighting approach in the standard lumped version of stepl 2 2 2 input data the distributed steplgrid model implemented in r used the scs cn method developed by the u s natural resources conservation service to estimate runoff produced from an average storm for each 30 m grid cell the scs cn approach requires lulc and hydrologic soil group data lulc data were obtained from the 2001 national land cover database nlcd homer et al 2012 hydrologic soil group data were obtained from gssurgo soil survey staff n d and accessed through arcgis esri 2018 gaps in hydrologic soil group values were filled with the resampled hysogs250m dataset ross et al 2018 scs curve numbers from nrcs tr 55 nrcs 1986 were assigned to each grid cell based on the cell s combination of lulc and hydrologic soil group table 1 to estimate annual direct runoff the average runoff generated from a storm was multiplied by the number of rainy days observed within the year precipitation was obtained and resampled from the gridmet dataset abatzoglou 2013 gridmet is a spatial dataset of surface meteorological variables with spatial resolution of 1 24 approximately 4 km to determine the nutrient loads associated with annual runoff produced in a watershed annual direct runoff was multiplied by lulc specific emcs emcs are an essential component in the nutrient load estimation procedures however emcs can be uncertain due to differences in how storm events are defined water quality sampling methods and variation in how emcs are reported across areas of different watershed lulc composition or size fu et al 2019 therefore a review was conducted to identify and summarize published emc values for tn and tp in total emcs from 22 publications including both peer reviewed articles and reports from the grey literature were tabulated tables s2 6 and fig s7 to ensure the emcs were representative of our area of study we excluded all emc values that were calculated outside the contiguous u s therefore the emc values were tabulated across the states included in our analysis e g north carolina texas washington and outside our study area e g minnesota kansas michigan furthermore to account for uncertainty in emc values a permutation analysis was performed with the emcs identified from the literature see section 2 4 2 to generate different parameter scenarios that encompassed the range of reported emc values permutations of the 0 1 0 5 median and 0 9 quantiles of the emc distributions were created for each of the lulc classes to generate different sets of emcs that spanned the range of emc uncertainty from our literature review then steplgrid was run for each watershed for each of the emc sets obtained from the permutation analysis the lulc classes were based on the nlcd lulc classification definitions for this study several nlcd classes were grouped together to create five broader classes used in the analysis 1 forest nlcd classes 41 deciduous forest 42 coniferous forest 43 mixed forest 2 agriculture nlcd class 82 cultivated crops 3 pasture nlcd class 81 pasture hay 4 range nlcd classes 51 shrubland 71 grasslands herbaceous and 5 urban nlcd classes 21 open urban 22 low density urban 23 medium density urban 24 high density urban 2 3 sparrow the spatially referenced regression on watershed attributes sparrow model was used as a reference model in this study sparrow developed by the us geological survey usgs is widely used by scientists and decision makers to predict long term average annual nutrient loads values of water quality constituents delivered to downstream receiving water bodies e g estuaries lakes preston et al 2009 sparrow is a hybrid process based and statistical model as it shares some attributes of both mechanistic and statistical models that are used to explain in stream loads of water quality constituents in relation to upstream sources and watershed properties e g precipitation lulc and soil characteristics the model uses statistical functions to estimate the sources fluxes and transformations of water quality constituents in a watershed water quality and streamflow measured at monitoring sites are used to estimate water quality constituent mass furthermore sparrow uses a spatially explicit structure defined by a river reach network through which upstream and downstream monitoring sites are connected within the model contributing drainage areas are determined for each stream reach allowing for watershed characteristics e g lulc soil to be used as explanatory variables in the model and assessed as water quality drivers sparrow requires expertise in hydrologic and water quality modelling for processing and assembling input data calibrating models and calculating model uncertainty therefore sparrow is relatively more structurally complex and challenging to use as compared to stepl the national water quality assessment nawqa program used sparrow to assess nutrient conditions across six regions of the u s for the year 2002 the nawqa regions are consistent with the five regions considered in this study but the nawqa assessment included the great lakes as an additional region garcia et al 2011 moore et al 2011 moorman et al 2014 preston et al 2009 rebich et al 2011 schwarz et al 2011 smith et al 1997 the regional models were calibrated and validated over several years to estimate nutrient load transport to u s estuaries and facilitate comparisons between watersheds regarding differences in dominant sources of nutrient pollution these regional sparrow models determined the proportion of watershed outlet tn and tp loads associated with different point and nonpoint sources e g nutrient loads resulting from fertilizer application urban runoff wastewater etc contrary to stepl sparrow has been used across the conterminous us and is designed to be used across spatial scales ranging from small watersheds i e tens of km2 to large watersheds i e several million km2 preston et al 2011 smith et al 1997 2 4 comparing steplgrid and sparrow outputs steplgrid and sparrow were compared based on their tn and tp load estimates from nonpoint sources modeled tn and tp loads from point sources were excluded from the analysis here steplgrid outputs were evaluated against sparrow outputs instead of observational in stream nutrient load data given that the tn and tp load estimates produced from steplgrid do not comprise all potential sources of nutrients e g nitrogen inputs through atmospheric deposition or nutrient sinks accordingly steplgrid is not designed to estimate in stream concentrations and loads but is instead a planning tool that allows for prediction of total runoff driven nutrient loads sparrow outputs also report total estimated nutrient loads generated in a watershed but while accounting for hydrologic connectivity and river network structure thus by comparing steplgrid and sparrow outputs this analysis tested whether additional model complexity i e hydrologic connectivity was needed to effectively estimate nonpoint tn and tp loads across watersheds from a range of environmental and climatic conditions both steplgrid and sparrow encounter uncertainties therefore uncertainties remain in the interpretation of steplgrid and its ability to predict real nutrient loads however the intercomparison between sparrow and steplgrid still improves understanding of water quality model differences and uncertainties related to model parameterization and structure van vliet et al 2019 further model intercomparison is useful for highlighting model disagreements and providing a more comprehensive assessment of dominant pollution processes and sources van vliet et al 2019 2 4 1 aligning model outputs the final outputs of the steplgrid model are annual tn and tp loads at the watershed scale in contrast sparrow nutrient load outputs are calibrated to the total load and as a function of distinct source types moreover the five regional sparrow models produced by the usgs divided nonpoint sources into different categories garcia et al 2011 moore et al 2011 moorman et al 2014 preston et al 2009 rebich et al 2011 schwarz et al 2011 smith et al 1997 to facilitate comparisons between sparrow and steplgrid model outputs the nutrient loads attributed to nonpoint sources in sparrow were aggregated for tn and tp 2 4 2 steplgrid uncertainty analysis an uncertainty analysis was performed with the steplgrid model to evaluate the variation in modeled tn and tp loads as a function of emc value uncertainty to evaluate the propagation of the uncertainty in the emc values we used the best available literature to select emc values across our study area see section 2 2 2 permutations of the 0 1 0 5 median and 0 9 quantiles of the emc distributions were created for each of the five lulc classes i e urban crop forest pasture range table 2 as a result 243 different emc sets n 35 were obtained from the permutation analysis steplgrid was run for each watershed for each of the 243 emc sets 2 4 3 sparrow output uncertainty the regional sparrow models reported output uncertainty with different metrics the gulf of mexico sparrow model reported output uncertainty as the 10th and 90th percentile confidence interval for the total delivered loads by watershed i e the total tn and tp from nonpoint and point sources using a parametric bootstrapping approach with hundreds of model iterations therefore for the gulf of mexico region the 10th and 90th percentile confidence interval was applied proportionally to the nonpoint source contribution for the north pacific region the uncertainty was considered to be the reported 10th and 90th percentile confidence interval of the individual nonpoint source contribution in contrast output uncertainty was not reported in the regional sparrow models for the south pacific north atlantic and south atlantic for these three regions an uncertainty interval of 35 of the mean nonpoint source tn and tp loads was assumed the range of 35 was selected as a conservative uncertainty estimate the minimum sparrow uncertainty associated with modeled nutrient loads reported from the sparrow models for the north pacific and gulf of mexico regions was 35 thus the actual uncertainty in modeled nutrient loads from sparrow for these two regions could either be greater or less than 35 2 4 4 model comparison metrics to quantify steplgrid s efficiency in predicting absolute tn and tp loads relative to sparrow several performance statistics were calculated table 3 these metrics included the nash sutcliffe efficiency nse the modified nash sutcliffe efficiency coefficient e that is less sensitive to extreme values krause et al 2005 the percent bias pbias and the ratio of the root mean square error to the standard deviation of observed data rsr of note rsr is not an independent measure of the model performance as nse 1 rsr2 the metrics were calculated using load estimates from all watersheds in each of the five study regions the steplgrid outputs used in these calculations were those that corresponded to the 0 5 quantile i e median emc set to evaluate whether the performance statistics produced satisfactory or unsatisfactory results the values were compared to benchmarks proposed by moriasi et al 2007 2015 table 4 additionally model outputs were compared to evaluate whether steplgrid was consistent with sparrow in terms of ranking watersheds by their tn and tp loads the signed rank test also called the wilcoxon signed rank test was used to determine whether the median difference between paired observations i e nutrient loads computed with steplgrid and sparrow was equal to zero the null hypothesis of the signed rank test was that the sparrow and steplgrid model outputs had the same distribution an alpha level of 0 05 was used to evaluate significance furthermore the relative ranking of nutrient loads across the watersheds was assessed to determine whether steplgrid model outputs were consistent with sparrow in terms of the ranked nutrient loads for watersheds across the study regions nutrient loads were sorted and replaced by their rank for both sparrow and steplgrid steplgrid and sparrow ranks were compared to the one to one line and the nse was computed hereafter nse values calculated from ranks are referred to as nserank furthermore the spearman s rank order correlation spearman s ρ was computed to quantify the strength and the direction of the monotonic association between the two models outputs 3 results 3 1 comparative performance statistics to compare steplgrid outputs to sparrow using common statistical performance tests steplgrid was run with the median of the emc distributions created for each of the lulc classes see section 2 4 2 the following section compared steplgrid results produced with the median emc to the mean sparrow outputs in order to quantity steplgrid s efficiency relative to sparrow 3 1 1 tn compared to sparrow steplgrid was effective at relatively ranking tn loads nserank 0 78 spearman s ρ 0 89 table 5 and fig 4 a but the tn load magnitudes differed nse 0 25 rsr 0 87 pbias 3 4 table 5 and fig 5 a when aggregating the total tn loads across all study watersheds from all regions the totals were comparable between sparrow 377 million kg and steplgrid 390 million kg with a 3 absolute difference pbias 25 nserank 0 60 spearman s coefficient ρ 0 89 p value 0 05 showed agreement between steplgrid and sparrow results while r2 0 39 nse 0 25 e 0 33 and rsr 0 87 showed poor agreement table 5 based on the nserank and spearman s ρ values steplgrid was similar to sparrow in rank ordering the watersheds in terms of tn loads across all regions table 5 and fig 4a however the remaining performance metrics reveal that steplgrid did not consistently produce comparable absolute load estimates for tn when considering all study watersheds in the five study regions table 5 although model efficiency statistics demonstrated that steplgrid estimates of tn loads were generally unsatisfactory compared to sparrow when all study watersheds were considered in aggregate steplgrid had comparable performance to sparrow within certain regions for all of the regions steplgrid captured the relative ranking of tn loads nserank 0 60 and spearman s ρ 0 80 when comparing absolute load estimates to sparrow steplgrid had very good performance for the south atlantic and satisfactory performance for the north atlantic and south pacific table 5 however the signed rank test p value 0 05 revealed that the tn load distributions of steplgrid and sparrow were not identical for the north atlantic region even though model performance metrics i e nse e rsr pbias nserank were satisfactory table 5 for both the gulf of mexico and the north pacific all statistical tests were judged unsatisfactory for estimating absolute tn loads compared to sparrow and the steplgrid and sparrow distributions for tn load estimates were not identical signed rank test p value 0 05 table 5 although there were regional trends in steplgrid model performance relative to sparrow no clear relationships between model performance and watershed characteristics were identified r2 for model prediction differences and watershed size 0 11 fig 5a similarly no trends were observed between topographic climatic and lulc patterns and tn load estimate differences between sparrow and steplgrid fig 6 a therefore given the nserank value and significant results of the spearman s correlation steplgrid equivalently estimated relative tn loads compared to sparrow at the national scale while model efficiency statistics revealed that estimates of absolute tn loads across all coastal watersheds were not consistently equivalent to sparrow however regional scale efficiency statistics showed that steplgrid predicted similarly to sparrow absolute and relative tn loads for the north atlantic south atlantic and south pacific but predicted poorly for the gulf of mexico and north pacific regions 3 1 2 tp in contrast to tn model performance metrics showed that both the magnitude nse 0 84 and the relative ranking nserank 0 85 spearman s ρ 0 93 of tp loads were equivalently captured by steplgrid compared to sparrow at the national scale table 5 figs 4b and 5b as for tn steplgrid and sparrow tp loads aggregated across all coastal areas of the continental us were comparable i e 3 absolute difference in tp load between the models with the aggregate steplgrid estimate being 49 74 million kg and sparrow being 48 03 million kg model performance evaluation metrics applied for all watersheds across the five regions confirmed that steplgrid and sparrow predicted similar tp loads at the national scale pbias 10 nse 0 84 e 0 67 rsr 0 40 nserank 0 85 spearman s ρ 0 93 and r2 0 88 indicated very good agreement between the two models table 5 thus based on the performance evaluation criteria table 4 steplgrid parameterized with the median emc set was judged very good in predicting tp loads using sparrow outputs as a benchmark at the regional scale the ranking analysis revealed that steplgrid was able to capture the relative ranking of tp loads across watersheds but performance statistics showed that steplgrid model efficiency as compared to sparrow was mixed across the regions steplgrid had very good performance for the north atlantic and gulf of mexico regions good performance for the south pacific region and unsatisfactory performance for the south atlantic and north pacific moreover for the three regions with very good performance north atlantic gulf of mexico south pacific steplgrid also captured the relative ranking of tp loads table 5 however results were mixed for the south atlantic region where all statistical performance tests indicated unsatisfactory results nse 0 07 e 0 32 rsr 0 94 except r2 0 88 pbias 41 2 and nserank 0 82 table 5 also the signed rank test for the south atlantic p value 0 026 revealed that the steplgrid tp load distribution was not equivalent to that of sparrow for the north pacific region statistical performance tests revealed that steplgrid was unsuccessful in capturing the magnitude of tp loads and had also different tp load distributions than sparrow signed rank test p value 0 05 table 5 although low agreement was observed between steplgrid and sparrow in predicting tp loads of watersheds in the north pacific and south atlantic regions steplgrid predicted more similarly tp at the national and regional levels than for tn furthermore as was seen for tn there were regional trends in steplgrid model outputs relative to sparrow however no clear relationships between model results and watershed characteristics were identified there was no clear relationship in model prediction differences and watershed size r2 0 0021 fig 5b moreover clear trends were not seen between topographic climatic and lulc patterns and tp load differences between sparrow and steplgrid fig 6b therefore given the model efficiency statistics and significant results of the spearman s correlation steplgrid was successful in estimating tp load as compared to sparrow for most of the study regions regional scale efficiency statistics showed that steplgrid predicted similarly to sparrow in the absolute and relative tp loads for the north atlantic gulf of mexico and south pacific regions but poorly for the south atlantic and north atlantic regions overall steplgrid was more efficient in predicting tp loads than tn loads across the 112 estuarine systems 3 2 comparison of steplgrid and sparrow load estimates sparrow and steplgrid outputs from the uncertainty analysis were compared when comparing steplgrid and sparrow outputs three cases were observed with regards to their load estimate uncertainties 1 no intersection i e steplgrid and sparrow do not share common values in their uncertainty intervals 2 partial intersection i e steplgrid and sparrow share some common values in their uncertainty intervals and 3 full intersection i e steplgrid uncertainty encompasses the entire range of sparrow values these three cases are referenced throughout the results 3 2 1 tn of the 112 study watersheds 89 79 had steplgrid and sparrow tn load uncertainties that fully n 26 or partially n 63 intersected fig 7a s8 s12 and table 6 the regions associated with the most comparable tn load estimate uncertainty ranges between sparrow and steplgrid were the north and south atlantic in the north atlantic 37 of 38 watersheds had intersecting uncertainty intervals between steplgrid and sparrow of the 37 watersheds with intersecting uncertainty ranges for tn 24 had partial intersection and 13 had full intersection in the north atlantic only the delaware inland bays watershed had no intersection between steplgrid and sparrow uncertainties all of the watersheds in the south atlantic n 19 had intersecting uncertainty ranges for tn in which 11 were partially intersecting and 8 were fully intersecting i e neuse river estuary bogue sound new river estuary winyah bay st helena sound savannah river estuary st marys river cumberland sound and st johns river estuary outside of the north and south atlantic tn estimates from steplgrid were not always consistent with those of sparrow of the 24 watersheds in the gulf of mexico 16 were partially intersecting 3 were fully intersecting i e charlotte harbor aransas bay and laguna madre and 5 had no intersecting uncertainty range i e apalachee bay apalachicola bay mobile bay mermentau bay and corpus christi bay for watersheds in the south pacific n 11 six had intersecting uncertainty ranges for tn in which five were partially intersecting i e san pedro bay san francisco bay north eel river humboldt bay and klamath river and one fully intersecting i e san francisco bay south the north pacific region where elevation and precipitation are the highest had 12 of 20 watersheds that did not have intersecting sparrow and steplgrid uncertainty ranges for tn fig 7 a i only one watershed in the north pacific was associated with a full intersection i e rogue river at the national scale half of the watersheds shared between 50 and 100 of the stepl and sparrow uncertainty ranges for tn load estimates and most watersheds with partial intersection had over 50 overlap between steplgrid and sparrow tn load estimates i e mean 56 69 and median 61 17 table 6 the extent to which uncertainty ranges overlapped between steplgrid and sparrow varied across watersheds and regions for example the upper chesapeake bay had only 2 30 overlap between the tn load uncertainty range for steplgrid and sparrow while san antonio bay had 95 78 overlap the north atlantic and south atlantic are the regions with the greatest overlap in tn load uncertainty ranges followed by the south pacific the gulf of mexico and the north pacific regions table 6 although steplgrid and sparrow uncertainties intersected for 79 of the watersheds studied trends were observed across regions in terms of whether steplgrid was likely to under or over predict tn loads compared to sparrow fig 7 s8 s12 steplgrid underpredicted tn loads relative to sparrow for most watersheds in the north atlantic n 22 fig 7a ii on the other hand steplgrid tended to overpredict tn loads in watersheds of the south atlantic n 14 and for almost all of the watersheds in the gulf of mexico n 23 as compared to sparrow fig 7a ii in the north and south pacific regions steplgrid underpredicted tn loads though steplgrid was more likely to overpredict tn loads in the south pacific for larger watersheds i e san francisco bay north eel river and klamath river for the gulf of mexico a similar trend was observed in that steplgrid generally overpredicted tn loads for large watersheds fig 7a ii 3 2 2 tp fully or partially intersecting uncertainty intervals were observed in tp load estimates between steplgrid and sparrow for 90 81 of the 112 study watersheds fig 7b s8 s12 and table 6 of the 90 watersheds with intersecting uncertainty ranges for tp 42 were partially intersecting and 48 were fully intersecting the majority of the watersheds with intersecting uncertainty intervals for tp load estimates were in the north atlantic south atlantic and gulf of mexico regions of the 29 watersheds with intersecting uncertainty ranges for tp in the north atlantic 11 had partial intersection and 18 had full intersection table 6 only nine of the watersheds in the north atlantic did not have common intersection intervals between steplgrid and sparrow uncertainties i e cobscook bay blue hill bay buzzards bay englishman machias bay damariscotta river estuary great south bay cape cod bay massachusetts bay boston harbor waquoit bay fig 7b i all of the watersheds with no intersection were relatively small in area ranging from 53 km2 i e waquoit bay to 2 461 km2 i e englishman machias bay as was observed for tn almost all of the watersheds in the south atlantic region n 17 had intersecting uncertainty ranges for tp in which 9 were partially intersecting and 8 were fully intersecting i e neuse river estuary bogue sound new river estuary winyah bay st helena sound savannah river estuary st marys river cumberland sound and st johns river estuary in the south atlantic the pamlico and st catherines sapelo sounds had no intersection between steplgrid and sparrow uncertainty intervals similar to the south atlantic region almost all of the gulf of mexico watersheds had intersecting uncertainty ranges for tp n 23 and most of them were fully intersecting n 18 in this region only the corpus christi watershed did not have intersecting steplgrid and sparrow uncertainty intervals for tp estimates fig 7b i the south pacific region had 7 watersheds with intersecting uncertainty ranges for tp in which 3 were partially intersecting i e elkhorn slough klamath river humboldt bay and 4 were fully intersecting i e san pedro bay san francisco bay south san francisco bay north eel river two thirds of the study watersheds had between 50 and 100 overlap between the steplgrid and sparrow uncertainty ranges with the north atlantic and gulf of mexico regions having the highest percent overlap followed by the south atlantic table 6 as with tn load estimates a wide range was observed in terms of the percent overlap of the uncertainty intervals between steplgrid and sparrow of the watersheds with partially intersecting uncertainty intervals willapa bay had the lowest percent overlap with only 0 17 while chester river estuary had the greatest overlap at 99 25 table 6 however most of the watersheds with partial uncertainty interval intersection shared over 50 overlap i e mean 61 24 and median 65 60 the north and the south pacific regions had the lowest percentage of common intervals between sparrow and steplgrid as observed for tn some regions were associated with steplgrid regularly under or over predicting tp loads as compared to sparrow fig 7 however whereas there were clear trends in under and over prediction of tn across the different study regions the results were more mixed for tp overall no clear trends were observed for the north atlantic south pacific gulf of mexico for example of the 8 watersheds with partial intersection in the gulf of mexico steplgrid overpredicted tp loads with steplgrid as compared to sparrow in approximately half of these watersheds and the other half were underpredicted fig 7b ii moreover of the 18 watersheds that had full intersection between steplgrid and sparrow uncertainty intervals in the gulf of mexico 50 had steplgrid uncertainty ranges that were centered within sparrow uncertainty ranges fig 7b iii the south pacific and the north pacific are the regions where predominant overprediction and underprediction trends were more readily observed 68 n 13 of the watersheds in the south pacific tended to have overpredicted tp loads computed with steplgrid as compared to sparrow while 70 n 14 of those in the north pacific tended to underpredict sparrow tp loads 4 discussion in the present work we compared runoff driven tn and tp load estimates from the steplgrid and sparrow models for 112 coastal watersheds across the contiguous u s although there is sometimes a perception that simple water quality models are limited in their capacity to effectively simulate biogeochemical responses our results show that the steplgrid model produced reasonable load estimates across regions of the u s that span different climates and have varied lulc compositions in particular the rank order of watersheds based on their tn and tp load estimates were very similar between steplgrid and sparrow with nserank and spearman s ρ values ranging from 0 63 to 0 88 and 0 81 to 0 94 respectively across the five study regions moreover when considering all 112 study watersheds the nserank and spearman s ρ values ranged from 0 78 to 0 88 and 0 89 to 0 93 respectively these results indicate that in terms of relatively ranking watersheds by nutrient load steplgrid is effective despite its simplicity however when estimating absolute magnitudes of watershed nutrient loads the results were mixed across the study regions overall when predicting load magnitudes steplgrid estimates of tp were more comparable to those of sparrow than for tn when evaluating the outputs of steplgrid as applied with the median emc set steplgrid and sparrow similarly predicted absolute tn load magnitudes for the north atlantic nse 0 52 south atlantic nse 0 78 and south pacific nse 0 57 but more dissimilarly for the gulf of mexico nse 2 33 and north pacific nse 0 06 regions better predictions were observed for tp for which steplgrid successfully estimated comparable tp loads to sparrow for the north atlantic nse 0 95 gulf of mexico nse 0 78 and south pacific nse 0 91 regions but predicted poorly for the south atlantic nse 0 07 and north pacific nse 0 23 regions however the uncertainty analysis in which steplgrid was run with 243 combinations of emc values provided a more comprehensive view of steplgrid s predictive ability as compared to sparrow for example although the nse for tp loads from watersheds in the south atlantic was low nse 0 07 the uncertainty analysis revealed that over three fourths of these watersheds had overlapping uncertainty intervals for tp which indicates that steplgrid provided reasonable load estimates for tp despite not predicting consistently equivalent values to those of sparrow had only the estimates produced with steplgrid parameterized with the median emc set been analyzed without considering uncertainty steplgrid could have been judged inefficient in predicting tp loads in the south atlantic thus when evaluating steplgrid s performance for individual watersheds and across regions it is essential to consider the uncertainty ranges along with the performance metrics when considering both uncertainty and performance metrics we conclude that steplgrid s tn and tp load estimates were comparable to those of sparrow for the north atlantic south atlantic gulf of mexico and south pacific regions but not the north pacific region for the north and south atlantic and south pacific regions results were interpreted with the conservative uncertainty range of 35 for sparrow load estimates due to the uncertainty ranges for the calibrated model estimates not being reported a lower assumed range in sparrow uncertainty could have resulted in different outcomes for some watersheds i e humboldt bay south puget sound for which stepl and sparrow share few common values in their uncertainty intervals for these watersheds a lower uncertainty range could have meant no intersection between stepl and sparrow uncertainty intervals nevertheless the uncertainty estimates of 35 appeared to be a conservative assumption for the majority of the watersheds studied for which a lower uncertainty range would have not changed the outcomes of the results figs s8 12 regional trends were also observed with regards to steplgrid s tendency to over or under predict tn and tp loads relative to sparrow for tn steplgrid tended to underpredict loads relative to sparrow for most watersheds in the north atlantic north pacific south pacific and overpredict tn loads for watersheds in the gulf of mexico south atlantic and large watersheds in the south pacific for tp steplgrid tended to overpredict phosphorus loads for watersheds in the south pacific and underpredict in the north pacific while no clear trends were observed for the north atlantic south atlantic and gulf of mexico regions trends in over and under prediction can likely be attributed to the way steplgrid and sparrow represent pollutant transport processes for example steplgrid uses emcs and curve numbers to represent nutrient loading and runoff processes and both emcs and curve numbers are empirical in contrast sparrow uses mass balance approaches that consider different n and p sources and retention on land and in river networks moreover sparrow uses a first order decay process as a function of water travel time and water depth preston et al 2009 while steplgrid assumes no losses or sinks and does not consider in stream n processing and travel time in its equations consequently we would expect steplgrid to produce larger tn and tp estimates as compared to sparrow especially for large watersheds thus steplgrid overpredicting loads as compared to sparrow was unsurprising however the frequent underprediction of tn loads for the north atlantic north pacific and south pacific was unexpected in particular for the north pacific region steplgrid underpredicted tn loads as compared to sparrow by a factor of 2 18 two commonalities were observed among the watersheds in the three regions with tn underprediction by steplgrid 1 forested areas were extensive and or 2 elevations and therefore slopes were relatively high especially as compared to watersheds in the gulf of mexico and south atlantic regions forested land cover accounted for a median of 71 4 31 5 and 59 9 of watershed area for the north pacific south pacific and north atlantic respectively and average elevations were 474 m 377 m and 64 m respectively emc values have been reported extensively for anthropogenic land uses e g urban crop pasture but fewer have been reported for natural lands e g forest range tables s2 6 fig s7 therefore the emcs for natural lands e g forest rangeland might not be fully representative of the real range of emcs for these land use types because natural lands can be major contributors of nitrogen to waterbodies the relative paucity of emc values for these land covers as compared to other human intensive land uses may be an important source of uncertainty in the steplgrid model additionally steplgrid estimates nutrient loads using the scs cn approach and the average annual runoff volumes tetra tech inc 2011 while the scs cn method is commonly used to simulate the amount of runoff from a specific rainfall event as a result steplgrid s process representation could mask the effects of individual high precipitation events that deliver large nutrient loads in watersheds with steeper slopes we expect that runoff peaks would be magnified so disparities between individual storm runoff dynamics and average annual runoff calculations might explain why regions with higher slopes were associated with underestimation of tn loads with steplgrid as compared to sparrow the more consistent agreement between steplgrid and sparrow when estimating tp loads may be explained by the greater importance of surface water runoff as a driver of tp as compared to tn erosion and sedimentation are the main mechanisms controlling phosphorus export and surface runoff is the dominant driver of phosphorus loading hart et al 2004 sharpley and syers 1979 because of its chemical physical properties phosphorus binds to most soil and sediments for this reason surface water primarily receives phosphorus through surface flows rather than groundwater resulting in runoff events being associated with large tp loads sharpley 1995a in contrast previous research has revealed that nitrogen is often sourced through legacy stores in groundwater reservoirs or thick unsaturated zones van meter et al 2016 making these zones an important source of nitrogen because steplgrid simulates nutrient loading solely as a function of surface runoff it is unsurprising that steplgrid was more effective in estimating tp loads than tn interestingly although there was greater agreement between steplgrid and sparrow for tp loads steplgrid tended to underpredict tp loads as compared to sparrow in the north pacific region one explanation of the discrepancy in prediction for this region is that steplgrid cannot integrate regional differences in natural nutrient background prior studies estimate that background tp accounted for over 50 of tp loads for multiple watersheds in the north and south pacific regions due to the presence of large natural soil phosphorus levels and phosphate rock deposits with background tp levels being especially high in washington and oregon sharpley 1995b wise and johnson 2011 thus as seen in the results presented here steplgrid appears to underpredict tp loads in areas with elevated background p although emcs should ideally account for background nutrient levels the literature currently does not include enough emc estimates to allow for emcs to be specified by both lulc class and region though modelers could choose to adjust emcs to account for background concentrations 5 conclusions our results show that steplgrid is particularly effective at producing rank ordered tn and tp load estimates as compared to sparrow making steplgrid a useful tool for performing inter watershed comparisons because steplgrid requires limited input data and computational power its utility would be particularly pronounced when applied to evaluate many watersheds under multiple scenarios e g climate land use policy and results presented here support the use of steplgrid for such purposes moreover data inputs required by steplgrid are publicly available for the contiguous u s making its application across multiple watersheds feasible and practical we compared steplgrid outputs to those of sparrow but it is important to acknowledge that sparrow is also an imperfect representation of reality so our findings should be interpreted within the context of existing uncertainty associated with sparrow although the approach of comparing steplgrid to sparrow is imperfect model intercomparison studies as pursued in the present work offer many advantages by building confidence using familiar benchmarks i e sparrow and outlining tradeoffs associated with different model approaches van vliet et al 2019 additionally whereas steplgrid was consistently effective at predicting relative nutrient loads steplgrid s ability to estimate the absolute magnitudes of tn and tp loads as compared to sparrow varied by watershed and region as a result steplgrid should only be used to estimate absolute nutrient load magnitudes for watersheds where its uncertainties are minimized such as in coastal plain regions with extensive human land use models with more comprehensive process representations than steplgrid should be used when analyzing nutrient loading dynamics in individual watersheds particularly if a user s interest is to estimate a single watershed s outlet loads or concentrations future applications of steplgrid should consider additional key findings from this study steplgrid appears to be effective at rank ordering watersheds by tn and tp loads as compared to sparrow regardless of the climate lulc composition or ecoregion of the watershed thus steplgrid is particularly useful as a tool for relative comparisons across multiple watersheds the minimum data and input parameter requirements and the ability to handle gridded data make steplgrid a potentially useful screening tool for modelling the impact of climate and lulc change on water quality at large spatial scales when used as a screening tool steplgrid could be used to identify coastal systems at higher risk of suffering from climate and lulc change which could then be studied through further detailed analysis steplgrid is relatively more effective at predicting tp loads than tn when compared to sparrow a trend that may be explained by steplgrid only considering surface water transport of nutrients steplgrid is also more likely to overpredict nutrient loads given that it does not account for nutrient losses as compared to sparrow steplgrid was most effective at estimating nutrient load magnitudes for watersheds in the gulf of mexico and south atlantic regions of the u s and least effective at estimating nutrient loads for north pacific watersheds the literature includes more emc values for land uses with intensive human use i e urban agricultural production and less for natural lands i e forest range consequently the relative lack of information on emcs for natural lands likely contributes large uncertainty to steplgrid outputs when the model is applied to watersheds with extensive natural areas in this study we observed that steplgrid was more likely to underpredict nutrient loads in areas with extensive forested land for studies aiming to compare or rank nutrients loads across several watersheds the use of regional or national emcs values might be suitable however for studies that require accurate pollutant estimates for a particular area local emc values may be necessary local emcs will better consider local characteristics that influence nutrient loading such as the presence of natural nutrient deposits steplgrid was only tested against sparrow for coastal watersheds across the u s additional research is needed to evaluate steplgrid s ability to estimate nutrient loads of interior watersheds where the effects of snowfall and steep elevations may be more pronounced relative to the watersheds studied here software and data availability all software and data used in this analysis are publicly available and outlined in the methods section of the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors report no conflict of interest the authors thank daniel wise from the u s geological survey oregon water science center who shared the uncertainty information for the west coast as well as lisa lowe and andrew petersen for facilitating the use of north carolina state university s high performance computing resources lrm performed all analyses interpreted results and prepared the manuscript ngn contributed to study design interpretation of results and manuscript preparation this work is supported by a u s geological survey southeast climate adaptation science center graduate fellowship awarded to lrm and the usda national institute of food and agriculture hatch project 1016068 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105344 
