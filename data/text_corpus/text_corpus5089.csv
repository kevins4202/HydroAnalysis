index,text
25445,estimates of potential flood inundation areas and depths are critical to informing the preparedness response and investment decisions of many government agencies and private sector organizations especially under a changing climate the standard modeling approaches however are often either computationally intensive or constrained in their accuracy or applicability a novel rivulet based 2d model of flooding is described in this article that is 10 000 to 10 million times less computationally complex than the full solution of the shallow water equations yet achieves inundation area hit rates of between 0 8 and 0 9 relative absolute mean errors of 10 20 across a wide range of flow depths and comparable accuracy at forecasting empirical high water marks this combination of accuracy and efficiency will significantly enhance real time depth estimates during flood events support detailed sensitivity analyses and allow for the generation of large ensembles to enable complex uncertainty analyses keywords modeling fluid flow flood modeling hydrodynamic model rivulet lagrangian data availability data will be made available on request 1 introduction flood characterization plays an important role in many urban planning emergency response and risk characterization activities including assessing the potential impacts of rain driven riverine or dam levee failure induced flooding planning of flood control structures characterizing future risks of climate change to urban populations infrastructure systems agricultural areas critical ecological habitats and commercial ventures setting insurance rates or government zoning regulations and informing risk based business and investment decisions in the real world flooding is a complex three dimensional process of free surface flow over varied topography in modeling practice though wide area flood simulations rarely approach this level of detail as a result of data limitations and computational constraints a variety of approximations and assumptions are instead made to yield targeted results of sufficient accuracy to meet the needs of a specific application while minimizing the computational burden the solution of the shallow water equations vreugdenhil 1994 swes remains the gold standard for accuracy in many wide area flood inundation applications the swes are themselves a two dimensional depth integrated approximation of the full navier stokes equations while they offer a significant simplification their solution still comes with a number of inherent challenges including numerical instabilities a need for small time steps issues in managing wetting drying fronts and significant overall computational complexity modest efficiency gains have been achieved solving the shallow water equations through the use of variable time steps trahan and dawson 2012 varying spatial resolutions liang et al november 2008 further simplifying the shallow water equations bates et al 2010 or the use of cylindrical water particles in a lagrangian frame kao and chang 2012 cellular automata based methods have further simplified the dynamical description and yielded additional efficiency gains rinaldi et al 2007 dottori and todini 2010 guidolin et al 2016 while forgoing an ability to capture some aspects of flow dynamics one dimensional approximations have been another fruitful avenue for efficiency improvements where applicable fread 1976 dimitriadis et al 2016 and even more significant reductions in computational complexity have been achieved with rapid flood spreading models that focus on topographically based depth forecasting liu and pender 2010 johnson et al 2019 though the approach has critical constraints on the situations it can be applied to and the flood characteristics it can be used to forecast deep learning approaches have been developed zhou et al 2021 but rely on large training sets which are computationally costly to develop and are applicable only to the area for which they have been trained the significant computation times preprocessing requirements or physical assumptions of many of these approaches can limit their applicability in the present manuscript a novel lagrangian rivulet based approximation is presented that has been able to reduce the computational complexity relative to the solution of the swes by between four 4 and seven 7 orders of magnitude in real world studies the approach allows the spatiotemporal dynamics of wide area floods to be readily and rapidly characterized potentially opening up new avenues for research and simulation related to climate change impact assessment emergency response support model integration uncertainty quantification etc 2 method imagine that we wanted to create a small scale physical model of a flood event but were not able to use any liquids to do so we might start by 3d printing a scale model of the terrain and its topographic relief across the region of interest to approximate the gravitational forces of the real system as a stand in for water we might use small teflon coated beads a small sifter could be machined to sit above the scale model with holes drilled such that their varying spatial density approximated the relative rainfall rates in different areas basins over which there was more rain or bead fall that had more feeder watersheds or that had shallower slopes would then be expected to collect more beads while smaller steeper basins would collect fewer approximating the dynamics of a real flood the approximation could be improved by vibrating the whole experiment slightly in an effort to keep the beads fluidized this tactic can also be used to numerically model a flood in fact thinking about problems this way from the perspective of the individual particles that make up a flow is referred to as a lagrangian approach the challenge is that simulating the motion of every individual bead and its interactions with the others can be computationally complex requiring millions or hundreds of millions of numerical beads to provide a reasonable approximation of the flow dynamics with each bead interacting with many of its near neighbors at each time step but what if we modeled the flow not with numerical beads but instead with the numerical equivalent of teflon coated tubules tubules whose tails followed along precisely in the path of their heads so that each mimics a single rivulet of the flow each tubule would represent the volumetric displacement of what might have been hundreds or thousands of separate beads with the computational complexity of a single element it is just such an approach that is described in this manuscript to reduce the computational complexity associated with forecasting flood depths and dynamics from a lagrangian perspective a trio of approximations is made each infinitesimal lagrangian fluid parcel or bead is abstracted into something with finite length a rivulet a single scalar depth grid is used to mediate interactions between rivulets and individual rivulet dynamics are reduced to local slope driven forcing neglecting the effects of both momentum and shear the result is that the simplistic surface elevation driven interactions of a comparatively small number of rivulets are tracked rather than the myriad complex interactions between a much larger number of infinitesimal fluid parcels the long tails of the rivulets provide one avenue of efficiency gains while their simplified interaction dynamics provide another this approach does not suffer from numerical instability does not have issues with wetting or drying fronts and does not necessitate the use of small time steps it does however provide sufficient spatiotemporal accuracy to be useful for many applications allows researchers to easily and continuously tradeoff between accuracy and computational burden and results in a significant reduction in computational complexity between four 4 and seven 7 orders of magnitude the implementation of these approximations is described in more detail in the following sections 2 1 rivulet description in fig 1 a pair of rivulets blue and green are shown moving through a topographic elevation grid e the rivulets interact with one another through a collocated depth grid h which tracks the aggregate depth of all the rivulets passing through a given cell all rivulets have identical characteristics which are unchanged throughout their existence each rivulet has the transverse width of a single grid cell a constant length of n grid cells and a thickness d if grid cells have side l then the volume of every rivulet is v n l 2 d the speed of a rivulet is constant along its length and fully determined by the dynamics at the rivulet head i e rivulets are not stretched or thinned as they move across the topography as a simulation unfolds the path x of each rivulet is tracked for a total of n grid cells x x 0 x 1 x n 1 where the x i denote the row column indices of each grid cell the rivulet head has passed through going back n 1 moves the simplest method for tracking the path would be to create a new copy of the array at each step copying x 0 x 1 x 1 x 2 and so on then replacing x 0 with the new head location this would result in an unnecessary memory allocation and copy step for each rivulet however in order to make the storage of each path as efficient as possible a single array is allocated for each rivulet and updated in place an index k tracks the current position of the rivulet head within the path array the index of the location the rivulet head passed through i time steps ago is then 1 f k i k i if k i k i n otherwise assuming that the array indices are zero offset the path array is then updated in place according to the rules laid out in the following sections 2 2 rivulet update rules 2 2 1 drying two operations are performed to move a rivulet a cell forward the operations take place in the context of the underlying depth h and elevation grids e the first operation is to dry the current tail of the rivulet drying consists of subtracting the rivulet depth d from the underlying aggregated depth grid that is if the tail location is at index j f k n 1 the depth grid is updated such that h x j h x j d because the tail location of the rivulet is known this operation can be performed in constant time 2 2 2 path selection after the tail is dried a new location for the head must be selected from among the current rivulet head s moore neighborhood kari 2005 let this set of neighbors be denoted by m x k and the surface water elevation by s e h the next head location x is the neighbor that maximizes the downslope gradient unless the rivulet is in a depression in that case the current head location is repeated and the rivulet begins to fill in the depression that is 2 x arg min x m x k s x s x k l x x k if s x s x k 0 x k otherwise where denotes the distance between two grid locations in units of cells and l scales to an absolute length the process of determining the next head location occurs in o m time where m is 8 corresponding to the eight neighbors the previous tail location is then updated to hold the location of the new head x f k n 1 x and the pointer to the rivulet head is incremented such that k f k n 1 note that this procedure explicitly ignores the effect of momentum on rivulet motion the exclusion of momentum effects on flow direction and speed means that the location of the maximum velocity streamline may not be precisely captured the objective of the present work however is not to provide precise forecasts of local flow speeds or turbulent dynamics the current work is motivated by a need to rapidly estimate basic flow depths and inundation areas in this application momentum plays a less critical role with slower water back filling into areas outside the main flow in the validation studies presented in the results section it will be shown that neglecting momentum appears to be reasonable for this context that said capturing some notion of rivulet speed is necessary to adequately characterize flood depths and their temporal evolution since both precipitation fluxes and drainage rates may vary spatiotemporally 2 2 3 rivulet speed rather than integrating the forces on each fluid element as is done in the solution of the shallow water equations each rivulet s speed is determined using the empirically derived manning formula this dramatically reduces the overall computational complexity if the slope between the rivulet s current and former head locations is negative the velocity is given by 3 v 1 n m r h 2 3 s x k s x f k 1 l x k x f k 1 1 2 where v is the speed n m the manning coefficient and r h the hydraulic radius for the wide channels of broad overland flooding targeted in this approach it will be assumed that r h h x k the local flow depth if the minimum slope at the rivulet head is positive the velocity is taken to be 0 in reality of course the surface elevation gradient is likely to vary along a rivulet s path as a result the speed at each rivulet segment should be expected to vary this would have the effect of stretching or compressing a rivulet along its length such effects are neglected here this has the consequence of spatiotemporally blurring the dynamical impact of variations in the underlying topography the magnitude of this impact can be traded off against computational complexity by adjusting the rivulet length longer rivulets may result in less spatiotemporal acuity but reduced computation time the use of shorter rivulets will do the opposite finally from both the computational and accounting standpoints it is preferable to avoid having to track rivulet motion at a resolution finer than a single grid cell calculation of the manning speed however will almost certainly result in updates to rivulet positions by non integer numbers of grid cells v δ t l in order to avoid this situation at each time step a rivulet is first deterministically moved an integer number of steps v δ t l a random variable y is then chosen from a uniform distribution y u 0 1 if y v δ t l v δ t l that is the fractional part of the velocity the rivulet is moved an additional step in this way each rivulet moves the correct distance on average but with some stochastic variation around the desired value unfortunately this numerically induced stochastic variation gets relatively larger as the speed decreases the opposite of what one would expect from many physical processes such as turbulent dynamics however since the persistence time of a rivulet in lower hydraulic grade areas will typically be longer than the persistence time in similarly sized regions of higher grade these rivulets are expected to sample more discrete velocities and thus come closer to the correct mean behavior during their transit time note that for simplicity s sake we also do not currently distinguish between horizontal vertical moves and diagonal moves when computing the number of grid cells to advance even though the latter are technically a factor of 2 further than the former 2 3 spatiotemporally varying precipitation flooding can be driven by spatiotemporally varying rainfall rates as part of the algorithm initialization spatial precipitation fluxes are read in as a series of grids r τ for each simulation time period τ that they are available if the rainfall grids are uniformly spaced in time with periodicity δ t then this index is τ t δ t a set of m potential source locations p τ p 0 p 1 p m 1 is generated by sampling points from each grid as though it were a discrete probability distribution so that a greater number of points are likely to fall in areas of higher rainfall see fig 2 let the total precipitation flux during time period τ be denoted by φ p τ if each rivulet has volume v n l 2 d then the number of new rivulets that must be released per time step δ t to account for the total rainfall is n τ φ p τ δ t v by randomly sampling n τ locations from the precomputed set p τ the rivulets will be distributed in proportion to the spatial rainfall distribution during that period by ensuring that each set p t is randomly shuffled during the initialization phase n τ points can be pulled off in series in constant time without the need to generate random numbers during algorithm execution for this to work reasonably well one generally wants m n τ note also that the value of m in combination with the relative spatial distribution of rainfall rates sets the mean and maximum spatial resolution of potential source locations this along with the rivulet volume v influences the potential coverage area i e having too few potential source points may result in incomplete estimates of inundation area 2 4 total algorithm overview putting this all together in pseudo code yields the algorithm below at a high level the precipitation forecasts are first ingested then for each time step in the simulation the rivulet locations are updated rivulets that have left the domain removed and new rivulets instantiated based on the current spatiotemporal distribution of precipitation flux finally the element wise peak depth grid may be updated note that to improve performance the peak depth grid may only be updated periodically the process of stepping each rivulet forward through a time step then requires three operations calculating how many cells to move forward then for each cell step drying the current tail location and moving the head 3 results a comparison of the rivulet based algorithm described above and the direct solution of the shallow water equations is presented in this section two very different study areas were chosen for analysis representing two major real world wide area flood events a three day period during the landfall of hurricane harvey in the houston area during the fall of 2017 and a seven day period of extreme precipitation in boulder county colorado during the fall of 2013 these two regions and events differed significantly in topography duration and spatiotemporal rainfall patterns in both cases the solution of the shallow water equations was accomplished using the rapid inundation forecasting tool rift judi et al 2018 2014 the houston study involves a 211 km by 255 km 53 800 km 2 region of southeastern texas resolved into 60 meter grid cells 15 000 000 cells during the 72 h simulation period a total of more than 3 7 1 0 9 m 3 of water 3 million acre feet fell across the study region topographic variation in the study area is modest with a rise from sea level to a maximum of 154 m 505 ft the boulder county colorado study comprises an area of 4 476 km 2 50 km by 90 km resolved into 30 m cells 5 000 000 cells over which 605 1 0 6 m 3 491 000 acre feet of water fell during the 168 h simulation period the boulder study area has a topographic relief of 2 902 m or more than 9 500 ft the two study areas thus differ by an order of magnitude in area a factor of 19 in topographic relief a factor of six in total rainfall volume more than a factor of two in duration and a factor of two in cell size the choice of dem resolution in each case was driven by the solution of the shallow water equations the rift simulations were configurations used to respond to actual events and are therefore representative of the typical tradeoffs that must be made between spatial resolution and computational complexity when solving the swes in a real world context the rivulet algorithm was run on the same grid to allow an apples to apples comparison note that in both studies the precipitation was bounded to a subset of the hydrologic units of interest and did not necessarily extend across the full dem overviews of the houston and boulder simulation results are shown in figs 3 and 4 respectively three different approaches were used to compare the swe and rivulet algorithm results an analysis of the inundation areas forecast by the two approaches a comparison of their respective peak depth estimates and a review of the time dependent depths forecast by each at a subset of locations the results of these analyses are described in the following sections 3 1 inundation area statistics an initial comparison can be made between the swe and rivulet based forecasts in terms of the models predicted inundation areas to do this wetted areas are compared on a cell by cell basis between the two models the confusion matrix showing the four possible configurations of each pair wise comparison is shown in fig 5 and plotted spatially in figs 6 and 7 for this comparison a rivulet algorithm solution with rivulet length n 50 and rivulet thickness d 0 0125 m was used four binary metrics characterizing the relationship between the forecast inundation areas were computed for each study the critical success index csi commission rate cr omission rate or and hit rate hr they are defined in terms of the confusion matrix elements as katiyar et al 2021 stephens et al 2014 csi tp tp fp fn cr fp tp fp or fn tp fn 4 hr tp tp fn the critical success index represents the number of correct positives from the total area suggested to be inundated by either algorithm the commission rate is the fraction of all positives tp fp that were falsely asserted fp the omission rate is the fraction of cells that should have been positive tp fn that are falsely negative fn and the hit rate is the fraction of cells that should have been positive tp fn that actually are tp given the widespread nature of rainfall during the events nearly every cell within the study areas was wetted at some level however small by the solution of the swes it is therefore necessary to define what wetted means the metrics were evaluated for arbitrary thresholds of between 2 5 cm and 50 cm of swe predicted depth cells with less than the threshold flood depth were considered dry and those with depth equal to or greater than the threshold were considered wet the binary metric values are shown in fig 8 in both the houston and boulder studies the hit rate and critical success index generally increased and the omission rate decreased as the threshold depth was elevated this suggests the rivulet algorithm has an increasing reliability in correctly identifying deeper areas of inundation which are the most critical to capture in most applications in both study areas the hit rate ranged between roughly 0 8 and 0 9 in the boulder study the csi ranged from 0 57 to 0 66 and in the houston study from 0 69 to 0 79 note that areas of 0 elevation in the houston dem corresponding to open ocean were excluded from the binary metric calculations and map 3 2 peak depth error quantification figs 3 and 4 suggest a qualitative agreement between the rivulet algorithm s peak depth forecasts and the full solution of the shallow water equations to quantify the peak depth forecast error relative to the full solution of the swes the mean error me mean absolute error mae and relative mean absolute error rmae were each computed for a set of swe predicted depth ranges 0 0 to 0 25 m 0 25 to 0 5 m 0 5 to 0 75 m etc out to a depth of 8 0 m by quantifying the error separately within each depth range it was possible to characterize the errors depth dependence a peak depth grid represents the cell wise maximum depth reached over the duration of a simulation t this was computed for both the solution of the swes as d t and rivulet based algorithm as h t formally the peak depth is x max t x t where x d h the set of grid cells where the swe predicted peak depth d falls within a particular depth range say r 0 0 0 25 is s r i j d i j r and the number of such cells is n r s r the me mae and rmae for this depth range are me r 1 n r s s r h s d s mae r 1 n r s s r h s d s 5 rmae r 1 r mae r where r is the mean of the range r the rivulet algorithm has a number of parameters that can be adjusted to potentially tradeoff between accuracy and computational complexity these include the thickness of the rivulets d their length n and the algorithm time step δ t in the following sections each parameter s impact on forecast error and computational complexity will be examined 3 2 1 impact of rivulet thickness on error the error metrics defined above can be used to more rigorously quantify forecast depth differences and their dependence on rivulet algorithm parameters fig 9 shows the me mae and rmae as a function of swe forecast depth for five different choices of rivulet thickness d 1 25 cm 2 5 cm 5 cm 10 cm and 20 cm rivulet thickness sets the minimum depth difference that can be resolved so the predictive error is expected to generally increase with increasing d the top panels of fig 9 suggest that across all rivulet thicknesses the absolute error increases with depth up to depths of roughly 5 m beyond which it roughly plateaus the relative error bottom panels however tends to be greater at smaller depths than large this basic trend is seen in both the houston and boulder studies the middle panels the mean error suggest that the rivulet algorithm over predicts depth on average in nearly all swe forecast depth ranges of the boulder study and for all rivulet thicknesses the same is true in the houston study area when rivulet thicknesses of 5 cm or less are used for thicker rivulets 10 and 20 cm the rivulet algorithm begins to underpredict on average depths greater than 4 m more work is needed to understand the precise cause s of these differences the results also confirm an increase in both absolute and relative error with increasing rivulet thickness d though both increase only minimally as the thickness is varied between 1 25 cm and 5 0 cm beyond that the error begins to increase more noticably this is important because the overall computational complexity of the rivulet algorithm simulation decreases as the thickness is increased see fig 10 this suggests an opportunity for simulation parameters to be tuned so as to reduce computational complexity while having a relatively minimal impact on error fig 10 shows the computational complexity of the rivulet algorithm solution both independently and relative to the full solution of the swes the swe solution in houston took 3 h and 41 min to perform using rift a c cuda swe solver parallelized across 4 992 gpu cuda cores an nvidia k80 for a rough estimate of 1 1 million gpu minutes of computation time for the boulder county event the swe solution took 6 6 million gpu minutes by contrast the rivulet algorithm was implemented in scala run on the java virtual machine jvm and parallelized across 4 cpu cores a single quad core intel xeon e5 processor the number of cpu minutes required to perform a rivulet based simulation is shown as a function of rivulet thickness in the left panel of fig 10 the relative computational complexity the ratio of swe to rivulet simulation complexity is shown in the right panel the relative complexity savings varies depending on study area and rivulet parameters from a low of 10 000 to a high of nearly 10 million less of an efficiency gain was observed in the houston study area than in boulder there are likely a couple of reasons for this the computational complexity of the swe solution does not vary significantly with rainfall rate since once a cell is wetted the computation time required to model depth changes at that location is constant by contrast the number of rivulets required to model a given precipitation flux φ p τ is n τ φ p τ δ t v as the rainfall rate increases the number of rivulets spawned at each time step increases in proportion the total rainfall over the simulation periods was six times greater in houston than in boulder accounting for a significant fraction of the relative complexity difference note too that the houston area has far less topographic relief than boulder leading to significantly longer mean persistence times for each rivulet even after accounting for the relative difference in simulation area this also contributes to a greater overall relative complexity 3 2 2 impacts of rivulet length on error both forecast error and computational complexity also vary with rivulet length n error rates are shown again as a function of swe forecast depth in fig 11 for rivulet lengths of 50 100 200 and 500 cells this corresponds to spatial distances of 1 5 3 6 and 15 km in the boulder county study where 30 m grid cells were used and distances of 3 6 12 and 30 km in the houston study 60 m grid cells as the rivulet length is increased the spatial resolution of the rivulets is effectively smoothed the impact may be smaller than one might imagine however if all rivulets moved parallel to one another along the direction of channel flow indeed the spatial resolution in the longitudinal direction would be constrained to the length of the rivulet in reality however rivulets trace complex paths back and forth across channels often moving perpendicular to the mean flow direction as they interact with one another through the depth grid the result is that their width transverse to their direction of motion a single grid cell becomes the dominant factor in the spatial resolution that can effectively be achieved rivulet length has an impact on the temporal scales that can be resolved as well as a single rivulet may take tens of minutes to pass a static point on the landscape this would for example prevent rapid shifts in rainfall rate from being immediately reflected in depth estimates in reality however precipitation data is typically only available on timescales of an hour or more and if finer temporal granularity is required the rivulet length can be reduced the computational complexity shown in fig 12 varies with rivulet length smaller values of n necessitate a greater number of rivulets to account for a given rainfall rate and hence a greater overall computational complexity 3 2 3 impact of time step on error the variation of error with time step is a little bit different recall that the number of rivulets that must be created at each time step is n τ φ p τ δ t v if the time step δ t is shortened the number of rivulets that must be created at each time step is reduced but the total number of time steps is increased by a compensatory amount as a result the computational complexity is not expected to vary significantly with time step this is indeed what is observed in fig 13 where the computational complexity remains relatively constant and only falls off at the highest and lowest values where the rivulet approximation itself begins to fail too short a time step and many rivulets may not be able to move far enough during each period to make their largely integer moves an accurate reflection of rivulet speed too long a time step and continuity between steps is lost a rivulet s tail might end up significantly further downstream in one time step than its head was in the previous this effect is apparent in the relative absolute error magnitudes shown in the bottom panels of fig 14 in the case of the houston study left panel the relative error rapidly asymptotes to 1 0 for both the shortest 0 25 min and longest 80 min time steps in the boulder study the shortest time step did better light red curve but still indicated significant error lowest relative errors were achieved with time steps between 1 min and 10 min in houston and between 0 5 min and 10 min in boulder in both cases simulations with a 10 min time step outperformed simulations with a 4 min time step additional investigation is needed to better understand some of these idiosyncrasies the upshot is that this relative insensitivity of the complexity and error to time step size means that a value of 1 to 4 min can be chosen relatively safely without having to make a complex assessment of the relative importance of computational efficiency and accuracy the downside is that the time step does not present an opportunity for further influencing this calculus either the primary factors that can be traded off are rivulet length and thickness 3 3 temporal dynamics in addition to comparing the peak depth forecasts of the rivulet algorithm and swes hourly depth forecasts were compared at a number of locations within the houston and boulder study areas a subset of which are included here the temporal pattern of rainfall differed significantly between the two studies during the landfall of hurricane harvey the vast majority of rain occurred within a single twenty four hour period the second day of the simulation left panel of fig 15 by contrast rainfall during the boulder flood event was much more temporally widespread and sporadic right panel of the same figure the result was that flood depths in the houston study rose sharply at each measurement location early on the second day of the simulation before falling off smoothly over a longer timescale see blue curves of fig 16 by contrast flood depths in boulder rose and fell repeatedly during the course of the event fig 17 as new waves of rainfall drove renewed flooding in many drainages in each case the rivulet algorithm captures the timing magnitude and persistence of the flooding reasonably well it does introduce some oscillatory behavior as illustrated in the orange curves of fig 16 and the top right panel of fig 17 more work is needed to tease out the cause of this behavior that the overall temporal dynamics are well characterized however suggests that the rivulet based algorithm may be useful not only for predicting inundation areas but also for rapidly estimating the gross temporal dynamics of wide area flooding events 3 4 comparison with empirical high water marks in the previous sections the rivulet algorithm forecast depths were compared to those of the full solution of the shallow water equations in this section peak depth forecasts from both the rivulet algorithm and the solution of the shallow water equations are compared to empirical high water mark observations reported following hurricane harvey u s geological survey a total of 907 high water mark observations fig 18 fell within the houston simulation area the index of the grid cell containing the latitude and longitude of each observation was computed and the models peak depth forecasts for those cells compared to the height above ground of the corresponding high water mark the mean error of the shallow water equation forecasts was 0 39 m and the mean absolute error 0 86 m the mean error in the rivulet forecasts was slightly greater at 0 45 m as was the mean absolute error 0 92 m we suspect that a major contribution to these errors the differences between both model forecasts and the observed high water marks is the finite resolution of the underlying dem as an example fig 19 shows a 3 3 subset of the dem in the area of river terrace park near channelview texas the rough location of the creek that runs through the park is indicated in darker blue the underlying gray cubes indicate the dem assessed elevation the translucent blue cubes the shallow water equation predicted depth and the black points the location and observed height of the high water marks the observed height above ground is indicated with white text on dark gray while the shallow water equation forecast depths are shown in black on white at the location of the two observations the differences between the shallow water equation forecasts and observed peak depths are 0 13 m and 0 41 m in some ways this level of agreement is fortuitous high water mark observation a was made on the bank of the creek which is essentially at equal elevation throughout this entire sub section of the grid the dem places the elevation of this cell at 5 41 m and the depth at 1 50 m had the observation been made at locations b or c however the high water mark height above ground should have been nearly the same because of the equal stream bed elevation but the model would have forecast depths of 3 96 m and 5 7 m respectively as the high water mark data includes elevation the potential impact that dem resolution may have on forecast error can be further explored through an analysis of the differences between observed and dem reported elevation the mean absolute difference between the measured local elevation and the corresponding elevation of the dem cell the measurement fell within is 1 07 m the mean standard deviation between the elevations of each measurement dem cell and its four nearest neighbors is 1 19 m this provides an estimate of the expected topographic variation over the scale of a cell width the difference between high water mark observations and model forecasts therefore lies within the variability expected to result from the finite horizontal resolution of the digital elevation model and associated elevation uncertainty with the relative differences between the reported error of the two model forecasts much smaller further because of the relative efficiency of the rivulet based approach the full houston scenario can be rerun using a 10 m 1 3 arc second dem resulting in a 25 487 x 21 131 grid of roughly 540 million grid cells see fig 20 a rivulet length of n 1 200 cells equivalent in absolute length to an n 200 rivulet at 60 m resolution and rivulet thickness of d 0 075 m was used the simulation ran in 19 min wall clock time on the same four core desktop computer used in the other simulations at 10 m resolution the mean error of the rivulet based forecasts was 0 09 m versus 0 45 m on a 60 m grid and the mean absolute error 0 67 m versus 0 92 m on a 60 m grid here too though we expect that much of the error is still driven by the spatial resolution of the dem a factor of six decrease in cell size from 60 m to 10 m does not necessarily correspond to a factor of six decrease in elevation variation on a 10 m grid the mean absolute difference between the reported high water mark and dem elevations is still 0 90 m and the nearest neighbor standard deviation 0 84 m in the scenarios investigated the finite spatial resolution of the underlying dem is thus likely a significant driver of the apparent forecast error by comparison the difference between the rivulet and shallow water equation based errors is relatively small with both performing comparably well 4 discussion and conclusions the rivulet based algorithm is a novel approach that may be used to support real time risk assessments studies of depth and inundation area sensitivity to climate change driven variations in precipitation generation of large scale wide area ensembles for uncertainty quantification etc one of the key benefits of the approach is the ability it provides researchers to smoothly trade off between computational complexity and mean error shown in fig 21 there is a clear trend indicating that by decreasing the rivulet length and or thickness one can reduce the mean absolute error of peak depth forecasts but with the side effect of reducing the complexity savings relative to the full solution of the swes this means that one could for example do a large suite of coarse simulations to identify the highest risk scenarios or regions a handful of more computationally intensive more accurate simulations could then be performed as a storm system approaches to refine estimates of depth inundation area and temporal dynamics for the riskiest scenarios the trade off between accuracy and complexity could likely be improved upon by adaptively tuning rivulet parameters to different topographies fig 22 shows the results of performing error calculations on the western and eastern halves of the boulder study area independently the results in the eastern portion look similar to the results on aggregate while the results in the western portion show that error rates there have almost no dependence on rivulet thickness topographically the eastern portion of the boulder study area is relatively flat quite similar to the houston study area the western portion however includes more than 9 000 ft of topographic relief and is characterized by narrow tightly constrained channels this result suggests that it may be possible to get away with far less computational complexity in such areas potentially saving one to two orders of magnitude in computation time there to implement this one could imagine allowing rivulets that originated in tight channelized regions to adaptively split into two or more shorter thinner rivulets when they moved into areas of flatter topography this would be done in a way so as to conserve overall flow volume the rivulet algorithm outlined here provides a novel lagrangian based approximation of flood dynamics it allows an orders of magnitude reduction in computational complexity over the solution of the shallow water equations while retaining sufficient spatiotemporal accuracy to be useful for many applications simulations are easy to setup requiring only a dem and precipitation grids with no other preprocessing or configuration steps the computational savings it represents can be spent in other ways for example generating large ensembles of model forecasts or significantly increasing the spatiotemporal breadth of study areas potentially enabling rapid continent scale simulation of hydrodynamics these opportunities could help researchers to better characterize flood risks under a changing climate they could also allow for tighter interdependent integration with other models for example integration with agent based models of populations in flood prone areas could allow for a better characterization of socioeconomic impacts and the potential value of various avenues for mitigation or adaptation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the multisector dynamics program area of the u s department of energy office of science office of biological and environmental research as part of the multi program collaborative coastal modeling icom project contract number de ac05 76rl01830 and by the u s department of homeland security cyber and infrastructure security agency national risk management center 
25445,estimates of potential flood inundation areas and depths are critical to informing the preparedness response and investment decisions of many government agencies and private sector organizations especially under a changing climate the standard modeling approaches however are often either computationally intensive or constrained in their accuracy or applicability a novel rivulet based 2d model of flooding is described in this article that is 10 000 to 10 million times less computationally complex than the full solution of the shallow water equations yet achieves inundation area hit rates of between 0 8 and 0 9 relative absolute mean errors of 10 20 across a wide range of flow depths and comparable accuracy at forecasting empirical high water marks this combination of accuracy and efficiency will significantly enhance real time depth estimates during flood events support detailed sensitivity analyses and allow for the generation of large ensembles to enable complex uncertainty analyses keywords modeling fluid flow flood modeling hydrodynamic model rivulet lagrangian data availability data will be made available on request 1 introduction flood characterization plays an important role in many urban planning emergency response and risk characterization activities including assessing the potential impacts of rain driven riverine or dam levee failure induced flooding planning of flood control structures characterizing future risks of climate change to urban populations infrastructure systems agricultural areas critical ecological habitats and commercial ventures setting insurance rates or government zoning regulations and informing risk based business and investment decisions in the real world flooding is a complex three dimensional process of free surface flow over varied topography in modeling practice though wide area flood simulations rarely approach this level of detail as a result of data limitations and computational constraints a variety of approximations and assumptions are instead made to yield targeted results of sufficient accuracy to meet the needs of a specific application while minimizing the computational burden the solution of the shallow water equations vreugdenhil 1994 swes remains the gold standard for accuracy in many wide area flood inundation applications the swes are themselves a two dimensional depth integrated approximation of the full navier stokes equations while they offer a significant simplification their solution still comes with a number of inherent challenges including numerical instabilities a need for small time steps issues in managing wetting drying fronts and significant overall computational complexity modest efficiency gains have been achieved solving the shallow water equations through the use of variable time steps trahan and dawson 2012 varying spatial resolutions liang et al november 2008 further simplifying the shallow water equations bates et al 2010 or the use of cylindrical water particles in a lagrangian frame kao and chang 2012 cellular automata based methods have further simplified the dynamical description and yielded additional efficiency gains rinaldi et al 2007 dottori and todini 2010 guidolin et al 2016 while forgoing an ability to capture some aspects of flow dynamics one dimensional approximations have been another fruitful avenue for efficiency improvements where applicable fread 1976 dimitriadis et al 2016 and even more significant reductions in computational complexity have been achieved with rapid flood spreading models that focus on topographically based depth forecasting liu and pender 2010 johnson et al 2019 though the approach has critical constraints on the situations it can be applied to and the flood characteristics it can be used to forecast deep learning approaches have been developed zhou et al 2021 but rely on large training sets which are computationally costly to develop and are applicable only to the area for which they have been trained the significant computation times preprocessing requirements or physical assumptions of many of these approaches can limit their applicability in the present manuscript a novel lagrangian rivulet based approximation is presented that has been able to reduce the computational complexity relative to the solution of the swes by between four 4 and seven 7 orders of magnitude in real world studies the approach allows the spatiotemporal dynamics of wide area floods to be readily and rapidly characterized potentially opening up new avenues for research and simulation related to climate change impact assessment emergency response support model integration uncertainty quantification etc 2 method imagine that we wanted to create a small scale physical model of a flood event but were not able to use any liquids to do so we might start by 3d printing a scale model of the terrain and its topographic relief across the region of interest to approximate the gravitational forces of the real system as a stand in for water we might use small teflon coated beads a small sifter could be machined to sit above the scale model with holes drilled such that their varying spatial density approximated the relative rainfall rates in different areas basins over which there was more rain or bead fall that had more feeder watersheds or that had shallower slopes would then be expected to collect more beads while smaller steeper basins would collect fewer approximating the dynamics of a real flood the approximation could be improved by vibrating the whole experiment slightly in an effort to keep the beads fluidized this tactic can also be used to numerically model a flood in fact thinking about problems this way from the perspective of the individual particles that make up a flow is referred to as a lagrangian approach the challenge is that simulating the motion of every individual bead and its interactions with the others can be computationally complex requiring millions or hundreds of millions of numerical beads to provide a reasonable approximation of the flow dynamics with each bead interacting with many of its near neighbors at each time step but what if we modeled the flow not with numerical beads but instead with the numerical equivalent of teflon coated tubules tubules whose tails followed along precisely in the path of their heads so that each mimics a single rivulet of the flow each tubule would represent the volumetric displacement of what might have been hundreds or thousands of separate beads with the computational complexity of a single element it is just such an approach that is described in this manuscript to reduce the computational complexity associated with forecasting flood depths and dynamics from a lagrangian perspective a trio of approximations is made each infinitesimal lagrangian fluid parcel or bead is abstracted into something with finite length a rivulet a single scalar depth grid is used to mediate interactions between rivulets and individual rivulet dynamics are reduced to local slope driven forcing neglecting the effects of both momentum and shear the result is that the simplistic surface elevation driven interactions of a comparatively small number of rivulets are tracked rather than the myriad complex interactions between a much larger number of infinitesimal fluid parcels the long tails of the rivulets provide one avenue of efficiency gains while their simplified interaction dynamics provide another this approach does not suffer from numerical instability does not have issues with wetting or drying fronts and does not necessitate the use of small time steps it does however provide sufficient spatiotemporal accuracy to be useful for many applications allows researchers to easily and continuously tradeoff between accuracy and computational burden and results in a significant reduction in computational complexity between four 4 and seven 7 orders of magnitude the implementation of these approximations is described in more detail in the following sections 2 1 rivulet description in fig 1 a pair of rivulets blue and green are shown moving through a topographic elevation grid e the rivulets interact with one another through a collocated depth grid h which tracks the aggregate depth of all the rivulets passing through a given cell all rivulets have identical characteristics which are unchanged throughout their existence each rivulet has the transverse width of a single grid cell a constant length of n grid cells and a thickness d if grid cells have side l then the volume of every rivulet is v n l 2 d the speed of a rivulet is constant along its length and fully determined by the dynamics at the rivulet head i e rivulets are not stretched or thinned as they move across the topography as a simulation unfolds the path x of each rivulet is tracked for a total of n grid cells x x 0 x 1 x n 1 where the x i denote the row column indices of each grid cell the rivulet head has passed through going back n 1 moves the simplest method for tracking the path would be to create a new copy of the array at each step copying x 0 x 1 x 1 x 2 and so on then replacing x 0 with the new head location this would result in an unnecessary memory allocation and copy step for each rivulet however in order to make the storage of each path as efficient as possible a single array is allocated for each rivulet and updated in place an index k tracks the current position of the rivulet head within the path array the index of the location the rivulet head passed through i time steps ago is then 1 f k i k i if k i k i n otherwise assuming that the array indices are zero offset the path array is then updated in place according to the rules laid out in the following sections 2 2 rivulet update rules 2 2 1 drying two operations are performed to move a rivulet a cell forward the operations take place in the context of the underlying depth h and elevation grids e the first operation is to dry the current tail of the rivulet drying consists of subtracting the rivulet depth d from the underlying aggregated depth grid that is if the tail location is at index j f k n 1 the depth grid is updated such that h x j h x j d because the tail location of the rivulet is known this operation can be performed in constant time 2 2 2 path selection after the tail is dried a new location for the head must be selected from among the current rivulet head s moore neighborhood kari 2005 let this set of neighbors be denoted by m x k and the surface water elevation by s e h the next head location x is the neighbor that maximizes the downslope gradient unless the rivulet is in a depression in that case the current head location is repeated and the rivulet begins to fill in the depression that is 2 x arg min x m x k s x s x k l x x k if s x s x k 0 x k otherwise where denotes the distance between two grid locations in units of cells and l scales to an absolute length the process of determining the next head location occurs in o m time where m is 8 corresponding to the eight neighbors the previous tail location is then updated to hold the location of the new head x f k n 1 x and the pointer to the rivulet head is incremented such that k f k n 1 note that this procedure explicitly ignores the effect of momentum on rivulet motion the exclusion of momentum effects on flow direction and speed means that the location of the maximum velocity streamline may not be precisely captured the objective of the present work however is not to provide precise forecasts of local flow speeds or turbulent dynamics the current work is motivated by a need to rapidly estimate basic flow depths and inundation areas in this application momentum plays a less critical role with slower water back filling into areas outside the main flow in the validation studies presented in the results section it will be shown that neglecting momentum appears to be reasonable for this context that said capturing some notion of rivulet speed is necessary to adequately characterize flood depths and their temporal evolution since both precipitation fluxes and drainage rates may vary spatiotemporally 2 2 3 rivulet speed rather than integrating the forces on each fluid element as is done in the solution of the shallow water equations each rivulet s speed is determined using the empirically derived manning formula this dramatically reduces the overall computational complexity if the slope between the rivulet s current and former head locations is negative the velocity is given by 3 v 1 n m r h 2 3 s x k s x f k 1 l x k x f k 1 1 2 where v is the speed n m the manning coefficient and r h the hydraulic radius for the wide channels of broad overland flooding targeted in this approach it will be assumed that r h h x k the local flow depth if the minimum slope at the rivulet head is positive the velocity is taken to be 0 in reality of course the surface elevation gradient is likely to vary along a rivulet s path as a result the speed at each rivulet segment should be expected to vary this would have the effect of stretching or compressing a rivulet along its length such effects are neglected here this has the consequence of spatiotemporally blurring the dynamical impact of variations in the underlying topography the magnitude of this impact can be traded off against computational complexity by adjusting the rivulet length longer rivulets may result in less spatiotemporal acuity but reduced computation time the use of shorter rivulets will do the opposite finally from both the computational and accounting standpoints it is preferable to avoid having to track rivulet motion at a resolution finer than a single grid cell calculation of the manning speed however will almost certainly result in updates to rivulet positions by non integer numbers of grid cells v δ t l in order to avoid this situation at each time step a rivulet is first deterministically moved an integer number of steps v δ t l a random variable y is then chosen from a uniform distribution y u 0 1 if y v δ t l v δ t l that is the fractional part of the velocity the rivulet is moved an additional step in this way each rivulet moves the correct distance on average but with some stochastic variation around the desired value unfortunately this numerically induced stochastic variation gets relatively larger as the speed decreases the opposite of what one would expect from many physical processes such as turbulent dynamics however since the persistence time of a rivulet in lower hydraulic grade areas will typically be longer than the persistence time in similarly sized regions of higher grade these rivulets are expected to sample more discrete velocities and thus come closer to the correct mean behavior during their transit time note that for simplicity s sake we also do not currently distinguish between horizontal vertical moves and diagonal moves when computing the number of grid cells to advance even though the latter are technically a factor of 2 further than the former 2 3 spatiotemporally varying precipitation flooding can be driven by spatiotemporally varying rainfall rates as part of the algorithm initialization spatial precipitation fluxes are read in as a series of grids r τ for each simulation time period τ that they are available if the rainfall grids are uniformly spaced in time with periodicity δ t then this index is τ t δ t a set of m potential source locations p τ p 0 p 1 p m 1 is generated by sampling points from each grid as though it were a discrete probability distribution so that a greater number of points are likely to fall in areas of higher rainfall see fig 2 let the total precipitation flux during time period τ be denoted by φ p τ if each rivulet has volume v n l 2 d then the number of new rivulets that must be released per time step δ t to account for the total rainfall is n τ φ p τ δ t v by randomly sampling n τ locations from the precomputed set p τ the rivulets will be distributed in proportion to the spatial rainfall distribution during that period by ensuring that each set p t is randomly shuffled during the initialization phase n τ points can be pulled off in series in constant time without the need to generate random numbers during algorithm execution for this to work reasonably well one generally wants m n τ note also that the value of m in combination with the relative spatial distribution of rainfall rates sets the mean and maximum spatial resolution of potential source locations this along with the rivulet volume v influences the potential coverage area i e having too few potential source points may result in incomplete estimates of inundation area 2 4 total algorithm overview putting this all together in pseudo code yields the algorithm below at a high level the precipitation forecasts are first ingested then for each time step in the simulation the rivulet locations are updated rivulets that have left the domain removed and new rivulets instantiated based on the current spatiotemporal distribution of precipitation flux finally the element wise peak depth grid may be updated note that to improve performance the peak depth grid may only be updated periodically the process of stepping each rivulet forward through a time step then requires three operations calculating how many cells to move forward then for each cell step drying the current tail location and moving the head 3 results a comparison of the rivulet based algorithm described above and the direct solution of the shallow water equations is presented in this section two very different study areas were chosen for analysis representing two major real world wide area flood events a three day period during the landfall of hurricane harvey in the houston area during the fall of 2017 and a seven day period of extreme precipitation in boulder county colorado during the fall of 2013 these two regions and events differed significantly in topography duration and spatiotemporal rainfall patterns in both cases the solution of the shallow water equations was accomplished using the rapid inundation forecasting tool rift judi et al 2018 2014 the houston study involves a 211 km by 255 km 53 800 km 2 region of southeastern texas resolved into 60 meter grid cells 15 000 000 cells during the 72 h simulation period a total of more than 3 7 1 0 9 m 3 of water 3 million acre feet fell across the study region topographic variation in the study area is modest with a rise from sea level to a maximum of 154 m 505 ft the boulder county colorado study comprises an area of 4 476 km 2 50 km by 90 km resolved into 30 m cells 5 000 000 cells over which 605 1 0 6 m 3 491 000 acre feet of water fell during the 168 h simulation period the boulder study area has a topographic relief of 2 902 m or more than 9 500 ft the two study areas thus differ by an order of magnitude in area a factor of 19 in topographic relief a factor of six in total rainfall volume more than a factor of two in duration and a factor of two in cell size the choice of dem resolution in each case was driven by the solution of the shallow water equations the rift simulations were configurations used to respond to actual events and are therefore representative of the typical tradeoffs that must be made between spatial resolution and computational complexity when solving the swes in a real world context the rivulet algorithm was run on the same grid to allow an apples to apples comparison note that in both studies the precipitation was bounded to a subset of the hydrologic units of interest and did not necessarily extend across the full dem overviews of the houston and boulder simulation results are shown in figs 3 and 4 respectively three different approaches were used to compare the swe and rivulet algorithm results an analysis of the inundation areas forecast by the two approaches a comparison of their respective peak depth estimates and a review of the time dependent depths forecast by each at a subset of locations the results of these analyses are described in the following sections 3 1 inundation area statistics an initial comparison can be made between the swe and rivulet based forecasts in terms of the models predicted inundation areas to do this wetted areas are compared on a cell by cell basis between the two models the confusion matrix showing the four possible configurations of each pair wise comparison is shown in fig 5 and plotted spatially in figs 6 and 7 for this comparison a rivulet algorithm solution with rivulet length n 50 and rivulet thickness d 0 0125 m was used four binary metrics characterizing the relationship between the forecast inundation areas were computed for each study the critical success index csi commission rate cr omission rate or and hit rate hr they are defined in terms of the confusion matrix elements as katiyar et al 2021 stephens et al 2014 csi tp tp fp fn cr fp tp fp or fn tp fn 4 hr tp tp fn the critical success index represents the number of correct positives from the total area suggested to be inundated by either algorithm the commission rate is the fraction of all positives tp fp that were falsely asserted fp the omission rate is the fraction of cells that should have been positive tp fn that are falsely negative fn and the hit rate is the fraction of cells that should have been positive tp fn that actually are tp given the widespread nature of rainfall during the events nearly every cell within the study areas was wetted at some level however small by the solution of the swes it is therefore necessary to define what wetted means the metrics were evaluated for arbitrary thresholds of between 2 5 cm and 50 cm of swe predicted depth cells with less than the threshold flood depth were considered dry and those with depth equal to or greater than the threshold were considered wet the binary metric values are shown in fig 8 in both the houston and boulder studies the hit rate and critical success index generally increased and the omission rate decreased as the threshold depth was elevated this suggests the rivulet algorithm has an increasing reliability in correctly identifying deeper areas of inundation which are the most critical to capture in most applications in both study areas the hit rate ranged between roughly 0 8 and 0 9 in the boulder study the csi ranged from 0 57 to 0 66 and in the houston study from 0 69 to 0 79 note that areas of 0 elevation in the houston dem corresponding to open ocean were excluded from the binary metric calculations and map 3 2 peak depth error quantification figs 3 and 4 suggest a qualitative agreement between the rivulet algorithm s peak depth forecasts and the full solution of the shallow water equations to quantify the peak depth forecast error relative to the full solution of the swes the mean error me mean absolute error mae and relative mean absolute error rmae were each computed for a set of swe predicted depth ranges 0 0 to 0 25 m 0 25 to 0 5 m 0 5 to 0 75 m etc out to a depth of 8 0 m by quantifying the error separately within each depth range it was possible to characterize the errors depth dependence a peak depth grid represents the cell wise maximum depth reached over the duration of a simulation t this was computed for both the solution of the swes as d t and rivulet based algorithm as h t formally the peak depth is x max t x t where x d h the set of grid cells where the swe predicted peak depth d falls within a particular depth range say r 0 0 0 25 is s r i j d i j r and the number of such cells is n r s r the me mae and rmae for this depth range are me r 1 n r s s r h s d s mae r 1 n r s s r h s d s 5 rmae r 1 r mae r where r is the mean of the range r the rivulet algorithm has a number of parameters that can be adjusted to potentially tradeoff between accuracy and computational complexity these include the thickness of the rivulets d their length n and the algorithm time step δ t in the following sections each parameter s impact on forecast error and computational complexity will be examined 3 2 1 impact of rivulet thickness on error the error metrics defined above can be used to more rigorously quantify forecast depth differences and their dependence on rivulet algorithm parameters fig 9 shows the me mae and rmae as a function of swe forecast depth for five different choices of rivulet thickness d 1 25 cm 2 5 cm 5 cm 10 cm and 20 cm rivulet thickness sets the minimum depth difference that can be resolved so the predictive error is expected to generally increase with increasing d the top panels of fig 9 suggest that across all rivulet thicknesses the absolute error increases with depth up to depths of roughly 5 m beyond which it roughly plateaus the relative error bottom panels however tends to be greater at smaller depths than large this basic trend is seen in both the houston and boulder studies the middle panels the mean error suggest that the rivulet algorithm over predicts depth on average in nearly all swe forecast depth ranges of the boulder study and for all rivulet thicknesses the same is true in the houston study area when rivulet thicknesses of 5 cm or less are used for thicker rivulets 10 and 20 cm the rivulet algorithm begins to underpredict on average depths greater than 4 m more work is needed to understand the precise cause s of these differences the results also confirm an increase in both absolute and relative error with increasing rivulet thickness d though both increase only minimally as the thickness is varied between 1 25 cm and 5 0 cm beyond that the error begins to increase more noticably this is important because the overall computational complexity of the rivulet algorithm simulation decreases as the thickness is increased see fig 10 this suggests an opportunity for simulation parameters to be tuned so as to reduce computational complexity while having a relatively minimal impact on error fig 10 shows the computational complexity of the rivulet algorithm solution both independently and relative to the full solution of the swes the swe solution in houston took 3 h and 41 min to perform using rift a c cuda swe solver parallelized across 4 992 gpu cuda cores an nvidia k80 for a rough estimate of 1 1 million gpu minutes of computation time for the boulder county event the swe solution took 6 6 million gpu minutes by contrast the rivulet algorithm was implemented in scala run on the java virtual machine jvm and parallelized across 4 cpu cores a single quad core intel xeon e5 processor the number of cpu minutes required to perform a rivulet based simulation is shown as a function of rivulet thickness in the left panel of fig 10 the relative computational complexity the ratio of swe to rivulet simulation complexity is shown in the right panel the relative complexity savings varies depending on study area and rivulet parameters from a low of 10 000 to a high of nearly 10 million less of an efficiency gain was observed in the houston study area than in boulder there are likely a couple of reasons for this the computational complexity of the swe solution does not vary significantly with rainfall rate since once a cell is wetted the computation time required to model depth changes at that location is constant by contrast the number of rivulets required to model a given precipitation flux φ p τ is n τ φ p τ δ t v as the rainfall rate increases the number of rivulets spawned at each time step increases in proportion the total rainfall over the simulation periods was six times greater in houston than in boulder accounting for a significant fraction of the relative complexity difference note too that the houston area has far less topographic relief than boulder leading to significantly longer mean persistence times for each rivulet even after accounting for the relative difference in simulation area this also contributes to a greater overall relative complexity 3 2 2 impacts of rivulet length on error both forecast error and computational complexity also vary with rivulet length n error rates are shown again as a function of swe forecast depth in fig 11 for rivulet lengths of 50 100 200 and 500 cells this corresponds to spatial distances of 1 5 3 6 and 15 km in the boulder county study where 30 m grid cells were used and distances of 3 6 12 and 30 km in the houston study 60 m grid cells as the rivulet length is increased the spatial resolution of the rivulets is effectively smoothed the impact may be smaller than one might imagine however if all rivulets moved parallel to one another along the direction of channel flow indeed the spatial resolution in the longitudinal direction would be constrained to the length of the rivulet in reality however rivulets trace complex paths back and forth across channels often moving perpendicular to the mean flow direction as they interact with one another through the depth grid the result is that their width transverse to their direction of motion a single grid cell becomes the dominant factor in the spatial resolution that can effectively be achieved rivulet length has an impact on the temporal scales that can be resolved as well as a single rivulet may take tens of minutes to pass a static point on the landscape this would for example prevent rapid shifts in rainfall rate from being immediately reflected in depth estimates in reality however precipitation data is typically only available on timescales of an hour or more and if finer temporal granularity is required the rivulet length can be reduced the computational complexity shown in fig 12 varies with rivulet length smaller values of n necessitate a greater number of rivulets to account for a given rainfall rate and hence a greater overall computational complexity 3 2 3 impact of time step on error the variation of error with time step is a little bit different recall that the number of rivulets that must be created at each time step is n τ φ p τ δ t v if the time step δ t is shortened the number of rivulets that must be created at each time step is reduced but the total number of time steps is increased by a compensatory amount as a result the computational complexity is not expected to vary significantly with time step this is indeed what is observed in fig 13 where the computational complexity remains relatively constant and only falls off at the highest and lowest values where the rivulet approximation itself begins to fail too short a time step and many rivulets may not be able to move far enough during each period to make their largely integer moves an accurate reflection of rivulet speed too long a time step and continuity between steps is lost a rivulet s tail might end up significantly further downstream in one time step than its head was in the previous this effect is apparent in the relative absolute error magnitudes shown in the bottom panels of fig 14 in the case of the houston study left panel the relative error rapidly asymptotes to 1 0 for both the shortest 0 25 min and longest 80 min time steps in the boulder study the shortest time step did better light red curve but still indicated significant error lowest relative errors were achieved with time steps between 1 min and 10 min in houston and between 0 5 min and 10 min in boulder in both cases simulations with a 10 min time step outperformed simulations with a 4 min time step additional investigation is needed to better understand some of these idiosyncrasies the upshot is that this relative insensitivity of the complexity and error to time step size means that a value of 1 to 4 min can be chosen relatively safely without having to make a complex assessment of the relative importance of computational efficiency and accuracy the downside is that the time step does not present an opportunity for further influencing this calculus either the primary factors that can be traded off are rivulet length and thickness 3 3 temporal dynamics in addition to comparing the peak depth forecasts of the rivulet algorithm and swes hourly depth forecasts were compared at a number of locations within the houston and boulder study areas a subset of which are included here the temporal pattern of rainfall differed significantly between the two studies during the landfall of hurricane harvey the vast majority of rain occurred within a single twenty four hour period the second day of the simulation left panel of fig 15 by contrast rainfall during the boulder flood event was much more temporally widespread and sporadic right panel of the same figure the result was that flood depths in the houston study rose sharply at each measurement location early on the second day of the simulation before falling off smoothly over a longer timescale see blue curves of fig 16 by contrast flood depths in boulder rose and fell repeatedly during the course of the event fig 17 as new waves of rainfall drove renewed flooding in many drainages in each case the rivulet algorithm captures the timing magnitude and persistence of the flooding reasonably well it does introduce some oscillatory behavior as illustrated in the orange curves of fig 16 and the top right panel of fig 17 more work is needed to tease out the cause of this behavior that the overall temporal dynamics are well characterized however suggests that the rivulet based algorithm may be useful not only for predicting inundation areas but also for rapidly estimating the gross temporal dynamics of wide area flooding events 3 4 comparison with empirical high water marks in the previous sections the rivulet algorithm forecast depths were compared to those of the full solution of the shallow water equations in this section peak depth forecasts from both the rivulet algorithm and the solution of the shallow water equations are compared to empirical high water mark observations reported following hurricane harvey u s geological survey a total of 907 high water mark observations fig 18 fell within the houston simulation area the index of the grid cell containing the latitude and longitude of each observation was computed and the models peak depth forecasts for those cells compared to the height above ground of the corresponding high water mark the mean error of the shallow water equation forecasts was 0 39 m and the mean absolute error 0 86 m the mean error in the rivulet forecasts was slightly greater at 0 45 m as was the mean absolute error 0 92 m we suspect that a major contribution to these errors the differences between both model forecasts and the observed high water marks is the finite resolution of the underlying dem as an example fig 19 shows a 3 3 subset of the dem in the area of river terrace park near channelview texas the rough location of the creek that runs through the park is indicated in darker blue the underlying gray cubes indicate the dem assessed elevation the translucent blue cubes the shallow water equation predicted depth and the black points the location and observed height of the high water marks the observed height above ground is indicated with white text on dark gray while the shallow water equation forecast depths are shown in black on white at the location of the two observations the differences between the shallow water equation forecasts and observed peak depths are 0 13 m and 0 41 m in some ways this level of agreement is fortuitous high water mark observation a was made on the bank of the creek which is essentially at equal elevation throughout this entire sub section of the grid the dem places the elevation of this cell at 5 41 m and the depth at 1 50 m had the observation been made at locations b or c however the high water mark height above ground should have been nearly the same because of the equal stream bed elevation but the model would have forecast depths of 3 96 m and 5 7 m respectively as the high water mark data includes elevation the potential impact that dem resolution may have on forecast error can be further explored through an analysis of the differences between observed and dem reported elevation the mean absolute difference between the measured local elevation and the corresponding elevation of the dem cell the measurement fell within is 1 07 m the mean standard deviation between the elevations of each measurement dem cell and its four nearest neighbors is 1 19 m this provides an estimate of the expected topographic variation over the scale of a cell width the difference between high water mark observations and model forecasts therefore lies within the variability expected to result from the finite horizontal resolution of the digital elevation model and associated elevation uncertainty with the relative differences between the reported error of the two model forecasts much smaller further because of the relative efficiency of the rivulet based approach the full houston scenario can be rerun using a 10 m 1 3 arc second dem resulting in a 25 487 x 21 131 grid of roughly 540 million grid cells see fig 20 a rivulet length of n 1 200 cells equivalent in absolute length to an n 200 rivulet at 60 m resolution and rivulet thickness of d 0 075 m was used the simulation ran in 19 min wall clock time on the same four core desktop computer used in the other simulations at 10 m resolution the mean error of the rivulet based forecasts was 0 09 m versus 0 45 m on a 60 m grid and the mean absolute error 0 67 m versus 0 92 m on a 60 m grid here too though we expect that much of the error is still driven by the spatial resolution of the dem a factor of six decrease in cell size from 60 m to 10 m does not necessarily correspond to a factor of six decrease in elevation variation on a 10 m grid the mean absolute difference between the reported high water mark and dem elevations is still 0 90 m and the nearest neighbor standard deviation 0 84 m in the scenarios investigated the finite spatial resolution of the underlying dem is thus likely a significant driver of the apparent forecast error by comparison the difference between the rivulet and shallow water equation based errors is relatively small with both performing comparably well 4 discussion and conclusions the rivulet based algorithm is a novel approach that may be used to support real time risk assessments studies of depth and inundation area sensitivity to climate change driven variations in precipitation generation of large scale wide area ensembles for uncertainty quantification etc one of the key benefits of the approach is the ability it provides researchers to smoothly trade off between computational complexity and mean error shown in fig 21 there is a clear trend indicating that by decreasing the rivulet length and or thickness one can reduce the mean absolute error of peak depth forecasts but with the side effect of reducing the complexity savings relative to the full solution of the swes this means that one could for example do a large suite of coarse simulations to identify the highest risk scenarios or regions a handful of more computationally intensive more accurate simulations could then be performed as a storm system approaches to refine estimates of depth inundation area and temporal dynamics for the riskiest scenarios the trade off between accuracy and complexity could likely be improved upon by adaptively tuning rivulet parameters to different topographies fig 22 shows the results of performing error calculations on the western and eastern halves of the boulder study area independently the results in the eastern portion look similar to the results on aggregate while the results in the western portion show that error rates there have almost no dependence on rivulet thickness topographically the eastern portion of the boulder study area is relatively flat quite similar to the houston study area the western portion however includes more than 9 000 ft of topographic relief and is characterized by narrow tightly constrained channels this result suggests that it may be possible to get away with far less computational complexity in such areas potentially saving one to two orders of magnitude in computation time there to implement this one could imagine allowing rivulets that originated in tight channelized regions to adaptively split into two or more shorter thinner rivulets when they moved into areas of flatter topography this would be done in a way so as to conserve overall flow volume the rivulet algorithm outlined here provides a novel lagrangian based approximation of flood dynamics it allows an orders of magnitude reduction in computational complexity over the solution of the shallow water equations while retaining sufficient spatiotemporal accuracy to be useful for many applications simulations are easy to setup requiring only a dem and precipitation grids with no other preprocessing or configuration steps the computational savings it represents can be spent in other ways for example generating large ensembles of model forecasts or significantly increasing the spatiotemporal breadth of study areas potentially enabling rapid continent scale simulation of hydrodynamics these opportunities could help researchers to better characterize flood risks under a changing climate they could also allow for tighter interdependent integration with other models for example integration with agent based models of populations in flood prone areas could allow for a better characterization of socioeconomic impacts and the potential value of various avenues for mitigation or adaptation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the multisector dynamics program area of the u s department of energy office of science office of biological and environmental research as part of the multi program collaborative coastal modeling icom project contract number de ac05 76rl01830 and by the u s department of homeland security cyber and infrastructure security agency national risk management center 
25446,eco hydraulic models are wide extended tools to assess physical habitat suitability on aquatic environments currently the application of these tools is limited to short river stretches and steady flow simulations however this limitation can be overcome with the application of a high performance computing technique graphics processing unit gpu computing r iber is a gpu based hydrodynamic code parallelised in cuda fortran that with the integration of a biological module performs as an eco hydraulic numerical tool r iber was validated and applied to real cases by using an optimised instream flow incremental methodology in long river reaches and long term simulations r iber reduces the computation time considerably reaching speed ups of two orders of magnitude compared to traditional computing r iber allows for overcoming the current limitations of the eco hydraulic tools with the simulation of high resolution numerical models calculated in a reasonable computation timeframe which provides a better representation of the hydrodynamics and the physical habitat keywords fish habitat weight useable area numerical modelling high performance computing gpgpu abbreviations swe shallow water equations hpc high performance computing gpgpu general purpose computing on graphics processing unit gpu graphics processing unit cpu central processing unit cuda compute unified device architecture ifim instream flow incremental methodology wua weight usable area data availability the authors do not have permission to share data software availability name of tool r iber developers david lópez gómez marcos sanz ramos ernest bladé year first available 2021 hardware required basic computer with a graphical power unit gpu based on cuda architecture mainly any nvidia gpu requirements windows os x64 source code availability the numerical tool is freely distributed through www iberaula com data availability the authors do not have permissions to share the data cost free program languages cuda fortran 1 introduction water resources and river management are no longer uniquely related to flood or drought scenarios on the contrary to properly study the system from a holistic point of view the relationship between hydraulics and biota must be considered benjankar et al 2018 palau and alcázar 2012 wilkes et al 2016 in all situations not only during extreme conditions eco hydraulics is a technique that analyses the effect of physical environmental properties flow depth velocity turbulence temperature substrate etc on aquatic environments bovee 1982 such as rivers to characterise the physical habitat several approaches have been proposed based on hydrologic geomorphologic or hydraulic criteria tonina and jorde 2013 particularly for hydraulics a common method for assessing physical habitat is the use of normalized univariate curves or suitability curves that relate variables of the river environment with the inhabiting species use of the space several numerical hydrodynamic tools have been developed or enhanced by integrating the biological requirements of one or several target species typically of fishes cassan et al 2022 hung et al 2022 jowett 2004 meza rodríguez et al 2019 nones 2019 sanz ramos et al 2019 shim et al 2020 stamou et al 2018 steffler and blackburn 2002 but not exclusively hamilton et al 2015 zohmann et al 2013 these tools are based on the solution of the one dimensional 1d and most recently on the two dimensional 2d saint venant equations or shallow water equations swe which provide the evolution of the water depth and the specific discharge or velocity of the water despite the current rapid increase in the use of three dimensional 3d river flow modelling bermúdez et al 2017 2018 meselhe et al 2012 pisaturo et al 2017 the computational cost of this method limits its application to very short river stretches and research purposes apart from the simplifications applied to the generation of the suitability curves boudreault et al 2022 palau et al 2016 limitations of 2d swe based eco hydraulic models typically relate to computational cost carlotto et al 2021 morales hernández et al 2021 the time required to simulate large river stretches flow properties in natural conditions e g long time series and the use of a very fine domain discretisation with thousands or millions of calculation points or mesh elements typically require high computational effort these limitations can be overcome with a good code design or and with the application of high performance computing hpc techniques currently one of the most applied techniques is the parallelisation of the computations by using general purpose computing on graphics processing unit gpgpu instead of the traditional central processing unit cpu computing buttinger kreuzhuber et al 2022 carlotto et al 2021 morales hernández et al 2021 a gpgpu technique commonly used in computational fluid dynamics cfd is that provided by the nvidia compute unified device architecture cuda which is currently available for the c and fortran programming languages nvidia 2022a the benefit of using gpu computing is the significant reduction in computation time with speed ups of one or two orders of magnitude compared to cpu computing nevertheless 2d swe based eco hydraulic numerical tools that include an integrated gpu parallelised hydrodynamic module and physical habitat module are lacking this study aims to fill this gap with the presentation of a new gpu based hydraulic code parallelised in cuda fortran named r iber and its performance as an eco hydraulic numerical tool with the integration of a physical habitat module the code was first validated using two different benchmark tests for 2d swe based models and a laboratory experiment with a vertical slot fishway then r iber was applied to two real cases to show not only the full integration of the hydrodynamic and physical habitat module for some fish species but also the benefits of using a fully integrated gpu parallelised eco hydraulic tool in high resolution numerical models and long term simulations 2 materials and methods 2 1 governing equations most existing eco hydraulic simulation tools are based on the solution of the mass and momentum conservation equations that when applied to a 2d framework are named 2d shallow water equations 2d swe they are derived from navier stokes equations through a time averaging to filtrate the turbulent fluctuations reynolds averaged navier stokes equations rans and a depth averaging to obtain the final 2d equations two dimensional swe are a hyperbolic nonlinear system of three differential equations in partial derivatives toro 2009 which can be written as follows 1 h t h u x x h u y y 0 h u x t x h u x 2 g h 2 2 y h u x u y g h s o x s f x x ν t h u x x y ν t h u y y h u x t y h u x u y y h u y 2 g h 2 2 g h s o y s f y x ν t h u y y y ν t h u y y where h is the water depth u x and u y are the x and y components of the depth averaged velocity g is the gravitational acceleration s o x z b x and s o y z b y are the spatial variation of the bed elevation z b on the x and y directions s f x n 2 u x u x 2 u y 2 h 4 3 and s o y n 2 u y u x 2 u y 2 h 4 3 are the energy dissipation in the x and y directions due to bed friction which is computed using the manning formula and ν t is the turbulent viscosity the previous system of equations are the main part of the hydrodynamics module of iber bladé et al 2014a 2014b a 2d numerical tool that was initially developed for hydrodynamics and sediment transport modelling bladé et al 2014a 2014b 2019b 2019a sanz ramos et al 2020a it solves the 2d swe on irregular geometries using the conservative scheme based on the finite volume method of roe 1986 which consists of the godunov method together with the roe approximate riemann solver toro 2009 the tool has been continuously enhanced and the sequential version currently includes a series of modules for different free surface flow processes such as hydrological processes cea and bladé 2015 sanz ramos et al 2020b 2021 pollutant propagation cea et al 2016 large wood transport ruiz villanueva et al 2014 and physical habitat suitability assessment sanz ramos et al 2019 2 2 eco hydraulics methods the combination of a hydrodynamic model and a biological model can be oriented to develop eco hydraulic tools nestler et al 2016 one of the most extended biological models to assess physical habitat particular for freshwater fishes is the use of suitability curves a normalized relation between environmental variables hydraulic water quality geomorphological etc and the use of the space of the target specie stadium the elemental habitat suitability of a physical variable ranges from 0 to 1 values being close to 1 those correspond to a high degree of suitability the multiplication of the elemental suitability depth and velocity dependent of each species stadium was used as habitat criteria in the following sections but other criteria can be used without loss of applicability the instream flow incremental methodology ifim is a technique that uses the combination of hydrodynamic and biologic models to create a habitat or eco hydraulic model it consists of the assessment of different constant discharges q in a river stretch to obtain the weight useable area wua for each one bovee 1982 with these q wua relations it is possible to obtain the theoretical discharge that maximises the wua for the species and stadiums analysed and which is usually utilized to propose the environmental flows in 2d eco hydraulic modelling the use of the ifim traditionally implies the generation of as many models as the discharges to be evaluated being an analysis conducted under steady flow conditions 2d swe based models usually require simulating the model from the beginning to an unknown maximum time of simulation additionally the time required to achieve each steady state of the series of discharges to evaluate is in general unknown this time depends not only on the geometry and length of the river but also on the flow intensity this issue is addressed with the definition of a series of constant discharges or stepped discharges as an inlet condition the stepped discharge option allows for computing in a unique model a series of discharges that generates steady conditions considering a tolerance between the inlet and the outlet when the steady conditions of the target discharge are achieved the model automatically steps to the next discharge the hydrodynamic conditions at the end of a step are used as initial conditions for the next step thus this option uses the time strictly necessary to reach the steady flow conditions of any of the constant discharges defined by the user without having to know the time to reach the steady flow conditions for any discharge and reducing the computational time to a minimum 2 3 cuda fortran code r iber the sequential version of iber is partially parallelised by means of the open multi processing openmp technique however the intrinsic limitations of this technique only allow for speed ups corresponding to the number of cpu cores at most despite the computing time of the hydrodynamics being reduced notably the simulation of long stretches of rivers even entire rivers or long time series for fish habitat assessment procedures remains unapproachable with techniques based solely on cpu computing the integration of several calculation modules in the fortran based code of iber was originally functionality oriented prioritising the full integration of all modules instead of multi core processing efficiency due to the growing application of more complex and detailed simulations in river hydrodynamics demands an increase in computational capacity and thus a code re ordering and optimization was already made in previous versions of iber despite that the simulation of models of thousands or millions of elements or long time series requires several hours or even days of computational time with the sequential version r iber is a new code that implements a gpu parallelisation of the hydrodynamic bladé et al 2014b and habitat sanz ramos et al 2019 modules of iber based on cuda fortran architecture an efficient cuda programming requires not only a hierarchical organisation of the computing units but also the programming of each thread block to efficiently process data in parallel zhang and jia 2013 to achieve this the original code of iber was re organised and re written to follow the main structure of cuda programming that is transfer data from cpu memory host to gpu memory device run parallel computing through kernels and transfer data back to the host from the device more details of the structure of the code are described in appendix a the main limitation of gpu parallelised codes is the memory access and especially the memory transfer garcía feal et al 2018 vacondio et al 2014 zhang and jia 2013 r iber reads transfers and writes memory data only when it is strictly necessary with the aim of maximising the code s efficiency another issue which also occurs in cpu computing is that of the multi core computing simulations running several simulations at the same time in the same processor unit creates a significant bottleneck particularly for gpu computing and reduces the global computing capacity of the device morales hernández et al 2021 to minimise heavy performance penalties r iber allows for the selection of the gpu device on which to carry out each calculation thus the option of a multi processor computation on a single computer with several gpus or on a cluster of gpus is available to practitioners fig 1 presents the main workflow of the r iber code additionally to achieve maximum performance on gpu processors on arithmetic operations it is necessary to use as many single precision arithmetic operations as possible hwu et al 2009 as it is done in r iber using single precision operations implies round off errors in the calculation results i e after a single floating point operation the result is rounded after the seventh digit however cpu based simulations perform double precision arithmetic operations as it is done in iber with similar performance to single precision narumi et al 2011 on the other hand the habitat module of iber and also of r iber is computed as a post process because the currently implemented fish suitability curves only depend on the hydrodynamics thus for optimum memory access transfer the information exchanged between hydrodynamics and habitat must be done directly in the host and only when the results have to be written this moment may occur when the simulation time is greater than the user defined time to write the results or when the steady flow conditions are accomplished if the stepped discharge option is used additionally the process of writing the results utilises an independent cpu core fig 1 therefore once all data is transferred back to the host the habitat module is computed in the cpu while the gpu continues calculating without any performance penalty it is worth noting that 2d swe based models do not solve the energy equation which in 1d only depends on the energy balance between two consecutive sections in 2d models a steady state is reached when the inlet discharge is equal to the outlet discharge and thus the whole flood front propagation process must be solved anyway this implies an increase in the computational time because the simulation time needed to reach steady conditions in general is unknown therefore longer simulation times are required to ensure a steady state this particular issue is solved with a specific option called stepped discharge which allows for the simulation of steady flow conditions for several discharges in the same model and directly provides flow habitat relationships such as the wua with this option enabled gpu parallel computation would potentially reach its maximum calculation capacity as the results would only be written a few times at each step 2 4 hardware and characteristics simulations were carried out with iber v3 1 which contains the new code of r iber and are freely distributed through www iberaula com both algorithms use the roe 1st order numerical scheme computations with the sequential version cpu were done with a cpu intel core i7 9750h while computations on the cuda fortran gpu version r iber were launch with the same cpu but computed in different gpu devices table 1 the nvidia gpus selected were the geforce gtx and rtx series which being quite common devices for standard desktop computers and mid range to high end gaming laptops cover a wide temporal range 3 validation 3 1 test 1 flooding of disconnected floodplains the first validation test was the so called test 1 proposed by the united kingdom environment agency within the defra flood and coastal erosion risk management research and development programme néelz and pender 2013 this test aims to assess the capability of the 2d hydrodynamic models to reproduce the flooding of disconnected floodplains and the wetting and drying processes the domain consisted of a 700 100 m channel with a bottom elevation composed of an increasing slope 0 02 followed by a decreasing slope 0 25 and ending in an increasing slope 0 3 thus a depressed area was generated where the water would be retained as boundary condition a water level was imposed varying from 9 7 to 10 35 m in 1 h keeping a constant value for 8 h to ensure the depressed area was completely filled then returning to 9 7 m in 1 h two calculation scenarios were performed one following the proposed domain discretisation with 1125 mesh elements and the other using the full digital elevation model dem resolution which provides 26 341 calculation points simulations were also done with the cuda c gpu code of iber called iber garcía feal et al 2018 fig 2 shows the water evolution at the two control points p1 400 50 and p2 600 50 the three numerical models show the same hydraulic behaviour water accumulated in the depressed area at an elevation of 10 25 m flush with the topography the computational times of the reference model iber v3 1 sequential were 44 and 2064 s for the proposed discretisation and the full dem models respectively the speed ups of r iber reached 3 4 and 39 4 times while iber reached 2 8 and 40 5 times respectively for the gtx 1660 ti similar values were obtained for the gtx 980 ti and the rtx 3070 devices table 2 it is worth noting that for the proposed discretisation in which only 1125 elements were used the speed up was not significant because the parallelised part of the code was faster than the time required to transfer information from the device to the host and to write the results file this bottleneck is practically neglected when the number of elements increases 3 2 test 2 hydraulic jumps and flow obstructions the second validation case is the test 6a of defra s benchmark néelz and pender 2013 this case aims to verify the capability of 2d swe based models to simulate transcritical flow regimes hydraulic jumps and wakes behind obstructions for that purpose the experiment of soares frazão and zech 2007 was simulated in iber and r iber using two different mesh discretisation of rectangular elements that proposed by defra s benchmark 56 616 elements and the full dem resolution 226 352 elements an initial water elevation of 0 4 m was imposed from the beginning of the flume to the narrowing of it upstream of the obstacle this configuration tries to reproduce a dam break process in the downstream area where the obstacle is placed non symmetrically on a wet area in the flume results were extracted each 0 1 s considering a fixed time step of 0 005 s six control points monitored the water depth and velocity during the experiment g1 to g6 all numerical approaches behaved similarly fig 3 shows the evolution of the water depth and velocity at points g1 g2 and g3 the arrival time of the flood front was the same and suitably captured both the supercritical and subcritical flow regimes at point g2 the presence of the obstacle generated a mobile hydraulic jump upstream at around 12 s where water depth evolved from 0 03 to 0 1 m during the 10 s after the shock wave in the case of the proposed mesh discretisation the hydraulic jump was produced at 9 4 s yellow and grey dashed line while with the full dem the hydraulic jump was produced at 16 3 s for iber black dashed line and 11 s for r iber red line iber generated a weak hydraulic jump at 11 s which then transformed into a strong hydraulic jump a few seconds later 16 3 s this difference in this particular case of the simulation of a hydraulic jump with a very fine discretisation of the calculation domain grid of 0 05 0 05 m can be attributed to the fact that r iber computes in single precision while iber does it in double precision thus after a single floating point operation the result is rounded after the seventh digit in r iber in general the reproduction of a hydraulic jump is quite complex to reach with full precision particularly in 2d swe based modelling when the hydraulic jump evolves along time due to the discharge is not constant a sharper solution is generally obtained néelz and pender 2013 despite that the flow patterns were well captured by r iber even using single precision and a roe 1st order numerical scheme cea et al 2010 the numerical simulation in the sequential code iber spanned 172 s for the proposed discretisation and 706 s for the full dem model while the parallelised r iber code computed the models in 35 45 s and from 130 to 197 s respectively this means an average speed up of around 3 6 5 4 times with respect of the sequential code table 3 it is worth noting that the necessity of writing results at very small time steps 0 1 s considerably reduces the global simulation time if only computational time is considered depending on the gpu used the speed up of r iber reaches values from 65 4 to 75 3 and from 96 8 to 140 4 times for the proposed and full dem models respectively table 3 values in brackets 3 3 test 3 fishway and recirculation areas test 3 is the numerical reproduction of the laboratory experiment presented by puertas et al 2004 which consisted of the hydraulic characterisation of a vertical slot fishway the original flume device was a 12 m long channel divided into 11 pools four for the design of test 1 and four for the test 2 design while the rest were the upper intermediate and lower pools pool 7 which belonged to the test 1 design fig 4 a was analysed considering half of the flume a 5 7 slope and a discharge of 0 0741 m3 s a detailed description of the flume experiments can be found in puertas et al 2004 previous numerical experiments confirm the capability of 2d swe based models for modelling the free surface flow in vertical slot fishways bermúdez et al 2010 cea et al 2007 puertas et al 2012 thus this validation case aimed to reproduce main jet and particular recirculation areas flow patterns fig 4b by considering four mesh discretisation 2751 m1 5502 m2 22 008 m3 and 88 032 m4 elements in general a mesh size reduction implies a better representation of the hydrodynamics and consequently the eco hydraulic assessment however higher computational times are required the numerical discretisation of 2751 elements m1 provided smooth results with only a weak recirculation area on the right side downstream of the pool inlet and a considerably straight main jet fig 5 a this recirculation area increased as the number of elements doubled m2 and another two weak eddies appeared on the upstream and downstream corners on the left side between the baffle and the wall fig 5b a computational mesh of eight times more elements m3 than the first mesh m1 allowed for the proper definition of two separately recirculation areas on both sides of the main jet fig 5c this domain discretisation effectively defined the general flow pattern shown in fig 4b with a well defined main jet in the model with a mesh of 88 032 elements m4 the flow patterns became more complex representing small eddies near the bigger eddies to both sides of the main jet fig 5d numerical results were compared to the experimental results throughout the mean depth of the middle traverse section h 0 the mean flow depth of the pool h m the maximum depth of the pool h max the minimum depth of the pool h min the depth of the slot h s the absolute velocity of the slot v s the discharge coefficient of the slot c d q b h s v s where b is the slot width and the energy dissipation rate in the pool e d ρ g q s 0 b h 0 where ρ is the flow density g is the gravitational acceleration q is the flow discharge s 0 is the bottom slope and b is the pool width table 4 summarises the experimental and numerical results obtained with each numerical mesh in general r iber performed suitably for all the simulations particularly for the finer meshes table 5 which provided a good fit for the observations reproducing the flow patterns such as eddies and the shape and dimension of the main jet the m1 mesh 2751 elements showed the poorest results with absolute relative errors slightly above 10 in some hydraulic parameters h 0 h m and e d by contrast the m3 and m4 meshes 22 008 and 88 032 elements respectively provided suitable results with errors below 3 5 numerical results of the discharge coefficient c d and the energy dissipation rate e d also adjusted suitably to the observations with a relative error less than 3 5 for m3 and m4 meshes the benefit of using the cuda fortran gpu parallelisation r iber instead of iber is the considerable reduction in computational time table 6 in this particular case r iber reached a speed up from 23 9 gtx 980 ti to 45 1 times rtx 3070 for mesh m4 in comparison with the sequential version additionally the efficiency of the code can be evaluated by accounting for the computational cost per number of elements the computational time required by iber increased from 24 7 m1 to 106 6 s m4 per 1000 elements while in the case of r iber it decreased from 5 to 2 9 s thus it can be seen that the new parallelised code of iber r iber provides not only a valuable computational cost reduction but also a notable improvement in the efficiency of the computations when the number of elements of the model increases 4 study cases the performance of the model has been tested in two real study cases eume river and cinca river fig 6 the eume case study is focused on showing the benefits of using not only the gpu parallelised eco hydraulic model of iber r iber but also the noticeable improvement of using the stepped discharge inlet condition instead of creating a separate model for each discharge an intrinsic benefit of the gpu parallelisation of 2d swe based models is the capability to assess long term hydrodynamics in long rivers in a reasonable timeframe if a fish habitat module is integrated into the code such as in r iber the eco hydraulic evaluation of rivers will no longer be limited to a few metres or short river stretches to demonstrate this the capacity of r iber to simulate long term fish suitability variations in long river stretches even entire rivers is presented in the case study of the cinca river 4 1 improved ifim for 2d swe based models eume river the analysed stretch of the eume river located in the north west of spain fig 6a is characterised by two well differentiated geomorphological facies a discontinuous channel with jumps and pools upstream and a continuous channel downstream all discharges were evaluated with iber and r iber considering a separate model for each discharge and also using the stepped discharge option which consisted of a unique model in this case the tolerance was fixed at 1 a new step of the series of inlet discharges was automatically imposed when the absolute value of the outlet discharge minus the inlet discharge all divided by the inlet discharge was less than the tolerance the study area of 3 km in length fig 6b was discretised into 72 426 calculation elements twelve discharges were evaluated ranging from 0 149 to 11 770 m3 s salmo trutta was the target specie and the depth and velocity dependent suitability curves described in che and aca 2008 were used as biological information a detailed description of the study area can be found in sanz ramos et al 2019 as expected the wua depended on the hydrodynamics of each evaluated flowing discharge fig 7 shows in the transition zone between two geomorphological facies fig 6b hollow black square the elemental suitability evaluated for different discharges and the adult stadium product of the depth and velocity suitability low discharges produced a narrow wet area particularly at the facies characterised by jumps and pools fig 7a and b the most fish suitability was obtained in zones with high depths pools when the discharge increased fig 7c the discontinuous channel became continuous the velocity increased and the suitability of this area decreased this is clearly seen in the lower part of the river where a moderate depth and low velocity area provided a high suitability zone however high discharges 11 77 m3 s modified the hydrodynamics in this area reducing the suitability of the central part of the channel and displacing it to the lateral sides fig 7d the resulting wua is reduced for all flows and stadiums of the target specie remaining below 14 of the total wetted area the maximum suitability conditions were achieved in the fry for a discharge of 0 560 m3 s 12 78 of the wetted area and in the juvenile and adult stadium for 2 176 m3 s fry 8 8 adult 6 2 of the wetted area identical hydrodynamic and habitat results were obtained with iber and r iber but with significant differences in the computational time table 7 the simulation process of computing each discharge per model required between 195 and 357 s in the sequential mode cpu while the gpu version required less than 30 s for each simulation fig 8 a accounting for the total computational time of all the models a global time of 2844 s was necessary for the cpu version this time was reduced to 207 s for the gtx 980 ti 189 s for the gtx 1600 ti and 124 s for the rtx 3070 fig 8a the benefits of using the stepped discharge option instead of a separate model for each discharge was evaluated as the quotient between the addition of the time of each model and the time of the model that used the stepped discharge option the advantage of using the stepped discharge option instead of a separate model for each discharge is proved by a 16 6 acceleration in the computation time in cpu based simulations fig 8b when gpu calculations were applied speed ups of 32 52 were achieved fig 8b using the rtx 3070 gpu did not reduce the computation time as much as possible due to the model s low number of elements however when a higher number of elements discretise the domain the advantage of using a more powerful gpu clearly shows its potentialities as shown in section 4 2 it is worth noting that only the computation time was considered in the previous comparison the time needed for changing the boundary condition was not taken into consideration if a constant time of 30 s for each simulated discharge is considered as the necessary time for changing the boundary conditions and re starting the simulation a global speed up of around 350 would be reached for the gpu computations of the stepped discharge option 4 2 long term simulations in long rivers stretches cinca river cinca is a 191 km long mountain river that begins in the central part of the south of the pyrenees and drains into the ebro river in spain fig 6a the analysed stretch of the cinca river is 11 km long starting 2 5 km upstream of the bellós river junction and ending at the ara river junction fig 6c this stretch is characterised as a braided river with a considerably active morphology béjar et al 2018 vericat et al 2017 the flood event of 9 october 2014 which had two peak discharges and spanned 10 days che 2021 was simulated with three different meshes of 74k elements m1 299k elements m2 and 1885k elements m3 the river domain was discretised using an irregular mesh of triangular elements with 220 m1 887 m2 and 5577 m3 elements per hectare the last one being one order of magnitude above the common values used in flood studies sanz ramos et al 2020a the mesh was updated with the 2 2 m dem provided by national geographic institute of spain ign 2022 during these 10 days where discharges ranged from 10 base flow to 250 m3 s maximum peak discharge variations in the fish habitat suitability were evaluated for salmonids salmo trutta che and aca 2008 and cyprinids chondrostoma polylepis and barbus bocagei martinez 2000 these target species were selected within the cinca s basin characteristics che 2002 in contrast with the ifim the main result of this analysis is the evolution of the wua over time besides the evolution of the fish suitability distribution fig 9 a presents the evolution of the wua in for the three abovementioned species and stadiums fry juvenile and adult and the m2 domain discretisation changes in the hydrological regime also modify the wua as expected low discharges favoured a more suitable area for fry and juvenile stadiums despite high discharges generating high depths and velocities in general other areas flooded provided more suitable zones for the chondrostoma polylepis black lines and barbus bocagei green lines while the wua of the salmo trutta red lines decreased fig 9b exemplifies the variation in the wua evolution for adult continuous line juvenile dashed line and fry dotted line of the species barbus bocagei when the models of 220 m1 black line 887 m2 green line and 5577 m3 red line elements per hectare were considered as has also been demonstrated in sections 3 2 and 3 3 a finer discretisation of the study area provides hydrodynamic results of a higher resolution which are the bases of wua assessment in general the m2 mesh provided a mean increment of 7 9 of the wua in comparison to the m1 mesh while the m3 mesh reached up to 18 1 more wua than the m1 mesh in terms of computational time the sequential code iber required 7 4 h for the mesh m1 71 3 h 3 days for the mesh m2 and 1081 3 h 45 days for the mesh m3 for the m1 m2 and m3 mesh configurations r iber reached speed ups from 19 8 to 183 1 times respectively table 8 the efficiency of the code can be evaluated by the computation time required per 1000 elements the time required by iber increased when the number of elements increased from 356 to 2062 s while in the case of r iber the time remained almost constant at around 10 20 s per 1000 elements a similar trend was obtained in the validation tests complementary to the traditionally used ifim the assessment of the fish habitat suitability considering real discharge scenarios even in real time can be a step forward in eco hydraulics as it is not limited to steady conditions particular constant discharges furthermore it also encourages the study of the convenience or inconvenience of reintroducing native species and the potential prevalence of invasive species to the natural or induced hydrologic regime of a river additionally since hydrodynamic conditions in rivers vary over time due to morphodynamic changes if sediment transport hung et al 2022 pisaturo et al 2021 and water quality wang and lin 2013 processes are considered this kind of modelling strategy would allow for the analysis of such conditions from a holistic point of view 5 conclusions currently eco hydraulic numerical tools are widely used for river habitat rehabilitation restoration and enhancement purposes these tools undergo continuous development due to the advances in data acquisition and data treatment and the use of high performance techniques to formulate more efficient robust and powerful numerical codes nevertheless 2d swe based eco hydraulic numerical tools that include an integrated gpu parallelised hydrodynamic module and physical habitat module developed for overcoming the main computational limitations are lacking r iber is a gpu parallelised hydrodynamic numerical tool that integrates a physical habitat module thus it is a fully integrated gpu parallelised eco hydraulic tool the code based on iber was developed in cuda fortran language for faster computations the hydrodynamic module of r iber was first validated using two benchmark test cases and a laboratory experiment with a fishway r iber was also applied to two real cases one following the ifim and the other simulating a 10 day real flood event the benefit of using a gpu parallelisation eco hydraulic tool instead of a cpu based tool is the significant reduction in computation time with speed ups of one or two orders of magnitude above 100 times with respect cpu computations but also a notable improvement in the efficiency of the computations when the model s number of elements increases is shown additionally the option stepped discharge demonstrated to be a suitable solution to address q wua computations following the ifim being the computations 50 faster than an in cascade simulation process this option uses the time strictly necessary to reach the steady flow conditions without having to know the time to reach the steady flow conditions for any discharge and reducing the computational time to a minimum r iber can simulate calculation domains discretised with finer calculation meshes providing high resolution numerical models with a better representation of the hydrodynamics and consequently the habitat with no performance penalties additionally it has been proven that the simulation of long river stretches even entire rivers or long term habitat analysis can be computed with gpu based numerical codes within a convenient timeframe funding the contract of the d d s is funded by the international center for numerical methods in engineering vac 2021 1 declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests danial dehghan souraki reports financial support was provided by centre internacional de mètodes numèrics en la enginyeria appendix a cuda fortran structure of r iber this appendix shows the internal structure of the new code r iber which is based on cuda fortran programming language nvidia 2022b the main structure of the code r iber is depicted in fig 1 of the main manuscript r iber code can be split in two main parts the host and the device the host relates to the central processing unit or cpu and controls the main instructions of the code variable reading allocatation saving transfer etc and directives e g loops even the execution of the graphics processing unit or gpu this last action is done through kernel loop directive kld the host part of code of r iber can be synthetized as follow image 1 loops of the original code of iber have been adapted to the new structure of the cuda fortran programming and both codes fortran and cuda fortran can be called from the host as follows image 2 a kernel may be invoked with many thread blocks each with the same thread block size the thread blocks are organized into a one two or three dimensional grid of blocks so each thread has a thread index within the block and a block index within the grid zhang and jia 2013 when invoking a kernel the first argument in the chevron syntax is the grid size and the second argument is the thread block size thread blocks must be able to execute independently two thread blocks may be executed in parallel or one after the other by the same core or by different cores image 3 
25446,eco hydraulic models are wide extended tools to assess physical habitat suitability on aquatic environments currently the application of these tools is limited to short river stretches and steady flow simulations however this limitation can be overcome with the application of a high performance computing technique graphics processing unit gpu computing r iber is a gpu based hydrodynamic code parallelised in cuda fortran that with the integration of a biological module performs as an eco hydraulic numerical tool r iber was validated and applied to real cases by using an optimised instream flow incremental methodology in long river reaches and long term simulations r iber reduces the computation time considerably reaching speed ups of two orders of magnitude compared to traditional computing r iber allows for overcoming the current limitations of the eco hydraulic tools with the simulation of high resolution numerical models calculated in a reasonable computation timeframe which provides a better representation of the hydrodynamics and the physical habitat keywords fish habitat weight useable area numerical modelling high performance computing gpgpu abbreviations swe shallow water equations hpc high performance computing gpgpu general purpose computing on graphics processing unit gpu graphics processing unit cpu central processing unit cuda compute unified device architecture ifim instream flow incremental methodology wua weight usable area data availability the authors do not have permission to share data software availability name of tool r iber developers david lópez gómez marcos sanz ramos ernest bladé year first available 2021 hardware required basic computer with a graphical power unit gpu based on cuda architecture mainly any nvidia gpu requirements windows os x64 source code availability the numerical tool is freely distributed through www iberaula com data availability the authors do not have permissions to share the data cost free program languages cuda fortran 1 introduction water resources and river management are no longer uniquely related to flood or drought scenarios on the contrary to properly study the system from a holistic point of view the relationship between hydraulics and biota must be considered benjankar et al 2018 palau and alcázar 2012 wilkes et al 2016 in all situations not only during extreme conditions eco hydraulics is a technique that analyses the effect of physical environmental properties flow depth velocity turbulence temperature substrate etc on aquatic environments bovee 1982 such as rivers to characterise the physical habitat several approaches have been proposed based on hydrologic geomorphologic or hydraulic criteria tonina and jorde 2013 particularly for hydraulics a common method for assessing physical habitat is the use of normalized univariate curves or suitability curves that relate variables of the river environment with the inhabiting species use of the space several numerical hydrodynamic tools have been developed or enhanced by integrating the biological requirements of one or several target species typically of fishes cassan et al 2022 hung et al 2022 jowett 2004 meza rodríguez et al 2019 nones 2019 sanz ramos et al 2019 shim et al 2020 stamou et al 2018 steffler and blackburn 2002 but not exclusively hamilton et al 2015 zohmann et al 2013 these tools are based on the solution of the one dimensional 1d and most recently on the two dimensional 2d saint venant equations or shallow water equations swe which provide the evolution of the water depth and the specific discharge or velocity of the water despite the current rapid increase in the use of three dimensional 3d river flow modelling bermúdez et al 2017 2018 meselhe et al 2012 pisaturo et al 2017 the computational cost of this method limits its application to very short river stretches and research purposes apart from the simplifications applied to the generation of the suitability curves boudreault et al 2022 palau et al 2016 limitations of 2d swe based eco hydraulic models typically relate to computational cost carlotto et al 2021 morales hernández et al 2021 the time required to simulate large river stretches flow properties in natural conditions e g long time series and the use of a very fine domain discretisation with thousands or millions of calculation points or mesh elements typically require high computational effort these limitations can be overcome with a good code design or and with the application of high performance computing hpc techniques currently one of the most applied techniques is the parallelisation of the computations by using general purpose computing on graphics processing unit gpgpu instead of the traditional central processing unit cpu computing buttinger kreuzhuber et al 2022 carlotto et al 2021 morales hernández et al 2021 a gpgpu technique commonly used in computational fluid dynamics cfd is that provided by the nvidia compute unified device architecture cuda which is currently available for the c and fortran programming languages nvidia 2022a the benefit of using gpu computing is the significant reduction in computation time with speed ups of one or two orders of magnitude compared to cpu computing nevertheless 2d swe based eco hydraulic numerical tools that include an integrated gpu parallelised hydrodynamic module and physical habitat module are lacking this study aims to fill this gap with the presentation of a new gpu based hydraulic code parallelised in cuda fortran named r iber and its performance as an eco hydraulic numerical tool with the integration of a physical habitat module the code was first validated using two different benchmark tests for 2d swe based models and a laboratory experiment with a vertical slot fishway then r iber was applied to two real cases to show not only the full integration of the hydrodynamic and physical habitat module for some fish species but also the benefits of using a fully integrated gpu parallelised eco hydraulic tool in high resolution numerical models and long term simulations 2 materials and methods 2 1 governing equations most existing eco hydraulic simulation tools are based on the solution of the mass and momentum conservation equations that when applied to a 2d framework are named 2d shallow water equations 2d swe they are derived from navier stokes equations through a time averaging to filtrate the turbulent fluctuations reynolds averaged navier stokes equations rans and a depth averaging to obtain the final 2d equations two dimensional swe are a hyperbolic nonlinear system of three differential equations in partial derivatives toro 2009 which can be written as follows 1 h t h u x x h u y y 0 h u x t x h u x 2 g h 2 2 y h u x u y g h s o x s f x x ν t h u x x y ν t h u y y h u x t y h u x u y y h u y 2 g h 2 2 g h s o y s f y x ν t h u y y y ν t h u y y where h is the water depth u x and u y are the x and y components of the depth averaged velocity g is the gravitational acceleration s o x z b x and s o y z b y are the spatial variation of the bed elevation z b on the x and y directions s f x n 2 u x u x 2 u y 2 h 4 3 and s o y n 2 u y u x 2 u y 2 h 4 3 are the energy dissipation in the x and y directions due to bed friction which is computed using the manning formula and ν t is the turbulent viscosity the previous system of equations are the main part of the hydrodynamics module of iber bladé et al 2014a 2014b a 2d numerical tool that was initially developed for hydrodynamics and sediment transport modelling bladé et al 2014a 2014b 2019b 2019a sanz ramos et al 2020a it solves the 2d swe on irregular geometries using the conservative scheme based on the finite volume method of roe 1986 which consists of the godunov method together with the roe approximate riemann solver toro 2009 the tool has been continuously enhanced and the sequential version currently includes a series of modules for different free surface flow processes such as hydrological processes cea and bladé 2015 sanz ramos et al 2020b 2021 pollutant propagation cea et al 2016 large wood transport ruiz villanueva et al 2014 and physical habitat suitability assessment sanz ramos et al 2019 2 2 eco hydraulics methods the combination of a hydrodynamic model and a biological model can be oriented to develop eco hydraulic tools nestler et al 2016 one of the most extended biological models to assess physical habitat particular for freshwater fishes is the use of suitability curves a normalized relation between environmental variables hydraulic water quality geomorphological etc and the use of the space of the target specie stadium the elemental habitat suitability of a physical variable ranges from 0 to 1 values being close to 1 those correspond to a high degree of suitability the multiplication of the elemental suitability depth and velocity dependent of each species stadium was used as habitat criteria in the following sections but other criteria can be used without loss of applicability the instream flow incremental methodology ifim is a technique that uses the combination of hydrodynamic and biologic models to create a habitat or eco hydraulic model it consists of the assessment of different constant discharges q in a river stretch to obtain the weight useable area wua for each one bovee 1982 with these q wua relations it is possible to obtain the theoretical discharge that maximises the wua for the species and stadiums analysed and which is usually utilized to propose the environmental flows in 2d eco hydraulic modelling the use of the ifim traditionally implies the generation of as many models as the discharges to be evaluated being an analysis conducted under steady flow conditions 2d swe based models usually require simulating the model from the beginning to an unknown maximum time of simulation additionally the time required to achieve each steady state of the series of discharges to evaluate is in general unknown this time depends not only on the geometry and length of the river but also on the flow intensity this issue is addressed with the definition of a series of constant discharges or stepped discharges as an inlet condition the stepped discharge option allows for computing in a unique model a series of discharges that generates steady conditions considering a tolerance between the inlet and the outlet when the steady conditions of the target discharge are achieved the model automatically steps to the next discharge the hydrodynamic conditions at the end of a step are used as initial conditions for the next step thus this option uses the time strictly necessary to reach the steady flow conditions of any of the constant discharges defined by the user without having to know the time to reach the steady flow conditions for any discharge and reducing the computational time to a minimum 2 3 cuda fortran code r iber the sequential version of iber is partially parallelised by means of the open multi processing openmp technique however the intrinsic limitations of this technique only allow for speed ups corresponding to the number of cpu cores at most despite the computing time of the hydrodynamics being reduced notably the simulation of long stretches of rivers even entire rivers or long time series for fish habitat assessment procedures remains unapproachable with techniques based solely on cpu computing the integration of several calculation modules in the fortran based code of iber was originally functionality oriented prioritising the full integration of all modules instead of multi core processing efficiency due to the growing application of more complex and detailed simulations in river hydrodynamics demands an increase in computational capacity and thus a code re ordering and optimization was already made in previous versions of iber despite that the simulation of models of thousands or millions of elements or long time series requires several hours or even days of computational time with the sequential version r iber is a new code that implements a gpu parallelisation of the hydrodynamic bladé et al 2014b and habitat sanz ramos et al 2019 modules of iber based on cuda fortran architecture an efficient cuda programming requires not only a hierarchical organisation of the computing units but also the programming of each thread block to efficiently process data in parallel zhang and jia 2013 to achieve this the original code of iber was re organised and re written to follow the main structure of cuda programming that is transfer data from cpu memory host to gpu memory device run parallel computing through kernels and transfer data back to the host from the device more details of the structure of the code are described in appendix a the main limitation of gpu parallelised codes is the memory access and especially the memory transfer garcía feal et al 2018 vacondio et al 2014 zhang and jia 2013 r iber reads transfers and writes memory data only when it is strictly necessary with the aim of maximising the code s efficiency another issue which also occurs in cpu computing is that of the multi core computing simulations running several simulations at the same time in the same processor unit creates a significant bottleneck particularly for gpu computing and reduces the global computing capacity of the device morales hernández et al 2021 to minimise heavy performance penalties r iber allows for the selection of the gpu device on which to carry out each calculation thus the option of a multi processor computation on a single computer with several gpus or on a cluster of gpus is available to practitioners fig 1 presents the main workflow of the r iber code additionally to achieve maximum performance on gpu processors on arithmetic operations it is necessary to use as many single precision arithmetic operations as possible hwu et al 2009 as it is done in r iber using single precision operations implies round off errors in the calculation results i e after a single floating point operation the result is rounded after the seventh digit however cpu based simulations perform double precision arithmetic operations as it is done in iber with similar performance to single precision narumi et al 2011 on the other hand the habitat module of iber and also of r iber is computed as a post process because the currently implemented fish suitability curves only depend on the hydrodynamics thus for optimum memory access transfer the information exchanged between hydrodynamics and habitat must be done directly in the host and only when the results have to be written this moment may occur when the simulation time is greater than the user defined time to write the results or when the steady flow conditions are accomplished if the stepped discharge option is used additionally the process of writing the results utilises an independent cpu core fig 1 therefore once all data is transferred back to the host the habitat module is computed in the cpu while the gpu continues calculating without any performance penalty it is worth noting that 2d swe based models do not solve the energy equation which in 1d only depends on the energy balance between two consecutive sections in 2d models a steady state is reached when the inlet discharge is equal to the outlet discharge and thus the whole flood front propagation process must be solved anyway this implies an increase in the computational time because the simulation time needed to reach steady conditions in general is unknown therefore longer simulation times are required to ensure a steady state this particular issue is solved with a specific option called stepped discharge which allows for the simulation of steady flow conditions for several discharges in the same model and directly provides flow habitat relationships such as the wua with this option enabled gpu parallel computation would potentially reach its maximum calculation capacity as the results would only be written a few times at each step 2 4 hardware and characteristics simulations were carried out with iber v3 1 which contains the new code of r iber and are freely distributed through www iberaula com both algorithms use the roe 1st order numerical scheme computations with the sequential version cpu were done with a cpu intel core i7 9750h while computations on the cuda fortran gpu version r iber were launch with the same cpu but computed in different gpu devices table 1 the nvidia gpus selected were the geforce gtx and rtx series which being quite common devices for standard desktop computers and mid range to high end gaming laptops cover a wide temporal range 3 validation 3 1 test 1 flooding of disconnected floodplains the first validation test was the so called test 1 proposed by the united kingdom environment agency within the defra flood and coastal erosion risk management research and development programme néelz and pender 2013 this test aims to assess the capability of the 2d hydrodynamic models to reproduce the flooding of disconnected floodplains and the wetting and drying processes the domain consisted of a 700 100 m channel with a bottom elevation composed of an increasing slope 0 02 followed by a decreasing slope 0 25 and ending in an increasing slope 0 3 thus a depressed area was generated where the water would be retained as boundary condition a water level was imposed varying from 9 7 to 10 35 m in 1 h keeping a constant value for 8 h to ensure the depressed area was completely filled then returning to 9 7 m in 1 h two calculation scenarios were performed one following the proposed domain discretisation with 1125 mesh elements and the other using the full digital elevation model dem resolution which provides 26 341 calculation points simulations were also done with the cuda c gpu code of iber called iber garcía feal et al 2018 fig 2 shows the water evolution at the two control points p1 400 50 and p2 600 50 the three numerical models show the same hydraulic behaviour water accumulated in the depressed area at an elevation of 10 25 m flush with the topography the computational times of the reference model iber v3 1 sequential were 44 and 2064 s for the proposed discretisation and the full dem models respectively the speed ups of r iber reached 3 4 and 39 4 times while iber reached 2 8 and 40 5 times respectively for the gtx 1660 ti similar values were obtained for the gtx 980 ti and the rtx 3070 devices table 2 it is worth noting that for the proposed discretisation in which only 1125 elements were used the speed up was not significant because the parallelised part of the code was faster than the time required to transfer information from the device to the host and to write the results file this bottleneck is practically neglected when the number of elements increases 3 2 test 2 hydraulic jumps and flow obstructions the second validation case is the test 6a of defra s benchmark néelz and pender 2013 this case aims to verify the capability of 2d swe based models to simulate transcritical flow regimes hydraulic jumps and wakes behind obstructions for that purpose the experiment of soares frazão and zech 2007 was simulated in iber and r iber using two different mesh discretisation of rectangular elements that proposed by defra s benchmark 56 616 elements and the full dem resolution 226 352 elements an initial water elevation of 0 4 m was imposed from the beginning of the flume to the narrowing of it upstream of the obstacle this configuration tries to reproduce a dam break process in the downstream area where the obstacle is placed non symmetrically on a wet area in the flume results were extracted each 0 1 s considering a fixed time step of 0 005 s six control points monitored the water depth and velocity during the experiment g1 to g6 all numerical approaches behaved similarly fig 3 shows the evolution of the water depth and velocity at points g1 g2 and g3 the arrival time of the flood front was the same and suitably captured both the supercritical and subcritical flow regimes at point g2 the presence of the obstacle generated a mobile hydraulic jump upstream at around 12 s where water depth evolved from 0 03 to 0 1 m during the 10 s after the shock wave in the case of the proposed mesh discretisation the hydraulic jump was produced at 9 4 s yellow and grey dashed line while with the full dem the hydraulic jump was produced at 16 3 s for iber black dashed line and 11 s for r iber red line iber generated a weak hydraulic jump at 11 s which then transformed into a strong hydraulic jump a few seconds later 16 3 s this difference in this particular case of the simulation of a hydraulic jump with a very fine discretisation of the calculation domain grid of 0 05 0 05 m can be attributed to the fact that r iber computes in single precision while iber does it in double precision thus after a single floating point operation the result is rounded after the seventh digit in r iber in general the reproduction of a hydraulic jump is quite complex to reach with full precision particularly in 2d swe based modelling when the hydraulic jump evolves along time due to the discharge is not constant a sharper solution is generally obtained néelz and pender 2013 despite that the flow patterns were well captured by r iber even using single precision and a roe 1st order numerical scheme cea et al 2010 the numerical simulation in the sequential code iber spanned 172 s for the proposed discretisation and 706 s for the full dem model while the parallelised r iber code computed the models in 35 45 s and from 130 to 197 s respectively this means an average speed up of around 3 6 5 4 times with respect of the sequential code table 3 it is worth noting that the necessity of writing results at very small time steps 0 1 s considerably reduces the global simulation time if only computational time is considered depending on the gpu used the speed up of r iber reaches values from 65 4 to 75 3 and from 96 8 to 140 4 times for the proposed and full dem models respectively table 3 values in brackets 3 3 test 3 fishway and recirculation areas test 3 is the numerical reproduction of the laboratory experiment presented by puertas et al 2004 which consisted of the hydraulic characterisation of a vertical slot fishway the original flume device was a 12 m long channel divided into 11 pools four for the design of test 1 and four for the test 2 design while the rest were the upper intermediate and lower pools pool 7 which belonged to the test 1 design fig 4 a was analysed considering half of the flume a 5 7 slope and a discharge of 0 0741 m3 s a detailed description of the flume experiments can be found in puertas et al 2004 previous numerical experiments confirm the capability of 2d swe based models for modelling the free surface flow in vertical slot fishways bermúdez et al 2010 cea et al 2007 puertas et al 2012 thus this validation case aimed to reproduce main jet and particular recirculation areas flow patterns fig 4b by considering four mesh discretisation 2751 m1 5502 m2 22 008 m3 and 88 032 m4 elements in general a mesh size reduction implies a better representation of the hydrodynamics and consequently the eco hydraulic assessment however higher computational times are required the numerical discretisation of 2751 elements m1 provided smooth results with only a weak recirculation area on the right side downstream of the pool inlet and a considerably straight main jet fig 5 a this recirculation area increased as the number of elements doubled m2 and another two weak eddies appeared on the upstream and downstream corners on the left side between the baffle and the wall fig 5b a computational mesh of eight times more elements m3 than the first mesh m1 allowed for the proper definition of two separately recirculation areas on both sides of the main jet fig 5c this domain discretisation effectively defined the general flow pattern shown in fig 4b with a well defined main jet in the model with a mesh of 88 032 elements m4 the flow patterns became more complex representing small eddies near the bigger eddies to both sides of the main jet fig 5d numerical results were compared to the experimental results throughout the mean depth of the middle traverse section h 0 the mean flow depth of the pool h m the maximum depth of the pool h max the minimum depth of the pool h min the depth of the slot h s the absolute velocity of the slot v s the discharge coefficient of the slot c d q b h s v s where b is the slot width and the energy dissipation rate in the pool e d ρ g q s 0 b h 0 where ρ is the flow density g is the gravitational acceleration q is the flow discharge s 0 is the bottom slope and b is the pool width table 4 summarises the experimental and numerical results obtained with each numerical mesh in general r iber performed suitably for all the simulations particularly for the finer meshes table 5 which provided a good fit for the observations reproducing the flow patterns such as eddies and the shape and dimension of the main jet the m1 mesh 2751 elements showed the poorest results with absolute relative errors slightly above 10 in some hydraulic parameters h 0 h m and e d by contrast the m3 and m4 meshes 22 008 and 88 032 elements respectively provided suitable results with errors below 3 5 numerical results of the discharge coefficient c d and the energy dissipation rate e d also adjusted suitably to the observations with a relative error less than 3 5 for m3 and m4 meshes the benefit of using the cuda fortran gpu parallelisation r iber instead of iber is the considerable reduction in computational time table 6 in this particular case r iber reached a speed up from 23 9 gtx 980 ti to 45 1 times rtx 3070 for mesh m4 in comparison with the sequential version additionally the efficiency of the code can be evaluated by accounting for the computational cost per number of elements the computational time required by iber increased from 24 7 m1 to 106 6 s m4 per 1000 elements while in the case of r iber it decreased from 5 to 2 9 s thus it can be seen that the new parallelised code of iber r iber provides not only a valuable computational cost reduction but also a notable improvement in the efficiency of the computations when the number of elements of the model increases 4 study cases the performance of the model has been tested in two real study cases eume river and cinca river fig 6 the eume case study is focused on showing the benefits of using not only the gpu parallelised eco hydraulic model of iber r iber but also the noticeable improvement of using the stepped discharge inlet condition instead of creating a separate model for each discharge an intrinsic benefit of the gpu parallelisation of 2d swe based models is the capability to assess long term hydrodynamics in long rivers in a reasonable timeframe if a fish habitat module is integrated into the code such as in r iber the eco hydraulic evaluation of rivers will no longer be limited to a few metres or short river stretches to demonstrate this the capacity of r iber to simulate long term fish suitability variations in long river stretches even entire rivers is presented in the case study of the cinca river 4 1 improved ifim for 2d swe based models eume river the analysed stretch of the eume river located in the north west of spain fig 6a is characterised by two well differentiated geomorphological facies a discontinuous channel with jumps and pools upstream and a continuous channel downstream all discharges were evaluated with iber and r iber considering a separate model for each discharge and also using the stepped discharge option which consisted of a unique model in this case the tolerance was fixed at 1 a new step of the series of inlet discharges was automatically imposed when the absolute value of the outlet discharge minus the inlet discharge all divided by the inlet discharge was less than the tolerance the study area of 3 km in length fig 6b was discretised into 72 426 calculation elements twelve discharges were evaluated ranging from 0 149 to 11 770 m3 s salmo trutta was the target specie and the depth and velocity dependent suitability curves described in che and aca 2008 were used as biological information a detailed description of the study area can be found in sanz ramos et al 2019 as expected the wua depended on the hydrodynamics of each evaluated flowing discharge fig 7 shows in the transition zone between two geomorphological facies fig 6b hollow black square the elemental suitability evaluated for different discharges and the adult stadium product of the depth and velocity suitability low discharges produced a narrow wet area particularly at the facies characterised by jumps and pools fig 7a and b the most fish suitability was obtained in zones with high depths pools when the discharge increased fig 7c the discontinuous channel became continuous the velocity increased and the suitability of this area decreased this is clearly seen in the lower part of the river where a moderate depth and low velocity area provided a high suitability zone however high discharges 11 77 m3 s modified the hydrodynamics in this area reducing the suitability of the central part of the channel and displacing it to the lateral sides fig 7d the resulting wua is reduced for all flows and stadiums of the target specie remaining below 14 of the total wetted area the maximum suitability conditions were achieved in the fry for a discharge of 0 560 m3 s 12 78 of the wetted area and in the juvenile and adult stadium for 2 176 m3 s fry 8 8 adult 6 2 of the wetted area identical hydrodynamic and habitat results were obtained with iber and r iber but with significant differences in the computational time table 7 the simulation process of computing each discharge per model required between 195 and 357 s in the sequential mode cpu while the gpu version required less than 30 s for each simulation fig 8 a accounting for the total computational time of all the models a global time of 2844 s was necessary for the cpu version this time was reduced to 207 s for the gtx 980 ti 189 s for the gtx 1600 ti and 124 s for the rtx 3070 fig 8a the benefits of using the stepped discharge option instead of a separate model for each discharge was evaluated as the quotient between the addition of the time of each model and the time of the model that used the stepped discharge option the advantage of using the stepped discharge option instead of a separate model for each discharge is proved by a 16 6 acceleration in the computation time in cpu based simulations fig 8b when gpu calculations were applied speed ups of 32 52 were achieved fig 8b using the rtx 3070 gpu did not reduce the computation time as much as possible due to the model s low number of elements however when a higher number of elements discretise the domain the advantage of using a more powerful gpu clearly shows its potentialities as shown in section 4 2 it is worth noting that only the computation time was considered in the previous comparison the time needed for changing the boundary condition was not taken into consideration if a constant time of 30 s for each simulated discharge is considered as the necessary time for changing the boundary conditions and re starting the simulation a global speed up of around 350 would be reached for the gpu computations of the stepped discharge option 4 2 long term simulations in long rivers stretches cinca river cinca is a 191 km long mountain river that begins in the central part of the south of the pyrenees and drains into the ebro river in spain fig 6a the analysed stretch of the cinca river is 11 km long starting 2 5 km upstream of the bellós river junction and ending at the ara river junction fig 6c this stretch is characterised as a braided river with a considerably active morphology béjar et al 2018 vericat et al 2017 the flood event of 9 october 2014 which had two peak discharges and spanned 10 days che 2021 was simulated with three different meshes of 74k elements m1 299k elements m2 and 1885k elements m3 the river domain was discretised using an irregular mesh of triangular elements with 220 m1 887 m2 and 5577 m3 elements per hectare the last one being one order of magnitude above the common values used in flood studies sanz ramos et al 2020a the mesh was updated with the 2 2 m dem provided by national geographic institute of spain ign 2022 during these 10 days where discharges ranged from 10 base flow to 250 m3 s maximum peak discharge variations in the fish habitat suitability were evaluated for salmonids salmo trutta che and aca 2008 and cyprinids chondrostoma polylepis and barbus bocagei martinez 2000 these target species were selected within the cinca s basin characteristics che 2002 in contrast with the ifim the main result of this analysis is the evolution of the wua over time besides the evolution of the fish suitability distribution fig 9 a presents the evolution of the wua in for the three abovementioned species and stadiums fry juvenile and adult and the m2 domain discretisation changes in the hydrological regime also modify the wua as expected low discharges favoured a more suitable area for fry and juvenile stadiums despite high discharges generating high depths and velocities in general other areas flooded provided more suitable zones for the chondrostoma polylepis black lines and barbus bocagei green lines while the wua of the salmo trutta red lines decreased fig 9b exemplifies the variation in the wua evolution for adult continuous line juvenile dashed line and fry dotted line of the species barbus bocagei when the models of 220 m1 black line 887 m2 green line and 5577 m3 red line elements per hectare were considered as has also been demonstrated in sections 3 2 and 3 3 a finer discretisation of the study area provides hydrodynamic results of a higher resolution which are the bases of wua assessment in general the m2 mesh provided a mean increment of 7 9 of the wua in comparison to the m1 mesh while the m3 mesh reached up to 18 1 more wua than the m1 mesh in terms of computational time the sequential code iber required 7 4 h for the mesh m1 71 3 h 3 days for the mesh m2 and 1081 3 h 45 days for the mesh m3 for the m1 m2 and m3 mesh configurations r iber reached speed ups from 19 8 to 183 1 times respectively table 8 the efficiency of the code can be evaluated by the computation time required per 1000 elements the time required by iber increased when the number of elements increased from 356 to 2062 s while in the case of r iber the time remained almost constant at around 10 20 s per 1000 elements a similar trend was obtained in the validation tests complementary to the traditionally used ifim the assessment of the fish habitat suitability considering real discharge scenarios even in real time can be a step forward in eco hydraulics as it is not limited to steady conditions particular constant discharges furthermore it also encourages the study of the convenience or inconvenience of reintroducing native species and the potential prevalence of invasive species to the natural or induced hydrologic regime of a river additionally since hydrodynamic conditions in rivers vary over time due to morphodynamic changes if sediment transport hung et al 2022 pisaturo et al 2021 and water quality wang and lin 2013 processes are considered this kind of modelling strategy would allow for the analysis of such conditions from a holistic point of view 5 conclusions currently eco hydraulic numerical tools are widely used for river habitat rehabilitation restoration and enhancement purposes these tools undergo continuous development due to the advances in data acquisition and data treatment and the use of high performance techniques to formulate more efficient robust and powerful numerical codes nevertheless 2d swe based eco hydraulic numerical tools that include an integrated gpu parallelised hydrodynamic module and physical habitat module developed for overcoming the main computational limitations are lacking r iber is a gpu parallelised hydrodynamic numerical tool that integrates a physical habitat module thus it is a fully integrated gpu parallelised eco hydraulic tool the code based on iber was developed in cuda fortran language for faster computations the hydrodynamic module of r iber was first validated using two benchmark test cases and a laboratory experiment with a fishway r iber was also applied to two real cases one following the ifim and the other simulating a 10 day real flood event the benefit of using a gpu parallelisation eco hydraulic tool instead of a cpu based tool is the significant reduction in computation time with speed ups of one or two orders of magnitude above 100 times with respect cpu computations but also a notable improvement in the efficiency of the computations when the model s number of elements increases is shown additionally the option stepped discharge demonstrated to be a suitable solution to address q wua computations following the ifim being the computations 50 faster than an in cascade simulation process this option uses the time strictly necessary to reach the steady flow conditions without having to know the time to reach the steady flow conditions for any discharge and reducing the computational time to a minimum r iber can simulate calculation domains discretised with finer calculation meshes providing high resolution numerical models with a better representation of the hydrodynamics and consequently the habitat with no performance penalties additionally it has been proven that the simulation of long river stretches even entire rivers or long term habitat analysis can be computed with gpu based numerical codes within a convenient timeframe funding the contract of the d d s is funded by the international center for numerical methods in engineering vac 2021 1 declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests danial dehghan souraki reports financial support was provided by centre internacional de mètodes numèrics en la enginyeria appendix a cuda fortran structure of r iber this appendix shows the internal structure of the new code r iber which is based on cuda fortran programming language nvidia 2022b the main structure of the code r iber is depicted in fig 1 of the main manuscript r iber code can be split in two main parts the host and the device the host relates to the central processing unit or cpu and controls the main instructions of the code variable reading allocatation saving transfer etc and directives e g loops even the execution of the graphics processing unit or gpu this last action is done through kernel loop directive kld the host part of code of r iber can be synthetized as follow image 1 loops of the original code of iber have been adapted to the new structure of the cuda fortran programming and both codes fortran and cuda fortran can be called from the host as follows image 2 a kernel may be invoked with many thread blocks each with the same thread block size the thread blocks are organized into a one two or three dimensional grid of blocks so each thread has a thread index within the block and a block index within the grid zhang and jia 2013 when invoking a kernel the first argument in the chevron syntax is the grid size and the second argument is the thread block size thread blocks must be able to execute independently two thread blocks may be executed in parallel or one after the other by the same core or by different cores image 3 
25447,in recent decades there has been an increasing demand in environmental sciences for harmonized climatic data at large spatial scales and long periods here we present easyclimate a software package to obtain daily climatic data at high resolution 0 0083 1 km with r the package facilitates the downloading and processing of precipitation minimum and maximum temperatures for europe from 1950 to 2020 using easyclimate and given a set of coordinates points or polygons and dates days or years the user can download the climatic information as a tidy table or a raster object in this package we implemented cloud optimized geotiffs which provide access to daily climate data for thousands of sites days without having to download huge rasters daily climate data are not available in many of the current climate databases and are essential for many research questions and applications in environmental modelling forestry and ecological and vegetation studies across europe keywords r package climate europe cloud optimized geotiff daily data reproducibility data availability data and scripts are publicly available 1 introduction in recent decades there has been an increasing demand for harmonized daily gridded climatic data at wide spatial scales and spanning long temporal periods such data is invaluable for vegetation wildlife climatic and hydrological studies and earth system modelling hasenauer et al 2003 thornton et al 2021 examples are the assessment of climate effects and climate change impacts on european forests archambeau et al 2020 george et al 2021 hlásny et al 2017 moreno et al 2018 neumann et al 2017 ruiz benito et al 2020 the initialization of large scale carbon cycle models pietsch and hasenauer 2006 the spatial temporal variability of rainfall erosivity micić ponjiger et al 2021 or the creation of a european net primary production dataset neumann et al 2016 plant distribution as well as plant growth phenology respiration and plant mortality are strongly driven by weather conditions e g kunstler et al 2021 any aggregation of climate data to average monthly or annual numbers may hide important climate effects on plants specifically if we expect changing environmental conditions in this sense daily climate data are of interest for many ecological research questions and applications including the study of the effects of late spring frosts zohner et al 2020 heat waves or dry periods on plant performance cruz alonso et al 2020 however accessing and processing such daily climate data is often cumbersome cáceres et al 2018 even more if harmonized data are required at large spatial scales and instead researchers use monthly or annually averaged climate data here we present easyclimate cruz alonso et al 2021 a software package available from github https github com verughub easyclimate to download and process climate data with r r core team 2022 easyclimate has been developed to facilitate the use of high resolution 0 0083 0 0083 1 km2 daily climate data for europe 24 5 w 45 25 e 25 25 n 75 5 n fig 1 daily precipitation and minimum and maximum temperature data are currently available from 1950 to 2020 and hosted at university of natural resources and life sciences vienna austria the climatic dataset was originally produced by moreno and hasenauer 2016 for the production the coarse daily e obs climate data cornes et al 2018 was downscaled by using the finer resolution worldclim data fick and hijmans 2017 e obs provides gridded daily climate data for europe at 0 25 resolution approximately 30 km by interpolating around 3700 weather stations for temperature and around 9000 stations for precipitation and worldclim provides global long term monthly averages of several climatic variables at 0 0083 resolution approximately 1 km downscaling was performed by applying a spatial delta method with a monotone cubic interpolation of anomalies moreno and hasenauer 2016 mosier et al 2014 the delta method combines climate data sets with differing spatio temporal resolutions to produce a new climate data set with a desired spatio temporal resolution moreno and hasenauer 2016 the downscaling involves several steps fig 2 i the worldclim data is upscaled to the 0 25 e obs resolution ii the difference between this upscaled worldclim and the e obs cell is calculated iii for each 0 0083 cell in the original worldclim data its value is retrieved and the corresponding location is marked in the difference cell iv the weighted difference between the selected 0 25 cell and its closest three adjacent cells is calculated v the final downscaled value is calculated using the original worldclim value retrieved earlier and the summed inverse distance weighted difference value moreno and hasenauer 2016 the calculation of the downscaled value in the last step varies depending on whether temperature or precipitation are downscaled see moreno and hasenauer 2016 for a more detailed description of the downscaling procedure evaluation and validation of the downscaled climate data were performed by comparing weather station data used for the original e obs data the e obs data and the downscaled data the comparison was performed by calculating the mean minimum and maximum values for all three data sets and error metrics for the e obs and downscaled data moreno and hasenauer 2016 additionally validation of the e obs and downscaled data was performed using the same statistics but with independent data from 430 austrian weather stations which were not used to create the original e obs data the validation showed that for these points the downscaling improved the accuracy of the climatic variables compared to the original e obs data moreno and hasenauer 2016 since its original release the downscaled climate data set has been further developed and updated and two releases v2 and v3 have been published for a review of the main changes see rammer et al 2022 and pucher and neumann 2022 the easyclimate r package enables easy and fast access to the latest version of the downscaled climate data v3 we achieved this by exploiting gdal gdal ogr contributors 2022 support for cloud optimized geotiffs https www cogeo org which provide access to daily climate data for thousands of sites and days within minutes without having to download huge rasters 2 functionality the main function in easyclimate is get daily climate which extracts daily climate data for a given set of coordinates points or polygons and a given period of days or years see examples in get daily climate help page and the vignettes analysing the climate at spatial points for a given period and analysing the climate of an area for a given period the output can be either a data frame or a multilayer spatraster object terra class hijmans 2022 with daily climatic values for each point or polygon as an api application programming interface by design easyclimate yields tidy datasets wickham 2014 that facilitate calculation of alternative climatic variables and indices following the tidyverse philosophy wickham et al 2019 also the results of the package easyclimate can be used directly or serve as input to calculate climatic indices with other packages such as climind reig gracia et al 2021 or spei beguería and vicente serrano 2017 see some examples in the vignette calculating basic climatic indices with data from easyclimate furthermore easyclimate might be integrated in other software providing environmental variables e g geodata hijmans et al 2021 3 case studies two case studies have been selected to demonstrate the core utilities of easyclimate and visualize the two types of data outputs i e data frame and spatraster objects in addition we show a third case where climatic variables are easily calculated based on those outputs 3 1 getting tidy datasets of climatic values get daily climate is called to obtain precipitation data for a single site between 1st and 3rd of january 2001 table 1 image 1 3 2 getting rasters of climatic values get daily climate is called to obtain a multilayer raster with values of minimum temperature since may 1st 2020 to may 10th 2020 for a region tirol austria delimited by a polygon fig 3 image 2 3 3 calculations based on easyclimate data in the next example we download daily climatic data precipitation minimum and maximum temperature for a five year period for a specific location and we store the data in a data frame then we calculate the mean temperature as the average between minimum and maximum temperature image 3 to calculate average temperatures and aggregated precipitation by site or time period table 2 we can use group by and summarise from dplyr or by and aggregate from base r image 4 4 discussion although the entire downscaled climatic data is available for downloading as geotiff raster layers in a public ftp server ftp palantir boku ac at public climatedata for small to moderately sized areas e g less than 10 000 sites or 10 000 km2 the cloud optimized geotiff technology implemented in easyclimate allows to efficiently extract the data and saves significant time furthermore with easyclimate we avoid downloading large rasters several gb for each year requiring storage space on local or remote servers energy and resources hilty and aebischer 2015 hischier et al 2015 in this sense easyclimate becomes even more efficient if we are interested in climate data for multiple years and a small number of sites for querying climate data from large areas it is recommended to download the raster layers and extract the data to local storage e g using the extract function from terra r package hijmans 2022 to avoid overloading the ftp server as a test comparing the two methodologies i e using easyclimate vs raster downloading and local extraction we downloaded daily precipitation data for one year in an area of ca 100 km2 while the local download and extraction took 9 10 min in a laptop with good internet connection 10 mb s cooper 2022 and stored 5960 mb easyclimate took 17 s to obtain the same data storing only the final dataset 2 3 mb image 5 5 summary and conclusions this paper presents the r package easyclimate which facilitates access to high resolution daily temperature and precipitation data for europe for the period 1950 to 2020 the package enables downloading two types of data outputs i e tidy tables and rasters this climatic information is available by direct download from a ftp server but the use of easyclimate can save time of downloading processing and storage resources author contributions conceptualization v c a p r b and f r s data curation c p and m n formal analysis v c a and f r s funding acquisition m n and h h investigation c p and m n methodology v c a s r p r b j a and f r s project administration v c a resources c p m n and h h software v c a s r and f r s supervision v c a p r b and f r s validation v c a p r b j a and f r s visualization v c a p r b and j a writing original draft v c a and f r s writing review editing v c a c p s r p r b j a m n h h and f r s declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we acknowledge the support by the university of natural resources and life sciences vienna austria vca was supported by the real colegio complutense postdoctoral fellowship 2020 cp mn and hh acknowledge the financial support by the bio based industries joint undertaking under the european union s horizon 2020 research and innovation program tech4effect techniques and technologies for effective wood procurement project grant number 720757 prb was supported by the community of madrid region and the universidad de alcalá stimulus to excellence for permanent university professors epu inv 2020 010 ayudas para la realización de proyectos para potenciar la creación y consolidación de grupos de investigación cg20 cc 005 ja was supported by the fpi fellowship of the department of education of the basque government vca prb and ja also acknowledge the financial support by ministerio de ciencia e innovación subproject large nº pid 2021 123675ob c41 frs was supported by the vi plan propio de investigación of universidad de sevilla vi ppit us by ministerio de ciencia e innovación through european regional development fund sumhal lifewatch 2019 09 csic 13 pope 2014 2020 and by feder 2014 2020 and consejería de economía conocimiento empresas y universidad of junta de andalucía grant us 1381388 
25447,in recent decades there has been an increasing demand in environmental sciences for harmonized climatic data at large spatial scales and long periods here we present easyclimate a software package to obtain daily climatic data at high resolution 0 0083 1 km with r the package facilitates the downloading and processing of precipitation minimum and maximum temperatures for europe from 1950 to 2020 using easyclimate and given a set of coordinates points or polygons and dates days or years the user can download the climatic information as a tidy table or a raster object in this package we implemented cloud optimized geotiffs which provide access to daily climate data for thousands of sites days without having to download huge rasters daily climate data are not available in many of the current climate databases and are essential for many research questions and applications in environmental modelling forestry and ecological and vegetation studies across europe keywords r package climate europe cloud optimized geotiff daily data reproducibility data availability data and scripts are publicly available 1 introduction in recent decades there has been an increasing demand for harmonized daily gridded climatic data at wide spatial scales and spanning long temporal periods such data is invaluable for vegetation wildlife climatic and hydrological studies and earth system modelling hasenauer et al 2003 thornton et al 2021 examples are the assessment of climate effects and climate change impacts on european forests archambeau et al 2020 george et al 2021 hlásny et al 2017 moreno et al 2018 neumann et al 2017 ruiz benito et al 2020 the initialization of large scale carbon cycle models pietsch and hasenauer 2006 the spatial temporal variability of rainfall erosivity micić ponjiger et al 2021 or the creation of a european net primary production dataset neumann et al 2016 plant distribution as well as plant growth phenology respiration and plant mortality are strongly driven by weather conditions e g kunstler et al 2021 any aggregation of climate data to average monthly or annual numbers may hide important climate effects on plants specifically if we expect changing environmental conditions in this sense daily climate data are of interest for many ecological research questions and applications including the study of the effects of late spring frosts zohner et al 2020 heat waves or dry periods on plant performance cruz alonso et al 2020 however accessing and processing such daily climate data is often cumbersome cáceres et al 2018 even more if harmonized data are required at large spatial scales and instead researchers use monthly or annually averaged climate data here we present easyclimate cruz alonso et al 2021 a software package available from github https github com verughub easyclimate to download and process climate data with r r core team 2022 easyclimate has been developed to facilitate the use of high resolution 0 0083 0 0083 1 km2 daily climate data for europe 24 5 w 45 25 e 25 25 n 75 5 n fig 1 daily precipitation and minimum and maximum temperature data are currently available from 1950 to 2020 and hosted at university of natural resources and life sciences vienna austria the climatic dataset was originally produced by moreno and hasenauer 2016 for the production the coarse daily e obs climate data cornes et al 2018 was downscaled by using the finer resolution worldclim data fick and hijmans 2017 e obs provides gridded daily climate data for europe at 0 25 resolution approximately 30 km by interpolating around 3700 weather stations for temperature and around 9000 stations for precipitation and worldclim provides global long term monthly averages of several climatic variables at 0 0083 resolution approximately 1 km downscaling was performed by applying a spatial delta method with a monotone cubic interpolation of anomalies moreno and hasenauer 2016 mosier et al 2014 the delta method combines climate data sets with differing spatio temporal resolutions to produce a new climate data set with a desired spatio temporal resolution moreno and hasenauer 2016 the downscaling involves several steps fig 2 i the worldclim data is upscaled to the 0 25 e obs resolution ii the difference between this upscaled worldclim and the e obs cell is calculated iii for each 0 0083 cell in the original worldclim data its value is retrieved and the corresponding location is marked in the difference cell iv the weighted difference between the selected 0 25 cell and its closest three adjacent cells is calculated v the final downscaled value is calculated using the original worldclim value retrieved earlier and the summed inverse distance weighted difference value moreno and hasenauer 2016 the calculation of the downscaled value in the last step varies depending on whether temperature or precipitation are downscaled see moreno and hasenauer 2016 for a more detailed description of the downscaling procedure evaluation and validation of the downscaled climate data were performed by comparing weather station data used for the original e obs data the e obs data and the downscaled data the comparison was performed by calculating the mean minimum and maximum values for all three data sets and error metrics for the e obs and downscaled data moreno and hasenauer 2016 additionally validation of the e obs and downscaled data was performed using the same statistics but with independent data from 430 austrian weather stations which were not used to create the original e obs data the validation showed that for these points the downscaling improved the accuracy of the climatic variables compared to the original e obs data moreno and hasenauer 2016 since its original release the downscaled climate data set has been further developed and updated and two releases v2 and v3 have been published for a review of the main changes see rammer et al 2022 and pucher and neumann 2022 the easyclimate r package enables easy and fast access to the latest version of the downscaled climate data v3 we achieved this by exploiting gdal gdal ogr contributors 2022 support for cloud optimized geotiffs https www cogeo org which provide access to daily climate data for thousands of sites and days within minutes without having to download huge rasters 2 functionality the main function in easyclimate is get daily climate which extracts daily climate data for a given set of coordinates points or polygons and a given period of days or years see examples in get daily climate help page and the vignettes analysing the climate at spatial points for a given period and analysing the climate of an area for a given period the output can be either a data frame or a multilayer spatraster object terra class hijmans 2022 with daily climatic values for each point or polygon as an api application programming interface by design easyclimate yields tidy datasets wickham 2014 that facilitate calculation of alternative climatic variables and indices following the tidyverse philosophy wickham et al 2019 also the results of the package easyclimate can be used directly or serve as input to calculate climatic indices with other packages such as climind reig gracia et al 2021 or spei beguería and vicente serrano 2017 see some examples in the vignette calculating basic climatic indices with data from easyclimate furthermore easyclimate might be integrated in other software providing environmental variables e g geodata hijmans et al 2021 3 case studies two case studies have been selected to demonstrate the core utilities of easyclimate and visualize the two types of data outputs i e data frame and spatraster objects in addition we show a third case where climatic variables are easily calculated based on those outputs 3 1 getting tidy datasets of climatic values get daily climate is called to obtain precipitation data for a single site between 1st and 3rd of january 2001 table 1 image 1 3 2 getting rasters of climatic values get daily climate is called to obtain a multilayer raster with values of minimum temperature since may 1st 2020 to may 10th 2020 for a region tirol austria delimited by a polygon fig 3 image 2 3 3 calculations based on easyclimate data in the next example we download daily climatic data precipitation minimum and maximum temperature for a five year period for a specific location and we store the data in a data frame then we calculate the mean temperature as the average between minimum and maximum temperature image 3 to calculate average temperatures and aggregated precipitation by site or time period table 2 we can use group by and summarise from dplyr or by and aggregate from base r image 4 4 discussion although the entire downscaled climatic data is available for downloading as geotiff raster layers in a public ftp server ftp palantir boku ac at public climatedata for small to moderately sized areas e g less than 10 000 sites or 10 000 km2 the cloud optimized geotiff technology implemented in easyclimate allows to efficiently extract the data and saves significant time furthermore with easyclimate we avoid downloading large rasters several gb for each year requiring storage space on local or remote servers energy and resources hilty and aebischer 2015 hischier et al 2015 in this sense easyclimate becomes even more efficient if we are interested in climate data for multiple years and a small number of sites for querying climate data from large areas it is recommended to download the raster layers and extract the data to local storage e g using the extract function from terra r package hijmans 2022 to avoid overloading the ftp server as a test comparing the two methodologies i e using easyclimate vs raster downloading and local extraction we downloaded daily precipitation data for one year in an area of ca 100 km2 while the local download and extraction took 9 10 min in a laptop with good internet connection 10 mb s cooper 2022 and stored 5960 mb easyclimate took 17 s to obtain the same data storing only the final dataset 2 3 mb image 5 5 summary and conclusions this paper presents the r package easyclimate which facilitates access to high resolution daily temperature and precipitation data for europe for the period 1950 to 2020 the package enables downloading two types of data outputs i e tidy tables and rasters this climatic information is available by direct download from a ftp server but the use of easyclimate can save time of downloading processing and storage resources author contributions conceptualization v c a p r b and f r s data curation c p and m n formal analysis v c a and f r s funding acquisition m n and h h investigation c p and m n methodology v c a s r p r b j a and f r s project administration v c a resources c p m n and h h software v c a s r and f r s supervision v c a p r b and f r s validation v c a p r b j a and f r s visualization v c a p r b and j a writing original draft v c a and f r s writing review editing v c a c p s r p r b j a m n h h and f r s declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we acknowledge the support by the university of natural resources and life sciences vienna austria vca was supported by the real colegio complutense postdoctoral fellowship 2020 cp mn and hh acknowledge the financial support by the bio based industries joint undertaking under the european union s horizon 2020 research and innovation program tech4effect techniques and technologies for effective wood procurement project grant number 720757 prb was supported by the community of madrid region and the universidad de alcalá stimulus to excellence for permanent university professors epu inv 2020 010 ayudas para la realización de proyectos para potenciar la creación y consolidación de grupos de investigación cg20 cc 005 ja was supported by the fpi fellowship of the department of education of the basque government vca prb and ja also acknowledge the financial support by ministerio de ciencia e innovación subproject large nº pid 2021 123675ob c41 frs was supported by the vi plan propio de investigación of universidad de sevilla vi ppit us by ministerio de ciencia e innovación through european regional development fund sumhal lifewatch 2019 09 csic 13 pope 2014 2020 and by feder 2014 2020 and consejería de economía conocimiento empresas y universidad of junta de andalucía grant us 1381388 
25448,this study proposes a new comprehensive remote sensing drought index crsdi based on the nested copulas of remotely sensed precipitation vegetation index and land surface temperature that represent water availability vegetation health and disturbance impact respectively for monitoring meteorological and agroecological droughts the qinghai tibet plateau qtp is chosen as a case study to test the crsdi and investigate drought changes and their impacts on vegetation the results show that crsdi can effectively monitor the propagation processes and characteristics of meteorological and agroecological droughts drought conditions on the qtp are overall relieved from 2000 to 2020 while these changes show an apparent spatial variability with a general drying wetting trend in the north south moreover vegetation response to drought is different among different vegetation types with shrubland having a stronger response followed by cropland grassland and forestland clearly crsdi is valuable for large scale drought monitoring and assessment keywords drought comprehensive drought index remote sensing spatiotemporal variation vegetation sensitivity data availability data availability is provided in the manuscript 1 introduction drought usually refers to a meteorological disaster in which there is no rain or little rain for a long time resulting in insufficient soil water destruction of crop water balance and reduction of production wilhite and glantz 1985 drought is a natural disaster with complex causes long duration and wide impact which seriously affects ecological protection agricultural production food security and economic development mishra and singh 2010 parsons et al 2019 shi et al 2020 l zhang et al 2017b because the impacts of drought are expected to gradually increase under global warming ayantobo et al 2018 chen and sun 2017 tao and zhang 2020 an in depth study of the drought hazards is urgently needed therefore many studies have been conducted in recent years to quantify droughts mishra and singh 2010 vicente serrano et al 2010 wells et al 2004 the construction of drought indices has become the important methodological and computational basis for studying droughts and their impacts because drought indices can capture the critical attributes of droughts such as their initiation time duration and ending time chang et al 2018 jiao et al 2019 furthermore a drought index can enable objective and quantitative analyses of drought s spatiotemporal characteristics and drought monitoring c hao et al 2015 kao and govindaraju 2010 keyantash and dracup 2004 droughts are usually classified as meteorological agricultural hydrological or socioeconomic wilhite and glantz 1985 in recent years groundwater ecological and flash droughts have also been defined as distinct drought types crausbay et al 2017 y liu et al 2020 marchant and bloomfield 2018 there are various drought indices such as the standardized precipitation index spi mckee et al 1993 and palmer drought severity index pdsi palmer 1965 used for quantifying meteorological drought the standardized runoff index sri shukla and wood 2008 used for quantifying hydrological drought and the soil moisture deficit index smdi narasimhan and srinivasan 2005 and crop moisture index cmi palmer 1968 used for quantifying agricultural drought the occurrence and evolution processes of different types of droughts are often interconnected meteorological drought caused by persistent lack of precipitation acts on the underlying surface which has different degrees of impacts on soil water availability vegetation vitality and hydrological processes thus consequently triggering agricultural and hydrological droughts however the above mentioned drought indices are mostly used for monitoring a certain type of drought considering that there are time lags and connections between different types of droughts these specialized drought indices cannot comprehensively quantify the characteristics of these events involving different drought types and their impacts it is easy to misreport and underestimate the drought impacts solely based on a certain type of drought index z hao et al 2017 therefore it is necessary to develop a drought index that integrates multiple elements representing the critical aspects of droughts recently many researchers have committed to creating a comprehensive drought index that integrates multiple drought related variables for instance the aggregate drought index developed by keyantash and dracup 2004 using principal component analysis pca can effectively reflect comprehensive drought information including the meteorological and hydrological impacts of droughts however this drought index based on pca cannot comprehensively maintain all information of the original variables or reflect the nonlinear characteristics of the hydro meteorological influencing factors y liu et al 2019 mo and lettenmaier 2014 constructed a grand mean index using a linear combination of spi sri and soil moisture percentiles it is worth noting that the linear combination method is based on the linear assumption and it is difficult to objectively determine the weights in contrast the copula function can construct a multivariate joint distribution function with different marginal distributions which can effectively prevent the above mentioned problems and has been widely used in the construction of comprehensive drought indices z hao and aghakouchak 2013 shah and mishra 2020 q zhang et al 2018 shah and mishra 2020 used the gaussian copula function to construct the joint distribution of the spi sri standardized groundwater index standardized soil moisture index ssi and several other drought indices the resultant integrated drought index successfully captured multiple severe drought events and runoff and groundwater depletion caused by droughts in india z hao and aghakouchak 2013 proposed a multivariate standardized drought index for the integrated assessment of meteorological and agricultural droughts by calculating the copula joint distribution function of spi and ssi these existing comprehensive drought indices consider a variety of factors such as precipitation runoff and soil water however they are mostly based on ground station observations or land surface model simulations although gauging data can accurately characterize the drought conditions around the station the density of observation stations is usually limited in many areas conventional ground station observations cannot fully describe the full picture of drought occurrence and consequent evolution process especially in these regions with sparse gauging stations land surface model simulation can compensate for the lack of observed data but the simulated results usually have high uncertainty leading to considerable uncertainty for drought monitoring mo et al 2012 with the development of remote sensing technology especially infrared and microwave remote sensing drought indices based on remote sensing data have been proposed and used these indices can provide large coverage and continuous monitoring in space and time aghakouchak et al 2015 jiao et al 2021 k zhang et al 2016 the normalized difference vegetation index ndvi was the earliest developed remote sensing product to monitor vegetation health and has been widely used it can effectively reflect vegetation growth status by combining the red and near infrared bands rouse 1973 however ndvi can be affected by atmospheric conditions and background information from soil and canopy and has a problem of saturation in these areas with dense vegetation to conquer these limitations the enhanced vegetation index evi optimizes all these aspects h q liu and huete 1995 the vegetation condition index vci utilizes the historical ndvi sequence of the monitoring target to detect relative change in ndvi over time f kogan and sullivan 1993 however the lagged response of vegetation indices to water deficit causes them to have some temporal lag kogan 1995 constructed the temperature condition index tci which is similar to the vci by introducing historical information on land surface temperature lst which compensates for the disadvantage of the strong lag in vci however there are uncertainties in the tci monitoring of drought due to factors such as subsurface conditions differences in satellite transit times and surface thermal advection kogan 1995 also proposed a vegetation health index vhi by combining the vci and tci the vhi has the advantages of the vci and tci and the calculation method is simple therefore it has been successfully used in fields for monitoring agricultural droughts and vegetation dynamics in many regions around the world bokusheva et al 2016 pei et al 2018 however the coefficients α and β in the vhi vary depending on the region and time making them difficult to obtain in practical applications although these indices can monitor drought on a large scale the factors considered are relatively singular and it is difficult to portray drought in an integrated manner from multiple perspectives such as meteorology and agroecology the qinghai tibet plateau qtp has an average elevation of more than 4000 m and is the largest plateau in the world fig 1 qtp is also the source areas of the yangtze river yellow river yarlung zangbo river indus river ganges and several other major rivers in asia therefore qtp is called asian water tower because it plays a key role on providing valuable water resources for asia due to the unique topography and geographical location of the qtp it has a complex climate system that spans several climate zones including humid tropical mountain climate humid subtropical climate temperate humid plateau monsoon climate temperate semi humid plateau monsoon climate temperate semi arid plateau monsoon climate sub frigid semi humid plateau monsoon climate sub frigid semi arid plateau climate sub frigid arid plateau climate and frigid arid plateau climate it also has strong kinetic and thermal impacts on the asian monsoon circulation zhou et al 2009 which is a key area when studying the climate of the entire asia therefore comprehensively analyzing the characteristics of drought changes on the qtp is of great significance for regional drought risk assessment and water resources management it is also a perfect study area to study different types of droughts and their propagation and interconnections since this region spans many climate zones therefore the objectives of this study are two fold 1 develop a new comprehensive remote sensing drought index crsdi based on satellite data products precipitation evi and lst and nested copulas to comprehensively describe droughts from multiple perspectives and compensate for the spatiotemporal discontinuities of discrete station monitoring and 2 investigate the spatiotemporal characteristics and changes of meteorological and agroecological droughts on the qtp during the past 20 years 2 methods 2 1 structure of the crsdi index to compensate for the shortage of observed data and the quality limitation of land surface model simulations and to accurately characterize meteorological and agroecological droughts we developed the crsdi index based on the nested copulas of the standardized values of three key remotely sensed variables including precipitation evi and lst since a copula function of two or more variables is not restricted by the marginal distributions of these single variables wilhite and glantz 1985 the copula function is used to construct the crsdi index 2 1 1 marginal distributions remote sensing data were collected at predetermined sliding windows to represent drought conditions at 1 3 6 9 12 24 and 36 month time scales a total of nine marginal distribution functions including pearson iii p iii exponential exp gamma gam log normal logn generalized pareto gp generalized extreme value gev weibull wbl log logistic lol and normal norm distributions were used to fit the remote sensing data of different vegetation types at different time scales furthermore parameter estimation was carried out using the maximum likelihood estimate the kolmogorov smirnov k s test and root mean square error rmse were used to test the goodness of fit for the marginal distributions and to determine the most suitable marginal distribution function that fits well on different time scales 2 1 2 joint distribution copula functions are constructed in various ways such as archimedean meta elliptic extreme value and miscellaneous archimedean copulas are simple and broadly representative but they have high dimensional problems thus more flexible nested archimedean copulas have been developed joe 1997 the nested archimedean copula function extends the binary archimedean copula function to multiple dimensions and is used to study the correlation of multidimensional variables in a simple and practical manner therefore the gumbel clayton and frank copula functions from the nested archimedean copula family and the student s t copula and gaussian copula from the meta elliptic copulas were used as alternative functions to seek the optimal copula function the parameter estimation was performed using the maximum likelihood estimate the akaike information criterion aic bayesian information criterion bic and rmse between the fitted copula function and the empirical copula were used to evaluate the goodness of fit the formulae are as follows 1 m s e 1 n k i 1 n p e i p i 2 2 r m s e m s e 3 a i c n ln m s e 2 k 4 b i c n ln m s e k ln n where n is the sample size k is the number of copula function parameters pe i is the empirical frequency of the joint distribution and p i is the theoretical probability of the joint distribution 2 1 3 standardized index a multivariate joint distribution function was constructed using precipitation evi and lst as the random variables because precipitation and evi have the opposite effect on drought to that of lst their joint distribution p can be expressed by the copula function c and joint cumulative probability p as follows 5 p x 1 x 1 x 2 x 2 x 3 x 3 p x 1 x 1 x 2 x 2 p x 1 x 1 x 2 x 2 x 3 x 3 c f 1 x 1 f 2 x 2 c f 1 x 1 f 2 x 2 f 3 x 3 c u 1 u 2 c u 1 u 2 u 3 p where random variables x 1 x 2 and x 3 represent precipitation evi and lst respectively x 1 x 2 and x 3 are the respective values of precipitation evi and lst and u i f i x i is the marginal distribution function when the cumulative probability p is small it indicates dry conditions otherwise it indicates wet conditions the joint probabilities may not be uniformly distributed which differs from the marginal probabilities in the univariate case therefore the distribution f was fitted to the joint probabilities which was standardized based on the standard normal distribution φ the crsdi can be defined based on the joint probability p as follows 6 c r s d i φ 1 f p where φ is the standard normal distribution function to avoid assumptions regarding the parametric distribution the gringorten plotting position formula was chosen to estimate the distribution f gringorten 1963 as shown in table 2 with reference to the classification method of spi the values of crsdi were classified into different drought classes such as mild drought moderate drought severe drought and extreme drought according to the comprehensive severity the index can be calculated on multiple time scales 1 3 12 24 months etc for example to study the changes of comprehensive drought events on the annual scale the crsdi with a time scale of 12 month is used 2 2 trend analysis the theil sen median trend analysis sen 1968 and mann kendall test kendall 1990 mann 1945 were used to calculate the temporal trends in meteorological and agroecological droughts across the study area since they are the mainstream methods for trend determination in the fields of meteorology and hydrology jiang et al 2015 the theil sen median trend analysis is a non parametric statistical method that is often used for determining the trend of long term series when the sen s slope β is larger than 0 it indicates an upward trend in the time series otherwise it indicates a downward trend the mann kendall test is conducted at the significance level of 0 05 when the test is pasted it indicates that there is a significant change in the time series at the significance level of 0 05 2 3 theory of runs this study identified and extracted typical features of comprehensive drought events based on the theory of runs including drought duration drought severity and drought severity peak a negative run occurs when the crsdi series is continuously less than a threshold value within a certain period indicating a drought event the drought duration is the number of months when the crsdi is continuously less than the threshold additionally drought severity is the cumulative sum of the crsdi for each drought event and the drought severity peak is the most negative value of the crsdi in a drought event 3 study area and data 3 1 study area in this study we chose the qtp fig 1 as our case study area due to its importance and uniqueness due to the unique topography and geographical location of the qtp it has a complex climate system that spans several climate zones including humid tropical mountain climate humid subtropical climate temperate humid plateau monsoon climate temperate semi humid plateau monsoon climate temperate semi arid plateau monsoon climate sub frigid semi humid plateau monsoon climate sub frigid semi arid plateau climate sub frigid arid plateau climate and frigid arid plateau climate correspondingly it is an ideal place to test the effectiveness of the newly developed drought index 3 2 data the data used in this study included precipitation evi lst land cover type and the self calibrating palmer drought severity index scpdsi the evi lst and land cover types were obtained from the moderate resolution imaging spectroradiometer modis v6 product with a spatial resolution of 0 05 https lpdaac usgs gov this product included the vegetation index vi values product mod13c2 land surface temperature and emissivity lst e values product mod11c3 and land cover climate modeling grid cmg values product mcd12c1 for the evi lst and land cover types respectively the mean values of daytime and nighttime lst were selected from the mod11c3 product integrated multi satellite retrievals for gpm imerg products are based on a large amount of remote sensing information microwave infrared and radar data acquired by a new generation of global precipitation measurement gpm satellite constellations the imerg products are of better quality than other commonly used satellite precipitation products tang et al 2020 and are more accurate for instantaneous precipitation estimates especially for trace precipitation and cool season solid precipitation hou et al 2014 monthly precipitation was obtained from the imerg v06 product using the gpm imerg final precipitation l3 gpm 3imergm with a spatiotemporal resolution of 0 1 https gpm nasa gov the 0 5 and monthly scpdsi database on a global scale were provided by the climatic research unit cru of east anglia university in the united kingdom and it is referred to as cru scpdsi http climexp knmi nl select cgi id someone somewhere field scpdsi van der schrier et al 2013 to unify the resolution the nearest neighbor interpolation method was used to interpolate the land cover type data and the other datasets were disaggregated to 0 05 using the bilinear interpolation method the nearest neighbor interpolation method was used for the pixels that did not pass the quality control and for those with missing values except for the data on land cover type selected in 2010 the temporal resolution of the remaining data was monthly and the temporal coverage was from june 2000 to may 2020 to explore the distribution of precipitation evi and lst of different vegetation types and the response of different vegetation types to drought this study used land cover type data and selected the international geosphere biosphere program igbp classification with 17 land use categories considering the uniqueness of the qtp we eliminated the categories of land cover outside the study area and reclassified the remaining 14 categories based on interclass similarity the results of the original classification and the reclassification are presented in table 1 4 results 4 1 distribution optimization in theory any time scale can be selected for deriving the corresponding optimal distributions of precipitation evi and lst on the same time scale and associated parameters however the complex algorithm and large workload significantly reduce the versatility of the model by following the commonly used time scales for drought analysis the developed drought index is calculated on the time scales of 1 3 6 9 12 24 and 36 months the k s test revealed that the exp and gp distributions failed to fit precipitation and evi so the wbl gam p iii logn gev norm and lol distributions were used to fit them in addition the exp gp and lol distributions failed to fit lst well so the wbl gam p iii logn gev and norm distributions were used to fit it the k s test and rmse calculation were further applied to test the goodness of fit for the remaining marginal functions on the time scales of 1 3 6 9 12 24 and 36 months according to the preference criterion the distribution with the minimum rmse is regarded as the optimal distribution if the rmse values are the same the distribution function with a maximum p value is selected as the final marginal function as shown in table 3 the optimal distributions for fitting precipitation evi and lst are the gev logn and p iii distributions for grassland respectively regarding forestland the optimal distribution for fitting precipitation and evi is the p iii distribution and the optimal distribution for lst is the logn distribution the optimal distribution for fitting precipitation evi and lst is the gev distribution for the shrubland and cropland the aic bic and rmse values of the joint distribution for the time scales of 1 3 6 9 12 24 and 36 months were averaged the minimum values of the aic bic and rmse were used as the judgment criteria for the best fit as shown in table 4 the gumbel copula is the optimal joint distribution of grassland and forestland for fitting precipitation evi p e and precipitation evi lst p e l the optimal joint distributions for fitting p e and p e l are the gumbel copula and clayton copula for shrubland respectively the optimal joint distributions of cropland for fitting p e and p e l are the gaussian copula and clayton copula respectively 4 2 evaluation of comprehensive drought index fig 2 shows the spatial distribution of the correlation coefficients of crsdi with spi vhi and cru scpdsi in the vegetation zone of the qtp from 2000 to 2020 on a monthly scale fig 2a and b depict the correlation coefficients of crsdi with spi and vhi greater than 0 6 and the mean values were 0 645 and 0 638 respectively indicating a strong correlation the correlation between the crsdi and cru scpdsi was not strong and most of them were between 0 2 and 0 6 fig 2c this may have been caused by the scarcity of stations on the qtp and the fact that the observed data from stations were used to calculate this index van der schrier et al 2013 the number of drought events was counted in the monthly crsdi spi vhi and cru scpdsi sequences in the vegetation zone of the qtp the percentage of drought events detected simultaneously by the crsdi with at least one of the spi vhi and cru scpdsi to the total drought events of crsdi was calculated as the accuracy rate of drought monitoring by the crsdi fig 3 furthermore the accuracy rate of drought monitoring by the crsdi was above 80 these analyses demonstrate that the crsdi has a certain degree of reliability for drought monitoring fig 4 a shows the regional average spi vhi and crsdi values on the monthly scale from june 2000 to may 2020 fig 4a shows that the trend and intraannual and interannual variability of crsdi are generally consistent with spi and vhi this shows that the newly constructed crsdi inherits the core variability of spi and vhi spi and vhi serve as well recognized indices for quantifying meteorological drought and agroecological drought respectively therefore there is some difference in the spi and vhi time series during the time periods marked the black boxes in fig 4b this is because spi can t capture some agroecological droughts and vhi can t capture some meteorological droughts however fig 4b show that the newly developed crsdi index can well capture these meteorological and agroecological droughts marked by the black boxes these results clearly demonstrate that the newly developed crsdi index really can serve as a comprehensive index that can capture both meteorological and agroecological droughts in other words the crsdi index is indeed able to take advantage of precipitation to monitor meteorological drought and use evi and lst to monitor agricultural drought at the same time after reviewing the relevant literature and information it was found that the southern part of the qtp suffered from drought in 2009 q li et al 2013 x li et al 2019a therefore the performances of the crsdi spi vhi and cru scpdsi used to monitor the 2009 drought event were compared table 5 shows the classification scales for the spi vhi and cru scpdsi fig 5 shows the spatial evolution of the 2009 drought in the vegetation zone of the qtp based on the crsdi spi vhi and cru scpdsi the vegetation zone began to experience widespread meteorological drought from the west to the east in january 2009 until april 2009 an agricultural drought monitored by the vhi occurred in the central and western parts of the vegetation zone indicated by the blue dashed circle of fig 5f but the meteorological drought indicator spi did not detect a large scale drought at this location indicated by the blue dashed circle of fig 5b there is a delay in the response of agricultural drought to meteorological drought narasimhan and srinivasan 2005 the crsdi monitored the occurrence of drought in this region indicated by the blue dashed circle of fig 5m and n in both january and april to provide timely warnings for agricultural drought the crsdi showed a similar spatial distribution pattern of drought as the spi indicating that it can capture the beginnings of a drought in july 2009 the southern part of the vegetation zone suffered from severe meteorological and agricultural droughts among them the most severe agricultural droughts were in linzhou county lhasa city shigatse city gamba county renbu county and sa gya county in the xigaze area the approximate locations of the areas mentioned above are indicated by the black dashed circles fig 5 although all four drought indices monitor drought in the area the cru scpdsi only detects mild droughts in the area indicating that the crsdi is more accurate in assessing drought levels than the cru scpdsi in october 2009 the drought in the southwestern part of the vegetation zone ended but then the drought shifted to the southeast indicated by the dashed green circles however the cru scpdsi still monitored the existence of a large area of severe drought in the southwest indicating that the crsdi can capture the end of a drought and drought duration more effectively than the cru scpdsi moreover the spatial distribution of the crsdi was similar to that of the spi and vhi indicating that the crsdi can monitor both meteorological and agroecological droughts these analyses show that the crsdi combined with various meteorological and agroecological elements such as precipitation evi and lst can sensitively and effectively capture important characteristics such as the beginning duration and ending of droughts 4 3 spatial and temporal variability of drought the temporal variation in the crsdi on an annual scale is shown in fig 6 the results showed that the crsdi increased at an annual rate of 0 024 from 2001 to 2019 indicating a wetting trend this result is consistent with those of previous studies y li et al 2019b c zhang et al 2017a only cropland among the four vegetation types shows a significant wetting trend with a value of 0 073 per year while the other vegetation types show insignificant wetting trends based on the theil sen median trend analysis and mann kendall test the spatial distribution of crsdi trends on annual and seasonal scales was studied the results showed that interannual and seasonal drought changes presented a trend of becoming less intense in the north and more intense in the south fig 7 on an annual scale the drought significantly worsened for grassland and forestland which are distributed in the central and southern parts of the qtp fig 7a areas that become significantly drier in winter are in the central part of the region which is dominated by grasslands fig 7e except for those areas during winter the areas that experienced dryness in the other seasons were similar to those in the interannual period compared with other seasons the trend of drought relief was the most evident in spring and the trend of becoming dry was the most evident in summer therefore focus should be placed on monitoring the drought in these areas during summer although the time series of the remote sensing data is relatively short no more than 30 years with the continuous update and development of remote sensing products products with longer time series can be obtained which will become more useful for the monitoring of spatiotemporal evolution 4 4 vegetation response to drought 4 4 1 drought characteristics to study the typical characteristics and drought frequencies of different vegetation types under drought conditions the drought frequency duration severity and severity peak of different vegetation types were analyzed from 2000 to 2020 using the crsdi series on a monthly scale fig 8 shows that drought occurs approximately once every 9 6 months while the average drought duration is 1 36 months the average drought severity is 0 42 and the average drought severity peak is 0 32 for the entire vegetated area cropland has the highest drought frequency which is a drought per 5 3 months among the four vegetation types shrubland has the hightest average drought duration drought severity and drought severity peak with a value of 1 63 months 0 76 and 0 55 respectively this indicates that shrubland on the qtp experiences the largest water deficits and is the most severely affected by droughts comparing to the other vegetation types 4 4 2 sensitivity analysis to further investigate the sensitivity of different vegetation types to drought the proportion of different vegetation types experiencing the same level of drought was calculated using the crsdi on an annual scale fig 9 according to the classification results in table 1 the drought sensitivities of forestland grassland shrubland and cropland were further explored in the last 19 years the entire vegetation zone and the four vegetation types have experienced droughts in more than 50 of the area and drought affected areas for the entire vegetation zone and four vegetation types all show a decreasing tendency fig 9 there are two peaks of drought in the vegetation zone namely 2006 and 2015 in which the drought affected areas are 66 9 and 67 1 respectively the whole vegetation zone suffers the most severe drought in 2015 fig 9a droughts on the grassland are mainly concentrated in 2006 2007 and 2015 however it is important to note that drought impacted area for grassland reaches the maximum extent in 2015 the drought affected percent area for grassland reaches 71 0 in 2015 while the percent area suffering severe or extreme droughts is 27 7 fig 9b because approximately 80 of the vegetation zone is grassland the percentage of drought affected areas of the grassland is very similar to that of the vegetation zone meanwhile over 50 of forestlands experience droughts in 2006 2009 and 2013 in particular the severest drought for forestlands occurs in 2009 fig 9c the total drought affected area accounted for 78 8 of the forestland area and 25 9 of the area affected by severe and extreme droughts fig 9c unlike forestland and grassland shrublands and croplands did not experience drought in some years however droughts on the shrublands and croplands tend to evolve into large scale droughts for example large scale droughts occurred on the shrublands in 5 years of the past 19 years in 2009 the percent area affected by drought was as high as 99 3 and the percent area affected by severe and extreme droughts was up to 83 1 fig 9d the drought affected percent area for cropland reached 99 1 in 2001 while the percent area experiencing severe and extreme droughts was approximately 75 0 in this year fig 9e this indicates that most croplands were severely affected by drought when a large scale drought occurs namely the drought affected percent area exceeding 50 the percent areas experiencing severe and extreme droughts are 19 2 23 8 25 5 and 29 9 for forestland grassland cropland and shrubland respectively these results indicate that forestland on the qtp suffers the least from severe droughts while shrubland experiences most from severe droughts among the four vegetation types 5 discussion the differential responses of different vegetation types to drought may be related to their different intrinsic biological structures and phenology forest has a root system that can absorb water in deep soil thereby contributing a certain drought tolerance to this vegetation type grasslands are slightly more sensitive to drought than forestland this is likely because grasses usually have shallower root systems than trees making grasses more sensitive to drought conditions for croplands irrigation is an important way for crops to obtain water apart from rainwater and crops generally have shallow root systems that are sensitive to drought stress therefore crop growth monitoring can provide early warnings of the occurrence and development of agricultural droughts it may also provide theoretical support for taking timely disaster prevention and relief measures when conducting agricultural drought monitoring in the context of global climate change meteorological or agricultural droughts are expected to occur more frequently and become interwoven forming comprehensive drought conditions therefore it is critical to understand the popragations of different drought types and develop comprehensive drought index that can accurately capture different drought types considering that the existing drought indices specialized for quantifying single type droughts cannot meet the needs of practice it can be seen that the crsdi developed in this study has several apparent advantages such as reliability and comprehensiveness for drought monitoring for example it has the advantages of both spi and vhi crsdi can capture the initial time of a drought as efficiently as spi and can also determine the duration and ending time of a drought as efficiently as vhi its performance on quantifying drought severity and monitoring the evolution process of drought is better than that of the cru scpdsi in addition it can comprehensively monitor meteorological and agroecological droughts thereby providing a new tool for monitoring early warning and forecasting of droughts it also facilitates a more comprehensive and accurate drought observation method for regions with a small number of gauging stations however there are still some limitations in this study that may influence the accuracy of the crsdi calculations and need be further addressed in the future studies first uncertainty in the remote sensing data may introduce uncertainty in the resultant crsdi and the associated quantified drought characteristics second this study is mainly focused on designing a new drought index testing its effectiveness and further analyzing the spatiotemporal characteristics and changing patterns of drought on the qtp however the potential causes and drivers of droughts need be further investigated in the future studies in addition in order to explore the most suitable marginal distribution functions of different variables for different vegetation types a variety of parameter distribution functions were selected to evaluate the goodness of fit in this study we used the nonparametric distribution functions to fit the marginal distributions of the variables considering its advantages it would be valuable to explore whether parametric distribution can achieve the same effect 6 conclusions this study used remote sensing data including the precipitation evi and lst to accurately quantify the meteorological and agroecological droughts for different vegetation types on the qtp this was realized by developing a new comprehensive drought index namely the crsdi index based on the best fitting marginal distribution functions and nested copula functions of remotely sensed precipitation evi and lst we applied the crsdi to quantify the meteorological and agroecological droughts on the qtp analyzed the spatiotemporal characteristics and changes of droughts on the qtp and investigated the differential responses of different vegetation types to droughts comparing to the results of spi vhi and cru scpdsi crsdi has the advantages of both spi and vhi it can capture the initial time of a drought and can also determine the duration and ending time of a drought moreover the performance of crsdi on monitoring the drought is better than that of the cru scpdsi this means that crsdi can comprehensively characterize both meteorological and agroecological droughts and provide reliable and valuable support for the monitoring prevention and forecasting of droughts the spatiotemporal changes of droughts were significant on the qtp among the four vegetation types only cropland shows a significant wetting trend from 2001 to 2019 although the other vegetation types show an insignificant wetting trend spatially droughts on the annual and seasonal scales show a trendency to becoming less intense in the north and becoming more intense in the south from 2001 to 2019 droughts are significantly aggravated on annual and seasonal scales in the central and southern parts of the qtp moreover the drying trend in the summer is the most significant among the four seasons indicating that summer is a period with a high incidence of drought although the drought conditions of croplands and shrublands were more severe droughts occurred most frequently in croplands meanwhile the average drought duration average drought severity and average drought severity peak of shrublands were the largest with a value of 1 63 months 0 76 and 0 55 respectively therefore drought monitoring in these areas should be strengthened and disaster prevention and relief measures should be implemented in a timely manner in addition different vegetation types show differential responses to drought with shrubland and cropland more vulnerable to droughts than forestland and grassland these differences are likely related to the differential degree of root system development among these vegetation types and their physical environment author contributions ke zhang and yujia cheng designed this study yujia cheng and ke zhang set up the models and method yujia cheng ke zhang lijun chao and wuzhi shi conducted this study ke zhang and yujia cheng wrote the manuscript ke zhang acquired the funding jing feng and yunping li contributed to the revision software availability the codes used to calculate crsdi can be downloaded from https github com ke zhang crsdi git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the national natural science foundation of china 51879067 fundamental research funds for the central universities of china b220203051 b220204014 national key research and development program of china 2018yfc1508101 natural science foundation of jiangsu province bk20180022 and six talent peaks project in jiangsu province ny 004 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105629 
25448,this study proposes a new comprehensive remote sensing drought index crsdi based on the nested copulas of remotely sensed precipitation vegetation index and land surface temperature that represent water availability vegetation health and disturbance impact respectively for monitoring meteorological and agroecological droughts the qinghai tibet plateau qtp is chosen as a case study to test the crsdi and investigate drought changes and their impacts on vegetation the results show that crsdi can effectively monitor the propagation processes and characteristics of meteorological and agroecological droughts drought conditions on the qtp are overall relieved from 2000 to 2020 while these changes show an apparent spatial variability with a general drying wetting trend in the north south moreover vegetation response to drought is different among different vegetation types with shrubland having a stronger response followed by cropland grassland and forestland clearly crsdi is valuable for large scale drought monitoring and assessment keywords drought comprehensive drought index remote sensing spatiotemporal variation vegetation sensitivity data availability data availability is provided in the manuscript 1 introduction drought usually refers to a meteorological disaster in which there is no rain or little rain for a long time resulting in insufficient soil water destruction of crop water balance and reduction of production wilhite and glantz 1985 drought is a natural disaster with complex causes long duration and wide impact which seriously affects ecological protection agricultural production food security and economic development mishra and singh 2010 parsons et al 2019 shi et al 2020 l zhang et al 2017b because the impacts of drought are expected to gradually increase under global warming ayantobo et al 2018 chen and sun 2017 tao and zhang 2020 an in depth study of the drought hazards is urgently needed therefore many studies have been conducted in recent years to quantify droughts mishra and singh 2010 vicente serrano et al 2010 wells et al 2004 the construction of drought indices has become the important methodological and computational basis for studying droughts and their impacts because drought indices can capture the critical attributes of droughts such as their initiation time duration and ending time chang et al 2018 jiao et al 2019 furthermore a drought index can enable objective and quantitative analyses of drought s spatiotemporal characteristics and drought monitoring c hao et al 2015 kao and govindaraju 2010 keyantash and dracup 2004 droughts are usually classified as meteorological agricultural hydrological or socioeconomic wilhite and glantz 1985 in recent years groundwater ecological and flash droughts have also been defined as distinct drought types crausbay et al 2017 y liu et al 2020 marchant and bloomfield 2018 there are various drought indices such as the standardized precipitation index spi mckee et al 1993 and palmer drought severity index pdsi palmer 1965 used for quantifying meteorological drought the standardized runoff index sri shukla and wood 2008 used for quantifying hydrological drought and the soil moisture deficit index smdi narasimhan and srinivasan 2005 and crop moisture index cmi palmer 1968 used for quantifying agricultural drought the occurrence and evolution processes of different types of droughts are often interconnected meteorological drought caused by persistent lack of precipitation acts on the underlying surface which has different degrees of impacts on soil water availability vegetation vitality and hydrological processes thus consequently triggering agricultural and hydrological droughts however the above mentioned drought indices are mostly used for monitoring a certain type of drought considering that there are time lags and connections between different types of droughts these specialized drought indices cannot comprehensively quantify the characteristics of these events involving different drought types and their impacts it is easy to misreport and underestimate the drought impacts solely based on a certain type of drought index z hao et al 2017 therefore it is necessary to develop a drought index that integrates multiple elements representing the critical aspects of droughts recently many researchers have committed to creating a comprehensive drought index that integrates multiple drought related variables for instance the aggregate drought index developed by keyantash and dracup 2004 using principal component analysis pca can effectively reflect comprehensive drought information including the meteorological and hydrological impacts of droughts however this drought index based on pca cannot comprehensively maintain all information of the original variables or reflect the nonlinear characteristics of the hydro meteorological influencing factors y liu et al 2019 mo and lettenmaier 2014 constructed a grand mean index using a linear combination of spi sri and soil moisture percentiles it is worth noting that the linear combination method is based on the linear assumption and it is difficult to objectively determine the weights in contrast the copula function can construct a multivariate joint distribution function with different marginal distributions which can effectively prevent the above mentioned problems and has been widely used in the construction of comprehensive drought indices z hao and aghakouchak 2013 shah and mishra 2020 q zhang et al 2018 shah and mishra 2020 used the gaussian copula function to construct the joint distribution of the spi sri standardized groundwater index standardized soil moisture index ssi and several other drought indices the resultant integrated drought index successfully captured multiple severe drought events and runoff and groundwater depletion caused by droughts in india z hao and aghakouchak 2013 proposed a multivariate standardized drought index for the integrated assessment of meteorological and agricultural droughts by calculating the copula joint distribution function of spi and ssi these existing comprehensive drought indices consider a variety of factors such as precipitation runoff and soil water however they are mostly based on ground station observations or land surface model simulations although gauging data can accurately characterize the drought conditions around the station the density of observation stations is usually limited in many areas conventional ground station observations cannot fully describe the full picture of drought occurrence and consequent evolution process especially in these regions with sparse gauging stations land surface model simulation can compensate for the lack of observed data but the simulated results usually have high uncertainty leading to considerable uncertainty for drought monitoring mo et al 2012 with the development of remote sensing technology especially infrared and microwave remote sensing drought indices based on remote sensing data have been proposed and used these indices can provide large coverage and continuous monitoring in space and time aghakouchak et al 2015 jiao et al 2021 k zhang et al 2016 the normalized difference vegetation index ndvi was the earliest developed remote sensing product to monitor vegetation health and has been widely used it can effectively reflect vegetation growth status by combining the red and near infrared bands rouse 1973 however ndvi can be affected by atmospheric conditions and background information from soil and canopy and has a problem of saturation in these areas with dense vegetation to conquer these limitations the enhanced vegetation index evi optimizes all these aspects h q liu and huete 1995 the vegetation condition index vci utilizes the historical ndvi sequence of the monitoring target to detect relative change in ndvi over time f kogan and sullivan 1993 however the lagged response of vegetation indices to water deficit causes them to have some temporal lag kogan 1995 constructed the temperature condition index tci which is similar to the vci by introducing historical information on land surface temperature lst which compensates for the disadvantage of the strong lag in vci however there are uncertainties in the tci monitoring of drought due to factors such as subsurface conditions differences in satellite transit times and surface thermal advection kogan 1995 also proposed a vegetation health index vhi by combining the vci and tci the vhi has the advantages of the vci and tci and the calculation method is simple therefore it has been successfully used in fields for monitoring agricultural droughts and vegetation dynamics in many regions around the world bokusheva et al 2016 pei et al 2018 however the coefficients α and β in the vhi vary depending on the region and time making them difficult to obtain in practical applications although these indices can monitor drought on a large scale the factors considered are relatively singular and it is difficult to portray drought in an integrated manner from multiple perspectives such as meteorology and agroecology the qinghai tibet plateau qtp has an average elevation of more than 4000 m and is the largest plateau in the world fig 1 qtp is also the source areas of the yangtze river yellow river yarlung zangbo river indus river ganges and several other major rivers in asia therefore qtp is called asian water tower because it plays a key role on providing valuable water resources for asia due to the unique topography and geographical location of the qtp it has a complex climate system that spans several climate zones including humid tropical mountain climate humid subtropical climate temperate humid plateau monsoon climate temperate semi humid plateau monsoon climate temperate semi arid plateau monsoon climate sub frigid semi humid plateau monsoon climate sub frigid semi arid plateau climate sub frigid arid plateau climate and frigid arid plateau climate it also has strong kinetic and thermal impacts on the asian monsoon circulation zhou et al 2009 which is a key area when studying the climate of the entire asia therefore comprehensively analyzing the characteristics of drought changes on the qtp is of great significance for regional drought risk assessment and water resources management it is also a perfect study area to study different types of droughts and their propagation and interconnections since this region spans many climate zones therefore the objectives of this study are two fold 1 develop a new comprehensive remote sensing drought index crsdi based on satellite data products precipitation evi and lst and nested copulas to comprehensively describe droughts from multiple perspectives and compensate for the spatiotemporal discontinuities of discrete station monitoring and 2 investigate the spatiotemporal characteristics and changes of meteorological and agroecological droughts on the qtp during the past 20 years 2 methods 2 1 structure of the crsdi index to compensate for the shortage of observed data and the quality limitation of land surface model simulations and to accurately characterize meteorological and agroecological droughts we developed the crsdi index based on the nested copulas of the standardized values of three key remotely sensed variables including precipitation evi and lst since a copula function of two or more variables is not restricted by the marginal distributions of these single variables wilhite and glantz 1985 the copula function is used to construct the crsdi index 2 1 1 marginal distributions remote sensing data were collected at predetermined sliding windows to represent drought conditions at 1 3 6 9 12 24 and 36 month time scales a total of nine marginal distribution functions including pearson iii p iii exponential exp gamma gam log normal logn generalized pareto gp generalized extreme value gev weibull wbl log logistic lol and normal norm distributions were used to fit the remote sensing data of different vegetation types at different time scales furthermore parameter estimation was carried out using the maximum likelihood estimate the kolmogorov smirnov k s test and root mean square error rmse were used to test the goodness of fit for the marginal distributions and to determine the most suitable marginal distribution function that fits well on different time scales 2 1 2 joint distribution copula functions are constructed in various ways such as archimedean meta elliptic extreme value and miscellaneous archimedean copulas are simple and broadly representative but they have high dimensional problems thus more flexible nested archimedean copulas have been developed joe 1997 the nested archimedean copula function extends the binary archimedean copula function to multiple dimensions and is used to study the correlation of multidimensional variables in a simple and practical manner therefore the gumbel clayton and frank copula functions from the nested archimedean copula family and the student s t copula and gaussian copula from the meta elliptic copulas were used as alternative functions to seek the optimal copula function the parameter estimation was performed using the maximum likelihood estimate the akaike information criterion aic bayesian information criterion bic and rmse between the fitted copula function and the empirical copula were used to evaluate the goodness of fit the formulae are as follows 1 m s e 1 n k i 1 n p e i p i 2 2 r m s e m s e 3 a i c n ln m s e 2 k 4 b i c n ln m s e k ln n where n is the sample size k is the number of copula function parameters pe i is the empirical frequency of the joint distribution and p i is the theoretical probability of the joint distribution 2 1 3 standardized index a multivariate joint distribution function was constructed using precipitation evi and lst as the random variables because precipitation and evi have the opposite effect on drought to that of lst their joint distribution p can be expressed by the copula function c and joint cumulative probability p as follows 5 p x 1 x 1 x 2 x 2 x 3 x 3 p x 1 x 1 x 2 x 2 p x 1 x 1 x 2 x 2 x 3 x 3 c f 1 x 1 f 2 x 2 c f 1 x 1 f 2 x 2 f 3 x 3 c u 1 u 2 c u 1 u 2 u 3 p where random variables x 1 x 2 and x 3 represent precipitation evi and lst respectively x 1 x 2 and x 3 are the respective values of precipitation evi and lst and u i f i x i is the marginal distribution function when the cumulative probability p is small it indicates dry conditions otherwise it indicates wet conditions the joint probabilities may not be uniformly distributed which differs from the marginal probabilities in the univariate case therefore the distribution f was fitted to the joint probabilities which was standardized based on the standard normal distribution φ the crsdi can be defined based on the joint probability p as follows 6 c r s d i φ 1 f p where φ is the standard normal distribution function to avoid assumptions regarding the parametric distribution the gringorten plotting position formula was chosen to estimate the distribution f gringorten 1963 as shown in table 2 with reference to the classification method of spi the values of crsdi were classified into different drought classes such as mild drought moderate drought severe drought and extreme drought according to the comprehensive severity the index can be calculated on multiple time scales 1 3 12 24 months etc for example to study the changes of comprehensive drought events on the annual scale the crsdi with a time scale of 12 month is used 2 2 trend analysis the theil sen median trend analysis sen 1968 and mann kendall test kendall 1990 mann 1945 were used to calculate the temporal trends in meteorological and agroecological droughts across the study area since they are the mainstream methods for trend determination in the fields of meteorology and hydrology jiang et al 2015 the theil sen median trend analysis is a non parametric statistical method that is often used for determining the trend of long term series when the sen s slope β is larger than 0 it indicates an upward trend in the time series otherwise it indicates a downward trend the mann kendall test is conducted at the significance level of 0 05 when the test is pasted it indicates that there is a significant change in the time series at the significance level of 0 05 2 3 theory of runs this study identified and extracted typical features of comprehensive drought events based on the theory of runs including drought duration drought severity and drought severity peak a negative run occurs when the crsdi series is continuously less than a threshold value within a certain period indicating a drought event the drought duration is the number of months when the crsdi is continuously less than the threshold additionally drought severity is the cumulative sum of the crsdi for each drought event and the drought severity peak is the most negative value of the crsdi in a drought event 3 study area and data 3 1 study area in this study we chose the qtp fig 1 as our case study area due to its importance and uniqueness due to the unique topography and geographical location of the qtp it has a complex climate system that spans several climate zones including humid tropical mountain climate humid subtropical climate temperate humid plateau monsoon climate temperate semi humid plateau monsoon climate temperate semi arid plateau monsoon climate sub frigid semi humid plateau monsoon climate sub frigid semi arid plateau climate sub frigid arid plateau climate and frigid arid plateau climate correspondingly it is an ideal place to test the effectiveness of the newly developed drought index 3 2 data the data used in this study included precipitation evi lst land cover type and the self calibrating palmer drought severity index scpdsi the evi lst and land cover types were obtained from the moderate resolution imaging spectroradiometer modis v6 product with a spatial resolution of 0 05 https lpdaac usgs gov this product included the vegetation index vi values product mod13c2 land surface temperature and emissivity lst e values product mod11c3 and land cover climate modeling grid cmg values product mcd12c1 for the evi lst and land cover types respectively the mean values of daytime and nighttime lst were selected from the mod11c3 product integrated multi satellite retrievals for gpm imerg products are based on a large amount of remote sensing information microwave infrared and radar data acquired by a new generation of global precipitation measurement gpm satellite constellations the imerg products are of better quality than other commonly used satellite precipitation products tang et al 2020 and are more accurate for instantaneous precipitation estimates especially for trace precipitation and cool season solid precipitation hou et al 2014 monthly precipitation was obtained from the imerg v06 product using the gpm imerg final precipitation l3 gpm 3imergm with a spatiotemporal resolution of 0 1 https gpm nasa gov the 0 5 and monthly scpdsi database on a global scale were provided by the climatic research unit cru of east anglia university in the united kingdom and it is referred to as cru scpdsi http climexp knmi nl select cgi id someone somewhere field scpdsi van der schrier et al 2013 to unify the resolution the nearest neighbor interpolation method was used to interpolate the land cover type data and the other datasets were disaggregated to 0 05 using the bilinear interpolation method the nearest neighbor interpolation method was used for the pixels that did not pass the quality control and for those with missing values except for the data on land cover type selected in 2010 the temporal resolution of the remaining data was monthly and the temporal coverage was from june 2000 to may 2020 to explore the distribution of precipitation evi and lst of different vegetation types and the response of different vegetation types to drought this study used land cover type data and selected the international geosphere biosphere program igbp classification with 17 land use categories considering the uniqueness of the qtp we eliminated the categories of land cover outside the study area and reclassified the remaining 14 categories based on interclass similarity the results of the original classification and the reclassification are presented in table 1 4 results 4 1 distribution optimization in theory any time scale can be selected for deriving the corresponding optimal distributions of precipitation evi and lst on the same time scale and associated parameters however the complex algorithm and large workload significantly reduce the versatility of the model by following the commonly used time scales for drought analysis the developed drought index is calculated on the time scales of 1 3 6 9 12 24 and 36 months the k s test revealed that the exp and gp distributions failed to fit precipitation and evi so the wbl gam p iii logn gev norm and lol distributions were used to fit them in addition the exp gp and lol distributions failed to fit lst well so the wbl gam p iii logn gev and norm distributions were used to fit it the k s test and rmse calculation were further applied to test the goodness of fit for the remaining marginal functions on the time scales of 1 3 6 9 12 24 and 36 months according to the preference criterion the distribution with the minimum rmse is regarded as the optimal distribution if the rmse values are the same the distribution function with a maximum p value is selected as the final marginal function as shown in table 3 the optimal distributions for fitting precipitation evi and lst are the gev logn and p iii distributions for grassland respectively regarding forestland the optimal distribution for fitting precipitation and evi is the p iii distribution and the optimal distribution for lst is the logn distribution the optimal distribution for fitting precipitation evi and lst is the gev distribution for the shrubland and cropland the aic bic and rmse values of the joint distribution for the time scales of 1 3 6 9 12 24 and 36 months were averaged the minimum values of the aic bic and rmse were used as the judgment criteria for the best fit as shown in table 4 the gumbel copula is the optimal joint distribution of grassland and forestland for fitting precipitation evi p e and precipitation evi lst p e l the optimal joint distributions for fitting p e and p e l are the gumbel copula and clayton copula for shrubland respectively the optimal joint distributions of cropland for fitting p e and p e l are the gaussian copula and clayton copula respectively 4 2 evaluation of comprehensive drought index fig 2 shows the spatial distribution of the correlation coefficients of crsdi with spi vhi and cru scpdsi in the vegetation zone of the qtp from 2000 to 2020 on a monthly scale fig 2a and b depict the correlation coefficients of crsdi with spi and vhi greater than 0 6 and the mean values were 0 645 and 0 638 respectively indicating a strong correlation the correlation between the crsdi and cru scpdsi was not strong and most of them were between 0 2 and 0 6 fig 2c this may have been caused by the scarcity of stations on the qtp and the fact that the observed data from stations were used to calculate this index van der schrier et al 2013 the number of drought events was counted in the monthly crsdi spi vhi and cru scpdsi sequences in the vegetation zone of the qtp the percentage of drought events detected simultaneously by the crsdi with at least one of the spi vhi and cru scpdsi to the total drought events of crsdi was calculated as the accuracy rate of drought monitoring by the crsdi fig 3 furthermore the accuracy rate of drought monitoring by the crsdi was above 80 these analyses demonstrate that the crsdi has a certain degree of reliability for drought monitoring fig 4 a shows the regional average spi vhi and crsdi values on the monthly scale from june 2000 to may 2020 fig 4a shows that the trend and intraannual and interannual variability of crsdi are generally consistent with spi and vhi this shows that the newly constructed crsdi inherits the core variability of spi and vhi spi and vhi serve as well recognized indices for quantifying meteorological drought and agroecological drought respectively therefore there is some difference in the spi and vhi time series during the time periods marked the black boxes in fig 4b this is because spi can t capture some agroecological droughts and vhi can t capture some meteorological droughts however fig 4b show that the newly developed crsdi index can well capture these meteorological and agroecological droughts marked by the black boxes these results clearly demonstrate that the newly developed crsdi index really can serve as a comprehensive index that can capture both meteorological and agroecological droughts in other words the crsdi index is indeed able to take advantage of precipitation to monitor meteorological drought and use evi and lst to monitor agricultural drought at the same time after reviewing the relevant literature and information it was found that the southern part of the qtp suffered from drought in 2009 q li et al 2013 x li et al 2019a therefore the performances of the crsdi spi vhi and cru scpdsi used to monitor the 2009 drought event were compared table 5 shows the classification scales for the spi vhi and cru scpdsi fig 5 shows the spatial evolution of the 2009 drought in the vegetation zone of the qtp based on the crsdi spi vhi and cru scpdsi the vegetation zone began to experience widespread meteorological drought from the west to the east in january 2009 until april 2009 an agricultural drought monitored by the vhi occurred in the central and western parts of the vegetation zone indicated by the blue dashed circle of fig 5f but the meteorological drought indicator spi did not detect a large scale drought at this location indicated by the blue dashed circle of fig 5b there is a delay in the response of agricultural drought to meteorological drought narasimhan and srinivasan 2005 the crsdi monitored the occurrence of drought in this region indicated by the blue dashed circle of fig 5m and n in both january and april to provide timely warnings for agricultural drought the crsdi showed a similar spatial distribution pattern of drought as the spi indicating that it can capture the beginnings of a drought in july 2009 the southern part of the vegetation zone suffered from severe meteorological and agricultural droughts among them the most severe agricultural droughts were in linzhou county lhasa city shigatse city gamba county renbu county and sa gya county in the xigaze area the approximate locations of the areas mentioned above are indicated by the black dashed circles fig 5 although all four drought indices monitor drought in the area the cru scpdsi only detects mild droughts in the area indicating that the crsdi is more accurate in assessing drought levels than the cru scpdsi in october 2009 the drought in the southwestern part of the vegetation zone ended but then the drought shifted to the southeast indicated by the dashed green circles however the cru scpdsi still monitored the existence of a large area of severe drought in the southwest indicating that the crsdi can capture the end of a drought and drought duration more effectively than the cru scpdsi moreover the spatial distribution of the crsdi was similar to that of the spi and vhi indicating that the crsdi can monitor both meteorological and agroecological droughts these analyses show that the crsdi combined with various meteorological and agroecological elements such as precipitation evi and lst can sensitively and effectively capture important characteristics such as the beginning duration and ending of droughts 4 3 spatial and temporal variability of drought the temporal variation in the crsdi on an annual scale is shown in fig 6 the results showed that the crsdi increased at an annual rate of 0 024 from 2001 to 2019 indicating a wetting trend this result is consistent with those of previous studies y li et al 2019b c zhang et al 2017a only cropland among the four vegetation types shows a significant wetting trend with a value of 0 073 per year while the other vegetation types show insignificant wetting trends based on the theil sen median trend analysis and mann kendall test the spatial distribution of crsdi trends on annual and seasonal scales was studied the results showed that interannual and seasonal drought changes presented a trend of becoming less intense in the north and more intense in the south fig 7 on an annual scale the drought significantly worsened for grassland and forestland which are distributed in the central and southern parts of the qtp fig 7a areas that become significantly drier in winter are in the central part of the region which is dominated by grasslands fig 7e except for those areas during winter the areas that experienced dryness in the other seasons were similar to those in the interannual period compared with other seasons the trend of drought relief was the most evident in spring and the trend of becoming dry was the most evident in summer therefore focus should be placed on monitoring the drought in these areas during summer although the time series of the remote sensing data is relatively short no more than 30 years with the continuous update and development of remote sensing products products with longer time series can be obtained which will become more useful for the monitoring of spatiotemporal evolution 4 4 vegetation response to drought 4 4 1 drought characteristics to study the typical characteristics and drought frequencies of different vegetation types under drought conditions the drought frequency duration severity and severity peak of different vegetation types were analyzed from 2000 to 2020 using the crsdi series on a monthly scale fig 8 shows that drought occurs approximately once every 9 6 months while the average drought duration is 1 36 months the average drought severity is 0 42 and the average drought severity peak is 0 32 for the entire vegetated area cropland has the highest drought frequency which is a drought per 5 3 months among the four vegetation types shrubland has the hightest average drought duration drought severity and drought severity peak with a value of 1 63 months 0 76 and 0 55 respectively this indicates that shrubland on the qtp experiences the largest water deficits and is the most severely affected by droughts comparing to the other vegetation types 4 4 2 sensitivity analysis to further investigate the sensitivity of different vegetation types to drought the proportion of different vegetation types experiencing the same level of drought was calculated using the crsdi on an annual scale fig 9 according to the classification results in table 1 the drought sensitivities of forestland grassland shrubland and cropland were further explored in the last 19 years the entire vegetation zone and the four vegetation types have experienced droughts in more than 50 of the area and drought affected areas for the entire vegetation zone and four vegetation types all show a decreasing tendency fig 9 there are two peaks of drought in the vegetation zone namely 2006 and 2015 in which the drought affected areas are 66 9 and 67 1 respectively the whole vegetation zone suffers the most severe drought in 2015 fig 9a droughts on the grassland are mainly concentrated in 2006 2007 and 2015 however it is important to note that drought impacted area for grassland reaches the maximum extent in 2015 the drought affected percent area for grassland reaches 71 0 in 2015 while the percent area suffering severe or extreme droughts is 27 7 fig 9b because approximately 80 of the vegetation zone is grassland the percentage of drought affected areas of the grassland is very similar to that of the vegetation zone meanwhile over 50 of forestlands experience droughts in 2006 2009 and 2013 in particular the severest drought for forestlands occurs in 2009 fig 9c the total drought affected area accounted for 78 8 of the forestland area and 25 9 of the area affected by severe and extreme droughts fig 9c unlike forestland and grassland shrublands and croplands did not experience drought in some years however droughts on the shrublands and croplands tend to evolve into large scale droughts for example large scale droughts occurred on the shrublands in 5 years of the past 19 years in 2009 the percent area affected by drought was as high as 99 3 and the percent area affected by severe and extreme droughts was up to 83 1 fig 9d the drought affected percent area for cropland reached 99 1 in 2001 while the percent area experiencing severe and extreme droughts was approximately 75 0 in this year fig 9e this indicates that most croplands were severely affected by drought when a large scale drought occurs namely the drought affected percent area exceeding 50 the percent areas experiencing severe and extreme droughts are 19 2 23 8 25 5 and 29 9 for forestland grassland cropland and shrubland respectively these results indicate that forestland on the qtp suffers the least from severe droughts while shrubland experiences most from severe droughts among the four vegetation types 5 discussion the differential responses of different vegetation types to drought may be related to their different intrinsic biological structures and phenology forest has a root system that can absorb water in deep soil thereby contributing a certain drought tolerance to this vegetation type grasslands are slightly more sensitive to drought than forestland this is likely because grasses usually have shallower root systems than trees making grasses more sensitive to drought conditions for croplands irrigation is an important way for crops to obtain water apart from rainwater and crops generally have shallow root systems that are sensitive to drought stress therefore crop growth monitoring can provide early warnings of the occurrence and development of agricultural droughts it may also provide theoretical support for taking timely disaster prevention and relief measures when conducting agricultural drought monitoring in the context of global climate change meteorological or agricultural droughts are expected to occur more frequently and become interwoven forming comprehensive drought conditions therefore it is critical to understand the popragations of different drought types and develop comprehensive drought index that can accurately capture different drought types considering that the existing drought indices specialized for quantifying single type droughts cannot meet the needs of practice it can be seen that the crsdi developed in this study has several apparent advantages such as reliability and comprehensiveness for drought monitoring for example it has the advantages of both spi and vhi crsdi can capture the initial time of a drought as efficiently as spi and can also determine the duration and ending time of a drought as efficiently as vhi its performance on quantifying drought severity and monitoring the evolution process of drought is better than that of the cru scpdsi in addition it can comprehensively monitor meteorological and agroecological droughts thereby providing a new tool for monitoring early warning and forecasting of droughts it also facilitates a more comprehensive and accurate drought observation method for regions with a small number of gauging stations however there are still some limitations in this study that may influence the accuracy of the crsdi calculations and need be further addressed in the future studies first uncertainty in the remote sensing data may introduce uncertainty in the resultant crsdi and the associated quantified drought characteristics second this study is mainly focused on designing a new drought index testing its effectiveness and further analyzing the spatiotemporal characteristics and changing patterns of drought on the qtp however the potential causes and drivers of droughts need be further investigated in the future studies in addition in order to explore the most suitable marginal distribution functions of different variables for different vegetation types a variety of parameter distribution functions were selected to evaluate the goodness of fit in this study we used the nonparametric distribution functions to fit the marginal distributions of the variables considering its advantages it would be valuable to explore whether parametric distribution can achieve the same effect 6 conclusions this study used remote sensing data including the precipitation evi and lst to accurately quantify the meteorological and agroecological droughts for different vegetation types on the qtp this was realized by developing a new comprehensive drought index namely the crsdi index based on the best fitting marginal distribution functions and nested copula functions of remotely sensed precipitation evi and lst we applied the crsdi to quantify the meteorological and agroecological droughts on the qtp analyzed the spatiotemporal characteristics and changes of droughts on the qtp and investigated the differential responses of different vegetation types to droughts comparing to the results of spi vhi and cru scpdsi crsdi has the advantages of both spi and vhi it can capture the initial time of a drought and can also determine the duration and ending time of a drought moreover the performance of crsdi on monitoring the drought is better than that of the cru scpdsi this means that crsdi can comprehensively characterize both meteorological and agroecological droughts and provide reliable and valuable support for the monitoring prevention and forecasting of droughts the spatiotemporal changes of droughts were significant on the qtp among the four vegetation types only cropland shows a significant wetting trend from 2001 to 2019 although the other vegetation types show an insignificant wetting trend spatially droughts on the annual and seasonal scales show a trendency to becoming less intense in the north and becoming more intense in the south from 2001 to 2019 droughts are significantly aggravated on annual and seasonal scales in the central and southern parts of the qtp moreover the drying trend in the summer is the most significant among the four seasons indicating that summer is a period with a high incidence of drought although the drought conditions of croplands and shrublands were more severe droughts occurred most frequently in croplands meanwhile the average drought duration average drought severity and average drought severity peak of shrublands were the largest with a value of 1 63 months 0 76 and 0 55 respectively therefore drought monitoring in these areas should be strengthened and disaster prevention and relief measures should be implemented in a timely manner in addition different vegetation types show differential responses to drought with shrubland and cropland more vulnerable to droughts than forestland and grassland these differences are likely related to the differential degree of root system development among these vegetation types and their physical environment author contributions ke zhang and yujia cheng designed this study yujia cheng and ke zhang set up the models and method yujia cheng ke zhang lijun chao and wuzhi shi conducted this study ke zhang and yujia cheng wrote the manuscript ke zhang acquired the funding jing feng and yunping li contributed to the revision software availability the codes used to calculate crsdi can be downloaded from https github com ke zhang crsdi git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the national natural science foundation of china 51879067 fundamental research funds for the central universities of china b220203051 b220204014 national key research and development program of china 2018yfc1508101 natural science foundation of jiangsu province bk20180022 and six talent peaks project in jiangsu province ny 004 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105629 
25449,geospatial web services gwss assist scientists developing models online cloud computing technologies have created opportunities for faster and more efficient environmental modeling among of the new public cloud products google earth engine gee is particularly well adapted to meet the needs of environmental scientists developing and sharing their models however the potential of gee to support geospatial community is less well explored scientists are still waiting to see what public cloud platforms can do for geospatial web services this paper proposes a prototype called ws4gee that can wrap gee embedded datasets functions and models as ogc web services such as wms wfs wcs and wps in this way ws4gee can support direct access to gee through ogc interfaces thus allowing geoprocessing workflows to use gee resources two experiments are presented one with a gee enabled geospatial data service and the other deploying geoprocessing composition with gee enabled gwss the results confirm that ws4gee with the support of gee significantly enhances the capabilities of gws to deal with labor intensive and time consuming tasks this study provides guidance for the wider geospatial community when integrating other existing public cloud platforms finally some criticism suggests that the ogc web service standard is still too complex thus slowing the adoption rate of gws by endpoint users sun et al 2019 nevertheless we argue that this problem can be alleviated by existing technology like geosquare wu et al 2015 that interprets ogc web services with a readable user interface on the other hand the ultimate goal of ws4gee is to integrate gee with gws community rather than the selection of the most suitable gws interfaces another consideration is about the backward compatibility issue of the standard for instance although wps 2 0 2 standard is the latest version of ogc wps standard which offers two new operations getstatus and getresult to support asynchronous processing as well as immediate processing ogc 2018 it does not interoperate with wps 1 0 well as wps 1 0 0 is the most common used wps standard in current gws applications zhang et al 2019 it takes additional efforts to make wps 2 0 2 compatible with existing gws applications thus the adoption of the latest versions of such ogc standards would be the next step for the ws4gee system and the efficiency of different web service approaches such as soap simple object access protocol restful standards and others also require further exploration we would like to express our sincere gratitude to the reviewers and editors for their valuable comments that help improve this paper the work was supported by national natural science foundation of china no 41930107 we would like to express our sincere gratitude to the reviewers and editors for their valuable comments that help improve this paper the work was supported by national natural science foundation of china no 41930107 0 https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s1364 8152 23 00022 1 s1364815223000221 1 s2 0 s1364815223000221 10 1016 j envsoft 2023 105636 271872 2023 02 07t07 30 59 02982z 2023 03 01 2023 03 31 1 s2 0 s1364815223000221 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 main application pdf 296b36814c3846c8d2217ef0241703b3 main pdf main pdf pdf true 8927430 main 15 1 s2 0 s1364815223000221 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 preview image png bec78a73e2067fec554914906eafc959 main 1 png main 1 png png 62307 849 656 image web pdf 1 1 s2 0 s1364815223000221 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr7 downsampled image jpeg fb4f2e5c9a7ee91ba43bbf29d3eb8118 gr7 jpg gr7 gr7 jpg jpg 187379 503 691 image downsampled 1 s2 0 s1364815223000221 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr6 downsampled image jpeg 330b58c80a3a69f03d42752879fac6ab gr6 jpg gr6 gr6 jpg jpg 213586 1022 535 image downsampled 1 s2 0 s1364815223000221 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr9 downsampled image jpeg de80e8d504d28f89ed91ea17e87a26d2 gr9 jpg gr9 gr9 jpg jpg 218810 506 691 image downsampled 1 s2 0 s1364815223000221 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr8 downsampled image jpeg b0263a401eb071da1d2853ef8d3345c5 gr8 jpg gr8 gr8 jpg jpg 146662 416 691 image downsampled 1 s2 0 s1364815223000221 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr3 downsampled image jpeg 7ad536fa48d699290f64a68d6502c103 gr3 jpg gr3 gr3 jpg jpg 153767 464 691 image downsampled 1 s2 0 s1364815223000221 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr2 downsampled image jpeg 3f07d8d115e2d7b3a161b3dfaa03e5ad gr2 jpg gr2 gr2 jpg jpg 145915 258 691 image downsampled 1 s2 0 s1364815223000221 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr5 downsampled image jpeg e753baf97394089cf9b250dbfa960828 gr5 jpg gr5 gr5 jpg jpg 111719 538 691 image downsampled 1 s2 0 s1364815223000221 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr4 downsampled image jpeg 7f8e8f0c30ccba1d1bb3777bd62a46bb gr4 jpg gr4 gr4 jpg jpg 164542 410 691 image downsampled 1 s2 0 s1364815223000221 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr1 downsampled image jpeg bebf67ce1e8d898a436d728faf2a87a6 gr1 jpg gr1 gr1 jpg jpg 159741 516 535 image downsampled 1 s2 0 s1364815223000221 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr7 thumbnail image gif d6e4c3faa3a9317da5e2a74bca96aea8 gr7 sml gr7 gr7 sml sml 81998 159 219 image thumbnail 1 s2 0 s1364815223000221 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr6 thumbnail image gif 2c83dc4dc30ce29c0c6606c9dc923b2d gr6 sml gr6 gr6 sml sml 72352 164 86 image thumbnail 1 s2 0 s1364815223000221 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr9 thumbnail image gif f6349f03b550282fd0868ff12b781fac gr9 sml gr9 gr9 sml sml 86721 160 219 image thumbnail 1 s2 0 s1364815223000221 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr8 thumbnail image gif 67576c1fe8b96fe4e4626532b45e8d42 gr8 sml gr8 gr8 sml sml 76024 132 219 image thumbnail 1 s2 0 s1364815223000221 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr3 thumbnail image gif 82046b8b60da6ea4d6a1f53f89aedbe9 gr3 sml gr3 gr3 sml sml 76381 147 219 image thumbnail 1 s2 0 s1364815223000221 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr2 thumbnail image gif c6a11d5914710c6d0a40c55ad0d9e2f8 gr2 sml gr2 gr2 sml sml 76484 82 219 image thumbnail 1 s2 0 s1364815223000221 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr5 thumbnail image gif cd411b4c57f107aeda390ccdc73c0452 gr5 sml gr5 gr5 sml sml 72771 164 211 image thumbnail 1 s2 0 s1364815223000221 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr4 thumbnail image gif 19f34bc7bd20297dc1e7fd96893e5502 gr4 sml gr4 gr4 sml sml 79458 130 219 image thumbnail 1 s2 0 s1364815223000221 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr1 thumbnail image gif 15d32b63e08ab516776aa6240e8ee529 gr1 sml gr1 gr1 sml sml 82536 164 170 image thumbnail 1 s2 0 s1364815223000221 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 3e7081254c27522f341819a84bcb58e9 gr7 lrg jpg gr7 gr7 lrg jpg jpg 1062777 2227 3060 image high res 1 s2 0 s1364815223000221 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 02c109d4bc69780125efd148ea53e118 gr6 lrg jpg gr6 gr6 lrg jpg jpg 1058125 4528 2371 image high res 1 s2 0 s1364815223000221 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 4a6e2c29a720eea2e033be5701eba3bd gr9 lrg jpg gr9 gr9 lrg jpg jpg 1122145 2238 3059 image high res 1 s2 0 s1364815223000221 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 8b565dd4e4a214050144af660d40136b gr8 lrg jpg gr8 gr8 lrg jpg jpg 536298 1841 3061 image high res 1 s2 0 s1364815223000221 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 9fee5237ffa887566a122bad21d65c70 gr3 lrg jpg gr3 gr3 lrg jpg jpg 706652 2055 3061 image high res 1 s2 0 s1364815223000221 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 219362f9f44958d286d3d74610cd892c gr2 lrg jpg gr2 gr2 lrg jpg jpg 547745 1140 3059 image high res 1 s2 0 s1364815223000221 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg d31c6d017a48aa173d6218b8bd1e2871 gr5 lrg jpg gr5 gr5 lrg jpg jpg 392257 2381 3060 image high res 1 s2 0 s1364815223000221 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg afe3d48bff0f954d5307af1e0c47f746 gr4 lrg jpg gr4 gr4 lrg jpg jpg 776012 1816 3060 image high res 1 s2 0 s1364815223000221 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg f154d04132d2313a154d7a16609e14c6 gr1 lrg jpg gr1 gr1 lrg jpg jpg 549863 2288 2370 image high res 1 s2 0 s1364815223000221 mmc1 docx https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 mmc1 main application vnd openxmlformats officedocument wordprocessingml document f104124bee38b75124d051e9d38532f6 mmc1 docx mmc1 mmc1 docx docx 16551 application 1 s2 0 s1364815223000221 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 7b80882232830a66395fc5cbc9ffc276 si1 svg si1 si1 svg svg 47481 altimg 1 s2 0 s1364815223000221 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 8f9b2837929e49819726d38970605c2e si8 svg si8 si8 svg svg 16340 altimg 1 s2 0 s1364815223000221 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml f21f79b8dd8acad3beadfced7c4d3418 si3 svg si3 si3 svg svg 67302 altimg 1 s2 0 s1364815223000221 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 7fd5eb49967233c8dabbc3b60b2fc32b si5 svg si5 si5 svg svg 11813 altimg 1 s2 0 s1364815223000221 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 05f197a4f9cb50fd58e2645051dabe19 si4 svg si4 si4 svg svg 102353 altimg 1 s2 0 s1364815223000221 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 956d7cd9532d87b4f755309549747cae si2 svg si2 si2 svg svg 63949 altimg 1 s2 0 s1364815223000221 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 748b96a520e97478bfe0f1f12179edb5 si6 svg si6 si6 svg svg 36143 altimg 1 s2 0 s1364815223000221 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 492bbfd6dd6b78a7441226e807559787 si7 svg si7 si7 svg svg 33839 altimg 1 s2 0 s1364815223000221 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 10tzrwscj60 main application pdf 79032ee7449e96ae66b69ffa3be789b5 am pdf am am pdf pdf false 1343533 aam pdf enso 105636 105636 s1364 8152 23 00022 1 10 1016 j envsoft 2023 105636 elsevier ltd fig 1 interoperability among google earth engine ws4gee and geospatial web services fig 1 fig 2 detailed architecture context of proposed ws4gee system fig 2 fig 3 sequence diagram of workflow for ogc request and response in ws4gee fig 3 fig 4 gee enabled geoprocessing registration fig 4 fig 5 a conventional method to invoke wcs for the coverage of aoi four wcs services should be called to retrieve subset a d from images 1 4 b obtain the coverage of aoi within ws4gee strategies subset a d are automatically merged as one piece region and served as one wcs service fig 5 fig 6 ws4gee client geographic interface of gee enabled service registration a geospatial data registration and b geoprocessing service registration c service search and preview and d service access and invocation fig 6 fig 7 the use of ws4gee search engine for gee imageries acquisition a search engine user interface and wcs service generation b boundary and extent of wuhan city c wcs operations getcapabilities describecoverage getcoverage d visualization with true color band 4 3 2 fig 7 fig 8 developing geoprocessing workflow for the model in geosquare with descriptions for involved elements fig 8 fig 9 the resulting map of geoprocessing workflow with ws4gee for grassland environmental monitoring fig 9 table 1 core operations in ws4gee workflow table 1 module operation description client serviceblock filter request check input ogc request and its operation type send request permit an ogc request to the server by its operation type receive response return an ogc response from the generator which might be an xml document or data file parser retrieve attr get required variables from gee model these variables will be assigned by parser in order convert to ee vector convert input vector to gee object convert to ee image convert input raster to gee object it will check input data resource if the input data resource belongs to gee it can be assigned to gee variable directly convert by ee cloud upload data from an ogc request to google cloud an identifier of this resource is returned which facilitates resource import to gee environment generator generate service description generate service descriptions for getcapabilities describeprocess describefeatures and describecoverage operations based on the service type generate service outcome generate results in xml format or files based on ogc request wps wfs wcs and wms and specific operation table 2 data type mapping between ogc web services and gee table 2 ogc web services gee literal data structure ee computedobject float double integer boolean string ee number ee string complex data structure ee computedobject vector shapefile geojson gml etc ee geometry ee feature ee featurecollection raster tiff geotiff etc ee image ee imagecollection text txt etc ee string other structure ee computedobject boundingbox data structure bbox envelop etc ee geometry timestamp ee date table 3 geospatial web services used for the grassland environmental monitoring table 3 service name format service source color in fig 8 fvc calculation model wps google earth engine brown precipitation layer vector data file csv chen et al 2018 work green aw3d30 dem wcs google earth engine brown rate soap geosquare blue imar boundary wfs geoserver red gwr soap geosquare blue add join soap geosquare blue zonal statistics as table soap geosquare blue table 4 comparison of the processing time between geoprocessing workflow with gee enabled geospatial web services and local desktop based processing table 4 processes local desktop geoprocessing e g arcgis gee enabled wps for fvc model major hardware configurations cpu speed 2 2 ghz memory ram 8 gb disk space 40 gb network speed 5 mb s not required data preparation including acquire landsat imageries and pre processing such as clip and cloud mask download 130 tiles on average for the study area per year each tile contains two bands red near infrared assume that we use the batch mode downloading and pre processing one image around 200 mb needs 1 min and other pre processing takes 3 min not required to upload boundary filethe total time cost is 1 3 130 3 year 1560min 26 h not required to download tiles required to upload boundary file 1 mb less than 1 min the total time cost is 1 min model execution including ndvi calculation imageries reduction and fvc calculation it takes about 10 min to calculate and composite for annual maximum ndvi imagery for each tile and then extract fvc for each yearthe total time cost is 130 10 1300 min 21 67 h the fvc model is executed by gee cloud computing the mapreduce performs all computation for tiles simultaneously gorelick et al 2017 the total time cost is 5 min retrieve results not required to export results required to export result from gee 20 min for each year summary the total time cost is around 48h the total time cost is less than 1h ws4gee enhancing geospatial web services and geoprocessing workflows by integrating the google earth engine jianyuan liang fengying jin xianyuan zhang huayi wu state key laboratory of information engineering in surveying mapping and remote sensing liesmars wuhan university 129 luoyu road wuhan hubei 430079 china state key laboratory of information engineering in surveying mapping and remote sensing liesmars wuhan university 129 luoyu road wuhan hubei 430079 china state key laboratory of information engineering in surveying mapping and remote sensing liesmars wuhan university 129 luoyu road wuhan hubei 430079 china corresponding author handling editor daniel p ames geospatial web services gwss assist scientists developing models online cloud computing technologies have created opportunities for faster and more efficient environmental modeling among of the new public cloud products google earth engine gee is particularly well adapted to meet the needs of environmental scientists developing and sharing their models however the potential of gee to support geospatial community is less well explored scientists are still waiting to see what public cloud platforms can do for geospatial web services this paper proposes a prototype called ws4gee that can wrap gee embedded datasets functions and models as ogc web services such as wms wfs wcs and wps in this way ws4gee can support direct access to gee through ogc interfaces thus allowing geoprocessing workflows to use gee resources two experiments are presented one with a gee enabled geospatial data service and the other deploying geoprocessing composition with gee enabled gwss the results confirm that ws4gee with the support of gee significantly enhances the capabilities of gws to deal with labor intensive and time consuming tasks this study provides guidance for the wider geospatial community when integrating other existing public cloud platforms keywords google earth engine geospatial web service geoprocessing workflow ogc standard service interoperation data availability data will be made available on request 1 introduction the rapid development of internet and big data has promoted the spread of web services significantly with the advancement of web service technology tens of thousands of geospatial web services gwss were developed and available online including spatial records datasets and functions following the model as a service concept different web based models were also implemented as geospatial web services li et al 2017 geospatial web service can be divided into geospatial data service and geoprocessing service where the term geospatial data service refers to geospatial services for spatial data acquisition while geoprocessing service is used to describe any function or model for processing geospatial and related data yue et al 2016 driven by the service oriented architecture soa many spatial data infrastructures sdis or cyberinfrastructures were developed for storing and sharing geospatial web services in a distributed and interoperable environment barik et al 2018 sun et al 2019 today it is common for earth scientists to perform environmental modeling online with gws gan et al 2020 gichamo et al 2020 sun et al 2019 the distributed geospatial data services and geoprocessing models or functions accessible through web service interfaces can be coupled to create more powerful and added value services yue et al 2015 zhang et al 2020b which is referred to as geoprocessing workflow or geospatial services composition the open geospatial consortium ogc has published a series of standards by adapting or extending the common web service standards to facilitate the integration of siloed and legacy geospatial web services lopez pellicer et al 2012 reed 2011 several well known products like wms web map service wfs web feature service wcs web coverage service csw catalog service for the web can provide standardizing service interfaces and data models meanwhile any environmental model or geospatial algorithm defined as geoprocessing service can be accessed through a web processing service wps interface foerster and stoter 2006 li et al 2017 in addition other standards like soap simple object access protocol or rest representational state transfer can also serve a similar purpose in several scenarios these standards enhance the interoperability among geospatial services and reduce the complexity when composing these services environmental scientists have already realized the advantages of gws and geospatial service composition they have come to expect services with higher quality and efficiency among several new developing information technologies cloud computing has become a potential alternative to satisfy this requirement the cloud brings powerful computing capabilities to end users by shifting computing resources to the cloud and capable of handling tasks with a large amount of data and services compared to conventional sdis or other cyberinfrastructures in the c s mode the cloud provides more rapid elastic and efficient on demand resources to help scientists publish their services on the internet yue et al 2013 zhang et al 2019 after years of development four categories of cloud deployment models were proposed the private cloud community cloud public cloud and hybrid cloud zhang et al 2019 the public cloud has caught the attention of many scientists because it is free or sold on demand and still inherits most of the characteristics of the cloud technology based on the public cloud product the google cloud google earth engine gee platform has been established and free to use for environmental scientists gee has contained a multi petabyte analysis ready data in the cloud and a set of application programming interfaces apis in its client library to perform geospatial analysis online gorelick et al 2017 consequently gee attracts much attentions from environmental scientists who used to traditional software centered and desktop based environmental modeling liang et al 2020 following this trend some other public cloud platforms for environmental scientist like microsoft planetary pie engine and geobrain cloud zhang et al 2019 have been also released or in the beta in summary gws has gained long term process in recent years and continues evolving along with new technologies which reflects in the following four aspects 1 web service technology facilitates the emergence and wide spread of gws 2 heterogeneous gwss can be coupled and integrated as geoprocessing workflow for data sharing and interactive operation based on standard specifications 3 cloud computing technologies injects new vitality into geospatial community and provides reliable and powerful resources for geospatial applications 4 the emergence of public cloud based geospatial analysis platforms such as gee leads to new application mode to perform environmental modeling online with sharable and analysis ready data and functions based on elastic cloud computing resources nevertheless even with the aforesaid tendency and the advancement of the cloud technology for geospatial analysis three obstacles could still hinder their wider use technical complexity the gap between user communities and public cloud platforms and appropriate methods to connect the cloud based services to geospatial workflows firstly the technical complexity must be overcome if these cloud based platforms are to be used effectively and easily taking gee as an example users should be familiar with javascript or python programming language to invoke client library apis even simple syntax or logical errors could easily result in an entire geospatial analysis failure this is extremely inconvenient for environmental scientists who are used to desktop analysis software that has user friendly geographic windows and user interfaces ui the subsequent obstacle concerns the gap between current gws communities and public cloud platforms for instance gee manipulates every process on the server cloud side this activity ranges from a simple assignment expression to complex functions gee implements the entire process in one space instead of by combining heterogeneous services together any interaction with external resources or services requires massive manual effort to deal with data transmission from one place to another especially when the data source or the outcome of a gws is the input of the gee model or the output of gee is the input for other services however it would be a huge waste to re develop existing geoprocessing or models on the cloud platform and sometimes even too complicated or even impossible although a number of datasets and processes functionalities are available in the cloud it is difficult to utilize these resources beyond the gee environment and impossible to apply gee for geoprocessing workflows combining because of a lack of appropriate methods to connect them with other gws outside the cloud therefore bridging the conventional gws with the cloud based geospatial service solution could be advantageous to the entire geospatial community we alleviated these three problems and made full use of existing public cloud platform to benefit gws community choosing gee as the experimental platform since gee is widely used by environmental community and more mature than other public cloud platforms a ws4gee framework and prototype are proposed to explore approaches to make gee interactive with existing gws community specifically gee resources are generated as gee enabled geospatial data and gee enabled geoprocessing services we selected ogc web services ows as the implementation standards for ws4gee while ogc wms wfs wcs and wps standards were implemented for data resources functionalities and models in gee the experiments show that ws4gee allows environmental scientists to take advantage of gee and its cloud computing capability and access resources as gwss the remainder of this paper is organized as follows section 2 introduces the related work in detail section 3 describes the specific architecture of ws4gee major module implementation and a prototype is proposed in section 4 in section 5 two experiments illustrate the capability of ws4gee section 6 discusses the contributions limitations and common issues of ws4gee and some conclusions are drawn in section 7 2 related work 2 1 existing geospatial web services gws uses large volumes of geospatial data and functionalities in a flexible manner and alleviates the pressure of tasks by utilizing the combined power of distributed services in the network wright 2011 compared to the traditional approach that completes all tasks on a personal computer it enables broader remote participation encourages collaboration and improves research reproducibility chen et al 2020 gan et al 2020 many web based geospatial applications or sdis are developed for using geospatial data geoprocessing services or both for instance geoss global earth observation system of systems implements a system of system architecture based on the web as a platform nativi et al 2015 zhang et al 2019 a set of independent earth observation information and processing resources with contributions from countries and international organizations in the group on earth observations geo are coordinated to facilitate access monitoring sharing of global environmental data and information via gws a geoportal named geospatial one stop gos was designed to promote communication and sharing of geospatial resources goodchild et al 2008 the amount of gws resources has significantly increased yue et al 2016 zhao et al 2012 according to gui et al 2016 more than 41 000 geospatial data services containing over 300 thousand layers were available 2 2 geoprocessing workflow and interoperation when plenty of gwss are available online researchers start to integrate different services to meet complex application demands geoprocessing workflow defines a process chain to accomplish this goal each process works as a component in the workflow for one or more goals synchronously or asynchronously tan et al 2015 many conventional desktop systems provide a user friendly interface to construct process chaining such as modelbuilder in arcgis and spatial modeler in erdas to generate workflows yue et al 2015 gws is the process unit for geoprocessing workflow in a web environment users aggregate spatiotemporally distributed gwss as a geospatial service chain however the gws combination process might involve different standards such as ogc soap rest sun et al 2019 service selection yue et al 2007 and service collaboration wu et al 2015 dealing with these issues is a complicated task for the users without gws background many attempts have been made to reduce the complexity of building geoprocessing workflow wu et al 2015 proposed a system design called geosquare with a collaboration mechanism for flexible geoprocessing yu et al 2012 utilized the business process execution language bpel to describe workflows and enabled the proper execution and coordination of geospatial workflows through standard geospatial web services e g wfs wps zhang and peng 2013 sun et al 2012 developed geoprocessing workflow modeling builders called geojmodelbuilder and geopwtmanager respectively these systems are not only capable of designing and executing geoprocessing workflows but also support service visualization and monitoring the web based user interface makes gws readable and simplifies the development of geoprocessing workflows additionally geoprocessing workflow can be enhanced by integrating them with other modeling frameworks like openmi yue et al 2015 or provenance models like w3c prov jiang et al 2018 zhang et al 2020b these works indicate that geoprocessing workflow approach with assistance of appropriate system or framework could be a promising option for constructing integrated models 2 3 cloud computing for geospatial application cloud computing plays an essential role in the geospatial community zhang et al 2019 it relieves users from conventional limitations such as software installation data management and local computing resource requirement by shifting the computing resources to the cloud the cloud benefits geospatial applications for data storing and retrieval wang et al 2019 geoprocessing and spatial calculation jiang et al 2021 yang et al 2017 service discovery and maintenance blower 2010 evangelidis et al 2014 and service sharing zhang et al 2019 thus geospatial analysis or the deployment of geospatial web services is widely performed on the cloud e g aws azure google cloud in environmental modeling the integration of cloud computing and geospatial applications usually comes up with specific cloud infrastructures and web services standards for instance tan et al 2015 designed a construction of an elastic parallel ogc wps service on a cloud based cluster to provide a higher performance wps service that uses fewer computing resources astsatryan et al 2015 proposed a gateway for large scale timeseries ndvi calculation in the cloud where geoprocessing can be published as wps zhang et al 2019 presented a cloud based wps framework that provides a general solution to integrate models in the cloud these works expand the capability of cloud computing perform geospatial analysis despite advantages of the cloud existing cloud based geospatial studies primarily focus on architecture design or specific topics rather than a general solution for the geospatial community in most cases environmental scientists still enjoy less benefit from the cloud technology because they have to develop configure and deploy their geospatial services in the cloud from the beginning 2 4 environmental modeling in google earth engine unlike previous cloud based geospatial applications gee provides a group of pre defined functions and datasets for environmental modeling initially these functions and datasets facilitate straightforward mapping for environmental modeling or applications especially in large time and space scales land use and land cover lulc problems angern et al 2021 chen et al 2017 hu et al 2018 huang et al 2017 liu et al 2018 teluguntla et al 2018 to make gee extendable and reusable many gee models are wrapped as toolboxes or packages so that they can be used as black boxes in gee this approach can hide technical issues and allow users to access gee models in gee environment with simple import and thus gradually becomes one important approach for environmental modeling in gee for instance liang et al 2020 developed two modeling tools to enable researchers to go beyond gee mapping capacity for comprehensive urban sustainability modeling zhang et al 2020a proposed a toolkit for agricultural land use modeling called agkit4ee that helps conduct geospatial agricultural and environmental modeling with cdl cropland data layer data in a simple way mhawej and faour 2020 built the sebaligee system for 30 m evapotranspiration rates retrieval based on the gee client library and a model called surface energy balance for land improved sebali similarly a gee based fractional vegetation cover fvc model was presented to estimate the vegetation cover over the southern african rangelands vermeulen et al 2021 and a gee enabled python toolkit called coastsat was introduced to retrieve shorelines in any user defined region of interest vos et al 2019 all these gee models are ready for use with simple interfaces or entrances other gee developers can still access the source code of these models or packages as gee makes code available for the developers nonetheless although user defined models can be shared in gee environment it is still complex to integrate them into other geospatial interfaces unlike cloud based applications that enable access with gws interfaces gee models or a simple function should entirely rely on the gee environment using gee enabled models is promising but it is still a challenge for environmental scientists to integrate other geospatial services with gee based services and it is also time consuming to perform each involved process manually in different execution environments 3 system design 3 1 design principles the fundamental objective of this study is to explore a framework called ws4gee to integrate gee resources as two categories of geospatial web services gee enabled geospatial data service and geoprocessing service fig 1 shows the relationship among google earth engine ws4gee and other geospatial web services we notice that gee contains abundant resources and computing power on the cloud and that there is a gap between gee and common geospatial web services because of access constraints runtime environmental isolation and other technical issues therefore the design principle bridges gee and other geospatial web services through gws interfaces among different gws interfaces we selected ogc standard compliant interface as our target implementation because of its applicability compatibility and common use li et al 2016 the interoperability between gee and other gws contains three levels as seen in fig 1 1 embedded datasets gee functions and models are accessible through ogc interfaces 2 gee enabled geospatial data can be utilized as complex data inputs for gws and vice versa 3 geospatial web services and gee enabled gws can cooperate in geoprocessing workflow by realizing these levels for interaction gee could be connected with geospatial web services and technical issues will be hidden for the interaction 3 2 system architecture this study proposes a ws4gee system to facilitate publishing gee data resources and gee functions models as geospatial web services and geoprocessing within ogc standards generally ws4gee implements wms wfs wcs and wps standards to data and functionalities in gee these standards have defined interfaces to retrieve valid operations and parameters and obtain an xml document that fully describes the requested data or processes thus achieving specific kinds of data in well known formats or to execute processes above all satellite imagery is the major data resource in gee different imagery datasets landsat modis sentinel 2 etc can be registered as wcs and grouped by imageries categories and periods functionalities in the gee client library and models developed with these functions can be registered as wps common used operations from ogc standards including wms 1 1 wfs 1 1 wcs 1 1 1 and wps 1 0 can also be implemented with the same approach ws4gee relies on a multiple component structure to establish interaction between gee and gws the architecture of ws4gee can be divided into two blocks the client client service block and server block along with the gee platform environment and gws environment the gee platform environment provides the original resources of data functions and models and the gws environment represents the gws users activities that utilize standardized services and service composition for geospatial tasks this framework aims at facilitating service transformation between geospatial web services and gee functionalities as shown in fig 2 the system consists of three components the graphical visualization and interaction component the resource encapsulation and publication component and the system management and configuration component the client block is based on the graphical visualization and interaction component which provides the gateway of the framework it allows users to send requests and receive responses from the server block through standard ogc interfaces a web based user interface provides the access point for invoking available gee enabled geospatial data and geoprocessing services ogc requests are received in xml format and assigned to specific service types thereafter the completeness and correctness of url and xml documents are checked through validation an exception is returned directly if they fail validation otherwise the request is sent to server block the server block is responsible for the interaction with the gee platform and service management several modules are developed in the resource encapsulation and publication component to handle the request from the client block and establish gee enabled gws from the original gee resources the ogc requests from the client block interact as the gee enabled gws and target gee resources are acquired by gee script templates that are pre defined in the server and invoked as service instances in the execution module during this process two core modules the generator and parser modules facilitate the conversion between gee script templates and gee enabled gws the generator wraps the gee results or generates a response compliant with the request service interfaces likewise the parser interprets ogc requests to obtain information required function parameters etc and translates this information into specific gee scripts to instantiate specific gee script templates for execution in addition the generation or creation of gee enabled geospatial web services is supported by the strategy module these modules form the workflow for the interaction with gee platform but still need to be controlled to ensure reliability and stability the system management and configuration component is designed to manage the entire procedure in the ws4gee server block the runtime module provides an initial environment to handle gee scripts the monitor module checks the status for every process a process will be retried if systematic error e g connection character encoding or decoding occurs in addition although ws4gee depends on gee to execute scripts and achieve results the storage and cache modules are developed to keep basic information for ogc requests and small capacity responses a fake call mechanism sun et al 2019 is set up in cache module to reduce the response time cost of frequently accessed services for efficiency once a request is made ws4gee will check whether a same request was made before if an identical request is found in the cache the result will be returned directly without executing gee scripts again for instance when a user calls the describecoverage operation defined by wcs to obtain the description of an image a response xml document will be generated after running a list of gee scripts the first time basic information like the extent coordinates and timestamp that uniquely identify the service as well as the response document are stored in the ws4gee once a request with the same identifiers is called the xml response will be sent to the end user from storage rather than reconstructing the same response from the gee 3 3 workflow the workflow describes the underlying logic of ws4gee from an external ogc request to the gee enabled service execution and the generation of an ogc response in the workflow the client service block is responsible for passing valid ogc requests and receiving ogc responses in the server block a set of modules including the gee enabled gws parser generator and the cache work together to generate ogc responses from gee or the ws4gee storage fig 3 ws4gee implements the workflow by invoking a sequence of internal operations from the different modules in the client service block filter request and send request are called to filter and send ogc requests to the server block the gee enabled gws module identifies the ogc requests and allocates them to the cache or parser module an ogc response will be directly returned if the same request is found in the cache otherwise the parser will parse the ogc request and initialize a gee task based on the information extracted from the ogc request the input parameters are fetched from ogc request and assigned to variables in the gee model with the retrieve attr operation simple literal inputs will be immediately assigned to gee variables while complex inputs will be translated through operations such as convert to ee vector or convert to ee image into gee variables the gee task relies on a registered script template either for data resources gee models or functions in the execution module a specific script template is loaded by the reflection mechanism once all the variables are set up the relative gee function or model is run in gee ws4gee retrieves gee results and generates an ogc response through the generator module for operations like describecoverage or describeprocess that aim to retrieve basic information from the service a response with xml format will be generated from generator modules after running pre defined gee script templates with generate service description for operations like the execute operation in wps that have data file return vector raster etc a generate service outcome operation can be invoked based on the primary ogc request these major operations in the workflow are listed in table 1 the workflow illustrates how gwss interact with gee in ws4gee framework it also shows the minimum requirements to implement a ws4gee prototype including the client service block and at least five modules for the server block users can develop other modules such as the monitor and the storage to build a more robust and stable system 3 4 core modules this section introduces the core modules of ws4gee that facilitate the interaction between gee resources and the gws as mentioned previously existing data resources on the gee platform can be accessed by wms wfs or wcs meanwhile gee based models and functions can be invoked as a wps and external gws can participate in gee enabled geoprocessing as gee input parameters the core modules include registration strategies and execution 3 4 1 registration ws4gee registration includes processing and temporary geospatial data registration components the processing registration component supports manual registration of user defined gee functions and models as wpss during the registration process the input output specifications descriptions of user defined functions and package of source scripts are defined and saved as gee script templates which will facilitate matching of the input output parameters and parameter types defined in gee to complex datatype objects in the wps the relationship between processing registration gws and gee is illustrated in fig 4 and a web based user interface is provided to simplify the registration the temporary geospatial data registration benefits the transmission of data sources to the gee in a wps execution task accessible resources like wfs wcs or a geospatial data file with a specific suffix such as tiff geotiff or geojson could be introduced through a url in a wps request as a complex data input reference previously it was a tedious task to use external geospatial data in the gee as a list of manual steps was required convert the format of the geospatial data upload them to the gee environment and then import this data as parameters for gee scripts to overcome this constraint ws4gee registration acts as an agency to complete all the necessary steps a copy of the geospatial data in a gee supported format will be created and registered as a data source on the google cloud with a unique identifier since gee can import data from the google cloud this entire process is performed automatically without manual intervention this copy is temporarily stored during the execution of wps request and destroyed after this process is finished 3 4 2 strategies the gee includes a tremendous amount of satellite imagery so wcs has become the primary gws to leverage these assets but accessing these images through wcs is still a challenge for instance landsat 8 satellite circles the earth around every 16 days and produces approximately 57784 image tiles more than 1 million images are collected each year but scientists are only interested in a specific research area or period thus an area of interest aoi is frequently comprised of a combination of several slices from two or more images in this case operations like describecoverage or getcoverage are called multiple times to access subsets of an aoi fig 5 a the more images are involved the more complicated this process thus two strategies were designed in ws4gee to resolve these issues first the response document of wcs operations can be generated in real time users obtain response documents from getcapabilities and describecoverage operations based on their aoi when the aoi is a subset of one image the description of the aoi will depend on the imagery otherwise it will be composed of two or more images second the imagery can be generated dynamically as fig 5 b shows a composite image of the three involved parts distributed across four images will be generated once the getcoverage operation is called since the wcs do not exist in ws4gee originally the user must provide the input criteria such as the aoi time range and so on to generate dynamic wcs 3 4 3 execution the execution module consists of two major processes mapping and execution to build a bridge between gee resources and gwss ws4gee focuses on mapping data types since gee has a separate variable system where variables are wrapped with proxy objects on the server side gorelick et al 2017 for instance a getcoverage operation requires the parameters bbox boundary box height and width to extract the coverage this process matches the function ee image clip on the gee where a geometry is required to execute the clip thus these ogc parameters are converted to ee geometry object to involve gee scripts the mapping for wps is more complicated as a request might include specific disparate data structures such as complexdata literaldata or boundingboxdata hence a mapping between the common data structures of ogc web services and gee data objects are listed in table 2 ws4gee relies on gee platform for service execution different gee scripts are prepared to handle various ogc requests these scripts serve as execution templates in ws4gee and become active after their variables are filled up through the mapping process ogc operations such as getcapabilites are common tasks implemented in ogc standards they share the same template the user need only to pass an ogc request to ws4gee no technical details are required and the complexity of service execution is hidden the interaction between ws4gee and gee platform is invisible to end users with the mapping and execution ws4gee offers the environment that connects gws and actual scripts running on the gee 4 implementation to validate the system design in this study a ws4gee prototype was constructed based on the aforementioned principles and framework the prototype consists of two sub systems called ws4gee client and ws4gee server manager respectively the link of the entire prototype with a comprehensive tutorial are provided in appendix a the ws4gee client and ws4gee server manager were developed following the system architecture as described in section 3 2 to facilitate the ui only users to create and access these gee enabled geospatial web services the web based user interfaces for service registration search and preview and invocation were realized shown in fig 6 a d the registration window is prepared for geospatial data and geoprocessing services registration based on the registration module and dynamic service generation strategies as detailed in section 3 4 1 and section 3 4 2 the search and preview window is used to check available gee enabled gwss and visualize geospatial data services in the system and the invocation window with examples is developed to help users access gee enabled wps in the client side the ws4gee server manager is the core realization of the server block and handles the interaction between gee and the client side for the rapid prototyping purpose python 3 was selected as the programming language for the ws4gee server manager as gee supports to invoke the client libraries in python environment offline in the prototype a few frequent used gee original datasets and client library functions were pre registered with executable script templates and ready to use as ogc services their service descriptions and other basic information were also retrieved from the gee portal or api documentations and stored in ws4gee for efficiency the actual operation to access relative gee resources is hidden for users based on the script templates mentioned in section 3 4 3 5 experiments to evaluate the utility and efficiency of ws4gee as well as importance of integrating gee with the gws community we applied the ws4gee prototype in two cases the first example illustrates one common use of the dynamic generation strategy when visiting gee imagery within a wcs the aim of the second example is to demonstrate that ws4gee facilitates the integration of gee enabled geospatial data and geoprocessing services with other heterogeneous geospatial web services this case study involves two tasks detection of grass vegetation coverage and spatial analysis in addition a geoprocessing service management system geosquare was applied for workflow conduction and visualization wu et al 2015 5 1 acquiring imageries through dynamic gee enabled wcs to demonstrate the capability to acquire gee resource within wcs we used the ws4gee user interface to get a set of images of a designated region fig 7 a the study area was wuhan city 30 593 n 114 306 e in china the goal was to obtain images of this region in 2020 from the landsat 8 sr dataset the region includes portions of three images path 122 123 row 38 39 fig 7 b it is wasteful however to download an entire set of images for this region during a particular time frame to splice together a single composite image of the study area the ws4gee prototype solved this problem by implementing the dynamic generation strategy the web based user interface requires input datasets filter options e g by months by year by mean no cloud bands and the boundary of region to design a dynamic wcs fig 7 a here our target is monthly no cloud imagery for wuhan city a list of wcss specified for wuhan city are created based on the dynamic generation strategy wcs operations such as getcapabilities describecoverage and getcoverage can be called fig 7 c the target imagery can be visualized either in local space or in ws4gee user interface that enables bands composition based on wms standard fig 7 d this example shows that ws4gee hides platform based and coding requirements and it enables direct access to gee resources through common ogc standards the dynamic generation strategy makes gee enabled wcs demand oriented this significantly enriches the data sources for environmental scientists and relieves them from the labor intensive and error prone tasks of downloading and processing datasets 5 2 grassland environmental monitoring within a hybrid service composition monitoring grassland ecosystem environment is critical for certain governmental departments assume that scientists want to evaluate the ecosystem condition of temperate typical grassland in xilingol league imar the northern border of china during 2010 and 2017 and investigate the spatial correlation of different grassland regions among the impact factors precipitation and elevation they design a grassland ecosystem investigation model to accomplish this task firstly the model quantitatively measures the vegetation coverage change between two time points using fractional vegetation coverage fvc wu et al 2014 as calculated from the normalized difference vegetation index ndvi extracted from satellite imagery then a spatial analysis algorithm geographically weighted regression gwr is applied to detect patterns of spatial correlation equations 1 4 for this model are shown below 1 n d v i n i r r e d n i r r e d 2 f v c n d v i n d v i s o i l n d v i v e g n d v i s o i l 3 f v c c h a n g e f v c a f t e r f v c b e f o r e 4 y i β 0 β 1 x 1 i β 2 x 2 i β n x n i ε i β i x t w i x x t w i y where in equation 1 the ndvi is calculated from the visible red and near infrared light nir reflected by vegetation in equation 2 f v c is the fractional vegetation cover in the mixed pixel n d v i s o i l is the ndvi of bare soils that are not covered by vegetation and n d v i v e g indicates the pixels that are completely covered by vegetation equation 3 calculates the change between two points in time equation 4 presents the regression models that underlie gwr in which w i is a matrix of weights specific to location i such that observations closer to i are given greater weight than observations further away the scientists need to access the landsat 5 8 surface reflectance image collection to find datasets to create ndvi and fvc images raster data from aw3d30 global digital surface model products for elevation vector layers for potential impact factors such as precipitation and the boundaries for the xilingol league study area however there are three challenges to overcome 1 the xilingol league covers a total area of approximately 199 776 km2 of grasslands from west to east which covers 23 tile grids path 122 128 row 27 31 in the landsat dataset the time used to download clip and merge a set of images should be considered 2 calculating the annual fvc is usually based on maximum ndvi composition which is reduced from a series of ndvi calculated from each landsat image the annual ndvi image for the xilingol league area is composed of the maximum value at that location calculated from all the images from july to september this task also takes up much manual effort and time 3 even if the other two issues are overcome the scientists still have to deal with data transmission problems if the datasets are distributed or processed in different software environments this study resolves these challenges through ws4gee in the preparation stage a number of heterogeneous gws including the gee enabled geospatial data and geoprocessing services geopw yue et al 2010 wps geoserver wfs soap services wu et al 2015 and ecological csv file chen et al 2018 were involved table 3 we retrieved the fvc images from the fvc calculation model realized in gee and registered as a wps in ws4gee the fvc calculation model performs a group of functions to extract a fvc image from the original satellite imagery for this case study about 130 images from the gee landsat 5 8 surface reflectance image collection were acquired and clipped with the clip function then all the clouds were masked by the internal masks2clouds function ndvi images were calculated for each image with the gee defined normalizeddifference function and stacked to create a single annual maximum ndvi image using the ee reducer max function the fvc was calculated based on ndvi values thereafter the fvc data participated in gwr as the dependent variable while two potential ecological impact factors elevation and precipitation were explanatory variables the elevation was acquired from the aw3d30 products in google earth engine using an embedded wcs request to ws4gee the vector data for precipitation was created from the ecological csv file these distributed gwss were generated as geoprocessing workflow for execution the entire geoprocessing workflow is built and run based on geosquare web based interface fig 8 the shapes and colors represent different types and sources of elements the circle represents geospatial data service the rectangle is geoprocessing service and the bold red circle represents final output gee enabled gws and the soap services are marked in the brown and blue color respectively this study produced two set of outputs for fvc change and gwr analysis as the bold red circles shown in fig 8 our results show that apparent coverage reduction appears in the southwestern and eastern parts of xilingol league during 2010 2014 and after 2014 the coverage in these regions increased more or less while much degradation occurs in the center of the study area fig 9 a b the result of gwr indicates different spatial correlation between dependent variable fvc and explanatory variables elevation precipitation fig 9 c d the elevation has a positive impact to the coverage in the southwestern part of the xilingol league but this correlation is negative in the northeastern part we could download the fvc change and spatial distribution diagram of two gwr explanatory variables to present our results shown in fig 9 furthermore this study focuses more on the performance when integrating gee into gws composition one significant evaluation is to measure the benefits that ws4gee brings to the individual users who have less computing or storage resources for geoscience research thus we compared the time cost for calculating fvc when using gee enabled wps through ws4gee with the time costs when doing the same tasks locally we executed geoprocessing services with geospatial data on a local desktop the local desktop scenario required downloading and data pre processing as well as execution tasks for the fvc model the data pre processing and the task execution were performed in the arcgis desktop 10 6 environment thus the local desktop required 2 2g for cpu main frequency and 8g of ram for arcgis desktop according to the esri official recommendation and additional 40 g for data storage besides we configured the network facility that allowed a download speed of 5m s and assumed the downloading condition was stable for the ws4gee based scenario the hardware configurations was not required since gee has provided free computing resources to perform the fvc model for noncommercial and research use gorelick et al 2017 and the cost of google cloud storage services was based on the consumed storage amounts the details and descriptions of these two processes are summarized in table 4 the table illustrates that downloading time series landsat images costs a tremendous amount of time which is not required in gee enabled gws desktop processing such as applying the image cloud mask clipping merging and ndvi and fvc calculations also take up more time than do the same tasks on the gee that s because the entire study area covers a large area and multiple images collected over a couple of years performing these processes requires more computing resources although the gee enabled gws takes additional time to upload the required data and download results its time cost is apparently lower than downloading the required dataset to a local environment while the uploading and downloading speed relies more on the bandwidth of the network and the condition of network facilities on the client side in short the gee enabled gws can benefit the geoprocessing workflow with efficient computing power for geoprocessing services and it is the supplement of geospatial data source etc without ws4gee these powerful resources will remain under utilized 6 discussion this section is organized to 1 summarize the contributions of ws4gee to the gee and gws communities 2 address limitations on the current implements 3 discuss four potential concerned topics by vendor scientific user and ws4gee developers 6 1 added value of ws4gee the ws4gee can build a bridge between gsw and an emerging cloud based platform google earth engine firstly both gee enabled dataset and processing gee functions models are published as gws within ogc standards hence they can easily interact with other gwss as illustrated in the experiments gee enabled wcss could participate in a wps service as its complex input and a gee enabled wps service can be integrated with other gwss in a geoprocessing workflow by integrating gee into gws the time cost for several tasks that demand high computing capacity could be reduced from a few hours or days to even a few minutes which greatly enhances the capabilities and promotes broader use of gws secondly ws4gee can enlarge the application scope of gee since it is unreasonable to expect gee to contain all functionalities applicable to any research problem it becomes significant to integrate gee with existing gwss in this regard ws4gee can relieve scientists from the complexity of the integration and avoid the technical complexity like the long learning curve of using gee and other issues such as time consuming manual operations once user defined gee models are wrapped and released as gsws they become accessible to every scientist through the network and are not restricted to developers in gee environment thirdly wrapping gee resources including dataset and processes functions and models as geospatial web services also benefits the gws community as previously said few studies treat gee as a resource base for gws because the gee environment is isolated from the gws community since gee has gathered thousands of datasets from their original providers obtaining a dataset through gee can save time in data queries and acquisition likewise functionalities and models in gee provide more choices to solve complicated environmental issues finally some criticism suggests that the ogc web service standard is still too complex thus slowing the adoption rate of gws by endpoint users sun et al 2019 nevertheless we argue that this problem can be alleviated by existing technology like geosquare wu et al 2015 that interprets ogc web services with a readable user interface on the other hand the ultimate goal of ws4gee is to integrate gee with gws community rather than the selection of the most suitable gws interfaces another consideration is about the backward compatibility issue of the standard for instance although wps 2 0 2 standard is the latest version of ogc wps standard which offers two new operations getstatus and getresult to support asynchronous processing as well as immediate processing ogc 2018 it does not interoperate with wps 1 0 well as wps 1 0 0 is the most common used wps standard in current gws applications zhang et al 2019 it takes additional efforts to make wps 2 0 2 compatible with existing gws applications thus the adoption of the latest versions of such ogc standards would be the next step for the ws4gee system and the efficiency of different web service approaches such as soap simple object access protocol restful standards and others also require further exploration 6 2 technical limitations the current release of ws4gee still has some limitations from a software development point of view the current system was just a prototype to validate feasibility of integrating gee with wider gws community richer functionalities are required for the ws4gee system we only wrapped part of the gee resources the integration of certain complex functionalities such as machine learning are still unfinished because of the complex types of parameters specific data structures should be developed to utilize these functionalities as a web service moreover gee apis and functions can be implemented with either javascript or python thus we should provide a javascript version of ws4gee for javascript models as well meanwhile building a more comprehensive human computer interaction hci environment for users should also be considered for better use experience fortunately several packages are already available for this issue for instance the geemap python package provides functionality to analyze and visualize earth engine datasets interactively wu 2020 thus the integration of geemap into ws4gee client side could be a potential solution from a research point of view service collaboration and high quality geoprocessing workflows are critical presenting new challenges to the current ws4gee how can we make comparisons between a gws provided by a gws server and gee how do we monitor the qos of gee enabled gws additionally what are the criteria when replacing a gee enabled gws with other gws if it does not work or runs overtime many questions remain to be answered 6 3 development topics finally it is of great importance to discuss issues that might benefit or impact further development of ws4gee reproductivity expense authentication and challenge 1 reproducibility reproducibility is strongly correlated with repeatability interoperability offers a partial solution for reproducibility lacayo et al 2021 ws4gee implemented ogc specifications for gee enabled gwss to solve interactive operation as a consequence the workflows consisting of gee enabled geospatial data and geoprocessing services and other gwss can be described by web service workflow solutions that are compliant with gws standards such as bpel or bpmn for reuse and repeat meek et al 2016 yu et al 2012 and the performance of the workflow is determined by the geospatial web services themselves 2 expense as mentioned several times previously gee supports free access to massive datasets with shared computing resource for geospatial analysis and thus enables time efficiency and low development expense services for gws communities and promotes cooperation among different groups our experiment in table 4 also demonstrates the same point indeed measuring the cost is a complex problem and cannot be judged simply more factors in the processing should be involved such as the time to upload data the transmission fee among different sizes of data etc nevertheless we can still draw that ws4gee reduce the time and usage cost for gws with large scale and multiple remote sensing images 3 authentication ws4gee build upon the gee platform based on the google cloud so the vendor s attitude determines the system s rationality to exist the original intention of gee is to empower a much wider nonprofit use to perform geospatial analysis gorelick et al 2017 the use of ws4gee does not go beyond this principle in spite of this google has released the commercial version of gee to avoid the commercial issues such as privacy or frequent use limitation and obtain a sustainable and dedicated environment the commercial use and the subscription of gee should be considered 4 challenge towards rapid development of the cloud technology several significant attempts have been made in the geospatial community such as the spatiotemporal asset catalog stac specification for describing and cataloging spatiotemporal assets and the cloud optimized geotiff cog for storing raster data in cloud storage these works are concluded as cloud native geospatial service in the standard and interoperation level these new coming services might come up with new interoperability standards for accessibility posing challenges to the ws4gee to adopt and integrate these standards with those existing standards fortunately the ogc has started the exploration for standardized geospatial specification for the cloud native services echterhoff et al 2021 the fit of ogc apis that build upon the legacy of the ogc web service standards become a potential solution to this challenge because the ogc api sits one level above the discussed cloud native constructs while it is compatible with existing gwss natively currently despite the ws4gee framework mainly considers the integration with google earth engine this approach can guide the integration between existing gwss and the cloud native geospatial architectures in the future 7 conclusion and future works this paper presents the design implementation and use cases of a novel system called ws4gee the results indicate that gee datasets functions and user defined models can be published with less effort and thus expands the data and processing services available to the wider gws community more importantly we illustrate how ws4gee supports gee in geoprocessing workflow composition to better leverage its cloud computing power by integrating gee resources as gee enabled geospatial data and geoprocessing services scientists can invoke gee for higher performance environmental modeling that is compliant with ogc standards in particular our case study introducing geosquare enables easy deployment and use of gwss demonstrates the usability and feasibility of gee enabled gws overall our study presents a promising approach to integrate gee enabled gws that will benefit scientists working to solve time consuming and labor intensive tasks based on a common ogc standard in the next stage we are going to tackle the limitations and challenges mentioned previously and improve proposed ws4gee framework more importantly the thought to couple gws with the geospatial service in the cloud through standard specification will be our guideline to face challenges from the upcoming cloud native geospatial services we believe that scientists from various domains will take advantage of this approach and ws4gee to apply cloud based geospatial resources for their scientific tasks in the future declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledges we would like to express our sincere gratitude to the reviewers and editors for their valuable comments that help improve this paper the work was supported by national natural science foundation of china no 41930107 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105636 
25449,geospatial web services gwss assist scientists developing models online cloud computing technologies have created opportunities for faster and more efficient environmental modeling among of the new public cloud products google earth engine gee is particularly well adapted to meet the needs of environmental scientists developing and sharing their models however the potential of gee to support geospatial community is less well explored scientists are still waiting to see what public cloud platforms can do for geospatial web services this paper proposes a prototype called ws4gee that can wrap gee embedded datasets functions and models as ogc web services such as wms wfs wcs and wps in this way ws4gee can support direct access to gee through ogc interfaces thus allowing geoprocessing workflows to use gee resources two experiments are presented one with a gee enabled geospatial data service and the other deploying geoprocessing composition with gee enabled gwss the results confirm that ws4gee with the support of gee significantly enhances the capabilities of gws to deal with labor intensive and time consuming tasks this study provides guidance for the wider geospatial community when integrating other existing public cloud platforms finally some criticism suggests that the ogc web service standard is still too complex thus slowing the adoption rate of gws by endpoint users sun et al 2019 nevertheless we argue that this problem can be alleviated by existing technology like geosquare wu et al 2015 that interprets ogc web services with a readable user interface on the other hand the ultimate goal of ws4gee is to integrate gee with gws community rather than the selection of the most suitable gws interfaces another consideration is about the backward compatibility issue of the standard for instance although wps 2 0 2 standard is the latest version of ogc wps standard which offers two new operations getstatus and getresult to support asynchronous processing as well as immediate processing ogc 2018 it does not interoperate with wps 1 0 well as wps 1 0 0 is the most common used wps standard in current gws applications zhang et al 2019 it takes additional efforts to make wps 2 0 2 compatible with existing gws applications thus the adoption of the latest versions of such ogc standards would be the next step for the ws4gee system and the efficiency of different web service approaches such as soap simple object access protocol restful standards and others also require further exploration we would like to express our sincere gratitude to the reviewers and editors for their valuable comments that help improve this paper the work was supported by national natural science foundation of china no 41930107 we would like to express our sincere gratitude to the reviewers and editors for their valuable comments that help improve this paper the work was supported by national natural science foundation of china no 41930107 0 https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s1364 8152 23 00022 1 s1364815223000221 1 s2 0 s1364815223000221 10 1016 j envsoft 2023 105636 271872 2023 02 07t07 30 59 02982z 2023 03 01 2023 03 31 1 s2 0 s1364815223000221 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 main application pdf 296b36814c3846c8d2217ef0241703b3 main pdf main pdf pdf true 8927430 main 15 1 s2 0 s1364815223000221 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 preview image png bec78a73e2067fec554914906eafc959 main 1 png main 1 png png 62307 849 656 image web pdf 1 1 s2 0 s1364815223000221 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr7 downsampled image jpeg fb4f2e5c9a7ee91ba43bbf29d3eb8118 gr7 jpg gr7 gr7 jpg jpg 187379 503 691 image downsampled 1 s2 0 s1364815223000221 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr6 downsampled image jpeg 330b58c80a3a69f03d42752879fac6ab gr6 jpg gr6 gr6 jpg jpg 213586 1022 535 image downsampled 1 s2 0 s1364815223000221 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr9 downsampled image jpeg de80e8d504d28f89ed91ea17e87a26d2 gr9 jpg gr9 gr9 jpg jpg 218810 506 691 image downsampled 1 s2 0 s1364815223000221 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr8 downsampled image jpeg b0263a401eb071da1d2853ef8d3345c5 gr8 jpg gr8 gr8 jpg jpg 146662 416 691 image downsampled 1 s2 0 s1364815223000221 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr3 downsampled image jpeg 7ad536fa48d699290f64a68d6502c103 gr3 jpg gr3 gr3 jpg jpg 153767 464 691 image downsampled 1 s2 0 s1364815223000221 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr2 downsampled image jpeg 3f07d8d115e2d7b3a161b3dfaa03e5ad gr2 jpg gr2 gr2 jpg jpg 145915 258 691 image downsampled 1 s2 0 s1364815223000221 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr5 downsampled image jpeg e753baf97394089cf9b250dbfa960828 gr5 jpg gr5 gr5 jpg jpg 111719 538 691 image downsampled 1 s2 0 s1364815223000221 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr4 downsampled image jpeg 7f8e8f0c30ccba1d1bb3777bd62a46bb gr4 jpg gr4 gr4 jpg jpg 164542 410 691 image downsampled 1 s2 0 s1364815223000221 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr1 downsampled image jpeg bebf67ce1e8d898a436d728faf2a87a6 gr1 jpg gr1 gr1 jpg jpg 159741 516 535 image downsampled 1 s2 0 s1364815223000221 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr7 thumbnail image gif d6e4c3faa3a9317da5e2a74bca96aea8 gr7 sml gr7 gr7 sml sml 81998 159 219 image thumbnail 1 s2 0 s1364815223000221 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr6 thumbnail image gif 2c83dc4dc30ce29c0c6606c9dc923b2d gr6 sml gr6 gr6 sml sml 72352 164 86 image thumbnail 1 s2 0 s1364815223000221 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr9 thumbnail image gif f6349f03b550282fd0868ff12b781fac gr9 sml gr9 gr9 sml sml 86721 160 219 image thumbnail 1 s2 0 s1364815223000221 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr8 thumbnail image gif 67576c1fe8b96fe4e4626532b45e8d42 gr8 sml gr8 gr8 sml sml 76024 132 219 image thumbnail 1 s2 0 s1364815223000221 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr3 thumbnail image gif 82046b8b60da6ea4d6a1f53f89aedbe9 gr3 sml gr3 gr3 sml sml 76381 147 219 image thumbnail 1 s2 0 s1364815223000221 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr2 thumbnail image gif c6a11d5914710c6d0a40c55ad0d9e2f8 gr2 sml gr2 gr2 sml sml 76484 82 219 image thumbnail 1 s2 0 s1364815223000221 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr5 thumbnail image gif cd411b4c57f107aeda390ccdc73c0452 gr5 sml gr5 gr5 sml sml 72771 164 211 image thumbnail 1 s2 0 s1364815223000221 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr4 thumbnail image gif 19f34bc7bd20297dc1e7fd96893e5502 gr4 sml gr4 gr4 sml sml 79458 130 219 image thumbnail 1 s2 0 s1364815223000221 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 gr1 thumbnail image gif 15d32b63e08ab516776aa6240e8ee529 gr1 sml gr1 gr1 sml sml 82536 164 170 image thumbnail 1 s2 0 s1364815223000221 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 3e7081254c27522f341819a84bcb58e9 gr7 lrg jpg gr7 gr7 lrg jpg jpg 1062777 2227 3060 image high res 1 s2 0 s1364815223000221 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 02c109d4bc69780125efd148ea53e118 gr6 lrg jpg gr6 gr6 lrg jpg jpg 1058125 4528 2371 image high res 1 s2 0 s1364815223000221 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 4a6e2c29a720eea2e033be5701eba3bd gr9 lrg jpg gr9 gr9 lrg jpg jpg 1122145 2238 3059 image high res 1 s2 0 s1364815223000221 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 8b565dd4e4a214050144af660d40136b gr8 lrg jpg gr8 gr8 lrg jpg jpg 536298 1841 3061 image high res 1 s2 0 s1364815223000221 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 9fee5237ffa887566a122bad21d65c70 gr3 lrg jpg gr3 gr3 lrg jpg jpg 706652 2055 3061 image high res 1 s2 0 s1364815223000221 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg 219362f9f44958d286d3d74610cd892c gr2 lrg jpg gr2 gr2 lrg jpg jpg 547745 1140 3059 image high res 1 s2 0 s1364815223000221 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg d31c6d017a48aa173d6218b8bd1e2871 gr5 lrg jpg gr5 gr5 lrg jpg jpg 392257 2381 3060 image high res 1 s2 0 s1364815223000221 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg afe3d48bff0f954d5307af1e0c47f746 gr4 lrg jpg gr4 gr4 lrg jpg jpg 776012 1816 3060 image high res 1 s2 0 s1364815223000221 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 highres image jpeg f154d04132d2313a154d7a16609e14c6 gr1 lrg jpg gr1 gr1 lrg jpg jpg 549863 2288 2370 image high res 1 s2 0 s1364815223000221 mmc1 docx https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 mmc1 main application vnd openxmlformats officedocument wordprocessingml document f104124bee38b75124d051e9d38532f6 mmc1 docx mmc1 mmc1 docx docx 16551 application 1 s2 0 s1364815223000221 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 7b80882232830a66395fc5cbc9ffc276 si1 svg si1 si1 svg svg 47481 altimg 1 s2 0 s1364815223000221 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 8f9b2837929e49819726d38970605c2e si8 svg si8 si8 svg svg 16340 altimg 1 s2 0 s1364815223000221 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml f21f79b8dd8acad3beadfced7c4d3418 si3 svg si3 si3 svg svg 67302 altimg 1 s2 0 s1364815223000221 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 7fd5eb49967233c8dabbc3b60b2fc32b si5 svg si5 si5 svg svg 11813 altimg 1 s2 0 s1364815223000221 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 05f197a4f9cb50fd58e2645051dabe19 si4 svg si4 si4 svg svg 102353 altimg 1 s2 0 s1364815223000221 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 956d7cd9532d87b4f755309549747cae si2 svg si2 si2 svg svg 63949 altimg 1 s2 0 s1364815223000221 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 748b96a520e97478bfe0f1f12179edb5 si6 svg si6 si6 svg svg 36143 altimg 1 s2 0 s1364815223000221 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815223000221 image svg xml 492bbfd6dd6b78a7441226e807559787 si7 svg si7 si7 svg svg 33839 altimg 1 s2 0 s1364815223000221 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 10tzrwscj60 main application pdf 79032ee7449e96ae66b69ffa3be789b5 am pdf am am pdf pdf false 1343533 aam pdf enso 105636 105636 s1364 8152 23 00022 1 10 1016 j envsoft 2023 105636 elsevier ltd fig 1 interoperability among google earth engine ws4gee and geospatial web services fig 1 fig 2 detailed architecture context of proposed ws4gee system fig 2 fig 3 sequence diagram of workflow for ogc request and response in ws4gee fig 3 fig 4 gee enabled geoprocessing registration fig 4 fig 5 a conventional method to invoke wcs for the coverage of aoi four wcs services should be called to retrieve subset a d from images 1 4 b obtain the coverage of aoi within ws4gee strategies subset a d are automatically merged as one piece region and served as one wcs service fig 5 fig 6 ws4gee client geographic interface of gee enabled service registration a geospatial data registration and b geoprocessing service registration c service search and preview and d service access and invocation fig 6 fig 7 the use of ws4gee search engine for gee imageries acquisition a search engine user interface and wcs service generation b boundary and extent of wuhan city c wcs operations getcapabilities describecoverage getcoverage d visualization with true color band 4 3 2 fig 7 fig 8 developing geoprocessing workflow for the model in geosquare with descriptions for involved elements fig 8 fig 9 the resulting map of geoprocessing workflow with ws4gee for grassland environmental monitoring fig 9 table 1 core operations in ws4gee workflow table 1 module operation description client serviceblock filter request check input ogc request and its operation type send request permit an ogc request to the server by its operation type receive response return an ogc response from the generator which might be an xml document or data file parser retrieve attr get required variables from gee model these variables will be assigned by parser in order convert to ee vector convert input vector to gee object convert to ee image convert input raster to gee object it will check input data resource if the input data resource belongs to gee it can be assigned to gee variable directly convert by ee cloud upload data from an ogc request to google cloud an identifier of this resource is returned which facilitates resource import to gee environment generator generate service description generate service descriptions for getcapabilities describeprocess describefeatures and describecoverage operations based on the service type generate service outcome generate results in xml format or files based on ogc request wps wfs wcs and wms and specific operation table 2 data type mapping between ogc web services and gee table 2 ogc web services gee literal data structure ee computedobject float double integer boolean string ee number ee string complex data structure ee computedobject vector shapefile geojson gml etc ee geometry ee feature ee featurecollection raster tiff geotiff etc ee image ee imagecollection text txt etc ee string other structure ee computedobject boundingbox data structure bbox envelop etc ee geometry timestamp ee date table 3 geospatial web services used for the grassland environmental monitoring table 3 service name format service source color in fig 8 fvc calculation model wps google earth engine brown precipitation layer vector data file csv chen et al 2018 work green aw3d30 dem wcs google earth engine brown rate soap geosquare blue imar boundary wfs geoserver red gwr soap geosquare blue add join soap geosquare blue zonal statistics as table soap geosquare blue table 4 comparison of the processing time between geoprocessing workflow with gee enabled geospatial web services and local desktop based processing table 4 processes local desktop geoprocessing e g arcgis gee enabled wps for fvc model major hardware configurations cpu speed 2 2 ghz memory ram 8 gb disk space 40 gb network speed 5 mb s not required data preparation including acquire landsat imageries and pre processing such as clip and cloud mask download 130 tiles on average for the study area per year each tile contains two bands red near infrared assume that we use the batch mode downloading and pre processing one image around 200 mb needs 1 min and other pre processing takes 3 min not required to upload boundary filethe total time cost is 1 3 130 3 year 1560min 26 h not required to download tiles required to upload boundary file 1 mb less than 1 min the total time cost is 1 min model execution including ndvi calculation imageries reduction and fvc calculation it takes about 10 min to calculate and composite for annual maximum ndvi imagery for each tile and then extract fvc for each yearthe total time cost is 130 10 1300 min 21 67 h the fvc model is executed by gee cloud computing the mapreduce performs all computation for tiles simultaneously gorelick et al 2017 the total time cost is 5 min retrieve results not required to export results required to export result from gee 20 min for each year summary the total time cost is around 48h the total time cost is less than 1h ws4gee enhancing geospatial web services and geoprocessing workflows by integrating the google earth engine jianyuan liang fengying jin xianyuan zhang huayi wu state key laboratory of information engineering in surveying mapping and remote sensing liesmars wuhan university 129 luoyu road wuhan hubei 430079 china state key laboratory of information engineering in surveying mapping and remote sensing liesmars wuhan university 129 luoyu road wuhan hubei 430079 china state key laboratory of information engineering in surveying mapping and remote sensing liesmars wuhan university 129 luoyu road wuhan hubei 430079 china corresponding author handling editor daniel p ames geospatial web services gwss assist scientists developing models online cloud computing technologies have created opportunities for faster and more efficient environmental modeling among of the new public cloud products google earth engine gee is particularly well adapted to meet the needs of environmental scientists developing and sharing their models however the potential of gee to support geospatial community is less well explored scientists are still waiting to see what public cloud platforms can do for geospatial web services this paper proposes a prototype called ws4gee that can wrap gee embedded datasets functions and models as ogc web services such as wms wfs wcs and wps in this way ws4gee can support direct access to gee through ogc interfaces thus allowing geoprocessing workflows to use gee resources two experiments are presented one with a gee enabled geospatial data service and the other deploying geoprocessing composition with gee enabled gwss the results confirm that ws4gee with the support of gee significantly enhances the capabilities of gws to deal with labor intensive and time consuming tasks this study provides guidance for the wider geospatial community when integrating other existing public cloud platforms keywords google earth engine geospatial web service geoprocessing workflow ogc standard service interoperation data availability data will be made available on request 1 introduction the rapid development of internet and big data has promoted the spread of web services significantly with the advancement of web service technology tens of thousands of geospatial web services gwss were developed and available online including spatial records datasets and functions following the model as a service concept different web based models were also implemented as geospatial web services li et al 2017 geospatial web service can be divided into geospatial data service and geoprocessing service where the term geospatial data service refers to geospatial services for spatial data acquisition while geoprocessing service is used to describe any function or model for processing geospatial and related data yue et al 2016 driven by the service oriented architecture soa many spatial data infrastructures sdis or cyberinfrastructures were developed for storing and sharing geospatial web services in a distributed and interoperable environment barik et al 2018 sun et al 2019 today it is common for earth scientists to perform environmental modeling online with gws gan et al 2020 gichamo et al 2020 sun et al 2019 the distributed geospatial data services and geoprocessing models or functions accessible through web service interfaces can be coupled to create more powerful and added value services yue et al 2015 zhang et al 2020b which is referred to as geoprocessing workflow or geospatial services composition the open geospatial consortium ogc has published a series of standards by adapting or extending the common web service standards to facilitate the integration of siloed and legacy geospatial web services lopez pellicer et al 2012 reed 2011 several well known products like wms web map service wfs web feature service wcs web coverage service csw catalog service for the web can provide standardizing service interfaces and data models meanwhile any environmental model or geospatial algorithm defined as geoprocessing service can be accessed through a web processing service wps interface foerster and stoter 2006 li et al 2017 in addition other standards like soap simple object access protocol or rest representational state transfer can also serve a similar purpose in several scenarios these standards enhance the interoperability among geospatial services and reduce the complexity when composing these services environmental scientists have already realized the advantages of gws and geospatial service composition they have come to expect services with higher quality and efficiency among several new developing information technologies cloud computing has become a potential alternative to satisfy this requirement the cloud brings powerful computing capabilities to end users by shifting computing resources to the cloud and capable of handling tasks with a large amount of data and services compared to conventional sdis or other cyberinfrastructures in the c s mode the cloud provides more rapid elastic and efficient on demand resources to help scientists publish their services on the internet yue et al 2013 zhang et al 2019 after years of development four categories of cloud deployment models were proposed the private cloud community cloud public cloud and hybrid cloud zhang et al 2019 the public cloud has caught the attention of many scientists because it is free or sold on demand and still inherits most of the characteristics of the cloud technology based on the public cloud product the google cloud google earth engine gee platform has been established and free to use for environmental scientists gee has contained a multi petabyte analysis ready data in the cloud and a set of application programming interfaces apis in its client library to perform geospatial analysis online gorelick et al 2017 consequently gee attracts much attentions from environmental scientists who used to traditional software centered and desktop based environmental modeling liang et al 2020 following this trend some other public cloud platforms for environmental scientist like microsoft planetary pie engine and geobrain cloud zhang et al 2019 have been also released or in the beta in summary gws has gained long term process in recent years and continues evolving along with new technologies which reflects in the following four aspects 1 web service technology facilitates the emergence and wide spread of gws 2 heterogeneous gwss can be coupled and integrated as geoprocessing workflow for data sharing and interactive operation based on standard specifications 3 cloud computing technologies injects new vitality into geospatial community and provides reliable and powerful resources for geospatial applications 4 the emergence of public cloud based geospatial analysis platforms such as gee leads to new application mode to perform environmental modeling online with sharable and analysis ready data and functions based on elastic cloud computing resources nevertheless even with the aforesaid tendency and the advancement of the cloud technology for geospatial analysis three obstacles could still hinder their wider use technical complexity the gap between user communities and public cloud platforms and appropriate methods to connect the cloud based services to geospatial workflows firstly the technical complexity must be overcome if these cloud based platforms are to be used effectively and easily taking gee as an example users should be familiar with javascript or python programming language to invoke client library apis even simple syntax or logical errors could easily result in an entire geospatial analysis failure this is extremely inconvenient for environmental scientists who are used to desktop analysis software that has user friendly geographic windows and user interfaces ui the subsequent obstacle concerns the gap between current gws communities and public cloud platforms for instance gee manipulates every process on the server cloud side this activity ranges from a simple assignment expression to complex functions gee implements the entire process in one space instead of by combining heterogeneous services together any interaction with external resources or services requires massive manual effort to deal with data transmission from one place to another especially when the data source or the outcome of a gws is the input of the gee model or the output of gee is the input for other services however it would be a huge waste to re develop existing geoprocessing or models on the cloud platform and sometimes even too complicated or even impossible although a number of datasets and processes functionalities are available in the cloud it is difficult to utilize these resources beyond the gee environment and impossible to apply gee for geoprocessing workflows combining because of a lack of appropriate methods to connect them with other gws outside the cloud therefore bridging the conventional gws with the cloud based geospatial service solution could be advantageous to the entire geospatial community we alleviated these three problems and made full use of existing public cloud platform to benefit gws community choosing gee as the experimental platform since gee is widely used by environmental community and more mature than other public cloud platforms a ws4gee framework and prototype are proposed to explore approaches to make gee interactive with existing gws community specifically gee resources are generated as gee enabled geospatial data and gee enabled geoprocessing services we selected ogc web services ows as the implementation standards for ws4gee while ogc wms wfs wcs and wps standards were implemented for data resources functionalities and models in gee the experiments show that ws4gee allows environmental scientists to take advantage of gee and its cloud computing capability and access resources as gwss the remainder of this paper is organized as follows section 2 introduces the related work in detail section 3 describes the specific architecture of ws4gee major module implementation and a prototype is proposed in section 4 in section 5 two experiments illustrate the capability of ws4gee section 6 discusses the contributions limitations and common issues of ws4gee and some conclusions are drawn in section 7 2 related work 2 1 existing geospatial web services gws uses large volumes of geospatial data and functionalities in a flexible manner and alleviates the pressure of tasks by utilizing the combined power of distributed services in the network wright 2011 compared to the traditional approach that completes all tasks on a personal computer it enables broader remote participation encourages collaboration and improves research reproducibility chen et al 2020 gan et al 2020 many web based geospatial applications or sdis are developed for using geospatial data geoprocessing services or both for instance geoss global earth observation system of systems implements a system of system architecture based on the web as a platform nativi et al 2015 zhang et al 2019 a set of independent earth observation information and processing resources with contributions from countries and international organizations in the group on earth observations geo are coordinated to facilitate access monitoring sharing of global environmental data and information via gws a geoportal named geospatial one stop gos was designed to promote communication and sharing of geospatial resources goodchild et al 2008 the amount of gws resources has significantly increased yue et al 2016 zhao et al 2012 according to gui et al 2016 more than 41 000 geospatial data services containing over 300 thousand layers were available 2 2 geoprocessing workflow and interoperation when plenty of gwss are available online researchers start to integrate different services to meet complex application demands geoprocessing workflow defines a process chain to accomplish this goal each process works as a component in the workflow for one or more goals synchronously or asynchronously tan et al 2015 many conventional desktop systems provide a user friendly interface to construct process chaining such as modelbuilder in arcgis and spatial modeler in erdas to generate workflows yue et al 2015 gws is the process unit for geoprocessing workflow in a web environment users aggregate spatiotemporally distributed gwss as a geospatial service chain however the gws combination process might involve different standards such as ogc soap rest sun et al 2019 service selection yue et al 2007 and service collaboration wu et al 2015 dealing with these issues is a complicated task for the users without gws background many attempts have been made to reduce the complexity of building geoprocessing workflow wu et al 2015 proposed a system design called geosquare with a collaboration mechanism for flexible geoprocessing yu et al 2012 utilized the business process execution language bpel to describe workflows and enabled the proper execution and coordination of geospatial workflows through standard geospatial web services e g wfs wps zhang and peng 2013 sun et al 2012 developed geoprocessing workflow modeling builders called geojmodelbuilder and geopwtmanager respectively these systems are not only capable of designing and executing geoprocessing workflows but also support service visualization and monitoring the web based user interface makes gws readable and simplifies the development of geoprocessing workflows additionally geoprocessing workflow can be enhanced by integrating them with other modeling frameworks like openmi yue et al 2015 or provenance models like w3c prov jiang et al 2018 zhang et al 2020b these works indicate that geoprocessing workflow approach with assistance of appropriate system or framework could be a promising option for constructing integrated models 2 3 cloud computing for geospatial application cloud computing plays an essential role in the geospatial community zhang et al 2019 it relieves users from conventional limitations such as software installation data management and local computing resource requirement by shifting the computing resources to the cloud the cloud benefits geospatial applications for data storing and retrieval wang et al 2019 geoprocessing and spatial calculation jiang et al 2021 yang et al 2017 service discovery and maintenance blower 2010 evangelidis et al 2014 and service sharing zhang et al 2019 thus geospatial analysis or the deployment of geospatial web services is widely performed on the cloud e g aws azure google cloud in environmental modeling the integration of cloud computing and geospatial applications usually comes up with specific cloud infrastructures and web services standards for instance tan et al 2015 designed a construction of an elastic parallel ogc wps service on a cloud based cluster to provide a higher performance wps service that uses fewer computing resources astsatryan et al 2015 proposed a gateway for large scale timeseries ndvi calculation in the cloud where geoprocessing can be published as wps zhang et al 2019 presented a cloud based wps framework that provides a general solution to integrate models in the cloud these works expand the capability of cloud computing perform geospatial analysis despite advantages of the cloud existing cloud based geospatial studies primarily focus on architecture design or specific topics rather than a general solution for the geospatial community in most cases environmental scientists still enjoy less benefit from the cloud technology because they have to develop configure and deploy their geospatial services in the cloud from the beginning 2 4 environmental modeling in google earth engine unlike previous cloud based geospatial applications gee provides a group of pre defined functions and datasets for environmental modeling initially these functions and datasets facilitate straightforward mapping for environmental modeling or applications especially in large time and space scales land use and land cover lulc problems angern et al 2021 chen et al 2017 hu et al 2018 huang et al 2017 liu et al 2018 teluguntla et al 2018 to make gee extendable and reusable many gee models are wrapped as toolboxes or packages so that they can be used as black boxes in gee this approach can hide technical issues and allow users to access gee models in gee environment with simple import and thus gradually becomes one important approach for environmental modeling in gee for instance liang et al 2020 developed two modeling tools to enable researchers to go beyond gee mapping capacity for comprehensive urban sustainability modeling zhang et al 2020a proposed a toolkit for agricultural land use modeling called agkit4ee that helps conduct geospatial agricultural and environmental modeling with cdl cropland data layer data in a simple way mhawej and faour 2020 built the sebaligee system for 30 m evapotranspiration rates retrieval based on the gee client library and a model called surface energy balance for land improved sebali similarly a gee based fractional vegetation cover fvc model was presented to estimate the vegetation cover over the southern african rangelands vermeulen et al 2021 and a gee enabled python toolkit called coastsat was introduced to retrieve shorelines in any user defined region of interest vos et al 2019 all these gee models are ready for use with simple interfaces or entrances other gee developers can still access the source code of these models or packages as gee makes code available for the developers nonetheless although user defined models can be shared in gee environment it is still complex to integrate them into other geospatial interfaces unlike cloud based applications that enable access with gws interfaces gee models or a simple function should entirely rely on the gee environment using gee enabled models is promising but it is still a challenge for environmental scientists to integrate other geospatial services with gee based services and it is also time consuming to perform each involved process manually in different execution environments 3 system design 3 1 design principles the fundamental objective of this study is to explore a framework called ws4gee to integrate gee resources as two categories of geospatial web services gee enabled geospatial data service and geoprocessing service fig 1 shows the relationship among google earth engine ws4gee and other geospatial web services we notice that gee contains abundant resources and computing power on the cloud and that there is a gap between gee and common geospatial web services because of access constraints runtime environmental isolation and other technical issues therefore the design principle bridges gee and other geospatial web services through gws interfaces among different gws interfaces we selected ogc standard compliant interface as our target implementation because of its applicability compatibility and common use li et al 2016 the interoperability between gee and other gws contains three levels as seen in fig 1 1 embedded datasets gee functions and models are accessible through ogc interfaces 2 gee enabled geospatial data can be utilized as complex data inputs for gws and vice versa 3 geospatial web services and gee enabled gws can cooperate in geoprocessing workflow by realizing these levels for interaction gee could be connected with geospatial web services and technical issues will be hidden for the interaction 3 2 system architecture this study proposes a ws4gee system to facilitate publishing gee data resources and gee functions models as geospatial web services and geoprocessing within ogc standards generally ws4gee implements wms wfs wcs and wps standards to data and functionalities in gee these standards have defined interfaces to retrieve valid operations and parameters and obtain an xml document that fully describes the requested data or processes thus achieving specific kinds of data in well known formats or to execute processes above all satellite imagery is the major data resource in gee different imagery datasets landsat modis sentinel 2 etc can be registered as wcs and grouped by imageries categories and periods functionalities in the gee client library and models developed with these functions can be registered as wps common used operations from ogc standards including wms 1 1 wfs 1 1 wcs 1 1 1 and wps 1 0 can also be implemented with the same approach ws4gee relies on a multiple component structure to establish interaction between gee and gws the architecture of ws4gee can be divided into two blocks the client client service block and server block along with the gee platform environment and gws environment the gee platform environment provides the original resources of data functions and models and the gws environment represents the gws users activities that utilize standardized services and service composition for geospatial tasks this framework aims at facilitating service transformation between geospatial web services and gee functionalities as shown in fig 2 the system consists of three components the graphical visualization and interaction component the resource encapsulation and publication component and the system management and configuration component the client block is based on the graphical visualization and interaction component which provides the gateway of the framework it allows users to send requests and receive responses from the server block through standard ogc interfaces a web based user interface provides the access point for invoking available gee enabled geospatial data and geoprocessing services ogc requests are received in xml format and assigned to specific service types thereafter the completeness and correctness of url and xml documents are checked through validation an exception is returned directly if they fail validation otherwise the request is sent to server block the server block is responsible for the interaction with the gee platform and service management several modules are developed in the resource encapsulation and publication component to handle the request from the client block and establish gee enabled gws from the original gee resources the ogc requests from the client block interact as the gee enabled gws and target gee resources are acquired by gee script templates that are pre defined in the server and invoked as service instances in the execution module during this process two core modules the generator and parser modules facilitate the conversion between gee script templates and gee enabled gws the generator wraps the gee results or generates a response compliant with the request service interfaces likewise the parser interprets ogc requests to obtain information required function parameters etc and translates this information into specific gee scripts to instantiate specific gee script templates for execution in addition the generation or creation of gee enabled geospatial web services is supported by the strategy module these modules form the workflow for the interaction with gee platform but still need to be controlled to ensure reliability and stability the system management and configuration component is designed to manage the entire procedure in the ws4gee server block the runtime module provides an initial environment to handle gee scripts the monitor module checks the status for every process a process will be retried if systematic error e g connection character encoding or decoding occurs in addition although ws4gee depends on gee to execute scripts and achieve results the storage and cache modules are developed to keep basic information for ogc requests and small capacity responses a fake call mechanism sun et al 2019 is set up in cache module to reduce the response time cost of frequently accessed services for efficiency once a request is made ws4gee will check whether a same request was made before if an identical request is found in the cache the result will be returned directly without executing gee scripts again for instance when a user calls the describecoverage operation defined by wcs to obtain the description of an image a response xml document will be generated after running a list of gee scripts the first time basic information like the extent coordinates and timestamp that uniquely identify the service as well as the response document are stored in the ws4gee once a request with the same identifiers is called the xml response will be sent to the end user from storage rather than reconstructing the same response from the gee 3 3 workflow the workflow describes the underlying logic of ws4gee from an external ogc request to the gee enabled service execution and the generation of an ogc response in the workflow the client service block is responsible for passing valid ogc requests and receiving ogc responses in the server block a set of modules including the gee enabled gws parser generator and the cache work together to generate ogc responses from gee or the ws4gee storage fig 3 ws4gee implements the workflow by invoking a sequence of internal operations from the different modules in the client service block filter request and send request are called to filter and send ogc requests to the server block the gee enabled gws module identifies the ogc requests and allocates them to the cache or parser module an ogc response will be directly returned if the same request is found in the cache otherwise the parser will parse the ogc request and initialize a gee task based on the information extracted from the ogc request the input parameters are fetched from ogc request and assigned to variables in the gee model with the retrieve attr operation simple literal inputs will be immediately assigned to gee variables while complex inputs will be translated through operations such as convert to ee vector or convert to ee image into gee variables the gee task relies on a registered script template either for data resources gee models or functions in the execution module a specific script template is loaded by the reflection mechanism once all the variables are set up the relative gee function or model is run in gee ws4gee retrieves gee results and generates an ogc response through the generator module for operations like describecoverage or describeprocess that aim to retrieve basic information from the service a response with xml format will be generated from generator modules after running pre defined gee script templates with generate service description for operations like the execute operation in wps that have data file return vector raster etc a generate service outcome operation can be invoked based on the primary ogc request these major operations in the workflow are listed in table 1 the workflow illustrates how gwss interact with gee in ws4gee framework it also shows the minimum requirements to implement a ws4gee prototype including the client service block and at least five modules for the server block users can develop other modules such as the monitor and the storage to build a more robust and stable system 3 4 core modules this section introduces the core modules of ws4gee that facilitate the interaction between gee resources and the gws as mentioned previously existing data resources on the gee platform can be accessed by wms wfs or wcs meanwhile gee based models and functions can be invoked as a wps and external gws can participate in gee enabled geoprocessing as gee input parameters the core modules include registration strategies and execution 3 4 1 registration ws4gee registration includes processing and temporary geospatial data registration components the processing registration component supports manual registration of user defined gee functions and models as wpss during the registration process the input output specifications descriptions of user defined functions and package of source scripts are defined and saved as gee script templates which will facilitate matching of the input output parameters and parameter types defined in gee to complex datatype objects in the wps the relationship between processing registration gws and gee is illustrated in fig 4 and a web based user interface is provided to simplify the registration the temporary geospatial data registration benefits the transmission of data sources to the gee in a wps execution task accessible resources like wfs wcs or a geospatial data file with a specific suffix such as tiff geotiff or geojson could be introduced through a url in a wps request as a complex data input reference previously it was a tedious task to use external geospatial data in the gee as a list of manual steps was required convert the format of the geospatial data upload them to the gee environment and then import this data as parameters for gee scripts to overcome this constraint ws4gee registration acts as an agency to complete all the necessary steps a copy of the geospatial data in a gee supported format will be created and registered as a data source on the google cloud with a unique identifier since gee can import data from the google cloud this entire process is performed automatically without manual intervention this copy is temporarily stored during the execution of wps request and destroyed after this process is finished 3 4 2 strategies the gee includes a tremendous amount of satellite imagery so wcs has become the primary gws to leverage these assets but accessing these images through wcs is still a challenge for instance landsat 8 satellite circles the earth around every 16 days and produces approximately 57784 image tiles more than 1 million images are collected each year but scientists are only interested in a specific research area or period thus an area of interest aoi is frequently comprised of a combination of several slices from two or more images in this case operations like describecoverage or getcoverage are called multiple times to access subsets of an aoi fig 5 a the more images are involved the more complicated this process thus two strategies were designed in ws4gee to resolve these issues first the response document of wcs operations can be generated in real time users obtain response documents from getcapabilities and describecoverage operations based on their aoi when the aoi is a subset of one image the description of the aoi will depend on the imagery otherwise it will be composed of two or more images second the imagery can be generated dynamically as fig 5 b shows a composite image of the three involved parts distributed across four images will be generated once the getcoverage operation is called since the wcs do not exist in ws4gee originally the user must provide the input criteria such as the aoi time range and so on to generate dynamic wcs 3 4 3 execution the execution module consists of two major processes mapping and execution to build a bridge between gee resources and gwss ws4gee focuses on mapping data types since gee has a separate variable system where variables are wrapped with proxy objects on the server side gorelick et al 2017 for instance a getcoverage operation requires the parameters bbox boundary box height and width to extract the coverage this process matches the function ee image clip on the gee where a geometry is required to execute the clip thus these ogc parameters are converted to ee geometry object to involve gee scripts the mapping for wps is more complicated as a request might include specific disparate data structures such as complexdata literaldata or boundingboxdata hence a mapping between the common data structures of ogc web services and gee data objects are listed in table 2 ws4gee relies on gee platform for service execution different gee scripts are prepared to handle various ogc requests these scripts serve as execution templates in ws4gee and become active after their variables are filled up through the mapping process ogc operations such as getcapabilites are common tasks implemented in ogc standards they share the same template the user need only to pass an ogc request to ws4gee no technical details are required and the complexity of service execution is hidden the interaction between ws4gee and gee platform is invisible to end users with the mapping and execution ws4gee offers the environment that connects gws and actual scripts running on the gee 4 implementation to validate the system design in this study a ws4gee prototype was constructed based on the aforementioned principles and framework the prototype consists of two sub systems called ws4gee client and ws4gee server manager respectively the link of the entire prototype with a comprehensive tutorial are provided in appendix a the ws4gee client and ws4gee server manager were developed following the system architecture as described in section 3 2 to facilitate the ui only users to create and access these gee enabled geospatial web services the web based user interfaces for service registration search and preview and invocation were realized shown in fig 6 a d the registration window is prepared for geospatial data and geoprocessing services registration based on the registration module and dynamic service generation strategies as detailed in section 3 4 1 and section 3 4 2 the search and preview window is used to check available gee enabled gwss and visualize geospatial data services in the system and the invocation window with examples is developed to help users access gee enabled wps in the client side the ws4gee server manager is the core realization of the server block and handles the interaction between gee and the client side for the rapid prototyping purpose python 3 was selected as the programming language for the ws4gee server manager as gee supports to invoke the client libraries in python environment offline in the prototype a few frequent used gee original datasets and client library functions were pre registered with executable script templates and ready to use as ogc services their service descriptions and other basic information were also retrieved from the gee portal or api documentations and stored in ws4gee for efficiency the actual operation to access relative gee resources is hidden for users based on the script templates mentioned in section 3 4 3 5 experiments to evaluate the utility and efficiency of ws4gee as well as importance of integrating gee with the gws community we applied the ws4gee prototype in two cases the first example illustrates one common use of the dynamic generation strategy when visiting gee imagery within a wcs the aim of the second example is to demonstrate that ws4gee facilitates the integration of gee enabled geospatial data and geoprocessing services with other heterogeneous geospatial web services this case study involves two tasks detection of grass vegetation coverage and spatial analysis in addition a geoprocessing service management system geosquare was applied for workflow conduction and visualization wu et al 2015 5 1 acquiring imageries through dynamic gee enabled wcs to demonstrate the capability to acquire gee resource within wcs we used the ws4gee user interface to get a set of images of a designated region fig 7 a the study area was wuhan city 30 593 n 114 306 e in china the goal was to obtain images of this region in 2020 from the landsat 8 sr dataset the region includes portions of three images path 122 123 row 38 39 fig 7 b it is wasteful however to download an entire set of images for this region during a particular time frame to splice together a single composite image of the study area the ws4gee prototype solved this problem by implementing the dynamic generation strategy the web based user interface requires input datasets filter options e g by months by year by mean no cloud bands and the boundary of region to design a dynamic wcs fig 7 a here our target is monthly no cloud imagery for wuhan city a list of wcss specified for wuhan city are created based on the dynamic generation strategy wcs operations such as getcapabilities describecoverage and getcoverage can be called fig 7 c the target imagery can be visualized either in local space or in ws4gee user interface that enables bands composition based on wms standard fig 7 d this example shows that ws4gee hides platform based and coding requirements and it enables direct access to gee resources through common ogc standards the dynamic generation strategy makes gee enabled wcs demand oriented this significantly enriches the data sources for environmental scientists and relieves them from the labor intensive and error prone tasks of downloading and processing datasets 5 2 grassland environmental monitoring within a hybrid service composition monitoring grassland ecosystem environment is critical for certain governmental departments assume that scientists want to evaluate the ecosystem condition of temperate typical grassland in xilingol league imar the northern border of china during 2010 and 2017 and investigate the spatial correlation of different grassland regions among the impact factors precipitation and elevation they design a grassland ecosystem investigation model to accomplish this task firstly the model quantitatively measures the vegetation coverage change between two time points using fractional vegetation coverage fvc wu et al 2014 as calculated from the normalized difference vegetation index ndvi extracted from satellite imagery then a spatial analysis algorithm geographically weighted regression gwr is applied to detect patterns of spatial correlation equations 1 4 for this model are shown below 1 n d v i n i r r e d n i r r e d 2 f v c n d v i n d v i s o i l n d v i v e g n d v i s o i l 3 f v c c h a n g e f v c a f t e r f v c b e f o r e 4 y i β 0 β 1 x 1 i β 2 x 2 i β n x n i ε i β i x t w i x x t w i y where in equation 1 the ndvi is calculated from the visible red and near infrared light nir reflected by vegetation in equation 2 f v c is the fractional vegetation cover in the mixed pixel n d v i s o i l is the ndvi of bare soils that are not covered by vegetation and n d v i v e g indicates the pixels that are completely covered by vegetation equation 3 calculates the change between two points in time equation 4 presents the regression models that underlie gwr in which w i is a matrix of weights specific to location i such that observations closer to i are given greater weight than observations further away the scientists need to access the landsat 5 8 surface reflectance image collection to find datasets to create ndvi and fvc images raster data from aw3d30 global digital surface model products for elevation vector layers for potential impact factors such as precipitation and the boundaries for the xilingol league study area however there are three challenges to overcome 1 the xilingol league covers a total area of approximately 199 776 km2 of grasslands from west to east which covers 23 tile grids path 122 128 row 27 31 in the landsat dataset the time used to download clip and merge a set of images should be considered 2 calculating the annual fvc is usually based on maximum ndvi composition which is reduced from a series of ndvi calculated from each landsat image the annual ndvi image for the xilingol league area is composed of the maximum value at that location calculated from all the images from july to september this task also takes up much manual effort and time 3 even if the other two issues are overcome the scientists still have to deal with data transmission problems if the datasets are distributed or processed in different software environments this study resolves these challenges through ws4gee in the preparation stage a number of heterogeneous gws including the gee enabled geospatial data and geoprocessing services geopw yue et al 2010 wps geoserver wfs soap services wu et al 2015 and ecological csv file chen et al 2018 were involved table 3 we retrieved the fvc images from the fvc calculation model realized in gee and registered as a wps in ws4gee the fvc calculation model performs a group of functions to extract a fvc image from the original satellite imagery for this case study about 130 images from the gee landsat 5 8 surface reflectance image collection were acquired and clipped with the clip function then all the clouds were masked by the internal masks2clouds function ndvi images were calculated for each image with the gee defined normalizeddifference function and stacked to create a single annual maximum ndvi image using the ee reducer max function the fvc was calculated based on ndvi values thereafter the fvc data participated in gwr as the dependent variable while two potential ecological impact factors elevation and precipitation were explanatory variables the elevation was acquired from the aw3d30 products in google earth engine using an embedded wcs request to ws4gee the vector data for precipitation was created from the ecological csv file these distributed gwss were generated as geoprocessing workflow for execution the entire geoprocessing workflow is built and run based on geosquare web based interface fig 8 the shapes and colors represent different types and sources of elements the circle represents geospatial data service the rectangle is geoprocessing service and the bold red circle represents final output gee enabled gws and the soap services are marked in the brown and blue color respectively this study produced two set of outputs for fvc change and gwr analysis as the bold red circles shown in fig 8 our results show that apparent coverage reduction appears in the southwestern and eastern parts of xilingol league during 2010 2014 and after 2014 the coverage in these regions increased more or less while much degradation occurs in the center of the study area fig 9 a b the result of gwr indicates different spatial correlation between dependent variable fvc and explanatory variables elevation precipitation fig 9 c d the elevation has a positive impact to the coverage in the southwestern part of the xilingol league but this correlation is negative in the northeastern part we could download the fvc change and spatial distribution diagram of two gwr explanatory variables to present our results shown in fig 9 furthermore this study focuses more on the performance when integrating gee into gws composition one significant evaluation is to measure the benefits that ws4gee brings to the individual users who have less computing or storage resources for geoscience research thus we compared the time cost for calculating fvc when using gee enabled wps through ws4gee with the time costs when doing the same tasks locally we executed geoprocessing services with geospatial data on a local desktop the local desktop scenario required downloading and data pre processing as well as execution tasks for the fvc model the data pre processing and the task execution were performed in the arcgis desktop 10 6 environment thus the local desktop required 2 2g for cpu main frequency and 8g of ram for arcgis desktop according to the esri official recommendation and additional 40 g for data storage besides we configured the network facility that allowed a download speed of 5m s and assumed the downloading condition was stable for the ws4gee based scenario the hardware configurations was not required since gee has provided free computing resources to perform the fvc model for noncommercial and research use gorelick et al 2017 and the cost of google cloud storage services was based on the consumed storage amounts the details and descriptions of these two processes are summarized in table 4 the table illustrates that downloading time series landsat images costs a tremendous amount of time which is not required in gee enabled gws desktop processing such as applying the image cloud mask clipping merging and ndvi and fvc calculations also take up more time than do the same tasks on the gee that s because the entire study area covers a large area and multiple images collected over a couple of years performing these processes requires more computing resources although the gee enabled gws takes additional time to upload the required data and download results its time cost is apparently lower than downloading the required dataset to a local environment while the uploading and downloading speed relies more on the bandwidth of the network and the condition of network facilities on the client side in short the gee enabled gws can benefit the geoprocessing workflow with efficient computing power for geoprocessing services and it is the supplement of geospatial data source etc without ws4gee these powerful resources will remain under utilized 6 discussion this section is organized to 1 summarize the contributions of ws4gee to the gee and gws communities 2 address limitations on the current implements 3 discuss four potential concerned topics by vendor scientific user and ws4gee developers 6 1 added value of ws4gee the ws4gee can build a bridge between gsw and an emerging cloud based platform google earth engine firstly both gee enabled dataset and processing gee functions models are published as gws within ogc standards hence they can easily interact with other gwss as illustrated in the experiments gee enabled wcss could participate in a wps service as its complex input and a gee enabled wps service can be integrated with other gwss in a geoprocessing workflow by integrating gee into gws the time cost for several tasks that demand high computing capacity could be reduced from a few hours or days to even a few minutes which greatly enhances the capabilities and promotes broader use of gws secondly ws4gee can enlarge the application scope of gee since it is unreasonable to expect gee to contain all functionalities applicable to any research problem it becomes significant to integrate gee with existing gwss in this regard ws4gee can relieve scientists from the complexity of the integration and avoid the technical complexity like the long learning curve of using gee and other issues such as time consuming manual operations once user defined gee models are wrapped and released as gsws they become accessible to every scientist through the network and are not restricted to developers in gee environment thirdly wrapping gee resources including dataset and processes functions and models as geospatial web services also benefits the gws community as previously said few studies treat gee as a resource base for gws because the gee environment is isolated from the gws community since gee has gathered thousands of datasets from their original providers obtaining a dataset through gee can save time in data queries and acquisition likewise functionalities and models in gee provide more choices to solve complicated environmental issues finally some criticism suggests that the ogc web service standard is still too complex thus slowing the adoption rate of gws by endpoint users sun et al 2019 nevertheless we argue that this problem can be alleviated by existing technology like geosquare wu et al 2015 that interprets ogc web services with a readable user interface on the other hand the ultimate goal of ws4gee is to integrate gee with gws community rather than the selection of the most suitable gws interfaces another consideration is about the backward compatibility issue of the standard for instance although wps 2 0 2 standard is the latest version of ogc wps standard which offers two new operations getstatus and getresult to support asynchronous processing as well as immediate processing ogc 2018 it does not interoperate with wps 1 0 well as wps 1 0 0 is the most common used wps standard in current gws applications zhang et al 2019 it takes additional efforts to make wps 2 0 2 compatible with existing gws applications thus the adoption of the latest versions of such ogc standards would be the next step for the ws4gee system and the efficiency of different web service approaches such as soap simple object access protocol restful standards and others also require further exploration 6 2 technical limitations the current release of ws4gee still has some limitations from a software development point of view the current system was just a prototype to validate feasibility of integrating gee with wider gws community richer functionalities are required for the ws4gee system we only wrapped part of the gee resources the integration of certain complex functionalities such as machine learning are still unfinished because of the complex types of parameters specific data structures should be developed to utilize these functionalities as a web service moreover gee apis and functions can be implemented with either javascript or python thus we should provide a javascript version of ws4gee for javascript models as well meanwhile building a more comprehensive human computer interaction hci environment for users should also be considered for better use experience fortunately several packages are already available for this issue for instance the geemap python package provides functionality to analyze and visualize earth engine datasets interactively wu 2020 thus the integration of geemap into ws4gee client side could be a potential solution from a research point of view service collaboration and high quality geoprocessing workflows are critical presenting new challenges to the current ws4gee how can we make comparisons between a gws provided by a gws server and gee how do we monitor the qos of gee enabled gws additionally what are the criteria when replacing a gee enabled gws with other gws if it does not work or runs overtime many questions remain to be answered 6 3 development topics finally it is of great importance to discuss issues that might benefit or impact further development of ws4gee reproductivity expense authentication and challenge 1 reproducibility reproducibility is strongly correlated with repeatability interoperability offers a partial solution for reproducibility lacayo et al 2021 ws4gee implemented ogc specifications for gee enabled gwss to solve interactive operation as a consequence the workflows consisting of gee enabled geospatial data and geoprocessing services and other gwss can be described by web service workflow solutions that are compliant with gws standards such as bpel or bpmn for reuse and repeat meek et al 2016 yu et al 2012 and the performance of the workflow is determined by the geospatial web services themselves 2 expense as mentioned several times previously gee supports free access to massive datasets with shared computing resource for geospatial analysis and thus enables time efficiency and low development expense services for gws communities and promotes cooperation among different groups our experiment in table 4 also demonstrates the same point indeed measuring the cost is a complex problem and cannot be judged simply more factors in the processing should be involved such as the time to upload data the transmission fee among different sizes of data etc nevertheless we can still draw that ws4gee reduce the time and usage cost for gws with large scale and multiple remote sensing images 3 authentication ws4gee build upon the gee platform based on the google cloud so the vendor s attitude determines the system s rationality to exist the original intention of gee is to empower a much wider nonprofit use to perform geospatial analysis gorelick et al 2017 the use of ws4gee does not go beyond this principle in spite of this google has released the commercial version of gee to avoid the commercial issues such as privacy or frequent use limitation and obtain a sustainable and dedicated environment the commercial use and the subscription of gee should be considered 4 challenge towards rapid development of the cloud technology several significant attempts have been made in the geospatial community such as the spatiotemporal asset catalog stac specification for describing and cataloging spatiotemporal assets and the cloud optimized geotiff cog for storing raster data in cloud storage these works are concluded as cloud native geospatial service in the standard and interoperation level these new coming services might come up with new interoperability standards for accessibility posing challenges to the ws4gee to adopt and integrate these standards with those existing standards fortunately the ogc has started the exploration for standardized geospatial specification for the cloud native services echterhoff et al 2021 the fit of ogc apis that build upon the legacy of the ogc web service standards become a potential solution to this challenge because the ogc api sits one level above the discussed cloud native constructs while it is compatible with existing gwss natively currently despite the ws4gee framework mainly considers the integration with google earth engine this approach can guide the integration between existing gwss and the cloud native geospatial architectures in the future 7 conclusion and future works this paper presents the design implementation and use cases of a novel system called ws4gee the results indicate that gee datasets functions and user defined models can be published with less effort and thus expands the data and processing services available to the wider gws community more importantly we illustrate how ws4gee supports gee in geoprocessing workflow composition to better leverage its cloud computing power by integrating gee resources as gee enabled geospatial data and geoprocessing services scientists can invoke gee for higher performance environmental modeling that is compliant with ogc standards in particular our case study introducing geosquare enables easy deployment and use of gwss demonstrates the usability and feasibility of gee enabled gws overall our study presents a promising approach to integrate gee enabled gws that will benefit scientists working to solve time consuming and labor intensive tasks based on a common ogc standard in the next stage we are going to tackle the limitations and challenges mentioned previously and improve proposed ws4gee framework more importantly the thought to couple gws with the geospatial service in the cloud through standard specification will be our guideline to face challenges from the upcoming cloud native geospatial services we believe that scientists from various domains will take advantage of this approach and ws4gee to apply cloud based geospatial resources for their scientific tasks in the future declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledges we would like to express our sincere gratitude to the reviewers and editors for their valuable comments that help improve this paper the work was supported by national natural science foundation of china no 41930107 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105636 
