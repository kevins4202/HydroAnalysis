index,text
25745,calibration is an essential step that is involved in the application of process based distributed hydrological models the calibration process focuses on improving the performance index values rather than improving the process dynamics the poor process dynamics in the model reduces its predictive power the current study proposes a modified calibration approach mca for the soil and water assessment tool swat model which helps to improve the process dynamics in model through constraining the parameters the modified calibration method uses the association between physically interpretable swat model parameters as soft data for calibration the proposed calibration approach is validated in the little river experimental watershed usa the results indicate that the proposed calibration method improves the process representation and spatial prediction in the calibrated model compared to the standard calibration procedure based on streamflow data graphical abstract comparison of modified calibration approach mca and standard calibration approach sca in terms of streamflow and soil moisture prediction at the watershed outlet for case i 2006 2011 and case ii 2012 2016 image 1 keywords soil and water assessment tool calibration amalgam curve number process based distributed models plant available water content hydraulic conductivity 1 introduction process based distributed hydrological models have gained importance in solving water resources and environmental management issues these models are ideal for analyzing the impact of environmental changes and human induced disturbances to the ecosystem beven and binley 1992 grayson et al 1992 tan et al 2019 the hydrological processes are modeled independently in these models and integrated through conservation laws neitsch et al 2005 jaber and shukla 2012 even though these models use the physical laws for process representation the number of parameters involved in these models are much higher than the conventional models the parameters in process based models are of two categories physically interpretable parameters and pure calibration parameters that cannot be identified with measurements the physically interpretable parameters will help to account for the spatial and temporal variations in the watershed due to the difference in the scale of field measurements these parameters need to be further fine tuned through the calibration process sun et al 2017 hence calibration is a crucial step that is involved in the application of these models the physically interpretable parameters in these models can significantly influence the prediction of more than one hydrological process since the hydrological processes are inter related the calibration of these models commonly uses streamflow as a calibration variable for optimizing the parameter values however the calibration process focuses on improving the performance indices rather than improving the process representation in these models white and chaubey 2005 have reported that the calibration process with a single hydrological variable may not ensure a proper simulation of other surface and sub surface hydrological fluxes the improved process representation in these models increases its predictive power the predictive power of these models can be improved by constraining the model architecture model parametrization model objectives and model prior knowledge hrachowitz et al 2014 there are studies that recommend the use of soft data for calibration of the process based distributed hydrological models seibert 1997 aronica et al 1998 seibert and mcdonnell 2002 2015 efstratiadis and koutsoyiannis 2010 pfannerstill et al 2017 the soft data need not be a piece of measured field information however it can be information on the individual process an average annual estimate or qualitative knowledge from the experiment the use of soft data and measured hydrological variables together for calibration improves the process representation in the model hrachowitz et al 2014 have used different hydrological signatures and expert knowledge about the watershed for calibration and it has significantly improved the process dynamics and the predictive performance of the model however prior knowledge about the watershed is a prerequisite for this approach the governing equation for most of the process based models is the water balance equation hence soil moisture is an important variable that affects the water and energy balance of the model brocca et al 2012 the accurate initialization and modeling of soil moisture are essential for the simulation of streamflow evapotranspiration percolation etc rajib et al 2016 however the uncertainties in the input data model structure and parameters limit the precise prediction of soil moisture in a watershed model the most common approach which is used to improve soil moisture estimation in a hydrological model is by integrating measured soil moisture data with the modeling process through calibration or assimilation the hydrological modeling studies widely use remote sensing based soil moisture data due to the lack of in situ soil moisture measurements kundu et al 2017 the rainfall runoff modeling is improved in many studies by the assimilation of soil moisture data brocca et al 2012 han et al 2012 liu et al 2018 chen et al 2011 however the assimilation of the surface soil moisture data could not improve the streamflow prediction and other subsurface fluxes chen et al 2011 the assimilation of the root zone soil moisture data has a significant impact on predicting other hydrological fluxes brocca et al 2012 kundu et al 2017 used remotely sensed surface soil moisture data to calibrate the soil and water assessment tool swat model the study recommends the use of soil moisture data along with streamflow for the calibration of hydrological models rajib et al 2016 used root zone soil moisture data and streamflow data for a multi variable calibration of the swat model and reported that root zone soil moisture can play a significant role in swat calibration the improvement in the vertical soil moisture prediction can improve the prediction of percolation baseflow and evapotranspiration estimate in a hydrological model yilmaz et al 2008 mcmillan et al 2011 hence a calibration approach that improves the simulation of both root zone soil moisture and streamflow can better represent the water balance of the watershed however the lack of field measured soil moisture data is a limiting factor the literature on calibration of hydrological models shows that further developments required in the existing calibration approaches are in the process representation spatial prediction and runtime efficiency arnold et al 2015 the current study proposes an improved calibration approach for the swat model the hypothesis of the proposed calibration approach is that even though the hydrological processes are modeled independently in swat most of the hydrological processes are interdependent to each other the current study proposes a calibration method that uses the association between physically interpretable swat model parameters as soft data to constrain the parameter generation in the standard calibration method the assumption behind the proposed approach is that the interaction between the parameters will preserve the interdependence between the hydrological processes in the model simulation the proposed calibration approach is validated in an experimental watershed little river experimental watershed usa the effectiveness of the proposed calibration approach is analyzed in terms of spatial prediction and process representation 2 materials and methods 2 1 soil and water assessment tool swat model description swat is one of the widely used process based semi distributed hydrological model developed by the agricultural research service of the united states department of agriculture arnold et al 1998 arnold and fohrer 2005 williams et al 2008 gassman et al 2015 has done a comprehensive review on swat development and its applications it is developed to analyze the impact of agricultural management practices on streamflow sediment and nutrients the inputs required for swat model simulation of a watershed are digital elevation model dem landuse soil data and meteorological data the watershed heterogeneity is accounted for in the model by discretizing the watershed into sub watersheds based on the topographic information of the watershed these sub watersheds are further discretized into hydrologic response units hrus hrus are the basic spatial unit of swat model simulations swat simulates fluxes like streamflow evapotranspiration soil water and loadings like sediments and nutrients for each hru individually these outputs from each hru are summed up and then they are routed through the routing phase using methods like variable storage williams 1969 and muskingum chow 1959 simulation of various processes in the land phase of the hydrological cycle in swat is based on the water balance equation 1 s w t s w 0 i 1 t r d a y q s u r f e a w s e e p q g w where s w t is the final soil water content mm h2o s w 0 is the initial soil water content mm h2o r d a y is the amount of precipitation on day i mm h2o q s u r f is the amount of surface runoff on day i mm h2o e a is the amount of evapotranspiration on day i mm h2o w s e e p is the amount of percolation and bypass flow exiting the soil profile bottom on day i mm h2o and q g w is the amount of return flow on day i mm h2o the current study has used streamflow as a calibration variable and the parameters which are significant for streamflow generation are considered for calibration the parameter curve number cn2 f is directly related to the surface runoff generation process the default value of the curve number is taken from the standard table based on the landuse and soil of the watershed scs engineering division 1986 the parameter surface runoff lag surlag will adjust the lag in the time of concentration of the watershed in the scs curve number method after accounting for the initial abstractions and runoff the excess available water infiltrates into the soil flow through each soil layer is simulated using the storage routing technique the parameters soil available water sol awc and soil hydraulic conductivity sol k are physically interpretable swat model parameters that decide the soil moisture storage in each hru the parameter sol awc indicates the available soil water content for the plants swat calculates the soil water for each layer and assumes that the water is uniformly distributed within a given layer the unsaturated flow between the soil layers is indirectly simulated with the depth distribution of plant water uptake and soil water evaporation three methods are incorporated in swat to estimate the potential evapotranspiration the penman monteith method monteith 1965 allen 1986 allen et al 1989 the priestley taylor method priestley and taylor 1972 and the hargreaves method hargreaves et al 1985 the parameters soil evaporation compensation factor esco and plant uptake compensation factor epco are involved in the evapotranspiration estimation process in swat these two parameters help redistribute the soil depth to meet the evaporation and transpiration demand from the upper layers of soil water starts to percolate when the water content in the soil layer exceeds the field capacity and the layer below it is not saturated water percolates to the lower soil layer and then enters the vadose zone thereafter it moves to the shallow aquifer and then to the deep aquifer the parameters alpha bf gwqmn gw revap etc are related to the groundwater component of the swat model more details about the swat model parameters are presented in table 1 with the description and range arnold et al 2012 have reported that the most significant parameters for the streamflow prediction in swat are cn2 f sol awc esco and surlag the swat model uses the soil conservation services scs curve number method and green ampt method for surface runoff prediction the parameters involved in these two methods are cn2 f sol awc and sol k the parameter sol k indicates the saturated hydraulic conductivity of soil the parameters sol awc and sol k are physically interpretable and directly measured from the field the parameter cn2 f is an empirical parameter and can be interpreted from physically measurable variables like landuse soil and slope few studies have tried to develop a relationship between the curve number effective hydraulic conductivity and saturated hydraulic conductivity of soils rudra et al 1985 risse et al 1994 elhakeem and papanicolauo 2012 nearing et al 1996 have proposed an empirical relationship that connects saturated hydraulic conductivity curve number and effective hydraulic conductivity of soil swat model has incorporated this relationship in the green ampt method of surface runoff estimation arnold et al 1998 the parameters sol k and sol awc are indicating the characteristics of the soil the default values of these parameters depends on the dataset used for soil the pedo transfer functions saxton et al 2006 are used to generate the default values of these parameters external to swat the calibration process will further fine tune these default values to improve the prediction of calibration variables 2 2 study area the current study focuses on improving the process representation in the swat model simulations the improvement in the process representation is quantified in terms of similarity in the predicted and measured hydrological variables hence the study needs data rich watersheds that have measurement of different hydrological components the current study considers the little river experimental watershed for validation of the proposed calibration approach the little river experimental watershed is located in georgia usa fig 1 the agricultural research service of united states department of agriculture usda ars has initiated the hydrological study in this watershed in 1967 bosch et al 2007 the watershed drainage area is around 340 km2 and it has a flat terrain with gently sloping uplands the major soil in this watershed is sand and sandy loam with a high infiltration rate there is a seasonally dependent shallow aquifer system present in the watershed and it drains to the stream network bosch et al 2007 the major land use in the watershed is woodland which covers 40 of the watershed area the major crops are peanut and cotton and together it covers 36 of the watershed area the other land use classes that present in the watershed are 18 pasture and 4 water soils having loamy sand texture are predominant in the watershed with an infiltration rate of approximately 5 cm h the soil in the little river experimental watershed is classified as tifton loamy sand 78 pelham loamy sand 8 osier sand 12 and troup sand 2 the climate is humid subtropical with an annual average precipitation of 1200 mm the warmest month in the watershed is july with an average temperature of 26 8 c and the coldest month is january with an average temperature of 10 6 c studies have reported that in the little river watershed the streamflow generation varies between 30 and 40 of the annual precipitation the geology of the watershed restricts the downward movement of infiltrated water and leads to groundwater flow contribution to the channels sheridan 1997 bosch et al 2017 reported that the baseflow contribution in the watershed is significant and it is 53 of the annual streamflow there is a high temporal variation of precipitation within the year 2 3 swat model setup the swat set up is done for the period 2006 2016 the data required for the swat model setup is collected from different sources the dem of 30 m resolution is obtained from the usgs national elevation dataset usgs ned the land use data of 30 m resolution for the year 2011 is obtained from the national land cover database usgs nlcd the state soil geographic dataset statsgo that obtained from the web soil survey usda nrcs https websoilsurvey sc egov usda gov is used as soil layer the current study has selected statsgo soil data for watershed modelling since it is matches with the observed soil moisture measurement in terms of depth the streamflow and meteorological data are collected from the database stewards on the usda website https www nrrig mwa ars usda gov stewards stewards html there are five sub basins present in the watershed the hru delineation is done in the watershed with a threshold of zero percent for land use soil and slope there are 386 hrus presents in the watershed three stream gauging stations are considered for the present study the usgs stream gauge site galr6840 is considered as the watershed outlet the usda ars has setup 18 soil moisture measurement stations at different locations within the watershed the soil moisture measurements are in volumetric unit and are available at depths 50 mm 200 mm and 300 mm the soil moisture measurements are converted into depth units for further comparison with the simulated soil moisture measurements from swat model actual evapotranspiration measurements are not available for the watershed the current study considers two cases of different time periods for the calibration and validation of the model setup the description of swat setup and calibration configuration is provided in table 2 2 4 methodology 2 4 1 modified calibration approach for swat the proposed calibration approach focuses on improving the soil moisture prediction along with the streamflow prediction in calibration there are studies which reported that the improvement in soil moisture prediction will improve the process dynamics in models chen 2011 brocca et al 2012 rajib et al 2016 hence the mca considers the parameters which are related to these two hydrological processes the hypothesis behind the mca is that there is correlation exist between the percentage deviations of the swat model parameters cn2 f sol awc and sol k these are the three hru level parameters significant for predicting surface runoff and soil moisture storage in swat a positive percentage deviation of cn2 f indicates an increase in runoff generation from each hru the increase in runoff generation results in a decrease in the infiltration and percolation losses in each hru the decrease in the infiltration and percolation losses are accounted for in the model by a decrease in the saturated hydraulic conductivity of the soil hence a positive percentage deviation for the parameter cn2 f can result in a negative percentage deviation for the parameter sol k as per the physics of the runoff generation process the negative percentage deviation for the parameter sol k indicates an increase in the water holding capacity of the soil which corresponds to a positive percentage deviation for the parameter sol awc in all the hrus soils are classified into four hydrological soil groups the natural resources conservation service soil survey staff 1996 has defined the hydrological soil groups as a group of soil types which are having similar runoff potential under similar storm and cover conditions also the soils which fall in the category of high runoff potential will have high water holding capacity hence a positive percentage deviation for sol awc is logical for a positive percentage deviation of cn2 f the hydrological processes are modeled independently in swat and integrated through the water balance equation in each hru even though the parameters are independent physical variables corresponding to different hydrological processes some inter connections exist between the parameter variations since the processes are interconnected a calibration approach that preserves the dependence between the parameter variations may preserve the process representation better in swat model predictions the current study considers the parameters which are related to the prediction of only streamflow and soil moisture with an assumption that an improvement in the soil moisture prediction will automatically improve the prediction of other components of the hydrological cycle the flowchart of the proposed calibration approach is presented in fig 2 2 4 2 parameterization of swat swat model setup is done for the little river experimental watershed from 2006 to 2016 it is an experimental watershed in which extensive research has occurred and considerable amount of soft data is available to check the plausibility of the model setup the current study considers 15 swat model parameters which are significant for streamflow generation table 1 since the main focus of the study is to improve the process representation in the calibrated model all the 15 parameters are considered for the calibration process these parameters are in different spatial scales in the swat model basin level and hru level the calibration process fine tunes the basin level parameters in the absolute values within the specified range the range type of these parameters are mentioned as absolute a in table 1 most of the pure calibration parameters are in the basin level and the physically interpretable parameters are in the hru level category the calibration process fine tunes the percentage deviation from the default value in case of hru level parameters hence the hru level parameters are calibrated for its relative values and the parameter range type is denoted as r in table 1 the calibrated value of percentage deviation will apply uniformly to all the hrus for each parameter the default value of the hru level parameters is assigned based on the hru characteristics and it represents the heterogeneity of the watershed the specific range of each parameter table 1 is fixed in such a way that the parameter value stays within the realistic range vema and sudheer 2020 2 4 3 calibration and performance analysis the evolutionary algorithm amalgam is used in the optimization of swat parameters for streamflow prediction at the watershed outlet amalgam is a matlab based optimization algorithm with wide applications and high efficiency raad et al 2009 her et al 2015 it is based on a concept of adaptive multimethod search and the current study used 4 different algorithms for calibration namely genetic algorithm particle swarm optimization adaptive metropolis search and differential evolution the major advantage of amalgam is that it can merge the strength of 4 different search strategies and increase the speed of convergence of solutions to pareto set vrugt 2015 latin hypercube sampling lhs is adopted in amalgam for the generation of the initial population from the specified range of parameters the study has used a population size of 100 in each generation and the maximum number of generations specified for the calibration is 4000 the model performance for streamflow prediction is analyzed with the performance index nash sutcliffe efficiency nse 2 n s e 1 i 1 n o i p i 2 i 1 n o i o 2 where o i and p i are the observed and simulated values of streamflow for the ith pair o is the mean of the observed values and n is the total number of paired values the nse is one of the most commonly used performance indices for swat model simulations in the little river watershed muleta 2011 feyereisen et al 2007 the objective function used in the study is 1 nse and it is a minimization problem the current study also uses a performance index pbias to quantify the performance of the calibrated parameter set the pbias moriasi et al 2015 is capable of analyzing the model performance in the medium flow ranges and is evaluated by 3 p b i a s i 1 n o i p i 100 i 1 n o i the performance index volumetric efficiency ve is used to quantify the accuracy of soil moisture prediction in the streamflow calibrated model ve is a relative absolute measure that can quantify the frequent variations in the soil moisture flux criss and winston 2008 muleta 2011 ve is calculated as 4 v e 1 i 1 n a b s p i o i i 1 n o i the ve value close to 1 indicates the best fit and the values range between to 1 the soil moisture estimates are also analyzed by the performance index pbias 2 4 4 updating of the parameters sol awc and sol k lhs in the amalgam generates the parameter values within the specified range for all the parameters except sol k and sol awc the cn2 f value from the algorithm is used to generate values for the parameters sol k and sol awc nearing et al 1996 have developed an empirical equation to incorporate the land cover impacts into the effective hydraulic conductivity of the soil this equation relates to effective hydraulic conductivity saturated hydraulic conductivity and curve number as 5 k e 56 82 k s 0 286 1 0 051 e 0 062 c n 2 where k e is the effective hydraulic conductivity mm hr k s is the saturated hydraulic conductivity mm hr and cn is the curve number bouwer 1969 has reported that the green ampt effective hydraulic conductivity k e is approximately equivalent to one half of the saturated hydraulic conductivity of the soil k s the percentage deviation of the parameter cn2 f is taken from the algorithm and then applied to the default curve number value of hru to get the updated value of the curve number corresponding to the default and updated value of the curve number for the selected hru the default and the updated values for saturated hydraulic conductivity values are obtained using nearing s and bouwer s relations the percentage deviation of the parameter sol k is obtained from the updated and the default value of saturated hydraulic conductivity for the selected hru the corresponding percentage deviation for the parameter sol awc is obtained from the percentage deviation of sol k through the pedo transfer functions developed by saxton et al 2006 these equations are developed based on the united states department of agriculture usda soil database using the readily available variables of soil texture and organic matter the saturated hydraulic conductivity k s is related to the field capacity θ 33 saturated soil moisture content θ s and permanent wilting point θ 1500 6 k s 1930 θ s θ 33 3 λ here λ 1 b b ln 1500 l n 33 ln θ 33 l n θ 1500 the saturated soil moisture content θ s is a function of field capacity percentage sand and s 33 kpa moisture content the s 33 kpa moisture content is dependent on percentage sand s percentage clay c and organic matter om the series of equations connecting these variables are presented below 7 θ s θ 33 θ s 33 0 097 s 0 043 7a θ 33 θ 33 t 1 283 θ 33 t 2 0 374 θ 33 t 0 015 θ 33 t 0 251 s 0 195 c 0 011 o m 0 006 s o m 7b 0 027 c o m 0 452 s c 0 299 7c θ s 33 θ s 33 t 0 636 θ s 33 t 0 107 θ s 33 t 0 278 s 0 034 c 0 022 o m 0 018 s o m 7d 0 027 c o m 0 584 s c 0 078 the description of the variables used in the above equations is presented in table 3 the percentage sand percentage clay and organic matter content of the soil in the selected hru is taken from the swat soil database usersoil for the chosen hru the default values of k s θ s θ 33 and θ 1500 are obtained from equations 6 8 based on the soil texture the percentage deviation of the saturated hydraulic conductivity sol k obtained earlier is applied to the default value of k s to obtain the updated value of k s then unknown in equation 6 is the updated field capacity value corresponding to the updated saturated hydraulic conductivity the permanent wilting point θ 1500 of a soil depends on percentage sand percentage clay and organic matter value saxton et al 2006 8 θ 1500 θ 1500 t 0 14 θ 1500 t 0 02 θ 1500 t 0 024 s 0 487 c 0 006 o m 0 005 s o m 8a 0 013 c o m 0 068 s c 0 031 the permanent wilting point is kept constant for a particular soil and the plant available water content sol awc is updated with the variation of field capacity in the current study the updated value of sol k is used to update the field capacity value using equation 6 the updated value of the parameter sol awc is obtained by extracting the permanent wilting point from the updated field capacity value and is used to calculate the percentage deviation from the default value for the selected hru the percentage deviations of sol k and sol awc values are uniformly applied to all hrus a realistic range of percentage deviation for these two parameters is ensured by putting additional constraints on the range specified in table 1 2 4 5 comparison of mca with standard calibration procedure the current study compares the mca with the standard calibration approach sca in terms of the process representation spatial prediction runtime efficiency and reduction in uncertainty in sca the parameter values are randomly selected from the prescribed range without considering the correlation the process representation in both the calibration approaches are analyzed by comparing the prediction of other hydrological variables like soil moisture evapotranspiration and lagged runoff spatial prediction ability of the calibrated parameter set is analyzed by considering the prediction of both streamflow and soil moisture in the upstream gauged sub basins the convergence speed of the objective function is considered to check the runtime efficiency of the calibration approaches in the current study the uncertainty reduction is correlated to the reduction in equifinality and it is analyzed by considering the variability in the values of each parameter from the final ensemble parameter sets the proposed calibration approach needs to be validated in different hydro climatic conditions however in the current study the analysis is restricted to the little river watershed due to the limited availability of experimental watershed data even then the calibration is done in two different time frames in the little river watershed to analyze the effect of different climatic conditions on the proposed approach the analysis is conducted as two cases and the calibration approaches are compared in both the cases in case i the watershed is calibrated for a period 2006 2011 and validated the calibrated parameter sets in the period 2012 2016 in case ii the model is calibrated for the period 2012 2016 and validated in the period 2006 2011 3 results and discussion 3 1 calibration and validation of the swat model the swat model setup is done for the little river experimental watershed over a period of ten years 2006 to 2016 the analysis of the study has been done in two time frames as case i and case ii the swat check analysis shows that the groundwater component is the significant hydrological component of the watershed bosch et al 2007 have also reported a similar fact that there is a groundwater contribution of 53 in the streamflow of the little river experimental watershed evapotranspiration and streamflow are the other two significant hydrological processes in the watershed feyereisen et al 2007 have reported that around 30 of annual rainfall is contributing to surface runoff and the remaining 70 is going as evapotranspiration in lrew a similar response is observed in the swat setup of the little river watershed the 15 swat model parameters which are significant for streamflow prediction in swat are considered for calibration the amalgam optimizer is used in the current study with the maximum number of iterations of 4000 in both the cases the maximum number of iterations generations are fixed such that the objective function will converge within the specified number of generations the calibration approaches have tried to minimize the objective function 1 nse in both the cases the streamflow measurements at the watershed outlet subbasin 5 is considered for optimizing the parameter sets one at a time sensitivity analysis arnold et al 2012 of parameters shows that the parameter cn2 f is the most significant parameter for streamflow prediction in little river watershed the parameters sol awc and sol k are in the fifth and sixth position in terms of sensitivity for streamflow prediction table 1 the association between the percentage deviations of the parameters cn2 f sol k and sol awc are used as soft data while calibrating the model in mca the percentage deviations of the parameters sol k and sol awc are obtained corresponding to the variations of the parameter cn2 f the swat model setup is calibrated for a period 2006 2011 in case i the mca has simulated the streamflow at the watershed outlet reasonably well in case i with a nse value of 0 72 and a pbias value of 28 4 the nse value above 0 5 and the pbias value within 25 are considered acceptable in the streamflow prediction of a process based distributed hydrological model moriasi et al 2007 the soil moisture prediction in the calibration outlet is reasonably good in the mca with a ve value of 0 74 and a pbias value of 9 9 table 4 the sca simulates the streamflow in the watershed outlet with an nse value of 0 74 and pbias of 8 47 fig 3 a the streamflow simulation from both the calibration approaches are presented in fig 3a it is observed that the streamflow prediction from sca is superior to that of mca the mca results in a compromising parameter set which can make reasonably good simulation for both streamflow and soil moisture the mca parameter selection is happening very similar to multi variable calibration of the model rajib et al 2016 the mca has improved the simulation of soil moisture with a slight compromise on the streamflow simulation in comparison with the sca the soil moisture prediction in the sca is an overprediction with a ve value of 0 57 and a pbias value of 37 the negative value of pbias indicates an overprediction of soil moisture in sca the soil moisture prediction from both the approaches at the watershed outlet is presented in fig 3a in case ii swat setup is calibrated for the period 2012 2016 in both the calibration approaches the mca simulates the streamflow in the watershed outlet with a nse value of 0 69 the soil moisture prediction in the calibration outlet is also reasonably good in the mca ve value of 0 70 the sca simulates the streamflow with a nse value of 0 75 in the watershed outlet table 5 the streamflow prediction from both the calibration approaches at the watershed is presented in fig 3b the calibrated parameter set from the mca is slightly overpredicting the streamflow in comparison with the sca however the soil moisture prediction in the sca is an overprediction with a ve value of 0 37 the soil moisture prediction from both the approaches at the watershed outlet is presented in fig 3b the improvement in the soil moisture prediction of the mca is clearly visible in case ii also this improvement in soil moisture prediction without much compromise on the streamflow prediction is a major advantage of the mca the calibrated parameter sets from both the cases are validated for the rest of the time period the calibrated parameter sets from case i is validated in the period 2012 2016 the parameter set of the mca in case i simulates the streamflow in the validation period with an nse value of 0 44 at the watershed outlet fig 4 a the sca also simulates the streamflow with similar level of accuracy nse value is 0 44 in the validation period table 4 the other performance index pbias is calculated and its value is comparatively better in the sca the mca is able to simulate the peaks better than sca however the overall performance is better for the sca the soil moisture prediction is better in mca with a ve value of 0 74 the soil moisture prediction of the sca is an overprediction with a ve value of 0 55 in the validation period similarly the calibrated parameter sets from the case ii is used for hydrological simulation for the period 2007 2011 the results are presented in fig 4b in case ii also the calibrated parameter set resulting from the mca is simulating the streamflow and soil moisture well in the validation period as compared to the sca at the watershed outlet the mca is able to simulate the streamflow well with an nse value of 0 57 and soil moisture with a ve value of 0 71 fig 4b the sca has resulted in an nse value of 0 39 for streamflow prediction and a ve value of 0 38 for soil moisture prediction the mca approach is simulating both streamflow and soil moisture reasonably well when compared to the sca in both the cases this improved prediction in the mca may be because of the improvement in the process representation the hydrological processes are modeled independently in swat and integrated through the water balance equation the swat model structure has a limitation in calibrating the model for both the streamflow and soil moisture the swat model is widely used for impact analysis studies due to its process based nature hence the process representation in the model is more important than its predictability on a single variable beven and o connell 1982 the optimal parameter sets from both the calibration approaches in case i and ii are presented in table 1 significant differences are observed in the calibrated parameter values of cn2 f sol k and sol awc these variations in the parameter values are indicating a considerable difference in the simulation of different hydrological fluxes in both the approaches the improvement in the soil moisture prediction can influence the simulation of evapotranspiration percolation and baseflow brocca et al 2012 rajib et al 2016 3 2 convergence of objective function the runtime required for calibration depends on the number of parameters involved number of variables considered for calibration optimization algorithm used for calibration and the calibration approach in the current analysis all the above mentioned factors are the same except the calibration approach the maximum number of generations considered in both the calibration approaches are 4000 in both the cases with nse as a likelihood index for calibration the convergence of the objective functions are very similar in both the calibration approaches the initial values of objective functions are less in mca as compared to that of sca however the improvement in the initial values has not resulted in a faster convergence of objective function in mca the most significant hydrological process that affect the streamflow generation in lrew is the groundwater contribution however the mca is not directly focuses on the groundwater parameters mca is based on the assumption that improvement in the soil moisture simulation improves the simulation of other hydrological fluxes faster convergence of the objective function may happen for mca in a watershed which has prominent surface runoff contribution to the streamflow the proposed calibration approach focuses only on the physics of the surface runoff generation process however there are many more physically interpretable parameters in swat and they are connected to multiple hydrological processes a calibration method that can accommodate the relative variations of all these parameters can further reduce the dimensionality of the calibration and result in faster convergence of the objective function 3 3 spatial prediction of streamflow and soil moisture the calibrated parameter sets from both the calibration approaches have tested in the upstream subbasins for the simulation of soil moisture and streamflow the current study restricts the spatial analysis to subbasins 1 and 2 due to the limited data availability in subbasins 3 and 4 the parameter set from the mca in case i has resulted in reasonably good streamflow simulation in the calibration period of subbasin 1 and 2 with nse values 0 58 and 0 63 respectively the pbias values are also within the acceptable limit table 4 the soil moisture simulation is also reasonably good in sub basin 2 with a ve value of 0 59 and pbias of 18 however the soil moisture prediction in subbasin 1 is poor with a ve value of 0 36 the simulation of both soil moisture and streamflow are reasonably good in the validation period in subbasins 1 and 2 table 4 the spatial prediction of both the fluxes during calibration and validation period are good for the parameter set from the mca in case ii however the soil moisture flux in subbasin 1 is poor during the validation period as observed in calibration period of case i this variation in the soil moisture simulation in subbasin 1 during 2007 2011 may be because of the uncertainty associated with the model setup the calibrated parameter set of the sca in case i simulates the streamflow well in the upstream subbasins with nse values 0 58 and 0 63 respectively during the calibration period however the prediction of corresponding soil moisture fluxes are very poor ve values of 0 19 and 0 22 the soil moisture prediction in subbasin 1 is poor in both the calibration approaches it may be because of the uncertainty associated with the hydrological model setup in general the spatial prediction using the parameter sets from sca has resulted in reasonably good streamflow prediction in the upstream subbasins and poor simulations of soil moisture fluxes this analysis indicates that the calibrated parameter set from mca is better than the sca for spatial prediction of both streamflow and soil moisture in little river watershed the improved process dynamics in mca may be the reason for better spatial prediction 3 4 process representation in the calibrated swat model the simulation of surface and sub surfaces hydrological fluxes in the calibrated swat model setups are analyzed the mca is able to improve the soil moisture prediction without considering the soil moisture data for calibration the better simulations of soil moisture flux can improve the simulation of other hydrological fluxes brocca et al 2012 mcmillan et al 2011 has developed a number of diagnostic tests to check the process representation in the conceptual hydrological models using experimental data the current study has taken a diagnostic test based on soil moisture and analyses the process representation in calibrated swat model setups 3 4 1 soil moisture soil moisture is an essential component of the water balance equation affecting the energy and water balance of the hydrological cycle the soil moisture estimate in the watershed outlet is the weighted average of soil moisture estimates of all the hrus in the sub basin 5 statsgo soil data has been used in the study and root zone depth in most of the hrus is in the range of 300 mm in the little river basin and the soil moisture measurements are available at a depth of 50 mm 200 mm and 300 mm as discussed earlier the mca can simulate the soil moisture well and soil moisture estimate is an average soil moisture for the entire soil thickness the soil moisture estimate from the model is compared with the average soil moisture measurements in a subbasin scale there are 5 soil moisture measurement stations present in the subbasin 5 and the average of these soil moisture measurements are compared with the simulated soil moisture estimate in subbasin level the vertical soil moisture distribution in different hrus in both the calibration approaches in both the cases are compared are presented in fig 5 a and b the vertical soil moisture distribution from the mca is close to the observed soil moisture values at different depths in both the cases the sca results in an overprediction of the soil moisture for the entire soil thickness and the deviations increase with the increase in depth the improvement in the vertical soil moisture distribution can affect the evapotranspiration percolation and groundwater component of the water balance equation in each hru the annual soil moisture storage in subbasin 5 is presented in fig 5 c and d the mca simulates the soil moisture storage better than the sca without considering the soil moisture data for calibration the mca preserves the physics of the runoff generation process by preserving the association between percentage variations of the parameters cn2 f sol awc and sol k this helps in an improved simulation of both streamflow and soil moisture in the calibrated swat model 3 4 2 evapotranspiration et the evapotranspiration demand of a watershed is met from different soil layers in different seasons yilmaz et al 2008 hence the improvement in the root zone soil moisture content will improve the estimation of et in all the seasons the interception storages are initially considered to meet the et demand in swat and the further et requirement is met from transpiration and soil water evaporation the soil water evaporation is modeled with an exponential depth distribution of soil in swat neitsch et al 2002 there are constraints to extract moisture from different soil layers to meet the evaporative demand the parameters esco and epco help in the redistribution of soil depth to meet the et demand of the watershed these parameter values have a significant difference in both the calibration approaches table 1 the difference in the soil moisture storage is reflected in the evapotranspiration estimates also the annual et estimates from both the calibration approaches differ by 50 60 mm in both the cases the et estimates are relatively less in mca compared to the sca this reduction is proportional to the reduction in soil moisture storage in the mca the values of the parameters esco and epco are functions of soil moisture content hydraulic conductivity water holding capacity of the soil etc the evapotranspiration component of the watershed is around 60 70 of the annual rainfall https www nrrig mwa ars usda gov stewards stewards html the et estimates from both the calibration approaches fall within this limit however a comparison of both the calibration approaches in terms of accuracy of et prediction is not possible since the measured actual evapotranspiration values are not available in the watershed 3 4 3 lagged runoff the lagged runoff component of streamflow includes the lateral flow and baseflow contributions to the streamflow this component in the measured streamflow at the watershed outlet is separated using an automatic baseflow filter program proposed by arnold et al 1995 it is a digital filter technique that separates the lagged runoff component from streamflow that is analogous to the filtering of high frequency signals in signal processing the baseflow contribution in the little river watershed is significant during the period january may bosch et al 2017 the lagged runoff separated using the filter program represents a similar nature that is observed in the field the simulated lagged runoff component from both the calibration approaches are compared with the separated lagged runoff from measured streamflow and it is presented in fig 6 for both the cases the mca in both the cases are able to simulate the lagged runoff close to the observed as presented in fig 6 the lagged runoff component in the sca is highly overpredicting the streamflow data used for calibration in case i carries more data points in the medium low flow ranges the lagged runoff contribution to a stream is directly connected to the flow in the medium low ranges the improvement in both streamflow and soil moisture may have resulted in an improved simulation of lagged runoff in mca of case i however the lagged runoff prediction in 2013 is a significant overprediction in both the calibration approaches the calibration period 2012 2016 has a medium to high flow range in the watershed the nature of streamflow in the watershed and the performance index used for calibration have significant influences on the parameter set the parameters alpha bf and gwqmn have significantly different values in both the calibration approaches and that influences the lagged runoff prediction in both the calibration approaches 3 4 4 surface runoff the streamflow of a watershed includes two components the lagged runoff component and the surface runoff component the surface runoff component of the measured streamflow is obtained by separating the lagged runoff the runoff simulation in both the calibration approaches are compared with the observed runoff values the analysis has been done separately for both the cases and presented in fig 6 the runoff simulation in the sca is close to the observed runoff values than that of the mca in case 1 the runoff estimates from the mca are over predictions during the low flow years and matches with the observed during the high flow in case i the swat model structure has limitations in calibrating the model for both surface runoff and soil moisture hence the compromising parameter set from the mca has resulted in an overprediction of surface runoff in low flow years of case i as discussed before the surface runoff generation in case ii is much higher than that of case i the surface runoff generation of the mca in case ii is an overprediction in most of the years except that of 2014 and 2015 the analysis of surface runoff in case ii indicates that the characteristics of streamflow data considered for calibration has significant effect on the parameter set 3 4 5 streamflow in the current study both the calibration approaches optimize the swat parameters with the streamflow measurement at the watershed outlet the streamflow prediction at different subbasins of the watershed from both the calibration approaches are analyzed and presented in section 3 3 the mca has optimized a parameter set which can simulate both the streamflow and soil moisture reasonably well with a slight compromise on the streamflow prediction in comparison with the sca the streamflow prediction in different flow ranges from both the calibration approaches are analyzed in this section the flow duration curve of simulated streamflow in both the cases are compared with that of the observed streamflow fig 7 the streamflow is classified into different flow ranges the flow with an exceedance probability range of 0 0 02 is considered as a high flow flow in a range of 0 02 0 7 exceedance probability is a medium flow if the exceedance probability range falls within the range of 0 7 1 then it is termed as a low flow the high flow prediction in case i is an underprediction in both the calibration approaches however both the calibration approaches have simulated the medium and low flow ranges well even though the streamflow simulation in mca is slightly inferior to that of the sca the prediction in medium and low flow ranges are equally good the improved lagged runoff simulation in the mca may have resulted in a better simulation of medium and low flow ranges in case ii the high flow simulation in the mca is slightly better than that of the sca however the medium flow prediction in mca is showing higher deviations from the observed in comparison with sca the streamflow prediction in different flow ranges depends on the nature of data considered for calibration and the performance index used for the study the current study accounts for the variation in the flow characteristics by doing analysis for different time periods however the effect of different performance indices are not considered and that is one of the limitations of the study 3 5 reduction of parametric uncertainty equifinality is an issue associated with the conventional calibration approaches the different parameter sets would result in similar model performance and the selection of a better performing parameter set is difficult beven and binley 1992 the current study analyses the equifinality by considering the ensemble parameter sets from both the calibration approaches the parameter sets that results in the minimum value of the objective function is considered for the analysis the variability in the values of each parameter in the ensemble sets from both the cases are represented using the boxplot and it is presented in fig 8 the x axis is the parameters considered in the calibration process and the y axis is the normalized range of the parameters the variability in the parameters related to groundwater and evapotranspiration has reduced in mca as compared to sca in both the cases this may be because of the improved process dynamics in the calibrated model however the parameters cn2 f sol k and sol awc are showing higher variability in mca than in sca the trade off that exist in the calibration of model for streamflow and soil moisture may be the reason for higher variability of those parameters in mca as compared to that of sca parameters other parameters significant for streamflow and peak runoff are showing less variability in mca than in sca the reduction of equifinality in the mca is indicating a reduced parametric uncertainty in the proposed calibration approach 4 summary and conclusion the current study proposes a modified calibration approach for the swat model it helps to improve the process representation in model with limited available data the hydrological processes are modeled independently in swat and it is integrated through the water balance equation in each hru the association between the parameters cn2 f sol k and sol awc for runoff generation process is used as soft data while calibrating the model the mca is tested in the little river experimental watershed georgia usa in two different time frames as two cases the goodness of fit statistics for streamflow and soil moisture prediction at the watershed outlet are reasonably good in both the cases of the mca in case i it is resulted in nse value of 0 72 for streamflow simulation and the ve value of 0 74 for soil moisture simulation while in case ii it gives nse value of 0 69 for streamflow prediction and ve value of 0 70 for soil moisture prediction the sca has simulated the streamflow well in both the cases with a poor simulation of soil moisture the results of the proposed calibration approach are compared with that of the sca in terms of process representation spatial prediction run time efficiency and reduction in uncertainty the proposed calibration approach has simulated the vertical soil moisture distribution in the watershed better the improvement in the root zone soil moisture has improved the simulation of other fluxes like evapotranspiration and groundwater contribution in the swat model the proposed calibration approach has resulted in a compromising parameter set for streamflow prediction in swat the improvement in the process representation has helped for a better spatial prediction of hydrological variables in the proposed calibration approach in both the cases the parametric uncertainty is considerably reduced in the mca as compared to the sca the proposed calibration approach has a central theme that is applicable for all hydrological models since mathematical representation of complex hydrological system is a global issue the improved process representation in the calibrated model will improve its applicability in the impact analysis studies the mca is validated in a us watershed however it is applicable to any watershed irrespective of its characteristics and location as the approach is based on the physics of the hydrological processes the calibration result is highly dependent on the performance index used for calibration though the current study has not analyzed the effect of different performance indices on the proposed calibration approach the interactions between the physically interpretable swat parameters can be accommodated in the model structure itself this may lead to more robust hydrological simulation in the swat model and it can also reduce the data requirements for better process representation in the swat declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25745,calibration is an essential step that is involved in the application of process based distributed hydrological models the calibration process focuses on improving the performance index values rather than improving the process dynamics the poor process dynamics in the model reduces its predictive power the current study proposes a modified calibration approach mca for the soil and water assessment tool swat model which helps to improve the process dynamics in model through constraining the parameters the modified calibration method uses the association between physically interpretable swat model parameters as soft data for calibration the proposed calibration approach is validated in the little river experimental watershed usa the results indicate that the proposed calibration method improves the process representation and spatial prediction in the calibrated model compared to the standard calibration procedure based on streamflow data graphical abstract comparison of modified calibration approach mca and standard calibration approach sca in terms of streamflow and soil moisture prediction at the watershed outlet for case i 2006 2011 and case ii 2012 2016 image 1 keywords soil and water assessment tool calibration amalgam curve number process based distributed models plant available water content hydraulic conductivity 1 introduction process based distributed hydrological models have gained importance in solving water resources and environmental management issues these models are ideal for analyzing the impact of environmental changes and human induced disturbances to the ecosystem beven and binley 1992 grayson et al 1992 tan et al 2019 the hydrological processes are modeled independently in these models and integrated through conservation laws neitsch et al 2005 jaber and shukla 2012 even though these models use the physical laws for process representation the number of parameters involved in these models are much higher than the conventional models the parameters in process based models are of two categories physically interpretable parameters and pure calibration parameters that cannot be identified with measurements the physically interpretable parameters will help to account for the spatial and temporal variations in the watershed due to the difference in the scale of field measurements these parameters need to be further fine tuned through the calibration process sun et al 2017 hence calibration is a crucial step that is involved in the application of these models the physically interpretable parameters in these models can significantly influence the prediction of more than one hydrological process since the hydrological processes are inter related the calibration of these models commonly uses streamflow as a calibration variable for optimizing the parameter values however the calibration process focuses on improving the performance indices rather than improving the process representation in these models white and chaubey 2005 have reported that the calibration process with a single hydrological variable may not ensure a proper simulation of other surface and sub surface hydrological fluxes the improved process representation in these models increases its predictive power the predictive power of these models can be improved by constraining the model architecture model parametrization model objectives and model prior knowledge hrachowitz et al 2014 there are studies that recommend the use of soft data for calibration of the process based distributed hydrological models seibert 1997 aronica et al 1998 seibert and mcdonnell 2002 2015 efstratiadis and koutsoyiannis 2010 pfannerstill et al 2017 the soft data need not be a piece of measured field information however it can be information on the individual process an average annual estimate or qualitative knowledge from the experiment the use of soft data and measured hydrological variables together for calibration improves the process representation in the model hrachowitz et al 2014 have used different hydrological signatures and expert knowledge about the watershed for calibration and it has significantly improved the process dynamics and the predictive performance of the model however prior knowledge about the watershed is a prerequisite for this approach the governing equation for most of the process based models is the water balance equation hence soil moisture is an important variable that affects the water and energy balance of the model brocca et al 2012 the accurate initialization and modeling of soil moisture are essential for the simulation of streamflow evapotranspiration percolation etc rajib et al 2016 however the uncertainties in the input data model structure and parameters limit the precise prediction of soil moisture in a watershed model the most common approach which is used to improve soil moisture estimation in a hydrological model is by integrating measured soil moisture data with the modeling process through calibration or assimilation the hydrological modeling studies widely use remote sensing based soil moisture data due to the lack of in situ soil moisture measurements kundu et al 2017 the rainfall runoff modeling is improved in many studies by the assimilation of soil moisture data brocca et al 2012 han et al 2012 liu et al 2018 chen et al 2011 however the assimilation of the surface soil moisture data could not improve the streamflow prediction and other subsurface fluxes chen et al 2011 the assimilation of the root zone soil moisture data has a significant impact on predicting other hydrological fluxes brocca et al 2012 kundu et al 2017 used remotely sensed surface soil moisture data to calibrate the soil and water assessment tool swat model the study recommends the use of soil moisture data along with streamflow for the calibration of hydrological models rajib et al 2016 used root zone soil moisture data and streamflow data for a multi variable calibration of the swat model and reported that root zone soil moisture can play a significant role in swat calibration the improvement in the vertical soil moisture prediction can improve the prediction of percolation baseflow and evapotranspiration estimate in a hydrological model yilmaz et al 2008 mcmillan et al 2011 hence a calibration approach that improves the simulation of both root zone soil moisture and streamflow can better represent the water balance of the watershed however the lack of field measured soil moisture data is a limiting factor the literature on calibration of hydrological models shows that further developments required in the existing calibration approaches are in the process representation spatial prediction and runtime efficiency arnold et al 2015 the current study proposes an improved calibration approach for the swat model the hypothesis of the proposed calibration approach is that even though the hydrological processes are modeled independently in swat most of the hydrological processes are interdependent to each other the current study proposes a calibration method that uses the association between physically interpretable swat model parameters as soft data to constrain the parameter generation in the standard calibration method the assumption behind the proposed approach is that the interaction between the parameters will preserve the interdependence between the hydrological processes in the model simulation the proposed calibration approach is validated in an experimental watershed little river experimental watershed usa the effectiveness of the proposed calibration approach is analyzed in terms of spatial prediction and process representation 2 materials and methods 2 1 soil and water assessment tool swat model description swat is one of the widely used process based semi distributed hydrological model developed by the agricultural research service of the united states department of agriculture arnold et al 1998 arnold and fohrer 2005 williams et al 2008 gassman et al 2015 has done a comprehensive review on swat development and its applications it is developed to analyze the impact of agricultural management practices on streamflow sediment and nutrients the inputs required for swat model simulation of a watershed are digital elevation model dem landuse soil data and meteorological data the watershed heterogeneity is accounted for in the model by discretizing the watershed into sub watersheds based on the topographic information of the watershed these sub watersheds are further discretized into hydrologic response units hrus hrus are the basic spatial unit of swat model simulations swat simulates fluxes like streamflow evapotranspiration soil water and loadings like sediments and nutrients for each hru individually these outputs from each hru are summed up and then they are routed through the routing phase using methods like variable storage williams 1969 and muskingum chow 1959 simulation of various processes in the land phase of the hydrological cycle in swat is based on the water balance equation 1 s w t s w 0 i 1 t r d a y q s u r f e a w s e e p q g w where s w t is the final soil water content mm h2o s w 0 is the initial soil water content mm h2o r d a y is the amount of precipitation on day i mm h2o q s u r f is the amount of surface runoff on day i mm h2o e a is the amount of evapotranspiration on day i mm h2o w s e e p is the amount of percolation and bypass flow exiting the soil profile bottom on day i mm h2o and q g w is the amount of return flow on day i mm h2o the current study has used streamflow as a calibration variable and the parameters which are significant for streamflow generation are considered for calibration the parameter curve number cn2 f is directly related to the surface runoff generation process the default value of the curve number is taken from the standard table based on the landuse and soil of the watershed scs engineering division 1986 the parameter surface runoff lag surlag will adjust the lag in the time of concentration of the watershed in the scs curve number method after accounting for the initial abstractions and runoff the excess available water infiltrates into the soil flow through each soil layer is simulated using the storage routing technique the parameters soil available water sol awc and soil hydraulic conductivity sol k are physically interpretable swat model parameters that decide the soil moisture storage in each hru the parameter sol awc indicates the available soil water content for the plants swat calculates the soil water for each layer and assumes that the water is uniformly distributed within a given layer the unsaturated flow between the soil layers is indirectly simulated with the depth distribution of plant water uptake and soil water evaporation three methods are incorporated in swat to estimate the potential evapotranspiration the penman monteith method monteith 1965 allen 1986 allen et al 1989 the priestley taylor method priestley and taylor 1972 and the hargreaves method hargreaves et al 1985 the parameters soil evaporation compensation factor esco and plant uptake compensation factor epco are involved in the evapotranspiration estimation process in swat these two parameters help redistribute the soil depth to meet the evaporation and transpiration demand from the upper layers of soil water starts to percolate when the water content in the soil layer exceeds the field capacity and the layer below it is not saturated water percolates to the lower soil layer and then enters the vadose zone thereafter it moves to the shallow aquifer and then to the deep aquifer the parameters alpha bf gwqmn gw revap etc are related to the groundwater component of the swat model more details about the swat model parameters are presented in table 1 with the description and range arnold et al 2012 have reported that the most significant parameters for the streamflow prediction in swat are cn2 f sol awc esco and surlag the swat model uses the soil conservation services scs curve number method and green ampt method for surface runoff prediction the parameters involved in these two methods are cn2 f sol awc and sol k the parameter sol k indicates the saturated hydraulic conductivity of soil the parameters sol awc and sol k are physically interpretable and directly measured from the field the parameter cn2 f is an empirical parameter and can be interpreted from physically measurable variables like landuse soil and slope few studies have tried to develop a relationship between the curve number effective hydraulic conductivity and saturated hydraulic conductivity of soils rudra et al 1985 risse et al 1994 elhakeem and papanicolauo 2012 nearing et al 1996 have proposed an empirical relationship that connects saturated hydraulic conductivity curve number and effective hydraulic conductivity of soil swat model has incorporated this relationship in the green ampt method of surface runoff estimation arnold et al 1998 the parameters sol k and sol awc are indicating the characteristics of the soil the default values of these parameters depends on the dataset used for soil the pedo transfer functions saxton et al 2006 are used to generate the default values of these parameters external to swat the calibration process will further fine tune these default values to improve the prediction of calibration variables 2 2 study area the current study focuses on improving the process representation in the swat model simulations the improvement in the process representation is quantified in terms of similarity in the predicted and measured hydrological variables hence the study needs data rich watersheds that have measurement of different hydrological components the current study considers the little river experimental watershed for validation of the proposed calibration approach the little river experimental watershed is located in georgia usa fig 1 the agricultural research service of united states department of agriculture usda ars has initiated the hydrological study in this watershed in 1967 bosch et al 2007 the watershed drainage area is around 340 km2 and it has a flat terrain with gently sloping uplands the major soil in this watershed is sand and sandy loam with a high infiltration rate there is a seasonally dependent shallow aquifer system present in the watershed and it drains to the stream network bosch et al 2007 the major land use in the watershed is woodland which covers 40 of the watershed area the major crops are peanut and cotton and together it covers 36 of the watershed area the other land use classes that present in the watershed are 18 pasture and 4 water soils having loamy sand texture are predominant in the watershed with an infiltration rate of approximately 5 cm h the soil in the little river experimental watershed is classified as tifton loamy sand 78 pelham loamy sand 8 osier sand 12 and troup sand 2 the climate is humid subtropical with an annual average precipitation of 1200 mm the warmest month in the watershed is july with an average temperature of 26 8 c and the coldest month is january with an average temperature of 10 6 c studies have reported that in the little river watershed the streamflow generation varies between 30 and 40 of the annual precipitation the geology of the watershed restricts the downward movement of infiltrated water and leads to groundwater flow contribution to the channels sheridan 1997 bosch et al 2017 reported that the baseflow contribution in the watershed is significant and it is 53 of the annual streamflow there is a high temporal variation of precipitation within the year 2 3 swat model setup the swat set up is done for the period 2006 2016 the data required for the swat model setup is collected from different sources the dem of 30 m resolution is obtained from the usgs national elevation dataset usgs ned the land use data of 30 m resolution for the year 2011 is obtained from the national land cover database usgs nlcd the state soil geographic dataset statsgo that obtained from the web soil survey usda nrcs https websoilsurvey sc egov usda gov is used as soil layer the current study has selected statsgo soil data for watershed modelling since it is matches with the observed soil moisture measurement in terms of depth the streamflow and meteorological data are collected from the database stewards on the usda website https www nrrig mwa ars usda gov stewards stewards html there are five sub basins present in the watershed the hru delineation is done in the watershed with a threshold of zero percent for land use soil and slope there are 386 hrus presents in the watershed three stream gauging stations are considered for the present study the usgs stream gauge site galr6840 is considered as the watershed outlet the usda ars has setup 18 soil moisture measurement stations at different locations within the watershed the soil moisture measurements are in volumetric unit and are available at depths 50 mm 200 mm and 300 mm the soil moisture measurements are converted into depth units for further comparison with the simulated soil moisture measurements from swat model actual evapotranspiration measurements are not available for the watershed the current study considers two cases of different time periods for the calibration and validation of the model setup the description of swat setup and calibration configuration is provided in table 2 2 4 methodology 2 4 1 modified calibration approach for swat the proposed calibration approach focuses on improving the soil moisture prediction along with the streamflow prediction in calibration there are studies which reported that the improvement in soil moisture prediction will improve the process dynamics in models chen 2011 brocca et al 2012 rajib et al 2016 hence the mca considers the parameters which are related to these two hydrological processes the hypothesis behind the mca is that there is correlation exist between the percentage deviations of the swat model parameters cn2 f sol awc and sol k these are the three hru level parameters significant for predicting surface runoff and soil moisture storage in swat a positive percentage deviation of cn2 f indicates an increase in runoff generation from each hru the increase in runoff generation results in a decrease in the infiltration and percolation losses in each hru the decrease in the infiltration and percolation losses are accounted for in the model by a decrease in the saturated hydraulic conductivity of the soil hence a positive percentage deviation for the parameter cn2 f can result in a negative percentage deviation for the parameter sol k as per the physics of the runoff generation process the negative percentage deviation for the parameter sol k indicates an increase in the water holding capacity of the soil which corresponds to a positive percentage deviation for the parameter sol awc in all the hrus soils are classified into four hydrological soil groups the natural resources conservation service soil survey staff 1996 has defined the hydrological soil groups as a group of soil types which are having similar runoff potential under similar storm and cover conditions also the soils which fall in the category of high runoff potential will have high water holding capacity hence a positive percentage deviation for sol awc is logical for a positive percentage deviation of cn2 f the hydrological processes are modeled independently in swat and integrated through the water balance equation in each hru even though the parameters are independent physical variables corresponding to different hydrological processes some inter connections exist between the parameter variations since the processes are interconnected a calibration approach that preserves the dependence between the parameter variations may preserve the process representation better in swat model predictions the current study considers the parameters which are related to the prediction of only streamflow and soil moisture with an assumption that an improvement in the soil moisture prediction will automatically improve the prediction of other components of the hydrological cycle the flowchart of the proposed calibration approach is presented in fig 2 2 4 2 parameterization of swat swat model setup is done for the little river experimental watershed from 2006 to 2016 it is an experimental watershed in which extensive research has occurred and considerable amount of soft data is available to check the plausibility of the model setup the current study considers 15 swat model parameters which are significant for streamflow generation table 1 since the main focus of the study is to improve the process representation in the calibrated model all the 15 parameters are considered for the calibration process these parameters are in different spatial scales in the swat model basin level and hru level the calibration process fine tunes the basin level parameters in the absolute values within the specified range the range type of these parameters are mentioned as absolute a in table 1 most of the pure calibration parameters are in the basin level and the physically interpretable parameters are in the hru level category the calibration process fine tunes the percentage deviation from the default value in case of hru level parameters hence the hru level parameters are calibrated for its relative values and the parameter range type is denoted as r in table 1 the calibrated value of percentage deviation will apply uniformly to all the hrus for each parameter the default value of the hru level parameters is assigned based on the hru characteristics and it represents the heterogeneity of the watershed the specific range of each parameter table 1 is fixed in such a way that the parameter value stays within the realistic range vema and sudheer 2020 2 4 3 calibration and performance analysis the evolutionary algorithm amalgam is used in the optimization of swat parameters for streamflow prediction at the watershed outlet amalgam is a matlab based optimization algorithm with wide applications and high efficiency raad et al 2009 her et al 2015 it is based on a concept of adaptive multimethod search and the current study used 4 different algorithms for calibration namely genetic algorithm particle swarm optimization adaptive metropolis search and differential evolution the major advantage of amalgam is that it can merge the strength of 4 different search strategies and increase the speed of convergence of solutions to pareto set vrugt 2015 latin hypercube sampling lhs is adopted in amalgam for the generation of the initial population from the specified range of parameters the study has used a population size of 100 in each generation and the maximum number of generations specified for the calibration is 4000 the model performance for streamflow prediction is analyzed with the performance index nash sutcliffe efficiency nse 2 n s e 1 i 1 n o i p i 2 i 1 n o i o 2 where o i and p i are the observed and simulated values of streamflow for the ith pair o is the mean of the observed values and n is the total number of paired values the nse is one of the most commonly used performance indices for swat model simulations in the little river watershed muleta 2011 feyereisen et al 2007 the objective function used in the study is 1 nse and it is a minimization problem the current study also uses a performance index pbias to quantify the performance of the calibrated parameter set the pbias moriasi et al 2015 is capable of analyzing the model performance in the medium flow ranges and is evaluated by 3 p b i a s i 1 n o i p i 100 i 1 n o i the performance index volumetric efficiency ve is used to quantify the accuracy of soil moisture prediction in the streamflow calibrated model ve is a relative absolute measure that can quantify the frequent variations in the soil moisture flux criss and winston 2008 muleta 2011 ve is calculated as 4 v e 1 i 1 n a b s p i o i i 1 n o i the ve value close to 1 indicates the best fit and the values range between to 1 the soil moisture estimates are also analyzed by the performance index pbias 2 4 4 updating of the parameters sol awc and sol k lhs in the amalgam generates the parameter values within the specified range for all the parameters except sol k and sol awc the cn2 f value from the algorithm is used to generate values for the parameters sol k and sol awc nearing et al 1996 have developed an empirical equation to incorporate the land cover impacts into the effective hydraulic conductivity of the soil this equation relates to effective hydraulic conductivity saturated hydraulic conductivity and curve number as 5 k e 56 82 k s 0 286 1 0 051 e 0 062 c n 2 where k e is the effective hydraulic conductivity mm hr k s is the saturated hydraulic conductivity mm hr and cn is the curve number bouwer 1969 has reported that the green ampt effective hydraulic conductivity k e is approximately equivalent to one half of the saturated hydraulic conductivity of the soil k s the percentage deviation of the parameter cn2 f is taken from the algorithm and then applied to the default curve number value of hru to get the updated value of the curve number corresponding to the default and updated value of the curve number for the selected hru the default and the updated values for saturated hydraulic conductivity values are obtained using nearing s and bouwer s relations the percentage deviation of the parameter sol k is obtained from the updated and the default value of saturated hydraulic conductivity for the selected hru the corresponding percentage deviation for the parameter sol awc is obtained from the percentage deviation of sol k through the pedo transfer functions developed by saxton et al 2006 these equations are developed based on the united states department of agriculture usda soil database using the readily available variables of soil texture and organic matter the saturated hydraulic conductivity k s is related to the field capacity θ 33 saturated soil moisture content θ s and permanent wilting point θ 1500 6 k s 1930 θ s θ 33 3 λ here λ 1 b b ln 1500 l n 33 ln θ 33 l n θ 1500 the saturated soil moisture content θ s is a function of field capacity percentage sand and s 33 kpa moisture content the s 33 kpa moisture content is dependent on percentage sand s percentage clay c and organic matter om the series of equations connecting these variables are presented below 7 θ s θ 33 θ s 33 0 097 s 0 043 7a θ 33 θ 33 t 1 283 θ 33 t 2 0 374 θ 33 t 0 015 θ 33 t 0 251 s 0 195 c 0 011 o m 0 006 s o m 7b 0 027 c o m 0 452 s c 0 299 7c θ s 33 θ s 33 t 0 636 θ s 33 t 0 107 θ s 33 t 0 278 s 0 034 c 0 022 o m 0 018 s o m 7d 0 027 c o m 0 584 s c 0 078 the description of the variables used in the above equations is presented in table 3 the percentage sand percentage clay and organic matter content of the soil in the selected hru is taken from the swat soil database usersoil for the chosen hru the default values of k s θ s θ 33 and θ 1500 are obtained from equations 6 8 based on the soil texture the percentage deviation of the saturated hydraulic conductivity sol k obtained earlier is applied to the default value of k s to obtain the updated value of k s then unknown in equation 6 is the updated field capacity value corresponding to the updated saturated hydraulic conductivity the permanent wilting point θ 1500 of a soil depends on percentage sand percentage clay and organic matter value saxton et al 2006 8 θ 1500 θ 1500 t 0 14 θ 1500 t 0 02 θ 1500 t 0 024 s 0 487 c 0 006 o m 0 005 s o m 8a 0 013 c o m 0 068 s c 0 031 the permanent wilting point is kept constant for a particular soil and the plant available water content sol awc is updated with the variation of field capacity in the current study the updated value of sol k is used to update the field capacity value using equation 6 the updated value of the parameter sol awc is obtained by extracting the permanent wilting point from the updated field capacity value and is used to calculate the percentage deviation from the default value for the selected hru the percentage deviations of sol k and sol awc values are uniformly applied to all hrus a realistic range of percentage deviation for these two parameters is ensured by putting additional constraints on the range specified in table 1 2 4 5 comparison of mca with standard calibration procedure the current study compares the mca with the standard calibration approach sca in terms of the process representation spatial prediction runtime efficiency and reduction in uncertainty in sca the parameter values are randomly selected from the prescribed range without considering the correlation the process representation in both the calibration approaches are analyzed by comparing the prediction of other hydrological variables like soil moisture evapotranspiration and lagged runoff spatial prediction ability of the calibrated parameter set is analyzed by considering the prediction of both streamflow and soil moisture in the upstream gauged sub basins the convergence speed of the objective function is considered to check the runtime efficiency of the calibration approaches in the current study the uncertainty reduction is correlated to the reduction in equifinality and it is analyzed by considering the variability in the values of each parameter from the final ensemble parameter sets the proposed calibration approach needs to be validated in different hydro climatic conditions however in the current study the analysis is restricted to the little river watershed due to the limited availability of experimental watershed data even then the calibration is done in two different time frames in the little river watershed to analyze the effect of different climatic conditions on the proposed approach the analysis is conducted as two cases and the calibration approaches are compared in both the cases in case i the watershed is calibrated for a period 2006 2011 and validated the calibrated parameter sets in the period 2012 2016 in case ii the model is calibrated for the period 2012 2016 and validated in the period 2006 2011 3 results and discussion 3 1 calibration and validation of the swat model the swat model setup is done for the little river experimental watershed over a period of ten years 2006 to 2016 the analysis of the study has been done in two time frames as case i and case ii the swat check analysis shows that the groundwater component is the significant hydrological component of the watershed bosch et al 2007 have also reported a similar fact that there is a groundwater contribution of 53 in the streamflow of the little river experimental watershed evapotranspiration and streamflow are the other two significant hydrological processes in the watershed feyereisen et al 2007 have reported that around 30 of annual rainfall is contributing to surface runoff and the remaining 70 is going as evapotranspiration in lrew a similar response is observed in the swat setup of the little river watershed the 15 swat model parameters which are significant for streamflow prediction in swat are considered for calibration the amalgam optimizer is used in the current study with the maximum number of iterations of 4000 in both the cases the maximum number of iterations generations are fixed such that the objective function will converge within the specified number of generations the calibration approaches have tried to minimize the objective function 1 nse in both the cases the streamflow measurements at the watershed outlet subbasin 5 is considered for optimizing the parameter sets one at a time sensitivity analysis arnold et al 2012 of parameters shows that the parameter cn2 f is the most significant parameter for streamflow prediction in little river watershed the parameters sol awc and sol k are in the fifth and sixth position in terms of sensitivity for streamflow prediction table 1 the association between the percentage deviations of the parameters cn2 f sol k and sol awc are used as soft data while calibrating the model in mca the percentage deviations of the parameters sol k and sol awc are obtained corresponding to the variations of the parameter cn2 f the swat model setup is calibrated for a period 2006 2011 in case i the mca has simulated the streamflow at the watershed outlet reasonably well in case i with a nse value of 0 72 and a pbias value of 28 4 the nse value above 0 5 and the pbias value within 25 are considered acceptable in the streamflow prediction of a process based distributed hydrological model moriasi et al 2007 the soil moisture prediction in the calibration outlet is reasonably good in the mca with a ve value of 0 74 and a pbias value of 9 9 table 4 the sca simulates the streamflow in the watershed outlet with an nse value of 0 74 and pbias of 8 47 fig 3 a the streamflow simulation from both the calibration approaches are presented in fig 3a it is observed that the streamflow prediction from sca is superior to that of mca the mca results in a compromising parameter set which can make reasonably good simulation for both streamflow and soil moisture the mca parameter selection is happening very similar to multi variable calibration of the model rajib et al 2016 the mca has improved the simulation of soil moisture with a slight compromise on the streamflow simulation in comparison with the sca the soil moisture prediction in the sca is an overprediction with a ve value of 0 57 and a pbias value of 37 the negative value of pbias indicates an overprediction of soil moisture in sca the soil moisture prediction from both the approaches at the watershed outlet is presented in fig 3a in case ii swat setup is calibrated for the period 2012 2016 in both the calibration approaches the mca simulates the streamflow in the watershed outlet with a nse value of 0 69 the soil moisture prediction in the calibration outlet is also reasonably good in the mca ve value of 0 70 the sca simulates the streamflow with a nse value of 0 75 in the watershed outlet table 5 the streamflow prediction from both the calibration approaches at the watershed is presented in fig 3b the calibrated parameter set from the mca is slightly overpredicting the streamflow in comparison with the sca however the soil moisture prediction in the sca is an overprediction with a ve value of 0 37 the soil moisture prediction from both the approaches at the watershed outlet is presented in fig 3b the improvement in the soil moisture prediction of the mca is clearly visible in case ii also this improvement in soil moisture prediction without much compromise on the streamflow prediction is a major advantage of the mca the calibrated parameter sets from both the cases are validated for the rest of the time period the calibrated parameter sets from case i is validated in the period 2012 2016 the parameter set of the mca in case i simulates the streamflow in the validation period with an nse value of 0 44 at the watershed outlet fig 4 a the sca also simulates the streamflow with similar level of accuracy nse value is 0 44 in the validation period table 4 the other performance index pbias is calculated and its value is comparatively better in the sca the mca is able to simulate the peaks better than sca however the overall performance is better for the sca the soil moisture prediction is better in mca with a ve value of 0 74 the soil moisture prediction of the sca is an overprediction with a ve value of 0 55 in the validation period similarly the calibrated parameter sets from the case ii is used for hydrological simulation for the period 2007 2011 the results are presented in fig 4b in case ii also the calibrated parameter set resulting from the mca is simulating the streamflow and soil moisture well in the validation period as compared to the sca at the watershed outlet the mca is able to simulate the streamflow well with an nse value of 0 57 and soil moisture with a ve value of 0 71 fig 4b the sca has resulted in an nse value of 0 39 for streamflow prediction and a ve value of 0 38 for soil moisture prediction the mca approach is simulating both streamflow and soil moisture reasonably well when compared to the sca in both the cases this improved prediction in the mca may be because of the improvement in the process representation the hydrological processes are modeled independently in swat and integrated through the water balance equation the swat model structure has a limitation in calibrating the model for both the streamflow and soil moisture the swat model is widely used for impact analysis studies due to its process based nature hence the process representation in the model is more important than its predictability on a single variable beven and o connell 1982 the optimal parameter sets from both the calibration approaches in case i and ii are presented in table 1 significant differences are observed in the calibrated parameter values of cn2 f sol k and sol awc these variations in the parameter values are indicating a considerable difference in the simulation of different hydrological fluxes in both the approaches the improvement in the soil moisture prediction can influence the simulation of evapotranspiration percolation and baseflow brocca et al 2012 rajib et al 2016 3 2 convergence of objective function the runtime required for calibration depends on the number of parameters involved number of variables considered for calibration optimization algorithm used for calibration and the calibration approach in the current analysis all the above mentioned factors are the same except the calibration approach the maximum number of generations considered in both the calibration approaches are 4000 in both the cases with nse as a likelihood index for calibration the convergence of the objective functions are very similar in both the calibration approaches the initial values of objective functions are less in mca as compared to that of sca however the improvement in the initial values has not resulted in a faster convergence of objective function in mca the most significant hydrological process that affect the streamflow generation in lrew is the groundwater contribution however the mca is not directly focuses on the groundwater parameters mca is based on the assumption that improvement in the soil moisture simulation improves the simulation of other hydrological fluxes faster convergence of the objective function may happen for mca in a watershed which has prominent surface runoff contribution to the streamflow the proposed calibration approach focuses only on the physics of the surface runoff generation process however there are many more physically interpretable parameters in swat and they are connected to multiple hydrological processes a calibration method that can accommodate the relative variations of all these parameters can further reduce the dimensionality of the calibration and result in faster convergence of the objective function 3 3 spatial prediction of streamflow and soil moisture the calibrated parameter sets from both the calibration approaches have tested in the upstream subbasins for the simulation of soil moisture and streamflow the current study restricts the spatial analysis to subbasins 1 and 2 due to the limited data availability in subbasins 3 and 4 the parameter set from the mca in case i has resulted in reasonably good streamflow simulation in the calibration period of subbasin 1 and 2 with nse values 0 58 and 0 63 respectively the pbias values are also within the acceptable limit table 4 the soil moisture simulation is also reasonably good in sub basin 2 with a ve value of 0 59 and pbias of 18 however the soil moisture prediction in subbasin 1 is poor with a ve value of 0 36 the simulation of both soil moisture and streamflow are reasonably good in the validation period in subbasins 1 and 2 table 4 the spatial prediction of both the fluxes during calibration and validation period are good for the parameter set from the mca in case ii however the soil moisture flux in subbasin 1 is poor during the validation period as observed in calibration period of case i this variation in the soil moisture simulation in subbasin 1 during 2007 2011 may be because of the uncertainty associated with the model setup the calibrated parameter set of the sca in case i simulates the streamflow well in the upstream subbasins with nse values 0 58 and 0 63 respectively during the calibration period however the prediction of corresponding soil moisture fluxes are very poor ve values of 0 19 and 0 22 the soil moisture prediction in subbasin 1 is poor in both the calibration approaches it may be because of the uncertainty associated with the hydrological model setup in general the spatial prediction using the parameter sets from sca has resulted in reasonably good streamflow prediction in the upstream subbasins and poor simulations of soil moisture fluxes this analysis indicates that the calibrated parameter set from mca is better than the sca for spatial prediction of both streamflow and soil moisture in little river watershed the improved process dynamics in mca may be the reason for better spatial prediction 3 4 process representation in the calibrated swat model the simulation of surface and sub surfaces hydrological fluxes in the calibrated swat model setups are analyzed the mca is able to improve the soil moisture prediction without considering the soil moisture data for calibration the better simulations of soil moisture flux can improve the simulation of other hydrological fluxes brocca et al 2012 mcmillan et al 2011 has developed a number of diagnostic tests to check the process representation in the conceptual hydrological models using experimental data the current study has taken a diagnostic test based on soil moisture and analyses the process representation in calibrated swat model setups 3 4 1 soil moisture soil moisture is an essential component of the water balance equation affecting the energy and water balance of the hydrological cycle the soil moisture estimate in the watershed outlet is the weighted average of soil moisture estimates of all the hrus in the sub basin 5 statsgo soil data has been used in the study and root zone depth in most of the hrus is in the range of 300 mm in the little river basin and the soil moisture measurements are available at a depth of 50 mm 200 mm and 300 mm as discussed earlier the mca can simulate the soil moisture well and soil moisture estimate is an average soil moisture for the entire soil thickness the soil moisture estimate from the model is compared with the average soil moisture measurements in a subbasin scale there are 5 soil moisture measurement stations present in the subbasin 5 and the average of these soil moisture measurements are compared with the simulated soil moisture estimate in subbasin level the vertical soil moisture distribution in different hrus in both the calibration approaches in both the cases are compared are presented in fig 5 a and b the vertical soil moisture distribution from the mca is close to the observed soil moisture values at different depths in both the cases the sca results in an overprediction of the soil moisture for the entire soil thickness and the deviations increase with the increase in depth the improvement in the vertical soil moisture distribution can affect the evapotranspiration percolation and groundwater component of the water balance equation in each hru the annual soil moisture storage in subbasin 5 is presented in fig 5 c and d the mca simulates the soil moisture storage better than the sca without considering the soil moisture data for calibration the mca preserves the physics of the runoff generation process by preserving the association between percentage variations of the parameters cn2 f sol awc and sol k this helps in an improved simulation of both streamflow and soil moisture in the calibrated swat model 3 4 2 evapotranspiration et the evapotranspiration demand of a watershed is met from different soil layers in different seasons yilmaz et al 2008 hence the improvement in the root zone soil moisture content will improve the estimation of et in all the seasons the interception storages are initially considered to meet the et demand in swat and the further et requirement is met from transpiration and soil water evaporation the soil water evaporation is modeled with an exponential depth distribution of soil in swat neitsch et al 2002 there are constraints to extract moisture from different soil layers to meet the evaporative demand the parameters esco and epco help in the redistribution of soil depth to meet the et demand of the watershed these parameter values have a significant difference in both the calibration approaches table 1 the difference in the soil moisture storage is reflected in the evapotranspiration estimates also the annual et estimates from both the calibration approaches differ by 50 60 mm in both the cases the et estimates are relatively less in mca compared to the sca this reduction is proportional to the reduction in soil moisture storage in the mca the values of the parameters esco and epco are functions of soil moisture content hydraulic conductivity water holding capacity of the soil etc the evapotranspiration component of the watershed is around 60 70 of the annual rainfall https www nrrig mwa ars usda gov stewards stewards html the et estimates from both the calibration approaches fall within this limit however a comparison of both the calibration approaches in terms of accuracy of et prediction is not possible since the measured actual evapotranspiration values are not available in the watershed 3 4 3 lagged runoff the lagged runoff component of streamflow includes the lateral flow and baseflow contributions to the streamflow this component in the measured streamflow at the watershed outlet is separated using an automatic baseflow filter program proposed by arnold et al 1995 it is a digital filter technique that separates the lagged runoff component from streamflow that is analogous to the filtering of high frequency signals in signal processing the baseflow contribution in the little river watershed is significant during the period january may bosch et al 2017 the lagged runoff separated using the filter program represents a similar nature that is observed in the field the simulated lagged runoff component from both the calibration approaches are compared with the separated lagged runoff from measured streamflow and it is presented in fig 6 for both the cases the mca in both the cases are able to simulate the lagged runoff close to the observed as presented in fig 6 the lagged runoff component in the sca is highly overpredicting the streamflow data used for calibration in case i carries more data points in the medium low flow ranges the lagged runoff contribution to a stream is directly connected to the flow in the medium low ranges the improvement in both streamflow and soil moisture may have resulted in an improved simulation of lagged runoff in mca of case i however the lagged runoff prediction in 2013 is a significant overprediction in both the calibration approaches the calibration period 2012 2016 has a medium to high flow range in the watershed the nature of streamflow in the watershed and the performance index used for calibration have significant influences on the parameter set the parameters alpha bf and gwqmn have significantly different values in both the calibration approaches and that influences the lagged runoff prediction in both the calibration approaches 3 4 4 surface runoff the streamflow of a watershed includes two components the lagged runoff component and the surface runoff component the surface runoff component of the measured streamflow is obtained by separating the lagged runoff the runoff simulation in both the calibration approaches are compared with the observed runoff values the analysis has been done separately for both the cases and presented in fig 6 the runoff simulation in the sca is close to the observed runoff values than that of the mca in case 1 the runoff estimates from the mca are over predictions during the low flow years and matches with the observed during the high flow in case i the swat model structure has limitations in calibrating the model for both surface runoff and soil moisture hence the compromising parameter set from the mca has resulted in an overprediction of surface runoff in low flow years of case i as discussed before the surface runoff generation in case ii is much higher than that of case i the surface runoff generation of the mca in case ii is an overprediction in most of the years except that of 2014 and 2015 the analysis of surface runoff in case ii indicates that the characteristics of streamflow data considered for calibration has significant effect on the parameter set 3 4 5 streamflow in the current study both the calibration approaches optimize the swat parameters with the streamflow measurement at the watershed outlet the streamflow prediction at different subbasins of the watershed from both the calibration approaches are analyzed and presented in section 3 3 the mca has optimized a parameter set which can simulate both the streamflow and soil moisture reasonably well with a slight compromise on the streamflow prediction in comparison with the sca the streamflow prediction in different flow ranges from both the calibration approaches are analyzed in this section the flow duration curve of simulated streamflow in both the cases are compared with that of the observed streamflow fig 7 the streamflow is classified into different flow ranges the flow with an exceedance probability range of 0 0 02 is considered as a high flow flow in a range of 0 02 0 7 exceedance probability is a medium flow if the exceedance probability range falls within the range of 0 7 1 then it is termed as a low flow the high flow prediction in case i is an underprediction in both the calibration approaches however both the calibration approaches have simulated the medium and low flow ranges well even though the streamflow simulation in mca is slightly inferior to that of the sca the prediction in medium and low flow ranges are equally good the improved lagged runoff simulation in the mca may have resulted in a better simulation of medium and low flow ranges in case ii the high flow simulation in the mca is slightly better than that of the sca however the medium flow prediction in mca is showing higher deviations from the observed in comparison with sca the streamflow prediction in different flow ranges depends on the nature of data considered for calibration and the performance index used for the study the current study accounts for the variation in the flow characteristics by doing analysis for different time periods however the effect of different performance indices are not considered and that is one of the limitations of the study 3 5 reduction of parametric uncertainty equifinality is an issue associated with the conventional calibration approaches the different parameter sets would result in similar model performance and the selection of a better performing parameter set is difficult beven and binley 1992 the current study analyses the equifinality by considering the ensemble parameter sets from both the calibration approaches the parameter sets that results in the minimum value of the objective function is considered for the analysis the variability in the values of each parameter in the ensemble sets from both the cases are represented using the boxplot and it is presented in fig 8 the x axis is the parameters considered in the calibration process and the y axis is the normalized range of the parameters the variability in the parameters related to groundwater and evapotranspiration has reduced in mca as compared to sca in both the cases this may be because of the improved process dynamics in the calibrated model however the parameters cn2 f sol k and sol awc are showing higher variability in mca than in sca the trade off that exist in the calibration of model for streamflow and soil moisture may be the reason for higher variability of those parameters in mca as compared to that of sca parameters other parameters significant for streamflow and peak runoff are showing less variability in mca than in sca the reduction of equifinality in the mca is indicating a reduced parametric uncertainty in the proposed calibration approach 4 summary and conclusion the current study proposes a modified calibration approach for the swat model it helps to improve the process representation in model with limited available data the hydrological processes are modeled independently in swat and it is integrated through the water balance equation in each hru the association between the parameters cn2 f sol k and sol awc for runoff generation process is used as soft data while calibrating the model the mca is tested in the little river experimental watershed georgia usa in two different time frames as two cases the goodness of fit statistics for streamflow and soil moisture prediction at the watershed outlet are reasonably good in both the cases of the mca in case i it is resulted in nse value of 0 72 for streamflow simulation and the ve value of 0 74 for soil moisture simulation while in case ii it gives nse value of 0 69 for streamflow prediction and ve value of 0 70 for soil moisture prediction the sca has simulated the streamflow well in both the cases with a poor simulation of soil moisture the results of the proposed calibration approach are compared with that of the sca in terms of process representation spatial prediction run time efficiency and reduction in uncertainty the proposed calibration approach has simulated the vertical soil moisture distribution in the watershed better the improvement in the root zone soil moisture has improved the simulation of other fluxes like evapotranspiration and groundwater contribution in the swat model the proposed calibration approach has resulted in a compromising parameter set for streamflow prediction in swat the improvement in the process representation has helped for a better spatial prediction of hydrological variables in the proposed calibration approach in both the cases the parametric uncertainty is considerably reduced in the mca as compared to the sca the proposed calibration approach has a central theme that is applicable for all hydrological models since mathematical representation of complex hydrological system is a global issue the improved process representation in the calibrated model will improve its applicability in the impact analysis studies the mca is validated in a us watershed however it is applicable to any watershed irrespective of its characteristics and location as the approach is based on the physics of the hydrological processes the calibration result is highly dependent on the performance index used for calibration though the current study has not analyzed the effect of different performance indices on the proposed calibration approach the interactions between the physically interpretable swat parameters can be accommodated in the model structure itself this may lead to more robust hydrological simulation in the swat model and it can also reduce the data requirements for better process representation in the swat declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25746,this study introduces the maximal overlap discrete wavelet packet transform modwpt for forecasting hydrological variables that exhibit change over multiple timescales e g rainfall streamflow the advantage of the modwpt over other recent wavelet decomposition methods à trous algorithm at and the maximal overlap discrete wavelet transform modwt is that it can extract finer scale information that may be important for improving forecast performance multiple wavelet decomposition methods modwpt at modwt are integrated within the wavelet data driven forecasting framework wddff applied for forecasting monthly rainfall at six meteorological stations in the awash river basin ethiopia and compared using eight statistical performance metrics results demonstrate that the modwpt can be used to generate more accurate forecasts than the at and modwt for the majority of stations and performance metrics certain settings within the wddff decomposition level wavelet filter input variable method and data driven model lead to improved performance more often than others keywords hydrological forecasting wavelets maximal overlap discrete wavelet packet transform data driven models input variable selection 1 introduction the use of data driven models ddm multiple linear regression mlr decision trees artificial neural networks etc are increasingly being adopted for hydrological forecasting sit et al 2020 tahmasebi et al 2020 tiyasha et al 2020 zhu et al 2020 zounemat kermani et al 2020 since they are useful when there is a lack of knowledge on the physical relationships between a target response and explanatory input variables are rapid to develop and deploy in real time are computationally efficient and often lead to performance that is competitive with physical and or conceptual process based models daliakopoulos and tsanis 2016 demirel et al 2015 gauch et al 2020 gumiere et al 2020 kratzert et al 2019 peijun li et al 2020 read et al 2019 furthermore ddm are starting to be recognized as a means to compliment process based models rather than replace them such as for data assimilation boucher et al 2020 or post processing hydrological model outputs ehlers et al 2019 tyralis et al 2019 however a growing area of interest in hydrological forecasting unique to ddm is addressing multiscale change through the use of wavelet decomposition wavelet decomposition a pre processing method extracts multiscale information from a set of explanatory variables time series and is often used in ddm to improve forecasting performance commonly shown to provide considerable gains in accuracy over benchmark methods i e without wavelet decomposition barzegar et al 2021 hammad et al 2021 jiang et al 2020 ni et al 2020 rahman et al 2020 yeditha et al 2020 zhou et al 2020 the two most popular methods used for wavelet decomposition in hydrological forecasting include the discrete wavelet transform based multiresolution analysis dwt mra and the maximal overlap dwt modwt based multiresolution analysis modwt mra while the à trous algorithm at and the modwt are alternatives that are used less frequently a recent study by quilty and adamowski 2018 demonstrated that both the dwt mra and modwt mra cannot be used correctly for real world forecasting applications since they make use of future data i e they use information from the future during model calibration and validation that would otherwise not be available in a real world setting as noted in earlier studies aussem et al 1998 bogner and pappenberger 2011 du et al 2017 maheswaran and khosa 2012 additionally dwt mra involves decimation requiring the data to be a power of two to provide perfect reconstruction and is shift variant applying the dwt mra to lagged time series causes the time lagged decomposed coefficients to differ when they should be equivalent in contrast the at and modwt avoid the use of future data are non decimating shift invariant and can be used correctly for real world forecasting this finding is significant as 90 of studies using wavelet decomposition for hydrological forecasting make use of either the dwt mra or modwt mra quilty and adamowski 2018 the authors quilty and adamowski 2018 proposed a set of best practices for developing wavelet based forecasts for real world scenarios culminating in the development of the wavelet data driven forecasting framework wddff see section 3 1 the wddff can use either the at or modwt alongside any input variable selection ivs method and ddm for developing wavelet based forecasting models for a given decomposition level and wavelet filter however the at and modwt are limited in the number of time scales that can be extracted from a particular explanatory variable time series wavelet packets cârsteanu et al 1999 allow for a finer scale wavelet decomposition that can potentially extract additional meaningful information from the explanatory variables that would be overlooked if only the at or modwt were explored el hendawi and wang 2020 pengtao li et al 2020 shrifan et al 2021 to date there have been very limited applications of wavelet packet based decomposition for hydrological forecasting a literature review returned the following examples in seo et al 2016 the authors explored several ddm artificial neural networks adaptive neuro fuzzy inference system and support vector machine coupled with wavelet packet decomposition for daily river stage forecasting in the gam stream watershed south korea and found these methods to significantly outperform the benchmark ddm without wavelet packet decomposition moosavi et al 2017 explored a wavelet packet group method of data handling gmdh model and compared it against the gmdh benchmark for daily runoff forecasting in the darian chay ghal chay and lilan chay rivers in east azerbaijan iran finding that the root mean square error could be reduced by 45 through wavelet packet decomposition support vector regression with and without wavelet packet decomposition was explored for forecasting monthly groundwater level fluctuations at three wells in mangalore india where it was demonstrated that the wavelet packet decomposition approach led to more accurate forecasts sujay raghavendra and deka 2015 wavelet packet decomposition based ddm have been explored for drought forecasting das et al 2020 and hindcasting using the standardized precipitation index spi das and deka 2017 and the standardized precipitation evapotranspiration index danandeh mehr et al 2020 respectively with significantly better performance being achieved by the wavelet packet approaches however in each of these studies the wavelet packet integrated ddm models utilized a decomposition approach similar to the dwt mra and therefore suffer from the future data issue as well as shift variance and decimation and are not suitable for real world forecasting applications outside of hydrology wavelet packet methods have been proposed based on a procedure akin to the modwt mra for applications such as retail sales forecasting michis 2009 wind speed forecasting liu and duan 2020 nason and sapatinas 2002 spatial air quality index prediction liu and chen 2020 and forecasting electricity spot prices ben amor et al 2018 however since these methods make use of future data they are not suitable for real world forecasting applications to overcome the future data issue associated with recent wavelet packet based forecasting approaches this study explores the maximal overlap discrete wavelet packet transform modwpt which follows the same decomposition principle as the modwt and is therefore applicable for real world forecasting scenarios the only other study found during a literature review that used the modwpt for forecasting is bruzda 2011 where the author applied the method for forecasting s p 500 returns therefore this study is the first to 1 explore the modwpt for hydrological forecasting 2 compare its performance against the at and modwt algorithms each of which are integrated within the wddff based on these contributions the goal of this research is to explore and answer the following two questions 1 can the modwpt improve forecasting performance over the at and or modwt when integrated within the wddff 2 are there any noticeable trends in terms of which ivs methods ddm wavelet decomposition methods at modwt and modwpt decomposition levels and wavelet filters lead to the best forecasting performance these questions are explored through a monthly rainfall forecasting case study in the awash river basin ethiopia the hope is that in answering the above mentioned questions the potential of the modwpt for rainfall forecasting can be assessed and its use guided towards other important hydrological forecasting applications streamflow groundwater level evaporation water quality and information concerning its performance as assessed through several commonly adopted evaluation metrics can guide the selection of appropriate modelling choices ivs methods ddm decomposition levels and wavelet filters within the wddff for similar applications the remainder of this study is outlined as follows section 2 briefly describes the study area and data section 3 includes information on the employed methods section 4 provides the experimental details for developing and evaluating the different forecasts section 5 includes the main results and a discussion of their significance and section 6 provides concluding remarks and describes future research topics 2 study area and data the awash river basin in ethiopia was selected as the study area to explore the capabilities of the modwpt integrated wddff for 1 month ahead rainfall forecasting this study site was chosen as it is familiar to the authors who have explored a variety of wavelet based ddm for drought forecasting in this area using the spi belayneh et al 2013 2014 2016a 2016b brief details on the study area are included below as of 2012 it is estimated that about 98 of the total cropland in the awash river basin is rainfed agriculture borgomeo et al 2018 and due to frequent and persistent droughts in the basin much of its inhabitants rely on international food aid edossa et al 2010 thus significant research has been devoted to the analysis of climate variability hydro meteorological modelling and forecasting the estimation of the onset and duration of drought events as well as their economic implications in the basin bekele et al 2017 berhe et al 2013 borgomeo et al 2018 dessu et al 2016 edossa et al 2010 tadese et al 2019 tolera et al 2018 given that supplemental irrigation is required to offset deficits of up to 40 of the crop water requirements and variations in rainfall may lead to a decline in agricultural production by 20 with the potential to increase poverty rates by 25 rainfall forecasts are of significant economic and social value to those residing in the basin bekele et al 2017 hagosa et al 2010 while previous studies focussed on forecasting spi in the awash river basin belayneh et al 2014 2016a 2016b this study instead focusses on forecasting rainfall since it can easily be converted into a drought index and may also be used directly in hydrological models or irrigation schedules monthly rainfall data was collected from six meteorlogical stations in this study area bati bantu liben dubti ejerselal modjo and wolenchiti table 1 includes descriptive statistics for the model calibration and validation data see section 4 1 3 additional detailed information on this study site can be found in the above cited studies the target variable at each station was the 1 month ahead observed rainfall while the explanatory input variables were the observed rainfall in the current month and previous months along with their wavelet decomposed counterparts generated via at modwt and modwpt see section 3 2 and 3 3 the next section provides a brief overview of the different methods used to generate forecasts for this study area before describing experimental details 3 methods the section begins with a brief overview of the wddff afterwards the different wavelet decomposition methods at modwt and modwpt integrated with the wddff are briefly described finally the different input variable selection and data driven models used within the wddff are discussed 3 1 wavelet data driven forecasting framework the wddff embodies a set of best practices for real world wavelet based forecasting that focuses on three key components quilty and adamowski 2018 1 proper wavelet decomposition of the explanatory input variables resulting in a set of wavelet and scaling coefficients for each explanatory variable see section 3 2 this component also includes decomposition level and wavelet filter selection 2 selecting the most important wavelet decomposed explanatory variables through ivs 3 using the selected wavelet decomposed explanatory variables as input to a ddm that is afterwards calibrated and validated using historical records proper wavelet decomposition entails using the at modwt or as proposed in this study modwpt in order to avoid the future data condition see section 2 3 1 and 2 3 2 in quilty and adamowski 2018 as well as ensuring boundary affected wavelet and scaling coefficients are removed from the beginning of the time series see section 2 3 3 in quilty and adamowski 2018 the number of boundary affected coefficients can be determined via percival and walden 2000 1 l j 2 j 1 l 1 1 where l j is the number of boundary affected wavelet and scaling coefficients at the beginning of each time series j is the decomposition level and l is the number of wavelet scaling filter coefficients finally j and l are selected such that there remains enough records from which to calibrate and validate a ddm the selection of j and l are primarily dependent on the time series properties and length e g for monthly rainfall data of sufficient length i e at least 200 records a decomposition level of three will allow the extraction of inter intra yearly and sub seasonal periodicities and changes thereof while wavelet filters with four or more coefficients will enable the extraction of second degree or higher polynomial behaviour and secondly on the modeller s preference available computational resources etc ivs is used to identify only those wavelet and scaling coefficients that are useful in generating accurate forecasts of the target variable while ddm calibration tunes model parameters such that their forecasts resemble as much as possible a set of historical calibration targets finally ddm validation confirms that the calibrated ddm provides accurate out of sample performance on a set of historical records of the target variable not used to calibrate the model finally an advantage of wddff is that it can be used with any ivs method or ddm see section 3 3 and 3 4 for the ivs methods and ddm adopted in this study the main components of the wddff wavelet decomposition algorithms ivs methods and ddm are discussed in the sub sections that follow 3 2 wavelet decomposition methods 3 2 1 à trous algorithm and maximal overlap discrete wavelet transform in simple terms both the at and modwt can be used to decompose a time series x t for t 0 1 n 1 where t is a time index and n is the number of samples in the time series into a number of sub components each of the same length as x t that capture the multiscale information in the series as it changes through time allowing for periodicities transients level shifts etc to be extracted from the time series the high frequency information extracted by the at or modwt is referred to as wavelet coefficients while the low frequency information is referred to as scaling coefficients the high and low frequency information is obtained by convolving the time series with high and low pass filters respectively known as wavelet and scaling filters further details on the unique properties of these filters can be found in percival and walden 2000 to maintain brevity the main equations for calculating wavelet and scaling coefficients via the at modwt and modwpt will be provided detailed treatment of these methods has been covered earlier alves et al 2017 aussem et al 1998 olhede and walden 2004 percival and walden 2000 renaud et al 2003 walden and contreras cristan 1998 the notation used here to define the at and modwt follows quilty and adamowski 2018 as derived from maheswaran and khosa 2012 percival and walden 2000 the at algorithm can be written as quilty and adamowski 2018 2 v j t a l 0 l 1 g l v j 1 t 2 j 1 l mod n a 3 w j t a v j 1 t a v j t a while the modwt can be written as quilty and adamowski 2018 4 v j t m l 0 l 1 g l v j 1 t 2 j 1 l mod n m 5 w j t m l 0 l 1 h l v j 1 t 2 j 1 l mod n m where x t v 0 t a v 0 t m represents the original time series at time t v j t a v j t m are the at modwt scaling coefficients at scale j j 1 2 j w j t a w j t m are the at modwt wavelet coefficients at scale j h g is a wavelet scaling filter mod refers to the modulo operator g l g l 2 h l h l 2 and g l 1 l 1 h l 1 t for l 0 1 l 1 and l is the length of the wavelet scaling filter at level j the wavelet coefficients belong to the nominal frequency band associated with frequencies in the interval 1 2 j 1 1 2 j while the scaling coefficients belong to the nominal frequency band associated with frequencies in the interval 0 1 2 j 1 a list of scaling filter coefficients including those used in this study can be found in the supplementary data of quilty and adamowski 2018 for example the haar scaling filter coefficients are given by g 0 1 2 and g 1 1 2 respectively wavelet decomposition via the at or modwt results in j sets of wavelet coefficients w 1 w 2 w j and a set of scaling coefficients v j sub and superscripts removed for simplicity note that the main difference between the at and modwt can be seen through the calculation of wavelet coefficients the at calculates wavelet coefficients as the difference between scaling coefficients at adjacent decomposition levels while the modwt uses a wavelet filter h to estimate wavelet coefficients in summary due to the difference in how wavelet coefficients are calculated between at and modwt methods the at can be interpreted as an additive wavelet decomposition i e the original time series can be reconstructed perfectly by aggregating the wavelet coefficients from scales 1 to j with the scaling coefficients at scale j while the modwt can be interpreted as an energy based wavelet decomposition from which it follows that the variance of the wavelet coefficients at scales 1 though j and the scaling coefficients at scale j when aggregated have the same variance as the original time series additive wavelet decomposition is not a property of the modwt coefficients nor is energy based wavelet decomposition a property of the at except for the haar wavelet filter whereby both the modwt and at are equivalent and thus share the same properties energy based and additive wavelet decomposition bruzda 2019 3 2 2 maximal overlap discrete wavelet packet transform wavelet decomposition via the modwpt can be formulated as walden and contreras cristan 1998 6 w j n t p l 0 l 1 f n l w j 1 n 2 t 2 j 1 l mod n p 7 f n l g l i f n mod 4 0 or 3 h l i f n mod 4 1 or 2 where w j n t p are the modwpt coefficients at time t for decomposition level j and band n where n 0 1 2 j 1 which is nominally associated with frequencies in the interval n 2 j 1 n 1 2 j 1 and represents the integer part operator sometimes also referred to as the floor operator similar to the at and modwt x t w 0 0 t p v 0 t a v 0 t m the modwpt like the modwt is an energy based wavelet decomposition algorithm however when the haar wavelet filter is adopted both the modwt and modwpt also permit an additive decomposition of a given time series the modwpt differs from the modwt in that at a given level both the wavelet and scaling coefficients from the previous level are decomposed via low and high pass filtering whereas in the modwt only the scaling coefficients from the previous level are decomposed through low and high pass filtering this leads to an increase in the number of wavelet coefficients or wavelet packets when the modwpt is used to decompose a time series thereby extracting further information from the wavelet coefficients that would remain hidden if the modwt was applied the schematic in fig 1 shows the coefficients produced by performing wavelet decomposition on a time series via modwpt and modwt by comparing fig 1 a and b it can be seen that the modwt is encapsulated by the wavelet packets generated via the modwpt the bolded components from the modwt b are equivalent to their counterparts in the modwpt a section a of the supplementary data includes an example of the modwt and modwpt applied to the rainfall time series at the bantu liben station taking fig 1 as an example the information contained in w 1 m from the modwt is decomposed into four separate wavelet packets in the modwpt w 3 4 p w 3 5 p w 3 6 p w 3 7 p the advantage of breaking this information in to several components is that there may be certain information in w 1 m that is not entirely useful or perhaps confounding and when broken into additional components may be filtered out e g perhaps only w 3 6 p and w 3 7 p provide useful information about the target variable therefore w 3 4 p and w 3 5 p can be discarded which would be entirely hidden in w 1 m if the modwpt was not used for a wavelet decomposition at level j the modwpt has a set of 2 j wavelet packets w j 0 p w j 1 p w j 2 j 1 p it is clear that as the decomposition level increases the number of wavelet packets generated through the modwpt rapidly increases therefore for ddm coupled with the modwpt it is important that some form of input variable selection be performed in order to select only those wavelet packets that are useful when forecasting the target variable this is the subject of the next sub section 3 3 input variable selection the benefits of ivs in the hydrological literature on ddm are well known galelli et al 2014 hejazi and cai 2009 kim et al 2020 may et al 2008 ren et al 2020 2021 snieder et al 2020 wan jaafar et al 2011 the main task of ivs is to ensure that only those variables relevant to forecasting the target variable are selected while those that are redundant and or irrelevant are discarded ideally resulting in accurate models that are not overly complex or hampered in their forecasting capabilities ivs methods are generally broken down into three categories filter wrapper and embedded approaches guyon and elisseeff 2003 filter methods select input variables external to a given model wrapper methods use a ddm to provide some measure of how a set of selected inputs impact model performance while embedded methods are directly incorporated into the calibration of ddm parameters effrosynidis and arampatzis 2021 may et al 2011 in general filter and wrapper methods tend to be the focus of most articles that compare ivs approaches against one another bommert et al 2020 this study explores both filter and wrapper ivs methods namely partial correlation input selection pcis may et al 2008 filter method edgeworth approximations based conditional mutual information ea quilty et al 2016 filter method k nearest neighbours based conditional mutual information knn quilty et al 2016 tsimpiris et al 2012 vlachos and kugiumtzis 2010 filter method iterative input selection iis galelli and castelletti 2013 wrapper method these methods were selected due to their high performance and or popularity in hydrology and water resources applications bertoni et al 2021 boucher et al 2020 hatami et al 2019 nunes carvalho et al 2021 quilty and adamowski 2020 ren et al 2020 zandmoghaddam et al 2019 brief details on the pcis ea knn and iis input variable selection methods along with their tunable hyper parameters if any are provided in section b of the supplementary data theoretical background on the various ivs methods can be found in the above mentioned studies and or the references therein 3 4 data driven models in order to map the explanatory variables determined via ivs into forecasts of the target variable data driven models are used since any ddm can be used within the wddff several varieties have thus far been explored including multiple linear regression mlr machine learning e g extreme learning machine elm multivariate adaptive regression splines and deep learning approaches long short term memory networks lstm ghaemi et al 2019 mouatadid et al 2019 quilty and adamowski 2018 in this study since the focus is on exploring the improvements in the wddff that may be realized by adopting modwpt instead of at and modwt algorithms two simple data driven methods were selected one linear method mlr and one nonlinear method elm these methods were chosen since their parameters are easily determined via least squares which makes them highly efficient methods further elm has become a very popular alternative to traditional feed forward backpropagation artificial neural networks bp ann since it provides similar and often better performance than bp ann with a significantly lower computational expense barzegar et al 2017 fan et al 2018 kisi et al 2020 li et al 2019 lima et al 2015 niu and feng 2021 parisouj et al 2020 patil and deka 2016 shamshirband et al 2020 yaseen et al 2019 yu et al 2020 while simple linear mlr and nonlinear elm models commonly used for hydrological forecasting are adopted in order to focus on the different wavelet decomposition approaches more recent ddm can be considered instead and may lead to more accurate forecasts than the ddm used here for instance recent machine learning and deep learning models applied within hydrology such as extreme gradient boosting gauch et al 2021 convolutional neural networks coupled with lstm barzegar et al 2020 or deep residual networks yan et al 2021 and across other domains such as online random forests with memories zhong et al 2020 fully recurrent lagged variable representation network fr lavrnet koutlis et al 2020 or deep kernel based extreme learning machine afzal et al 2021 can easily be integrated within the proposed approach the main parameters that need to be determined in mlr are the slope coefficients for each input variable along with the bias term for elm a set of hyper parameters need to be selected before estimating model parameters the hyper parameters are the number of hidden neurons in the hidden layer as well as the hidden and output layer activation functions additionally to help reduce the impact of the random generation process for input weights and hidden neuron biases on the identification of the optimal number of hidden neurons this study uses an ensemble elm liu and wang 2010 therefore the number of ensemble members in elm is an additional hyper parameter after selecting hyper parameters for each ensemble member in elm the output layer weights parameters can be determined through a least squares solution by taking the product of the moore penrose generalized inverse of the hidden layer output matrix and the target variable the overall forecast is the mean of the forecasts produced by the different ensemble members the interested reader can find theoretical background on elm in huang et al 2006 2012 finally the linear and nonlinear models are benchmarked against the random walk rw model a useful benchmark in hydrological forecasting due to its simplicity ferdosian and haie 2016 fullerton and molina 2010 quilty and adamowski 2018 which does not require the selection of any parameters since a one step ahead forecast is equivalent to the most recently observed measurement which is referred to as the persistence method or the naïve method in other studies amaranto et al 2018 pacchin et al 2019 papacharalampous and tyralis 2018 pappenberger et al 2015 in the next section experimental details are provided concerning the case study which includes information on model development and evaluation 4 experimental details this section outlines the main model development steps in the wddff and then introduces the performance metrics used to evaluate the forecasting models 4 1 model development the main model development steps are outlined in the sub sections below and summarized in fig 2 4 1 1 generating the candidate input variable set the first step in the development of the wddff is the generation of the candidate input variable set since the only data used in this study is monthly rainfall observations available at six different stations in the awash river basin bati bantu liben dubti ejerselal modjo and wolenchiti the candidate input variables are time lagged versions of observed rainfall at each station as well as their wavelet decomposed components in many studies partial autocorrelation is used to identify which lags to include as candidate input variables since the physical processes that govern rainfall are often represented through nonlinear relationships valverde ramírez et al 2006 it is useful to replace partial autocorrelation with conditional mutual information it was found by using the ea and knn based conditional mutual information section 3 1 at each station that the current and previous 17 time lags are useful for forecasting rainfall 1 month ahead i e y t 1 therefore the candidate input variable set for the non wavelet based models considered 18 input variables i e y t y t 1 y t 17 for forecasting rainfall at a lead time of 1 month i e y ˆ t 1 for the wavelet based models the candidate input variable set included the same inputs in addition to their wavelet decomposed counterparts as discussed in the next sub section each of the lagged rainfall time series were wavelet decomposed separately due to the internal function of the software used for wavelet decomposition however since the modwpt modwt and at are all shift invariant the original rainfall time series could be decomposed first and the resulting wavelet and scaling coefficients time lagged afterwards to produce the same results which is a more computationally efficient approach to follow however since monthly data is employed rather than very fine scale information such as 10 min rainfall data this slightly less efficient approach to wavelet decomposition does not create a significant bottleneck in the forecasting routine that would be employed in practice finally while a very large number of input variables are considered as candidates for each wavelet based model see section 4 1 2 the number of inputs used in the ddm are considerably reduced via ivs see the last paragraph in section 5 4 1 2 wavelet decomposition wavelet decomposition was carried out in this study using the at modwt and modwpt algorithms according to the best practices outlined in quilty and adamowski 2018 see also section 3 1 the main considerations during wavelet decomposition are the selection of the decomposition level s and wavelet filter s as well as the removal of boundary affected coefficients according to equation 1 the decomposition level and wavelet filter must be chosen such that after removing the boundary affected coefficients a reasonable amount of calibration records remain since only 432 monthly rainfall records were collected from each of the six stations it was determined that a maximum decomposition level of three and wavelet filter length of 12 would be explored this required the removal of 78 boundary affected coefficients from the beginning of the target variable and candidate input variable sets i e according to equation 1 decomposition levels from one to three were explored for 16 different wavelet filters with filter lengths ranging from two to twelve the different wavelet filters explored in this study can be found in table 2 see quilty and adamowski 2018 and references therein for further details wavelet decomposed candidate input sets that adopted a decomposition level less than three or wavelet filters with length less than 12 had fewer than 78 boundary affected coefficients however 78 coefficients were removed from the beginning of the target and candidate input variable sets in order to maintain the same number of calibration records and permit a fair judgement between different wavelet decomposition algorithms decomposition levels wavelet filters ivs methods and ddm the non wavelet based models followed the same principle however in practice once an optimal model has been identified there is no need to remove more coefficients than required and the ddm can be recalibrated with the extra records removed during model comparison applying a level three modwt or at based decomposition to a given time series results in four sub series three sets of wavelet coefficients and a single set of scaling coefficients see fig 1 b while applying a level three modwpt based decomposition to a time series results in eight sub series referred to as wavelet packets see fig 1 a and section 3 2 1 and section 3 2 2 since there are 18 lagged rainfall time series considered as candidate inputs in the non wavelet based models see section 4 1 1 there are 90 18 4 18 and 162 18 8 18 candidate input variables in the modwt and at based models and modwpt based models that consider a level three decomposition respectively the candidate input variable sets for all wavelet based models considering decomposition levels from one to three are provided in table a 1 in the supplementary data 4 1 3 dataset partitioning due to time lagging the 432 measurements of observed rainfall at each station by one time lag forward to produce the target variable and 17 time lags backward to produce the initial candidate input variable set along with the removal of 78 boundary affected coefficients a total of 336 records remained spanning february 1978 to december 2005 the 336 records included in the target candidate input variable datasets were divided into a calibration and validation set consisting of 209 february 1978 june 1995 and 126 july 1995 december 2005 records respectively this dataset partitioning was selected since it provided high generalization performance during earlier experimentation however it is important to note that many different techniques could be followed for dataset partitioning see zheng et al 2018 for relevant examples since there is no universal partitioning strategy that works best in all situations although in a split sample validation approach as followed here having a relatively high number of validation records in comparison to the number of training records is generally preferred if one seeks to achieve a lower variance in the estimation of a given performance metric over the validation set arlot and celisse 2010 while there are many different data partitioning percentages that could be adopted or different types of data partitioning methods that could be followed e g k fold cross validation leave one out cross validation leave p out cross validation jackknifing bootstrapping arlot and celisse 2010 exploration of such methods is outside the scope of the present work and may be considered in the future in correspondence with the wddff quilty and adamowski 2018 the calibration set was used to select only those candidate input variables pertinent to the one month ahead forecasts which were thereafter used for calibrating training the ddm parameters using the same data after calibration the ddm were applied to the validation set to evaluate the out of sample performance of the different approaches 4 1 4 input variable selection in the experiments carried out in this study the linear and nonlinear ivs methods and ddm were paired with one another i e pcis was paired with mlr while ea knn and iis were paired with elm this was done in order to compare strictly linear versus nonlinear methods the pcis method does not require hyper parameters to be selected however both ea and knn require the selection of a tolerance value ranging between 0 and 1 the tolerance was set to a value of 0 1 in all experiments which ensured that at least a single input variable was selected it was found that a stricter tolerance e g 0 01 led to significantly fewer input variables being selected additionally knn requires the number of nearest neighbours to be specified which was set as five for all experiments this is one of several values typically used for estimating information theoretic quantities via knn based methods frenzel and pompe 2007 kraskov et al 2004 tsimpiris et al 2012 vlachos and kugiumtzis 2010 in summary ea and knn require the selection of one and two hyper parameters respectively the hyper parameters in iis were selected to ensure reasonable computational efficiency since it requires significantly higher computational run times than pcis ea and knn the various hyper parameters were set as follows five for the maximum number of iterations two for the number of single regression tree models to explore when searching for candidate inputs two for the number of folds to consider in k fold cross validation 10 3 for the stopping tolerance 50 for the number of trees one for the number of alternative cut directions and two for the minimum cardinality for splitting a node 4 1 5 data driven models both mlr and elm parameters were estimated via least squares through the moore penrose generalized inverse elm required the selection of three hyper parameters prior to parameter estimation the number of hidden neurons in the hidden layer the activation functions in the hidden and output layers and the number of ensemble members the activation functions in the hidden and output layer were selected as the sigmoid and linear functions respectively which are common choices eshtay et al 2019 huang et al 2006 the number of ensemble members was set to 50 since this stabilized elm performance at a reasonable computational cost the number of neurons in the hidden layer was explored within the range of 1 2 d 1 where d is the number of inputs used in the model hecht nielsen 1989 and the optimal value was selected according to the number of hidden neurons that minimized the predicted residual sum of square errors metric benoît et al 2013 4 1 6 summary of the developed forecasting models for the 1 month ahead rainfall forecasting case study in the awash river basin a total of 3486 forecasts were generated 8 m t o t a l m a l g o r i t h m m l e v e l m f i l t e r m p a i r s m s t a t i o n s m p a i r s 1 m s t a t i o n s m t o t a l 3 3 16 4 6 5 6 3456 30 3486 where m a l g o r i t h m is the number of wavelet decomposition algorithms at modwt and modwpt m l e v e l is the number of decomposition levels one to three m f i l t e r is the number of wavelet filters see table 2 m p a i r s is the number of ivs ddm pairs pcis mlr ea elm knn elm and iis elm and m s t a t i o n s is the number of stations bati bantu liben dubti ejerselal modjo and wolenchiti this amounts to 3456 wavelet based forecasts and an additional 30 forecasts for the non wavelet based models since the non wavelet based models also include a rw benchmark the next sub section briefly introduces the different performance metrics used to evaluate the forecasting models 4 2 model evaluation several statistical performance metrics commonly adopted in hydrology were used to assess the accuracy of the forecasts produced by the wddff variants and its non wavelet based counterparts to measure the accuracy of the different forecasts a variety of popular metrics were used root mean square error r m s e nash sutcliffe efficiency index n s e bias b i a s volumetric efficiency v e ratio of standard deviations r s d pearson correlation coefficient c c willmott s index of agreement d and kling gupta efficiency index k g e criss and winston 2008 de gooijer and hyndman 2006 gupta et al 2009 hauduc et al 2015 knoben et al 2019 krause et al 2005 legates and mccabe jr 1999 moriasi et al 2007 nash and sutcliffe 1970 willmott et al 2012 zambrano bigiarini 2020 the formulae for these metrics are provided in section c of the supplementary data all models were applied to the validation set once and each performance metric was calculated across the entire validation period i e models are not re calibrated for each forecast additionally a graphical assessment of forecast performance was carried out using tools such as violin plots hintze and nelson 1998 and time series plots all graphical plots in section 5 were created using the gramm matlab toolbox morel 2018 the next section uses the above mentioned quantitative and graphical methods to assess the various forecasts 5 results and discussion the 1 month ahead rainfall forecasts for the six stations in the awash river basin are explored here by several means all reported performance metrics and plots pertain to the validation set only 1 using violin plots to summarize the performance of all wavelet at modwt and modwpt and non wavelet based forecasts built using each ivs method and ddm for several performance metrics n s e v e k g e b i a s r s d and c c 2 a summary of the best performing wavelet and non wavelet based forecasts for each station and performance metric r m s e n s e b i a s v e r s d c c d and k g e 3 a summary that counts the number of times an ivs method ddm wavelet decomposition algorithm decomposition level and wavelet filter was used in the best performing forecast considering all stations and performance metrics 4 time series plots for each station that compare the best wavelet and non wavelet based forecasts according to the k g e metric against the observed rainfall for a two year period in the validation set 5 a comparison of the sum of absolute errors for the best wavelet and non wavelet based forecasts according to the k g e metric at each station 6 a summary of the average number of selected input variables based on different wavelet decomposition and input variable selection methods fig 3 presents violin plots for six different metrics n s e v e k g e b i a s r s d and c c covering the different combinations of wavelet decomposition algorithms decomposition levels wavelet filters ivs methods and ddm for each station thus fig 3 shows the distribution of performance according to each metric for all forecasts at each station according to the different wavelet decomposition algorithms allowing for generalizations to be made given the large sample size there are 1152 wavelet based forecasts for each wavelet decomposition algorithm and an additional 30 forecasts for the non wavelet based models for a total of 3486 forecasts see section 4 1 6 in general it can be seen from fig 3 that the wavelet based forecasts had the best performance for the efficiency indices n s e v e and k g e and c c while the non wavelet based forecasts are competitive in terms of b i a s and superior in terms of r s d the latter result is unsurprising as the best performing non wavelet based model in terms of r s d is the random walk model whose forecasts are simply the observed rainfall from the month before i e y f t 1 y o t therefore the ratio of standard deviations of the forecasts and observed rainfall are nearly equivalent the six stations can be ranked in terms of their forecastability by assessing the relative performance between stations therefore according to all metrics except for b i a s the most forecastable stations are bantu liben then modjo while wolenchiti bati ejerselal are contenders for rank 3 5 depending on the chosen metric n s e v e k g e r s d or c c and dubti is ranked last however in terms of b i a s dubti had the best performing forecasts this result may be partially explained by the bias variance trade off since forecasts at dubti station have low bias but exhibit high variance dubti also has the highest coefficient of variation ratio between the standard deviation and the mean of the monthly rainfall among the six sites see table 1 it can be concluded from fig 3 that 1 the wavelet based models in general have the best performance 2 the best wavelet decomposition algorithm varies by site and performance metric and 3 in most cases the distribution of their performance regardless of the adopted performance metric does not significantly differ compared to the non wavelet based models however in the evaluation of different forecasting models one is often interested in identifying a single best model or class of models in this case with a focus on the wavelet decomposition algorithm amongst many competing alternatives table 3 provides a summary of the best performing combination of wavelet decomposition algorithm ivs method ddm decomposition level and wavelet filter across all stations by considering each performance metric as the means for deciphering the optimal forecasting model at each site note that the r m s e and n s e metrics share the same best performing forecasts this is because the r m s e and n s e have a one to one relationship as pointed out in gupta et al 2009 table 3 clearly indicates that the optimal forecasting model is highly dependent on the performance metric that is used to identify the best model corresponding with the results shown in fig 3 for example consider the bantu liben station the best performing model in terms of r m s e ea elm that used the modwpt at a decomposition level of three and the fk6 wavelet filter achieved an r m s e of 73 83 mm while the best performing model in terms of b i a s iis elm that used the modwt at a decomposition level of three and the sym4 wavelet filter achieved an r m s e of 87 49 mm a difference in performance of 19 87 49 73 83 73 83 considering the best performing forecast for the same station in terms of r s d i e the rw model an r m s e of 104 11 mm was obtained a difference in performance of 41 104 11 73 83 73 83 several other examples could be given that denote the same scenario i e completely different models being deemed optimal depending on the performance metric being adopted of course this is not a unique finding as this is common when exploring data driven forecasting models papacharalampous et al 2019 however given that this is the first study to explore the modwpt in hydrological forecasting especially against other wavelet decomposition algorithms at and modwt which is rarely done quilty and adamowski 2018 this result is still interesting to consider as it helps guide the selection of an appropriate forecasting model depending on the application and the objective that the modeller user is seeking to optimize finally it should be noted that while the optimal forecasting model is dependent on the performance metric used to define the best model there are several cases where different metrics identify the same optimal model for example the d and k g e metrics identify the same optimal model at dubti bati and ejerselal stations the r m s e n s e and v e metrics lead to the same optimal model at modjo station the r m s e n s e v e c c and d identify the same optimal model at wolenchiti station and r m s e n s e and c c identify the same optimal model at ejerselal station since the first goal of this study is to assess whether the modwpt can lead to better performance than at and modwt when integrated within wddff it is useful to explore aggregated information to see if out of all the cases explored modwpt led to a greater number of optimal models than at and or modwt this aggregated information could easily be used to fulfill the second main goal of this study i e to identify any trends in which ivs methods ddm wavelet decomposition algorithms decomposition levels and wavelet filters lead to the best forecasting performance to permit such an analysis table 4 summarizes the results of table 3 this was done by taking the 42 potentially unique cases 7 metrics 6 sites whereby a particular ivs method ddm wavelet decomposition algorithm decomposition level and wavelet filer led to the best performance and tallying the results for each category for example table 4 can be used to identify the number of times the modwpt was included in an optimal forecast based on the results of table 3 the result is 17 out of the 42 cases 40 included the modwpt in the optimal forecasting model while the number of times at and modwt were included in the optimal forecasting model was two 5 and 13 31 out of the 42 cases therefore a general finding of this study is that the modwpt led to a higher number of optimal forecasts than at and modwt when integrated with wddff as determined by seven performance metrics across six sites r m s e and n s e are counted as equivalent further interesting results can be extracted from table 4 linear ivs pcis and ddm mlr were found to produce better forecasts more often than their nonlinear counterparts ea elm knn elm and iis elm 19 42 cases vs 14 42 respectively although for an entirely different application daily urban water demand forecasting a similar finding was described in quilty and adamowski 2018 where the authors cited an earlier study koutsoyiannis et al 2010 that hypothesized that when hydrological processes e g streamflow groundwater levels that respond nonlinearly to driving factors e g rainfall snowmelt are modelled using multiscale approaches e g wavelet based methods or hurst kolmogorov processes there may exist linear relationships between the multiscale predictor s and the target variable however when the multiscale behaviour of the processes are not accounted for explicitly then nonlinear models may provide better performance than linear models which is a general finding of the data driven forecasting literature in hydrology and water resources out of the nonlinear ivs methods the ea approach led to a higher number of better performing forecasts than the knn and iis methods 12 42 vs 0 42 and 2 42 respectively ea has been shown to outperform knn for several synthetic ivs problems as well as an urban water demand forecasting case study quilty et al 2016 while this is the first study to compare ea and knn against the iis method this is an encouraging result as the ea method is much more computationally efficient than both knn and iis higher decomposition levels were included in the best wavelet based forecasting models for example a decomposition level of three was used in 29 42 cases while a decomposition level of one and two were used in only 1 42 and 3 42 cases since a decomposition level of three extracts frequency components between 1 16 1 2 i e 2 16 months it is clear that intra and inter annual frequency components contain important information that can be used to improve model performance for 1 month ahead rainfall forecasts at the six stations in the awash river basin wavelet filters with a higher number of coefficients six to twelve were included in the best performing forecasting models more often 30 42 than wavelet filters with four or less coefficients 3 42 cases since wavelet filters with a higher number of coefficients can encode higher order polynomial behaviour than those with fewer maheswaran and khosa 2012 this potentially indicates the presence of higher order polynomial behaviour and or the manifestation of long range dependence in the rainfall data at the six sites see related discussion in enescu et al 2006 while statistical metrics are useful tools for summarizing in a single value the overall performance of a given forecast it is often just as valuable to see how a forecast compares with the observed variable as it evolves through time in fig 4 time series plots of observed rainfall and the best performing model according to the k g e metric at each station are presented for a two year period in the validation set january 2004 to december 2005 it is clear from fig 4 that there is no single model wavelet or non wavelet based that provides the best forecast at each time a good example of this can be seen by exploring the different forecasts at ejerselal station the maximum monthly rainfall in 2004 is best captured by the modwpt based model while the two months with the highest rainfall in 2005 are best captured by the at based model while both the modwt and modwpt based models generate nearly identical forecasts in this same period similarly at dubti and modjo stations all models inaccurately forecast the maximum monthly rainfall in 2004 but each wavelet based model very accurately forecasts the second highest monthly rainfall amount in the same year at other times such as the end of 2004 at dubti station all models generate dissimilar forecasts however this two year period demonstrates that for the most part the wavelet based forecasting models track the observed rainfall as it evolves through time better than the non wavelet based models furthermore it can be seen that the different models at certain stations and different time periods e g dubti station at the end of 2004 and beginning of 2005 as well as mid year 2005 have complimentary features that could likely be combined in an ensemble framework to take advantage of the strengths of the different models to render improved forecasts akin to recent ensemble approaches proposed in the literature e g quilty et al 2019 tyralis et al 2020 since the amount of rainfall over a given time period and in particular how much rainfall deviates above or below a given anticipated amount plays an important role in the operation of reservoirs informing irrigation practices monitoring groundwater levels crop planning etc it is sometimes useful to judge a forecasting model by its sum of absolute errors sae instead of sum of squared error approaches e g r m s e that are very sensitive to large errors fowler et al 2018 hu et al 2001 fig 5 presents the sae for the best performing model at each site as determined by the k g e metric which clearly shows that at least one wavelet based forecasting model has a lower sae than the non wavelet based models confirming that improved rainfall forecasts in the study area can be generated by considering variables that capture multiscale changes as input to the ddm finally given that wavelet decomposition can extract important multiscale information from explanatory variables and be used to improve ddm performance it seems reasonable to expect that the various input variable selection methods may select a higher number of inputs for the wavelet based models than the non wavelet based models since the former considers both the original and wavelet decomposed input variables table 5 presents the average number of selected input variables considered in the different forecasting models based on different wavelet decomposition and ivs methods considering the 3480 developed forecasts which does not include the random walk models the hypothesis that a higher number of input variables may be selected when an ivs method considers both the original and wavelet decomposed input variables seems to hold interestingly in connection with table 4 the wavelet decomposition methods that led to a higher number of best performing models corresponds with a higher average number of selected input variables similarly both the pcis and ea methods which led to the highest number of best performing models 31 42 cases also had the highest average number of selected input variables amongst ivs methods these results appear to justify the selection of the tolerance parameter for ea and knn and also the maximum number of iterations for the iis method although it does not necessarily justify the selection of the number of trees the number of folds in k fold cross validation etc which could be explored in future work 6 conclusion and future work this study introduces the maximal overlap discrete wavelet packet transform modwpt as a useful pre processing method for hydrological forecasting first the modwpt is used to decompose explanatory variables time series into multiscale components second input variable selection ivs is used to identify the most important multiscale components finally data driven models ddm are used to convert the selected multiscale components into a forecast of the target variable e g rainfall the main advantage of the modwpt over existing wavelet decomposition methods such as the maximal overlap discrete wavelet transform modwt and the à trous algorithm at is that it is able to uncover finer temporal scale information that may be useful for improving forecast performance of hydrological processes that vary over multiple timescales e g rainfall streamflow to test whether the modwpt can lead to improved performance over the modwt and at each wavelet decomposition algorithm is integrated within the wavelet data driven forecasting framework wddff quilty and adamowski 2018 applied for monthly rainfall forecasting at six meteorological stations in the awash river basin ethiopia and evaluated using several statistical performance metrics further different wavelet decomposition settings decomposition level and wavelet filter as well as linear and nonlinear ivs methods and ddm are considered within wddff to explore whether forecasting performance is dependent upon these variants finally the wavelet based methods are benchmarked against their non wavelet based counterparts the main experiment results confirm that 1 modwpt leads to a higher number of better forecasts than modwt and at 2 linear ivs and ddm tend to outperform their nonlinear counterparts when coupled with wavelet decomposition methods 3 the vast majority of best performing models adopt high decomposition levels and wide wavelet filters indicating the importance of considering intra and inter annual changes as well as polynomial behaviour when forecasting the studied rainfall time series 4 the non wavelet based models provide better performance for limited and easily explainable cases see section 5 and discussion surrounding table 3 overall the modwpt integrated wddff is a promising new approach for monthly rainfall forecasting and it is anticipated that it will be useful for forecasting other hydrological variables streamflow groundwater levels etc future research may consider exploring the proposed approach for forecasting a diverse range of hydrological variables streamflow groundwater levels etc as well as including computationally efficient ivs methods e g rrelief robnik šikonja and kononenko 2003 infinite feature selection roffo et al 2015 and new ddm e g seriesnet shen et al 2020 dual stage attention based recurrent neural networks totaro et al 2020 active learning bayesian support vector regression cheng and lu 2021 generalized random forests athey et al 2019 in the modwpt integrated wddff finally the modwpt integrated wddff can also be used to account for multiscale changes in the state variables associated with physical or conceptual models by taking on the form of a data assimilation framework as proposed in boucher et al 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are grateful to the two anonymous reviewers whose suggestions helped to improve the quality of the final manuscript the authors would also like to thank the national meteorological agency of ethiopia for providing the data used in this study partial funding for this research was provided by natural sciences and engineering research council of canada nserc discovery and accelerator grants held by jan adamowski appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105119 
25746,this study introduces the maximal overlap discrete wavelet packet transform modwpt for forecasting hydrological variables that exhibit change over multiple timescales e g rainfall streamflow the advantage of the modwpt over other recent wavelet decomposition methods à trous algorithm at and the maximal overlap discrete wavelet transform modwt is that it can extract finer scale information that may be important for improving forecast performance multiple wavelet decomposition methods modwpt at modwt are integrated within the wavelet data driven forecasting framework wddff applied for forecasting monthly rainfall at six meteorological stations in the awash river basin ethiopia and compared using eight statistical performance metrics results demonstrate that the modwpt can be used to generate more accurate forecasts than the at and modwt for the majority of stations and performance metrics certain settings within the wddff decomposition level wavelet filter input variable method and data driven model lead to improved performance more often than others keywords hydrological forecasting wavelets maximal overlap discrete wavelet packet transform data driven models input variable selection 1 introduction the use of data driven models ddm multiple linear regression mlr decision trees artificial neural networks etc are increasingly being adopted for hydrological forecasting sit et al 2020 tahmasebi et al 2020 tiyasha et al 2020 zhu et al 2020 zounemat kermani et al 2020 since they are useful when there is a lack of knowledge on the physical relationships between a target response and explanatory input variables are rapid to develop and deploy in real time are computationally efficient and often lead to performance that is competitive with physical and or conceptual process based models daliakopoulos and tsanis 2016 demirel et al 2015 gauch et al 2020 gumiere et al 2020 kratzert et al 2019 peijun li et al 2020 read et al 2019 furthermore ddm are starting to be recognized as a means to compliment process based models rather than replace them such as for data assimilation boucher et al 2020 or post processing hydrological model outputs ehlers et al 2019 tyralis et al 2019 however a growing area of interest in hydrological forecasting unique to ddm is addressing multiscale change through the use of wavelet decomposition wavelet decomposition a pre processing method extracts multiscale information from a set of explanatory variables time series and is often used in ddm to improve forecasting performance commonly shown to provide considerable gains in accuracy over benchmark methods i e without wavelet decomposition barzegar et al 2021 hammad et al 2021 jiang et al 2020 ni et al 2020 rahman et al 2020 yeditha et al 2020 zhou et al 2020 the two most popular methods used for wavelet decomposition in hydrological forecasting include the discrete wavelet transform based multiresolution analysis dwt mra and the maximal overlap dwt modwt based multiresolution analysis modwt mra while the à trous algorithm at and the modwt are alternatives that are used less frequently a recent study by quilty and adamowski 2018 demonstrated that both the dwt mra and modwt mra cannot be used correctly for real world forecasting applications since they make use of future data i e they use information from the future during model calibration and validation that would otherwise not be available in a real world setting as noted in earlier studies aussem et al 1998 bogner and pappenberger 2011 du et al 2017 maheswaran and khosa 2012 additionally dwt mra involves decimation requiring the data to be a power of two to provide perfect reconstruction and is shift variant applying the dwt mra to lagged time series causes the time lagged decomposed coefficients to differ when they should be equivalent in contrast the at and modwt avoid the use of future data are non decimating shift invariant and can be used correctly for real world forecasting this finding is significant as 90 of studies using wavelet decomposition for hydrological forecasting make use of either the dwt mra or modwt mra quilty and adamowski 2018 the authors quilty and adamowski 2018 proposed a set of best practices for developing wavelet based forecasts for real world scenarios culminating in the development of the wavelet data driven forecasting framework wddff see section 3 1 the wddff can use either the at or modwt alongside any input variable selection ivs method and ddm for developing wavelet based forecasting models for a given decomposition level and wavelet filter however the at and modwt are limited in the number of time scales that can be extracted from a particular explanatory variable time series wavelet packets cârsteanu et al 1999 allow for a finer scale wavelet decomposition that can potentially extract additional meaningful information from the explanatory variables that would be overlooked if only the at or modwt were explored el hendawi and wang 2020 pengtao li et al 2020 shrifan et al 2021 to date there have been very limited applications of wavelet packet based decomposition for hydrological forecasting a literature review returned the following examples in seo et al 2016 the authors explored several ddm artificial neural networks adaptive neuro fuzzy inference system and support vector machine coupled with wavelet packet decomposition for daily river stage forecasting in the gam stream watershed south korea and found these methods to significantly outperform the benchmark ddm without wavelet packet decomposition moosavi et al 2017 explored a wavelet packet group method of data handling gmdh model and compared it against the gmdh benchmark for daily runoff forecasting in the darian chay ghal chay and lilan chay rivers in east azerbaijan iran finding that the root mean square error could be reduced by 45 through wavelet packet decomposition support vector regression with and without wavelet packet decomposition was explored for forecasting monthly groundwater level fluctuations at three wells in mangalore india where it was demonstrated that the wavelet packet decomposition approach led to more accurate forecasts sujay raghavendra and deka 2015 wavelet packet decomposition based ddm have been explored for drought forecasting das et al 2020 and hindcasting using the standardized precipitation index spi das and deka 2017 and the standardized precipitation evapotranspiration index danandeh mehr et al 2020 respectively with significantly better performance being achieved by the wavelet packet approaches however in each of these studies the wavelet packet integrated ddm models utilized a decomposition approach similar to the dwt mra and therefore suffer from the future data issue as well as shift variance and decimation and are not suitable for real world forecasting applications outside of hydrology wavelet packet methods have been proposed based on a procedure akin to the modwt mra for applications such as retail sales forecasting michis 2009 wind speed forecasting liu and duan 2020 nason and sapatinas 2002 spatial air quality index prediction liu and chen 2020 and forecasting electricity spot prices ben amor et al 2018 however since these methods make use of future data they are not suitable for real world forecasting applications to overcome the future data issue associated with recent wavelet packet based forecasting approaches this study explores the maximal overlap discrete wavelet packet transform modwpt which follows the same decomposition principle as the modwt and is therefore applicable for real world forecasting scenarios the only other study found during a literature review that used the modwpt for forecasting is bruzda 2011 where the author applied the method for forecasting s p 500 returns therefore this study is the first to 1 explore the modwpt for hydrological forecasting 2 compare its performance against the at and modwt algorithms each of which are integrated within the wddff based on these contributions the goal of this research is to explore and answer the following two questions 1 can the modwpt improve forecasting performance over the at and or modwt when integrated within the wddff 2 are there any noticeable trends in terms of which ivs methods ddm wavelet decomposition methods at modwt and modwpt decomposition levels and wavelet filters lead to the best forecasting performance these questions are explored through a monthly rainfall forecasting case study in the awash river basin ethiopia the hope is that in answering the above mentioned questions the potential of the modwpt for rainfall forecasting can be assessed and its use guided towards other important hydrological forecasting applications streamflow groundwater level evaporation water quality and information concerning its performance as assessed through several commonly adopted evaluation metrics can guide the selection of appropriate modelling choices ivs methods ddm decomposition levels and wavelet filters within the wddff for similar applications the remainder of this study is outlined as follows section 2 briefly describes the study area and data section 3 includes information on the employed methods section 4 provides the experimental details for developing and evaluating the different forecasts section 5 includes the main results and a discussion of their significance and section 6 provides concluding remarks and describes future research topics 2 study area and data the awash river basin in ethiopia was selected as the study area to explore the capabilities of the modwpt integrated wddff for 1 month ahead rainfall forecasting this study site was chosen as it is familiar to the authors who have explored a variety of wavelet based ddm for drought forecasting in this area using the spi belayneh et al 2013 2014 2016a 2016b brief details on the study area are included below as of 2012 it is estimated that about 98 of the total cropland in the awash river basin is rainfed agriculture borgomeo et al 2018 and due to frequent and persistent droughts in the basin much of its inhabitants rely on international food aid edossa et al 2010 thus significant research has been devoted to the analysis of climate variability hydro meteorological modelling and forecasting the estimation of the onset and duration of drought events as well as their economic implications in the basin bekele et al 2017 berhe et al 2013 borgomeo et al 2018 dessu et al 2016 edossa et al 2010 tadese et al 2019 tolera et al 2018 given that supplemental irrigation is required to offset deficits of up to 40 of the crop water requirements and variations in rainfall may lead to a decline in agricultural production by 20 with the potential to increase poverty rates by 25 rainfall forecasts are of significant economic and social value to those residing in the basin bekele et al 2017 hagosa et al 2010 while previous studies focussed on forecasting spi in the awash river basin belayneh et al 2014 2016a 2016b this study instead focusses on forecasting rainfall since it can easily be converted into a drought index and may also be used directly in hydrological models or irrigation schedules monthly rainfall data was collected from six meteorlogical stations in this study area bati bantu liben dubti ejerselal modjo and wolenchiti table 1 includes descriptive statistics for the model calibration and validation data see section 4 1 3 additional detailed information on this study site can be found in the above cited studies the target variable at each station was the 1 month ahead observed rainfall while the explanatory input variables were the observed rainfall in the current month and previous months along with their wavelet decomposed counterparts generated via at modwt and modwpt see section 3 2 and 3 3 the next section provides a brief overview of the different methods used to generate forecasts for this study area before describing experimental details 3 methods the section begins with a brief overview of the wddff afterwards the different wavelet decomposition methods at modwt and modwpt integrated with the wddff are briefly described finally the different input variable selection and data driven models used within the wddff are discussed 3 1 wavelet data driven forecasting framework the wddff embodies a set of best practices for real world wavelet based forecasting that focuses on three key components quilty and adamowski 2018 1 proper wavelet decomposition of the explanatory input variables resulting in a set of wavelet and scaling coefficients for each explanatory variable see section 3 2 this component also includes decomposition level and wavelet filter selection 2 selecting the most important wavelet decomposed explanatory variables through ivs 3 using the selected wavelet decomposed explanatory variables as input to a ddm that is afterwards calibrated and validated using historical records proper wavelet decomposition entails using the at modwt or as proposed in this study modwpt in order to avoid the future data condition see section 2 3 1 and 2 3 2 in quilty and adamowski 2018 as well as ensuring boundary affected wavelet and scaling coefficients are removed from the beginning of the time series see section 2 3 3 in quilty and adamowski 2018 the number of boundary affected coefficients can be determined via percival and walden 2000 1 l j 2 j 1 l 1 1 where l j is the number of boundary affected wavelet and scaling coefficients at the beginning of each time series j is the decomposition level and l is the number of wavelet scaling filter coefficients finally j and l are selected such that there remains enough records from which to calibrate and validate a ddm the selection of j and l are primarily dependent on the time series properties and length e g for monthly rainfall data of sufficient length i e at least 200 records a decomposition level of three will allow the extraction of inter intra yearly and sub seasonal periodicities and changes thereof while wavelet filters with four or more coefficients will enable the extraction of second degree or higher polynomial behaviour and secondly on the modeller s preference available computational resources etc ivs is used to identify only those wavelet and scaling coefficients that are useful in generating accurate forecasts of the target variable while ddm calibration tunes model parameters such that their forecasts resemble as much as possible a set of historical calibration targets finally ddm validation confirms that the calibrated ddm provides accurate out of sample performance on a set of historical records of the target variable not used to calibrate the model finally an advantage of wddff is that it can be used with any ivs method or ddm see section 3 3 and 3 4 for the ivs methods and ddm adopted in this study the main components of the wddff wavelet decomposition algorithms ivs methods and ddm are discussed in the sub sections that follow 3 2 wavelet decomposition methods 3 2 1 à trous algorithm and maximal overlap discrete wavelet transform in simple terms both the at and modwt can be used to decompose a time series x t for t 0 1 n 1 where t is a time index and n is the number of samples in the time series into a number of sub components each of the same length as x t that capture the multiscale information in the series as it changes through time allowing for periodicities transients level shifts etc to be extracted from the time series the high frequency information extracted by the at or modwt is referred to as wavelet coefficients while the low frequency information is referred to as scaling coefficients the high and low frequency information is obtained by convolving the time series with high and low pass filters respectively known as wavelet and scaling filters further details on the unique properties of these filters can be found in percival and walden 2000 to maintain brevity the main equations for calculating wavelet and scaling coefficients via the at modwt and modwpt will be provided detailed treatment of these methods has been covered earlier alves et al 2017 aussem et al 1998 olhede and walden 2004 percival and walden 2000 renaud et al 2003 walden and contreras cristan 1998 the notation used here to define the at and modwt follows quilty and adamowski 2018 as derived from maheswaran and khosa 2012 percival and walden 2000 the at algorithm can be written as quilty and adamowski 2018 2 v j t a l 0 l 1 g l v j 1 t 2 j 1 l mod n a 3 w j t a v j 1 t a v j t a while the modwt can be written as quilty and adamowski 2018 4 v j t m l 0 l 1 g l v j 1 t 2 j 1 l mod n m 5 w j t m l 0 l 1 h l v j 1 t 2 j 1 l mod n m where x t v 0 t a v 0 t m represents the original time series at time t v j t a v j t m are the at modwt scaling coefficients at scale j j 1 2 j w j t a w j t m are the at modwt wavelet coefficients at scale j h g is a wavelet scaling filter mod refers to the modulo operator g l g l 2 h l h l 2 and g l 1 l 1 h l 1 t for l 0 1 l 1 and l is the length of the wavelet scaling filter at level j the wavelet coefficients belong to the nominal frequency band associated with frequencies in the interval 1 2 j 1 1 2 j while the scaling coefficients belong to the nominal frequency band associated with frequencies in the interval 0 1 2 j 1 a list of scaling filter coefficients including those used in this study can be found in the supplementary data of quilty and adamowski 2018 for example the haar scaling filter coefficients are given by g 0 1 2 and g 1 1 2 respectively wavelet decomposition via the at or modwt results in j sets of wavelet coefficients w 1 w 2 w j and a set of scaling coefficients v j sub and superscripts removed for simplicity note that the main difference between the at and modwt can be seen through the calculation of wavelet coefficients the at calculates wavelet coefficients as the difference between scaling coefficients at adjacent decomposition levels while the modwt uses a wavelet filter h to estimate wavelet coefficients in summary due to the difference in how wavelet coefficients are calculated between at and modwt methods the at can be interpreted as an additive wavelet decomposition i e the original time series can be reconstructed perfectly by aggregating the wavelet coefficients from scales 1 to j with the scaling coefficients at scale j while the modwt can be interpreted as an energy based wavelet decomposition from which it follows that the variance of the wavelet coefficients at scales 1 though j and the scaling coefficients at scale j when aggregated have the same variance as the original time series additive wavelet decomposition is not a property of the modwt coefficients nor is energy based wavelet decomposition a property of the at except for the haar wavelet filter whereby both the modwt and at are equivalent and thus share the same properties energy based and additive wavelet decomposition bruzda 2019 3 2 2 maximal overlap discrete wavelet packet transform wavelet decomposition via the modwpt can be formulated as walden and contreras cristan 1998 6 w j n t p l 0 l 1 f n l w j 1 n 2 t 2 j 1 l mod n p 7 f n l g l i f n mod 4 0 or 3 h l i f n mod 4 1 or 2 where w j n t p are the modwpt coefficients at time t for decomposition level j and band n where n 0 1 2 j 1 which is nominally associated with frequencies in the interval n 2 j 1 n 1 2 j 1 and represents the integer part operator sometimes also referred to as the floor operator similar to the at and modwt x t w 0 0 t p v 0 t a v 0 t m the modwpt like the modwt is an energy based wavelet decomposition algorithm however when the haar wavelet filter is adopted both the modwt and modwpt also permit an additive decomposition of a given time series the modwpt differs from the modwt in that at a given level both the wavelet and scaling coefficients from the previous level are decomposed via low and high pass filtering whereas in the modwt only the scaling coefficients from the previous level are decomposed through low and high pass filtering this leads to an increase in the number of wavelet coefficients or wavelet packets when the modwpt is used to decompose a time series thereby extracting further information from the wavelet coefficients that would remain hidden if the modwt was applied the schematic in fig 1 shows the coefficients produced by performing wavelet decomposition on a time series via modwpt and modwt by comparing fig 1 a and b it can be seen that the modwt is encapsulated by the wavelet packets generated via the modwpt the bolded components from the modwt b are equivalent to their counterparts in the modwpt a section a of the supplementary data includes an example of the modwt and modwpt applied to the rainfall time series at the bantu liben station taking fig 1 as an example the information contained in w 1 m from the modwt is decomposed into four separate wavelet packets in the modwpt w 3 4 p w 3 5 p w 3 6 p w 3 7 p the advantage of breaking this information in to several components is that there may be certain information in w 1 m that is not entirely useful or perhaps confounding and when broken into additional components may be filtered out e g perhaps only w 3 6 p and w 3 7 p provide useful information about the target variable therefore w 3 4 p and w 3 5 p can be discarded which would be entirely hidden in w 1 m if the modwpt was not used for a wavelet decomposition at level j the modwpt has a set of 2 j wavelet packets w j 0 p w j 1 p w j 2 j 1 p it is clear that as the decomposition level increases the number of wavelet packets generated through the modwpt rapidly increases therefore for ddm coupled with the modwpt it is important that some form of input variable selection be performed in order to select only those wavelet packets that are useful when forecasting the target variable this is the subject of the next sub section 3 3 input variable selection the benefits of ivs in the hydrological literature on ddm are well known galelli et al 2014 hejazi and cai 2009 kim et al 2020 may et al 2008 ren et al 2020 2021 snieder et al 2020 wan jaafar et al 2011 the main task of ivs is to ensure that only those variables relevant to forecasting the target variable are selected while those that are redundant and or irrelevant are discarded ideally resulting in accurate models that are not overly complex or hampered in their forecasting capabilities ivs methods are generally broken down into three categories filter wrapper and embedded approaches guyon and elisseeff 2003 filter methods select input variables external to a given model wrapper methods use a ddm to provide some measure of how a set of selected inputs impact model performance while embedded methods are directly incorporated into the calibration of ddm parameters effrosynidis and arampatzis 2021 may et al 2011 in general filter and wrapper methods tend to be the focus of most articles that compare ivs approaches against one another bommert et al 2020 this study explores both filter and wrapper ivs methods namely partial correlation input selection pcis may et al 2008 filter method edgeworth approximations based conditional mutual information ea quilty et al 2016 filter method k nearest neighbours based conditional mutual information knn quilty et al 2016 tsimpiris et al 2012 vlachos and kugiumtzis 2010 filter method iterative input selection iis galelli and castelletti 2013 wrapper method these methods were selected due to their high performance and or popularity in hydrology and water resources applications bertoni et al 2021 boucher et al 2020 hatami et al 2019 nunes carvalho et al 2021 quilty and adamowski 2020 ren et al 2020 zandmoghaddam et al 2019 brief details on the pcis ea knn and iis input variable selection methods along with their tunable hyper parameters if any are provided in section b of the supplementary data theoretical background on the various ivs methods can be found in the above mentioned studies and or the references therein 3 4 data driven models in order to map the explanatory variables determined via ivs into forecasts of the target variable data driven models are used since any ddm can be used within the wddff several varieties have thus far been explored including multiple linear regression mlr machine learning e g extreme learning machine elm multivariate adaptive regression splines and deep learning approaches long short term memory networks lstm ghaemi et al 2019 mouatadid et al 2019 quilty and adamowski 2018 in this study since the focus is on exploring the improvements in the wddff that may be realized by adopting modwpt instead of at and modwt algorithms two simple data driven methods were selected one linear method mlr and one nonlinear method elm these methods were chosen since their parameters are easily determined via least squares which makes them highly efficient methods further elm has become a very popular alternative to traditional feed forward backpropagation artificial neural networks bp ann since it provides similar and often better performance than bp ann with a significantly lower computational expense barzegar et al 2017 fan et al 2018 kisi et al 2020 li et al 2019 lima et al 2015 niu and feng 2021 parisouj et al 2020 patil and deka 2016 shamshirband et al 2020 yaseen et al 2019 yu et al 2020 while simple linear mlr and nonlinear elm models commonly used for hydrological forecasting are adopted in order to focus on the different wavelet decomposition approaches more recent ddm can be considered instead and may lead to more accurate forecasts than the ddm used here for instance recent machine learning and deep learning models applied within hydrology such as extreme gradient boosting gauch et al 2021 convolutional neural networks coupled with lstm barzegar et al 2020 or deep residual networks yan et al 2021 and across other domains such as online random forests with memories zhong et al 2020 fully recurrent lagged variable representation network fr lavrnet koutlis et al 2020 or deep kernel based extreme learning machine afzal et al 2021 can easily be integrated within the proposed approach the main parameters that need to be determined in mlr are the slope coefficients for each input variable along with the bias term for elm a set of hyper parameters need to be selected before estimating model parameters the hyper parameters are the number of hidden neurons in the hidden layer as well as the hidden and output layer activation functions additionally to help reduce the impact of the random generation process for input weights and hidden neuron biases on the identification of the optimal number of hidden neurons this study uses an ensemble elm liu and wang 2010 therefore the number of ensemble members in elm is an additional hyper parameter after selecting hyper parameters for each ensemble member in elm the output layer weights parameters can be determined through a least squares solution by taking the product of the moore penrose generalized inverse of the hidden layer output matrix and the target variable the overall forecast is the mean of the forecasts produced by the different ensemble members the interested reader can find theoretical background on elm in huang et al 2006 2012 finally the linear and nonlinear models are benchmarked against the random walk rw model a useful benchmark in hydrological forecasting due to its simplicity ferdosian and haie 2016 fullerton and molina 2010 quilty and adamowski 2018 which does not require the selection of any parameters since a one step ahead forecast is equivalent to the most recently observed measurement which is referred to as the persistence method or the naïve method in other studies amaranto et al 2018 pacchin et al 2019 papacharalampous and tyralis 2018 pappenberger et al 2015 in the next section experimental details are provided concerning the case study which includes information on model development and evaluation 4 experimental details this section outlines the main model development steps in the wddff and then introduces the performance metrics used to evaluate the forecasting models 4 1 model development the main model development steps are outlined in the sub sections below and summarized in fig 2 4 1 1 generating the candidate input variable set the first step in the development of the wddff is the generation of the candidate input variable set since the only data used in this study is monthly rainfall observations available at six different stations in the awash river basin bati bantu liben dubti ejerselal modjo and wolenchiti the candidate input variables are time lagged versions of observed rainfall at each station as well as their wavelet decomposed components in many studies partial autocorrelation is used to identify which lags to include as candidate input variables since the physical processes that govern rainfall are often represented through nonlinear relationships valverde ramírez et al 2006 it is useful to replace partial autocorrelation with conditional mutual information it was found by using the ea and knn based conditional mutual information section 3 1 at each station that the current and previous 17 time lags are useful for forecasting rainfall 1 month ahead i e y t 1 therefore the candidate input variable set for the non wavelet based models considered 18 input variables i e y t y t 1 y t 17 for forecasting rainfall at a lead time of 1 month i e y ˆ t 1 for the wavelet based models the candidate input variable set included the same inputs in addition to their wavelet decomposed counterparts as discussed in the next sub section each of the lagged rainfall time series were wavelet decomposed separately due to the internal function of the software used for wavelet decomposition however since the modwpt modwt and at are all shift invariant the original rainfall time series could be decomposed first and the resulting wavelet and scaling coefficients time lagged afterwards to produce the same results which is a more computationally efficient approach to follow however since monthly data is employed rather than very fine scale information such as 10 min rainfall data this slightly less efficient approach to wavelet decomposition does not create a significant bottleneck in the forecasting routine that would be employed in practice finally while a very large number of input variables are considered as candidates for each wavelet based model see section 4 1 2 the number of inputs used in the ddm are considerably reduced via ivs see the last paragraph in section 5 4 1 2 wavelet decomposition wavelet decomposition was carried out in this study using the at modwt and modwpt algorithms according to the best practices outlined in quilty and adamowski 2018 see also section 3 1 the main considerations during wavelet decomposition are the selection of the decomposition level s and wavelet filter s as well as the removal of boundary affected coefficients according to equation 1 the decomposition level and wavelet filter must be chosen such that after removing the boundary affected coefficients a reasonable amount of calibration records remain since only 432 monthly rainfall records were collected from each of the six stations it was determined that a maximum decomposition level of three and wavelet filter length of 12 would be explored this required the removal of 78 boundary affected coefficients from the beginning of the target variable and candidate input variable sets i e according to equation 1 decomposition levels from one to three were explored for 16 different wavelet filters with filter lengths ranging from two to twelve the different wavelet filters explored in this study can be found in table 2 see quilty and adamowski 2018 and references therein for further details wavelet decomposed candidate input sets that adopted a decomposition level less than three or wavelet filters with length less than 12 had fewer than 78 boundary affected coefficients however 78 coefficients were removed from the beginning of the target and candidate input variable sets in order to maintain the same number of calibration records and permit a fair judgement between different wavelet decomposition algorithms decomposition levels wavelet filters ivs methods and ddm the non wavelet based models followed the same principle however in practice once an optimal model has been identified there is no need to remove more coefficients than required and the ddm can be recalibrated with the extra records removed during model comparison applying a level three modwt or at based decomposition to a given time series results in four sub series three sets of wavelet coefficients and a single set of scaling coefficients see fig 1 b while applying a level three modwpt based decomposition to a time series results in eight sub series referred to as wavelet packets see fig 1 a and section 3 2 1 and section 3 2 2 since there are 18 lagged rainfall time series considered as candidate inputs in the non wavelet based models see section 4 1 1 there are 90 18 4 18 and 162 18 8 18 candidate input variables in the modwt and at based models and modwpt based models that consider a level three decomposition respectively the candidate input variable sets for all wavelet based models considering decomposition levels from one to three are provided in table a 1 in the supplementary data 4 1 3 dataset partitioning due to time lagging the 432 measurements of observed rainfall at each station by one time lag forward to produce the target variable and 17 time lags backward to produce the initial candidate input variable set along with the removal of 78 boundary affected coefficients a total of 336 records remained spanning february 1978 to december 2005 the 336 records included in the target candidate input variable datasets were divided into a calibration and validation set consisting of 209 february 1978 june 1995 and 126 july 1995 december 2005 records respectively this dataset partitioning was selected since it provided high generalization performance during earlier experimentation however it is important to note that many different techniques could be followed for dataset partitioning see zheng et al 2018 for relevant examples since there is no universal partitioning strategy that works best in all situations although in a split sample validation approach as followed here having a relatively high number of validation records in comparison to the number of training records is generally preferred if one seeks to achieve a lower variance in the estimation of a given performance metric over the validation set arlot and celisse 2010 while there are many different data partitioning percentages that could be adopted or different types of data partitioning methods that could be followed e g k fold cross validation leave one out cross validation leave p out cross validation jackknifing bootstrapping arlot and celisse 2010 exploration of such methods is outside the scope of the present work and may be considered in the future in correspondence with the wddff quilty and adamowski 2018 the calibration set was used to select only those candidate input variables pertinent to the one month ahead forecasts which were thereafter used for calibrating training the ddm parameters using the same data after calibration the ddm were applied to the validation set to evaluate the out of sample performance of the different approaches 4 1 4 input variable selection in the experiments carried out in this study the linear and nonlinear ivs methods and ddm were paired with one another i e pcis was paired with mlr while ea knn and iis were paired with elm this was done in order to compare strictly linear versus nonlinear methods the pcis method does not require hyper parameters to be selected however both ea and knn require the selection of a tolerance value ranging between 0 and 1 the tolerance was set to a value of 0 1 in all experiments which ensured that at least a single input variable was selected it was found that a stricter tolerance e g 0 01 led to significantly fewer input variables being selected additionally knn requires the number of nearest neighbours to be specified which was set as five for all experiments this is one of several values typically used for estimating information theoretic quantities via knn based methods frenzel and pompe 2007 kraskov et al 2004 tsimpiris et al 2012 vlachos and kugiumtzis 2010 in summary ea and knn require the selection of one and two hyper parameters respectively the hyper parameters in iis were selected to ensure reasonable computational efficiency since it requires significantly higher computational run times than pcis ea and knn the various hyper parameters were set as follows five for the maximum number of iterations two for the number of single regression tree models to explore when searching for candidate inputs two for the number of folds to consider in k fold cross validation 10 3 for the stopping tolerance 50 for the number of trees one for the number of alternative cut directions and two for the minimum cardinality for splitting a node 4 1 5 data driven models both mlr and elm parameters were estimated via least squares through the moore penrose generalized inverse elm required the selection of three hyper parameters prior to parameter estimation the number of hidden neurons in the hidden layer the activation functions in the hidden and output layers and the number of ensemble members the activation functions in the hidden and output layer were selected as the sigmoid and linear functions respectively which are common choices eshtay et al 2019 huang et al 2006 the number of ensemble members was set to 50 since this stabilized elm performance at a reasonable computational cost the number of neurons in the hidden layer was explored within the range of 1 2 d 1 where d is the number of inputs used in the model hecht nielsen 1989 and the optimal value was selected according to the number of hidden neurons that minimized the predicted residual sum of square errors metric benoît et al 2013 4 1 6 summary of the developed forecasting models for the 1 month ahead rainfall forecasting case study in the awash river basin a total of 3486 forecasts were generated 8 m t o t a l m a l g o r i t h m m l e v e l m f i l t e r m p a i r s m s t a t i o n s m p a i r s 1 m s t a t i o n s m t o t a l 3 3 16 4 6 5 6 3456 30 3486 where m a l g o r i t h m is the number of wavelet decomposition algorithms at modwt and modwpt m l e v e l is the number of decomposition levels one to three m f i l t e r is the number of wavelet filters see table 2 m p a i r s is the number of ivs ddm pairs pcis mlr ea elm knn elm and iis elm and m s t a t i o n s is the number of stations bati bantu liben dubti ejerselal modjo and wolenchiti this amounts to 3456 wavelet based forecasts and an additional 30 forecasts for the non wavelet based models since the non wavelet based models also include a rw benchmark the next sub section briefly introduces the different performance metrics used to evaluate the forecasting models 4 2 model evaluation several statistical performance metrics commonly adopted in hydrology were used to assess the accuracy of the forecasts produced by the wddff variants and its non wavelet based counterparts to measure the accuracy of the different forecasts a variety of popular metrics were used root mean square error r m s e nash sutcliffe efficiency index n s e bias b i a s volumetric efficiency v e ratio of standard deviations r s d pearson correlation coefficient c c willmott s index of agreement d and kling gupta efficiency index k g e criss and winston 2008 de gooijer and hyndman 2006 gupta et al 2009 hauduc et al 2015 knoben et al 2019 krause et al 2005 legates and mccabe jr 1999 moriasi et al 2007 nash and sutcliffe 1970 willmott et al 2012 zambrano bigiarini 2020 the formulae for these metrics are provided in section c of the supplementary data all models were applied to the validation set once and each performance metric was calculated across the entire validation period i e models are not re calibrated for each forecast additionally a graphical assessment of forecast performance was carried out using tools such as violin plots hintze and nelson 1998 and time series plots all graphical plots in section 5 were created using the gramm matlab toolbox morel 2018 the next section uses the above mentioned quantitative and graphical methods to assess the various forecasts 5 results and discussion the 1 month ahead rainfall forecasts for the six stations in the awash river basin are explored here by several means all reported performance metrics and plots pertain to the validation set only 1 using violin plots to summarize the performance of all wavelet at modwt and modwpt and non wavelet based forecasts built using each ivs method and ddm for several performance metrics n s e v e k g e b i a s r s d and c c 2 a summary of the best performing wavelet and non wavelet based forecasts for each station and performance metric r m s e n s e b i a s v e r s d c c d and k g e 3 a summary that counts the number of times an ivs method ddm wavelet decomposition algorithm decomposition level and wavelet filter was used in the best performing forecast considering all stations and performance metrics 4 time series plots for each station that compare the best wavelet and non wavelet based forecasts according to the k g e metric against the observed rainfall for a two year period in the validation set 5 a comparison of the sum of absolute errors for the best wavelet and non wavelet based forecasts according to the k g e metric at each station 6 a summary of the average number of selected input variables based on different wavelet decomposition and input variable selection methods fig 3 presents violin plots for six different metrics n s e v e k g e b i a s r s d and c c covering the different combinations of wavelet decomposition algorithms decomposition levels wavelet filters ivs methods and ddm for each station thus fig 3 shows the distribution of performance according to each metric for all forecasts at each station according to the different wavelet decomposition algorithms allowing for generalizations to be made given the large sample size there are 1152 wavelet based forecasts for each wavelet decomposition algorithm and an additional 30 forecasts for the non wavelet based models for a total of 3486 forecasts see section 4 1 6 in general it can be seen from fig 3 that the wavelet based forecasts had the best performance for the efficiency indices n s e v e and k g e and c c while the non wavelet based forecasts are competitive in terms of b i a s and superior in terms of r s d the latter result is unsurprising as the best performing non wavelet based model in terms of r s d is the random walk model whose forecasts are simply the observed rainfall from the month before i e y f t 1 y o t therefore the ratio of standard deviations of the forecasts and observed rainfall are nearly equivalent the six stations can be ranked in terms of their forecastability by assessing the relative performance between stations therefore according to all metrics except for b i a s the most forecastable stations are bantu liben then modjo while wolenchiti bati ejerselal are contenders for rank 3 5 depending on the chosen metric n s e v e k g e r s d or c c and dubti is ranked last however in terms of b i a s dubti had the best performing forecasts this result may be partially explained by the bias variance trade off since forecasts at dubti station have low bias but exhibit high variance dubti also has the highest coefficient of variation ratio between the standard deviation and the mean of the monthly rainfall among the six sites see table 1 it can be concluded from fig 3 that 1 the wavelet based models in general have the best performance 2 the best wavelet decomposition algorithm varies by site and performance metric and 3 in most cases the distribution of their performance regardless of the adopted performance metric does not significantly differ compared to the non wavelet based models however in the evaluation of different forecasting models one is often interested in identifying a single best model or class of models in this case with a focus on the wavelet decomposition algorithm amongst many competing alternatives table 3 provides a summary of the best performing combination of wavelet decomposition algorithm ivs method ddm decomposition level and wavelet filter across all stations by considering each performance metric as the means for deciphering the optimal forecasting model at each site note that the r m s e and n s e metrics share the same best performing forecasts this is because the r m s e and n s e have a one to one relationship as pointed out in gupta et al 2009 table 3 clearly indicates that the optimal forecasting model is highly dependent on the performance metric that is used to identify the best model corresponding with the results shown in fig 3 for example consider the bantu liben station the best performing model in terms of r m s e ea elm that used the modwpt at a decomposition level of three and the fk6 wavelet filter achieved an r m s e of 73 83 mm while the best performing model in terms of b i a s iis elm that used the modwt at a decomposition level of three and the sym4 wavelet filter achieved an r m s e of 87 49 mm a difference in performance of 19 87 49 73 83 73 83 considering the best performing forecast for the same station in terms of r s d i e the rw model an r m s e of 104 11 mm was obtained a difference in performance of 41 104 11 73 83 73 83 several other examples could be given that denote the same scenario i e completely different models being deemed optimal depending on the performance metric being adopted of course this is not a unique finding as this is common when exploring data driven forecasting models papacharalampous et al 2019 however given that this is the first study to explore the modwpt in hydrological forecasting especially against other wavelet decomposition algorithms at and modwt which is rarely done quilty and adamowski 2018 this result is still interesting to consider as it helps guide the selection of an appropriate forecasting model depending on the application and the objective that the modeller user is seeking to optimize finally it should be noted that while the optimal forecasting model is dependent on the performance metric used to define the best model there are several cases where different metrics identify the same optimal model for example the d and k g e metrics identify the same optimal model at dubti bati and ejerselal stations the r m s e n s e and v e metrics lead to the same optimal model at modjo station the r m s e n s e v e c c and d identify the same optimal model at wolenchiti station and r m s e n s e and c c identify the same optimal model at ejerselal station since the first goal of this study is to assess whether the modwpt can lead to better performance than at and modwt when integrated within wddff it is useful to explore aggregated information to see if out of all the cases explored modwpt led to a greater number of optimal models than at and or modwt this aggregated information could easily be used to fulfill the second main goal of this study i e to identify any trends in which ivs methods ddm wavelet decomposition algorithms decomposition levels and wavelet filters lead to the best forecasting performance to permit such an analysis table 4 summarizes the results of table 3 this was done by taking the 42 potentially unique cases 7 metrics 6 sites whereby a particular ivs method ddm wavelet decomposition algorithm decomposition level and wavelet filer led to the best performance and tallying the results for each category for example table 4 can be used to identify the number of times the modwpt was included in an optimal forecast based on the results of table 3 the result is 17 out of the 42 cases 40 included the modwpt in the optimal forecasting model while the number of times at and modwt were included in the optimal forecasting model was two 5 and 13 31 out of the 42 cases therefore a general finding of this study is that the modwpt led to a higher number of optimal forecasts than at and modwt when integrated with wddff as determined by seven performance metrics across six sites r m s e and n s e are counted as equivalent further interesting results can be extracted from table 4 linear ivs pcis and ddm mlr were found to produce better forecasts more often than their nonlinear counterparts ea elm knn elm and iis elm 19 42 cases vs 14 42 respectively although for an entirely different application daily urban water demand forecasting a similar finding was described in quilty and adamowski 2018 where the authors cited an earlier study koutsoyiannis et al 2010 that hypothesized that when hydrological processes e g streamflow groundwater levels that respond nonlinearly to driving factors e g rainfall snowmelt are modelled using multiscale approaches e g wavelet based methods or hurst kolmogorov processes there may exist linear relationships between the multiscale predictor s and the target variable however when the multiscale behaviour of the processes are not accounted for explicitly then nonlinear models may provide better performance than linear models which is a general finding of the data driven forecasting literature in hydrology and water resources out of the nonlinear ivs methods the ea approach led to a higher number of better performing forecasts than the knn and iis methods 12 42 vs 0 42 and 2 42 respectively ea has been shown to outperform knn for several synthetic ivs problems as well as an urban water demand forecasting case study quilty et al 2016 while this is the first study to compare ea and knn against the iis method this is an encouraging result as the ea method is much more computationally efficient than both knn and iis higher decomposition levels were included in the best wavelet based forecasting models for example a decomposition level of three was used in 29 42 cases while a decomposition level of one and two were used in only 1 42 and 3 42 cases since a decomposition level of three extracts frequency components between 1 16 1 2 i e 2 16 months it is clear that intra and inter annual frequency components contain important information that can be used to improve model performance for 1 month ahead rainfall forecasts at the six stations in the awash river basin wavelet filters with a higher number of coefficients six to twelve were included in the best performing forecasting models more often 30 42 than wavelet filters with four or less coefficients 3 42 cases since wavelet filters with a higher number of coefficients can encode higher order polynomial behaviour than those with fewer maheswaran and khosa 2012 this potentially indicates the presence of higher order polynomial behaviour and or the manifestation of long range dependence in the rainfall data at the six sites see related discussion in enescu et al 2006 while statistical metrics are useful tools for summarizing in a single value the overall performance of a given forecast it is often just as valuable to see how a forecast compares with the observed variable as it evolves through time in fig 4 time series plots of observed rainfall and the best performing model according to the k g e metric at each station are presented for a two year period in the validation set january 2004 to december 2005 it is clear from fig 4 that there is no single model wavelet or non wavelet based that provides the best forecast at each time a good example of this can be seen by exploring the different forecasts at ejerselal station the maximum monthly rainfall in 2004 is best captured by the modwpt based model while the two months with the highest rainfall in 2005 are best captured by the at based model while both the modwt and modwpt based models generate nearly identical forecasts in this same period similarly at dubti and modjo stations all models inaccurately forecast the maximum monthly rainfall in 2004 but each wavelet based model very accurately forecasts the second highest monthly rainfall amount in the same year at other times such as the end of 2004 at dubti station all models generate dissimilar forecasts however this two year period demonstrates that for the most part the wavelet based forecasting models track the observed rainfall as it evolves through time better than the non wavelet based models furthermore it can be seen that the different models at certain stations and different time periods e g dubti station at the end of 2004 and beginning of 2005 as well as mid year 2005 have complimentary features that could likely be combined in an ensemble framework to take advantage of the strengths of the different models to render improved forecasts akin to recent ensemble approaches proposed in the literature e g quilty et al 2019 tyralis et al 2020 since the amount of rainfall over a given time period and in particular how much rainfall deviates above or below a given anticipated amount plays an important role in the operation of reservoirs informing irrigation practices monitoring groundwater levels crop planning etc it is sometimes useful to judge a forecasting model by its sum of absolute errors sae instead of sum of squared error approaches e g r m s e that are very sensitive to large errors fowler et al 2018 hu et al 2001 fig 5 presents the sae for the best performing model at each site as determined by the k g e metric which clearly shows that at least one wavelet based forecasting model has a lower sae than the non wavelet based models confirming that improved rainfall forecasts in the study area can be generated by considering variables that capture multiscale changes as input to the ddm finally given that wavelet decomposition can extract important multiscale information from explanatory variables and be used to improve ddm performance it seems reasonable to expect that the various input variable selection methods may select a higher number of inputs for the wavelet based models than the non wavelet based models since the former considers both the original and wavelet decomposed input variables table 5 presents the average number of selected input variables considered in the different forecasting models based on different wavelet decomposition and ivs methods considering the 3480 developed forecasts which does not include the random walk models the hypothesis that a higher number of input variables may be selected when an ivs method considers both the original and wavelet decomposed input variables seems to hold interestingly in connection with table 4 the wavelet decomposition methods that led to a higher number of best performing models corresponds with a higher average number of selected input variables similarly both the pcis and ea methods which led to the highest number of best performing models 31 42 cases also had the highest average number of selected input variables amongst ivs methods these results appear to justify the selection of the tolerance parameter for ea and knn and also the maximum number of iterations for the iis method although it does not necessarily justify the selection of the number of trees the number of folds in k fold cross validation etc which could be explored in future work 6 conclusion and future work this study introduces the maximal overlap discrete wavelet packet transform modwpt as a useful pre processing method for hydrological forecasting first the modwpt is used to decompose explanatory variables time series into multiscale components second input variable selection ivs is used to identify the most important multiscale components finally data driven models ddm are used to convert the selected multiscale components into a forecast of the target variable e g rainfall the main advantage of the modwpt over existing wavelet decomposition methods such as the maximal overlap discrete wavelet transform modwt and the à trous algorithm at is that it is able to uncover finer temporal scale information that may be useful for improving forecast performance of hydrological processes that vary over multiple timescales e g rainfall streamflow to test whether the modwpt can lead to improved performance over the modwt and at each wavelet decomposition algorithm is integrated within the wavelet data driven forecasting framework wddff quilty and adamowski 2018 applied for monthly rainfall forecasting at six meteorological stations in the awash river basin ethiopia and evaluated using several statistical performance metrics further different wavelet decomposition settings decomposition level and wavelet filter as well as linear and nonlinear ivs methods and ddm are considered within wddff to explore whether forecasting performance is dependent upon these variants finally the wavelet based methods are benchmarked against their non wavelet based counterparts the main experiment results confirm that 1 modwpt leads to a higher number of better forecasts than modwt and at 2 linear ivs and ddm tend to outperform their nonlinear counterparts when coupled with wavelet decomposition methods 3 the vast majority of best performing models adopt high decomposition levels and wide wavelet filters indicating the importance of considering intra and inter annual changes as well as polynomial behaviour when forecasting the studied rainfall time series 4 the non wavelet based models provide better performance for limited and easily explainable cases see section 5 and discussion surrounding table 3 overall the modwpt integrated wddff is a promising new approach for monthly rainfall forecasting and it is anticipated that it will be useful for forecasting other hydrological variables streamflow groundwater levels etc future research may consider exploring the proposed approach for forecasting a diverse range of hydrological variables streamflow groundwater levels etc as well as including computationally efficient ivs methods e g rrelief robnik šikonja and kononenko 2003 infinite feature selection roffo et al 2015 and new ddm e g seriesnet shen et al 2020 dual stage attention based recurrent neural networks totaro et al 2020 active learning bayesian support vector regression cheng and lu 2021 generalized random forests athey et al 2019 in the modwpt integrated wddff finally the modwpt integrated wddff can also be used to account for multiscale changes in the state variables associated with physical or conceptual models by taking on the form of a data assimilation framework as proposed in boucher et al 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are grateful to the two anonymous reviewers whose suggestions helped to improve the quality of the final manuscript the authors would also like to thank the national meteorological agency of ethiopia for providing the data used in this study partial funding for this research was provided by natural sciences and engineering research council of canada nserc discovery and accelerator grants held by jan adamowski appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105119 
25747,high fidelity hydrological models are increasingly built and used to investigate the effects of management activities and climate change on water availability and quality for large areas with datasets of high spatial and temporal resolution however these advantages come at the price of greater computational demand and run time this becomes challenging when modeling routines involve iterative model simulations in this study we proposed a generic scheme to reduce the soil and water assessment tool swat runtime by decomposing a watershed model into subbasin models and optimizing the subbasin model simulations based on a parallel approach based on this scheme we implemented a generic tool named spark swat which allows subbasin models to be simulated in parallel on a spark computer cluster we then evaluated spark swat with two sets of experiments to demonstrate the potential of spark swat to accelerate single and iterative model simulations in each test set spark swat was applied to simulate 12 synthetic hydrological models in parallel with different i o input output burdens and river network complexities in a spark cluster with five virtual machines the single model parallelization results showed that spark swat yielded a speedup value of 7 84 for the most complex model but was less effective with simple models when applied to use cases with iterative model runs spark swat yielded a speedup of 6 55 24 58 depending on the model complexity these results indicate that the proposed scheme can effectively solve high computational demand problems of complex models as a subbasin level parallelization tool spark swat can be very computationally frugal and useful in use cases in which the model input changes pertain to only a few subbasins because only the changed and downstream subbasins require new computations moreover it is possible to apply this generic method to other subbasin based hydrological models to alleviate i o demands and optimize model computational performance keywords cluster computing generic method hydrological models spark swat software availability the source codes and tools developed in this research are freely available through the gnu general public license for the general public they are hosted in github and can be accessed through the following link https github com djzhang80 spark swat 1 introduction the soil and water assessment tool swat has been widely applied to investigate water availability water quality stream channel erosion plant growth climate change and watershed management options at large watershed and river basin scales abbaspour et al 2015 douglas mankin et al 2010 gassman et al 2007 tuppad et al 2011 however because the model includes a large number of immeasurable parameters and various sources of uncertainty careful model calibration and uncertainty analysis are imperative humphrey et al 2012 joseph and guillaume 2013 rouholahnejad et al 2012 zhang et al 2019 unfortunately these processes usually require many iterative model simulations and thus a prohibitively long computational time making calibration and uncertainty analysis highly labor intensive and time consuming tasks bryan 2013 hu et al 2015 liu et al 2013 2014a zhang et al 2012 zhu et al 2019 additionally the increase in the availability of length series observed hydrometeorological data and large scale high resolution topographic and land use land cover maps combined with the evolution of swat have enabled modelers to build more complex and larger spatial and temporal scale watershed models than ever before however these changes have led to an increased computational time which has further increased the difficulty of model calibration and uncertainty analysis liu et al 2016 wu et al 2014 yalew et al 2013 therefore without satisfactory tools modelers may be tempted to limit the number of model runs or explore only a limited number of model parameters and or small portions of the parameter space such limitations can undoubtedly reduce the quality of the modeling results when these inferior quality models are employed even greater costs may be incurred by making decisions based on the model outcomes that are thought to provide robust insights into the future but that in fact provide no such thing to reduce the computational burden of model simulations and build highly robust hydrologic models over the past few decades the hydrologic community has devised many methods and technologies to reduce the computational time in large scale hydrological modeling with swat gorgan et al 2012 wu and liu 2012 wu et al 2014 2016 yalew et al 2013 these endeavors include but are not limited to 1 reducing the dimensionality of the calibration and uncertainty analysis by conducting sensitivity analyses guo and su 2019 khalid et al 2016 2 utilizing optimization algorithms to find an acceptable solution s by efficiently searching each parameter space zhang et al 2009b 2013 3 using lightweight surrogate models to approximate model behavior razavi et al 2012 sun et al 2015 zhang et al 2009a 4 optimizing the swat structure to improve its computational performance ki et al 2015 rouholahnejad et al 2012 yalew et al 2013 5 combining parallel computation technologies and high performance computing hpc systems ercan et al 2014 humphrey et al 2012 sloboda and swayne 2013 zhang et al 2016 and 6 combining two or more of these technologies or algorithms ki et al 2015 rouholahnejad et al 2012 yalew et al 2013 zhang et al 2019 among these methods parallel computation is one of the most widely used for model acceleration in general the most important point that users should consider before proceeding with model parallelization is the level and or spatial scale of the parallelization process e g model or submodel level from the model structure perspective a model can be parallelized at the model and submodel levels at the former scale an integrated model is used as a parallel unit and at the latter scale the subunits of a model are parallelized although submodel level parallelization is good for maximizing the performance of a model it is relatively complex and usually requires model reconstruction as a result a steep learning curve is expected for modelers who are unfamiliar with the source codes of the swat model and parallel computation frameworks such as openmp open multi processing and the message passing interface mpi in addition submodel level parallelization is more difficult because component communications failover and task management are required model level parallelization is comparatively easy to perform for model routines that involve many simulations nevertheless model level parallelization is not applicable in single model application cases e g emergency decision making support with a single model run as this method does not reduce the execution time for a single model run from the spatial domain perspective a watershed can be partitioned into subbasins which can be further split into additional fine scale units although subbasins must be simulated from upstream to downstream it is still possible to simultaneously simulate subbasins on different routes since there are no interactions among these subbasins in addition unlike fine scale unit parallelization subbasin parallelization usually requires no model reconstruction as a subbasin can be treated as a watershed consisting of only one subbasin and upstream inputs can be treated as boundary conditions therefore subbasin parallelization can be used to accelerate model execution for either iterative or single model simulations to date many studies have been conducted to accelerate distributed hydrological model simulations through submodel level parallelization for example wu et al 2013 developed a parallel version of the swat model using the mpi in microsoft windows by parallelizing subbasin simulations on different computational cores wang et al 2013 proposed an effective parallel computing method named the temporal spatial discretization method tsdm for exploiting the parallelization degree of subbasins to the maximum extent by properly organizing the simulation sequences of dependent subbasins liu et al 2014b proposed a layered approach and implemented a grid based fully sequential dependent hydrological model fsdhm with openmp and the model can leverage the power of the multicore central processing units cpus of a computer this model was further extended with the mpi by parallelizing tasks at the subbasin level to different nodes in a computer cluster liu et al 2016 zhu et al 2019 introduced an open source modular and parallelized watershed modeling framework called the spatially explicit integrated modeling system seims to support model parallelization similar to the study of liu et al 2016 this framework utilizes both the mpi and openmp to achieve submodel parallelization and leverage the computational power of a computer cluster consisting of multiple nodes although the aforementioned solutions are effective in solving the highly demanding computational problems of distributed hydrological models they usually require tight coupling between models and parallel processing or message passing frameworks yalew et al 2013 developed generic tools and techniques to enable the swat model to run on the enabling grids for e science projects in europe egee grid by employing subbasin level parallelization inspired by the study of yalew et al 2013 this study proposes a model parallelization scheme at the subbasin level through watershed configuration file reconstruction the parallel simulation of independent subbasins and the linking of upstream subbasin outputs to downstream subbasins via point source files upstream outputs are treated as the boundary conditions of the downstream subbasin model according to this approach a parallel simulation tool for swat named spark swat was developed with an open source general purpose distributed cluster computing framework spark spark swat was then assessed with two sets of experiments to demonstrate the potential of spark swat to accelerate single and iterative model simulations in each of the test cases spark swat was evaluated with 12 synthetic hydrological models representing different i o input output burdens and river network complexities in a spark cluster with five virtual machines this study differs from the study of yalew et al 2013 in the following aspects the most distinct difference between these two studies is the distributed framework used for parallel subbasin simulations we used spark as our distributed framework which allows our tool to be run on an in house spark cluster or cloud based spark cluster such as azure hdinsight amazon web services emr or google dataproc we also inspected the model outputs of the original and split models analyzed the causes of the inconsistent results generated by these two methods and addressed part of this issue the remainder does not affect the overall accuracy 2 materials and methods 2 1 swat model swat arnold and fohrer 2005 arnold et al 1998 is a semidistributed watershed scale hydrological model that was initially developed by the agricultural research service of the united states department of agriculture to predict the impact of watershed management practices on water sediments nutrients pesticides and fecal bacterial yields in the agricultural landscapes of north america due to its distributed physically based and open access nature it has been adapted and applied to different landscapes and diverse land uses around the world according to the swat literature database https www card iastate edu swat articles approximately 4000 academic papers on swat were published in peer reviewed journals from 2001 to 2020 swat first subdivides a watershed into subbasins and further delineates hydrological response units hrus which have unique combinations of soil land cover and slope range characteristics and represent the smallest unit in swat for each subbasin the hydrological cycle simulated by swat is divided into two major phases the land phase and river routing phase in the land phase the fluxes of water sediment nutrients and pesticide loads are first calculated for each hru and then calculated by aggregating the fluxes and loads of the hrus in the subbasin for each subbasin the results are used as the input for the main channel in the subbasin the routing phase controls the movement of for example water and sediment through the main channel to the subbasin outlet 2 2 parallelization methods in swat there are no interactions between any two individual subbasins in the land phase simulation which means that the land phase processes of subbasins can be simulated simultaneously additionally the routing cycle simulation follows an upstream to downstream relationship therefore the upstream river routing process must be simulated before the downstream river routing process however it is still possible to simultaneously simulate the routing cycle of river branches since some of them are independent in addition within each subbasin there is no spatial relationship or interaction among hrus i e the sediment chemicals or nutrient loads from each hru are computed independently and thus can serve as a basis for the parallelization of swat models however an hru might be too small to serve as a unit for splitting a large model into subcomponents all hrus within a subbasin share the same weather input files which are among the largest model input files in swat especially for climate change studies which usually include century long weather datasets in the input files an hru level parallel scheme would thus require the splitting of these large input files and the reconstruction of the swat model which is computationally complex and temporally costly parallelization at the subbasin level seems to be a viable option as the subbasins in swat are neither too interdependent nor too small to serve as a computation unit swat uses a watershed configuration file fig fig that contains a series of commands to model the processes in a subbasin and to route the stream loads through the channel network of the watershed among these commands the most commonly used commands include subbasin route add save recday and finish the subbasin command is involved in the simulation of the land phase of the hydrological cycle and determines the loads routed to the main channel the route command simulates the movements and transformations that occur in the main channel the add command sums the outputs from upstream subbasins and point sources the save command stores the executed results of certain commands to a file that can later be incorporated into swat by using the recday or rechour command depending on the frequency option of the save command the recday command incorporates a daily point source and the finish command indicates the end of the routing command sequence in this study the parallelization of the swat model is achieved at the subbasin scale in particular a large scale watershed is first split into several subbasin scale models and these models are then executed in parallel whenever possible on a cluster of computers during this execution process the results of upstream subbasin models are collected and incorporated into the downstream subbasin model to simulate the transfer of water sediments and chemicals from upstream to downstream subbasins the splitting of large scale watersheds into subbasin scale models is achieved through two processes creating watershed configuration files for each subbasin model and preparing other required model input files for subbasin models fig 1 shows a small watershed consisting of five subbasins and the watershed configuration file fig 2 shows the watershed configuration files of the subbasin models for subbasin models without upstream subbasins subbasins 1 2 and 4 the watershed configuration file is constructed with the subbasin route save and finish commands and for models with upstream subbasins subbasins 3 and 5 several recday and add commands are added to incorporate and merge the simulation results from upstream subbasins the number of recday and add commands is equal to the number of upstream subbasins to simplify the process of preparing model input files for a particular subbasin model we simply duplicate several copies of the original model inputs the number of copies is the number of cpu cores plus 1 to execute a particular subbasin model we must simply replace the original watershed configuration file with a particular model after splitting the model subbasin models are split into groups according to the position level of each subbasin in the route network of the original model taking the watershed presented in fig 1 as an example subbasins 1 2 and 4 belong to the same group because they are all leaf or first level subbasins subbasin 3 is a second level subbasin and subbasin 5 is a third level subbasin within each group the models are independent and thus can be executed in parallel because high level group downstream models depend on the outputs of low level upstream models models within the high level groups must be executed after low level models for example subbasin 3 can be executed only after the models for subbasins 1 2 and 4 are executed and subbasin 5 is assessed after the model for subbasin 3 runs to completion 2 3 building blocks and spark swat implementation 2 3 1 building blocks apache spark is an open source general purpose distributed cluster computing framework that provides distributed task dispatching scheduling and basic i o functionalities through an application programming interface available in the java python scala and r languages with this interface users can easily build driver programs that invoke parallel operations such as map filter or reduce based on a resilient distributed dataset rdd by passing a function to spark spark then schedules the function execution task in parallel on the cluster in this study apache spark is used to perform parallel subbasin model simulations the network file system nfs is a distributed file system that allows users to share files across a network hence other hosts on the network can access remote storage in the same manner as they access their local storage in this study the nfs is used to exchange data among computational nodes on the computer cluster for example low level subbasin models can save their outputs in the nfs where later the high level subbasin models usually run on different computational nodes can assess these outputs the route information generator is a newly developed tool in the java programming language used to derive route information from watershed configuration files table 1 shows some route information generated by this tool for the demonstration watershed fig 1a the output of this tool is a file containing information for the source and destination subbasins and the source subbasin level the watershed configuration file generator is a newly developed java tool used to generate watershed configuration files for each subbasin model based on route information from the route information generator 2 3 2 spark swat implementation spark swat was implemented via the application programming interface api provided by spark fig 3 the first step combines the route information including the subbasin identifier its direct downstream subbasin identifier and simulating order with the simulation identifiers each simulation is assigned a distinct number via the cartesian operator provided by the spark api in particular a prefix denoting the simulation identifier is added to the identifier of each subbasin to uniquely identify a subbasin in a particular simulation this operation will generate n m combinations where n is the number of simulations and m is the number of subbasins table 2 demonstrates part of the results generated in this step for the demonstration watershed fig 1a the next step groups these combinations according to their simulating level order with the filter operation of the spark api for each of these groups map operations are used to simulate subbasin models in parallel as map operations are designed to be executed in parallel over a spark cluster we must ensure that a subbasin model is invoked by only one map operation at a time thus the first step is to find a model that is not being run this task is achieved by using static indicators using java static variables to signal whether a specified model is occupied if a free model is identified the watershed configuration file is replaced and the upstream point source files stored in the nfs are copied to the model directory finally the subbasin model is executed and the output file is generated and copied to the nfs for later use 3 case study 3 1 test environment a spark cluster consisting of five virtual machines was established to test the performance of spark swat these virtual machines are built on three physical servers by using vmware esxi version 4 1 software there are 10 and 20 physical and logical cores for each physical server respectively the clock speed random access memory ram and disk storage of each physical server are 2 2 ghz 128 gb and 5 tb respectively each of these physical servers runs vmware esxi which is an enterprise class type 1 hypervisor developed by vmware as a type 1 hypervisor esxi is not a software application that is installed on an operating system os instead it includes and integrates vital os components and is installed directly on a physical server thus esxi serves a similar function as an os for deploying and serving virtual computers each of the virtual machines built on the physical servers is allocated one quad core cpu 8 gb of ram and 50 gb of disk storage each of these virtual machines runs a 64 bit linux os detailed descriptions of these virtual and physical machines are given in table 3 3 2 test models in this study hydrological models of three watersheds harp lake fig 4 a jinjiang fig 4b and jiulong river fig 4c representing small medium and large spatial scales and river network complexities are used as a basis to construct synthetic hydrological models to evaluate the performance of spark swat the harp lake watershed with a drainage area of 5 20 km2 is located within the boreal shield ecozone in south central ontario canada the jinjiang and jiulong rivers are located in southeastern fujian province of china and have drainage areas of 5629 and 14700 km2 respectively the harp lake and jinjiang hydrological models were built in previous studies fu et al 2014 zhang et al 2015 2020 a small subbasin threshold 100 ha was adopted to generate as many subbasins as possible and thus increase the complexity of the stream rooting network when creating the jiulong river hydrological model based on the harp lake and jinjiang hydrological models built in previous studies fu et al 2014 zhang et al 2015 2020 as well as the newly created jiulong river hydrological model 12 synthetic hydrological models representing different i o burdens and river network complexities were created to evaluate how these factors affect the efficiency of spark swat the harp lake jinjiang and jiulong river hydrological models include 38 99 and 793 subbasins respectively all these models have one reservoir except for the jiulong river hydrological model summary information for these 12 synthetic models is presented in table 4 as seen in fig 5 the subbasins of the harp lake jinjiang and jiulong river hydrological models can be divided into 9 22 and 75 groups respectively according to their simulation order synthetic hydrological models based on the same hydrological model have the same number of subbasins simulation period and other configurations but different numbers of hrus in the subbasins specifically all the synthetic models built based on the jinjiang hydrological model referred to as jj5 jj50 jj100 and jj150 have 99 subbasins and are set to run for 10 years each subbasin consists of an equal number of hrus within one hydrological model the subbasins in jj5 jj50 jj100 and jj150 consist of 5 50 100 and 150 hrus respectively the synthetic models hp5 hp50 hp100 and hp150 have some configurations of the harp lake hydrological model i e with 38 subbasins and a 10 year simulation period similar to the synthetic models built based on the jinjiang hydrological model hp5 hp50 hp100 and hp150 include 5 50 100 and 150 hrus respectively all the synthetic models built based on the jiulong river hydrological model referred to as jl5 jl10 jl20 and jl25 have 793 subbasins each synthetic model based on the jiulong river hydrological model has the same simulation period as the other two sets of synthetic models nevertheless the hru numbers in each subbasin of jl5 jl10 jl20 and jl25 are different from those of the other two sets of synthetic models for jl5 jl10 jl20 and jl25 the numbers of hrus are 5 10 20 and 25 respectively this is because the maximum number of total hrus allowed in swat is 20000 3 3 experimental setup we used the aforementioned synthetic models and test environment to assess the performance of spark swat in particular we designed three sets of test experiments to demonstrate the potential of spark swat to accelerate 1 single model simulation and 2 iterative model simulations for test experiments involving only one model simulation the model was run 10 times and the average run time was used to estimate the performance of spark swat for test experiments with iterative simulations 1000 model simulations were performed in each experiment as the performance of parallel programs is mainly determined by the maximum io throughput and number of available computational cores five experiments were conducted to examine how these factors can affect the performance of spark swat for each of the aforementioned test sets these five experiments allowed different parallel tasks to be executed in the computational nodes of the test environment in particular experiment 1 allowed only one task at a time experiment 2 allowed two parallel tasks at a time and so forth in this study the performance of spark swat was evaluated based on the speedup metric houstis et al 1997 which is defined as follows 1 s p e e d u p a c t s t n j e t where n is the total number of simulations for a given job jet is the execution time of the job run in the test environment st is the average execution time of one model simulation and speedup measures how much faster a program runs in parallel than sequentially on a single computer we also calculated the theoretical speedup which considers the theoretical speedup that a model can achieve under different test configurations the theoretical speedup is calculated as follows 2 s p e e d u p r e f s u b s i 1 n c e i l l v l i n o d e s c o r e s where subs is the total number of subbasins n is the total level of watersheds i denotes the ith level lvl i is the subbasin count at the ith level nodes is the number of computational nodes cores is the number of parallel tasks performed at a computational node and ceil is a function that returns the smallest integer value that is greater than or equal to a predetermined parameter value 4 results and discussion 4 1 model result comparisons in theory the model outputs of undivided and split models should be identical however the model output comparison in this study shows that the stream flow sediment and other chemicals are not identical fig 6 fig 6a shows the simulated stream flows of the undivided and split models and errors between them for clarity only a half year of simulated stream flow was plotted as seen in this plot for most cases the simulated stream flows of these two models are identical only a small portion of the simulated stream flows show subtle differences dots with a green background the simulated errors of the split model except for water temperature are the result of precision loss when converting variables to a short format and writing them to the external file however these rounding errors do not affect the overall accuracy of the modeling results as seen in fig 6a b and c the errors are many orders of magnitude smaller than those of the simulated values fig 6d shows a simulated water temperature comparison for the undivided and split models unlike stream flow and other chemicals the water temperatures simulated by these two models exhibit large differences especially in the summer season after scrutinizing the source codes of the swat model we found that the water temperature is calculated as a function of the ratio of the hru area to the watershed area virtual f which is defined as follows 3 t c t p 5 0 0 75 t a q f r where t c is the stream water temperature at the current calculation step t p is the stream water temperature at the previous calculation step t a is the average air temperature on a given day q is the total amount of water entering the main channel from the hru on a given day and fr is the ratio of the hru area to the watershed area in the subbasin model fr increases as the watershed area decreases thus leading to an increase in the stream water temperature in the swat model the watershed area is calculated by summing the areas of all subbasins in the initiation stage to fix this problem in split models the watershed area is assigned based on a variable that is passed to swat as an argument instead of by summing the area of all subbasins as expected this amendment resulted in consistent simulated water temperatures for the undivided model and the split simulate merge approach 4 2 performance analysis to test the performance of spark swat applied to use cases with a single model run the aforementioned 12 synthetic models were executed in the test environment with one to five parallel tasks implemented at each computational node each of these five experiments was repeated 10 times for the 12 synthetic models and the actual and theoretical speedup values were calculated fig 7 shows the speedup values versus the number of cores used in each computational node in general the actual speedup values increase with increasing model complexity for the hp based jj based and jl based synthetic models which indicates that the split simulate merge scheme works especially well for complex models for example the maximum speedup values achieved for hp150 jj150 and jl25 which represent the most complex hp based jj based and jl based models are 5 04 6 65 and 7 84 respectively moreover the actual speedup values surpass the theoretical values for hp based and jj based synthetic models with 100 or 150 hrus per subbasin and for jl based synthetic models with 20 or 25 hrus per subbasin this phenomenon is mostly due to the i o saturation of these models the large numbers of hrus in the jj100 jj150 hp100 hp150 jl20 and jl25 synthetic models certainly result in a large number of simultaneous access requests for the hard disk thus creating a performance bottleneck this result can be verified by comparing the execution time of the undivided model with the total execution time of the subbasin models for example the execution time of undivided jj150 is 425 02 s and the total execution time for the subbasin and reservoir models is close to 235 s the execution time for one of the subbasin models is 2 35 s jj150 has 99 subbasins and one reservoir thus we can reasonably infer that this performance improvement is achieved by alleviating the i o bottleneck for complex models in addition the calculation of the theoretical speedup considers only the complexity of the routing network and the total number of available computational cores in the test environment i o factors are not considered these two factors explain why the actual speedup values of these complex models are better than the theoretical speedup values fig 8 shows the detailed information pertaining to the execution times of these synthetic models versus the parallel tasks performed at each computational node the job execution time for the synthetic model simulations was split into the time required by the model itself and the system time needed to perform the relevant tasks the model execution times were obtained by summing the total execution time for each subbasin model and the system execution times were calculated by subtracting the model execution times from the job execution times the system execution times encompassed job initiation task orchestration and model result transfer and were stable for the hp based jj based and jl based models with different hrus nevertheless the average system execution times for the hp based jj based and jl based models were 15 12 s 30 46 s and 123 18 s respectively indicating that these times are mainly influenced by the network complexity of a model and not the number of hrus per subbasin the model execution times dramatically declined from one to two parallel tasks stabilized at three or four parallel tasks and increased slowly for five parallel tasks per computational node the decrease in the execution time after reaching a maximum was due to the conflicting demands for disk i o and computational cores as can be inferred from the dramatic divergence of the model execution times at three or four parallel tasks to test the performance of spark swat when applied to use cases with iterative model runs 1000 model simulations were performed in experiments with one to five parallel tasks implemented at each computational node the ideal speedup red equal to the product of cores and nodes and actual speedup achieved for these experiments blue are plotted against the number of cores used in each node of the spark cluster in fig 9 for most synthetic models except for hp150 and jj150 the speedup increased gradually with up to four cores used in each node and slightly decreased at five cores while for hp150 and jj150 the speedup ceased at three cores the early performance suspension of these two models was mostly due to i o saturation in general the speedups of the complex models were much higher than those of the less complex models for example the maximum speedups for less complex models such as hp5 jj5 and jl5 were 6 55 8 25 and 8 62 respectively while the speedups for hp150 jj150 and jl25 were 20 75 24 24 and 24 58 respectively however simple models showed better scalability for example the speedups of hp5 and jj5 increased approximately linearly until the number of cores used reached the maximum available cores in each node four cores similar to the case of a single model run we also observed that actual speedup values surpassed the ideal speedup for complex synthetic models as discussed before this phenomenon is caused by additional performance gains resulting from model decomposition 4 3 possible usages and limitations the proposed scheme and tool implemented in this study can be applied for both individual and iterative model parallelization for modeling routines with iterative model simulations such as model calibration sensitivity and uncertainty analysis and beneficial management practice bmp optimization procedures modelers only need to generate route information for each simulation by adding a prefix that can individually identify a simulation to each subbasin number and aggregate the generated route information for all simulations that are used later in parallel model simulations notably users can simply view use cases with iterative model runs as a single virtual large scale watershed model in which each simulation is a small part of the virtual model moreover compared with the undivided model the split simulate merge scheme can be computationally frugal in some cases for example in some model applications e g bmp identification or subbasin scale model calibration the changes in the model input pertain only to a small portion of the model components e g a subbasin or reservoir in such cases only the changed and downstream subbasins and reservoirs require new computations during the iterative simulation process thus largely reducing the computational complexity by reusing the model results for the unchanged subbasins because the split simulate merge scheme requires extra computational time to perform task management and model result transfer it may not be suitable for lightweight models however if model input changes pertain only to a small portion of the model components our proposed method may still be suitable for an i o intensive model the split simulate merge scheme can largely alleviate the i o bottleneck thereby reducing the computational time in this case the extra time required for task management and model result transfer is negligible in addition the performance of spark swat can be affected by the watershed route network characteristics in general it is less suitable for watersheds that are thin and long than for watersheds with short route networks and many branches the applicability of a model parallelization tool is also influenced by the running environment as an open source tool implemented in java spark swat can run on major oss such linux unix and windows in addition the use of the spark framework is sufficiently widespread in the it industry and many cloud providers currently offer convenient on demand managed spark clusters e g azure hdinsight amazon web services emr and google dataproc with out of the box support for spark based applications thus users can easily adapt spark swat to run in these environments which will reduce the technical and financial burdens encountered when building an in house spark cluster 5 conclusions in this paper we proposed a scheme for swat parallelization by splitting a watershed model into multiple subbasin models and orchestrating parallel simulations according to the watershed route network we implemented a parallel computing tool for swat spark swat by using an open source general purpose distributed cluster computing framework spark according to the proposed scheme based on synthetic models spark swat was tested and evaluated with a small spark cluster consisting of five virtual machines we also discussed the issues e g how modelers can avoid inconsistent results between the original model and the proposed method and possible limitations associated with applying the proposed scheme and implementation tool our experimental results suggest that spark swat can achieve a remarkable performance improvement over traditional swat models by alleviating the i o burden in large scale watershed models and leveraging the computational power of a spark cluster nevertheless the performance gains are influenced to some extent by the complexity of the model and the route network characteristics spark swat was designed for the parallelization of single and iterative simulations on a cloud based spark cluster thus the authors believe that spark swat can be used to perform model calibration sensitivity and uncertainty analyses and bmp optimization procedures in cloud infrastructures such as azure hdinsight amazon web services emr and google dataproc users can deploy spark swat with any of these products to avoid building an in house spark cluster which will largely reduce the corresponding technical and financial requirements declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was financially supported by the natural science foundation of fujian province grant number 2020j01779 the science and technology project of xiamen grant number 3502z20183056 and the science and technology climbing program of xiamen university of technology grant number xpdkt19014 appendix a supplementary data the following are the supplementary data to this article figs1 figs1 figs2 figs2 figs3 figs3 figs4 figs4 figs5 figs5 figs6 figs6 figs7 figs7 figs8 figs8 figs9 figs9 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105133 
25747,high fidelity hydrological models are increasingly built and used to investigate the effects of management activities and climate change on water availability and quality for large areas with datasets of high spatial and temporal resolution however these advantages come at the price of greater computational demand and run time this becomes challenging when modeling routines involve iterative model simulations in this study we proposed a generic scheme to reduce the soil and water assessment tool swat runtime by decomposing a watershed model into subbasin models and optimizing the subbasin model simulations based on a parallel approach based on this scheme we implemented a generic tool named spark swat which allows subbasin models to be simulated in parallel on a spark computer cluster we then evaluated spark swat with two sets of experiments to demonstrate the potential of spark swat to accelerate single and iterative model simulations in each test set spark swat was applied to simulate 12 synthetic hydrological models in parallel with different i o input output burdens and river network complexities in a spark cluster with five virtual machines the single model parallelization results showed that spark swat yielded a speedup value of 7 84 for the most complex model but was less effective with simple models when applied to use cases with iterative model runs spark swat yielded a speedup of 6 55 24 58 depending on the model complexity these results indicate that the proposed scheme can effectively solve high computational demand problems of complex models as a subbasin level parallelization tool spark swat can be very computationally frugal and useful in use cases in which the model input changes pertain to only a few subbasins because only the changed and downstream subbasins require new computations moreover it is possible to apply this generic method to other subbasin based hydrological models to alleviate i o demands and optimize model computational performance keywords cluster computing generic method hydrological models spark swat software availability the source codes and tools developed in this research are freely available through the gnu general public license for the general public they are hosted in github and can be accessed through the following link https github com djzhang80 spark swat 1 introduction the soil and water assessment tool swat has been widely applied to investigate water availability water quality stream channel erosion plant growth climate change and watershed management options at large watershed and river basin scales abbaspour et al 2015 douglas mankin et al 2010 gassman et al 2007 tuppad et al 2011 however because the model includes a large number of immeasurable parameters and various sources of uncertainty careful model calibration and uncertainty analysis are imperative humphrey et al 2012 joseph and guillaume 2013 rouholahnejad et al 2012 zhang et al 2019 unfortunately these processes usually require many iterative model simulations and thus a prohibitively long computational time making calibration and uncertainty analysis highly labor intensive and time consuming tasks bryan 2013 hu et al 2015 liu et al 2013 2014a zhang et al 2012 zhu et al 2019 additionally the increase in the availability of length series observed hydrometeorological data and large scale high resolution topographic and land use land cover maps combined with the evolution of swat have enabled modelers to build more complex and larger spatial and temporal scale watershed models than ever before however these changes have led to an increased computational time which has further increased the difficulty of model calibration and uncertainty analysis liu et al 2016 wu et al 2014 yalew et al 2013 therefore without satisfactory tools modelers may be tempted to limit the number of model runs or explore only a limited number of model parameters and or small portions of the parameter space such limitations can undoubtedly reduce the quality of the modeling results when these inferior quality models are employed even greater costs may be incurred by making decisions based on the model outcomes that are thought to provide robust insights into the future but that in fact provide no such thing to reduce the computational burden of model simulations and build highly robust hydrologic models over the past few decades the hydrologic community has devised many methods and technologies to reduce the computational time in large scale hydrological modeling with swat gorgan et al 2012 wu and liu 2012 wu et al 2014 2016 yalew et al 2013 these endeavors include but are not limited to 1 reducing the dimensionality of the calibration and uncertainty analysis by conducting sensitivity analyses guo and su 2019 khalid et al 2016 2 utilizing optimization algorithms to find an acceptable solution s by efficiently searching each parameter space zhang et al 2009b 2013 3 using lightweight surrogate models to approximate model behavior razavi et al 2012 sun et al 2015 zhang et al 2009a 4 optimizing the swat structure to improve its computational performance ki et al 2015 rouholahnejad et al 2012 yalew et al 2013 5 combining parallel computation technologies and high performance computing hpc systems ercan et al 2014 humphrey et al 2012 sloboda and swayne 2013 zhang et al 2016 and 6 combining two or more of these technologies or algorithms ki et al 2015 rouholahnejad et al 2012 yalew et al 2013 zhang et al 2019 among these methods parallel computation is one of the most widely used for model acceleration in general the most important point that users should consider before proceeding with model parallelization is the level and or spatial scale of the parallelization process e g model or submodel level from the model structure perspective a model can be parallelized at the model and submodel levels at the former scale an integrated model is used as a parallel unit and at the latter scale the subunits of a model are parallelized although submodel level parallelization is good for maximizing the performance of a model it is relatively complex and usually requires model reconstruction as a result a steep learning curve is expected for modelers who are unfamiliar with the source codes of the swat model and parallel computation frameworks such as openmp open multi processing and the message passing interface mpi in addition submodel level parallelization is more difficult because component communications failover and task management are required model level parallelization is comparatively easy to perform for model routines that involve many simulations nevertheless model level parallelization is not applicable in single model application cases e g emergency decision making support with a single model run as this method does not reduce the execution time for a single model run from the spatial domain perspective a watershed can be partitioned into subbasins which can be further split into additional fine scale units although subbasins must be simulated from upstream to downstream it is still possible to simultaneously simulate subbasins on different routes since there are no interactions among these subbasins in addition unlike fine scale unit parallelization subbasin parallelization usually requires no model reconstruction as a subbasin can be treated as a watershed consisting of only one subbasin and upstream inputs can be treated as boundary conditions therefore subbasin parallelization can be used to accelerate model execution for either iterative or single model simulations to date many studies have been conducted to accelerate distributed hydrological model simulations through submodel level parallelization for example wu et al 2013 developed a parallel version of the swat model using the mpi in microsoft windows by parallelizing subbasin simulations on different computational cores wang et al 2013 proposed an effective parallel computing method named the temporal spatial discretization method tsdm for exploiting the parallelization degree of subbasins to the maximum extent by properly organizing the simulation sequences of dependent subbasins liu et al 2014b proposed a layered approach and implemented a grid based fully sequential dependent hydrological model fsdhm with openmp and the model can leverage the power of the multicore central processing units cpus of a computer this model was further extended with the mpi by parallelizing tasks at the subbasin level to different nodes in a computer cluster liu et al 2016 zhu et al 2019 introduced an open source modular and parallelized watershed modeling framework called the spatially explicit integrated modeling system seims to support model parallelization similar to the study of liu et al 2016 this framework utilizes both the mpi and openmp to achieve submodel parallelization and leverage the computational power of a computer cluster consisting of multiple nodes although the aforementioned solutions are effective in solving the highly demanding computational problems of distributed hydrological models they usually require tight coupling between models and parallel processing or message passing frameworks yalew et al 2013 developed generic tools and techniques to enable the swat model to run on the enabling grids for e science projects in europe egee grid by employing subbasin level parallelization inspired by the study of yalew et al 2013 this study proposes a model parallelization scheme at the subbasin level through watershed configuration file reconstruction the parallel simulation of independent subbasins and the linking of upstream subbasin outputs to downstream subbasins via point source files upstream outputs are treated as the boundary conditions of the downstream subbasin model according to this approach a parallel simulation tool for swat named spark swat was developed with an open source general purpose distributed cluster computing framework spark spark swat was then assessed with two sets of experiments to demonstrate the potential of spark swat to accelerate single and iterative model simulations in each of the test cases spark swat was evaluated with 12 synthetic hydrological models representing different i o input output burdens and river network complexities in a spark cluster with five virtual machines this study differs from the study of yalew et al 2013 in the following aspects the most distinct difference between these two studies is the distributed framework used for parallel subbasin simulations we used spark as our distributed framework which allows our tool to be run on an in house spark cluster or cloud based spark cluster such as azure hdinsight amazon web services emr or google dataproc we also inspected the model outputs of the original and split models analyzed the causes of the inconsistent results generated by these two methods and addressed part of this issue the remainder does not affect the overall accuracy 2 materials and methods 2 1 swat model swat arnold and fohrer 2005 arnold et al 1998 is a semidistributed watershed scale hydrological model that was initially developed by the agricultural research service of the united states department of agriculture to predict the impact of watershed management practices on water sediments nutrients pesticides and fecal bacterial yields in the agricultural landscapes of north america due to its distributed physically based and open access nature it has been adapted and applied to different landscapes and diverse land uses around the world according to the swat literature database https www card iastate edu swat articles approximately 4000 academic papers on swat were published in peer reviewed journals from 2001 to 2020 swat first subdivides a watershed into subbasins and further delineates hydrological response units hrus which have unique combinations of soil land cover and slope range characteristics and represent the smallest unit in swat for each subbasin the hydrological cycle simulated by swat is divided into two major phases the land phase and river routing phase in the land phase the fluxes of water sediment nutrients and pesticide loads are first calculated for each hru and then calculated by aggregating the fluxes and loads of the hrus in the subbasin for each subbasin the results are used as the input for the main channel in the subbasin the routing phase controls the movement of for example water and sediment through the main channel to the subbasin outlet 2 2 parallelization methods in swat there are no interactions between any two individual subbasins in the land phase simulation which means that the land phase processes of subbasins can be simulated simultaneously additionally the routing cycle simulation follows an upstream to downstream relationship therefore the upstream river routing process must be simulated before the downstream river routing process however it is still possible to simultaneously simulate the routing cycle of river branches since some of them are independent in addition within each subbasin there is no spatial relationship or interaction among hrus i e the sediment chemicals or nutrient loads from each hru are computed independently and thus can serve as a basis for the parallelization of swat models however an hru might be too small to serve as a unit for splitting a large model into subcomponents all hrus within a subbasin share the same weather input files which are among the largest model input files in swat especially for climate change studies which usually include century long weather datasets in the input files an hru level parallel scheme would thus require the splitting of these large input files and the reconstruction of the swat model which is computationally complex and temporally costly parallelization at the subbasin level seems to be a viable option as the subbasins in swat are neither too interdependent nor too small to serve as a computation unit swat uses a watershed configuration file fig fig that contains a series of commands to model the processes in a subbasin and to route the stream loads through the channel network of the watershed among these commands the most commonly used commands include subbasin route add save recday and finish the subbasin command is involved in the simulation of the land phase of the hydrological cycle and determines the loads routed to the main channel the route command simulates the movements and transformations that occur in the main channel the add command sums the outputs from upstream subbasins and point sources the save command stores the executed results of certain commands to a file that can later be incorporated into swat by using the recday or rechour command depending on the frequency option of the save command the recday command incorporates a daily point source and the finish command indicates the end of the routing command sequence in this study the parallelization of the swat model is achieved at the subbasin scale in particular a large scale watershed is first split into several subbasin scale models and these models are then executed in parallel whenever possible on a cluster of computers during this execution process the results of upstream subbasin models are collected and incorporated into the downstream subbasin model to simulate the transfer of water sediments and chemicals from upstream to downstream subbasins the splitting of large scale watersheds into subbasin scale models is achieved through two processes creating watershed configuration files for each subbasin model and preparing other required model input files for subbasin models fig 1 shows a small watershed consisting of five subbasins and the watershed configuration file fig 2 shows the watershed configuration files of the subbasin models for subbasin models without upstream subbasins subbasins 1 2 and 4 the watershed configuration file is constructed with the subbasin route save and finish commands and for models with upstream subbasins subbasins 3 and 5 several recday and add commands are added to incorporate and merge the simulation results from upstream subbasins the number of recday and add commands is equal to the number of upstream subbasins to simplify the process of preparing model input files for a particular subbasin model we simply duplicate several copies of the original model inputs the number of copies is the number of cpu cores plus 1 to execute a particular subbasin model we must simply replace the original watershed configuration file with a particular model after splitting the model subbasin models are split into groups according to the position level of each subbasin in the route network of the original model taking the watershed presented in fig 1 as an example subbasins 1 2 and 4 belong to the same group because they are all leaf or first level subbasins subbasin 3 is a second level subbasin and subbasin 5 is a third level subbasin within each group the models are independent and thus can be executed in parallel because high level group downstream models depend on the outputs of low level upstream models models within the high level groups must be executed after low level models for example subbasin 3 can be executed only after the models for subbasins 1 2 and 4 are executed and subbasin 5 is assessed after the model for subbasin 3 runs to completion 2 3 building blocks and spark swat implementation 2 3 1 building blocks apache spark is an open source general purpose distributed cluster computing framework that provides distributed task dispatching scheduling and basic i o functionalities through an application programming interface available in the java python scala and r languages with this interface users can easily build driver programs that invoke parallel operations such as map filter or reduce based on a resilient distributed dataset rdd by passing a function to spark spark then schedules the function execution task in parallel on the cluster in this study apache spark is used to perform parallel subbasin model simulations the network file system nfs is a distributed file system that allows users to share files across a network hence other hosts on the network can access remote storage in the same manner as they access their local storage in this study the nfs is used to exchange data among computational nodes on the computer cluster for example low level subbasin models can save their outputs in the nfs where later the high level subbasin models usually run on different computational nodes can assess these outputs the route information generator is a newly developed tool in the java programming language used to derive route information from watershed configuration files table 1 shows some route information generated by this tool for the demonstration watershed fig 1a the output of this tool is a file containing information for the source and destination subbasins and the source subbasin level the watershed configuration file generator is a newly developed java tool used to generate watershed configuration files for each subbasin model based on route information from the route information generator 2 3 2 spark swat implementation spark swat was implemented via the application programming interface api provided by spark fig 3 the first step combines the route information including the subbasin identifier its direct downstream subbasin identifier and simulating order with the simulation identifiers each simulation is assigned a distinct number via the cartesian operator provided by the spark api in particular a prefix denoting the simulation identifier is added to the identifier of each subbasin to uniquely identify a subbasin in a particular simulation this operation will generate n m combinations where n is the number of simulations and m is the number of subbasins table 2 demonstrates part of the results generated in this step for the demonstration watershed fig 1a the next step groups these combinations according to their simulating level order with the filter operation of the spark api for each of these groups map operations are used to simulate subbasin models in parallel as map operations are designed to be executed in parallel over a spark cluster we must ensure that a subbasin model is invoked by only one map operation at a time thus the first step is to find a model that is not being run this task is achieved by using static indicators using java static variables to signal whether a specified model is occupied if a free model is identified the watershed configuration file is replaced and the upstream point source files stored in the nfs are copied to the model directory finally the subbasin model is executed and the output file is generated and copied to the nfs for later use 3 case study 3 1 test environment a spark cluster consisting of five virtual machines was established to test the performance of spark swat these virtual machines are built on three physical servers by using vmware esxi version 4 1 software there are 10 and 20 physical and logical cores for each physical server respectively the clock speed random access memory ram and disk storage of each physical server are 2 2 ghz 128 gb and 5 tb respectively each of these physical servers runs vmware esxi which is an enterprise class type 1 hypervisor developed by vmware as a type 1 hypervisor esxi is not a software application that is installed on an operating system os instead it includes and integrates vital os components and is installed directly on a physical server thus esxi serves a similar function as an os for deploying and serving virtual computers each of the virtual machines built on the physical servers is allocated one quad core cpu 8 gb of ram and 50 gb of disk storage each of these virtual machines runs a 64 bit linux os detailed descriptions of these virtual and physical machines are given in table 3 3 2 test models in this study hydrological models of three watersheds harp lake fig 4 a jinjiang fig 4b and jiulong river fig 4c representing small medium and large spatial scales and river network complexities are used as a basis to construct synthetic hydrological models to evaluate the performance of spark swat the harp lake watershed with a drainage area of 5 20 km2 is located within the boreal shield ecozone in south central ontario canada the jinjiang and jiulong rivers are located in southeastern fujian province of china and have drainage areas of 5629 and 14700 km2 respectively the harp lake and jinjiang hydrological models were built in previous studies fu et al 2014 zhang et al 2015 2020 a small subbasin threshold 100 ha was adopted to generate as many subbasins as possible and thus increase the complexity of the stream rooting network when creating the jiulong river hydrological model based on the harp lake and jinjiang hydrological models built in previous studies fu et al 2014 zhang et al 2015 2020 as well as the newly created jiulong river hydrological model 12 synthetic hydrological models representing different i o burdens and river network complexities were created to evaluate how these factors affect the efficiency of spark swat the harp lake jinjiang and jiulong river hydrological models include 38 99 and 793 subbasins respectively all these models have one reservoir except for the jiulong river hydrological model summary information for these 12 synthetic models is presented in table 4 as seen in fig 5 the subbasins of the harp lake jinjiang and jiulong river hydrological models can be divided into 9 22 and 75 groups respectively according to their simulation order synthetic hydrological models based on the same hydrological model have the same number of subbasins simulation period and other configurations but different numbers of hrus in the subbasins specifically all the synthetic models built based on the jinjiang hydrological model referred to as jj5 jj50 jj100 and jj150 have 99 subbasins and are set to run for 10 years each subbasin consists of an equal number of hrus within one hydrological model the subbasins in jj5 jj50 jj100 and jj150 consist of 5 50 100 and 150 hrus respectively the synthetic models hp5 hp50 hp100 and hp150 have some configurations of the harp lake hydrological model i e with 38 subbasins and a 10 year simulation period similar to the synthetic models built based on the jinjiang hydrological model hp5 hp50 hp100 and hp150 include 5 50 100 and 150 hrus respectively all the synthetic models built based on the jiulong river hydrological model referred to as jl5 jl10 jl20 and jl25 have 793 subbasins each synthetic model based on the jiulong river hydrological model has the same simulation period as the other two sets of synthetic models nevertheless the hru numbers in each subbasin of jl5 jl10 jl20 and jl25 are different from those of the other two sets of synthetic models for jl5 jl10 jl20 and jl25 the numbers of hrus are 5 10 20 and 25 respectively this is because the maximum number of total hrus allowed in swat is 20000 3 3 experimental setup we used the aforementioned synthetic models and test environment to assess the performance of spark swat in particular we designed three sets of test experiments to demonstrate the potential of spark swat to accelerate 1 single model simulation and 2 iterative model simulations for test experiments involving only one model simulation the model was run 10 times and the average run time was used to estimate the performance of spark swat for test experiments with iterative simulations 1000 model simulations were performed in each experiment as the performance of parallel programs is mainly determined by the maximum io throughput and number of available computational cores five experiments were conducted to examine how these factors can affect the performance of spark swat for each of the aforementioned test sets these five experiments allowed different parallel tasks to be executed in the computational nodes of the test environment in particular experiment 1 allowed only one task at a time experiment 2 allowed two parallel tasks at a time and so forth in this study the performance of spark swat was evaluated based on the speedup metric houstis et al 1997 which is defined as follows 1 s p e e d u p a c t s t n j e t where n is the total number of simulations for a given job jet is the execution time of the job run in the test environment st is the average execution time of one model simulation and speedup measures how much faster a program runs in parallel than sequentially on a single computer we also calculated the theoretical speedup which considers the theoretical speedup that a model can achieve under different test configurations the theoretical speedup is calculated as follows 2 s p e e d u p r e f s u b s i 1 n c e i l l v l i n o d e s c o r e s where subs is the total number of subbasins n is the total level of watersheds i denotes the ith level lvl i is the subbasin count at the ith level nodes is the number of computational nodes cores is the number of parallel tasks performed at a computational node and ceil is a function that returns the smallest integer value that is greater than or equal to a predetermined parameter value 4 results and discussion 4 1 model result comparisons in theory the model outputs of undivided and split models should be identical however the model output comparison in this study shows that the stream flow sediment and other chemicals are not identical fig 6 fig 6a shows the simulated stream flows of the undivided and split models and errors between them for clarity only a half year of simulated stream flow was plotted as seen in this plot for most cases the simulated stream flows of these two models are identical only a small portion of the simulated stream flows show subtle differences dots with a green background the simulated errors of the split model except for water temperature are the result of precision loss when converting variables to a short format and writing them to the external file however these rounding errors do not affect the overall accuracy of the modeling results as seen in fig 6a b and c the errors are many orders of magnitude smaller than those of the simulated values fig 6d shows a simulated water temperature comparison for the undivided and split models unlike stream flow and other chemicals the water temperatures simulated by these two models exhibit large differences especially in the summer season after scrutinizing the source codes of the swat model we found that the water temperature is calculated as a function of the ratio of the hru area to the watershed area virtual f which is defined as follows 3 t c t p 5 0 0 75 t a q f r where t c is the stream water temperature at the current calculation step t p is the stream water temperature at the previous calculation step t a is the average air temperature on a given day q is the total amount of water entering the main channel from the hru on a given day and fr is the ratio of the hru area to the watershed area in the subbasin model fr increases as the watershed area decreases thus leading to an increase in the stream water temperature in the swat model the watershed area is calculated by summing the areas of all subbasins in the initiation stage to fix this problem in split models the watershed area is assigned based on a variable that is passed to swat as an argument instead of by summing the area of all subbasins as expected this amendment resulted in consistent simulated water temperatures for the undivided model and the split simulate merge approach 4 2 performance analysis to test the performance of spark swat applied to use cases with a single model run the aforementioned 12 synthetic models were executed in the test environment with one to five parallel tasks implemented at each computational node each of these five experiments was repeated 10 times for the 12 synthetic models and the actual and theoretical speedup values were calculated fig 7 shows the speedup values versus the number of cores used in each computational node in general the actual speedup values increase with increasing model complexity for the hp based jj based and jl based synthetic models which indicates that the split simulate merge scheme works especially well for complex models for example the maximum speedup values achieved for hp150 jj150 and jl25 which represent the most complex hp based jj based and jl based models are 5 04 6 65 and 7 84 respectively moreover the actual speedup values surpass the theoretical values for hp based and jj based synthetic models with 100 or 150 hrus per subbasin and for jl based synthetic models with 20 or 25 hrus per subbasin this phenomenon is mostly due to the i o saturation of these models the large numbers of hrus in the jj100 jj150 hp100 hp150 jl20 and jl25 synthetic models certainly result in a large number of simultaneous access requests for the hard disk thus creating a performance bottleneck this result can be verified by comparing the execution time of the undivided model with the total execution time of the subbasin models for example the execution time of undivided jj150 is 425 02 s and the total execution time for the subbasin and reservoir models is close to 235 s the execution time for one of the subbasin models is 2 35 s jj150 has 99 subbasins and one reservoir thus we can reasonably infer that this performance improvement is achieved by alleviating the i o bottleneck for complex models in addition the calculation of the theoretical speedup considers only the complexity of the routing network and the total number of available computational cores in the test environment i o factors are not considered these two factors explain why the actual speedup values of these complex models are better than the theoretical speedup values fig 8 shows the detailed information pertaining to the execution times of these synthetic models versus the parallel tasks performed at each computational node the job execution time for the synthetic model simulations was split into the time required by the model itself and the system time needed to perform the relevant tasks the model execution times were obtained by summing the total execution time for each subbasin model and the system execution times were calculated by subtracting the model execution times from the job execution times the system execution times encompassed job initiation task orchestration and model result transfer and were stable for the hp based jj based and jl based models with different hrus nevertheless the average system execution times for the hp based jj based and jl based models were 15 12 s 30 46 s and 123 18 s respectively indicating that these times are mainly influenced by the network complexity of a model and not the number of hrus per subbasin the model execution times dramatically declined from one to two parallel tasks stabilized at three or four parallel tasks and increased slowly for five parallel tasks per computational node the decrease in the execution time after reaching a maximum was due to the conflicting demands for disk i o and computational cores as can be inferred from the dramatic divergence of the model execution times at three or four parallel tasks to test the performance of spark swat when applied to use cases with iterative model runs 1000 model simulations were performed in experiments with one to five parallel tasks implemented at each computational node the ideal speedup red equal to the product of cores and nodes and actual speedup achieved for these experiments blue are plotted against the number of cores used in each node of the spark cluster in fig 9 for most synthetic models except for hp150 and jj150 the speedup increased gradually with up to four cores used in each node and slightly decreased at five cores while for hp150 and jj150 the speedup ceased at three cores the early performance suspension of these two models was mostly due to i o saturation in general the speedups of the complex models were much higher than those of the less complex models for example the maximum speedups for less complex models such as hp5 jj5 and jl5 were 6 55 8 25 and 8 62 respectively while the speedups for hp150 jj150 and jl25 were 20 75 24 24 and 24 58 respectively however simple models showed better scalability for example the speedups of hp5 and jj5 increased approximately linearly until the number of cores used reached the maximum available cores in each node four cores similar to the case of a single model run we also observed that actual speedup values surpassed the ideal speedup for complex synthetic models as discussed before this phenomenon is caused by additional performance gains resulting from model decomposition 4 3 possible usages and limitations the proposed scheme and tool implemented in this study can be applied for both individual and iterative model parallelization for modeling routines with iterative model simulations such as model calibration sensitivity and uncertainty analysis and beneficial management practice bmp optimization procedures modelers only need to generate route information for each simulation by adding a prefix that can individually identify a simulation to each subbasin number and aggregate the generated route information for all simulations that are used later in parallel model simulations notably users can simply view use cases with iterative model runs as a single virtual large scale watershed model in which each simulation is a small part of the virtual model moreover compared with the undivided model the split simulate merge scheme can be computationally frugal in some cases for example in some model applications e g bmp identification or subbasin scale model calibration the changes in the model input pertain only to a small portion of the model components e g a subbasin or reservoir in such cases only the changed and downstream subbasins and reservoirs require new computations during the iterative simulation process thus largely reducing the computational complexity by reusing the model results for the unchanged subbasins because the split simulate merge scheme requires extra computational time to perform task management and model result transfer it may not be suitable for lightweight models however if model input changes pertain only to a small portion of the model components our proposed method may still be suitable for an i o intensive model the split simulate merge scheme can largely alleviate the i o bottleneck thereby reducing the computational time in this case the extra time required for task management and model result transfer is negligible in addition the performance of spark swat can be affected by the watershed route network characteristics in general it is less suitable for watersheds that are thin and long than for watersheds with short route networks and many branches the applicability of a model parallelization tool is also influenced by the running environment as an open source tool implemented in java spark swat can run on major oss such linux unix and windows in addition the use of the spark framework is sufficiently widespread in the it industry and many cloud providers currently offer convenient on demand managed spark clusters e g azure hdinsight amazon web services emr and google dataproc with out of the box support for spark based applications thus users can easily adapt spark swat to run in these environments which will reduce the technical and financial burdens encountered when building an in house spark cluster 5 conclusions in this paper we proposed a scheme for swat parallelization by splitting a watershed model into multiple subbasin models and orchestrating parallel simulations according to the watershed route network we implemented a parallel computing tool for swat spark swat by using an open source general purpose distributed cluster computing framework spark according to the proposed scheme based on synthetic models spark swat was tested and evaluated with a small spark cluster consisting of five virtual machines we also discussed the issues e g how modelers can avoid inconsistent results between the original model and the proposed method and possible limitations associated with applying the proposed scheme and implementation tool our experimental results suggest that spark swat can achieve a remarkable performance improvement over traditional swat models by alleviating the i o burden in large scale watershed models and leveraging the computational power of a spark cluster nevertheless the performance gains are influenced to some extent by the complexity of the model and the route network characteristics spark swat was designed for the parallelization of single and iterative simulations on a cloud based spark cluster thus the authors believe that spark swat can be used to perform model calibration sensitivity and uncertainty analyses and bmp optimization procedures in cloud infrastructures such as azure hdinsight amazon web services emr and google dataproc users can deploy spark swat with any of these products to avoid building an in house spark cluster which will largely reduce the corresponding technical and financial requirements declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was financially supported by the natural science foundation of fujian province grant number 2020j01779 the science and technology project of xiamen grant number 3502z20183056 and the science and technology climbing program of xiamen university of technology grant number xpdkt19014 appendix a supplementary data the following are the supplementary data to this article figs1 figs1 figs2 figs2 figs3 figs3 figs4 figs4 figs5 figs5 figs6 figs6 figs7 figs7 figs8 figs8 figs9 figs9 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105133 
25748,models that predict wildfire rate of spread ros play an important role in decision making during firefighting operations including fire crew placement and timing of community evacuations here we use a large set of remotely sensed wildfire observations and explanatory data focusing on weather to demonstrate a bayesian probabilistic ros modelling approach our approach has two major advantages 1 using actual wildfire observations instead of controlled fire observations makes models developed well suited to wildfire prediction 2 bayesian modelling accounts for the complex nature of wildfire spread by explicitly considering uncertainty in the data to produce probabilistic ros predictions we show that highly informative probabilistic predictions can be made from a simple bayesian model containing wind speed relative humidity and soil moisture we provide current operational context to our work by calculating predictions from widely used deterministic ros models in australia keywords wildfire bushfire fire behaviour bayesian bayesian modelling rate of spread software image analysis and gis geographic information system mapping for this research were carried out in arcgis pro 2 0 analysis and sampling of spatial data was carried out using r 3 6 r core team 2020 and the associated packages sf pebesma 2018 spatstat baddeley et al 2015 raster hijmans 2020 dplyr wickham et al 2020 and lubridate grolemund and wickham 2011 bayesian models were fitted using jags 4 3 plummer 2017 which was run via the runjags package denwood 2016 in r 3 6 r code used for model fitting and prediction is available on github github com mstorey87 bayesianros and the article supplementary material section 1 introduction rate of spread ros models are used by fire behaviour specialists to predict how fast a wildfire will spread ros predictions are a critical part of the information used along with field observations weather forecasts etc to make operational decisions around what firefighting strategies to use e g offensive or defensive simpson et al 2019 where to concentrate firefighting efforts e g near a town that could be overrun within an hour and when community warnings and evacuation alerts should be sent aemc and aemc 2009 alexander and cruz 2013 anderson berry et al 2018 poor predictions of ros can have potentially disastrous consequences for example a large under prediction may result in fire reaching a community before an evacuation is complete ros models are also integral to methods of wildfire risk assessment and planning collins et al 2015 delwp 2015 price and bedward 2020 wildfire spread is an inherently stochastic process that responds to complex multi scale interactions between numerous environmental factors and the fire itself e g spotting pyroconvection fire atmosphere interaction even minute by minute ros measurements on small fires can vary substantially despite broadly similar environmental conditions burrows et al 1991 cruz and alexander 2013 in wildfires underlying conditions including fuel amount type and structure wind speed and direction slope steepness and aspect can vary substantially across a single large flaming area and as the flames move forward leading to surges and lulls in ros both over time and on different sections of a fire dynamic fire behaviours are also associated with highly erratic fire spread for example turbulent winds and mass spotting can create complex spread patterns as spot fires and the main fire area interact and coalesce filkov et al 2020 deterministic empirical or quasi empirical models are most commonly used for wildfire spread predictions during firefighting operations sullivan 2009 these models can quickly calculate a ros prediction from a small set of environmental input variables such as wind speed and fuel amount cheney et al 2012 noble et al 1980 rothermel 1972 sneeuwjagt and peet 1985 such models as routinely applied produce a single ros value but no statistical representation of expected prediction precision or uncertainty deterministic ros model accuracy can vary substantially including a bias towards under prediction in some models cruz and alexander 2013 limitations during model development incorrect application of a model and inaccurate model input data can all affect predictive accuracy alexander and cruz 2013 gill and zylstra 2005 sullivan and knight 2001 for example an underprediction of ros may occur if actual wind speed on the fire ground is faster than forecast however a more fundamental issue is that given the complex and stochastic nature of wildfire spread especially extreme fires it is unrealistic to expect that consistently accurate predictions will be produced by models that define a unique wildfire ros from a small specific set of environmental conditions predicting ros for example based on mean forecast weather and mean fuel conditions does not account for potential variation in ros and prediction uncertainty stemming from variation in underlying conditions e g mixed fuels wind gusts and factors such as wind topography interaction spotting and fire atmosphere interaction it can be unclear to a model user how underlying variation should be accounted for when making predictions for example the mcarthur mk5 model mcarthur 1973 noble et al 1980 which is widely used in australia requires a fuel load input but it is unclear how to select a single representative fuel load value to best predict ros for a forest with fine scale variation in fuel type and or disturbance history one of the most significant limitations in developing empirical ros models for wildfires has been a lack of suitable model training data empirical ros models have usually been developed from observations of mild to moderate controlled fires at relatively small scales under a constrained set of weather conditions cruz and alexander 2013 sullivan 2009 perhaps including some observations from wildfires e g case studies cheney et al 2012 mcarthur 1967 rawson et al 1983 however wildfires burn under weather conditions and at spatial scales that may span orders of magnitude beyond the conditions used to derive empirical fire spread models making predictions for wildfires therefore requires substantial extrapolation beyond the scale environmental conditions and ros observed during model development a model developed on controlled fires may be inaccurate if used to predict ros for an extreme fire sharples et al 2016 this is problematic as extreme fires are the most dangerous and difficult for firefighters to control so reliable predictions are vital there are two major ways that the approach to ros modelling could be improved firstly using wildfire spread data for model development remotely sensed landscape scale fire spread data is available e g line scans from airborne sensors storey et al 2020a storey et al 2020b which provides the opportunity to use such information to derive and test empirical models that better reflect the behaviour and spatial scale of actual wildfires and also encompass a much broader range of weather conditions and other environmental influences secondly the task of making interpreting and communicating ros predictions would greatly benefit from a statistical approach that acknowledges the stochastic nature of wildfire spread through statistical estimates of prediction uncertainty or precision bayesian statistical methods are ideally suited to this task because they deal explicitly with uncertainty can accommodate prior knowledge or theory and allow complex effects to be represented bayesian inference is a probabilistic technique that produces predictions in the form of a distribution of plausible values given the data and model structure this could be queried to determine for a given set of environmental conditions what is the most likely ros what is the most likely range of values of ros for example what range of ros has a 90 chance of occurring what is the probability that ros will be greater than some value or less than some value or lie within a given range of values e g between 1 km h 1 and 2 km h 1 a major advantage of bayesian statistical methods is that variability in model inputs and uncertainty in the fitted values of model parameters is carried through to model predictions meaning uncertainty associated with a ros prediction is readily apparent to a model user and could be taken into account in decisions made during firefighting operations in this paper we use a large set of remotely sensed wildfire observations spanning a wide range of weather conditions to 1 demonstrate how a bayesian approach to ros modelling can take into account variability in model inputs and the precision with which parameters can be estimated 2 run a model selection process and report a best performing bayesian model from a limited set of alternative models that contain surface weather variables typically used for empirical ros prediction 3 provide current operational context to our work by calculating predictions and performance statistics for two deterministic ros models used operationally in australia we present a preliminary bayesian ros model here to demonstrate and communicate the advantages of a bayesian approach to ros modelling particularly when coupled with wildfire ros observations we discuss the results and behaviour of the bayesian model with the context of current deterministic model results and discuss its advantages and further steps required to develop a bayesian ros model that could be deployed operationally 2 methods 2 1 rate of spread data we created a set of 223 wildfire ros observations from pairs of gis polygons covering active wildfires from southern australia fig 1 each pair represents the same wildfire captured at different times fire polygons were manually digitized in arcgis pro 2 0 with one person ms loading in the remotely sensed fire images visually identifying all actively burning and extinguished fire perimeter before drawing a polygon to cover the fire area fires were identified in 1 aircraft acquired line scan images 87 5 of observations used and 2 satellite acquired images modis and himawari 8 10 and 2 5 of observations respectively line scans were provided by the victorian department of environment land water and planning delwp and nsw rural fire service rfs line scans are georectified images of a fire and surrounding land generally at 5 m 15 m resolution and were acquired by aircraft fitted with multispectral or infrared ir line scanners cook et al 2009 matthews 1997 the ir band s allow for smoke to be mostly removed for visual interpretation in a gis allowing for visual discrimination between burning and non burning areas see storey et al 2020b for more detailed description we used all line scans from the datasets provided where we could identify at least two images of an actively burning fire within 5 h modis images were pre processed modis corrected reflectance images using band combination 7 short wave ir 2 near ir and 1 green red and were downloaded directly from the nasa worldview online application nasa 2020 images were identified in worldview for selected dates where a we had a line scan of a very large fire but no prior or subsequent line scan and b dates where large fires occurred but no line scans were available the 7 2 1 modis band combination provides a similar product to the line scans i e smoke removed for visual interpretation but at coarser spatial resolution 250 m for bands 1 and 2 500 m for band 7 due to the coarser resolution only large fires could be digitized using modis images modis images are captured only twice per day with one late morning and one early afternoon generally with a 3 4 h interval we included a small set of coarse fire polygons based on himawari 8 bessho et al 2016 geostationary satellite band 7 images for a group of large fires near esperance in western australia 17 nov 2015 for which line scans and modis images were not available in comparison to modis himawari 8 has high temporal resolution 10 min between captures but a coarse spatial resolution of 2 km for band 7 we made a single ros measurement from selected pairs of images 3 h apart for each of these fires inter image intervals ranged between 20 min and 5 h we excluded observations where fires were subject to a significant mid interval wind change and where fire spread was obviously impeded for example where the fire was extinguished in image two or ran into a previously burnt area some limitations in using line scan data should be noted when interpreting the results in particular fire behaviour between the two image acquisitions is unknown thus it was not possible to determine if all fires were completely free burning for the entire interval or the degree in variation in fire spread between images for example a fire run may have been slowed by suppression when burning down a slope or by a fuel break e g a road or river fire spread may have surged or lulled at different times between two images in response to gusting winds and spotting or extreme fire behaviours in some images spotting clearly allowed fire spread to continue there were five spread observations where the main fire consisted of more than one polygon as it appeared the fire had spotted over a damp valley or road from which a new head fire formed without the two fire sections polygons merging into one while these limitations produce some uncertainty in ros using real wildfire observations is important in order to produce a model that accounts for such variation in landscape scale wildfire spread to measure ros for each pair of fire polygons we created a single gis line representing the line of fastest spread the process was to 1 create a convex hull around each polygon which assisted in automation of the ros measurement 2 measure the straight line distance from the head of the fire at time one i e polygon one to the head of the fire at time two through an automated process see fig 2 for details 3 calculate ros km h 1 as spread line distance over total spread time with spread time being the difference between acquisition times of the polygon source images the ros determined is therefore the average ros over the time between the two images 2 2 weather data weather data for all ros lines was extracted from the bureau of meteorology atmospheric high resolution regional reanalysis for australia barra barra uses a combination of atmospheric model forecasts based on the australian community climate and earth simulator system access framework and observations from a range of sources including satellite and ground based sensors to produce a four dimensional latitude longitude atmospheric pressure level time i e hourly gridded reanalysis product su et al 2019 each ros observation was associated with a set of hourly weather values for our analysis which were extracted from the barra r 12 km spatial resolution grids we extracted several fire influencing variables see table 1 for each observation including hourly values within the spread interval start time to end time plus the immediately preceding and following hourly values this means that a 20 min spread interval e g between 4 20 p m and 4 40 p m was associated with the 2 weather values the 4 00 p m and 5 00 p m weather while a 5 h spread interval was associated with 7 weather values the 5 hourly values during the interval plus the hour before and after for each hour sampled we extracted the maximum value of each variable minimum for relative humidity and soil moisture within 50 km of the spread line i e barra pixel centre points within 50 km we calculated hourly forest fire danger index ffdi noble et al 1980 which is widely used in australia for fire danger forecasts and fire behaviour prediction the variables to calculate ffdi were extracted from barra temperature wind speed relative humidity and an inverse distance weighted interpolation from bureau of meteorology bom weather stations records drought factor table 1 2 3 fuel our bayesian analysis focused on weather variables but fuel load was also used for a supplementary bayesian model accumulated since last fire fuel load surface elevated was calculated after sampling state based fire history polygons and gridded fuel type base layers developed by each state fire management agency for use in phoenix rapidfire wildfire spread modelling software not available for western australia tolhurst et al 2008 the accumulated fuel load is derived from the fuel type and its associated fuel load accumulation rate as a function of time since fire we calculated accumulated fuel load t ha 1 at 100 m points along each spread line table 2 by sampling the fuel type and time since fire calculating the fuel hazard rating for the surface and elevated fuel layers then converting these hazard ratings to fuel load with equations used in phoenix rapidfire tolhurst et al 2008 the fuel loads for surface and elevated layers were added and summarised to create a single mean fuel load value for each spread line most fires in our dataset burnt within forest fuels with only a few grass fires n 22 ros 0 3 18 3 km h 1 due to the small number and different behaviour expected cruz et al 2015 we excluded grass fires in cases where grass was mixed with other fuels types we excluded those ros observations where more than 50 of the spread line was grass this was identified via a simple visual inspection of the spread lines against a background aerial image as phoenix fuel types were not available for western australia based on sampling at 100 m points of national vegetation information system vegetation classes department of agriculture 2018 most spread lines were majority forest or woodland vegetation types 95 with some majority shrubland vegetation fires included 5 3 4 bayesian model of ros we ran a model selection process to select a best performing bayesian ros model from a limited set of alternatives i e a comparison of models with alternative combinations of the surface weather variables in table 1 a main purpose of the model was to demonstrate and communicate a bayesian approach to ros modelling thus only a limited set of predictor variables focusing on surface weather were considered to simplify the bayesian analysis and exploration of results topography upper level weather and fuel variables except fuel load were not considered in the modelling we fitted all combinations of the surface weather variables fuel load was added later with the caveat that each candidate model n 25 included either wind speed or wind gust plus at least one predictor for temperature or moisture e g vapor pressure deficit or temperature or relative humidity temperature a model containing only forest fire danger index ffdi was also a candidate model candidate model performance was compared using the widely applicable information criterion waic vehtari et al 2017 watanabe 2010 which penalises for poor model fit and model complexity number of variables as a supplementary analysis we tested if fuel load table 2 may improve model performance as indicated by a lower waic by recalculating the best performing model and then the best performing model mean fuel load using the subset of data for which fuel load was available i e all except western australia fires this was an indicative analysis only as it did not include all our ros observations all candidate models were structured as multilevel or hierarchical bayesian regressions with two levels to each model the main level in which ros was related to predictor values for a given spread line and a sub model in which a representative value for each weather variable was drawn from the distribution of hourly values for the spread line the weather sub model allowed for variability in sampled hourly weather to be carried through to model predictions for example wind speed may have been relatively constant or varied substantially during the spread period of a line the sub model drew estimates of mean weather e g wind speed for each spread line which is the mean of a normal distribution fitted to the sampled hourly wind speed values fig 4 weather variables were standardised for modelling which simplified the process of model computation and comparison the main level of the model assumed that observed ros values followed a gamma distribution with a mean or expected value given by a sigmoidal function r o b s g a m m a r φ φ 1 r r m a x 1 e l p l p a b 1 x 1 b p x p z f i r e where r o b s is an observed value of ros r is the mean expected ros φ is a dispersion parameter and r m a x is the higher asymptote of the sigmoid curve i e an estimated maximum potential ros l p is a linear predictor where a is the intercept b 1 b p are regression parameters associated with the predictor variables x 1 x p and z f i r e is a fire level random effect term to account for multiple sequential ros observations from individual fires e g fig 3 and the likelihood of fire specific effects e g local topography or weather conditions we chose to fit a sigmoid shape curve based on an expectation that there is an upper limit to possible forest ros and because this form produces more reasonable predictions e g compared to an exponential curve at upper levels where observed data are sparse including extreme fire weather we chose r m a x 15 km h 1 as a reasonable estimate for our purpose here of exploring bayesian modelling of ros this is within the range of reported ros for forest in australia 10 km h 1 has been reported from case studies of dry sclerophyll forest fires cruz et al 2012 rawson et al 1983 while cheney et al 2012 estimated a ros of around 23 km h 1 for a fire near canberra in 2003 but did not include it in their model validation set as it was from a short scan interval 12 min and was probably the result of interaction between two large fires cheney et al 2012 we used weakly informative priors for regression parameters to allow the observed data to be most influential on the posterior distribution and predictions the priors made no assumption about direction positive or negative of predictor effects we chose a gamma distribution for the likelihood function to reflect the non negative nature of ros and the expected increasing variance with mean in the observed data all models were fitted using jags plummer 2017 via the runjags package in r denwood 2016 r core team 2020 all models were run with 6 chains for 2000 iterations burn in of 1000 iterations and thinning of 10 model convergence was assessed through visual inspection of markov chain trace plots we present and discuss predictions from the models in two main ways mean level predictions and predictive distributions individual level a mean level or mean trend prediction e g fig 6 answers the question for multiple fires under a given set of weather conditions what is the average expected ros these are useful for exploring the direction and strength of effects of different variables predictive distributions e g fig 7 are drawn from a gamma distribution where the gamma mean values are the mean level predictions and the gamma scale parameter values are drawn from the model fitting process predictive distributions answer questions such as given a set of weather conditions how likely is it ros will be greater than or less than some value or between two values e g between 1 km h 1 and 3 km h 1 a predictive distribution could be used as the basis for operational predictions of ros for an individual fire appendix b 3 5 deterministic model predictions we explored the performance of two deterministic ros models used operationally in australia the mcarthur mk5 model mcarthur 1973 noble et al 1980 and the dry eucalypt forest fire model deffm also known as the vesta model cheney et al 2012 gould et al 2008 deffm is the model currently recommended for operational wildfire spread prediction in australian dry eucalypt forests cruz et al 2015 this analysis was carried out to provide context for the bayesian ros modelling in particular benchmarking the relative performance of two current operational deterministic ros models using our wildfire data for the weather variables required for each model we used the mean of the sampled hourly weather values assigned to each spread line fuel variables were the mean of the fuel values sampled at 100 m intervals along the spread line fig 3 input variables for mcarthur mk5 are forest fire danger index ffdi and fuel load noble et al 1980 we used the fuel load values surface elevated derived from state based phoenix rapidfire fuel grids table 2 which were available for the nsw victorian and tasmanian fires n 216 additionally we calculated ros with fuel load set at a very high level 25 t ha 1 to explore the influence of fuel load on predictions a topographic slope correction factor can be applied but we assumed zero slope as is the common operational practice and recommendation for undulating terrain mcarthur 1967 noble et al 1980 although this assumption may contribute to prediction errors for some fires in our data that were burning on a single slope e g a fire with a spread interval of 20 min under mild weather we used the fuel hazard score version of deffm to calculate ros predictions cheney et al 2012 as deffm is described as a ros model specifically for dry eucalypt forest with shrubby understorey cheney et al 2012 we used a subset of observations where at least 50 matched this vegetation description n 130 deffm inputs are wind speed a moisture function based on temperature and humidity and fuel surface fuel hazard score near surface fuel hazard score and near surface fuel height fuel maps for deffm were not available across the study area so we compared two alternative methods to calculate the required fuel related inputs the first method was to calculate simple representative fuel input values based on time since fire as suggested by cheney et al 2012 when local fuel data are unavailable this approach is also representative of operational practice where deffm fuel maps are not currently available the second method was to predict fuel input values using the random forest models of mccoll gausden et al 2020 the random forest models were trained on field based visual assessments hines et al 2010 and predict fuel hazard and height based on bio physical predictors mccoll gausden et al 2020 the predicted fuel hazard ratings which has five classes were converted to deffm fuel hazard scores between one and four using the conversion method of hines et al 2010 and mean values of each fuel variable from the 100 m sample points along each spread line were used as model input values to indicate how well existing models predict ros measured from the wildfire line scan data we calculated mean absolute error mae and mean bias error mbe using model predictions deffm and mcarthur mk5 and the ros observations we also calculated these statistics for observed ros vs the median of the predictive distributions from the best performing bayesian model however this comparison was done only to give an indication of the performance of the preliminary bayesian model presented and should not be treated as an independent assessment of performance as all observations were used for bayesian model training i e accuracy statistics were calculated using the model training data future versions of a bayesian ros model would need to be tested on an independent ros dataset for thorough validation in addition it should be noted that direct comparisons between deterministic models and probabilistic models are not straightforward as the predictive distributions must be summarised for comparison meaning information on prediction uncertainty a main advantage of bayesian modelling is ignored nevertheless this provided a useful initial indication of model performance and behaviour cruz and alexander 2013 suggest that a ros model has performed well if the prediction is within 35 of the observed value thus we also calculated the percentage of mcarthur mk5 and deffm predictions within 35 of observed values for the bayesian model we calculated the percentage of median predictive distribution values within 35 and the percentage of interquartile ranges from the predictive distributions that fell within the 35 range 3 results 3 1 rate of spread data a total of 223 ros observations were derived from the fire progression data and used for modelling most ros values were at the lower end of the observed range mean ros was 1 34 km h 1 sd 1 57 km h 1 and 77 of observations were 2 km h 1 fig 5 however some very high ros values were also captured 9 were 4 km h 1 and maximum ros measured was 9 2 km h 1 3 2 bayesian model the best performing model lowest waic contained wind speed relative humidity and soil moisture wind speed had a strong positive influence on ros while relative humidity and soil moisture had negative influences although the soil moisture effect was weak fig 6 the credible interval bounds increased with increasing wind speed and also to a lesser extent as humidity decreased fig 6 for soil moisture credible intervals bounds were widest at low and high values fig 6 note credible intervals are simply bounds around the posterior distribution that describe where the central portion of the distribution sits e g fig 6 represents the variation in mean trend values with the middle 50 of the distribution of mean trend values falling within the 50 credible intervals under mild fire weather conditions predictive distributions fig 7 which were centred on low ros values indicated a relatively narrow range of predicted ros were plausible relatively precise statistical inferences were possible from the narrow predictive distributions for example with a wind speed of 20 km h 1 rh of 40 and soil moisture of 3 kg m 2 there was a predicted 98 31 chance of ros 1 km h 1 i e 98 31 of the predictive distribution was less than 1 km h 1 fig 7 under more extreme fire weather conditions wind speed 40 km h 1 rh 10 soil moisture 3 kg m 2 the predictive distribution was much wider indicating that a wider range of predicted ros were plausible i e there is a large uncertainty about what the ros will be under these weather conditions the model predicted only a 11 2 chance of ros 1 km h 1 but also a 48 84 chance of ros 3 km h 1 fig 7 when the ros values are converted to distances the different widths of predictive distributions translate into large differences in plausible spread distances for the different conditions fig 8 note that a code and table of posterior samples required to produce predictive distributions have been included in the supplementary material see explanation in appendix b and github com mstorey87 bayesianros of the ten best performing models considered in the model selection process table 3 seven contained wind speed and only three contained wind gust relative humidity featured in the six of the best performing models while the remaining models had temperature or vpd soil moisture and drought factor also featured in the ten best performing models including in the top two although the effects were generally small fig 6 appendix a despite drought factor featuring in the second best model the mean effect of drought factor on ros was negligible in each model and varied from being slightly negative to slightly positive depending on the model for example see model a 2 in appendix a when mean fuel load was added to the model using the subset of observations excluding western australia fires there was a small improvement in model performance waic reduced by 3 however the effect of fuel load was only minor and was slightly negative in the model higher fuel loads decreased ros opposite to the expected direction of the effect 3 3 deterministic models the mcarthur mk5 model tended to under predict ros mbe 0 56 km h 1 fig 9 table 4 the under prediction bias appeared to increase as observed ros increased fig 9 overall mae was 0 85 km h 1 while mae was 2 53 km h 1 for observations 2 km h 1 and 0 41 for observations 2 km h 1 there appeared to be an upper limit on predicted ros that was not apparent in the observed data 21 of observed ros observations were 2 km h 1 but only 1 of predicted ros were 2 km h 1 11 of observations were 3 km h 1 but no predictions were 3 km h 1 under prediction bias particularly at higher observed ros was still apparent when fuel load was artificially set to 25 t ha 1 mbe 0 19 km h 1 of the two variables used to estimate mk5 ros ffdi had the stronger individual linear relationship with ros r2 0 41 while there was almost no linear relationship between fuel load and ros r2 0 001 for deffm there was a similar scatter in the predicted vs observed ros for both fuel methods fig 9 table 4 mae was 1 35 km h 1 and there was an over prediction bias mbe 1 18 km h 1 using the time since fire converted fuel values using the random forest based fuel values mae was lower mae 0 85 km h 1 but there was a higher over prediction bias mbe 1 25 km h 1 overprediction was most obvious for ros values 2 km h 1 whereas very high ros observations e g 4 km h 1 were better matched by predictions fig 9 although there were only a small number of these observations amongst the set used for the deffm predictions 7 observations 4 km h 1 some deffm predictions appeared highly sensitive to the fuel method used for example for the observed ros at 7 5 km h 1 predicted ros varied by 4 km h 1 depending on the fuel method fig 9 for comparison the mae for the bayesian model was 0 81 km h 1 and mbe was 0 29 km h 1 table 4 where observed ros was 2 km h 1 where the bulk of the observations were situated there was a dense cloud of points around the line of agreement with some over predictions of ros fig 9 for higher ros the bayesian model produced predictions that ranged from being close to observed ros to under predictions of ros the median predictive distribution value from the bayesian model was within the 35 range for 24 of the time compared to 22 for mcarthur mk5 and 17 for deffm the interquartile ranges from each bayesian model fell somewhere within the 35 error range for 69 of the observations 4 discussion 4 1 ros modelling approach we have demonstrated a practical approach to wildfire ros modelling that has two notable advantages 1 ros observations from actual wildfires over a range of conditions are used and 2 bayesian statistical methods are employed bayesian models carry uncertainty from model fitting through to predictions the utility of which is best demonstrated by the bayesian predictive distributions a predictive distribution can be queried to make a range of probabilistic inferences of ros and provide defined levels of certainty in predictions e g 90 chance of ros between 1 km h 1 and 3 km h 1 although only surface weather variables were considered in our bayesian model selection the results indicate that highly informative probabilistic prediction information can be produced from relatively simple predictors while the model presented provides informative predictions of ros and clearly communicates prediction uncertainty the prediction intervals are likely too wide particularly in more extreme conditions to be deployed operationally however the approach employed here of bayesian model development based on ros observations from wildfires could readily be extended to create a higher precision bayesian ros model for operational predictions this work should consider that 1 the bayesian model presented here only considered surface weather and one possible model type function a broader range of environmental predictor variables must be tested in model development including fuel and upper level weather variables this could also include testing of alternative models types functions to the gamma sigmoid model we used 2 the model presented here showed very wide predictive distributions at high ros more high ros observations are needed to improve model precision which could be gained through the collection and use of more remotely sensed wildfire images particularly line scans e g incorporating line scan derived ros observations from 2019 to 20 eastern australian wildfires is a clear priority 3 the model presented here is not validated on an independent dataset validation of bayesian model predictions on an independent ros dataset would be required for an operational ros model development of an operational bayesian ros model deployed either as a separate or complimentary model amongst a suite of models in a fire prediction toolkit should consider the context of currently deployed deterministic ros models we tested deffm and the mcarthur mk5 model and found predictive accuracy was limited when applied to the wildfire data given this and as applied operationally statistical bounds to indicate prediction uncertainty are not part of the model output we argue that application of these is likely to result in considerable errors in operational prediction of wildfire ros this is based on our results where deffm had an over prediction bias while the mcarthur mk5 model had an under prediction bias the mcarthur mk5 model was particularly limited in predicting higher ros no mcarthur mk5 predictions were above 3 km h 1 despite 11 of the observations being above 3 km h 1 the finding of under prediction using the mcarthur mk5 model is consistent with findings from other authors where underprediction bias has been found including by a factor of 2 or more burrows 1994 cheney and gould 1996 cruz et al 2018 mccaw et al 2008 for deffm agreement between observed and predicted was poorer overall here compared to other evaluations which have found lower prediction error cruz et al 2018 a notable difference is that here deffm overpredicted many of the low ros values whereas other studies have not indicated such a bias cheney et al 2012 cruz et al 2018 uncertainty in the measurement of certain predictor variables likely contributed to prediction errors for the deterministic models in this study for example deffm is highly sensitive to near surface fuel height such that doubling fuel height say from 15 cm to 30 cm results in a 65 increase in predicted ros cruz et al 2015 overestimates of fuel parameters both height and hazard scores in deffm which here are above the original model range may be a reason for overpredictions although both methods we applied to estimate fuel parameters resulted in overpredictions an alternative approach to producing bounded predictions for ros is to repeatedly perturb the inputs to a deterministic model to produce a set of varying predictions e g ensemble modelling cruz 2010 finney et al 2011 however while this can provide useful insights and address real world variability in biophysical variables it does not account for the inherent uncertainty in the parameter values of the model this parameter uncertainty could be included if for example standard errors on regression model coefficients were used to simulate values from the model posterior as part of the prediction process but in practice model predictions are based only on the reported mean coefficient values 4 2 environmental variables our best performing bayesian model had three predictor variables wind speed relative humidity and soil moisture the latter of which had only a small effect this suggests that ros modelling can be based on simple predictors as noted in other studies e g ros as 10 of wind speed rule of thumb cruz and alexander 2019 a large amount of scatter in ros under similar levels of these variables possibly due to uncertainty in quantification of the predictor variables e g actual may have differed from barra wind speed meant uncertainty about the strength of the effects however the predictions produced by the bayesian model demonstrate a strength of the approach i e greater uncertainty in model fitting is directly apparent in predictions from the wide prediction intervals while this clearly communicates to a model user prediction uncertainty if prediction intervals are too wide the predictions would only provide limited guidance on what ros could be expected and what fire response strategies would be most effective during wildfire response operations to increase model precision as evident through narrower predictive distributions further research that explores alternative environmental predictors is needed our model selection results indicate that wind speed was a better predictor than wind gust and relative humidity was the best short term moisture related predictor however more complex effects of moisture related weather variables may not have been captured in our data for example fire can spread rapidly in the moist air conditions immediately after a southerly wind change in se australia due to a lagged response of dead fuel moisture to the changes in temperature and humidity luke and mcarthur 1978 weather conditions above surface level which were not considered here have been also linked to high ros including extreme fire development this includes factors such as instability influencing plume development and dry air and or upper level winds mixing down to the surface e g dry slots mountain waves mills and mccaw 2010 sharples 2009 srock et al 2018 given that large sets of quality multi dimensional reanalysis data are now being produced e g barra or era5 a detailed empirical analysis considering upper level wind temperature moisture and related stability variables should be undertaken to identify the most useful predictor s that could be incorporated into a bayesian ros model it should be noted that reanalysis data can have systematic biases in certain variables that will affect the accuracy of a ros model produced using such data for example barra has been shown to underestimate high wind speeds i e 10 m wind speed su et al 2019 which may lead to overpredictions from a ros model trained using the data long term landscape dryness which is linked to live and dead fuel moisture content is clearly important in fire behaviour extended dry periods are a prerequisite for large wildfires boer et al 2020 nolan et al 2016 barra soil moisture was more informative than drought factor as it was selected in the best model although the effect was small drought factor was selected in some supported models but the effects were mostly very small e g see model a 2 in appendix a where drought factor has a flat regression line however there is scope to explore alternative landscape dryness variables methods that more precisely predict dryness from remote sensing and or drought related plant trait information may have closer links to ros nolan et al 2016 2020a yebra et al 2018 fuel load made a small improvement in model performance when tested in a supplementary analysis although the effect was minor and was actually slightly negative contrary to the expected effect noble et al 1980 fuel load had almost no correlation with ros in our deterministic model analysis which has also been noted previously gould 2007 this may reflect inherent limitations in using fuel load to represent fuel or the imprecise data derived from existing spatial layers rather than fuel not being important for ros duff et al 2017 a detailed analysis of fuel related predictors for potential inclusion in a bayesian ros model is an important next step variables that describe fuel structure e g remotely sensed tree height or density may be more useful for predicting ros zylstra et al 2016 mapping fuel structure information from remote sensing platforms including lidar or predicting selected structure variables based on biophysical predictors jenkins et al 2020 mccoll gausden et al 2020 to create gridded datasets is likely a more accurate and useful approach for operational purposes gill et al 2017 price and gordon 2016 simard et al 2011 topography variables particularly slope are often included as predictors in ros models cheney et al 2012 mcarthur 1973 with fires travelling faster up slope and slower downslope topography can also interact with weather e g altering wind speed and direction and fuel e g vegetation differing between a ridge and valley inclusion of variables such as slope in a future version of a bayesian model could be difficult given that large wildfires can burn across multiple slopes at once nevertheless the best way to incorporate topographic effects perhaps via alternative variables e g ruggedness into a landscape scale ros model must be considered in future research in addition interaction effects between variables and possible threshold effects of certain variables neither of which were explored in our analysis could be integrated into the bayesian modelling approach and should be explored 4 3 ros data a strength of our approach is that we used a large sample of high quality wildfire ros observations with a large ros range from remotely sensed images this avoids the problem of needing to extrapolate for wildfire predictions which is necessary when using a model trained on observations at smaller scales under relatively moderate weather conditions however our data still had many more observations from fires where ros was at the lower end of the observed range e g 77 were 2 km h 1 this may explain the under prediction at higher ros in the best performing bayesian model i e waic selects the model that fits the best across all of the observations which means that the bulk of the data i e lower ros has the most influence on waic thus the variables selected some non selected candidate models appeared to produce better predictions for higher ros but performed worse overall as evident from a higher waic for example a model with only ffdi as a predictor see model a 1 in appendix a improving ros predictions for rapidly spreading wildfires is important as these are the most dangerous to public safety improvements could be achieved by adding more high ros observations e g 3 km h 1 in model development including from wildfires with extreme behaviours e g mass spotting and by employing statistical methods that specifically consider model performance at high ros in model selection in addition to the overall average performance i e waic this may include selecting a more equitable distribution of ros observations i e similar numbers of high and low values in model training data additionally more observations from rapidly spreading wildfires would also enable interactions between predictors to be modelled which may greatly improve model performance the continued use of operationally acquired line scans currently the best way to map and analyse wildfire spread for ros model development and evaluation is vital ideally a single fire would be scanned at regular intervals so that variation in ros e g short surges could be monitored although line scan frequency depends on operational need our data only included fires up to 2018 thus the clearest next step in model development is the analysis of ros from line scans acquired during the massive 2019 2020 wildfires in eastern australia boer et al 2020 nolan et al 2020b agencies and researchers should consider implementing a process by which operational data of wildfires including line scans and data from other sources e g satellite imagery could be validated and stored in standardised and readily accessible databases for research use this would streamline data access and analysis and enhance model development efforts 4 4 extending the approach the amount and quality of data collected from wildfires continues to increase which will no doubt improve our ability to understand fire spread however while technology is improving there will always be limitations to our ability to perfectly measure wildfire spread and its drivers for example accurately recording local weather across a fire area or measuring the influence of spotting at different stages of spread as such there will continue to be a need for analytical methods that can deal with the complex and inherently variable nature of wildfire spread data we have demonstrated that a bayesian statistical approach is well suited to modelling ros as it explicitly deals with variability or uncertainty in the data the model presented here is simple but the approach can be readily extended to more complex models including different predictors such as topography upper level weather and their interactions further research should be conducted to extend the modelling approach outlined here to produce a more precise bayesian ros model for operational predictions and eventually incorporate such a model into spatial systems to produce probabilistic ros prediction maps for example fig 8 a system to regularly access new observations to validate and improve a bayesian ros model would greatly assist with the task of model development research data line scan images and fuel data were provided and used with permission of the new south wales rural fire service rfs and victorian department of environment land water and planning delwp weather reanalysis data barra was accessed after registering with the bureau of meteorology bom 2020 su et al 2019 rate of spread data may be made available on request to the authors subject to a data use agreement and permission by the rfs delwp and bushfire and natural hazards cooperative research centre declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors gratefully acknowledge the assistance of sarah mccoll gausden who provided random forest based fuel predictions for this study we acknowledge the use of imagery from the nasa worldview application https worldview earthdata nasa gov part of the nasa earth observing system data and information system eosdis the provision of a phd scholarship to michael storey from the bushfire and natural hazards cooperative research centre and university of wollongong is gratefully acknowledged appendix a this appendix includes effects and prediction plots from two candidate models considered during the bayesian model selection process these are presented to demonstrate possible alternative models and to explore the effects of commonly used fire behaviour predictors in eastern australia the two models presented in this appendix are model a 1 which has forest fire danger index ffdi as the only predictor variable ffdi is the most widely used weather index by fire agencies in eastern australia so is representative of operational practice ffdi is one of the input variables along with fuel to make deterministic predictions of ros using the mcarthur mk5 ros model model a 2 included the individual component variables of ffdi as predictors temperature relative humidity wind speed and drought factor a model a 1 ffdi model fig a 1 mean posterior trend regression line black for model a 1 ffdi forest fire danger index darker grey ribbon is 50 credible interval and lighter grey is 95 credible interval the credible intervals represent variability considered by the model in fitting regression lines points are observations of rate of spread ros and mean values of sampled ffdi for the spread period of each spread line mean of sampled ffdi values are shown for clarity but individual hourly observations were used in modelling fig a 1 fig a 2 predictive distribution from model a 1 at two levels of ffdi forest fire danger index red highlighted areas indicate percent of distribution i e chance ros will be below 1 km h 1 between 1 and 3 km h 1 and above 3 km h 1 with thresholds indicated by black vertical lines note different x and y scales fig a 2 fig a 3 scatterplots showing observed ros vs predicted ros for a bayesian model with forest fire danger index as the only predictor model a 1 grey lines show interquartile ranges black point is median of predictive distributions black diagonal line shows perfect prediction with black dotted lines showing 35 of the perfect prediction representing reasonable prediction accuracy cruz and alexander 2013 fig a 3 b model a 2 ffdi component variables model fig a 4 mean posterior trend regression line black for each variable in model a 2 while other variables are held at mean observed values darker grey ribbon is 50 credible interval and lighter grey is 95 credible interval points are observations of rate of spread ros and mean values of sampled weather for the spread period of each spread line mean of sampled weather values are shown for clarity but individual hourly observations were used in modelling fig a 4 fig a 5 predictive distributions from model a 2 for mild a and more extreme b fire conditions red highlighted areas indicate percent of distribution i e chance ros will be below 1 km h 1 between 1 and 3 km h 1 and above 3 km h 1 with thresholds indicated by black vertical lines note different y scales ws wind speed km h 1 t temperature oc rh relative humidity df drought factor fig a 5 fig a 6 scatterplots showing observed ros vs predicted ros for model a 2 which contains the individual component variables of forest fire danger index as predictors i e wind speed relative humidity temperature and drought factor grey lines show interquartile ranges black point is median of predictive distributions black diagonal line shows perfect prediction with black dotted lines showing 35 of the perfect prediction representing reasonable prediction accuracy cruz and alexander 2013 fig a 6 appendix b a bayesian model predictions predictions from the bayesian ros model can be produced using the table of posterior distribution samples produced by jags which has samples of model intercept slope phi and random effect terms note predictors were standardised for analysis in our analysis jags was run in r via the runjags package with 6 chains for 2000 iterations each which produced a table with 12000 rows rerunning jags to produce this table does not need to be carried out each time a user wants to make a prediction rather a new predictive distribution can be produced by selecting input predictor variable values i e wind speed relative humidity and soil moisture and calculating a ros prediction for each row in the table this results in a distribution of ros predictions here 12000 predictions that can be plotted and queried e g of predictive distribution 1 km h 1 this would be the process employed if a bayesian model was used for prediction during firefighting operations an example r code and table of posterior samples for the model reported in the main text is available at github com mstorey87 bayesianros and in the article supplementary material section these can be used to explore the behaviour of the bayesian ros model by producing predictive distributions based on user defined predictor variable values the code produces predictive distributions and plots of predictive distributions and also recreates fig 7 from the main text note that percentages in fig 7 and produced from the r code e g 3 km h 1 will be slightly different 1 as there is a random component to the model fitting and the posterior distribution used in the main text and included in the supplementary material were produced from different runs of jags appendix c supplementary data the following are the supplementary data to this article bayesian ros prediction bayesian ros prediction dat sum 4 dat sum 4 post 4 post 4 runjags ros code example runjags ros code example appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105127 or at github com mstorey87 bayesianros 
25748,models that predict wildfire rate of spread ros play an important role in decision making during firefighting operations including fire crew placement and timing of community evacuations here we use a large set of remotely sensed wildfire observations and explanatory data focusing on weather to demonstrate a bayesian probabilistic ros modelling approach our approach has two major advantages 1 using actual wildfire observations instead of controlled fire observations makes models developed well suited to wildfire prediction 2 bayesian modelling accounts for the complex nature of wildfire spread by explicitly considering uncertainty in the data to produce probabilistic ros predictions we show that highly informative probabilistic predictions can be made from a simple bayesian model containing wind speed relative humidity and soil moisture we provide current operational context to our work by calculating predictions from widely used deterministic ros models in australia keywords wildfire bushfire fire behaviour bayesian bayesian modelling rate of spread software image analysis and gis geographic information system mapping for this research were carried out in arcgis pro 2 0 analysis and sampling of spatial data was carried out using r 3 6 r core team 2020 and the associated packages sf pebesma 2018 spatstat baddeley et al 2015 raster hijmans 2020 dplyr wickham et al 2020 and lubridate grolemund and wickham 2011 bayesian models were fitted using jags 4 3 plummer 2017 which was run via the runjags package denwood 2016 in r 3 6 r code used for model fitting and prediction is available on github github com mstorey87 bayesianros and the article supplementary material section 1 introduction rate of spread ros models are used by fire behaviour specialists to predict how fast a wildfire will spread ros predictions are a critical part of the information used along with field observations weather forecasts etc to make operational decisions around what firefighting strategies to use e g offensive or defensive simpson et al 2019 where to concentrate firefighting efforts e g near a town that could be overrun within an hour and when community warnings and evacuation alerts should be sent aemc and aemc 2009 alexander and cruz 2013 anderson berry et al 2018 poor predictions of ros can have potentially disastrous consequences for example a large under prediction may result in fire reaching a community before an evacuation is complete ros models are also integral to methods of wildfire risk assessment and planning collins et al 2015 delwp 2015 price and bedward 2020 wildfire spread is an inherently stochastic process that responds to complex multi scale interactions between numerous environmental factors and the fire itself e g spotting pyroconvection fire atmosphere interaction even minute by minute ros measurements on small fires can vary substantially despite broadly similar environmental conditions burrows et al 1991 cruz and alexander 2013 in wildfires underlying conditions including fuel amount type and structure wind speed and direction slope steepness and aspect can vary substantially across a single large flaming area and as the flames move forward leading to surges and lulls in ros both over time and on different sections of a fire dynamic fire behaviours are also associated with highly erratic fire spread for example turbulent winds and mass spotting can create complex spread patterns as spot fires and the main fire area interact and coalesce filkov et al 2020 deterministic empirical or quasi empirical models are most commonly used for wildfire spread predictions during firefighting operations sullivan 2009 these models can quickly calculate a ros prediction from a small set of environmental input variables such as wind speed and fuel amount cheney et al 2012 noble et al 1980 rothermel 1972 sneeuwjagt and peet 1985 such models as routinely applied produce a single ros value but no statistical representation of expected prediction precision or uncertainty deterministic ros model accuracy can vary substantially including a bias towards under prediction in some models cruz and alexander 2013 limitations during model development incorrect application of a model and inaccurate model input data can all affect predictive accuracy alexander and cruz 2013 gill and zylstra 2005 sullivan and knight 2001 for example an underprediction of ros may occur if actual wind speed on the fire ground is faster than forecast however a more fundamental issue is that given the complex and stochastic nature of wildfire spread especially extreme fires it is unrealistic to expect that consistently accurate predictions will be produced by models that define a unique wildfire ros from a small specific set of environmental conditions predicting ros for example based on mean forecast weather and mean fuel conditions does not account for potential variation in ros and prediction uncertainty stemming from variation in underlying conditions e g mixed fuels wind gusts and factors such as wind topography interaction spotting and fire atmosphere interaction it can be unclear to a model user how underlying variation should be accounted for when making predictions for example the mcarthur mk5 model mcarthur 1973 noble et al 1980 which is widely used in australia requires a fuel load input but it is unclear how to select a single representative fuel load value to best predict ros for a forest with fine scale variation in fuel type and or disturbance history one of the most significant limitations in developing empirical ros models for wildfires has been a lack of suitable model training data empirical ros models have usually been developed from observations of mild to moderate controlled fires at relatively small scales under a constrained set of weather conditions cruz and alexander 2013 sullivan 2009 perhaps including some observations from wildfires e g case studies cheney et al 2012 mcarthur 1967 rawson et al 1983 however wildfires burn under weather conditions and at spatial scales that may span orders of magnitude beyond the conditions used to derive empirical fire spread models making predictions for wildfires therefore requires substantial extrapolation beyond the scale environmental conditions and ros observed during model development a model developed on controlled fires may be inaccurate if used to predict ros for an extreme fire sharples et al 2016 this is problematic as extreme fires are the most dangerous and difficult for firefighters to control so reliable predictions are vital there are two major ways that the approach to ros modelling could be improved firstly using wildfire spread data for model development remotely sensed landscape scale fire spread data is available e g line scans from airborne sensors storey et al 2020a storey et al 2020b which provides the opportunity to use such information to derive and test empirical models that better reflect the behaviour and spatial scale of actual wildfires and also encompass a much broader range of weather conditions and other environmental influences secondly the task of making interpreting and communicating ros predictions would greatly benefit from a statistical approach that acknowledges the stochastic nature of wildfire spread through statistical estimates of prediction uncertainty or precision bayesian statistical methods are ideally suited to this task because they deal explicitly with uncertainty can accommodate prior knowledge or theory and allow complex effects to be represented bayesian inference is a probabilistic technique that produces predictions in the form of a distribution of plausible values given the data and model structure this could be queried to determine for a given set of environmental conditions what is the most likely ros what is the most likely range of values of ros for example what range of ros has a 90 chance of occurring what is the probability that ros will be greater than some value or less than some value or lie within a given range of values e g between 1 km h 1 and 2 km h 1 a major advantage of bayesian statistical methods is that variability in model inputs and uncertainty in the fitted values of model parameters is carried through to model predictions meaning uncertainty associated with a ros prediction is readily apparent to a model user and could be taken into account in decisions made during firefighting operations in this paper we use a large set of remotely sensed wildfire observations spanning a wide range of weather conditions to 1 demonstrate how a bayesian approach to ros modelling can take into account variability in model inputs and the precision with which parameters can be estimated 2 run a model selection process and report a best performing bayesian model from a limited set of alternative models that contain surface weather variables typically used for empirical ros prediction 3 provide current operational context to our work by calculating predictions and performance statistics for two deterministic ros models used operationally in australia we present a preliminary bayesian ros model here to demonstrate and communicate the advantages of a bayesian approach to ros modelling particularly when coupled with wildfire ros observations we discuss the results and behaviour of the bayesian model with the context of current deterministic model results and discuss its advantages and further steps required to develop a bayesian ros model that could be deployed operationally 2 methods 2 1 rate of spread data we created a set of 223 wildfire ros observations from pairs of gis polygons covering active wildfires from southern australia fig 1 each pair represents the same wildfire captured at different times fire polygons were manually digitized in arcgis pro 2 0 with one person ms loading in the remotely sensed fire images visually identifying all actively burning and extinguished fire perimeter before drawing a polygon to cover the fire area fires were identified in 1 aircraft acquired line scan images 87 5 of observations used and 2 satellite acquired images modis and himawari 8 10 and 2 5 of observations respectively line scans were provided by the victorian department of environment land water and planning delwp and nsw rural fire service rfs line scans are georectified images of a fire and surrounding land generally at 5 m 15 m resolution and were acquired by aircraft fitted with multispectral or infrared ir line scanners cook et al 2009 matthews 1997 the ir band s allow for smoke to be mostly removed for visual interpretation in a gis allowing for visual discrimination between burning and non burning areas see storey et al 2020b for more detailed description we used all line scans from the datasets provided where we could identify at least two images of an actively burning fire within 5 h modis images were pre processed modis corrected reflectance images using band combination 7 short wave ir 2 near ir and 1 green red and were downloaded directly from the nasa worldview online application nasa 2020 images were identified in worldview for selected dates where a we had a line scan of a very large fire but no prior or subsequent line scan and b dates where large fires occurred but no line scans were available the 7 2 1 modis band combination provides a similar product to the line scans i e smoke removed for visual interpretation but at coarser spatial resolution 250 m for bands 1 and 2 500 m for band 7 due to the coarser resolution only large fires could be digitized using modis images modis images are captured only twice per day with one late morning and one early afternoon generally with a 3 4 h interval we included a small set of coarse fire polygons based on himawari 8 bessho et al 2016 geostationary satellite band 7 images for a group of large fires near esperance in western australia 17 nov 2015 for which line scans and modis images were not available in comparison to modis himawari 8 has high temporal resolution 10 min between captures but a coarse spatial resolution of 2 km for band 7 we made a single ros measurement from selected pairs of images 3 h apart for each of these fires inter image intervals ranged between 20 min and 5 h we excluded observations where fires were subject to a significant mid interval wind change and where fire spread was obviously impeded for example where the fire was extinguished in image two or ran into a previously burnt area some limitations in using line scan data should be noted when interpreting the results in particular fire behaviour between the two image acquisitions is unknown thus it was not possible to determine if all fires were completely free burning for the entire interval or the degree in variation in fire spread between images for example a fire run may have been slowed by suppression when burning down a slope or by a fuel break e g a road or river fire spread may have surged or lulled at different times between two images in response to gusting winds and spotting or extreme fire behaviours in some images spotting clearly allowed fire spread to continue there were five spread observations where the main fire consisted of more than one polygon as it appeared the fire had spotted over a damp valley or road from which a new head fire formed without the two fire sections polygons merging into one while these limitations produce some uncertainty in ros using real wildfire observations is important in order to produce a model that accounts for such variation in landscape scale wildfire spread to measure ros for each pair of fire polygons we created a single gis line representing the line of fastest spread the process was to 1 create a convex hull around each polygon which assisted in automation of the ros measurement 2 measure the straight line distance from the head of the fire at time one i e polygon one to the head of the fire at time two through an automated process see fig 2 for details 3 calculate ros km h 1 as spread line distance over total spread time with spread time being the difference between acquisition times of the polygon source images the ros determined is therefore the average ros over the time between the two images 2 2 weather data weather data for all ros lines was extracted from the bureau of meteorology atmospheric high resolution regional reanalysis for australia barra barra uses a combination of atmospheric model forecasts based on the australian community climate and earth simulator system access framework and observations from a range of sources including satellite and ground based sensors to produce a four dimensional latitude longitude atmospheric pressure level time i e hourly gridded reanalysis product su et al 2019 each ros observation was associated with a set of hourly weather values for our analysis which were extracted from the barra r 12 km spatial resolution grids we extracted several fire influencing variables see table 1 for each observation including hourly values within the spread interval start time to end time plus the immediately preceding and following hourly values this means that a 20 min spread interval e g between 4 20 p m and 4 40 p m was associated with the 2 weather values the 4 00 p m and 5 00 p m weather while a 5 h spread interval was associated with 7 weather values the 5 hourly values during the interval plus the hour before and after for each hour sampled we extracted the maximum value of each variable minimum for relative humidity and soil moisture within 50 km of the spread line i e barra pixel centre points within 50 km we calculated hourly forest fire danger index ffdi noble et al 1980 which is widely used in australia for fire danger forecasts and fire behaviour prediction the variables to calculate ffdi were extracted from barra temperature wind speed relative humidity and an inverse distance weighted interpolation from bureau of meteorology bom weather stations records drought factor table 1 2 3 fuel our bayesian analysis focused on weather variables but fuel load was also used for a supplementary bayesian model accumulated since last fire fuel load surface elevated was calculated after sampling state based fire history polygons and gridded fuel type base layers developed by each state fire management agency for use in phoenix rapidfire wildfire spread modelling software not available for western australia tolhurst et al 2008 the accumulated fuel load is derived from the fuel type and its associated fuel load accumulation rate as a function of time since fire we calculated accumulated fuel load t ha 1 at 100 m points along each spread line table 2 by sampling the fuel type and time since fire calculating the fuel hazard rating for the surface and elevated fuel layers then converting these hazard ratings to fuel load with equations used in phoenix rapidfire tolhurst et al 2008 the fuel loads for surface and elevated layers were added and summarised to create a single mean fuel load value for each spread line most fires in our dataset burnt within forest fuels with only a few grass fires n 22 ros 0 3 18 3 km h 1 due to the small number and different behaviour expected cruz et al 2015 we excluded grass fires in cases where grass was mixed with other fuels types we excluded those ros observations where more than 50 of the spread line was grass this was identified via a simple visual inspection of the spread lines against a background aerial image as phoenix fuel types were not available for western australia based on sampling at 100 m points of national vegetation information system vegetation classes department of agriculture 2018 most spread lines were majority forest or woodland vegetation types 95 with some majority shrubland vegetation fires included 5 3 4 bayesian model of ros we ran a model selection process to select a best performing bayesian ros model from a limited set of alternatives i e a comparison of models with alternative combinations of the surface weather variables in table 1 a main purpose of the model was to demonstrate and communicate a bayesian approach to ros modelling thus only a limited set of predictor variables focusing on surface weather were considered to simplify the bayesian analysis and exploration of results topography upper level weather and fuel variables except fuel load were not considered in the modelling we fitted all combinations of the surface weather variables fuel load was added later with the caveat that each candidate model n 25 included either wind speed or wind gust plus at least one predictor for temperature or moisture e g vapor pressure deficit or temperature or relative humidity temperature a model containing only forest fire danger index ffdi was also a candidate model candidate model performance was compared using the widely applicable information criterion waic vehtari et al 2017 watanabe 2010 which penalises for poor model fit and model complexity number of variables as a supplementary analysis we tested if fuel load table 2 may improve model performance as indicated by a lower waic by recalculating the best performing model and then the best performing model mean fuel load using the subset of data for which fuel load was available i e all except western australia fires this was an indicative analysis only as it did not include all our ros observations all candidate models were structured as multilevel or hierarchical bayesian regressions with two levels to each model the main level in which ros was related to predictor values for a given spread line and a sub model in which a representative value for each weather variable was drawn from the distribution of hourly values for the spread line the weather sub model allowed for variability in sampled hourly weather to be carried through to model predictions for example wind speed may have been relatively constant or varied substantially during the spread period of a line the sub model drew estimates of mean weather e g wind speed for each spread line which is the mean of a normal distribution fitted to the sampled hourly wind speed values fig 4 weather variables were standardised for modelling which simplified the process of model computation and comparison the main level of the model assumed that observed ros values followed a gamma distribution with a mean or expected value given by a sigmoidal function r o b s g a m m a r φ φ 1 r r m a x 1 e l p l p a b 1 x 1 b p x p z f i r e where r o b s is an observed value of ros r is the mean expected ros φ is a dispersion parameter and r m a x is the higher asymptote of the sigmoid curve i e an estimated maximum potential ros l p is a linear predictor where a is the intercept b 1 b p are regression parameters associated with the predictor variables x 1 x p and z f i r e is a fire level random effect term to account for multiple sequential ros observations from individual fires e g fig 3 and the likelihood of fire specific effects e g local topography or weather conditions we chose to fit a sigmoid shape curve based on an expectation that there is an upper limit to possible forest ros and because this form produces more reasonable predictions e g compared to an exponential curve at upper levels where observed data are sparse including extreme fire weather we chose r m a x 15 km h 1 as a reasonable estimate for our purpose here of exploring bayesian modelling of ros this is within the range of reported ros for forest in australia 10 km h 1 has been reported from case studies of dry sclerophyll forest fires cruz et al 2012 rawson et al 1983 while cheney et al 2012 estimated a ros of around 23 km h 1 for a fire near canberra in 2003 but did not include it in their model validation set as it was from a short scan interval 12 min and was probably the result of interaction between two large fires cheney et al 2012 we used weakly informative priors for regression parameters to allow the observed data to be most influential on the posterior distribution and predictions the priors made no assumption about direction positive or negative of predictor effects we chose a gamma distribution for the likelihood function to reflect the non negative nature of ros and the expected increasing variance with mean in the observed data all models were fitted using jags plummer 2017 via the runjags package in r denwood 2016 r core team 2020 all models were run with 6 chains for 2000 iterations burn in of 1000 iterations and thinning of 10 model convergence was assessed through visual inspection of markov chain trace plots we present and discuss predictions from the models in two main ways mean level predictions and predictive distributions individual level a mean level or mean trend prediction e g fig 6 answers the question for multiple fires under a given set of weather conditions what is the average expected ros these are useful for exploring the direction and strength of effects of different variables predictive distributions e g fig 7 are drawn from a gamma distribution where the gamma mean values are the mean level predictions and the gamma scale parameter values are drawn from the model fitting process predictive distributions answer questions such as given a set of weather conditions how likely is it ros will be greater than or less than some value or between two values e g between 1 km h 1 and 3 km h 1 a predictive distribution could be used as the basis for operational predictions of ros for an individual fire appendix b 3 5 deterministic model predictions we explored the performance of two deterministic ros models used operationally in australia the mcarthur mk5 model mcarthur 1973 noble et al 1980 and the dry eucalypt forest fire model deffm also known as the vesta model cheney et al 2012 gould et al 2008 deffm is the model currently recommended for operational wildfire spread prediction in australian dry eucalypt forests cruz et al 2015 this analysis was carried out to provide context for the bayesian ros modelling in particular benchmarking the relative performance of two current operational deterministic ros models using our wildfire data for the weather variables required for each model we used the mean of the sampled hourly weather values assigned to each spread line fuel variables were the mean of the fuel values sampled at 100 m intervals along the spread line fig 3 input variables for mcarthur mk5 are forest fire danger index ffdi and fuel load noble et al 1980 we used the fuel load values surface elevated derived from state based phoenix rapidfire fuel grids table 2 which were available for the nsw victorian and tasmanian fires n 216 additionally we calculated ros with fuel load set at a very high level 25 t ha 1 to explore the influence of fuel load on predictions a topographic slope correction factor can be applied but we assumed zero slope as is the common operational practice and recommendation for undulating terrain mcarthur 1967 noble et al 1980 although this assumption may contribute to prediction errors for some fires in our data that were burning on a single slope e g a fire with a spread interval of 20 min under mild weather we used the fuel hazard score version of deffm to calculate ros predictions cheney et al 2012 as deffm is described as a ros model specifically for dry eucalypt forest with shrubby understorey cheney et al 2012 we used a subset of observations where at least 50 matched this vegetation description n 130 deffm inputs are wind speed a moisture function based on temperature and humidity and fuel surface fuel hazard score near surface fuel hazard score and near surface fuel height fuel maps for deffm were not available across the study area so we compared two alternative methods to calculate the required fuel related inputs the first method was to calculate simple representative fuel input values based on time since fire as suggested by cheney et al 2012 when local fuel data are unavailable this approach is also representative of operational practice where deffm fuel maps are not currently available the second method was to predict fuel input values using the random forest models of mccoll gausden et al 2020 the random forest models were trained on field based visual assessments hines et al 2010 and predict fuel hazard and height based on bio physical predictors mccoll gausden et al 2020 the predicted fuel hazard ratings which has five classes were converted to deffm fuel hazard scores between one and four using the conversion method of hines et al 2010 and mean values of each fuel variable from the 100 m sample points along each spread line were used as model input values to indicate how well existing models predict ros measured from the wildfire line scan data we calculated mean absolute error mae and mean bias error mbe using model predictions deffm and mcarthur mk5 and the ros observations we also calculated these statistics for observed ros vs the median of the predictive distributions from the best performing bayesian model however this comparison was done only to give an indication of the performance of the preliminary bayesian model presented and should not be treated as an independent assessment of performance as all observations were used for bayesian model training i e accuracy statistics were calculated using the model training data future versions of a bayesian ros model would need to be tested on an independent ros dataset for thorough validation in addition it should be noted that direct comparisons between deterministic models and probabilistic models are not straightforward as the predictive distributions must be summarised for comparison meaning information on prediction uncertainty a main advantage of bayesian modelling is ignored nevertheless this provided a useful initial indication of model performance and behaviour cruz and alexander 2013 suggest that a ros model has performed well if the prediction is within 35 of the observed value thus we also calculated the percentage of mcarthur mk5 and deffm predictions within 35 of observed values for the bayesian model we calculated the percentage of median predictive distribution values within 35 and the percentage of interquartile ranges from the predictive distributions that fell within the 35 range 3 results 3 1 rate of spread data a total of 223 ros observations were derived from the fire progression data and used for modelling most ros values were at the lower end of the observed range mean ros was 1 34 km h 1 sd 1 57 km h 1 and 77 of observations were 2 km h 1 fig 5 however some very high ros values were also captured 9 were 4 km h 1 and maximum ros measured was 9 2 km h 1 3 2 bayesian model the best performing model lowest waic contained wind speed relative humidity and soil moisture wind speed had a strong positive influence on ros while relative humidity and soil moisture had negative influences although the soil moisture effect was weak fig 6 the credible interval bounds increased with increasing wind speed and also to a lesser extent as humidity decreased fig 6 for soil moisture credible intervals bounds were widest at low and high values fig 6 note credible intervals are simply bounds around the posterior distribution that describe where the central portion of the distribution sits e g fig 6 represents the variation in mean trend values with the middle 50 of the distribution of mean trend values falling within the 50 credible intervals under mild fire weather conditions predictive distributions fig 7 which were centred on low ros values indicated a relatively narrow range of predicted ros were plausible relatively precise statistical inferences were possible from the narrow predictive distributions for example with a wind speed of 20 km h 1 rh of 40 and soil moisture of 3 kg m 2 there was a predicted 98 31 chance of ros 1 km h 1 i e 98 31 of the predictive distribution was less than 1 km h 1 fig 7 under more extreme fire weather conditions wind speed 40 km h 1 rh 10 soil moisture 3 kg m 2 the predictive distribution was much wider indicating that a wider range of predicted ros were plausible i e there is a large uncertainty about what the ros will be under these weather conditions the model predicted only a 11 2 chance of ros 1 km h 1 but also a 48 84 chance of ros 3 km h 1 fig 7 when the ros values are converted to distances the different widths of predictive distributions translate into large differences in plausible spread distances for the different conditions fig 8 note that a code and table of posterior samples required to produce predictive distributions have been included in the supplementary material see explanation in appendix b and github com mstorey87 bayesianros of the ten best performing models considered in the model selection process table 3 seven contained wind speed and only three contained wind gust relative humidity featured in the six of the best performing models while the remaining models had temperature or vpd soil moisture and drought factor also featured in the ten best performing models including in the top two although the effects were generally small fig 6 appendix a despite drought factor featuring in the second best model the mean effect of drought factor on ros was negligible in each model and varied from being slightly negative to slightly positive depending on the model for example see model a 2 in appendix a when mean fuel load was added to the model using the subset of observations excluding western australia fires there was a small improvement in model performance waic reduced by 3 however the effect of fuel load was only minor and was slightly negative in the model higher fuel loads decreased ros opposite to the expected direction of the effect 3 3 deterministic models the mcarthur mk5 model tended to under predict ros mbe 0 56 km h 1 fig 9 table 4 the under prediction bias appeared to increase as observed ros increased fig 9 overall mae was 0 85 km h 1 while mae was 2 53 km h 1 for observations 2 km h 1 and 0 41 for observations 2 km h 1 there appeared to be an upper limit on predicted ros that was not apparent in the observed data 21 of observed ros observations were 2 km h 1 but only 1 of predicted ros were 2 km h 1 11 of observations were 3 km h 1 but no predictions were 3 km h 1 under prediction bias particularly at higher observed ros was still apparent when fuel load was artificially set to 25 t ha 1 mbe 0 19 km h 1 of the two variables used to estimate mk5 ros ffdi had the stronger individual linear relationship with ros r2 0 41 while there was almost no linear relationship between fuel load and ros r2 0 001 for deffm there was a similar scatter in the predicted vs observed ros for both fuel methods fig 9 table 4 mae was 1 35 km h 1 and there was an over prediction bias mbe 1 18 km h 1 using the time since fire converted fuel values using the random forest based fuel values mae was lower mae 0 85 km h 1 but there was a higher over prediction bias mbe 1 25 km h 1 overprediction was most obvious for ros values 2 km h 1 whereas very high ros observations e g 4 km h 1 were better matched by predictions fig 9 although there were only a small number of these observations amongst the set used for the deffm predictions 7 observations 4 km h 1 some deffm predictions appeared highly sensitive to the fuel method used for example for the observed ros at 7 5 km h 1 predicted ros varied by 4 km h 1 depending on the fuel method fig 9 for comparison the mae for the bayesian model was 0 81 km h 1 and mbe was 0 29 km h 1 table 4 where observed ros was 2 km h 1 where the bulk of the observations were situated there was a dense cloud of points around the line of agreement with some over predictions of ros fig 9 for higher ros the bayesian model produced predictions that ranged from being close to observed ros to under predictions of ros the median predictive distribution value from the bayesian model was within the 35 range for 24 of the time compared to 22 for mcarthur mk5 and 17 for deffm the interquartile ranges from each bayesian model fell somewhere within the 35 error range for 69 of the observations 4 discussion 4 1 ros modelling approach we have demonstrated a practical approach to wildfire ros modelling that has two notable advantages 1 ros observations from actual wildfires over a range of conditions are used and 2 bayesian statistical methods are employed bayesian models carry uncertainty from model fitting through to predictions the utility of which is best demonstrated by the bayesian predictive distributions a predictive distribution can be queried to make a range of probabilistic inferences of ros and provide defined levels of certainty in predictions e g 90 chance of ros between 1 km h 1 and 3 km h 1 although only surface weather variables were considered in our bayesian model selection the results indicate that highly informative probabilistic prediction information can be produced from relatively simple predictors while the model presented provides informative predictions of ros and clearly communicates prediction uncertainty the prediction intervals are likely too wide particularly in more extreme conditions to be deployed operationally however the approach employed here of bayesian model development based on ros observations from wildfires could readily be extended to create a higher precision bayesian ros model for operational predictions this work should consider that 1 the bayesian model presented here only considered surface weather and one possible model type function a broader range of environmental predictor variables must be tested in model development including fuel and upper level weather variables this could also include testing of alternative models types functions to the gamma sigmoid model we used 2 the model presented here showed very wide predictive distributions at high ros more high ros observations are needed to improve model precision which could be gained through the collection and use of more remotely sensed wildfire images particularly line scans e g incorporating line scan derived ros observations from 2019 to 20 eastern australian wildfires is a clear priority 3 the model presented here is not validated on an independent dataset validation of bayesian model predictions on an independent ros dataset would be required for an operational ros model development of an operational bayesian ros model deployed either as a separate or complimentary model amongst a suite of models in a fire prediction toolkit should consider the context of currently deployed deterministic ros models we tested deffm and the mcarthur mk5 model and found predictive accuracy was limited when applied to the wildfire data given this and as applied operationally statistical bounds to indicate prediction uncertainty are not part of the model output we argue that application of these is likely to result in considerable errors in operational prediction of wildfire ros this is based on our results where deffm had an over prediction bias while the mcarthur mk5 model had an under prediction bias the mcarthur mk5 model was particularly limited in predicting higher ros no mcarthur mk5 predictions were above 3 km h 1 despite 11 of the observations being above 3 km h 1 the finding of under prediction using the mcarthur mk5 model is consistent with findings from other authors where underprediction bias has been found including by a factor of 2 or more burrows 1994 cheney and gould 1996 cruz et al 2018 mccaw et al 2008 for deffm agreement between observed and predicted was poorer overall here compared to other evaluations which have found lower prediction error cruz et al 2018 a notable difference is that here deffm overpredicted many of the low ros values whereas other studies have not indicated such a bias cheney et al 2012 cruz et al 2018 uncertainty in the measurement of certain predictor variables likely contributed to prediction errors for the deterministic models in this study for example deffm is highly sensitive to near surface fuel height such that doubling fuel height say from 15 cm to 30 cm results in a 65 increase in predicted ros cruz et al 2015 overestimates of fuel parameters both height and hazard scores in deffm which here are above the original model range may be a reason for overpredictions although both methods we applied to estimate fuel parameters resulted in overpredictions an alternative approach to producing bounded predictions for ros is to repeatedly perturb the inputs to a deterministic model to produce a set of varying predictions e g ensemble modelling cruz 2010 finney et al 2011 however while this can provide useful insights and address real world variability in biophysical variables it does not account for the inherent uncertainty in the parameter values of the model this parameter uncertainty could be included if for example standard errors on regression model coefficients were used to simulate values from the model posterior as part of the prediction process but in practice model predictions are based only on the reported mean coefficient values 4 2 environmental variables our best performing bayesian model had three predictor variables wind speed relative humidity and soil moisture the latter of which had only a small effect this suggests that ros modelling can be based on simple predictors as noted in other studies e g ros as 10 of wind speed rule of thumb cruz and alexander 2019 a large amount of scatter in ros under similar levels of these variables possibly due to uncertainty in quantification of the predictor variables e g actual may have differed from barra wind speed meant uncertainty about the strength of the effects however the predictions produced by the bayesian model demonstrate a strength of the approach i e greater uncertainty in model fitting is directly apparent in predictions from the wide prediction intervals while this clearly communicates to a model user prediction uncertainty if prediction intervals are too wide the predictions would only provide limited guidance on what ros could be expected and what fire response strategies would be most effective during wildfire response operations to increase model precision as evident through narrower predictive distributions further research that explores alternative environmental predictors is needed our model selection results indicate that wind speed was a better predictor than wind gust and relative humidity was the best short term moisture related predictor however more complex effects of moisture related weather variables may not have been captured in our data for example fire can spread rapidly in the moist air conditions immediately after a southerly wind change in se australia due to a lagged response of dead fuel moisture to the changes in temperature and humidity luke and mcarthur 1978 weather conditions above surface level which were not considered here have been also linked to high ros including extreme fire development this includes factors such as instability influencing plume development and dry air and or upper level winds mixing down to the surface e g dry slots mountain waves mills and mccaw 2010 sharples 2009 srock et al 2018 given that large sets of quality multi dimensional reanalysis data are now being produced e g barra or era5 a detailed empirical analysis considering upper level wind temperature moisture and related stability variables should be undertaken to identify the most useful predictor s that could be incorporated into a bayesian ros model it should be noted that reanalysis data can have systematic biases in certain variables that will affect the accuracy of a ros model produced using such data for example barra has been shown to underestimate high wind speeds i e 10 m wind speed su et al 2019 which may lead to overpredictions from a ros model trained using the data long term landscape dryness which is linked to live and dead fuel moisture content is clearly important in fire behaviour extended dry periods are a prerequisite for large wildfires boer et al 2020 nolan et al 2016 barra soil moisture was more informative than drought factor as it was selected in the best model although the effect was small drought factor was selected in some supported models but the effects were mostly very small e g see model a 2 in appendix a where drought factor has a flat regression line however there is scope to explore alternative landscape dryness variables methods that more precisely predict dryness from remote sensing and or drought related plant trait information may have closer links to ros nolan et al 2016 2020a yebra et al 2018 fuel load made a small improvement in model performance when tested in a supplementary analysis although the effect was minor and was actually slightly negative contrary to the expected effect noble et al 1980 fuel load had almost no correlation with ros in our deterministic model analysis which has also been noted previously gould 2007 this may reflect inherent limitations in using fuel load to represent fuel or the imprecise data derived from existing spatial layers rather than fuel not being important for ros duff et al 2017 a detailed analysis of fuel related predictors for potential inclusion in a bayesian ros model is an important next step variables that describe fuel structure e g remotely sensed tree height or density may be more useful for predicting ros zylstra et al 2016 mapping fuel structure information from remote sensing platforms including lidar or predicting selected structure variables based on biophysical predictors jenkins et al 2020 mccoll gausden et al 2020 to create gridded datasets is likely a more accurate and useful approach for operational purposes gill et al 2017 price and gordon 2016 simard et al 2011 topography variables particularly slope are often included as predictors in ros models cheney et al 2012 mcarthur 1973 with fires travelling faster up slope and slower downslope topography can also interact with weather e g altering wind speed and direction and fuel e g vegetation differing between a ridge and valley inclusion of variables such as slope in a future version of a bayesian model could be difficult given that large wildfires can burn across multiple slopes at once nevertheless the best way to incorporate topographic effects perhaps via alternative variables e g ruggedness into a landscape scale ros model must be considered in future research in addition interaction effects between variables and possible threshold effects of certain variables neither of which were explored in our analysis could be integrated into the bayesian modelling approach and should be explored 4 3 ros data a strength of our approach is that we used a large sample of high quality wildfire ros observations with a large ros range from remotely sensed images this avoids the problem of needing to extrapolate for wildfire predictions which is necessary when using a model trained on observations at smaller scales under relatively moderate weather conditions however our data still had many more observations from fires where ros was at the lower end of the observed range e g 77 were 2 km h 1 this may explain the under prediction at higher ros in the best performing bayesian model i e waic selects the model that fits the best across all of the observations which means that the bulk of the data i e lower ros has the most influence on waic thus the variables selected some non selected candidate models appeared to produce better predictions for higher ros but performed worse overall as evident from a higher waic for example a model with only ffdi as a predictor see model a 1 in appendix a improving ros predictions for rapidly spreading wildfires is important as these are the most dangerous to public safety improvements could be achieved by adding more high ros observations e g 3 km h 1 in model development including from wildfires with extreme behaviours e g mass spotting and by employing statistical methods that specifically consider model performance at high ros in model selection in addition to the overall average performance i e waic this may include selecting a more equitable distribution of ros observations i e similar numbers of high and low values in model training data additionally more observations from rapidly spreading wildfires would also enable interactions between predictors to be modelled which may greatly improve model performance the continued use of operationally acquired line scans currently the best way to map and analyse wildfire spread for ros model development and evaluation is vital ideally a single fire would be scanned at regular intervals so that variation in ros e g short surges could be monitored although line scan frequency depends on operational need our data only included fires up to 2018 thus the clearest next step in model development is the analysis of ros from line scans acquired during the massive 2019 2020 wildfires in eastern australia boer et al 2020 nolan et al 2020b agencies and researchers should consider implementing a process by which operational data of wildfires including line scans and data from other sources e g satellite imagery could be validated and stored in standardised and readily accessible databases for research use this would streamline data access and analysis and enhance model development efforts 4 4 extending the approach the amount and quality of data collected from wildfires continues to increase which will no doubt improve our ability to understand fire spread however while technology is improving there will always be limitations to our ability to perfectly measure wildfire spread and its drivers for example accurately recording local weather across a fire area or measuring the influence of spotting at different stages of spread as such there will continue to be a need for analytical methods that can deal with the complex and inherently variable nature of wildfire spread data we have demonstrated that a bayesian statistical approach is well suited to modelling ros as it explicitly deals with variability or uncertainty in the data the model presented here is simple but the approach can be readily extended to more complex models including different predictors such as topography upper level weather and their interactions further research should be conducted to extend the modelling approach outlined here to produce a more precise bayesian ros model for operational predictions and eventually incorporate such a model into spatial systems to produce probabilistic ros prediction maps for example fig 8 a system to regularly access new observations to validate and improve a bayesian ros model would greatly assist with the task of model development research data line scan images and fuel data were provided and used with permission of the new south wales rural fire service rfs and victorian department of environment land water and planning delwp weather reanalysis data barra was accessed after registering with the bureau of meteorology bom 2020 su et al 2019 rate of spread data may be made available on request to the authors subject to a data use agreement and permission by the rfs delwp and bushfire and natural hazards cooperative research centre declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors gratefully acknowledge the assistance of sarah mccoll gausden who provided random forest based fuel predictions for this study we acknowledge the use of imagery from the nasa worldview application https worldview earthdata nasa gov part of the nasa earth observing system data and information system eosdis the provision of a phd scholarship to michael storey from the bushfire and natural hazards cooperative research centre and university of wollongong is gratefully acknowledged appendix a this appendix includes effects and prediction plots from two candidate models considered during the bayesian model selection process these are presented to demonstrate possible alternative models and to explore the effects of commonly used fire behaviour predictors in eastern australia the two models presented in this appendix are model a 1 which has forest fire danger index ffdi as the only predictor variable ffdi is the most widely used weather index by fire agencies in eastern australia so is representative of operational practice ffdi is one of the input variables along with fuel to make deterministic predictions of ros using the mcarthur mk5 ros model model a 2 included the individual component variables of ffdi as predictors temperature relative humidity wind speed and drought factor a model a 1 ffdi model fig a 1 mean posterior trend regression line black for model a 1 ffdi forest fire danger index darker grey ribbon is 50 credible interval and lighter grey is 95 credible interval the credible intervals represent variability considered by the model in fitting regression lines points are observations of rate of spread ros and mean values of sampled ffdi for the spread period of each spread line mean of sampled ffdi values are shown for clarity but individual hourly observations were used in modelling fig a 1 fig a 2 predictive distribution from model a 1 at two levels of ffdi forest fire danger index red highlighted areas indicate percent of distribution i e chance ros will be below 1 km h 1 between 1 and 3 km h 1 and above 3 km h 1 with thresholds indicated by black vertical lines note different x and y scales fig a 2 fig a 3 scatterplots showing observed ros vs predicted ros for a bayesian model with forest fire danger index as the only predictor model a 1 grey lines show interquartile ranges black point is median of predictive distributions black diagonal line shows perfect prediction with black dotted lines showing 35 of the perfect prediction representing reasonable prediction accuracy cruz and alexander 2013 fig a 3 b model a 2 ffdi component variables model fig a 4 mean posterior trend regression line black for each variable in model a 2 while other variables are held at mean observed values darker grey ribbon is 50 credible interval and lighter grey is 95 credible interval points are observations of rate of spread ros and mean values of sampled weather for the spread period of each spread line mean of sampled weather values are shown for clarity but individual hourly observations were used in modelling fig a 4 fig a 5 predictive distributions from model a 2 for mild a and more extreme b fire conditions red highlighted areas indicate percent of distribution i e chance ros will be below 1 km h 1 between 1 and 3 km h 1 and above 3 km h 1 with thresholds indicated by black vertical lines note different y scales ws wind speed km h 1 t temperature oc rh relative humidity df drought factor fig a 5 fig a 6 scatterplots showing observed ros vs predicted ros for model a 2 which contains the individual component variables of forest fire danger index as predictors i e wind speed relative humidity temperature and drought factor grey lines show interquartile ranges black point is median of predictive distributions black diagonal line shows perfect prediction with black dotted lines showing 35 of the perfect prediction representing reasonable prediction accuracy cruz and alexander 2013 fig a 6 appendix b a bayesian model predictions predictions from the bayesian ros model can be produced using the table of posterior distribution samples produced by jags which has samples of model intercept slope phi and random effect terms note predictors were standardised for analysis in our analysis jags was run in r via the runjags package with 6 chains for 2000 iterations each which produced a table with 12000 rows rerunning jags to produce this table does not need to be carried out each time a user wants to make a prediction rather a new predictive distribution can be produced by selecting input predictor variable values i e wind speed relative humidity and soil moisture and calculating a ros prediction for each row in the table this results in a distribution of ros predictions here 12000 predictions that can be plotted and queried e g of predictive distribution 1 km h 1 this would be the process employed if a bayesian model was used for prediction during firefighting operations an example r code and table of posterior samples for the model reported in the main text is available at github com mstorey87 bayesianros and in the article supplementary material section these can be used to explore the behaviour of the bayesian ros model by producing predictive distributions based on user defined predictor variable values the code produces predictive distributions and plots of predictive distributions and also recreates fig 7 from the main text note that percentages in fig 7 and produced from the r code e g 3 km h 1 will be slightly different 1 as there is a random component to the model fitting and the posterior distribution used in the main text and included in the supplementary material were produced from different runs of jags appendix c supplementary data the following are the supplementary data to this article bayesian ros prediction bayesian ros prediction dat sum 4 dat sum 4 post 4 post 4 runjags ros code example runjags ros code example appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105127 or at github com mstorey87 bayesianros 
25749,previous criticisms of knowledge based fuzzy logic modelling have identified some of its limitations and revealed weaknesses regarding the development of fuzzy sets the integration of expert knowledge and the outcomes of different defuzzification processes we show here how expert disagreement and fuzzy logic mechanisms associated with the rule development and combinations can positively or adversely affect model performance and the interpretation of results we highlight how expert disagreement can induce uncertainty into model outputs when defining fuzzy sets and selecting a defuzzification method we present a framework to account for sources of error and bias and improve the performance and robustness of knowledge based fuzzy logic models we recommend to 1 provide clear unambiguous instructions on model development processes and objectives including the definition of input variables and fuzzy sets 2 incorporate the disagreement among experts into the analysis 3 increase the use of short rules and the or operator to reduce complexity and 4 improve model performance and robustness by using narrow fuzzy sets for extreme values of input variables to expand the universe of discourse adequately our framework is focused on fuzzy logic models but can be applied to all knowledge based models that require expert judgment including expert systems decision trees and fuzzy bayesian inference systems keywords fuzzy logic critic expert knowledge model optimization decision framework 1 introduction fuzzy logic is a knowledge based modelling method that uses linguistic variables obtained from human operators and or expert knowledge as model inputs with gradual fuzzy boundaries rather than discrete crisp numeric ones mamdani 1977 mamdani and assilian 1975 it mimicks an analytical decision making process similar to human thinking shrestha and simonovic 2010 zadeh 1965 this method handles data imprecision and provides the ability to deal naturally with the vagueness of information qualitative and linguistic data input through an easily interpretable rule based system using inference methods the fuzzy inference method allows to reach an integrative conclusion deduced from a collection of imprecise premises by using if then rules e g pal and mandal 1991 mamdani and takagi sugeno inferences systems are the most commonly used processes for treating a set of given inputs to an output fuzzy logic thus provides the capacity to efficiently deal with information from data compiled in a qualitative rather than quantitative way incomplete databases or imprecise knowledge ahmadi nedushan et al 2008 this ability is based on the integration of gradual changes in environmental variables allowing simultaneous membership of a given numerical crisp value to more than one linguistic fuzzy variable category at various degrees van broekhoven et al 2006 this advantage and the easy incorporation of expert knowledge using the mamdani inference system is one reason for the increasing popularity of this method in environmental sciences and ecosystem management table si1 indeed ecological processes 1 often follow continuous monotonic trends rather than discontinuous dichotomous jumps jorde et al 2001 kerle et al 2002 2 can spread over large scales or periods or 3 include varying often unknown and unpredicted sources of uncertainty like stochastic variation instrumental imprecision or subjectivity salski 1992 making fuzzy logic approaches particularly suitable to describe these processes when validated by field observations expert knowledge fuzzy models regularly provide high predictive power adriaenssens et al 2006 bock and salski 1998 mclaughlin et al 2006 however some models have significant uncertainties as highlighted by low correctly classified instances 50 muñoz mas et al 2016 high observed abundance associated with low predicted hsi fukuda et al 2011 theodoropoulos et al 2018a low coefficient of determination r2 small proportion of explained abundance beaupré et al 2020 or substantial difference between predictions and observed values mouton et al 2008 complex animal behaviour inter and intra annual environmental variability that strongly influences ecological processes and populations and stochastic events may partially explain model weaknesses weak model validation can also result from knowledge gaps of the habitat preferences of specific taxa and the use of invalid validation protocols or ill developed models expert knowledge strengths and weaknesses have been mainly reviewed by drescher et al 2013 surprisingly beyond the recurrent misunderstandings and misconceptions about this method zadeh 2008 few criticisms exist about the application of the method itself e g mendoza and martins 2006 therefore important pitfalls remain regarding fuzzy logic application for hsi and are yet to be addressed table 1 using non discrete classification offers a clear advantage when studying ecological processes that rarely follow a dichotomic trend singh et al 2013 zadeh 1965 2008 however such methods need to be analyzed to improve their robustness and decrease the uncertainties associated with model development as the application of fuzzy logic quickly spreads in animal habitat modelling supplementary table s1 we use fuzzy logic for fish habitat modelling as a case study to improve modelling relying on expert based knowledge and provide examples for the discussion points we provide a critical overview of the methodological pitfalls that may affect the performance of fuzzy models using mamdani inference system but applicable to any knowledge based methods we then propose solutions to each issue to reduce negative influences on model outcomes and increase the model accuracy more specifically based on a brief review of fuzzy logic method developments we discuss inherent challenges related to 1 the definition of model variables inputs to be used by experts 2 the consideration of expert disagreement and potential misunderstandings 3 the sources of inherent uncertainties of fuzzy logic mechanisms such as input variables fuzzy sets and rules definitions and 4 the handling of extremes conditions and model complexity we finally propose solutions to improve the accuracy and robustness of methods such as fuzzy logic or bayesian inference that rely on expert knowledge 2 fuzzy logic rudiments to better understand the sources of uncertainties inherent to the method itself we briefly remind here how data are fuzzy transformed and processed to return a numeric model output the first fuzzy logic step consists of transforming numerical data into fuzzy sets a process called fuzzification this step requires a precise definition of each set based on expert knowledge to determine membership functions and boundaries fig 1 for each set of every variable membership functions are commonly simple combinations of linear functions in trapezoidal or triangular shapes allowing for overlap between consecutive sets to overlap and thus allowing for a value to belong to two sets with a different degree of membership four boundary values describe the trapezoidal membership function a1 a2 a3 and a4 representing an initial increase of the degree of membership from 0 to 1 between a1 and a2 followed by a constant membership value between a2 and a3 the core and a final decrease between a3 and a4 fig 1 the range from a1 to a4 constitutes the support while the increasing and decreasing part of the function are the boundaries triangular functions are a particular case of trapezoidal functions where a2 a3 the increasing and decreasing parts of the membership functions allow for consecutive sets to overlap a consequent concludes each rule e g if variable 1 is moderate and if variable 2 is moderate and if variable 3 is low then the hsi is high according to mamdani inference system the determination of a rule s conclusion called implication derives the minimum membership value among the triggered sets of all variables and clips the conclusion membership functions to this minimum value if more than one rule is triggered the clipped membership functions of every involved rule are then aggregated finally the last step of the fuzzy model can also be a source of error depending on the methodological choice the conclusion is initially represented by one or two fuzzy sets and needs to be transformed into a crisp single numerical output through the defuzzification process although various defuzzification methods exist bisector of area mean of maximum smallest of maximum weighted average the most common is the center of gravity method which selects the value associated with the center of gravity of the final fuzzy set output naaz et al 2011 3 a critical analysis of the fuzzy model process following the expert based fuzzy logic model development steps we develop a decision framework to demonstrate and discuss potential pitfalls and underlying sources of uncertainties linked with the fuzzy inference process fig 2 from this framework we discuss the importance of expert knowledge selection and validation the implications of excessively broad variables and sets and potential benefits of comparing different defuzzification methods combining critical aspects that have not been previously discussed or comprehensively reviewed e g drescher et al 2013 table 2 although our framework uses fuzzy logic habitat modelling as an example it applies to different methods relying on expert based knowledge such as expert systems decision trees or bayesian analysis gelman et al 2013 its flexibility allows to use multiple forms of information as inputs in different types of expert based models indeed mixing qualitative and quantitative sources of information is common and using more vague or imprecise inputs as it is sometimes the only available information the proposed methodological steps can improve the way the information is used and combined to inform models as it also deals with the randomness and imprecision of information in a modular way our framework can guide existing risk assessment systems and understand sources of uncertainties in general it ensures that the application of expert based knowledge models is carried out in a consistent and well managed manner therefore promoting the reduction of biases and a better understanding of how uncertainties can affect the model outputs especially regarding the model s robustness 3 1 expert s knowledge and disagreement identifying selecting and retaining experts to compile their knowledge for model development is a tedious task burgman et al 2011 knol et al 2010 experts recruitment and involvement issues commitment disengagement etc represent challenges similar to other knowledge based models drescher et al 2013 fuzzy modelling typically requires expert information at two levels 1 to define the fuzzy sets boundary values for each input variable and 2 to determine the consequents of every if then rule successful implementation of these tasks relies on adequate incorporation of expert knowledge information obtained from a systematic review or a mix of both the first challenge of this step is inherent to the expert knowledge specific attributes local vs global applied vs theoretical field vs lab based and the different perceptions surrounding the definition of the expert itself as previously discussed by drescher et al 2013 furthermore individual bias perception experiences etc will influence the experts understanding of the model s variables and processes possible divergence in the perceptions and opinions of experts may result from the fact that expert knowledge is fundamentally the sum of professional training education field experience and personal experiences fazey et al 2006 ford and sterman 1997 huesemann 2002 krueger et al 2012 that may influence model outputs e g the geographic area of expertise mocq et al 2013 this unique combination of experiences explains why each expert processes the same data set differently to solve a problem and provide a different answer to the same situation table 2 fig 3 the consequences of divergent opinions in a fuzzy model depend on the extent of these disagreements gradual transitions between sets in fuzzy logic systems can relax disagreements on the fuzzy set definition selection while larger disagreements will provide different outputs strong disagreements in fuzzy logic are a two sided issue while they can be a source of uncertainty they can also be a valuable source of information providing insights into the natural ecological variability aspinall 2010 knol et al 2010 as a potential source of disagreement local knowledge could be valuable information for generic large scale models with a purposely broad range of conditions and applicability knol et al 2010 at the risk of introducing irrelevant data for the model e g anecdotal knowledge or its context of application e g potential non transferable site specific knowledge increasing the uncertainty of the model divergent opinions e g differences in experts perceptions of the consequences of interactions between physical variables on habitat quality are eventually expressed in the definitions of the fuzzy sets and rules based on the same rule divergent definitions of fuzzy sets will lead to divergent model outputs fig 3 like two different conclusions for one rule working with identical fuzzy sets simultaneous divergent membership function boundaries and divergent rule conclusions can exacerbate or reduce the differences of the outputs the inclusion of these divergent opinions may be a source of serious concern for a modeller since it strongly influences the model s results increases uncertainties and may lead to question the expert s judgment meyer and booker 1991 julien mocq et al 2013 mocq et al 2015 the quality of the expert s knowledge the rigour of the knowledge collection and the elicitation method i e the method used to merge and process the knowledge data influence the magnitude of the disagreement and directly affect the model accuracy reliability and quality unfortunately the consequences of disagreement for model outputs have been barely addressed in the literature mocq et al 2013 3 1 1 identifying and managing disagreement each expert is usually considered individually and disagreements among them are therefore easy to identify for instance disagreements in fuzzy set definition can be directly observable by comparing the membership functions boundary values the area occupied by the functions and the extent of overlaps between consecutive sets in mocq et al 2013 2015 the experts defined four values of trapezoidal membership function boundaries for substrate size mm using three fuzzy sets small medium large fig 2 the sets boundary values chosen by the experts for a given variable differed sharply table 2 and resulted in entirely different non overlapping fuzzy sets in extreme cases the range of a single set defined by a particular expert encompassed two or more sets as defined by other experts if the rules may attenuate the influence of such a significant difference the domain of variability covered by the fuzzy sets can be broad and different enough to influence the model s final outputs significantly the direct comparison of rule consequents may be an easy and straightforward solution to assess the ratio of agreement between experts highest agreements between experts occur when defining extreme results like excellent or unsuitable habitat julien mocq et al 2013 radinger et al 2017 and high disagreements could be interpreted as unsure knowledge or discussed opinions the disagreement in rules consequents can be directly evaluated when fuzzy sets are similar between experts or predetermined and common for every expert to develop the fuzzy rules these predefined fuzzy sets can be a consensual system of sets among experts or imposed by the modeller with the risk of the disappearance of marginal opinions and however a direct comparison is meaningless when the experts worked on deeply different or non overlapping fuzzy sets for instance two fuzzy sets determining medium substrate size can reflect two different perceptions of the substrate size e g 20 110 mm for the first expert 130 200 mm for the second leading to differences in the sets definitions e g 20 110 mm seen as the medium set for the first expert and as the small set for the second one here the comparison of experts disagreement solely based on the rules is erroneous since the discrepancy between the fuzzy sets in interaction with the different rule conclusions could eventually provide similar fuzzy outputs in the case of non overlapping sets or independently defined sets by the experts the comparison of the model outputs remains the most robust method to assess the outcome of experts disagreement e g mocq et al 2015 every expert must understand the questions in the manner intended by the modeller clear wordings with strict and unambiguous definitions can avoid different interpretations due to personal experiences cultural background or language differences knol et al 2010 giving each expert a clear data set with definitions and instructions that do not leave room for personal interpretation for instance the sets of substrate size can be defined from an objective physical perspective in which the grain size is considered regardless of other variables e g life stage fish size or function or it may be considered within a functional perspective in which fuzzy sets are defined in regards to an ecological function e g rearing or spawning mocq et al 2013 beaupré et al 2020 these two perspectives will inevitably produce different fuzzy set values and rule conclusions and may explain the differences between the set definitions of the experts in fig 3 the functional perspective often facilitates the definition of the sets it was probably the default perspective of the majority of the experts involved in mocq et al 2013 2015 when an odd number of sets is proposed usually 3 or 5 the central medium set was spontaneously considered as the optimal range of values for a targeted function considering the others sets as less adequate for this function i e too slow shallow and too fast deep mechanically triggering this set more often e g beaupré et al 2020 and systematically related to the highest habitat quality conclusion mocq et al 2013 the modeller must clearly state the understanding of the problem and the definition of the fuzzy sets despite explicit instruction slightly divergent interpretations may occur their consequences are difficult to test and their influence remains vastly unknown we therefore emphasize the importance of clear statements and instructions for the experts to minimize or even avoid this bias 3 2 variables sets and inference number vs precision the choice of the number of variables and sets influences the model performance assuming an equal number of fuzzy sets per variable and every possible combination the number of fuzzy rules follows the formula mn with m being the number of fuzzy sets and n the number of variables this power relation results in a sharp increase in the number of rules for which the expert has to determine a consequent strategic choices in the habitat variable selection can avoid an absurd number of rules to manage as the amount of information that an expert can simultaneously integrate to provide a synthetic consequent constitutes the first limitation indeed an expert can mentally deal simultaneously with a limited number of items meyer and booker 1991 handling more than five variables at the same time and determining the different influences of each of them into a single consequent is a challenge therefore it is vital to consider the trade off between additional variables and sets that may improve the predictive power and the precision of the developed model and reduce model accuracy due to unsure habitat suitability definition fukuda et al 2011 too many variables and sets can also increase the experts risk of fatigue and loss of motivation related to the number of rules and consequents the experts fatigue and boredom are mainly experienced when demanding tasks result in unsatisfactory result outcomes reed 2008 with four observed consequences 1 mistakes in the data sets e g inversion of numbers filling in the wrong column during the knowledge collection 2 increase of the inaccuracy e g hasty filling of the survey to finish quickly resulting in a difference of data quality between the first data sets and the last ones and 3 desertion abandonment of the exercise each of these consequences is detrimental to the model and need to be considered when consulting the experts finally the last issue linked to the increase in the number of variables and sets is the multiplication of impossible or rare situations for instance in ouellet 2013 the association of deep fast flow and extremely high water temperatures was one possible combination but never occurred in the st lawrence river experts have frequently to determine the consequences of a combination that will not or rarely occur in the real world in these cases they have to determine or guess at the best of their possibility an unknown outcome 3 2 1 avoiding confusion and boredom several strategies can limit the number of rules and sets and consequently the risk of experts boredom a wisely cogitated knowledge collection protocol can reduce the experts fatigue avoidance of correlated variables restriction of variable and set number use of the operator or in rules to avoid the multiplying of and rules the influence of one set of a given variable may override the influence of other variables therefore determining the consequent for the entire rule independently of the other variables in this case the use of one short rule retaining only the significant parameter will cover every combination involving it reducing the total number of rules for instance if low flow velocity systematically leads to low habitat suitability whatever the sets of the other variables involved the rule if velocity is low then habitat quality is low will cover every combination triggering the low velocity indeed experts tend to choose the consequence by selecting one or a few variables of the rule that have the strongest or the most known influence on the output variable when looking at the salmon rearing and spawning habitat most experts considered the harmful impact of fine substrates in salmon habitat and almost always choose the most conservative consequence mocq et al 2013 the operator or can also reduce the number of rules by condensing the rules leading to the same consequence e g if velocity is low or if depth is deep then habitat quality is low both solutions imply that the expert can modify and create her his rules without the modeller s supervision and consequently she he understands the fuzzy logic functioning 3 3 defuzzification defuzzification also requires attention since different methods can modify the outcome resulting in less or more accurate calculations theodoropoulos et al 2018b because of consistency and similarity in their outputs and the fact that defuzzification is based on the values of membership at all point of that set užga rebrovs and kuļ the most suitable methods are center of gravity cog and bisector or center area method coa naaz et al 2011 still one needs to be aware of their limitations our previous experiences highlighted some problems which could potentially affect the interpretation of the results ouellet 2013 theodoropoulos et al 2018a to our knowledge most studies do not address the bias in the model outputs introduced by choosing a particular defuzzification method over another additionally few studies compared the limitations and consequences of the different defuzzification methods in fuzzy logic which can significantly influence the model s outputs rotshtein and shtovba 2002 saletic et al 2002 indeed the chosen defuzzification method through the calculated indices should accurately represent the domain described by the fuzzy outputs first although one method gives the best results under specific sets it should not be assumed that it is always the case the cog exhibited a particular type of problem when the resulting polygon to be defuzzified has a trapezoidal form the resulting mathematical value is the same no matter how strong the membership value is ouellet 2013 the height of a trapezoidal polygon does not influence the corresponding cog value this affects the model outputs as the degree of membership matters and should change the final value score also when the data trigger only one rule with the extreme sets as a consequent i e the final aggregated shape corresponds to the fuzzy sets of one of the two extreme sets of the output variables the output values reach a maximum corresponding to the cog of the trapezoid not the real limit values of the universe of discourse the first and last sets of premises and consequents corresponding to extreme conditions should be systematically defined by narrow sets i e the set support extending on a short range mechanically increasing the range of the universe of discourse of the variables reachable the cog fig 4 when the triggered rules imply the narrow set of the consequent its weight in the fuzzy consequent will pull the center of gravity toward the extremity consequently the model will provide values close to 0 overtaking the border effect of cog method and the model accuracy will be improved by widening the useable universe of discourse in general when applied to the trapezoidal shape the cog method systematically underestimates the final score even when the membership i e the high of the trapezoid increases we recommend a particular precaution using the cog with trapezoidal forms the choice of a defuzzification method should rely on understanding its influence on the results and its suitability to account for the membership value testing different defuzzification methods could be an option for instance the weighted average method and coa provide results close to cog but one should consider the complex computational procedures required by the coa method for his methods such as first of maxima method or middle of the maxima method are generally to avoid as the final hsi values are therefore overestimated therefore choosing a defuzzification method requires investigating how the resulting values calculated by the different techniques accurately describe the output base on the polygon shape it can involve more computational time and analysis but could potentially avoid the bias resulting from wildly applying the same method regardless of the functions to be defuzzified additionally runkler and glesner 1993 provided a list of properties to consider when choosing a method ranging from consistency invariance and compatibility criteria to exclude negative information thus in this section we emphasize the importance of considering alternative methods and testing them should be viewed as an application of the best practice methodology to improve the model performance 3 4 model performance evaluation according to zadeh s principle there is a trade off between the complexity of a problem and the precision in formulating conclusions on the question ruspini 2012 ruspini et al 1998 some combinations of variables may be rare or impossible handling more than one non normal random variable leads to an unwieldy computational burden and random variables found in extreme value analysis are hardly ever normal in this case the choice of the most conservative consequence or the deletion of rules describing impossible combinations will optimize the rules definition and computing time however determining which rules are unrealistic becomes harder when numerous variables are involved and require a deeper understanding of the considered habitat in mocq et al 2013 every possible variable velocity depth and substrate size combination of salmon rearing and spawning habitat was triggered at least once depending on how the experts defined the fuzzy sets and the range of each variable in this case any missing rule would have affected the model s robustness so improbable and impossible rules need to be differentiated to optimize the model without compromising its robustness if a modeller has sufficient knowledge of the topic there is a possibility to rely on their judgment only to identify which rules can be improbable however it is preferable to rely on optimization methods as the modeller knowledge can introduce the same source of errors associated with the expert knowledge mentioned in section 3 1 studying the convergence of the rules can optimize the model by determining which variables are the most meaningful to the targeted species and avoid redundancy e g fukuda et al 2011 this optimization can also be addressed by studying the collinearity between habitat variables at the selection step some genetic fuzzy models bai and wang 2006 fukuda et al 2011 takagi and sugeno 1985 avoid the effect of unnecessary variables therefore reducing the number of combination and consequences by using a do not care condition that ignores the combinations that do not lead to compensation another option for the rare combination is using the or operator to cover the domain of the deleted rules one can also look at the more recent advances in bayesian computational techniques using adapted monte carlo markov chain algorithms to overcome these challenges other methods such as cross validation can also be an option for optimizing fuzzy model finally the inference step can dismiss these rules by using a specific threshold yen 1999 all these different optimization strategies require a fair understanding of the involved mechanism and an in depth analysis of the data sets before the modelling variable selection and during the construction of the fuzzy model convergence rules influence consequent influence to determine efficient optimization strategies unique to each fuzzy model from this perspective the consequences of different optimization strategies must be compared to understand how they improve or not the model s performance and accuracy although we use the application of fuzzy logic for habitat modelling to exemplify the points discussed in this paper our proposed solutions apply more generally to every knowledge based models such as expert systems decision trees or bayesian analysis gelman et al 2013 indeed bayesian analysis uses the probabilistic and inferential reasoning of the experts to represent their knowledge and automate their reasoning adlouni et al 2018 as for the fuzzy logic the interest of bayesian analysis also lies in their ability to take into account in the same model several types of information subjective information from experts and that from statistical data processing chen and pollino 2012 but share some of similar pitfalls more generally our proposed methodological framework applies to different types of models dealing with the combination of several sources of information that are often qualitative or imprecise such as risk management applied to flood for example our recommendations also apply to methods for which similar reflection on defining optimal thresholds to stop gathering information to reduce the uncertainty has been made li et al 2013 these approaches used for fuzzy models are the same for combining and eliciting expert opinions elicitation makes it possible to transform the different forms of information the combination is made by mathematical formulas linked to means variances skewness or shape of the prior distributions discussion sessions between experts also make it possible to find an agreement or consensus for a good representation of the uncertainty 4 conclusion although we discuss a method suitable for linear ecological changes we want to acknowledge that not all ecological patterns follow such trends moreover understanding the discontinuities can provide more critical knowledge of ecological phenomena as it is also essential to understand non linear dynamics feedback loops and multiple stable equilibria so we can better what trigger changes in a system e g decrease in biodiversity and how to mitigate these changes e g restoration effort to improve biodiversity therefore the choice of habitat modelling methods firmly rests on the type of trends being studied here we highlighted important aspects of the fuzzy inference systems which can impact the model s performance robustness and results however it is essential to highlight common pitfalls as they alter the reliability of the expert knowledge elicitation or the models performance raise awareness of those issues and propose solutions that may improve the model development every model has limitations and improving the awareness of the expert knowledge based limitations helps to avoid recurrent mistakes observed between studies furthermore we need to remember that those are just models which in any case have to be adapted as research advances rajabi and ataie ashtiani 2016 ultimately we recommend the following when developing expert knowledge based fuzzy models 1 formulating unambiguous instructions to avoid non informative disagreements and misunderstandings of the task s processes and goals 2 systematically assessing experts disagreements and adapting the model based on the derived uncertainty 3 limiting the number of input habitat variables a maximum of four 4 considering and comparing the accuracy of different defuzzification methods 5 defining of narrow extreme fuzzy sets to widen the universe of discourse 6 evaluating the model performances in relation to expert inputs and rare unnecessary fuzzy rules and 7 using carefully short rules and the or operator as much as possible to limit the number and the complexity of the rules if more than four variables are required the fuzzy logic model could be data driven rather than based on expert knowledge e g mouton et al 2009 consider a hybrid approach such as neuro fuzzy inference e g sedighkia et al 2021 or using another approach for the hsi that does not involve the use of rules and the associated problems as highlighted when using fuzzy logic following these simples but yet crucial guidelines will improve the knowledge validation reduce the sources of errors and improve the overall model performance our framework provides a valuable guide for future fuzzy modelling applications relying on expert knowledge to reduce uncertainty and bias and improving model accuracy and robustness declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors are thankful to rafael muñoz mas for providing feedback on the manuscript they also express their most heartfelt thanks to andre st hilaire who has been a tremendous mentor and for his support of dr mocq s and dr ouellet s academic careers appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105138 
25749,previous criticisms of knowledge based fuzzy logic modelling have identified some of its limitations and revealed weaknesses regarding the development of fuzzy sets the integration of expert knowledge and the outcomes of different defuzzification processes we show here how expert disagreement and fuzzy logic mechanisms associated with the rule development and combinations can positively or adversely affect model performance and the interpretation of results we highlight how expert disagreement can induce uncertainty into model outputs when defining fuzzy sets and selecting a defuzzification method we present a framework to account for sources of error and bias and improve the performance and robustness of knowledge based fuzzy logic models we recommend to 1 provide clear unambiguous instructions on model development processes and objectives including the definition of input variables and fuzzy sets 2 incorporate the disagreement among experts into the analysis 3 increase the use of short rules and the or operator to reduce complexity and 4 improve model performance and robustness by using narrow fuzzy sets for extreme values of input variables to expand the universe of discourse adequately our framework is focused on fuzzy logic models but can be applied to all knowledge based models that require expert judgment including expert systems decision trees and fuzzy bayesian inference systems keywords fuzzy logic critic expert knowledge model optimization decision framework 1 introduction fuzzy logic is a knowledge based modelling method that uses linguistic variables obtained from human operators and or expert knowledge as model inputs with gradual fuzzy boundaries rather than discrete crisp numeric ones mamdani 1977 mamdani and assilian 1975 it mimicks an analytical decision making process similar to human thinking shrestha and simonovic 2010 zadeh 1965 this method handles data imprecision and provides the ability to deal naturally with the vagueness of information qualitative and linguistic data input through an easily interpretable rule based system using inference methods the fuzzy inference method allows to reach an integrative conclusion deduced from a collection of imprecise premises by using if then rules e g pal and mandal 1991 mamdani and takagi sugeno inferences systems are the most commonly used processes for treating a set of given inputs to an output fuzzy logic thus provides the capacity to efficiently deal with information from data compiled in a qualitative rather than quantitative way incomplete databases or imprecise knowledge ahmadi nedushan et al 2008 this ability is based on the integration of gradual changes in environmental variables allowing simultaneous membership of a given numerical crisp value to more than one linguistic fuzzy variable category at various degrees van broekhoven et al 2006 this advantage and the easy incorporation of expert knowledge using the mamdani inference system is one reason for the increasing popularity of this method in environmental sciences and ecosystem management table si1 indeed ecological processes 1 often follow continuous monotonic trends rather than discontinuous dichotomous jumps jorde et al 2001 kerle et al 2002 2 can spread over large scales or periods or 3 include varying often unknown and unpredicted sources of uncertainty like stochastic variation instrumental imprecision or subjectivity salski 1992 making fuzzy logic approaches particularly suitable to describe these processes when validated by field observations expert knowledge fuzzy models regularly provide high predictive power adriaenssens et al 2006 bock and salski 1998 mclaughlin et al 2006 however some models have significant uncertainties as highlighted by low correctly classified instances 50 muñoz mas et al 2016 high observed abundance associated with low predicted hsi fukuda et al 2011 theodoropoulos et al 2018a low coefficient of determination r2 small proportion of explained abundance beaupré et al 2020 or substantial difference between predictions and observed values mouton et al 2008 complex animal behaviour inter and intra annual environmental variability that strongly influences ecological processes and populations and stochastic events may partially explain model weaknesses weak model validation can also result from knowledge gaps of the habitat preferences of specific taxa and the use of invalid validation protocols or ill developed models expert knowledge strengths and weaknesses have been mainly reviewed by drescher et al 2013 surprisingly beyond the recurrent misunderstandings and misconceptions about this method zadeh 2008 few criticisms exist about the application of the method itself e g mendoza and martins 2006 therefore important pitfalls remain regarding fuzzy logic application for hsi and are yet to be addressed table 1 using non discrete classification offers a clear advantage when studying ecological processes that rarely follow a dichotomic trend singh et al 2013 zadeh 1965 2008 however such methods need to be analyzed to improve their robustness and decrease the uncertainties associated with model development as the application of fuzzy logic quickly spreads in animal habitat modelling supplementary table s1 we use fuzzy logic for fish habitat modelling as a case study to improve modelling relying on expert based knowledge and provide examples for the discussion points we provide a critical overview of the methodological pitfalls that may affect the performance of fuzzy models using mamdani inference system but applicable to any knowledge based methods we then propose solutions to each issue to reduce negative influences on model outcomes and increase the model accuracy more specifically based on a brief review of fuzzy logic method developments we discuss inherent challenges related to 1 the definition of model variables inputs to be used by experts 2 the consideration of expert disagreement and potential misunderstandings 3 the sources of inherent uncertainties of fuzzy logic mechanisms such as input variables fuzzy sets and rules definitions and 4 the handling of extremes conditions and model complexity we finally propose solutions to improve the accuracy and robustness of methods such as fuzzy logic or bayesian inference that rely on expert knowledge 2 fuzzy logic rudiments to better understand the sources of uncertainties inherent to the method itself we briefly remind here how data are fuzzy transformed and processed to return a numeric model output the first fuzzy logic step consists of transforming numerical data into fuzzy sets a process called fuzzification this step requires a precise definition of each set based on expert knowledge to determine membership functions and boundaries fig 1 for each set of every variable membership functions are commonly simple combinations of linear functions in trapezoidal or triangular shapes allowing for overlap between consecutive sets to overlap and thus allowing for a value to belong to two sets with a different degree of membership four boundary values describe the trapezoidal membership function a1 a2 a3 and a4 representing an initial increase of the degree of membership from 0 to 1 between a1 and a2 followed by a constant membership value between a2 and a3 the core and a final decrease between a3 and a4 fig 1 the range from a1 to a4 constitutes the support while the increasing and decreasing part of the function are the boundaries triangular functions are a particular case of trapezoidal functions where a2 a3 the increasing and decreasing parts of the membership functions allow for consecutive sets to overlap a consequent concludes each rule e g if variable 1 is moderate and if variable 2 is moderate and if variable 3 is low then the hsi is high according to mamdani inference system the determination of a rule s conclusion called implication derives the minimum membership value among the triggered sets of all variables and clips the conclusion membership functions to this minimum value if more than one rule is triggered the clipped membership functions of every involved rule are then aggregated finally the last step of the fuzzy model can also be a source of error depending on the methodological choice the conclusion is initially represented by one or two fuzzy sets and needs to be transformed into a crisp single numerical output through the defuzzification process although various defuzzification methods exist bisector of area mean of maximum smallest of maximum weighted average the most common is the center of gravity method which selects the value associated with the center of gravity of the final fuzzy set output naaz et al 2011 3 a critical analysis of the fuzzy model process following the expert based fuzzy logic model development steps we develop a decision framework to demonstrate and discuss potential pitfalls and underlying sources of uncertainties linked with the fuzzy inference process fig 2 from this framework we discuss the importance of expert knowledge selection and validation the implications of excessively broad variables and sets and potential benefits of comparing different defuzzification methods combining critical aspects that have not been previously discussed or comprehensively reviewed e g drescher et al 2013 table 2 although our framework uses fuzzy logic habitat modelling as an example it applies to different methods relying on expert based knowledge such as expert systems decision trees or bayesian analysis gelman et al 2013 its flexibility allows to use multiple forms of information as inputs in different types of expert based models indeed mixing qualitative and quantitative sources of information is common and using more vague or imprecise inputs as it is sometimes the only available information the proposed methodological steps can improve the way the information is used and combined to inform models as it also deals with the randomness and imprecision of information in a modular way our framework can guide existing risk assessment systems and understand sources of uncertainties in general it ensures that the application of expert based knowledge models is carried out in a consistent and well managed manner therefore promoting the reduction of biases and a better understanding of how uncertainties can affect the model outputs especially regarding the model s robustness 3 1 expert s knowledge and disagreement identifying selecting and retaining experts to compile their knowledge for model development is a tedious task burgman et al 2011 knol et al 2010 experts recruitment and involvement issues commitment disengagement etc represent challenges similar to other knowledge based models drescher et al 2013 fuzzy modelling typically requires expert information at two levels 1 to define the fuzzy sets boundary values for each input variable and 2 to determine the consequents of every if then rule successful implementation of these tasks relies on adequate incorporation of expert knowledge information obtained from a systematic review or a mix of both the first challenge of this step is inherent to the expert knowledge specific attributes local vs global applied vs theoretical field vs lab based and the different perceptions surrounding the definition of the expert itself as previously discussed by drescher et al 2013 furthermore individual bias perception experiences etc will influence the experts understanding of the model s variables and processes possible divergence in the perceptions and opinions of experts may result from the fact that expert knowledge is fundamentally the sum of professional training education field experience and personal experiences fazey et al 2006 ford and sterman 1997 huesemann 2002 krueger et al 2012 that may influence model outputs e g the geographic area of expertise mocq et al 2013 this unique combination of experiences explains why each expert processes the same data set differently to solve a problem and provide a different answer to the same situation table 2 fig 3 the consequences of divergent opinions in a fuzzy model depend on the extent of these disagreements gradual transitions between sets in fuzzy logic systems can relax disagreements on the fuzzy set definition selection while larger disagreements will provide different outputs strong disagreements in fuzzy logic are a two sided issue while they can be a source of uncertainty they can also be a valuable source of information providing insights into the natural ecological variability aspinall 2010 knol et al 2010 as a potential source of disagreement local knowledge could be valuable information for generic large scale models with a purposely broad range of conditions and applicability knol et al 2010 at the risk of introducing irrelevant data for the model e g anecdotal knowledge or its context of application e g potential non transferable site specific knowledge increasing the uncertainty of the model divergent opinions e g differences in experts perceptions of the consequences of interactions between physical variables on habitat quality are eventually expressed in the definitions of the fuzzy sets and rules based on the same rule divergent definitions of fuzzy sets will lead to divergent model outputs fig 3 like two different conclusions for one rule working with identical fuzzy sets simultaneous divergent membership function boundaries and divergent rule conclusions can exacerbate or reduce the differences of the outputs the inclusion of these divergent opinions may be a source of serious concern for a modeller since it strongly influences the model s results increases uncertainties and may lead to question the expert s judgment meyer and booker 1991 julien mocq et al 2013 mocq et al 2015 the quality of the expert s knowledge the rigour of the knowledge collection and the elicitation method i e the method used to merge and process the knowledge data influence the magnitude of the disagreement and directly affect the model accuracy reliability and quality unfortunately the consequences of disagreement for model outputs have been barely addressed in the literature mocq et al 2013 3 1 1 identifying and managing disagreement each expert is usually considered individually and disagreements among them are therefore easy to identify for instance disagreements in fuzzy set definition can be directly observable by comparing the membership functions boundary values the area occupied by the functions and the extent of overlaps between consecutive sets in mocq et al 2013 2015 the experts defined four values of trapezoidal membership function boundaries for substrate size mm using three fuzzy sets small medium large fig 2 the sets boundary values chosen by the experts for a given variable differed sharply table 2 and resulted in entirely different non overlapping fuzzy sets in extreme cases the range of a single set defined by a particular expert encompassed two or more sets as defined by other experts if the rules may attenuate the influence of such a significant difference the domain of variability covered by the fuzzy sets can be broad and different enough to influence the model s final outputs significantly the direct comparison of rule consequents may be an easy and straightforward solution to assess the ratio of agreement between experts highest agreements between experts occur when defining extreme results like excellent or unsuitable habitat julien mocq et al 2013 radinger et al 2017 and high disagreements could be interpreted as unsure knowledge or discussed opinions the disagreement in rules consequents can be directly evaluated when fuzzy sets are similar between experts or predetermined and common for every expert to develop the fuzzy rules these predefined fuzzy sets can be a consensual system of sets among experts or imposed by the modeller with the risk of the disappearance of marginal opinions and however a direct comparison is meaningless when the experts worked on deeply different or non overlapping fuzzy sets for instance two fuzzy sets determining medium substrate size can reflect two different perceptions of the substrate size e g 20 110 mm for the first expert 130 200 mm for the second leading to differences in the sets definitions e g 20 110 mm seen as the medium set for the first expert and as the small set for the second one here the comparison of experts disagreement solely based on the rules is erroneous since the discrepancy between the fuzzy sets in interaction with the different rule conclusions could eventually provide similar fuzzy outputs in the case of non overlapping sets or independently defined sets by the experts the comparison of the model outputs remains the most robust method to assess the outcome of experts disagreement e g mocq et al 2015 every expert must understand the questions in the manner intended by the modeller clear wordings with strict and unambiguous definitions can avoid different interpretations due to personal experiences cultural background or language differences knol et al 2010 giving each expert a clear data set with definitions and instructions that do not leave room for personal interpretation for instance the sets of substrate size can be defined from an objective physical perspective in which the grain size is considered regardless of other variables e g life stage fish size or function or it may be considered within a functional perspective in which fuzzy sets are defined in regards to an ecological function e g rearing or spawning mocq et al 2013 beaupré et al 2020 these two perspectives will inevitably produce different fuzzy set values and rule conclusions and may explain the differences between the set definitions of the experts in fig 3 the functional perspective often facilitates the definition of the sets it was probably the default perspective of the majority of the experts involved in mocq et al 2013 2015 when an odd number of sets is proposed usually 3 or 5 the central medium set was spontaneously considered as the optimal range of values for a targeted function considering the others sets as less adequate for this function i e too slow shallow and too fast deep mechanically triggering this set more often e g beaupré et al 2020 and systematically related to the highest habitat quality conclusion mocq et al 2013 the modeller must clearly state the understanding of the problem and the definition of the fuzzy sets despite explicit instruction slightly divergent interpretations may occur their consequences are difficult to test and their influence remains vastly unknown we therefore emphasize the importance of clear statements and instructions for the experts to minimize or even avoid this bias 3 2 variables sets and inference number vs precision the choice of the number of variables and sets influences the model performance assuming an equal number of fuzzy sets per variable and every possible combination the number of fuzzy rules follows the formula mn with m being the number of fuzzy sets and n the number of variables this power relation results in a sharp increase in the number of rules for which the expert has to determine a consequent strategic choices in the habitat variable selection can avoid an absurd number of rules to manage as the amount of information that an expert can simultaneously integrate to provide a synthetic consequent constitutes the first limitation indeed an expert can mentally deal simultaneously with a limited number of items meyer and booker 1991 handling more than five variables at the same time and determining the different influences of each of them into a single consequent is a challenge therefore it is vital to consider the trade off between additional variables and sets that may improve the predictive power and the precision of the developed model and reduce model accuracy due to unsure habitat suitability definition fukuda et al 2011 too many variables and sets can also increase the experts risk of fatigue and loss of motivation related to the number of rules and consequents the experts fatigue and boredom are mainly experienced when demanding tasks result in unsatisfactory result outcomes reed 2008 with four observed consequences 1 mistakes in the data sets e g inversion of numbers filling in the wrong column during the knowledge collection 2 increase of the inaccuracy e g hasty filling of the survey to finish quickly resulting in a difference of data quality between the first data sets and the last ones and 3 desertion abandonment of the exercise each of these consequences is detrimental to the model and need to be considered when consulting the experts finally the last issue linked to the increase in the number of variables and sets is the multiplication of impossible or rare situations for instance in ouellet 2013 the association of deep fast flow and extremely high water temperatures was one possible combination but never occurred in the st lawrence river experts have frequently to determine the consequences of a combination that will not or rarely occur in the real world in these cases they have to determine or guess at the best of their possibility an unknown outcome 3 2 1 avoiding confusion and boredom several strategies can limit the number of rules and sets and consequently the risk of experts boredom a wisely cogitated knowledge collection protocol can reduce the experts fatigue avoidance of correlated variables restriction of variable and set number use of the operator or in rules to avoid the multiplying of and rules the influence of one set of a given variable may override the influence of other variables therefore determining the consequent for the entire rule independently of the other variables in this case the use of one short rule retaining only the significant parameter will cover every combination involving it reducing the total number of rules for instance if low flow velocity systematically leads to low habitat suitability whatever the sets of the other variables involved the rule if velocity is low then habitat quality is low will cover every combination triggering the low velocity indeed experts tend to choose the consequence by selecting one or a few variables of the rule that have the strongest or the most known influence on the output variable when looking at the salmon rearing and spawning habitat most experts considered the harmful impact of fine substrates in salmon habitat and almost always choose the most conservative consequence mocq et al 2013 the operator or can also reduce the number of rules by condensing the rules leading to the same consequence e g if velocity is low or if depth is deep then habitat quality is low both solutions imply that the expert can modify and create her his rules without the modeller s supervision and consequently she he understands the fuzzy logic functioning 3 3 defuzzification defuzzification also requires attention since different methods can modify the outcome resulting in less or more accurate calculations theodoropoulos et al 2018b because of consistency and similarity in their outputs and the fact that defuzzification is based on the values of membership at all point of that set užga rebrovs and kuļ the most suitable methods are center of gravity cog and bisector or center area method coa naaz et al 2011 still one needs to be aware of their limitations our previous experiences highlighted some problems which could potentially affect the interpretation of the results ouellet 2013 theodoropoulos et al 2018a to our knowledge most studies do not address the bias in the model outputs introduced by choosing a particular defuzzification method over another additionally few studies compared the limitations and consequences of the different defuzzification methods in fuzzy logic which can significantly influence the model s outputs rotshtein and shtovba 2002 saletic et al 2002 indeed the chosen defuzzification method through the calculated indices should accurately represent the domain described by the fuzzy outputs first although one method gives the best results under specific sets it should not be assumed that it is always the case the cog exhibited a particular type of problem when the resulting polygon to be defuzzified has a trapezoidal form the resulting mathematical value is the same no matter how strong the membership value is ouellet 2013 the height of a trapezoidal polygon does not influence the corresponding cog value this affects the model outputs as the degree of membership matters and should change the final value score also when the data trigger only one rule with the extreme sets as a consequent i e the final aggregated shape corresponds to the fuzzy sets of one of the two extreme sets of the output variables the output values reach a maximum corresponding to the cog of the trapezoid not the real limit values of the universe of discourse the first and last sets of premises and consequents corresponding to extreme conditions should be systematically defined by narrow sets i e the set support extending on a short range mechanically increasing the range of the universe of discourse of the variables reachable the cog fig 4 when the triggered rules imply the narrow set of the consequent its weight in the fuzzy consequent will pull the center of gravity toward the extremity consequently the model will provide values close to 0 overtaking the border effect of cog method and the model accuracy will be improved by widening the useable universe of discourse in general when applied to the trapezoidal shape the cog method systematically underestimates the final score even when the membership i e the high of the trapezoid increases we recommend a particular precaution using the cog with trapezoidal forms the choice of a defuzzification method should rely on understanding its influence on the results and its suitability to account for the membership value testing different defuzzification methods could be an option for instance the weighted average method and coa provide results close to cog but one should consider the complex computational procedures required by the coa method for his methods such as first of maxima method or middle of the maxima method are generally to avoid as the final hsi values are therefore overestimated therefore choosing a defuzzification method requires investigating how the resulting values calculated by the different techniques accurately describe the output base on the polygon shape it can involve more computational time and analysis but could potentially avoid the bias resulting from wildly applying the same method regardless of the functions to be defuzzified additionally runkler and glesner 1993 provided a list of properties to consider when choosing a method ranging from consistency invariance and compatibility criteria to exclude negative information thus in this section we emphasize the importance of considering alternative methods and testing them should be viewed as an application of the best practice methodology to improve the model performance 3 4 model performance evaluation according to zadeh s principle there is a trade off between the complexity of a problem and the precision in formulating conclusions on the question ruspini 2012 ruspini et al 1998 some combinations of variables may be rare or impossible handling more than one non normal random variable leads to an unwieldy computational burden and random variables found in extreme value analysis are hardly ever normal in this case the choice of the most conservative consequence or the deletion of rules describing impossible combinations will optimize the rules definition and computing time however determining which rules are unrealistic becomes harder when numerous variables are involved and require a deeper understanding of the considered habitat in mocq et al 2013 every possible variable velocity depth and substrate size combination of salmon rearing and spawning habitat was triggered at least once depending on how the experts defined the fuzzy sets and the range of each variable in this case any missing rule would have affected the model s robustness so improbable and impossible rules need to be differentiated to optimize the model without compromising its robustness if a modeller has sufficient knowledge of the topic there is a possibility to rely on their judgment only to identify which rules can be improbable however it is preferable to rely on optimization methods as the modeller knowledge can introduce the same source of errors associated with the expert knowledge mentioned in section 3 1 studying the convergence of the rules can optimize the model by determining which variables are the most meaningful to the targeted species and avoid redundancy e g fukuda et al 2011 this optimization can also be addressed by studying the collinearity between habitat variables at the selection step some genetic fuzzy models bai and wang 2006 fukuda et al 2011 takagi and sugeno 1985 avoid the effect of unnecessary variables therefore reducing the number of combination and consequences by using a do not care condition that ignores the combinations that do not lead to compensation another option for the rare combination is using the or operator to cover the domain of the deleted rules one can also look at the more recent advances in bayesian computational techniques using adapted monte carlo markov chain algorithms to overcome these challenges other methods such as cross validation can also be an option for optimizing fuzzy model finally the inference step can dismiss these rules by using a specific threshold yen 1999 all these different optimization strategies require a fair understanding of the involved mechanism and an in depth analysis of the data sets before the modelling variable selection and during the construction of the fuzzy model convergence rules influence consequent influence to determine efficient optimization strategies unique to each fuzzy model from this perspective the consequences of different optimization strategies must be compared to understand how they improve or not the model s performance and accuracy although we use the application of fuzzy logic for habitat modelling to exemplify the points discussed in this paper our proposed solutions apply more generally to every knowledge based models such as expert systems decision trees or bayesian analysis gelman et al 2013 indeed bayesian analysis uses the probabilistic and inferential reasoning of the experts to represent their knowledge and automate their reasoning adlouni et al 2018 as for the fuzzy logic the interest of bayesian analysis also lies in their ability to take into account in the same model several types of information subjective information from experts and that from statistical data processing chen and pollino 2012 but share some of similar pitfalls more generally our proposed methodological framework applies to different types of models dealing with the combination of several sources of information that are often qualitative or imprecise such as risk management applied to flood for example our recommendations also apply to methods for which similar reflection on defining optimal thresholds to stop gathering information to reduce the uncertainty has been made li et al 2013 these approaches used for fuzzy models are the same for combining and eliciting expert opinions elicitation makes it possible to transform the different forms of information the combination is made by mathematical formulas linked to means variances skewness or shape of the prior distributions discussion sessions between experts also make it possible to find an agreement or consensus for a good representation of the uncertainty 4 conclusion although we discuss a method suitable for linear ecological changes we want to acknowledge that not all ecological patterns follow such trends moreover understanding the discontinuities can provide more critical knowledge of ecological phenomena as it is also essential to understand non linear dynamics feedback loops and multiple stable equilibria so we can better what trigger changes in a system e g decrease in biodiversity and how to mitigate these changes e g restoration effort to improve biodiversity therefore the choice of habitat modelling methods firmly rests on the type of trends being studied here we highlighted important aspects of the fuzzy inference systems which can impact the model s performance robustness and results however it is essential to highlight common pitfalls as they alter the reliability of the expert knowledge elicitation or the models performance raise awareness of those issues and propose solutions that may improve the model development every model has limitations and improving the awareness of the expert knowledge based limitations helps to avoid recurrent mistakes observed between studies furthermore we need to remember that those are just models which in any case have to be adapted as research advances rajabi and ataie ashtiani 2016 ultimately we recommend the following when developing expert knowledge based fuzzy models 1 formulating unambiguous instructions to avoid non informative disagreements and misunderstandings of the task s processes and goals 2 systematically assessing experts disagreements and adapting the model based on the derived uncertainty 3 limiting the number of input habitat variables a maximum of four 4 considering and comparing the accuracy of different defuzzification methods 5 defining of narrow extreme fuzzy sets to widen the universe of discourse 6 evaluating the model performances in relation to expert inputs and rare unnecessary fuzzy rules and 7 using carefully short rules and the or operator as much as possible to limit the number and the complexity of the rules if more than four variables are required the fuzzy logic model could be data driven rather than based on expert knowledge e g mouton et al 2009 consider a hybrid approach such as neuro fuzzy inference e g sedighkia et al 2021 or using another approach for the hsi that does not involve the use of rules and the associated problems as highlighted when using fuzzy logic following these simples but yet crucial guidelines will improve the knowledge validation reduce the sources of errors and improve the overall model performance our framework provides a valuable guide for future fuzzy modelling applications relying on expert knowledge to reduce uncertainty and bias and improving model accuracy and robustness declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors are thankful to rafael muñoz mas for providing feedback on the manuscript they also express their most heartfelt thanks to andre st hilaire who has been a tremendous mentor and for his support of dr mocq s and dr ouellet s academic careers appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105138 
