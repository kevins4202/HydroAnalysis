index,text
24790,parameterization is one of the most challenging steps in the construction of individual based models and it is particularly relevant for the case of dynamic energy budget deb theory given that deb parameters are mapped to a multimodal fitness landscape this multimodal fitness landscape could correspond to parameterizations that provide the right outcome for the wrong reasons given the lack of available data to directly parameterize some aspects of deb models mathematical tools are becoming the state of the art approach to estimate or refine unknown parameters the aim of this study is to explore the use of a novel mathematical algorithm that recognizes the multimodal nature of the fitness landscape as a way to provide alternative equally good parameterizations for deb models the multimodal optimization for model calibration momca framework was used to calibrate a deb model for the blue mussel mytilus edulis using datasets that included environmental information growth and physiological rates the inclusion of physiological rates an uncommon approach in deb parameterization allowed for constraining the range of solutions and reducing parameter uncertainty the application of the momca framework allowed for the identification of the energy acquisition sub model as one of the top priorities for improving the mechanistic understanding of mussel bioenergetics and consequently for enhancing model performance the momca framework could complement the standard procedures to estimate deb parameters keywords dynamic energy budget individual based model mytilus edulis multimodal fitness landscape model calibration introduction dynamic energy budget deb theory kooijman 2010 mechanistically explains individual bioenergetics throughout the life cycle under dynamic environmental conditions building on thermodynamics first principles and assuming that the mechanisms responsible for the organization of individual metabolism are not species specific sousa et al 2008 deb theory can be applied to all species the potential of such a mechanistic theory has led to an increase in its popularity in the scientific literature being applied to more than 2 000 species add my pet https www bio vu nl thb deb deblab add my pet among which bivalves are one of the most studied group e g pouvreau et al 2006 rosland et al 2009 despite the successful application of the theory deb has been criticized for not being efficient due to the large number of parameters namely 14 in its standard versions and the fact that most of them are species specific marquet et al 2014 the complex and data demanding procedure to estimate the value of these parameters lika et al 2011a may also limit its expansion contrarily it has been argued that deb is efficient given the large number of processes that can be derived from those parameters for instance life cycle development feeding growth maintenance metabolic heating reproduction and senescence kearney et al 2015 the large suite of predictions that can be derived is precisely one of the strengths of deb implementation in ecosystem scale models e g guyondet et al 2010 the need to parameterize such a relatively large number of parameters most of them species specific is one of the most challenging aspects of deb modelling early on the parameterization of deb relied on the empirical and independent approximation to each parameter e g van der veer et al 2006 mathematical calibration such as the use of the nelder mead method was also implemented to estimate the values of unknown parameters or to improve model fit in populations of the same species e g bacher and gangnery 2006 a breakthrough in deb parameterization came with the introduction of the covariation method lika et al 2011a a non linear least squares regression approach lika et al 2011a suggested that the most robust parameterization would be to simultaneously estimate all deb parameters by minimizing the discrepancies between simulations and all available observations these available observations range from single data points that represent a single value zero variate data in deb jargon such as maximum reproduction rate to arrays of values uni variate data such as growth over time this method also takes into account the inter dependency of some parameters avoiding potentially incoherent combinations of parameters in addition lika et al 2011a used deb theory principles to constraint the potential range of solutions and introduced the concept of pseudo data which are well known values for a generalized animal that could be scaled to the modelled species the use of pseudo data has demonstrated to be beneficial to exploit existing knowledge about parameter values that cannot be extracted from observations and it constitutes a key component to confine potential solutions to those that make sense biologically marques et al 2019 marques et al 2019 has replaced lika et al 2011a as the standard procedure to estimate deb parameters by adding more advanced loss functions and filters to prevent parameter sets that are not compatible with the general principles of the theory despite this progress there are still major challenges in deb parameter estimation which are common to most mechanistic models in biology for example interindividual variability is included through population level averages but not directly included in the parameter estimation and similarly the uncertainty in the estimated parameters is not usually considered johnson et al 2013 some mathematical approaches have already included this uncertainty by applying a state space method to determine the probability distribution of the estimated parameters fujiwara et al 2005 similarly the use of a bayesian inference framework allows for the consideration of this uncertainty johnson et al 2013 and it has already been used to estimate deb parameters boersch supan and johnson 2018 another challenge for a robust estimation in deb is related to the large number of parameters that could lead to the curse of dimensionality type of problems jusup et al 2017 the challenge of dimensionality is that an increase in the number of parameters requires an exponential increase in available empirical data to avoid sparsity and obtain parameters that are statistically significant this problem is magnified by the fact that some deb parameters covariate fujiwara et al 2005 chica et al 2017 suggesting that different combinations of values can result in the same solution which could lead to the simulation of the right solutions for the wrong reasons for example the same growth could be observed with high feeding and high metabolic costs or with low feeding and low costs therefore the advantage of deb regarding the potential to simulate a large variety of processes could become a weakness if the available datasets do not allow for a meaningful parameterization that addresses this multimodal optimization problem understood as a problem without a unique global optimum solution but multiple optima either global or local ehrgott 2005 the multidimensional space of deb parameters together with data gaps could be considered one of the major hurdles for deb parameterization traditionally the uni variate data used for deb calibration focus on growth curves biometric data and datasets that can inform about the effect of temperature on the physiology of the individual among these datasets long term growth curves are usually preferred given the relative simplicity to collect them their availability in the literature and the straightforward calculation of growth using deb outputs but they lack the power to inform about short term individual responses recent optimization algorithms consider the coexistence of model parameters that are mapped to a multimodal fitness landscape e g momca framework chica et al 2017 they aim to simultaneously locate more than one optimal solution subsequently such approaches represent the nature of deb theory and they could thus be an ideal way to move forward on deb parameterization by providing alternative solutions rather than a single one in this study a combined dataset strohmeier et al 2015 of growth and physiological rates respiration and clearance rates of the mussel mytilus edulis from two locations of the lysefjord norway was used to parameterize a deb model using a novel optimization approach that considers that deb parameters are mapped to a multimodal fitness landscape chica et al 2017 the inclusion of physiological rates in the parameterization aims to constrain the range of parameters and inform about short term physiological responses the ultimate goal of this study is not to provide a generalized set of parameters for m edulis but to demonstrate that this novel optimization can provide alternative solutions that could identify knowledge gaps and research priorities for further improvement of deb parameterization furthermore the provision of alternative equally good solutions becomes an ideal complement to the standard procedures to estimate deb parameters marques et al 2019 material and methods dynamic energy budget model deb theory describes the energy of an individual in terms of three state variables reserve s structure s and maturity reproduction in brief the assimilated energy is stored as reserves a fixed fraction of the mobilized energy κ is then directed towards maintenance and growth of the structural body and the remainder 1 κ is directed towards maturity maintenance and maturation or gamete production depending on the life cycle stage of the organism fig 1 a description of model equations is presented in table 1 and a detailed explanation is provided in pouvreau et al 2006 and rosland et al 2009 the mathematical formulation follows the original notation by kooijman 2010 in which denotes quantities expressed as per unit structural volume denotes quantities expressed as per unit surface area and a dot over a symbol denotes a rate or a dimension per time note that the current model aims to simulate the life cycle from the juvenile stage onwards and not the whole life cycle the constraint k j k m maturity maintenance rate coefficient somatic maintenance rate coefficient has also been implemented in the model to facilitate the comparison with previous deb models this simplification is not part of the current standard deb model but it was used by the three main papers in which deb was parameterized for m edulis van der veer et al 2006 rosland et al 2009 and saraiva et al 2011a in addition the model allowed the use of energy in reproduction buffer when energy in reserves was not enough to pay maintenance costs the model has been forced with observed temperature and using chlorophyll concentration µg l 1 as a proxy for food density spawning which implied the full release of the reproduction buffer was triggered manually on specific dates based on field observations respiration and clearance rates were modeled as follows for simplicity respiration rate was assumed to be proportional to the mobilization rate of reserve energy ṗc j d 1 table 1 following pouvreau et al 2006 and guyondet et al 2010 this simplification introduces uncertainty in the estimated value by assuming that the role of feeding in respiration is also proportional to ṗc but it avoids the estimation of an additional parameter κp the faecation efficiency accordingly the respiration rate was estimated as follows 1 r r deb p c 13 8 where rrdeb is the respiration rate estimated with deb mg o2 d 1 ṗc the mobilization rate of reserve energy j d 1 and 13 8 j mg o2 1 a conversion factor following guyondet et al 2010 for the purpose of comparison with strohmeier et al 2015 the respiration rate was standardized to an equivalent individual of 1 g dry tissue weight as follows 2 r r std r r deb d w std d w deb b where rrstd and rrdeb are the respiration rates standardized to a 1 g dry tissue weight individual and estimated with deb respectively dwstd and dwdeb are 1 g and estimated dry tissue weights respectively and b is the allometric exponent 0 7 smaal et al 1997 clearance rate was derived from the assimilation rate ṗa j d 1 table 1 the assimilation efficiency κx table 1 and the food density x table 1 but expressed in j l 1 as follows 3 c r deb p a kx x where crdeb is the clearance rate estimated with deb l d 1 ṗa the assimilation rate j d 1 κx the assimilation efficiency dimensionless and x the food density expressed in j l 1 assuming a carbon to chlorophyll ratio of 50 1 filgueira et al 2019 and a conversion factor of 21 j mg c 1 filgueira et al 2019 for the purpose of comparison with strohmeier et al 2015 the rate was standardized to an equivalent individual of 5 cm shell length as follows 4 c r std c r deb s l std s l deb b where crstd and crdeb are the clearance rates standardized to a 5 cm individual and estimated with deb respectively slstd and sldeb are 5 cm and estimated shell lengths respectively and b is the allometric exponent 2 09 jones et al 1992 note that respiration rate has been standardized to dry tissue weight but clearance rate has been standardized to shell length while respiration rate is related to the weight of an individual clearance rate is related to the surface of the gills therefore the standardization of clearance rate to shell length is recommended given that gill surface area and shell length are not affected by the fluctuation of weight over time due to the reproduction cycle filgueira et al 2008 furthermore strohmeier et al 2015 standardized both rates following the same approach and using the same allometric exponents which facilitates the comparison among both studies strohmeier datasets the m edulis strohmeier dataset strohmeier et al 2015 was chosen because of the availability of growth shell length dry weight and ecophysiological data clearance and respiration rates on a bi weekly or monthly basis for several cohorts over long periods of time and under different environmental conditions these experiments were carried out in lysefjord norway during 2010 and 2011 note that strohmeier et al 2015 reports data only for 2010 however the environmental conditions bivalve growth and physiological data collected in 2011 followed the same methodology the mussels from the same local population were deployed at 7 m depth at two sampling locations using 1 m long lantern nets at an approximate stocking density of 80 mussels per m2 individually marked individuals were used to monitor shell length during the experiment and additional individuals were sacrificed at each sampling for weight monitoring one of the sampling locations was sited close to the head of the fjord and the other 14 km downstream at the station close to the head of the fjord a pump was used to bring to the surface nutrient rich deep waters and enhance primary productivity see aure et al 2007 for technical details this design allows to have two sampling stations with very similar salinity and temperature 27 8 and 29 7 psu and 11 7 and 12 3 c under upwelling and non upwelling conditions respectively but statistically different food density 2 9 1 8 and 1 4 0 4 µg chla l 1 under upwelling and non upwelling conditions respectively in summary mussel growth from the same local population was monitored over two years 2010 and 2011 under two different environmental conditions upwelling conditions up in tables and figures and normal fjordic conditions noup in tables and figures three different cohorts were used 2008 coh08 2009 coh09 used twice over two consecutive years and 2010 coh10 a first and second year class cohorts y1 and y2 with approximate shell lengths of 2 4 and 4 7 cm respectively were monitored each year 2010 and 2011 at each site up and noup totaling eight datasets table 2 physiological data is only available for the second year class cohorts all methodological aspects are explained in detail in strohmeier et al 2015 optimization procedure the multimodal optimization for model calibration momca framework chica et al 2017 was used to parameterize deb models for the different datasets momca assumes that the parameters of the model are mapped to a multimodal fitness landscape and applies search algorithms to find the set of sub optimal configurations that better fit the observed data in an optimization search space each potential solution is linked to the values for all decision variables defined by the user these decision variables define the quality of each solution according to the fitness function i e minimize the distance between prediction and observation meeting a predefined constraint such as meeting mass conservation which ultimately determines if the solution belongs to the set of multimodal optima from a graphical perspective these solutions to a multimodal landscape would be peaks in a maximization problem and valleys in a minimization problem in such a multimodal problem several sub optimal solutions make it difficult to find a unique and optimal set of parameters goldberg and richardson 1987 the momca framework facilitates the calibration of a model by providing alternative solutions and tools to understand the relationships among parameters and the sensitivity of the model to changes in these parameters a brief description of the momca framework is given below and a more detailed description is available in chica et al 2017 in this work the momca framework applies a traditional niching genetic algorithm back 1997 for finding different sets of parameters for the deb model such that the predictions obtained by the calibrated model have similar quality in the first phase of momca the algorithm starts with a set of different possible calibrations for the model known as the population of the algorithm and tests the model s accuracy under each of them a niching method known as clearing is then applied to the solutions of the population this method sorts the solutions in ascending order according to their accuracy later the method keeps the α many calibration solutions α being the niche capacity with the best accuracy within a clearing radius and discards the rest of the population in other words if several calibration solutions are too close to each other in the decision space the α best ranked solutions will be taken into account and the others will be discarded afterwards the method launches an iterative process in which the best calibration solutions are selected based on their fitness by using a stochastic universal sampling technique baker 1987 the selected calibrations are recombined using a well known blx crossover operator herrera 1998 and a reset mutation is also applied to increase the search diversity of the algorithm the offspring from the latter two operators constitute the population in the next generation and the loop continues until the set of obtained calibrations converges for the second phase of the momca framework and after a preliminary analysis of the method behavior a filtering mechanism is introduced to ensure the diversity of calibrations for this work parameter values were normalized within their given ranges and calibration solutions were considered diverse enough if the euclidean distance between every pair of calibrations is at least 0 1 the third phase includes a quantitative and visual analysis for understanding the set of parameters in summary momca tackles the calibration in three automated steps 1 search for alternative sets of parameters with equally good fitting of observed data and diverse values 2 evaluation of the performances of the different sets of parameters and prioritization based on fitting and diversity and 3 quantitative and visual sensitivity analysis to understand the robustness of the set of parameters the momca framework was used to calibrate 11 deb parameters the range of the parameters was defined based on parameters from van der veer et al 2006 rosland et al 2009 and saraiva et al 2011a the ranges were larger than reported in these studies to ensure that the confidence intervals of these parameters were included as potential solutions the set of potential solutions was restricted by limiting the yield of structure on reserve yve to values below 0 8 otherwise certain combination of parameters can result in over efficient conversion from reserves into structure that violates the principle of mass conservation saraiva et al 2011a in addition no shrinking of structure was allowed in the model furthermore the individual was assumed to be dead if maintenance costs cannot be paid the deb model was calibrated using only the 8 datasets described above i e no complementary zero or uni variate data was used two calibration modes were tested to explore the potential of a single parameterization for all datasets global calibration or the need for specific calibration for each individual dataset local calibration global calibration the 4 datasets with physiological and growth data were used for calibration purposes and the remaining 4 datasets were used for validation table 2 different weights were assigned to the different types of measurements 0 5 to shell length 0 3 to dry weight which is more affected by seasonality and reproductive cycle than shell length and 0 1 to respiration and clearance rate which are discrete data points in time rather than integrated measurements over time such as shell length or dry tissue weight local calibration all 8 datasets were calibrated independently using a slightly larger range for some of the 11 parameters in order to increase the potential range of solutions to improve the fitness in the case of the 4 datasets in which only growth data are available weights of 0 625 and 0 375 were used for shell length and dry weight respectively the goodness of fit err of each set of parameters was calculated as follows 5 e rr i n β i y i y i y i where n is the total number of observations βi is the relative weight coefficient for each observation considering the type of data i e shell length dry weight or respiration and clearance rate and the number of observations within each type and yi and ŷi are the observation and the model prediction respectively results global calibration the best momca set of parameters for the simultaneous global optimization of the four datasets used for calibration table 2 averaged an err of 0 115 table 3 with a total of 3 and 15 solutions within a range from the best solution of 0 001 and 0 002 respectively the fitness diverged among the different types of measurements shell length was the best predicted observation and clearance rate the worst with an average err of 0 007 and 0 390 respectively table 3 as expected the fitness of each type of observation is correlated to the weight that is given to each observation in the optimization process the fitness dropped to an average of 0 151 table 3 for the four datasets used for validation table 2 it is important to highlight that the validation datasets do not include any information on physiological rates which reported the worst fit in the calibration datasets and consequently the fitness of calibration and validation datasets must be compared with caution the 10 best set of momca parameters estimated with the global calibration table 4 revealed that three of the parameters xk κ and dw were within the range of parameters from the literature fig 2 two of them ta and em in the lower part of the range fig 2 and the remaining six parameters ṗxm ṗm δm eg κx and κr in the upper part fig 2 the variation of the estimated parameters within the best 10 momca solutions ranged between 0 5 and 11 5 for κx and eg respectively the calibration dataset up coh08 y2 was used to visualize the fitness over time fig 3 the match between the predictions of the global calibration and observations remained similar over time for shell length fig 3a however there was a clear mismatch in dry weight and both physiological rates during june which coincides with a spawning event the model overestimated dry weight at the beginning of the month followed by an underestimation at the end fig 3b before and after june the predictions matched observations suggesting that the model did not properly capture the dynamics of the spawning event meanwhile the observed clearance rate was correctly predicted by the model at the beginning of june and underestimated by the end fig 3c respiration rate followed the opposite pattern with underestimations at the beginning and good predictions at the end fig 3d it is important to note that in both situations in which the model underestimated physiological rates the observed values were the highest ones for the whole time series this pattern was observed to some degree in all second year class cohorts initial shell length 4 5 cm table 2 in both growing areas with and without upwelling results not shown local calibration the use of momca for local calibration of each dataset improved the fitness of the model table 5 as expected this was more evident for the datasets that were used as validation datasets in the global calibration in which the fitness improved from an err of 0 155 down to 0 051 in the case of the datasets that were used for calibration the fitness also improved when using the local calibration but only from 0 115 to 0 103 similar to the global calibration shell length and clearance rate were the best and worst predicted observations with an average err of 0 009 and 0 367 respectively table 5 regarding the specific fit of the data for dataset up coh08 y2 the local calibration improved the fit from an err of 0 108 up to 0 095 this improvement was most obvious in the case of respiration rate motivated by a steeper change in respiration during the month of june fig 3d despite this improvement in predicting the short term changes in metabolism the model was not able to fully capture the pattern observed during june the best set of parameters for the local calibrations differed among each dataset table 6 both parameters controlling the ingestion of food xk and ṗxm exhibited the highest variability across datasets 59 and 33 respectively another parameter that displayed a large variability was ṗm with a variation of 40 contrarily δm eg and κx showed the lowest variation across datasets with values 7 it is important to note that the potential range of ṗxm ṗm and κ were expanded compared to the global calibration to increase the probability of improving the fitness of the solutions in the case of ṗxm the average value across the different datasets 499 j cm 2 d 1 was higher than the upper threshold used in the global calibration 300 j cm 2 d 1 which had been defined based on literature values regarding ṗm and κ although some optimized values exceeded the originally pre defined range of values based on existing literature the average values for all the datasets did not exceed that range discussion selecting the correct parameters of a model is key for its successful application the multidimensional space in which deb parameters coexist increases the complexity of parameterization furthermore different parametrizations are typically mapped to equally optimal solutions i e a multimodal landscape in this study the application of a novel mathematical tool together with the combination of growth and physiological data was used to estimate new sets of parameters for the mytilus edulis deb model in a norwegian fjord this novel mathematical tool provides a range of equally good solutions which captures the fact that different physiological strategies can result in similar growth patterns existing sets of parameters deb models for m edulis have been parameterized multiple times in the scientific literature with van der veer et al 2006 rosland et al 2009 and saraiva et al 2011a being the most relevant contributions although saraiva et al 2011a had correctly pointed out that the parameters from van der veer et al 2006 and rosland et al 2009 violate the principle of mass conservation they have been used in this study to define potential ranges of parameters and for comparative purposes the estimated parameters using the momca framework to strohmeier et al 2015 were farther from saraiva et al 2011a than from van der veer et al 2006 and rosland et al 2009 a plausible explanation for this finding could be related to the fact that saraiva et al 2011a focused on wild populations of m edulis growing on the benthic zone but rosland et al 2009 focused on farmed populations cultivated suspended in the water column while van der veer et al 2006 used a mixed source of data it is well know that wild populations grow at a lower rate than farmed populations wild mussels are exposed to additional stressors such as currents waves and predators as well as a higher fluctuation in seston quantity and quality lachance et al 2008 these stressors require additional energy expenditures in wild mussels namely more byssus production babarro and carrington 2013 and investment in thicker shells beadman et al 2003 to cope with a high energy environment and predators respectively in addition despite the physiological plasticity of mussels when individuals from different origins are transferred to the same environment there is an acclimation period before reaching similar physiological rates labarta et al 1997 the mechanistic nature of deb should be able to cope with environmental changes over time but due to the simplicity that is required to construct a model not all environmental drivers are included in it in the current deb model only temperature and food are used as forcing functions and for example hydrodynamics and the presence of predators are not included despite being relevant for individual bioenergetics especially in the case of wild populations choosing different parameterization could be a simple solution for the use of the same modelling structure to simulate both wild and farmed populations this new parameterization would need to focus on the parameters affecting the physiology of the individual under different conditions e g byssus production shell thickness or inter and intra specific competition affecting food searching rate predicting growth and physiology in lysefjord the inclusion of physiological data in the parameterization process aims to constraint the range of valid solutions without constraining the parameters using physiological rates the model could correctly predict growth based on the combination of erroneous rates for example an individual with high ingestion and maintenance could result in similar growth to an individual with low ingestion and maintenance to a certain degree the pre defined range of parameters account for this e g the value of the maximum surface area specific ingestion rate ultimately determines the maximum clearance rate however the addition of physiological rates to the parameterization process provides information regarding the short term variability and plasticity of individuals in fact the mismatch between observations and predictions in clearance rate shortly after spawning suggest that the current version of the deb model cannot fully capture this short term physiological response this is expected given that clearance rate in deb is a function of food availability and temperature but strohmeier et al 2015 had already suggested that clearance rate is not correlated with seston characteristics in this specific dataset similar conclusions could be extracted for respiration rates although it is important to acknowledge the simplification in the mathematical formulation to estimate respiration rates from deb in general the overall fitness of the model in terms of growth and physiology for the validation datasets is within 15 error which is within the acceptable range expected for these types of exercises the punctual mismatches on physiological rates suggests that additional exogenous or endogenous drivers should be added to the deb model to properly simulate short term plasticity for example several studies have aimed to improve the characterization of food proxies that could result in a fully mechanistic ingestion model e g bourlès et al 2009 picoche et al 2014 similarly the use of synthesizing units is intended to improve this aspect of deb by accounting for the capacity of mussels to select food types based on their quality saraiva et al 2011b which inherently tackles an aspect often overlooked in deb modelling the digestive processes including differential assimilation efficiency of seston and gut passage time e g bayne et al 1987 most of the efforts for improvement are directed towards the understanding of exogenous drivers however the endogenous status could play a relevant role in short term physiological responses for example it has been suggested that an immediate decrease in filtration activity is observed after spawning as a mechanism to avoid the ingestion of gametes newell and thompson 1984 although the temporal frequency of strohmeier s dataset does not allow for testing such an immediate response the highest clearance observed in the study occurs a couple of weeks after spawning and coincides with a rapid re building of mussel s tissue strohmeier et al 2015 based on these results it could be hypothesized that spawning might trigger additional physiological responses and not only the release of gametes for example brigolin et al 2009 stopped the allocation of energy towards reproduction in m galloprovincialis after the second spawning diverting all energy towards somatic growth diverting energy towards somatic growth after spawning would not explain the observations in strohmeier s dataset using the current version of deb given that the observed increase in tissue is not concomitant with an increase in shell length which is mostly the case in deb theory with the exception of the role that reserves play on tissue mass shell is not directly modelled in the present deb model but is indirectly estimated based on the amount of energy allocated in the structure and the shape coefficient shell is considered in deb theory as a product of the growth flux proportional to growth overheads therefore some adjustments of the model would be required to account for the well known decoupling of shell and tissue growth observed in the literature e g hilbish 1986 which has already been incorporated in other modelling frameworks by adding additional energy allocation rules fuentes santos et al 2019 a dynamic allocation of energy between structure and reproduction and or additional allocation rules together with a change in energy acquisition triggered by an endogenous driver such as spawning could result in a better short term agreement between observations and simulations another option would be to model explicitly shell accretion pecquerie et al 2012 in any case empirical data at a higher temporal resolution would be needed to validate these structural changes in deb and to generate further hypotheses on other potential effects of spawning on the physiological of mussels deb as a multimodal problem the mathematical estimation of a set of parameters of a non linear dynamic model is a complex task automated calibration packages and search algorithms facilitate this process but the solution or solutions should be carefully analyzed or there is a risk of treating the model as a black box saleh et al 2010 this problem is amplified when the model operates in a multimodal environment which could result in multiple sub optimal solutions muñoz et al 2015 the challenge is magnified with the increase in the number of parameters jusup et al 2017 and the potential dependence among these parameters muñoz et al 2015 moya et al 2019 this is the case for deb in which the relatively large number of parameters kearney et al 2015 and the inter dependence of some of them fujiwara et al 2005 chica et al 2017 are mapped to a multimodal continuous landscape the existence of multiple sub optimal solutions is a challenge from the mathematical perspective but it could be an advantage from the biological perspective the analysis of sub optimal solutions that are close to the best solution can be used to inform about the impact of the parameters on model performance and the relationships among themselves in addition these sub optimal solutions could reflect the inter individual variability within a population which could reflect different physiological strategies that could result in similar outcomes in terms of growth usually the scaling up from individual to population in deb is carried out by simulating several individuals with different initial conditions namely length and dry weight e g gourault et al 2019 but keeping the same set of parameters it has been suggested in the past that future improvement in deb modelling could come from focusing on genetic and phenotypic traits to integrate inter and intra specific variability lika et al 2011b marques et al 2018 therefore the set of equally good solutions provided by the momca framework would not only provide valuable information regarding the parameters but also become a tool to improve the scaling up from individual based models to population model allowing a direct analysis on the set of returned solutions i e set of values to the calibrated variables of the model the challenge of the mathematical estimation of parameters is amplified by the methodological and logistic challenges to collect the appropriate datasets for calibration and validation despite the numerous scientific studies and monitoring programs tackling the growth of mussels and bivalves in general the number of available datasets with growth and environmental data collected at the right temporal resolution is still limited despite the numerous scientific studies and monitoring programs tackling the growth of mussels and bivalves in general the number of available datasets with growth and environmental data collected at the right temporal resolution is still limited for example one limitation in this study is the high dependence of three parameters xk ṗxm and κx on the information provided by clearance rate in this case the number of potential solutions can only be constrained by restricting the theoretical range of the parameters but additional information would be needed to fully disentangle their inter dependency in any case the information from multiple equally good solutions from the momca framework can also be used to define future priorities for data collection in this regard the high variability in the half saturation constant xk and the maximum surface area specific ingestion rate ṗxm which average value is higher than the upper threshold reported in the literature suggests that further research should focus on food ingestion despite the long history of studies on mussel feeding behaviour e g bayne et al 1987 riisgard et al 2011 it seems that these types of studies together with an improved characterization of seston are still critical to improve our understanding of the trophic interactions of bivalves similar to the need of data at a higher temporal resolution for physiological responses the characterization of seston for the identification of food proxies should also be carried out at a more detailed temporal resolution from a practical perspective of modelling parameterization it is essential to emphasize that momca solutions similarly to all automatic optimization tools need to be considered carefully by the user as it was stated above the upper limit for the maximum surface area specific ingestion rate ṗxm was reached in five out of ten solutions in the global calibration and was reached twice in the local calibration of strohmeier datasets while physiological plasticity is well known in bivalves especially regarding feeding activity values of 600 j cm 2 d 1 seem unlikely the user must make a decision to prioritize more physiologically relevant solutions compared to others this highlights the necessity of a critical evaluation of the optimized parameter set this can be done as in marques et al 2019 where the combination of pseudo data and a series of filters ensures that the solutions are compatible with deb theory e g ensuring mass conservation additional functionalities could also be included in momca to inform the user for example when a parameter is reaching the boundary of the pre defined range and is having too much impact on the solutions an alert could be displayed subsequently the optimization could automatically explore the implications of increasing the range of the parameter in terms of fitness something that has been manually done in this study the implementation of these filters and functionalities in momca is beyond the scope of this study while these upgrades could be included in momca a better solution would be incorporating momca into the current standard procedure to estimate deb parameters developed by marques et al 2019 this approach would complement the state of the art deb theory with the power of multimodal optimization e g equally good solutions and confidence intervals for parameters including momca in the standard procedures to estimate deb parameters i e marques et al 2019 could provide an additional tool to scope the potential parameter range this would bring together multimodal optimization with the benefits from the use of pseudo data relevant filters to ensure the compatibility with deb theory user friendly tools and extensive learning materials and still allowing control on the tuning of the deb parameters conclusions individual based models such as deb are critical tools to explore the current understanding of bivalve bioenergetics these models are also cornerstone in ecosystem modelling for example in models that explore the effects of aquaculture on the environment or the suitability of an area for aquaculture development however the parameterization of these models is challenging in this study the momca framework a novel multimodal optimization approach has been used to parameterize deb for mytilus edulis in a norwegian fjord the inclusion of physiological rates to constraint the range of the parameters together with the analysis of multiple equally good solutions rather than a single optimal one suggest that 1 deb can effectively simulate mussel growth and physiology in the long term but punctual physiological changes such as the response to spawning can compromise the performance of the model in the short term and 2 deb s energy acquisition sub model namely feeding and characterization of seston should be a priority for improving model performance these data gaps are bottlenecks for the ultimate goal of constructing a truly mechanistic model furthermore the successful use of momca has demonstrated the potential for complementing the current state of the art procedures to estimate deb parameters i e marques et al 2019 with multimodal optimization declaration of competing interests the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank two anonymous reviewers who provided a thorough and constructive critique of our work this work was funded by the research council of norway project no 196560 the institute of marine research project no 14898 exasoco pgc2018 101216 b i00 and nserc discovery grant to rf mc acknowledges the ramón y cajal program ryc 2016 19800 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j ecolmodel 2020 109139 appendix supplementary materials image application 1 
24790,parameterization is one of the most challenging steps in the construction of individual based models and it is particularly relevant for the case of dynamic energy budget deb theory given that deb parameters are mapped to a multimodal fitness landscape this multimodal fitness landscape could correspond to parameterizations that provide the right outcome for the wrong reasons given the lack of available data to directly parameterize some aspects of deb models mathematical tools are becoming the state of the art approach to estimate or refine unknown parameters the aim of this study is to explore the use of a novel mathematical algorithm that recognizes the multimodal nature of the fitness landscape as a way to provide alternative equally good parameterizations for deb models the multimodal optimization for model calibration momca framework was used to calibrate a deb model for the blue mussel mytilus edulis using datasets that included environmental information growth and physiological rates the inclusion of physiological rates an uncommon approach in deb parameterization allowed for constraining the range of solutions and reducing parameter uncertainty the application of the momca framework allowed for the identification of the energy acquisition sub model as one of the top priorities for improving the mechanistic understanding of mussel bioenergetics and consequently for enhancing model performance the momca framework could complement the standard procedures to estimate deb parameters keywords dynamic energy budget individual based model mytilus edulis multimodal fitness landscape model calibration introduction dynamic energy budget deb theory kooijman 2010 mechanistically explains individual bioenergetics throughout the life cycle under dynamic environmental conditions building on thermodynamics first principles and assuming that the mechanisms responsible for the organization of individual metabolism are not species specific sousa et al 2008 deb theory can be applied to all species the potential of such a mechanistic theory has led to an increase in its popularity in the scientific literature being applied to more than 2 000 species add my pet https www bio vu nl thb deb deblab add my pet among which bivalves are one of the most studied group e g pouvreau et al 2006 rosland et al 2009 despite the successful application of the theory deb has been criticized for not being efficient due to the large number of parameters namely 14 in its standard versions and the fact that most of them are species specific marquet et al 2014 the complex and data demanding procedure to estimate the value of these parameters lika et al 2011a may also limit its expansion contrarily it has been argued that deb is efficient given the large number of processes that can be derived from those parameters for instance life cycle development feeding growth maintenance metabolic heating reproduction and senescence kearney et al 2015 the large suite of predictions that can be derived is precisely one of the strengths of deb implementation in ecosystem scale models e g guyondet et al 2010 the need to parameterize such a relatively large number of parameters most of them species specific is one of the most challenging aspects of deb modelling early on the parameterization of deb relied on the empirical and independent approximation to each parameter e g van der veer et al 2006 mathematical calibration such as the use of the nelder mead method was also implemented to estimate the values of unknown parameters or to improve model fit in populations of the same species e g bacher and gangnery 2006 a breakthrough in deb parameterization came with the introduction of the covariation method lika et al 2011a a non linear least squares regression approach lika et al 2011a suggested that the most robust parameterization would be to simultaneously estimate all deb parameters by minimizing the discrepancies between simulations and all available observations these available observations range from single data points that represent a single value zero variate data in deb jargon such as maximum reproduction rate to arrays of values uni variate data such as growth over time this method also takes into account the inter dependency of some parameters avoiding potentially incoherent combinations of parameters in addition lika et al 2011a used deb theory principles to constraint the potential range of solutions and introduced the concept of pseudo data which are well known values for a generalized animal that could be scaled to the modelled species the use of pseudo data has demonstrated to be beneficial to exploit existing knowledge about parameter values that cannot be extracted from observations and it constitutes a key component to confine potential solutions to those that make sense biologically marques et al 2019 marques et al 2019 has replaced lika et al 2011a as the standard procedure to estimate deb parameters by adding more advanced loss functions and filters to prevent parameter sets that are not compatible with the general principles of the theory despite this progress there are still major challenges in deb parameter estimation which are common to most mechanistic models in biology for example interindividual variability is included through population level averages but not directly included in the parameter estimation and similarly the uncertainty in the estimated parameters is not usually considered johnson et al 2013 some mathematical approaches have already included this uncertainty by applying a state space method to determine the probability distribution of the estimated parameters fujiwara et al 2005 similarly the use of a bayesian inference framework allows for the consideration of this uncertainty johnson et al 2013 and it has already been used to estimate deb parameters boersch supan and johnson 2018 another challenge for a robust estimation in deb is related to the large number of parameters that could lead to the curse of dimensionality type of problems jusup et al 2017 the challenge of dimensionality is that an increase in the number of parameters requires an exponential increase in available empirical data to avoid sparsity and obtain parameters that are statistically significant this problem is magnified by the fact that some deb parameters covariate fujiwara et al 2005 chica et al 2017 suggesting that different combinations of values can result in the same solution which could lead to the simulation of the right solutions for the wrong reasons for example the same growth could be observed with high feeding and high metabolic costs or with low feeding and low costs therefore the advantage of deb regarding the potential to simulate a large variety of processes could become a weakness if the available datasets do not allow for a meaningful parameterization that addresses this multimodal optimization problem understood as a problem without a unique global optimum solution but multiple optima either global or local ehrgott 2005 the multidimensional space of deb parameters together with data gaps could be considered one of the major hurdles for deb parameterization traditionally the uni variate data used for deb calibration focus on growth curves biometric data and datasets that can inform about the effect of temperature on the physiology of the individual among these datasets long term growth curves are usually preferred given the relative simplicity to collect them their availability in the literature and the straightforward calculation of growth using deb outputs but they lack the power to inform about short term individual responses recent optimization algorithms consider the coexistence of model parameters that are mapped to a multimodal fitness landscape e g momca framework chica et al 2017 they aim to simultaneously locate more than one optimal solution subsequently such approaches represent the nature of deb theory and they could thus be an ideal way to move forward on deb parameterization by providing alternative solutions rather than a single one in this study a combined dataset strohmeier et al 2015 of growth and physiological rates respiration and clearance rates of the mussel mytilus edulis from two locations of the lysefjord norway was used to parameterize a deb model using a novel optimization approach that considers that deb parameters are mapped to a multimodal fitness landscape chica et al 2017 the inclusion of physiological rates in the parameterization aims to constrain the range of parameters and inform about short term physiological responses the ultimate goal of this study is not to provide a generalized set of parameters for m edulis but to demonstrate that this novel optimization can provide alternative solutions that could identify knowledge gaps and research priorities for further improvement of deb parameterization furthermore the provision of alternative equally good solutions becomes an ideal complement to the standard procedures to estimate deb parameters marques et al 2019 material and methods dynamic energy budget model deb theory describes the energy of an individual in terms of three state variables reserve s structure s and maturity reproduction in brief the assimilated energy is stored as reserves a fixed fraction of the mobilized energy κ is then directed towards maintenance and growth of the structural body and the remainder 1 κ is directed towards maturity maintenance and maturation or gamete production depending on the life cycle stage of the organism fig 1 a description of model equations is presented in table 1 and a detailed explanation is provided in pouvreau et al 2006 and rosland et al 2009 the mathematical formulation follows the original notation by kooijman 2010 in which denotes quantities expressed as per unit structural volume denotes quantities expressed as per unit surface area and a dot over a symbol denotes a rate or a dimension per time note that the current model aims to simulate the life cycle from the juvenile stage onwards and not the whole life cycle the constraint k j k m maturity maintenance rate coefficient somatic maintenance rate coefficient has also been implemented in the model to facilitate the comparison with previous deb models this simplification is not part of the current standard deb model but it was used by the three main papers in which deb was parameterized for m edulis van der veer et al 2006 rosland et al 2009 and saraiva et al 2011a in addition the model allowed the use of energy in reproduction buffer when energy in reserves was not enough to pay maintenance costs the model has been forced with observed temperature and using chlorophyll concentration µg l 1 as a proxy for food density spawning which implied the full release of the reproduction buffer was triggered manually on specific dates based on field observations respiration and clearance rates were modeled as follows for simplicity respiration rate was assumed to be proportional to the mobilization rate of reserve energy ṗc j d 1 table 1 following pouvreau et al 2006 and guyondet et al 2010 this simplification introduces uncertainty in the estimated value by assuming that the role of feeding in respiration is also proportional to ṗc but it avoids the estimation of an additional parameter κp the faecation efficiency accordingly the respiration rate was estimated as follows 1 r r deb p c 13 8 where rrdeb is the respiration rate estimated with deb mg o2 d 1 ṗc the mobilization rate of reserve energy j d 1 and 13 8 j mg o2 1 a conversion factor following guyondet et al 2010 for the purpose of comparison with strohmeier et al 2015 the respiration rate was standardized to an equivalent individual of 1 g dry tissue weight as follows 2 r r std r r deb d w std d w deb b where rrstd and rrdeb are the respiration rates standardized to a 1 g dry tissue weight individual and estimated with deb respectively dwstd and dwdeb are 1 g and estimated dry tissue weights respectively and b is the allometric exponent 0 7 smaal et al 1997 clearance rate was derived from the assimilation rate ṗa j d 1 table 1 the assimilation efficiency κx table 1 and the food density x table 1 but expressed in j l 1 as follows 3 c r deb p a kx x where crdeb is the clearance rate estimated with deb l d 1 ṗa the assimilation rate j d 1 κx the assimilation efficiency dimensionless and x the food density expressed in j l 1 assuming a carbon to chlorophyll ratio of 50 1 filgueira et al 2019 and a conversion factor of 21 j mg c 1 filgueira et al 2019 for the purpose of comparison with strohmeier et al 2015 the rate was standardized to an equivalent individual of 5 cm shell length as follows 4 c r std c r deb s l std s l deb b where crstd and crdeb are the clearance rates standardized to a 5 cm individual and estimated with deb respectively slstd and sldeb are 5 cm and estimated shell lengths respectively and b is the allometric exponent 2 09 jones et al 1992 note that respiration rate has been standardized to dry tissue weight but clearance rate has been standardized to shell length while respiration rate is related to the weight of an individual clearance rate is related to the surface of the gills therefore the standardization of clearance rate to shell length is recommended given that gill surface area and shell length are not affected by the fluctuation of weight over time due to the reproduction cycle filgueira et al 2008 furthermore strohmeier et al 2015 standardized both rates following the same approach and using the same allometric exponents which facilitates the comparison among both studies strohmeier datasets the m edulis strohmeier dataset strohmeier et al 2015 was chosen because of the availability of growth shell length dry weight and ecophysiological data clearance and respiration rates on a bi weekly or monthly basis for several cohorts over long periods of time and under different environmental conditions these experiments were carried out in lysefjord norway during 2010 and 2011 note that strohmeier et al 2015 reports data only for 2010 however the environmental conditions bivalve growth and physiological data collected in 2011 followed the same methodology the mussels from the same local population were deployed at 7 m depth at two sampling locations using 1 m long lantern nets at an approximate stocking density of 80 mussels per m2 individually marked individuals were used to monitor shell length during the experiment and additional individuals were sacrificed at each sampling for weight monitoring one of the sampling locations was sited close to the head of the fjord and the other 14 km downstream at the station close to the head of the fjord a pump was used to bring to the surface nutrient rich deep waters and enhance primary productivity see aure et al 2007 for technical details this design allows to have two sampling stations with very similar salinity and temperature 27 8 and 29 7 psu and 11 7 and 12 3 c under upwelling and non upwelling conditions respectively but statistically different food density 2 9 1 8 and 1 4 0 4 µg chla l 1 under upwelling and non upwelling conditions respectively in summary mussel growth from the same local population was monitored over two years 2010 and 2011 under two different environmental conditions upwelling conditions up in tables and figures and normal fjordic conditions noup in tables and figures three different cohorts were used 2008 coh08 2009 coh09 used twice over two consecutive years and 2010 coh10 a first and second year class cohorts y1 and y2 with approximate shell lengths of 2 4 and 4 7 cm respectively were monitored each year 2010 and 2011 at each site up and noup totaling eight datasets table 2 physiological data is only available for the second year class cohorts all methodological aspects are explained in detail in strohmeier et al 2015 optimization procedure the multimodal optimization for model calibration momca framework chica et al 2017 was used to parameterize deb models for the different datasets momca assumes that the parameters of the model are mapped to a multimodal fitness landscape and applies search algorithms to find the set of sub optimal configurations that better fit the observed data in an optimization search space each potential solution is linked to the values for all decision variables defined by the user these decision variables define the quality of each solution according to the fitness function i e minimize the distance between prediction and observation meeting a predefined constraint such as meeting mass conservation which ultimately determines if the solution belongs to the set of multimodal optima from a graphical perspective these solutions to a multimodal landscape would be peaks in a maximization problem and valleys in a minimization problem in such a multimodal problem several sub optimal solutions make it difficult to find a unique and optimal set of parameters goldberg and richardson 1987 the momca framework facilitates the calibration of a model by providing alternative solutions and tools to understand the relationships among parameters and the sensitivity of the model to changes in these parameters a brief description of the momca framework is given below and a more detailed description is available in chica et al 2017 in this work the momca framework applies a traditional niching genetic algorithm back 1997 for finding different sets of parameters for the deb model such that the predictions obtained by the calibrated model have similar quality in the first phase of momca the algorithm starts with a set of different possible calibrations for the model known as the population of the algorithm and tests the model s accuracy under each of them a niching method known as clearing is then applied to the solutions of the population this method sorts the solutions in ascending order according to their accuracy later the method keeps the α many calibration solutions α being the niche capacity with the best accuracy within a clearing radius and discards the rest of the population in other words if several calibration solutions are too close to each other in the decision space the α best ranked solutions will be taken into account and the others will be discarded afterwards the method launches an iterative process in which the best calibration solutions are selected based on their fitness by using a stochastic universal sampling technique baker 1987 the selected calibrations are recombined using a well known blx crossover operator herrera 1998 and a reset mutation is also applied to increase the search diversity of the algorithm the offspring from the latter two operators constitute the population in the next generation and the loop continues until the set of obtained calibrations converges for the second phase of the momca framework and after a preliminary analysis of the method behavior a filtering mechanism is introduced to ensure the diversity of calibrations for this work parameter values were normalized within their given ranges and calibration solutions were considered diverse enough if the euclidean distance between every pair of calibrations is at least 0 1 the third phase includes a quantitative and visual analysis for understanding the set of parameters in summary momca tackles the calibration in three automated steps 1 search for alternative sets of parameters with equally good fitting of observed data and diverse values 2 evaluation of the performances of the different sets of parameters and prioritization based on fitting and diversity and 3 quantitative and visual sensitivity analysis to understand the robustness of the set of parameters the momca framework was used to calibrate 11 deb parameters the range of the parameters was defined based on parameters from van der veer et al 2006 rosland et al 2009 and saraiva et al 2011a the ranges were larger than reported in these studies to ensure that the confidence intervals of these parameters were included as potential solutions the set of potential solutions was restricted by limiting the yield of structure on reserve yve to values below 0 8 otherwise certain combination of parameters can result in over efficient conversion from reserves into structure that violates the principle of mass conservation saraiva et al 2011a in addition no shrinking of structure was allowed in the model furthermore the individual was assumed to be dead if maintenance costs cannot be paid the deb model was calibrated using only the 8 datasets described above i e no complementary zero or uni variate data was used two calibration modes were tested to explore the potential of a single parameterization for all datasets global calibration or the need for specific calibration for each individual dataset local calibration global calibration the 4 datasets with physiological and growth data were used for calibration purposes and the remaining 4 datasets were used for validation table 2 different weights were assigned to the different types of measurements 0 5 to shell length 0 3 to dry weight which is more affected by seasonality and reproductive cycle than shell length and 0 1 to respiration and clearance rate which are discrete data points in time rather than integrated measurements over time such as shell length or dry tissue weight local calibration all 8 datasets were calibrated independently using a slightly larger range for some of the 11 parameters in order to increase the potential range of solutions to improve the fitness in the case of the 4 datasets in which only growth data are available weights of 0 625 and 0 375 were used for shell length and dry weight respectively the goodness of fit err of each set of parameters was calculated as follows 5 e rr i n β i y i y i y i where n is the total number of observations βi is the relative weight coefficient for each observation considering the type of data i e shell length dry weight or respiration and clearance rate and the number of observations within each type and yi and ŷi are the observation and the model prediction respectively results global calibration the best momca set of parameters for the simultaneous global optimization of the four datasets used for calibration table 2 averaged an err of 0 115 table 3 with a total of 3 and 15 solutions within a range from the best solution of 0 001 and 0 002 respectively the fitness diverged among the different types of measurements shell length was the best predicted observation and clearance rate the worst with an average err of 0 007 and 0 390 respectively table 3 as expected the fitness of each type of observation is correlated to the weight that is given to each observation in the optimization process the fitness dropped to an average of 0 151 table 3 for the four datasets used for validation table 2 it is important to highlight that the validation datasets do not include any information on physiological rates which reported the worst fit in the calibration datasets and consequently the fitness of calibration and validation datasets must be compared with caution the 10 best set of momca parameters estimated with the global calibration table 4 revealed that three of the parameters xk κ and dw were within the range of parameters from the literature fig 2 two of them ta and em in the lower part of the range fig 2 and the remaining six parameters ṗxm ṗm δm eg κx and κr in the upper part fig 2 the variation of the estimated parameters within the best 10 momca solutions ranged between 0 5 and 11 5 for κx and eg respectively the calibration dataset up coh08 y2 was used to visualize the fitness over time fig 3 the match between the predictions of the global calibration and observations remained similar over time for shell length fig 3a however there was a clear mismatch in dry weight and both physiological rates during june which coincides with a spawning event the model overestimated dry weight at the beginning of the month followed by an underestimation at the end fig 3b before and after june the predictions matched observations suggesting that the model did not properly capture the dynamics of the spawning event meanwhile the observed clearance rate was correctly predicted by the model at the beginning of june and underestimated by the end fig 3c respiration rate followed the opposite pattern with underestimations at the beginning and good predictions at the end fig 3d it is important to note that in both situations in which the model underestimated physiological rates the observed values were the highest ones for the whole time series this pattern was observed to some degree in all second year class cohorts initial shell length 4 5 cm table 2 in both growing areas with and without upwelling results not shown local calibration the use of momca for local calibration of each dataset improved the fitness of the model table 5 as expected this was more evident for the datasets that were used as validation datasets in the global calibration in which the fitness improved from an err of 0 155 down to 0 051 in the case of the datasets that were used for calibration the fitness also improved when using the local calibration but only from 0 115 to 0 103 similar to the global calibration shell length and clearance rate were the best and worst predicted observations with an average err of 0 009 and 0 367 respectively table 5 regarding the specific fit of the data for dataset up coh08 y2 the local calibration improved the fit from an err of 0 108 up to 0 095 this improvement was most obvious in the case of respiration rate motivated by a steeper change in respiration during the month of june fig 3d despite this improvement in predicting the short term changes in metabolism the model was not able to fully capture the pattern observed during june the best set of parameters for the local calibrations differed among each dataset table 6 both parameters controlling the ingestion of food xk and ṗxm exhibited the highest variability across datasets 59 and 33 respectively another parameter that displayed a large variability was ṗm with a variation of 40 contrarily δm eg and κx showed the lowest variation across datasets with values 7 it is important to note that the potential range of ṗxm ṗm and κ were expanded compared to the global calibration to increase the probability of improving the fitness of the solutions in the case of ṗxm the average value across the different datasets 499 j cm 2 d 1 was higher than the upper threshold used in the global calibration 300 j cm 2 d 1 which had been defined based on literature values regarding ṗm and κ although some optimized values exceeded the originally pre defined range of values based on existing literature the average values for all the datasets did not exceed that range discussion selecting the correct parameters of a model is key for its successful application the multidimensional space in which deb parameters coexist increases the complexity of parameterization furthermore different parametrizations are typically mapped to equally optimal solutions i e a multimodal landscape in this study the application of a novel mathematical tool together with the combination of growth and physiological data was used to estimate new sets of parameters for the mytilus edulis deb model in a norwegian fjord this novel mathematical tool provides a range of equally good solutions which captures the fact that different physiological strategies can result in similar growth patterns existing sets of parameters deb models for m edulis have been parameterized multiple times in the scientific literature with van der veer et al 2006 rosland et al 2009 and saraiva et al 2011a being the most relevant contributions although saraiva et al 2011a had correctly pointed out that the parameters from van der veer et al 2006 and rosland et al 2009 violate the principle of mass conservation they have been used in this study to define potential ranges of parameters and for comparative purposes the estimated parameters using the momca framework to strohmeier et al 2015 were farther from saraiva et al 2011a than from van der veer et al 2006 and rosland et al 2009 a plausible explanation for this finding could be related to the fact that saraiva et al 2011a focused on wild populations of m edulis growing on the benthic zone but rosland et al 2009 focused on farmed populations cultivated suspended in the water column while van der veer et al 2006 used a mixed source of data it is well know that wild populations grow at a lower rate than farmed populations wild mussels are exposed to additional stressors such as currents waves and predators as well as a higher fluctuation in seston quantity and quality lachance et al 2008 these stressors require additional energy expenditures in wild mussels namely more byssus production babarro and carrington 2013 and investment in thicker shells beadman et al 2003 to cope with a high energy environment and predators respectively in addition despite the physiological plasticity of mussels when individuals from different origins are transferred to the same environment there is an acclimation period before reaching similar physiological rates labarta et al 1997 the mechanistic nature of deb should be able to cope with environmental changes over time but due to the simplicity that is required to construct a model not all environmental drivers are included in it in the current deb model only temperature and food are used as forcing functions and for example hydrodynamics and the presence of predators are not included despite being relevant for individual bioenergetics especially in the case of wild populations choosing different parameterization could be a simple solution for the use of the same modelling structure to simulate both wild and farmed populations this new parameterization would need to focus on the parameters affecting the physiology of the individual under different conditions e g byssus production shell thickness or inter and intra specific competition affecting food searching rate predicting growth and physiology in lysefjord the inclusion of physiological data in the parameterization process aims to constraint the range of valid solutions without constraining the parameters using physiological rates the model could correctly predict growth based on the combination of erroneous rates for example an individual with high ingestion and maintenance could result in similar growth to an individual with low ingestion and maintenance to a certain degree the pre defined range of parameters account for this e g the value of the maximum surface area specific ingestion rate ultimately determines the maximum clearance rate however the addition of physiological rates to the parameterization process provides information regarding the short term variability and plasticity of individuals in fact the mismatch between observations and predictions in clearance rate shortly after spawning suggest that the current version of the deb model cannot fully capture this short term physiological response this is expected given that clearance rate in deb is a function of food availability and temperature but strohmeier et al 2015 had already suggested that clearance rate is not correlated with seston characteristics in this specific dataset similar conclusions could be extracted for respiration rates although it is important to acknowledge the simplification in the mathematical formulation to estimate respiration rates from deb in general the overall fitness of the model in terms of growth and physiology for the validation datasets is within 15 error which is within the acceptable range expected for these types of exercises the punctual mismatches on physiological rates suggests that additional exogenous or endogenous drivers should be added to the deb model to properly simulate short term plasticity for example several studies have aimed to improve the characterization of food proxies that could result in a fully mechanistic ingestion model e g bourlès et al 2009 picoche et al 2014 similarly the use of synthesizing units is intended to improve this aspect of deb by accounting for the capacity of mussels to select food types based on their quality saraiva et al 2011b which inherently tackles an aspect often overlooked in deb modelling the digestive processes including differential assimilation efficiency of seston and gut passage time e g bayne et al 1987 most of the efforts for improvement are directed towards the understanding of exogenous drivers however the endogenous status could play a relevant role in short term physiological responses for example it has been suggested that an immediate decrease in filtration activity is observed after spawning as a mechanism to avoid the ingestion of gametes newell and thompson 1984 although the temporal frequency of strohmeier s dataset does not allow for testing such an immediate response the highest clearance observed in the study occurs a couple of weeks after spawning and coincides with a rapid re building of mussel s tissue strohmeier et al 2015 based on these results it could be hypothesized that spawning might trigger additional physiological responses and not only the release of gametes for example brigolin et al 2009 stopped the allocation of energy towards reproduction in m galloprovincialis after the second spawning diverting all energy towards somatic growth diverting energy towards somatic growth after spawning would not explain the observations in strohmeier s dataset using the current version of deb given that the observed increase in tissue is not concomitant with an increase in shell length which is mostly the case in deb theory with the exception of the role that reserves play on tissue mass shell is not directly modelled in the present deb model but is indirectly estimated based on the amount of energy allocated in the structure and the shape coefficient shell is considered in deb theory as a product of the growth flux proportional to growth overheads therefore some adjustments of the model would be required to account for the well known decoupling of shell and tissue growth observed in the literature e g hilbish 1986 which has already been incorporated in other modelling frameworks by adding additional energy allocation rules fuentes santos et al 2019 a dynamic allocation of energy between structure and reproduction and or additional allocation rules together with a change in energy acquisition triggered by an endogenous driver such as spawning could result in a better short term agreement between observations and simulations another option would be to model explicitly shell accretion pecquerie et al 2012 in any case empirical data at a higher temporal resolution would be needed to validate these structural changes in deb and to generate further hypotheses on other potential effects of spawning on the physiological of mussels deb as a multimodal problem the mathematical estimation of a set of parameters of a non linear dynamic model is a complex task automated calibration packages and search algorithms facilitate this process but the solution or solutions should be carefully analyzed or there is a risk of treating the model as a black box saleh et al 2010 this problem is amplified when the model operates in a multimodal environment which could result in multiple sub optimal solutions muñoz et al 2015 the challenge is magnified with the increase in the number of parameters jusup et al 2017 and the potential dependence among these parameters muñoz et al 2015 moya et al 2019 this is the case for deb in which the relatively large number of parameters kearney et al 2015 and the inter dependence of some of them fujiwara et al 2005 chica et al 2017 are mapped to a multimodal continuous landscape the existence of multiple sub optimal solutions is a challenge from the mathematical perspective but it could be an advantage from the biological perspective the analysis of sub optimal solutions that are close to the best solution can be used to inform about the impact of the parameters on model performance and the relationships among themselves in addition these sub optimal solutions could reflect the inter individual variability within a population which could reflect different physiological strategies that could result in similar outcomes in terms of growth usually the scaling up from individual to population in deb is carried out by simulating several individuals with different initial conditions namely length and dry weight e g gourault et al 2019 but keeping the same set of parameters it has been suggested in the past that future improvement in deb modelling could come from focusing on genetic and phenotypic traits to integrate inter and intra specific variability lika et al 2011b marques et al 2018 therefore the set of equally good solutions provided by the momca framework would not only provide valuable information regarding the parameters but also become a tool to improve the scaling up from individual based models to population model allowing a direct analysis on the set of returned solutions i e set of values to the calibrated variables of the model the challenge of the mathematical estimation of parameters is amplified by the methodological and logistic challenges to collect the appropriate datasets for calibration and validation despite the numerous scientific studies and monitoring programs tackling the growth of mussels and bivalves in general the number of available datasets with growth and environmental data collected at the right temporal resolution is still limited despite the numerous scientific studies and monitoring programs tackling the growth of mussels and bivalves in general the number of available datasets with growth and environmental data collected at the right temporal resolution is still limited for example one limitation in this study is the high dependence of three parameters xk ṗxm and κx on the information provided by clearance rate in this case the number of potential solutions can only be constrained by restricting the theoretical range of the parameters but additional information would be needed to fully disentangle their inter dependency in any case the information from multiple equally good solutions from the momca framework can also be used to define future priorities for data collection in this regard the high variability in the half saturation constant xk and the maximum surface area specific ingestion rate ṗxm which average value is higher than the upper threshold reported in the literature suggests that further research should focus on food ingestion despite the long history of studies on mussel feeding behaviour e g bayne et al 1987 riisgard et al 2011 it seems that these types of studies together with an improved characterization of seston are still critical to improve our understanding of the trophic interactions of bivalves similar to the need of data at a higher temporal resolution for physiological responses the characterization of seston for the identification of food proxies should also be carried out at a more detailed temporal resolution from a practical perspective of modelling parameterization it is essential to emphasize that momca solutions similarly to all automatic optimization tools need to be considered carefully by the user as it was stated above the upper limit for the maximum surface area specific ingestion rate ṗxm was reached in five out of ten solutions in the global calibration and was reached twice in the local calibration of strohmeier datasets while physiological plasticity is well known in bivalves especially regarding feeding activity values of 600 j cm 2 d 1 seem unlikely the user must make a decision to prioritize more physiologically relevant solutions compared to others this highlights the necessity of a critical evaluation of the optimized parameter set this can be done as in marques et al 2019 where the combination of pseudo data and a series of filters ensures that the solutions are compatible with deb theory e g ensuring mass conservation additional functionalities could also be included in momca to inform the user for example when a parameter is reaching the boundary of the pre defined range and is having too much impact on the solutions an alert could be displayed subsequently the optimization could automatically explore the implications of increasing the range of the parameter in terms of fitness something that has been manually done in this study the implementation of these filters and functionalities in momca is beyond the scope of this study while these upgrades could be included in momca a better solution would be incorporating momca into the current standard procedure to estimate deb parameters developed by marques et al 2019 this approach would complement the state of the art deb theory with the power of multimodal optimization e g equally good solutions and confidence intervals for parameters including momca in the standard procedures to estimate deb parameters i e marques et al 2019 could provide an additional tool to scope the potential parameter range this would bring together multimodal optimization with the benefits from the use of pseudo data relevant filters to ensure the compatibility with deb theory user friendly tools and extensive learning materials and still allowing control on the tuning of the deb parameters conclusions individual based models such as deb are critical tools to explore the current understanding of bivalve bioenergetics these models are also cornerstone in ecosystem modelling for example in models that explore the effects of aquaculture on the environment or the suitability of an area for aquaculture development however the parameterization of these models is challenging in this study the momca framework a novel multimodal optimization approach has been used to parameterize deb for mytilus edulis in a norwegian fjord the inclusion of physiological rates to constraint the range of the parameters together with the analysis of multiple equally good solutions rather than a single optimal one suggest that 1 deb can effectively simulate mussel growth and physiology in the long term but punctual physiological changes such as the response to spawning can compromise the performance of the model in the short term and 2 deb s energy acquisition sub model namely feeding and characterization of seston should be a priority for improving model performance these data gaps are bottlenecks for the ultimate goal of constructing a truly mechanistic model furthermore the successful use of momca has demonstrated the potential for complementing the current state of the art procedures to estimate deb parameters i e marques et al 2019 with multimodal optimization declaration of competing interests the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank two anonymous reviewers who provided a thorough and constructive critique of our work this work was funded by the research council of norway project no 196560 the institute of marine research project no 14898 exasoco pgc2018 101216 b i00 and nserc discovery grant to rf mc acknowledges the ramón y cajal program ryc 2016 19800 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j ecolmodel 2020 109139 appendix supplementary materials image application 1 
24791,tropical forests are a critical component of the earth system storing half of the global forest carbon stocks and accounting for a third of terrestrial photosynthesis lianas are structural parasites that can substantially reduce the carbon sequestration capacity of these forests simulations of this peculiar growth form have only recently started and a single vegetation model included lianas so far in this work we present a new liana implementation within the individual based model formind initial tests indicate high structural realism both horizontal and vertical in particular we benchmarked the model against empirical observations of size distribution mean liana cluster size and vertical leaf distribution for the paracou site in french guiana our model predicted a reduction of above ground biomass between 10 for mature stands to 45 for secondary plots upon inclusion of lianas in the simulations the reduced biomass was the result of a lower productivity due to a combination of lower tree photosynthesis and high liana respiration we evaluated structural metrics lai basal area mean tree height and carbon fluxes gpp respiration by comparing simulations with and without lianas at the equilibrium liana productivity was 1 9t c ha 1 y 1 or 23 of the total gpp and the forest carbon stocks were between 5 and 11 lower in simulations with lianas we also highlight the main strengths and limitations of this new approach and propose new field measurements to further the understanding of liana ecology in a modelling framework 1 introduction lianas are key organisms of tropical forests where they can constitute more than 25 percent of the woody plant species and up to 40 percent of the woody stems schnitzer and bongers 2011 lianas are often referred to as structural parasites because although their development starts from the ground they use existing tree structures to climb up to the top of the canopy once in the canopy lianas deploy large crowns often blanketing their hosts tobin et al 2012 lianas compete with trees for both above light and below ground water nutrients resources putz 1984 pérez salicrup 2001 due to lower investment in structural tissues compared to trees lianas are left with a greater fraction of carbon to use for reproduction canopy development and stem and root elongation schnitzer and bongers 2002 this shift in allocation to more ephemeral tissues can reduce the carbon residence time in liana abundant forests phillips et al 2005 van der heijden et al 2015 in some regions like the neotropics lianas are increasing in both density and dominance phillips et al 2005 lianas are particularly well adapted to thrive in forests edges campbell et al 2018 logged areas magrach et al 2016 and disturbed forests in general dewalt et al 2000 secondary or disturbed forests may provide ideal conditions for liana proliferation by providing an optimal balance of trellises and high light conditions madeira et al 2009 as of 2008 the amount of secondary forest in the neotropics was estimated to be 2 4 million km2 over the next 40 years this land can potentially accumulate a total above ground carbon stock of 8 48pgc chazdon et al 2016 lianas have the potential to substantially reduce this carbon sequestration capacity despite lianas being regarded as a key driver of tropical forest change lewis et al 2004 only limited research has addressed their role within a modelling framework the first process based model to account for this growth form is the ecosystem demography ed model di porcia e brugnera et al 2019 ed is able to capture some features of liana infested forests e g the differential impact across successional stages however the underlying structure of ed prevents a realistic representation of a number of liana characteristics for example the localized nature of liana effect on their host is harder to represent in a cohort based spatially implicit model by simulating single trees individual based models ibms provide the correct resolution to represent these local processes shugart et al 2018 in the era of global change model projections of the land carbon sink are essential to the design of effective mitigation strategies with this study we want to test the impact of lianas on the carbon dynamics of tropical forest across different successional stages with the ibm formind fischer et al 2016 thanks to its structural realism we expect the new liana plant functional type pft in formind to capture more accurately than ed the horizontal and vertical distribution of individual lianas and their impact on forest structure in addition the individual based nature of the formind model allows us to study liana clustering how many lianas does a tree carry on average and whether this property depends on the mechanism through which lianas attach to their host by upscaling the individual responses to the ecosystem level we also assess the impact of lianas on carbon fluxes such as gross primary productivity gpp and net ecosystem exchange nee output from this type of representation may if properly validated be useful to parametrize models with coarser resolution like ed 2 methods 2 1 simulation and data sites 2 1 1 paracou all of the simulations presented in this study were conducted at the paracou site which is located in the coastal part of french guiana and is classified as a lowland moist primary forest records indicate a mean annual precipitation of 3088 mm with a well marked dry season from mid august to mid november the floristic composition is highly diverse with high standing biomass guitet et al 2014 we used a simplified meteorological forcing hiltner et al 2018 that assumed the length of the daily photosynthetic active period to be 12h köhler et al 2003 and the mean annual irradiance above the canopy to be 694µmolphotons m 2 s 1 huth and ditzer 2000 for this study we used inventories that were conducted in 10 plots of undisturbed forest these 70 m 70 m plots were established in 2004 in the paracou flux tower footprint in 2015 all lianas with dbh 2cm were censused using the standard protocol gerwing et al 2006 for a total of 839 lianas over a 4 9ha area during the same field campaign the intrinsic quantum yield and the light saturated photosynthesis were measured for 10 liana individuals pausenberger 2016 with a ciras 3 instrument in a successive field campaign in 2016 terrestrial laser scans tls were performed to estimate the forest vertical structure and the plant area index pai pieters 2017 2 1 2 other sites to derive liana growth parameters we used diameter inventories that were collected for a total of 4623 lianas at the gigante peninsula site in panama schnitzer and van der heijden 2019 the inventories were carried out in 2011 and 2014 and we calculated the yearly dbh increments by averaging over this 3 year period for size distributions we compared simulations results with data from paracou and literature data of two additional sites the first one is point calimere wildlife sanctuary pcws a 2 hectares tropical dry evergreen forest site in south east india pandi and parthasarathy 2017 the second one is yasunã national park ynp a 0 4 hectares tropical moist forest site in ecuador nabe nielsen 2001 2 2 liana functional type in formind formind is an individual based spatially explicit process based model designed for simulating species rich vegetation communities fischer et al 2016 each hectare is partitioned with a 20 m 20 m grid for a total of 25 plots per hectare competition for light and space takes place at the plot level but tree positions are not resolved within the plot the demographic processes considered are recruitment growth and mortality to model the light climate within the forest canopy vertical canopy layers are discretized with 0 5 m strata temporarily the model is discretized with yearly time steps for more details about the model structure we refer to the original model description fischer et al 2016 for this work we developed a new liana plant functional type the challenge was to include processes in the formind model to capture the scandent physiology of the liana growth form in the next paragraphs we describe in detail the representation of the liana pft and the parameters that we used 2 2 1 recruitment like trees new lianas in formind are recruited with a seed rain that happens whenever the available light is higher than a predefined threshold as such the recruitment process is governed by two parameters available light as a fraction of total incoming radiation and number of seeds for ingrowth each year if the light conditions are met a plot will receive a number of new individuals nnew equal to 1 n n e w n s e e d s n p l o t s where nseeds is the seed ingrowth parameter and nplots is the number of plots per hectare if the number of ingrowing seeds is not a multiple of the number of plots the remaining seeds will be distributed randomly to the plots as very little data is available to parametrize this process we assumed liana seed ingrowth to be equal to the maximum used for trees high seed production we also assumed the minimum light threshold for seed ingrowth to be equal to the average value used for tree pfts the parameters are shown in table 1 we assumed that upon establishment all lianas and trees have an initial stem diameter of 1cm 2 2 2 growth in the model lianas undergo three main stages of developmental growth 1 self supporting 2 climbing and 3 in the canopy fig 1 a when in the self supporting stage lianas are assigned a virtual tree pft whereby they inherit the growth curve of a randomly chosen tree pft as a result the diameter growth of self supporting lianas is similar to the one used for trees until they find a host fig 2 once a liana finds a suitable host it enters stage 2 where net primary productivity npp is used exclusively for vertical elongation stem diameter and crown area are kept constant until the liana reaches the host canopy once in the canopy top stage 3 liana height is constrained to its host height and stem growth is derived from a growth curve the growth curve is a prescribed function that assigns a maximum annual diameter increase for each diameter the curve was obtained by fitting the gigante observations of diameter increase with a characteristic function as described in fischer 2010 the function is shown as a blue line in fig 2 crown ratio crown length divided by tree height is assumed to be the same for lianas and trees until stage 3 once in the top of the canopy we assumed lianas to deploy all of their leaves in the highest stratum that the tree crown is occupying avalos et al 2007 canopy area for lianas depends on their developmental stage and is described in detail in appendix b 2 2 3 mortality mortality in formind is a stochastic process that every annual time step kills each individual with a certain probability trees can die of multiple causes including background mortality diameter dependent mortality diameter increment dependent mortality crowding and damage by a falling dead tree or by external disturbance events like logging fires or landslides fischer et al 2016 like trees lianas have a fixed background mortality rate table 1 once climbing the liana mortality is complemented with a new process based mortality that depends on the mortality of their host this additional mortality depends on both the fate of the dying host and on the growth stage of the liana fig 1b provides a schematic representation of these processes which were developed based on mechanical considerations if the hosting tree dies without falling it is assumed that lianas can find a new host if the hosting tree falls the liana dies if it has not yet reached the top of the canopy and either dies or moves to a new host if it is already in the top of the canopy with a 50 chance this last condition was constructed assuming that lianas in the top of the canopy may be attached to multiple hosts and hence be able to withstand the fall of their main host 2 2 4 biomass while trees in the formind model have a one to one correspondence between diameter and biomass a certain degree of variability exists for lianas due to the non bijective nature of the height diameter relation liana height is host dependent rather than dbh dependent in formind given a plant height h m its biomass agb tc is calculated as 2 a g b π 4 d b h 2 h ρ f 1 s where dbh m is the stem diameter ρ tc m 3 is the wood density f is the form factor and s is the stem factor the form factor accounts for the tapering of the stem while for trees the form factor is a function of dbh for lianas we assumed f 1 perfect cylinder the stem factor s is the proportion between the total tree agb and the biomass of the main stem thus providing a correction for branches and leaves given that for lianas f 1 s 1 becomes the ratio between the actual biomass and the biomass of a cylinder of equivalent diameter and height thus including shape corrections such as for helical stem structure assuming an accurate tree vertical structure h of eq 2 we calculated the optimal stem factor to match the observed liana agb allometry schnitzer et al 2006 we used a bisection algorithm to estimate the s value that minimized the root mean square deviation between our simulations letting s vary and the published allometry obtaining a value of s 0 78 fig 3 2 2 5 liana host interactions the selection of the liana host has two possible pathways in the first one which we will refer to as method 1 possible hosts are all trees in the plot in the second one which we will refer to as method 2 we also include other lianas as possible hosts for both methods the only requirement we impose is that the potential host height should be higher than the liana height once a host is selected the attachment is a stochastic process with a probability p dbh y 1 the probability was constructed based on the observed probability of finding lianas in the canopy as a function of their dbh kurzel et al 2006 the probability is given by 3 p d b h 0 d b h 1 5 cm 1 d b h 4 cm f d b h f 1 5 f 4 f 1 5 e l s e w h e r e where f x 1 1 e k x μ is the logistic function with k 2 and μ 2 5 the parameter k controls the steepness of the curve so that for large k the function becomes linear μ is the midpoint of the sigmoidal curve so that f μ 0 5 this type of probability ensures a realistic fraction of lianas in the different stages of growth as shown in fig 4 we tested different distributions e g linear interpolation and the results were similar experimental evidence suggests that a single liana is able to colonize multiple trees ichihashi and tateno 2011 putz 1984 from an ecological viewpoint this could have multiple benefits for lianas mechanically it would reduce the risk of falling ichihashi and tateno 2011 and the increased vertical and horizontal growth putz 1984 due to liana s specific physiology could lead to higher photosynthesis and deployment of leaves in optimal conditions although our model allows multiple lianas to colonize a single tree only one host is allowed for each liana to compensate for this limitation we allowed lianas to change their host when light conditions are sub optimal roeder et al 2015 more specifically if a liana has reached its host canopy but still receives less that 50 of the total incoming radiation we assign a 10 y 1 probability of switching to a new host the new host is selected randomly within all trees taller than the current host that satisfy the criterion h t h l 2 m where hl is the liana height and ht is the new host height as we have seen host change can also take place if the liana host dies as described in the mortality section in this case we only require that h l h t 2 m to prevent large discontinuities in height 2 2 6 liana impact on trees lianas are known to affect the architecture of their hosts studies have shown that significant liana loads alter tree allometry by decreasing slenderness dias et al 2017 and that lianas replace tree leaves on a one to one biomass basis ogawa et al 1965 however the specific impact of lianas on tree leaves remains uncertain and a recent study failed to find correlation between liana canopy area and understory measurements cox et al 2019 in our model leaf area index lai and crown ratio cr of the hosting tree are affected by the proportion of liana and tree leaves given the original lai and the original crown ratio cr of the host tree its new parameters lai and cr are calculated as 4 l a i f l a i 5 c r f c r where 6 f a c t a c l 2 a c t a c l is a reduction factor and act and acl are the total crown area of the tree and of the lianas on it respectively a detailed analysis of the consequences of this penalization scheme is presented in appendix b this penalization is the only modification that was made to trees in the formind model for this study 2 3 parametrization to parametrize the liana pft we used a combination of published and non published data if the required parameter was not available in literature we resorted to realistic assumptions for recruitment we derived the parameters from other pfts by using the maximum number of seed ingrowth used for trees and averaging the light threshold for establishment for the maximum height we assumed lianas to be able to climb all tree pfts in the simulation thus assigning a maximum height equal to the maximum tree height parameters and the corresponding references are presented in table 1 the range of values for the tree pfts are also given for comparison 2 4 simulation details the simulated 16 hectares were initialized from bare ground and were continued for 500 years to reach an equilibrium state the runs with and without lianas were performed with the same conditions but turning on and off the liana pft liana densities were measured by sampling all attached lianas with dbh 2 cm in addition to liana density the realism of the simulated distribution of lianas within the hosting trees i e number of lianas per host tree was tested by comparing simulations for paracou with observations at ynp and pcws the total number of modelled leaf strata was 81 equal to the ceiling of maximum tree height 40 4 m divided by the height layer width 0 5 m vertical leaf profiles were derived by aggregating leaf area contribution of each plant for every height strata to assess the impact of lianas on the different tree size classes trees were categorized based on three levels of infestation free low and high liana load when the liana crown area was less than half that of its host the tree was classified as having low liana load otherwise the tree was classified with high liana load gas exchanges were calculated at the plant level and aggregated for every individual to obtain forest level gpp and autotrophic respiration heterotrophic respiration was calculated adding respiration from dead wood biomass and the fast and slow cycling components of soil respiration nee was calculated as gpp minus the autotrophic and heterotrophic respiration for the complete description of the carbon cycle in formind we refer to fischer et al 2016 2 5 statistics correlations between liana density and mean liana age and tree basal area were calculated using the pearson s correlation test to study liana clustering trees with one liana were tested to find whether they had a higher probability of having more than one liana nabe nielsen 2001 putz 1984 expected poisson distributions were generated with the parameter λ equal to the simulated data average number of lianas per tree expected and simulated distributions were compared using a χ 2 goodness of fit test trees hosting 3 or more lianas were aggregated to avoid expected values smaller than one fits for liana counts were performed by linearizing the data and using a least squares fit all statistical analyses were performed in r version 3 5 1 r core team 2018 unless differently specified model results are presented as mean standard deviation of the 16 hectares 3 results 3 1 size and spatial distribution of lianas simulations gave an overall liana density of 333 170individuals ha 1 while observed data showed a lower density of 171stems ha 1 in terms of liana basal area the model predicted a value of 0 46 0 14cm2 m 2 compared to an observed value of 0 42cm2 m 2 in the model large lianas dbh 10cm accounted for 41 13 of the total liana basal area similar to the empirical observation of 40 around 30 in a large scale study in peru phillips et al 2005 liana density across different hectares was negatively correlated with mean liana age pearson s correlation r 0 58 p 0 017 and tree basal area pearson s correlation r 0 47 p 0 06 indicating that lianas decrease in abundance with forest succession the model was able to qualitatively reproduce the trend in size distribution observed at paracou fig 5 compared to the observed size distribution the model slightly overestimated the fraction of small lianas dbh 5cm and underestimated the fraction of larger lianas the large variability among the different hectares can be traced back to different light environments due to their disturbance history and to stochastic effects despite paracou ynp and pcws being different types of forest the simulated pattern of liana cluster size had a similar exponential decay ynp y 784 e 0 97 x pcws y 604 e 0 81 x this study y 449 e 0 86 x fig 6 the similarity of the modelled decay constant to the observed ones indicates that the model may be able to capture the tree liana and liana liana competition for space and light we also tested the impact of the two different attachment mechanisms with method 1 the simulated liana count per tree was perfectly random upon attachment by contrast if lianas could use other lianas to climb the canopy method 2 the liana count was skewed towards larger clusters as trees with more lianas became stronger attractors to test whether the model kept memory of the initial distribution sample of all lianas of age 1 we extracted the simulated liana count per host at the equilibrium the number of trees with two or more lianas was larger than would be expected by chance with both method 1 χ 2 59563 d f 3 p 0 0001 table a 1 and method 2 χ 2 67121 d f 3 p 0 0001 table a 2 suggesting that lianas tend to aggregate the loss of memory of the initial distribution was consistent with the implementation of a routine that allows lianas to change their host the mean liana cluster size was 1 59 and 1 57 for method 1 and 2 respectively the similarity of these numbers suggests that upon reaching equilibrium liana clusters adjust to an optimal size to avoid conspecific competition liana clumping was time dependent after year 50 of the simulation mean cluster size slightly increased over time for both attachment mechanisms fig a 1 3 2 leaf profiles total simulated lai of the paracou site was 4 93 0 1 m2 m 2 compared to a tls observed pai of 5 17 m2 m 2 as it includes contributions from trunks and branches pai is expected to be slightly higher than lai comparison between simulations and tls showed that the model overestimated total lai at low heights and underestimated it above 15 m fig 7 liana leaves tended to occupy the higher strata of the canopy with 62 of the leaves found above 20 m when the forest is at the equilibrium although the overall liana lai was 6 9 2 4 of the total it grew to 17 3 6 4 for the 20 m 30 m stratum and to 38 3 22 0 for the 30 m 40 m stratum the significant proportion of liana leaves simulated below 5 m was due to the high liana seedling density as a result of our reproduction parametrization we point out that while the simulations considered 16ha of forest the tls scans were taken at 9 plots close to the flux tower and may not represent an accurate forest average introduction of lianas in the simulations did not significantly affect the total amount and distribution of leaves fig 7 dotted curve 3 3 simulations with and without lianas 3 3 1 biomass and forest structure we compared simulations with and without lianas to assess their impact on forest structure and carbon stocks one of the most dramatic effects of lianas was the reduced basal area across the entire forest succession after 50 years basal area of the forest with lianas was about half that of the forest without lianas 16 9 0 8 m2 ha 1 vs 30 6 1 5 m2 ha 1 respectively this difference was due to a lower plant density 10521 vs 11374 plants for the 16 hectares respectively and to a lower average tree diameter 16 2 9 2cm vs 18 9 13 3cm respectively the quadratic dependency of basal area upon dbh amplified this difference fig 8 a after the forest equilibrated mean basal area was still lower in the presence of lianas 28 4 2 2 m2 ha 1 vs 32 1 1 7 m2 ha 1 mean adult tree height was proportionally less impacted by lianas than basal area fig 8a because unlike basal area tree height is a concave function of dbh after 50 years mean adult tree height was 16 6 0 1 m for the simulation with lianas and 17 6 0 1 m for the simulation without lianas at the equilibrium there was no significant difference for this metric after year 60 of the simulation total lai was generally higher for the simulation with lianas however the difference was always less than 5 at the equilibrium lai for the simulation with lianas was about 2 higher than the non liana simulation with lianas accounting for circa 7 of the total leaf area results from simulations showed a strong impact of lianas on stand level above ground biomass the reduction of agb was more pronounced in the early stage of succession when lianas were more abundant fig 8 b c the maximum reduction in biomass was 43 when the forest was 45 years old after 100 years the reduction in agb was 13 and fluctuated between 5 and 11 after reaching the equilibrium the simulated value of agb at year 500 was 185 15t c ha 1 while empirical observations estimated agb at 186 7t c ha 1 rutishauser et al 2010 at the individual level we analysed the mean biomass increments averaged over the simulation from year 100 to year 500 for three different classes of liana infestation and for four different tree size classes as expected yearly biomass increments were lower with increasing liana load table 2 for all size classes for the taller canopy trees dbh 80cm mean biomass increment with high liana load was only 64 of the one for trees without lianas the biomass impact per pft showed a correlation with successional stage and with tree maximum height for example when a pioneer pft peaked in abundance its total biomass was up to 70 less upon inclusion of lianas pft 4 fig a 2 pfts that reach lower maximum heights were proportionally less impacted by lianas see pfts 1 2 and 3 of fig a 2 these results are consistent with the ability for lianas to move up the canopy and with the absence of any host pft specific process 3 3 2 carbon fluxes carbon fluxes were sensitive to the introduction of lianas in the simulations liana maximum photosynthetic rate was assumed to be higher than climax species but lower than pioneers table 1 as a result gpp in the simulation with lianas was lower maximum reduction of 46 at year 50 than in the simulation without lianas when pioneers are abundant that is until year 100 120 for the same reason gpp for the simulation with lianas was up to 20 higher at the equilibrium when the pft composition shifted towards shade tolerance maximum liana gpp was 5 5t c ha 1 y 1 at year 24 38 of the total gpp the average liana contribution to gpp for the years 400 to 500 was 1 9t c ha 1 y 1 23 of the total fig 9 a autotrophic respiration was also impacted by the introduction of lianas the trend in the first 100 years of succession was similar to the one observed for gpp with lianas reducing total respiration by up to 51 the simulated liana contribution to biomass respiration was high throughout the succession and at the equilibrium accounted for 54 of the total 3 44tc ha 1 y 1 respired by vegetation after year 250 biomass respiration with lianas was almost twice that of the simulation without lianas fig 9b these trends can be understood in light of the very low diameter growth rate for lianas which resulted in a large fraction of gpp respired back to the atmosphere nee converged to zero more slowly when lianas were included in the model this result was consistent with the observed ability for lianas to slow down the ecosystem succession tymen et al 2016 despite being positive for about 250 years lianas reduced the maximum yearly carbon uptake by about 50 fig 9c by integrating nee over the entire simulation we found that the total carbon sink for the liana free forest was 8 higher 261 4tc ha 1 vs 240 1tcha 1 there was a compensation effect of heterotrophic respiration which unlike biomass respiration was always lower when lianas were included fig a 3 the lower value of heterotrophic respiration was the consequence of a forest with lower agb but similar agb mortality rate at the equilibrium carbon residence time agb npp was 36 years regardless of the inclusion of the liana pft 4 discussion 4 1 model structure our liana model introduces a custom representation for the climber growth form that is able to distinguish between three phases of ontogeny fig 4 the probabilistic transition between the different growth phases was constructed based on logical assumptions in fact the model is likely overestimating the fraction of lianas in the climbing stage and smaller lianas should have a higher probability of being in the canopy compared to the current probability in the model for example compare fig 4 with fig 1 of kurzel et al 2006 albeit only qualitative this multi phase structure constitutes a significant improvement over the representation of lianas in ed where the climbing phase was not explicitly represented di porcia e brugnera et al 2019 future modelling efforts seeking to simulate lianas throughout their development will benefit from field data that discriminates lianas as self supporting climbing or in the canopy equally important for a realistic model will be a statistic mechanistic understanding of the transition from one stage to the next one of the key features of the formind model is the calculation of growth and respiration from observed diameter increments in woody vines annual diameter growth has been shown to be substantially lower than trees on average 1 4 mm y 1 vs 6 mm y 1 for the bci site putz 1990 as a result of a markedly different allocation pattern in our liana representation the low values of diameter growth prevented proper liana establishment as a correction we simulate growth of self supporting lianas as if they were trees this use of a virtual tree pft for lianas could be extended to canopy lianas especially when a tree trait distribution is similar to a liana trait distribution e g for leaf mass area wyka et al 2013 in addition to growth most other mechanisms describing lianas were built with some degree of speculation for example host dynamic the ability for lianas to change their host relies on the assumption that lianas will move across the canopy seeking better light conditions the probability which we assumed to be 10 y 1 to find a new host when light conditions are sub optimal needs additional enquiry even though experimental measurements will be challenging to obtain in the model host dynamic was also connected to tree mortality the fate of lianas on dying trees is still unclear and needs to be addressed with more observations if we are to construct a realistic mortality process finally although some studies have tried to understand liana impact on trees at an individual level dias et al 2017 ogawa et al 1965 van der heijden et al 2010 a more thorough analysis of shading and mechanical stress is needed to improve our tree penalization scheme 4 2 model findings size distribution size distributions result from the interplay of many different processes and being one of the most common field measurement are an important model benchmark our liana implementation proved successful in capturing the qualitative trend in size distribution fig 1 to confirm that the underlying processes and their parametrization are realistic these results will need additional testing in sites with different external conditions total liana density was about twice the observed one however it should be noted that compared to other sites liana density in paracou is particularly low dewalt et al 2015 the bulk of this overestimation was for lianas with dbh 5cm which may be explained by the high value of the recruitment rate parameter and to the fact that observations in the old growth paracou plots may not have had tree fall disturbances recently clustering and horizontal distribution our model predicted a clumped individual distribution with an average cluster size of around 1 6 lianas since this result was independent of the attachment mechanism the clustering is likely to be driven by host change and light and space competition from a modelling point of view host change is a necessary feature to ensure that lianas stay in the higher part of the canopy although this assumption of lianas seeking better light condition is reasonable its mechanistic representation and the host change probability should be evaluated against new observations if host change makes liana more likely to colonize the same trees light and space competition bind this process by making large clusters prone to conspecific competition as we have seen the exponential decay constant of liana cluster size is similar between modelled and observed data this suggests that the implementation of light competition and crowding mortality that was developed for trees can be generalized to lianas leaf profiles in lowland tropical forests light is one of the most limiting resources kitajima et al 2005 vertical leaf profiles are an important metric to understand how light is extinguished while reaching the understory recent technological developments such as laser scanning krishna moorthy et al 2018 have allowed to measure these quantities with greater precision however these instruments cannot parse the different components of the leaf profile i e species growth form in our simulations lianas deployed the majority of their leaves in the higher part of the canopy this result while consistent with lianas expected behaviour is a significant improvement over the previous liana model where liana leaves were concentrated only at low heights the introduction of lianas did not significantly alter the forest lai as a whole suggesting that lianas substitute tree leaves in similar spatial locations the small lai bulge at low heights was due to the large number of saplings and to the contribution of plots where a tree fall event occurred although observations confirm that lianas abound in correspondence of canopy openings schnitzer et al 2000 the parameters for seed dispersion may be overestimated in this sense the addition of more liana pfts with a broad trait dispersion could be a solution to ensure establishment under the highly variable light conditions of the 500 years simulation biomass liana impact on agb was strong both at an individual level and at the landscape level at the individual level tree penalization resulted in smaller biomass increments when the host had a high liana load due to a reduced lai to test our penalization scheme field measurement of trees with and without lianas would be needed as it is unlikely to find trees with similar characteristics but varying liana loads such measurements can only be of statistical nature at the plot level these types of comparison have already started van der heijden et al 2019 and may be used in the future to better parametrize liana burden on trees carbon fluxes overall most of the reduction in biomass could be accounted for by the very high rate of respiration in lianas discussed in appendix b as formind does not explicitly represent leaves and fine roots much of the gpp that is not used for growth is thus respired in reality part of this gpp is likely to be used for production of tissues with fast turnover rates like leaves or fine roots zhu and cao 2010 since the model does not explicitly represent these allocation processes the carbon is directly re emitted through respiration instead of going through litter decomposition in other words the model is overestimating autotrophic respiration and underestimating heterotrophic respiration given that the carbon residence time of leaves and fine roots is short this simplification may have little consequences in terms of carbon fluxes an inclusion of more realistic mortality impact of lianas to their host such as by considering mechanical stress may result in shorter carbon residence time and thus an even lower forest carbon sink potential 4 3 additional considerations in the current stage the model clumps the entire diversity of climbers into one liana pft this is a strong limitation as climbers appear in about half putz et al 1991 of vascular plant families and their trait spectra are known to be dispersed wyka et al 2013 the development of additional liana pfts for example a shade tolerant liana nabe nielsen 2004 may also reduce the strong impact of lianas on respiration furthermore many aspects of the climber growth form have not been considered e g below ground competition or host specificity as many studies have linked liana abundance to hydrology for example finding correlations between liana abundance and mean annual precipitation or precipitation seasonality dewalt et al 2000 a greater model complexity needs to be incorporated to make general predictions of ecological value from a computational point of view the addition of lianas often introduces a second order cost because of the interaction with trees for example when calculating the lai penalization due to lianas the model needs to check each tree for all of its lianas an additional computational burden is due to the use of open arrays these could be simplified by assuming a maximum number of lianas per tree in terms of model structure both formind and ed are now able to simulate trees grasses and lianas both models make use of keywords to create parallel computational regions for each growth form in the case of formind or other models written in object oriented languages we advise for a greater use of polymorphism and inheritance to make the code more compact and abstract for example most allometric equations now require an explicit check of the growth form whenever they are used the use of a parent plant class could help to hide these specific implementations from where these methods are called finally for researchers interested in implementing lianas in different models we advise to start with two liana pfts although the complexity from one pft is normally more than enough to start the use of a second dummy pft can help to create more robust and general code from the start 5 conclusions liana modelling is still in its infancy and this work should lay the ground for additional investigations including with the use of new modelling frameworks with formind we were able to capture many aspects of a liana infested forest in particular we concentrated on correctly reproducing demography and spatial distributions in the current stage the model could already be tested on real scenarios for example quantifying liana impact on carbon stocks in disturbed or logged forests or making forecasts for the future of liana removal plots to expand the applicability of the present model for example to produce regional estimates we advise to first test the model under the extremely variable climate soil and topographical conditions under which lianas are found author contributions mdp lead the model development and data analysis with contributions from ft and rf all authors contributed to discussions and revised the final version of the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was funded by the european research council starting grant 637643 treeclimbers fwo research project g018319n and the special research fund of ghent university bof project 01n00816 special thanks to m piantoni for her help with the graphics of fig 1 
24791,tropical forests are a critical component of the earth system storing half of the global forest carbon stocks and accounting for a third of terrestrial photosynthesis lianas are structural parasites that can substantially reduce the carbon sequestration capacity of these forests simulations of this peculiar growth form have only recently started and a single vegetation model included lianas so far in this work we present a new liana implementation within the individual based model formind initial tests indicate high structural realism both horizontal and vertical in particular we benchmarked the model against empirical observations of size distribution mean liana cluster size and vertical leaf distribution for the paracou site in french guiana our model predicted a reduction of above ground biomass between 10 for mature stands to 45 for secondary plots upon inclusion of lianas in the simulations the reduced biomass was the result of a lower productivity due to a combination of lower tree photosynthesis and high liana respiration we evaluated structural metrics lai basal area mean tree height and carbon fluxes gpp respiration by comparing simulations with and without lianas at the equilibrium liana productivity was 1 9t c ha 1 y 1 or 23 of the total gpp and the forest carbon stocks were between 5 and 11 lower in simulations with lianas we also highlight the main strengths and limitations of this new approach and propose new field measurements to further the understanding of liana ecology in a modelling framework 1 introduction lianas are key organisms of tropical forests where they can constitute more than 25 percent of the woody plant species and up to 40 percent of the woody stems schnitzer and bongers 2011 lianas are often referred to as structural parasites because although their development starts from the ground they use existing tree structures to climb up to the top of the canopy once in the canopy lianas deploy large crowns often blanketing their hosts tobin et al 2012 lianas compete with trees for both above light and below ground water nutrients resources putz 1984 pérez salicrup 2001 due to lower investment in structural tissues compared to trees lianas are left with a greater fraction of carbon to use for reproduction canopy development and stem and root elongation schnitzer and bongers 2002 this shift in allocation to more ephemeral tissues can reduce the carbon residence time in liana abundant forests phillips et al 2005 van der heijden et al 2015 in some regions like the neotropics lianas are increasing in both density and dominance phillips et al 2005 lianas are particularly well adapted to thrive in forests edges campbell et al 2018 logged areas magrach et al 2016 and disturbed forests in general dewalt et al 2000 secondary or disturbed forests may provide ideal conditions for liana proliferation by providing an optimal balance of trellises and high light conditions madeira et al 2009 as of 2008 the amount of secondary forest in the neotropics was estimated to be 2 4 million km2 over the next 40 years this land can potentially accumulate a total above ground carbon stock of 8 48pgc chazdon et al 2016 lianas have the potential to substantially reduce this carbon sequestration capacity despite lianas being regarded as a key driver of tropical forest change lewis et al 2004 only limited research has addressed their role within a modelling framework the first process based model to account for this growth form is the ecosystem demography ed model di porcia e brugnera et al 2019 ed is able to capture some features of liana infested forests e g the differential impact across successional stages however the underlying structure of ed prevents a realistic representation of a number of liana characteristics for example the localized nature of liana effect on their host is harder to represent in a cohort based spatially implicit model by simulating single trees individual based models ibms provide the correct resolution to represent these local processes shugart et al 2018 in the era of global change model projections of the land carbon sink are essential to the design of effective mitigation strategies with this study we want to test the impact of lianas on the carbon dynamics of tropical forest across different successional stages with the ibm formind fischer et al 2016 thanks to its structural realism we expect the new liana plant functional type pft in formind to capture more accurately than ed the horizontal and vertical distribution of individual lianas and their impact on forest structure in addition the individual based nature of the formind model allows us to study liana clustering how many lianas does a tree carry on average and whether this property depends on the mechanism through which lianas attach to their host by upscaling the individual responses to the ecosystem level we also assess the impact of lianas on carbon fluxes such as gross primary productivity gpp and net ecosystem exchange nee output from this type of representation may if properly validated be useful to parametrize models with coarser resolution like ed 2 methods 2 1 simulation and data sites 2 1 1 paracou all of the simulations presented in this study were conducted at the paracou site which is located in the coastal part of french guiana and is classified as a lowland moist primary forest records indicate a mean annual precipitation of 3088 mm with a well marked dry season from mid august to mid november the floristic composition is highly diverse with high standing biomass guitet et al 2014 we used a simplified meteorological forcing hiltner et al 2018 that assumed the length of the daily photosynthetic active period to be 12h köhler et al 2003 and the mean annual irradiance above the canopy to be 694µmolphotons m 2 s 1 huth and ditzer 2000 for this study we used inventories that were conducted in 10 plots of undisturbed forest these 70 m 70 m plots were established in 2004 in the paracou flux tower footprint in 2015 all lianas with dbh 2cm were censused using the standard protocol gerwing et al 2006 for a total of 839 lianas over a 4 9ha area during the same field campaign the intrinsic quantum yield and the light saturated photosynthesis were measured for 10 liana individuals pausenberger 2016 with a ciras 3 instrument in a successive field campaign in 2016 terrestrial laser scans tls were performed to estimate the forest vertical structure and the plant area index pai pieters 2017 2 1 2 other sites to derive liana growth parameters we used diameter inventories that were collected for a total of 4623 lianas at the gigante peninsula site in panama schnitzer and van der heijden 2019 the inventories were carried out in 2011 and 2014 and we calculated the yearly dbh increments by averaging over this 3 year period for size distributions we compared simulations results with data from paracou and literature data of two additional sites the first one is point calimere wildlife sanctuary pcws a 2 hectares tropical dry evergreen forest site in south east india pandi and parthasarathy 2017 the second one is yasunã national park ynp a 0 4 hectares tropical moist forest site in ecuador nabe nielsen 2001 2 2 liana functional type in formind formind is an individual based spatially explicit process based model designed for simulating species rich vegetation communities fischer et al 2016 each hectare is partitioned with a 20 m 20 m grid for a total of 25 plots per hectare competition for light and space takes place at the plot level but tree positions are not resolved within the plot the demographic processes considered are recruitment growth and mortality to model the light climate within the forest canopy vertical canopy layers are discretized with 0 5 m strata temporarily the model is discretized with yearly time steps for more details about the model structure we refer to the original model description fischer et al 2016 for this work we developed a new liana plant functional type the challenge was to include processes in the formind model to capture the scandent physiology of the liana growth form in the next paragraphs we describe in detail the representation of the liana pft and the parameters that we used 2 2 1 recruitment like trees new lianas in formind are recruited with a seed rain that happens whenever the available light is higher than a predefined threshold as such the recruitment process is governed by two parameters available light as a fraction of total incoming radiation and number of seeds for ingrowth each year if the light conditions are met a plot will receive a number of new individuals nnew equal to 1 n n e w n s e e d s n p l o t s where nseeds is the seed ingrowth parameter and nplots is the number of plots per hectare if the number of ingrowing seeds is not a multiple of the number of plots the remaining seeds will be distributed randomly to the plots as very little data is available to parametrize this process we assumed liana seed ingrowth to be equal to the maximum used for trees high seed production we also assumed the minimum light threshold for seed ingrowth to be equal to the average value used for tree pfts the parameters are shown in table 1 we assumed that upon establishment all lianas and trees have an initial stem diameter of 1cm 2 2 2 growth in the model lianas undergo three main stages of developmental growth 1 self supporting 2 climbing and 3 in the canopy fig 1 a when in the self supporting stage lianas are assigned a virtual tree pft whereby they inherit the growth curve of a randomly chosen tree pft as a result the diameter growth of self supporting lianas is similar to the one used for trees until they find a host fig 2 once a liana finds a suitable host it enters stage 2 where net primary productivity npp is used exclusively for vertical elongation stem diameter and crown area are kept constant until the liana reaches the host canopy once in the canopy top stage 3 liana height is constrained to its host height and stem growth is derived from a growth curve the growth curve is a prescribed function that assigns a maximum annual diameter increase for each diameter the curve was obtained by fitting the gigante observations of diameter increase with a characteristic function as described in fischer 2010 the function is shown as a blue line in fig 2 crown ratio crown length divided by tree height is assumed to be the same for lianas and trees until stage 3 once in the top of the canopy we assumed lianas to deploy all of their leaves in the highest stratum that the tree crown is occupying avalos et al 2007 canopy area for lianas depends on their developmental stage and is described in detail in appendix b 2 2 3 mortality mortality in formind is a stochastic process that every annual time step kills each individual with a certain probability trees can die of multiple causes including background mortality diameter dependent mortality diameter increment dependent mortality crowding and damage by a falling dead tree or by external disturbance events like logging fires or landslides fischer et al 2016 like trees lianas have a fixed background mortality rate table 1 once climbing the liana mortality is complemented with a new process based mortality that depends on the mortality of their host this additional mortality depends on both the fate of the dying host and on the growth stage of the liana fig 1b provides a schematic representation of these processes which were developed based on mechanical considerations if the hosting tree dies without falling it is assumed that lianas can find a new host if the hosting tree falls the liana dies if it has not yet reached the top of the canopy and either dies or moves to a new host if it is already in the top of the canopy with a 50 chance this last condition was constructed assuming that lianas in the top of the canopy may be attached to multiple hosts and hence be able to withstand the fall of their main host 2 2 4 biomass while trees in the formind model have a one to one correspondence between diameter and biomass a certain degree of variability exists for lianas due to the non bijective nature of the height diameter relation liana height is host dependent rather than dbh dependent in formind given a plant height h m its biomass agb tc is calculated as 2 a g b π 4 d b h 2 h ρ f 1 s where dbh m is the stem diameter ρ tc m 3 is the wood density f is the form factor and s is the stem factor the form factor accounts for the tapering of the stem while for trees the form factor is a function of dbh for lianas we assumed f 1 perfect cylinder the stem factor s is the proportion between the total tree agb and the biomass of the main stem thus providing a correction for branches and leaves given that for lianas f 1 s 1 becomes the ratio between the actual biomass and the biomass of a cylinder of equivalent diameter and height thus including shape corrections such as for helical stem structure assuming an accurate tree vertical structure h of eq 2 we calculated the optimal stem factor to match the observed liana agb allometry schnitzer et al 2006 we used a bisection algorithm to estimate the s value that minimized the root mean square deviation between our simulations letting s vary and the published allometry obtaining a value of s 0 78 fig 3 2 2 5 liana host interactions the selection of the liana host has two possible pathways in the first one which we will refer to as method 1 possible hosts are all trees in the plot in the second one which we will refer to as method 2 we also include other lianas as possible hosts for both methods the only requirement we impose is that the potential host height should be higher than the liana height once a host is selected the attachment is a stochastic process with a probability p dbh y 1 the probability was constructed based on the observed probability of finding lianas in the canopy as a function of their dbh kurzel et al 2006 the probability is given by 3 p d b h 0 d b h 1 5 cm 1 d b h 4 cm f d b h f 1 5 f 4 f 1 5 e l s e w h e r e where f x 1 1 e k x μ is the logistic function with k 2 and μ 2 5 the parameter k controls the steepness of the curve so that for large k the function becomes linear μ is the midpoint of the sigmoidal curve so that f μ 0 5 this type of probability ensures a realistic fraction of lianas in the different stages of growth as shown in fig 4 we tested different distributions e g linear interpolation and the results were similar experimental evidence suggests that a single liana is able to colonize multiple trees ichihashi and tateno 2011 putz 1984 from an ecological viewpoint this could have multiple benefits for lianas mechanically it would reduce the risk of falling ichihashi and tateno 2011 and the increased vertical and horizontal growth putz 1984 due to liana s specific physiology could lead to higher photosynthesis and deployment of leaves in optimal conditions although our model allows multiple lianas to colonize a single tree only one host is allowed for each liana to compensate for this limitation we allowed lianas to change their host when light conditions are sub optimal roeder et al 2015 more specifically if a liana has reached its host canopy but still receives less that 50 of the total incoming radiation we assign a 10 y 1 probability of switching to a new host the new host is selected randomly within all trees taller than the current host that satisfy the criterion h t h l 2 m where hl is the liana height and ht is the new host height as we have seen host change can also take place if the liana host dies as described in the mortality section in this case we only require that h l h t 2 m to prevent large discontinuities in height 2 2 6 liana impact on trees lianas are known to affect the architecture of their hosts studies have shown that significant liana loads alter tree allometry by decreasing slenderness dias et al 2017 and that lianas replace tree leaves on a one to one biomass basis ogawa et al 1965 however the specific impact of lianas on tree leaves remains uncertain and a recent study failed to find correlation between liana canopy area and understory measurements cox et al 2019 in our model leaf area index lai and crown ratio cr of the hosting tree are affected by the proportion of liana and tree leaves given the original lai and the original crown ratio cr of the host tree its new parameters lai and cr are calculated as 4 l a i f l a i 5 c r f c r where 6 f a c t a c l 2 a c t a c l is a reduction factor and act and acl are the total crown area of the tree and of the lianas on it respectively a detailed analysis of the consequences of this penalization scheme is presented in appendix b this penalization is the only modification that was made to trees in the formind model for this study 2 3 parametrization to parametrize the liana pft we used a combination of published and non published data if the required parameter was not available in literature we resorted to realistic assumptions for recruitment we derived the parameters from other pfts by using the maximum number of seed ingrowth used for trees and averaging the light threshold for establishment for the maximum height we assumed lianas to be able to climb all tree pfts in the simulation thus assigning a maximum height equal to the maximum tree height parameters and the corresponding references are presented in table 1 the range of values for the tree pfts are also given for comparison 2 4 simulation details the simulated 16 hectares were initialized from bare ground and were continued for 500 years to reach an equilibrium state the runs with and without lianas were performed with the same conditions but turning on and off the liana pft liana densities were measured by sampling all attached lianas with dbh 2 cm in addition to liana density the realism of the simulated distribution of lianas within the hosting trees i e number of lianas per host tree was tested by comparing simulations for paracou with observations at ynp and pcws the total number of modelled leaf strata was 81 equal to the ceiling of maximum tree height 40 4 m divided by the height layer width 0 5 m vertical leaf profiles were derived by aggregating leaf area contribution of each plant for every height strata to assess the impact of lianas on the different tree size classes trees were categorized based on three levels of infestation free low and high liana load when the liana crown area was less than half that of its host the tree was classified as having low liana load otherwise the tree was classified with high liana load gas exchanges were calculated at the plant level and aggregated for every individual to obtain forest level gpp and autotrophic respiration heterotrophic respiration was calculated adding respiration from dead wood biomass and the fast and slow cycling components of soil respiration nee was calculated as gpp minus the autotrophic and heterotrophic respiration for the complete description of the carbon cycle in formind we refer to fischer et al 2016 2 5 statistics correlations between liana density and mean liana age and tree basal area were calculated using the pearson s correlation test to study liana clustering trees with one liana were tested to find whether they had a higher probability of having more than one liana nabe nielsen 2001 putz 1984 expected poisson distributions were generated with the parameter λ equal to the simulated data average number of lianas per tree expected and simulated distributions were compared using a χ 2 goodness of fit test trees hosting 3 or more lianas were aggregated to avoid expected values smaller than one fits for liana counts were performed by linearizing the data and using a least squares fit all statistical analyses were performed in r version 3 5 1 r core team 2018 unless differently specified model results are presented as mean standard deviation of the 16 hectares 3 results 3 1 size and spatial distribution of lianas simulations gave an overall liana density of 333 170individuals ha 1 while observed data showed a lower density of 171stems ha 1 in terms of liana basal area the model predicted a value of 0 46 0 14cm2 m 2 compared to an observed value of 0 42cm2 m 2 in the model large lianas dbh 10cm accounted for 41 13 of the total liana basal area similar to the empirical observation of 40 around 30 in a large scale study in peru phillips et al 2005 liana density across different hectares was negatively correlated with mean liana age pearson s correlation r 0 58 p 0 017 and tree basal area pearson s correlation r 0 47 p 0 06 indicating that lianas decrease in abundance with forest succession the model was able to qualitatively reproduce the trend in size distribution observed at paracou fig 5 compared to the observed size distribution the model slightly overestimated the fraction of small lianas dbh 5cm and underestimated the fraction of larger lianas the large variability among the different hectares can be traced back to different light environments due to their disturbance history and to stochastic effects despite paracou ynp and pcws being different types of forest the simulated pattern of liana cluster size had a similar exponential decay ynp y 784 e 0 97 x pcws y 604 e 0 81 x this study y 449 e 0 86 x fig 6 the similarity of the modelled decay constant to the observed ones indicates that the model may be able to capture the tree liana and liana liana competition for space and light we also tested the impact of the two different attachment mechanisms with method 1 the simulated liana count per tree was perfectly random upon attachment by contrast if lianas could use other lianas to climb the canopy method 2 the liana count was skewed towards larger clusters as trees with more lianas became stronger attractors to test whether the model kept memory of the initial distribution sample of all lianas of age 1 we extracted the simulated liana count per host at the equilibrium the number of trees with two or more lianas was larger than would be expected by chance with both method 1 χ 2 59563 d f 3 p 0 0001 table a 1 and method 2 χ 2 67121 d f 3 p 0 0001 table a 2 suggesting that lianas tend to aggregate the loss of memory of the initial distribution was consistent with the implementation of a routine that allows lianas to change their host the mean liana cluster size was 1 59 and 1 57 for method 1 and 2 respectively the similarity of these numbers suggests that upon reaching equilibrium liana clusters adjust to an optimal size to avoid conspecific competition liana clumping was time dependent after year 50 of the simulation mean cluster size slightly increased over time for both attachment mechanisms fig a 1 3 2 leaf profiles total simulated lai of the paracou site was 4 93 0 1 m2 m 2 compared to a tls observed pai of 5 17 m2 m 2 as it includes contributions from trunks and branches pai is expected to be slightly higher than lai comparison between simulations and tls showed that the model overestimated total lai at low heights and underestimated it above 15 m fig 7 liana leaves tended to occupy the higher strata of the canopy with 62 of the leaves found above 20 m when the forest is at the equilibrium although the overall liana lai was 6 9 2 4 of the total it grew to 17 3 6 4 for the 20 m 30 m stratum and to 38 3 22 0 for the 30 m 40 m stratum the significant proportion of liana leaves simulated below 5 m was due to the high liana seedling density as a result of our reproduction parametrization we point out that while the simulations considered 16ha of forest the tls scans were taken at 9 plots close to the flux tower and may not represent an accurate forest average introduction of lianas in the simulations did not significantly affect the total amount and distribution of leaves fig 7 dotted curve 3 3 simulations with and without lianas 3 3 1 biomass and forest structure we compared simulations with and without lianas to assess their impact on forest structure and carbon stocks one of the most dramatic effects of lianas was the reduced basal area across the entire forest succession after 50 years basal area of the forest with lianas was about half that of the forest without lianas 16 9 0 8 m2 ha 1 vs 30 6 1 5 m2 ha 1 respectively this difference was due to a lower plant density 10521 vs 11374 plants for the 16 hectares respectively and to a lower average tree diameter 16 2 9 2cm vs 18 9 13 3cm respectively the quadratic dependency of basal area upon dbh amplified this difference fig 8 a after the forest equilibrated mean basal area was still lower in the presence of lianas 28 4 2 2 m2 ha 1 vs 32 1 1 7 m2 ha 1 mean adult tree height was proportionally less impacted by lianas than basal area fig 8a because unlike basal area tree height is a concave function of dbh after 50 years mean adult tree height was 16 6 0 1 m for the simulation with lianas and 17 6 0 1 m for the simulation without lianas at the equilibrium there was no significant difference for this metric after year 60 of the simulation total lai was generally higher for the simulation with lianas however the difference was always less than 5 at the equilibrium lai for the simulation with lianas was about 2 higher than the non liana simulation with lianas accounting for circa 7 of the total leaf area results from simulations showed a strong impact of lianas on stand level above ground biomass the reduction of agb was more pronounced in the early stage of succession when lianas were more abundant fig 8 b c the maximum reduction in biomass was 43 when the forest was 45 years old after 100 years the reduction in agb was 13 and fluctuated between 5 and 11 after reaching the equilibrium the simulated value of agb at year 500 was 185 15t c ha 1 while empirical observations estimated agb at 186 7t c ha 1 rutishauser et al 2010 at the individual level we analysed the mean biomass increments averaged over the simulation from year 100 to year 500 for three different classes of liana infestation and for four different tree size classes as expected yearly biomass increments were lower with increasing liana load table 2 for all size classes for the taller canopy trees dbh 80cm mean biomass increment with high liana load was only 64 of the one for trees without lianas the biomass impact per pft showed a correlation with successional stage and with tree maximum height for example when a pioneer pft peaked in abundance its total biomass was up to 70 less upon inclusion of lianas pft 4 fig a 2 pfts that reach lower maximum heights were proportionally less impacted by lianas see pfts 1 2 and 3 of fig a 2 these results are consistent with the ability for lianas to move up the canopy and with the absence of any host pft specific process 3 3 2 carbon fluxes carbon fluxes were sensitive to the introduction of lianas in the simulations liana maximum photosynthetic rate was assumed to be higher than climax species but lower than pioneers table 1 as a result gpp in the simulation with lianas was lower maximum reduction of 46 at year 50 than in the simulation without lianas when pioneers are abundant that is until year 100 120 for the same reason gpp for the simulation with lianas was up to 20 higher at the equilibrium when the pft composition shifted towards shade tolerance maximum liana gpp was 5 5t c ha 1 y 1 at year 24 38 of the total gpp the average liana contribution to gpp for the years 400 to 500 was 1 9t c ha 1 y 1 23 of the total fig 9 a autotrophic respiration was also impacted by the introduction of lianas the trend in the first 100 years of succession was similar to the one observed for gpp with lianas reducing total respiration by up to 51 the simulated liana contribution to biomass respiration was high throughout the succession and at the equilibrium accounted for 54 of the total 3 44tc ha 1 y 1 respired by vegetation after year 250 biomass respiration with lianas was almost twice that of the simulation without lianas fig 9b these trends can be understood in light of the very low diameter growth rate for lianas which resulted in a large fraction of gpp respired back to the atmosphere nee converged to zero more slowly when lianas were included in the model this result was consistent with the observed ability for lianas to slow down the ecosystem succession tymen et al 2016 despite being positive for about 250 years lianas reduced the maximum yearly carbon uptake by about 50 fig 9c by integrating nee over the entire simulation we found that the total carbon sink for the liana free forest was 8 higher 261 4tc ha 1 vs 240 1tcha 1 there was a compensation effect of heterotrophic respiration which unlike biomass respiration was always lower when lianas were included fig a 3 the lower value of heterotrophic respiration was the consequence of a forest with lower agb but similar agb mortality rate at the equilibrium carbon residence time agb npp was 36 years regardless of the inclusion of the liana pft 4 discussion 4 1 model structure our liana model introduces a custom representation for the climber growth form that is able to distinguish between three phases of ontogeny fig 4 the probabilistic transition between the different growth phases was constructed based on logical assumptions in fact the model is likely overestimating the fraction of lianas in the climbing stage and smaller lianas should have a higher probability of being in the canopy compared to the current probability in the model for example compare fig 4 with fig 1 of kurzel et al 2006 albeit only qualitative this multi phase structure constitutes a significant improvement over the representation of lianas in ed where the climbing phase was not explicitly represented di porcia e brugnera et al 2019 future modelling efforts seeking to simulate lianas throughout their development will benefit from field data that discriminates lianas as self supporting climbing or in the canopy equally important for a realistic model will be a statistic mechanistic understanding of the transition from one stage to the next one of the key features of the formind model is the calculation of growth and respiration from observed diameter increments in woody vines annual diameter growth has been shown to be substantially lower than trees on average 1 4 mm y 1 vs 6 mm y 1 for the bci site putz 1990 as a result of a markedly different allocation pattern in our liana representation the low values of diameter growth prevented proper liana establishment as a correction we simulate growth of self supporting lianas as if they were trees this use of a virtual tree pft for lianas could be extended to canopy lianas especially when a tree trait distribution is similar to a liana trait distribution e g for leaf mass area wyka et al 2013 in addition to growth most other mechanisms describing lianas were built with some degree of speculation for example host dynamic the ability for lianas to change their host relies on the assumption that lianas will move across the canopy seeking better light conditions the probability which we assumed to be 10 y 1 to find a new host when light conditions are sub optimal needs additional enquiry even though experimental measurements will be challenging to obtain in the model host dynamic was also connected to tree mortality the fate of lianas on dying trees is still unclear and needs to be addressed with more observations if we are to construct a realistic mortality process finally although some studies have tried to understand liana impact on trees at an individual level dias et al 2017 ogawa et al 1965 van der heijden et al 2010 a more thorough analysis of shading and mechanical stress is needed to improve our tree penalization scheme 4 2 model findings size distribution size distributions result from the interplay of many different processes and being one of the most common field measurement are an important model benchmark our liana implementation proved successful in capturing the qualitative trend in size distribution fig 1 to confirm that the underlying processes and their parametrization are realistic these results will need additional testing in sites with different external conditions total liana density was about twice the observed one however it should be noted that compared to other sites liana density in paracou is particularly low dewalt et al 2015 the bulk of this overestimation was for lianas with dbh 5cm which may be explained by the high value of the recruitment rate parameter and to the fact that observations in the old growth paracou plots may not have had tree fall disturbances recently clustering and horizontal distribution our model predicted a clumped individual distribution with an average cluster size of around 1 6 lianas since this result was independent of the attachment mechanism the clustering is likely to be driven by host change and light and space competition from a modelling point of view host change is a necessary feature to ensure that lianas stay in the higher part of the canopy although this assumption of lianas seeking better light condition is reasonable its mechanistic representation and the host change probability should be evaluated against new observations if host change makes liana more likely to colonize the same trees light and space competition bind this process by making large clusters prone to conspecific competition as we have seen the exponential decay constant of liana cluster size is similar between modelled and observed data this suggests that the implementation of light competition and crowding mortality that was developed for trees can be generalized to lianas leaf profiles in lowland tropical forests light is one of the most limiting resources kitajima et al 2005 vertical leaf profiles are an important metric to understand how light is extinguished while reaching the understory recent technological developments such as laser scanning krishna moorthy et al 2018 have allowed to measure these quantities with greater precision however these instruments cannot parse the different components of the leaf profile i e species growth form in our simulations lianas deployed the majority of their leaves in the higher part of the canopy this result while consistent with lianas expected behaviour is a significant improvement over the previous liana model where liana leaves were concentrated only at low heights the introduction of lianas did not significantly alter the forest lai as a whole suggesting that lianas substitute tree leaves in similar spatial locations the small lai bulge at low heights was due to the large number of saplings and to the contribution of plots where a tree fall event occurred although observations confirm that lianas abound in correspondence of canopy openings schnitzer et al 2000 the parameters for seed dispersion may be overestimated in this sense the addition of more liana pfts with a broad trait dispersion could be a solution to ensure establishment under the highly variable light conditions of the 500 years simulation biomass liana impact on agb was strong both at an individual level and at the landscape level at the individual level tree penalization resulted in smaller biomass increments when the host had a high liana load due to a reduced lai to test our penalization scheme field measurement of trees with and without lianas would be needed as it is unlikely to find trees with similar characteristics but varying liana loads such measurements can only be of statistical nature at the plot level these types of comparison have already started van der heijden et al 2019 and may be used in the future to better parametrize liana burden on trees carbon fluxes overall most of the reduction in biomass could be accounted for by the very high rate of respiration in lianas discussed in appendix b as formind does not explicitly represent leaves and fine roots much of the gpp that is not used for growth is thus respired in reality part of this gpp is likely to be used for production of tissues with fast turnover rates like leaves or fine roots zhu and cao 2010 since the model does not explicitly represent these allocation processes the carbon is directly re emitted through respiration instead of going through litter decomposition in other words the model is overestimating autotrophic respiration and underestimating heterotrophic respiration given that the carbon residence time of leaves and fine roots is short this simplification may have little consequences in terms of carbon fluxes an inclusion of more realistic mortality impact of lianas to their host such as by considering mechanical stress may result in shorter carbon residence time and thus an even lower forest carbon sink potential 4 3 additional considerations in the current stage the model clumps the entire diversity of climbers into one liana pft this is a strong limitation as climbers appear in about half putz et al 1991 of vascular plant families and their trait spectra are known to be dispersed wyka et al 2013 the development of additional liana pfts for example a shade tolerant liana nabe nielsen 2004 may also reduce the strong impact of lianas on respiration furthermore many aspects of the climber growth form have not been considered e g below ground competition or host specificity as many studies have linked liana abundance to hydrology for example finding correlations between liana abundance and mean annual precipitation or precipitation seasonality dewalt et al 2000 a greater model complexity needs to be incorporated to make general predictions of ecological value from a computational point of view the addition of lianas often introduces a second order cost because of the interaction with trees for example when calculating the lai penalization due to lianas the model needs to check each tree for all of its lianas an additional computational burden is due to the use of open arrays these could be simplified by assuming a maximum number of lianas per tree in terms of model structure both formind and ed are now able to simulate trees grasses and lianas both models make use of keywords to create parallel computational regions for each growth form in the case of formind or other models written in object oriented languages we advise for a greater use of polymorphism and inheritance to make the code more compact and abstract for example most allometric equations now require an explicit check of the growth form whenever they are used the use of a parent plant class could help to hide these specific implementations from where these methods are called finally for researchers interested in implementing lianas in different models we advise to start with two liana pfts although the complexity from one pft is normally more than enough to start the use of a second dummy pft can help to create more robust and general code from the start 5 conclusions liana modelling is still in its infancy and this work should lay the ground for additional investigations including with the use of new modelling frameworks with formind we were able to capture many aspects of a liana infested forest in particular we concentrated on correctly reproducing demography and spatial distributions in the current stage the model could already be tested on real scenarios for example quantifying liana impact on carbon stocks in disturbed or logged forests or making forecasts for the future of liana removal plots to expand the applicability of the present model for example to produce regional estimates we advise to first test the model under the extremely variable climate soil and topographical conditions under which lianas are found author contributions mdp lead the model development and data analysis with contributions from ft and rf all authors contributed to discussions and revised the final version of the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was funded by the european research council starting grant 637643 treeclimbers fwo research project g018319n and the special research fund of ghent university bof project 01n00816 special thanks to m piantoni for her help with the graphics of fig 1 
24792,network analysis is an indispensable part of ecological studies specifically networks have played a pivotal role in studying the diversity dynamics and functionality of pollination systems recording plant pollinator interaction networks is a laborious task prone to missing or false negative interactions several methods enable the assessment of sampling completeness of the network with the use of species accumulation curves or chao estimators however these methods do not provide a way to identify which interactions might be missed in the field methods that enable a more directed and focused field sampling are needed such methods would greatly benefit plant pollinator studies and network studies in general we additively decomposed the information of an interaction matrix into three parts the difference in importance of the species the specificity of the interactions and the generality of the interactions this information is exploited by a previously proposed linear filtering method to re score absent interactions thus pinpointing the likely missing interactions we evaluated this approach using null models intensive cross validation as well as external validation with the web of life database by means of a case study we provide insight into the structure of the network using an information theoretic approach we show how to use linear filtering to suggest missing interactions a thorough evaluation shows that these results are both statistically stable and useful to guide the search for missing interactions in real world networks the non uniformity of pollination interactions can be quantified using information theory and extracted using linear filtering our work can be valuable as a way to study different interaction networks as well as a tool to help identifying missing interactions keywords pollination false negatives missing interaction networks information theory linear filtering 1 introduction networks are widely used in ecology to study species interactions ings et al 2009 poisot et al 2016b over the past decade the use of networks increasingly gained attention in pollination ecology as a way to address a variety of ecological questions these include assessing the role of different drivers such as land use change weiner et al 2014 urbanization reviewed by harrison and winfree 2015 phenological shift burkle et al 2013 and invasive plant or animal species bartomeus et al 2008 de m santos et al 2012 geslin et al 2017 in the current worldwide pollinator decline pollination networks are typically represented as bipartite graphs these consist of a set of pollinator species a set of plant species and the biological interactions between them bascompte and jordano 2013 in this work all of the pollinators are bees we will use the term bee species throughout unless the term pollinator is more appropriate the collection of bee pant interaction networks is mostly based on field research which is very labor intensive and seldom yields all occurring interactions jordano 2016 moreover the contact networks are often very complex and highly nested with few species having many interactions and many specialists with few interactions bascompte et al 2003 it is thus possible that some interactions especially those of rare species are missed during the period of the sampling campaign for example fijen and kleijn 2017 recorded a total of 36 pollinator species on a single flower during three consecutive days of observation on day three they still observed eight pollinator species that were not recorded during the previous days of observation as a consequence even with a substantial sampling effort most plant pollinator interactions remain undersampled chacoff et al 2012 this undersampling may affect the network properties derived from the collected data banašek richter et al 2004 dorado et al 2011 nielsen and bascompte 2007 different methods have been put forward to overcome the undersampling problem such as the use of pollen barcoding from caught bees pornon et al 2016 the use of trap nests chacoff et al 2018 or by merely increasing the sampling time sampling time and effort are often the limiting factors and a directed sampling approach where specific interactions are targeted would be beneficial to map all the plant pollinator interactions present as shown by chacoff et al 2012 an equal sampling effort for all plant species is often insufficient to record all interactions methods that predict the potentially missing interactions in a network could contribute considerably to solving the problem of undersampling such methods allow for a more directed and focused sampling effort here we provide a framework to identify possible missing interactions in field collected pollination networks based on the distribution of the sampled interactions using known metrics from information theory entropy mutual information and variation of information to describe the network and algorithms based on the network structure between specialists and generalists we can predict which missing interactions are more likely to occur in nature we decompose the distribution of interactions and quantify 1 the relative importance of the different species in the network 2 the specificity of the interactions between bee and plant species and 3 the redundancy or generality of the interactions we validate the information theoretic metrics with four established null models for networks delmas et al 2018 we then exploit the non uniformity of the interactions using the linear filtering method proposed in stock et al 2017 we validate this method as a recommender system to suggest potential missing interactions we do this using both fair cross validation on our pollination network and external validation on the complete web of life database 2 2 http www web of life es this work can be seen as a case study showing how these previously disconnected ideas can work together 2 materials and methods 2 1 the florabeilles pollination network for this study we used the florabeilles dataset 3 3 http www florabeiles org this database has freely accessible data on plant bee interactions that have been reported in the scientific literature over mainland france it records all observed interactions aggregated over time and space the florabeilles dataset hence records which bee plant interactions are biologically plausible while new data are continuously being added at present it holds records for six bee families i e andrenidae 44 species apidae 68 species colletidae 20 species halictidae 57 species megachilidae 114 species and melittidae 2 species for each interaction reported in published work research paper or book information about the interaction is extracted including bee and plant species names forage nectar or pollen gathering number of foraging bees and whether pollinating activity is confirmed this set was then completed by recording the date country town or gps location and habitat type for each interaction for this study we used the binary incidence matrix y where the m rows represent the different bee species and the n columns the plant species we set y i j 1 if bee species i visits plant species j and y i j 0 otherwise this incidence matrix is shown in fig 1 it contains 305 bee species and 452 plant species the network is quite sparse the connectance density the ratio between the number of observed interactions and the number of potential interactions ρ equals 1515 137 860 1 10 the histograms of the number of interactions per bee and plant species are also shown in fig 1 as apparent from the incidence matrix the network is quite nested with a network nestedness bastolla et al 2009a of 0 1015 2 2 information theory for interaction networks ecological interactions between two species at two trophic levels in casu plants and bees are not purely deterministic though interactions are often facilitated by a morphological of physiological complementarity of the two partners the observed interaction network is also a function of the species abundances since many bee species pollinate several plant species and various plant species are pollinated by multiple bee species there is inherent freedom of choice associated with a pollination network for these reasons several authors e g kallimanis et al 2009 poisot et al 2016a have argued that the language of probability theory can adequately describe interaction networks as such we compute the m n probability matrix p as 1 p i j y i j k 1 m l 1 n y k l it is assumed that all interactions are independent and can be represented as a series of bernoulli trials as put forward in the framework of poisot et al 2016a the number pij can be interpreted as the probability that bee species i visits plant species j hence the matrix p embodies a discrete bivariate probability distribution that describes how the two trophic levels interact importantly we assume that the probability matrix p is fixed the following framework relates to how this matrix is structured not to how accurately it is sampled the marginals of this distribution represent the relative abundances of the bee species and the plant species these marginals are 2 p i b j 1 n p i j and 3 p j f i 1 m p i j where p i b is the probability that a visit is made by bee species i and p j f the probability that plant species j is visited regardless of its ecological partner note that we have implicitly introduced two random variables b and f for the bee and plant species respectively 4 4 we use the capital letter f for flower to refer to the plants given that the symbol p is already used to denote the normalized incidence matrix given such a probabilistic interpretation some metrics borrowed from information theory can characterize the network information theory see mackay 2003 for an accessible introduction is a branch of probability theory it provides a quantitative framework for determining how efficiently information can be compressed or communicated the use of information theory to study stability and efficiency in ecological networks was first proposed by macarthur 1955 we refer to ulanowicz 2001 2011 for a modern synthesis foremost we introduce the concept of entropy which is defined for a discrete random variable x as 4 h x x prob x x log 2 1 prob x x 5 x prob x x log 2 prob x x where the sum ranges over the domain of x by convention 0 log 0 is evaluated as 0 from here on we will drop the subindex in the logarithm log will always denote the logarithm of base two the units of the information theoretic indices are thus expressed in bits as opposed to expressing them in nats when using the natural logarithm the interpretation is that the uncertainty associated with an index can be cleared by asking a specific number of binary questions for example if the entropy of b is 5 3 bits this means that for a random interaction one has to pose on average 5 3 yes or no questions e g is it a polylectic bee species is it a bee species of the megachilidae family to identify the bee species for the marginals of the bee eq 2 and plant species eq 3 the entropy is computed as 6 h b i 1 m p i b log p i b and 7 h f j 1 n p j f log p j f the joint entropy of the bivariate distribution is computed as 8 h b f i 1 m j 1 n p i j log p i j in the context of pollination the marginal entropies quantify the equality of the bee and plant species larger entropies indicate that these marginals are closer to a uniform distribution hence no specific species is dominating the network the joint entropy can be used to analyze the distribution of the interactions i e whether individual interactions dominate or not we can also define the conditional entropies for the two variables 9 h b f i j p i j log p i j p j f and 10 h f b i j p i j log p i j p i b they express the entropy of bees resp plant species given that the plant resp bee species is known suppose that there exists a one to one correspondence between bee and plant species in that case the entropies of the marginal distributions are maximal because both distributions are uniform however when looking at a given plant species it is only visited by a single bee species hence the conditional entropy is zero there is no freedom of choice anymore the specificity of the interactions is quantified more directly by the mutual information 11 i b f h b h b f 12 h f h f b 13 i 1 m j 1 n log p i j p i b p j f for which i b f i f b and i b f 0 hold the mutual information measures the average reduction in uncertainty about b given that we know f or vice versa to be precise it represents the reduction of the average number of binary questions that have to be posed to identify an interaction given that one knows one of the partners compared to not knowing the partner the mutual information represents the efficiency of the interactions as high mutual information implies that the species are highly specialized towards a single or a few ecological partners we denote by ub and uf uniformly distributed random variables on a universe of size m and n respectively the entropy of these random variables is given by 14 h u b log m 15 h u f log n 16 h u b u f log m n the difference in entropy between the distributions of b and f and uniform distributions on the same underlying universe is defined as 17 d b h u b h b 18 d f h u f h f and 19 d b f h u b u f h b h f these metrics quantify to what extent the marginals deviate from a uniform distribution d b f represents the average reduction of binary questions needed to pose to identify an interaction pair compared to the case when all marginals would be uniform i e all species are equally important for example if d f is close to zero a single plant species dominates the network whereas a more significant value indicates that many plants are essential finally the variation of information sometimes also called the entropy distance is defined as 20 v b f h b f i b f 21 h b f h f b 22 i 1 m j 1 n p i j log p i j p i b log p i j p j f this index shows the residual freedom of choice of the bee species given that the plant species are known and vice versa the variation of information represents the average number of binary questions that one needs to pose to identify one species in an interaction pair given that one knows the other one it can be seen as a measure of network stability rutledge et al 1976 the following balance equation valverde albacete and peláez moreno 2010 relates the difference in entropy of a distribution compared to the entropy of the corresponding distribution with uniform marginals the mutual information and the variation of information 23 h u b u f log m n 24 d b f 2 i b f v b f or decomposed in the separate contributions of the bee species 25 h u b log m d b i b h b f and for the plant species 26 h u f log n d f i f h f b these equations show how the maximal hypothetical information of an ecological network is decomposed in a term quantifying the degree that some species are more important or active than others d a term about the specific interactions between species i and the remaining freedom of interactions v this equality is illustrated for some small example networks in fig 2 as can be seen efficiency or specificity measured by the mutual information and stability or freedom of choice measured by the conditional entropy are antagonistic one comes at the cost of the other if the freedom of choice of the species increases meaning they have a wider variety of interaction partners the stability of the overall network grows but the efficiency of their interactions decreases 2 3 linear filtering method the linear filtering is a method suggested in stock et al 2017 to find missing interactions in an interaction network it operates by computing a score for every interaction i j in the incidence matrix 27 f i j α 1 y i j α 2 1 m k 1 m y k j α 3 1 n l 1 n y i l α 4 1 m n k 1 m l 1 n y k l or using the introduced notation 28 f i j α 1 y i j α 2 p i b α 3 p j f α 4 ρ with parameters α 1 α 2 α 3 α 4 0 1 and α 1 α 2 α 3 α 4 1 these parameters have the following interpretation α 1 determines the importance of an observed or absent interaction for the given bee plant species pair given by yij α 2 determines the relative weight of the promiscuity of the bee species given by p i b α 3 determines the relative weight of the number of pollination partners of the plant species given by p j f α 4 determines the importance of the global connectedness or density of the network given by ρ the linear filtering method is illustrated in fig 3 in addition to being a weighted average of four averages the linear filter can also be obtained as a special case of a more general pairwise learning framework stock et al 2018a pairwise learning methods typically make use of similarity between the species e g how similar bee species are with regard to their traits when this similarity is only based on the identity of the species i e these species are the same or not the pairwise prediction function takes to the form of eq 28 the computed scores can suggest missing interactions in the network as negative interactions with a high score are likely missing interactions after scoring all interactions in the network one has to rank the negative interactions according to their filtered value the top of these interactions is expected to be enriched with missing interactions i e plant bee interactions that might have been missed during field sampling the effectiveness of the linear filtering method was demonstrated in the original publication where validation was performed on pollination networks host parasite networks food webs and prey predator networks of various sizes the effectiveness of the linear filtering method can be assessed through cross validation we perform every step of the cross validation by setting a positive interaction to zero computing the score for this interaction and repeating this for every interaction the score computed for every interaction represents the score of this interaction using the linear filter of eq 28 given that the interaction value would be zero this score does not depend on the original interaction score this approach is called zero one out zoo cross validation stock et al 2018b and is a suitable method for dealing with data where one has only positive observations there is a simple formula to compute the zoo values efficiently stock et al 2017 29 f i j zoo f i j α 1 α 2 m α 3 n α 4 m n y i j the importance of the above formula is that f i j zoo can be shown the be independent of yij its effect is cancelled in the two terms as such eq 29 can be used to assess whether the linear filtering method can detect missing interactions here we will set α 1 α 2 α 3 α 4 0 25 given equal importance to all averages these values were recommended in the original publication based on an exhaustive search using various ecological networks stock et al 2017 the linear filtering for a given interaction then merely yields the average of the interaction value the row average the column average and the total average of the incidence matrix 2 4 evaluation in this work we perform a thorough evaluation of our approach foremost we evaluate the linear filtering method as a typical recommendation system here we quantify how well this method can detect missing negatives in the network and discuss how to select a suitable threshold next we consider null models for ecological networks to show how the information theoretic decomposition changes under several null models finally we cross check the web of life database to assess whether the linear filtering method can detect convincing missing interactions 2 4 1 evaluation of the linear filtering method the linear filtering method is evaluated in a similar way as recommendation engines isinkaye et al 2015 this means that the linear filtering method is used to provide a real valued score to rank interactions from likely negative to likely positive interactions 5 5 note that we explicitly distinguish between missing interactions which occur in reality but are not sampled in the network and false negative interactions which are present in the sampled network but are not predicted as positive by the model interactions that receive higher scores are more likely to be positive interactions whereas lower scores indicate negative interactions a negative interaction that nevertheless receives a high score is a potential missing interaction to predict an interaction we have to provide a threshold t 0 1 for the computed scores scores higher than or equal to t are treated as positive interactions scores lower than t are treated as negative interactions hence when predicting an interaction four situations can arise depending on whether the interaction is positive or negative in the network and whether it was predicted as positive or not for these combinations the standard terminology is outlined in table 1 the number of predictions in each class depends on the threshold t when the threshold is set high the criterion for predicting a positive interaction is more severe fewer false positive predictions will be made at the cost of missing some true positive interactions if the threshold is set low however more interactions will be predicted as positives but these will be enriched with false positives since the value of the optimal threshold depends on the preferences of the user we evaluate the linear filtering method using metrics agnostic to its value to make the dependence on the threshold explicit we will use the notations tp t fp t fn t and tn t to denote the number of interactions in each class using this notation the false positive rate fpr and true positive rate tpr are defined as 30 tpr t tp t tp t fn t 31 fpr t fp t fp t tn t the tpr represents the fraction of true positives in the predicted interactions i e the fraction of interactions suggested by the method that were indeed interacting the fpr represents the fraction of interactions that have been wrongly deemed to occur i e the fractions of interactions suggested by the method that are not interacting in reality these two metrics evaluate the quality of the predicted interactions a slightly different and complementary view can be obtained by considering precision and recall defined as 32 precision t tp t tp t fp t 33 recall t tp t tp t fn t the precision indicates the fraction of positive interactions that have been found to occur i e the fraction of interactions that have been detected by the method recall is merely a different name for the true positive rate since all the metrics have trade offs depending on the threshold it is common practice to plot each of the two metrics for varying values of t in this way no choice of threshold needs to be made beforehand and the user can select a threshold based on an acceptable trade off between precision and recall the curve that plots the tpr as a function of the fpr is often called a roc receiver operator characteristic curve we refer to fawcett 2004 for an in depth discussion on the use of roc curves the area under this curve is called unsurprisingly the area under the roc curve auroc it is a value between zero and one values greater than 0 5 indicate that the method is better than random at selecting positive interactions the precision and recall are set out in a precision recall curve the area under this curve the aupr is also an important metric for diagnostic purposes the literature recommends to additionally use aupr for highly unbalanced datasets where negative observations greatly outnumber positive observations as is commonly the case in ecological data davis and goadrich 2006 sofaer et al 2018 this unbalancedness is the case here since our network is quite sparse with a density of 1 1 we will compute all performance metrics on the matrix f zoo hence our evaluation assesses the linear filtering method s true potential to find missing interactions in the network since every prediction is performed on a simulated negative interaction 2 4 2 null models for the information theoretic indices null models are commonly used in ecology to verify that the computed quantities in casu the information theoretic metrics and the performance of the linear filtering method do not arise by chance dormann et al 2009 for ecological networks one frequently uses topological methods in which random networks are generated according to some null model delmas et al 2018 these null models randomly generate networks with the same number of species and expected number of interactions but with vastly different topologies interestingly these null models correspond to specific parameterizations of the linear filtering method in this interpretation the score fij computed by eq 28 is used as the probability of an interaction between species i and j we will consider four standard null models for ecological networks each embodying a different type of bias in how the network is sampled type i fortuna and bascompte 2006 the interactions are sampled randomly every interaction is independently sampled from a bernoulli distribution with a probability of success equal to the density of the original network this model assumes that the interactions are distributed randomly between the bee and plant species the corresponding null distribution can be obtained using the linear filtering method setting α 1 0 α 2 0 α 3 0 and α 4 1 type ii bascompte et al 2003 the interactions are sampled according to the degree distribution of the bee and plant species this means that under this null model the probability of an interaction between bee species i and plant species j is m n p i b p j f 2 the corresponding null distribution can be obtained using the linear filtering method setting α 1 0 α 2 0 5 α 3 0 5 and α 4 0 type iii bee species the interactions are sampled according to the marginal distribution of the bee species i e the probability of an interaction between bee species i and plant species j is n m p i b under this null model the corresponding null distribution can be obtained using the linear filtering method setting α 1 0 α 2 1 α 3 0 and α 4 0 type iii plant species similarly the interactions are sampled according to the marginal distribution of the plant species i e the probability of an interaction between bee i and plant j is m n p j f under this null model the corresponding null distribution can be obtained using the linear filtering method setting α 1 0 α 2 0 α 3 1 and α 4 0 the four null models are illustrated in fig 4 we compute the information theoretic indices and the auroc using zero one out cross validation for 100 simulations according to the four different null models these are averaged and hence estimate the expected value of the quantities under the respective null distributions 3 results 3 1 information theoretic decomposition table 2 shows the information theoretic decomposition for the complete network and the bee and plant species separately note that the v component for the marginals is used to indicate the conditional entropy i e h b f and h f b respectively firstly we note that the d component of the information theoretic decomposition is quite low for both the bee and plant species and the network in general the number of interactions per bee species p b and the number of interactions per plant species p f are distributed relatively homogeneously over the different species this is not surprising even though certain bees species can interact with many different plant species e g apis mellifera interacts with 164 of the 452 plant species a large number of bee species however interact only with a single plant species likewise some plant species have many pollination partners e g centaurea solstitialis and calendula arvensis are each visited by 35 bee species while many plant species interact only with one or two bee species a deviation from the uniform distribution explains a little more than 10 of the information in the network the mutual information is symmetric since it quantifies how specific the interactions are between bee and plant species the mutual information component is roughly five times as large as the d component and 1 5 times the variation of information v component the specificity of the interactions explains more than half of the information of the network the remaining part of the information in the network is given by the v component the variation of information for the complete network this is 6 bits or 35 of the total information in the network it shows that the species are not completely specific learning the identity of the bee species only partially reduces the uncertainty of the identity of the plant species looking at the individual distributions of the bee and plant species we see that h b f 2 64 is higher than h f b 3 35 thus the plant species act in a more generalist way than the bee species this is also confirmed when looking at the relative fractions in the information decomposition 3 2 performance of the linear filtering method we apply the linear filtering method on the pollination network and compute the zero one out values we assess the performance using the roc and precision recall curves shown in fig 5 the roc curve is consistently above the first bisector dotted line indicating that for every threshold the linear filtering method performs better than random selection in detecting positive interactions the auroc is 0 8393 meaning that when randomly selecting a positive and a negative interaction in the network the linear filtering method gives a higher score to the positive interaction in 83 93 of the cases we see that the first part of the roc curve has a very steep increase at a threshold of about 10 5 about 20 of the positive interactions can be recovered with virtually no false positives lowering the threshold allows us to recover a more substantial fraction of the occurring interactions but a much larger number of false negatives will seep in for example a slightly lower threshold will recover about 70 of the interactions but will result in roughly 20 of mistakes any threshold will always result in errors one way or another diagnostic tools such as the roc and pr curves allow the field ecologist to select a suitable trade off a complementary picture can be obtained using the precision recall curve a strict cutoff of the score will have a nearly maximal precision all selected interactions are true positives but only a tiny fraction of the positive interactions are found a second local maximum is found at t 7 10 6 this recovers about 15 of the interactions in a top that has a bit less than 40 of the interactions correctly predicted as positive the aupr is 0 1179 which is substantially larger than the connectance density of 0 011 which would be the performance obtained by randomly scoring interactions 3 3 validation using null models table 3 shows the average of the indices and auroc computed on 100 simulations using the four null models the null model i distributes the interactions randomly it destroys all of the network structure as the auroc demonstrates which drops to about 50 it is no longer possible to detect missing interactions using the linear filtering method in the information theoretic decomposition this is reflected in the decrease of the d component this means that the marginals are now approximately uniformly distributed we also observe an increase in the i component this is because interactions are distributed randomly and species appear much more specialized under this null model the indices cannot distinguish between interactions that are specific in the actual network and specificity is caused by change due to reshuffling in the two instantiations of null model iii either the marginals of the bee or the plant species are used to sample a new network both seem to perform quite similarly suggesting the equal importance of both bee and plant species this structure can be explored by the linear filtering method though with a deteriorated performance compared to the original network here the average information decomposition matches the decomposition of the original network closely null model ii samples the interactions concerning both marginals it seems to result in worse performance for the linear filtering method than model iii as noted in the methods section this is likely because this null model does not adequately reproduce the original degree distributions of the original network 3 4 validation using web of life we also compared our results with the pollination networks from the web of life database for increasing thresholds of t 0 01 t 0 05 and t 0 1 we could recover all positive interactions in the florabeilles dataset we counted the number of false positives i e interactions predicted by our model not recovered in the florabeilles dataset based on the florabeilles dataset as these interactions might be missing we screened all plant pollinator interactions in web of life and recorded the number of confirmed interactions in table 4 several of the missing interactions detected by the linear filtering method could be found in the literature for example the four interactions confirmed using threshold t 0 1 contain apis mellifera interacting with the plants calendula arvensis petanidou 1991 scabiosa atropurpurea petanidou 1991 epilobium hirsutum bundgaard 2003 and rubus ulmifolius herrera 1988 4 discussion pollination networks and ecological networks in general are not random but are often highly structured bascompte and jordano 2013 holt and bonsall 2017 vázquez et al 2009 in the last decades research has found them to be nested bascompte et al 2003 bastolla et al 2009b staniczenko et al 2013 modular olesen et al 2007 and low dimensional eklöf et al 2013 ecologists have proposed numerous metrics to quantify this structure delmas et al 2018 information theory is a particularly attractive lens to look at species interaction networks since it provides a mathematically sound way to quantify how distributions the trophic levels of an interaction network are coupled in this case we can quantify the average promiscuity and specificity in a species interaction network we are not the first to explore the link between the metrics of information theory and ecological networks e g macarthur 1955 ulanowicz 2001 blüthgen et al 2006 dormann et al 2009 and ohlmann et al 2019 however we are the first to explore the information in the network using the relatively obscure balance equation of valverde albacete and peláez moreno 2010 an important distinction has to be made between modeling the network on the one hand and modeling the interactions in the network on the other hand this work only relates to the former the network is considered fixed we do not conjecture any generative process of how the network is realized either as an incomplete field sample or arising from species interacting depending on their abundances and traits as such our information theoretic indices as most metrics used in ecology can only give insight into the distribution of the interactions within the network only the use of null models can yield insight into how assumptions on the network engendering influence these metrics there is a tantalizing relation between the information theoretic decomposition of section 2 2 the ecological null models of section 2 4 2 and the linear filtering method of section 2 3 foremost many ecological structures can be explained by neutral effects canard et al 2012 rosindell et al 2012 i e species interactions only driven by relative abundances neutral effects are captured by the i component which captures the degree to which observed interactions deviate from what would be expected by a neutral assumption in statistical analyses one can use the null models ii and iii to test if observed effects could have arisen solely due to the relative abundance of one or two of the trophic levels similarly the linear filtering method allows for the filtering of any non neutral effects destroying parts of the network s information content for the florabeilles plant pollinator dataset we found that the structure is primarily dominated by specific interactions between species i component and to a lesser extent by the v component quantifying the generalist nature of the species the relatively low value of the d component shows that the marginal distributions are far from uniform some species have many more interacting partners than others our filtering method exploits this to find likely missing interactions by definition the mutual information component is due to specific interactions between species as such we cannot recover this connection using the marginal distributions as exploited by the linear filtering method to predict these interactions one needs external information such as species traits and phylogeny to build a predictive model santamaría and rodríguez gironés 2007 the linear filtering method was able to detect all true positive interactions present in the dataset with thresholds up to 0 1 all potential missing interactions put forward by our approach were validated with the web of life database up to 16 of these missing interactions were confirmed depending on how stringent one sets the threshold applying our method therefore allows for detecting missing interactions within a plant pollinator network as no additional features need to be assigned to either plant or pollinator species the method can be easily applied to documented plant pollinator networks or any other types of networks in pollinator ecology current standard protocols to document plant pollinator interactions typically include transects timed observations or both however both methods have their shortcomings most lead to documentation of a network that is distinct from the real network present due to sampling effects vázquez et al 2009 moreover gibson et al showed that the method used to construct plant pollinator networks i e transects or timed observations significantly affects network metrics gibson et al 2011 while transects are faster in generating vast quantities of data they are more prone to missing rare interactions timed observations on the other hand are less prone to missing rare observations even with the use of timed observations and a considerable amount of observation time there is still a substantial undersampling of the real network present chacoff et al 2012 this undersampling of the network could lead to false or obscured conclusions regarding the network structure since it may change network metrics dorado et al 2011 nielsen and bascompte 2007 classically rarefaction and species accumulation curve analyses are used to estimate the sampling completeness in mutualistic networks chacoff et al 2012 nielsen and bascompte 2007 several other methods have recently been developed to help in the assessment of sampling completeness macgregor et al 2017 and to provide guidance on the minimal sample size needed to estimate network metrics reliably casas et al 2018 however there is still a lack of methods that offer a guided and directed sampling method to enable a full sampling of the real network here we put forward a simple but effective way able to assist in a directed sampling design to complete interaction network datasets we investigated the non uniformity of the marginal and joint distributions of the interactions of a plant pollinator network with the use of information theory removing this structure by shuffling under null models deteriorates the performance of the linear filtering method illustrating the relationship between the structure of the network and the effectiveness of our methods these null models are intrinsically linked with the linear filtering method as they correspond to ignoring parts of the information in the network by setting specific parameters to zero by using all the information that can be extracted by the linear filtering method we were able to predict missing interactions which were validated with external databases a plenitude of methods exists that is capable of recovering missing edges in networks a similar model to infer missing pollination interactions called the interaction distribution was proposed by sorensen et al 2011 this method fits two dirichlet distributions to estimate the probability of a pollinator species visiting plant species on the one hand and the likelihood of a plant species being visited by a particular pollinator species on the other hand a similar recommender systems approach was used by desjardins proulx et al 2017 to find missing species using the popular k nearest neighbor method more general methods to detect missing links in networks are often based on bayesian reasoning guimerà and sales pardo 2009 newman 2018 the linear filtering method can be considered as the simplest possible of such methods as it only exploits the marginal interaction probabilities in a weighted sum in this work we provided a theoretical and empirical justification for using the linear filtering method as a reliable baseline 5 conclusion in this work we have shown how information theory can decompose the information content of a pollination network into the relative contribution of the species the specificity of the interactions and the redundancy or freedom of the interactions to the best of our knowledge we are the first to translate the information theoretical decomposition of valverde albacete and peláez moreno 2010 into an ecological context the difference in the importance of the species and the redundancy in the network can be exploited by the linear filtering method to find missing interactions the performance of this linear filtering method was confirmed by stringent performance evaluation as well as validation with external data availability the analysis of the network was done in ecologicalnetworks jl poisot et al 2019 a package in julia to study species interaction networks in which the information theoretic methods as well as the linear filtering method are implemented the pollination dataset and code will be made available upon acceptance declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests credit authorship contribution statement michiel stock conceptualization formal analysis methodology software supervision validation visualization writing original draft niels piot conceptualization methodology validation writing original draft sarah vanbesien data curation investigation validation bernard vaissière resources clémentine coiffait gombault resources guy smagghe supervision writing review editing bernard de baets supervision writing review editing acknowledgements m s is supported by the research foundation flanders fwo17 pdo 067 g s received support from the research foundation flanders via the eos program clips project 30947854 supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j ecolmodel 2020 109161 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
24792,network analysis is an indispensable part of ecological studies specifically networks have played a pivotal role in studying the diversity dynamics and functionality of pollination systems recording plant pollinator interaction networks is a laborious task prone to missing or false negative interactions several methods enable the assessment of sampling completeness of the network with the use of species accumulation curves or chao estimators however these methods do not provide a way to identify which interactions might be missed in the field methods that enable a more directed and focused field sampling are needed such methods would greatly benefit plant pollinator studies and network studies in general we additively decomposed the information of an interaction matrix into three parts the difference in importance of the species the specificity of the interactions and the generality of the interactions this information is exploited by a previously proposed linear filtering method to re score absent interactions thus pinpointing the likely missing interactions we evaluated this approach using null models intensive cross validation as well as external validation with the web of life database by means of a case study we provide insight into the structure of the network using an information theoretic approach we show how to use linear filtering to suggest missing interactions a thorough evaluation shows that these results are both statistically stable and useful to guide the search for missing interactions in real world networks the non uniformity of pollination interactions can be quantified using information theory and extracted using linear filtering our work can be valuable as a way to study different interaction networks as well as a tool to help identifying missing interactions keywords pollination false negatives missing interaction networks information theory linear filtering 1 introduction networks are widely used in ecology to study species interactions ings et al 2009 poisot et al 2016b over the past decade the use of networks increasingly gained attention in pollination ecology as a way to address a variety of ecological questions these include assessing the role of different drivers such as land use change weiner et al 2014 urbanization reviewed by harrison and winfree 2015 phenological shift burkle et al 2013 and invasive plant or animal species bartomeus et al 2008 de m santos et al 2012 geslin et al 2017 in the current worldwide pollinator decline pollination networks are typically represented as bipartite graphs these consist of a set of pollinator species a set of plant species and the biological interactions between them bascompte and jordano 2013 in this work all of the pollinators are bees we will use the term bee species throughout unless the term pollinator is more appropriate the collection of bee pant interaction networks is mostly based on field research which is very labor intensive and seldom yields all occurring interactions jordano 2016 moreover the contact networks are often very complex and highly nested with few species having many interactions and many specialists with few interactions bascompte et al 2003 it is thus possible that some interactions especially those of rare species are missed during the period of the sampling campaign for example fijen and kleijn 2017 recorded a total of 36 pollinator species on a single flower during three consecutive days of observation on day three they still observed eight pollinator species that were not recorded during the previous days of observation as a consequence even with a substantial sampling effort most plant pollinator interactions remain undersampled chacoff et al 2012 this undersampling may affect the network properties derived from the collected data banašek richter et al 2004 dorado et al 2011 nielsen and bascompte 2007 different methods have been put forward to overcome the undersampling problem such as the use of pollen barcoding from caught bees pornon et al 2016 the use of trap nests chacoff et al 2018 or by merely increasing the sampling time sampling time and effort are often the limiting factors and a directed sampling approach where specific interactions are targeted would be beneficial to map all the plant pollinator interactions present as shown by chacoff et al 2012 an equal sampling effort for all plant species is often insufficient to record all interactions methods that predict the potentially missing interactions in a network could contribute considerably to solving the problem of undersampling such methods allow for a more directed and focused sampling effort here we provide a framework to identify possible missing interactions in field collected pollination networks based on the distribution of the sampled interactions using known metrics from information theory entropy mutual information and variation of information to describe the network and algorithms based on the network structure between specialists and generalists we can predict which missing interactions are more likely to occur in nature we decompose the distribution of interactions and quantify 1 the relative importance of the different species in the network 2 the specificity of the interactions between bee and plant species and 3 the redundancy or generality of the interactions we validate the information theoretic metrics with four established null models for networks delmas et al 2018 we then exploit the non uniformity of the interactions using the linear filtering method proposed in stock et al 2017 we validate this method as a recommender system to suggest potential missing interactions we do this using both fair cross validation on our pollination network and external validation on the complete web of life database 2 2 http www web of life es this work can be seen as a case study showing how these previously disconnected ideas can work together 2 materials and methods 2 1 the florabeilles pollination network for this study we used the florabeilles dataset 3 3 http www florabeiles org this database has freely accessible data on plant bee interactions that have been reported in the scientific literature over mainland france it records all observed interactions aggregated over time and space the florabeilles dataset hence records which bee plant interactions are biologically plausible while new data are continuously being added at present it holds records for six bee families i e andrenidae 44 species apidae 68 species colletidae 20 species halictidae 57 species megachilidae 114 species and melittidae 2 species for each interaction reported in published work research paper or book information about the interaction is extracted including bee and plant species names forage nectar or pollen gathering number of foraging bees and whether pollinating activity is confirmed this set was then completed by recording the date country town or gps location and habitat type for each interaction for this study we used the binary incidence matrix y where the m rows represent the different bee species and the n columns the plant species we set y i j 1 if bee species i visits plant species j and y i j 0 otherwise this incidence matrix is shown in fig 1 it contains 305 bee species and 452 plant species the network is quite sparse the connectance density the ratio between the number of observed interactions and the number of potential interactions ρ equals 1515 137 860 1 10 the histograms of the number of interactions per bee and plant species are also shown in fig 1 as apparent from the incidence matrix the network is quite nested with a network nestedness bastolla et al 2009a of 0 1015 2 2 information theory for interaction networks ecological interactions between two species at two trophic levels in casu plants and bees are not purely deterministic though interactions are often facilitated by a morphological of physiological complementarity of the two partners the observed interaction network is also a function of the species abundances since many bee species pollinate several plant species and various plant species are pollinated by multiple bee species there is inherent freedom of choice associated with a pollination network for these reasons several authors e g kallimanis et al 2009 poisot et al 2016a have argued that the language of probability theory can adequately describe interaction networks as such we compute the m n probability matrix p as 1 p i j y i j k 1 m l 1 n y k l it is assumed that all interactions are independent and can be represented as a series of bernoulli trials as put forward in the framework of poisot et al 2016a the number pij can be interpreted as the probability that bee species i visits plant species j hence the matrix p embodies a discrete bivariate probability distribution that describes how the two trophic levels interact importantly we assume that the probability matrix p is fixed the following framework relates to how this matrix is structured not to how accurately it is sampled the marginals of this distribution represent the relative abundances of the bee species and the plant species these marginals are 2 p i b j 1 n p i j and 3 p j f i 1 m p i j where p i b is the probability that a visit is made by bee species i and p j f the probability that plant species j is visited regardless of its ecological partner note that we have implicitly introduced two random variables b and f for the bee and plant species respectively 4 4 we use the capital letter f for flower to refer to the plants given that the symbol p is already used to denote the normalized incidence matrix given such a probabilistic interpretation some metrics borrowed from information theory can characterize the network information theory see mackay 2003 for an accessible introduction is a branch of probability theory it provides a quantitative framework for determining how efficiently information can be compressed or communicated the use of information theory to study stability and efficiency in ecological networks was first proposed by macarthur 1955 we refer to ulanowicz 2001 2011 for a modern synthesis foremost we introduce the concept of entropy which is defined for a discrete random variable x as 4 h x x prob x x log 2 1 prob x x 5 x prob x x log 2 prob x x where the sum ranges over the domain of x by convention 0 log 0 is evaluated as 0 from here on we will drop the subindex in the logarithm log will always denote the logarithm of base two the units of the information theoretic indices are thus expressed in bits as opposed to expressing them in nats when using the natural logarithm the interpretation is that the uncertainty associated with an index can be cleared by asking a specific number of binary questions for example if the entropy of b is 5 3 bits this means that for a random interaction one has to pose on average 5 3 yes or no questions e g is it a polylectic bee species is it a bee species of the megachilidae family to identify the bee species for the marginals of the bee eq 2 and plant species eq 3 the entropy is computed as 6 h b i 1 m p i b log p i b and 7 h f j 1 n p j f log p j f the joint entropy of the bivariate distribution is computed as 8 h b f i 1 m j 1 n p i j log p i j in the context of pollination the marginal entropies quantify the equality of the bee and plant species larger entropies indicate that these marginals are closer to a uniform distribution hence no specific species is dominating the network the joint entropy can be used to analyze the distribution of the interactions i e whether individual interactions dominate or not we can also define the conditional entropies for the two variables 9 h b f i j p i j log p i j p j f and 10 h f b i j p i j log p i j p i b they express the entropy of bees resp plant species given that the plant resp bee species is known suppose that there exists a one to one correspondence between bee and plant species in that case the entropies of the marginal distributions are maximal because both distributions are uniform however when looking at a given plant species it is only visited by a single bee species hence the conditional entropy is zero there is no freedom of choice anymore the specificity of the interactions is quantified more directly by the mutual information 11 i b f h b h b f 12 h f h f b 13 i 1 m j 1 n log p i j p i b p j f for which i b f i f b and i b f 0 hold the mutual information measures the average reduction in uncertainty about b given that we know f or vice versa to be precise it represents the reduction of the average number of binary questions that have to be posed to identify an interaction given that one knows one of the partners compared to not knowing the partner the mutual information represents the efficiency of the interactions as high mutual information implies that the species are highly specialized towards a single or a few ecological partners we denote by ub and uf uniformly distributed random variables on a universe of size m and n respectively the entropy of these random variables is given by 14 h u b log m 15 h u f log n 16 h u b u f log m n the difference in entropy between the distributions of b and f and uniform distributions on the same underlying universe is defined as 17 d b h u b h b 18 d f h u f h f and 19 d b f h u b u f h b h f these metrics quantify to what extent the marginals deviate from a uniform distribution d b f represents the average reduction of binary questions needed to pose to identify an interaction pair compared to the case when all marginals would be uniform i e all species are equally important for example if d f is close to zero a single plant species dominates the network whereas a more significant value indicates that many plants are essential finally the variation of information sometimes also called the entropy distance is defined as 20 v b f h b f i b f 21 h b f h f b 22 i 1 m j 1 n p i j log p i j p i b log p i j p j f this index shows the residual freedom of choice of the bee species given that the plant species are known and vice versa the variation of information represents the average number of binary questions that one needs to pose to identify one species in an interaction pair given that one knows the other one it can be seen as a measure of network stability rutledge et al 1976 the following balance equation valverde albacete and peláez moreno 2010 relates the difference in entropy of a distribution compared to the entropy of the corresponding distribution with uniform marginals the mutual information and the variation of information 23 h u b u f log m n 24 d b f 2 i b f v b f or decomposed in the separate contributions of the bee species 25 h u b log m d b i b h b f and for the plant species 26 h u f log n d f i f h f b these equations show how the maximal hypothetical information of an ecological network is decomposed in a term quantifying the degree that some species are more important or active than others d a term about the specific interactions between species i and the remaining freedom of interactions v this equality is illustrated for some small example networks in fig 2 as can be seen efficiency or specificity measured by the mutual information and stability or freedom of choice measured by the conditional entropy are antagonistic one comes at the cost of the other if the freedom of choice of the species increases meaning they have a wider variety of interaction partners the stability of the overall network grows but the efficiency of their interactions decreases 2 3 linear filtering method the linear filtering is a method suggested in stock et al 2017 to find missing interactions in an interaction network it operates by computing a score for every interaction i j in the incidence matrix 27 f i j α 1 y i j α 2 1 m k 1 m y k j α 3 1 n l 1 n y i l α 4 1 m n k 1 m l 1 n y k l or using the introduced notation 28 f i j α 1 y i j α 2 p i b α 3 p j f α 4 ρ with parameters α 1 α 2 α 3 α 4 0 1 and α 1 α 2 α 3 α 4 1 these parameters have the following interpretation α 1 determines the importance of an observed or absent interaction for the given bee plant species pair given by yij α 2 determines the relative weight of the promiscuity of the bee species given by p i b α 3 determines the relative weight of the number of pollination partners of the plant species given by p j f α 4 determines the importance of the global connectedness or density of the network given by ρ the linear filtering method is illustrated in fig 3 in addition to being a weighted average of four averages the linear filter can also be obtained as a special case of a more general pairwise learning framework stock et al 2018a pairwise learning methods typically make use of similarity between the species e g how similar bee species are with regard to their traits when this similarity is only based on the identity of the species i e these species are the same or not the pairwise prediction function takes to the form of eq 28 the computed scores can suggest missing interactions in the network as negative interactions with a high score are likely missing interactions after scoring all interactions in the network one has to rank the negative interactions according to their filtered value the top of these interactions is expected to be enriched with missing interactions i e plant bee interactions that might have been missed during field sampling the effectiveness of the linear filtering method was demonstrated in the original publication where validation was performed on pollination networks host parasite networks food webs and prey predator networks of various sizes the effectiveness of the linear filtering method can be assessed through cross validation we perform every step of the cross validation by setting a positive interaction to zero computing the score for this interaction and repeating this for every interaction the score computed for every interaction represents the score of this interaction using the linear filter of eq 28 given that the interaction value would be zero this score does not depend on the original interaction score this approach is called zero one out zoo cross validation stock et al 2018b and is a suitable method for dealing with data where one has only positive observations there is a simple formula to compute the zoo values efficiently stock et al 2017 29 f i j zoo f i j α 1 α 2 m α 3 n α 4 m n y i j the importance of the above formula is that f i j zoo can be shown the be independent of yij its effect is cancelled in the two terms as such eq 29 can be used to assess whether the linear filtering method can detect missing interactions here we will set α 1 α 2 α 3 α 4 0 25 given equal importance to all averages these values were recommended in the original publication based on an exhaustive search using various ecological networks stock et al 2017 the linear filtering for a given interaction then merely yields the average of the interaction value the row average the column average and the total average of the incidence matrix 2 4 evaluation in this work we perform a thorough evaluation of our approach foremost we evaluate the linear filtering method as a typical recommendation system here we quantify how well this method can detect missing negatives in the network and discuss how to select a suitable threshold next we consider null models for ecological networks to show how the information theoretic decomposition changes under several null models finally we cross check the web of life database to assess whether the linear filtering method can detect convincing missing interactions 2 4 1 evaluation of the linear filtering method the linear filtering method is evaluated in a similar way as recommendation engines isinkaye et al 2015 this means that the linear filtering method is used to provide a real valued score to rank interactions from likely negative to likely positive interactions 5 5 note that we explicitly distinguish between missing interactions which occur in reality but are not sampled in the network and false negative interactions which are present in the sampled network but are not predicted as positive by the model interactions that receive higher scores are more likely to be positive interactions whereas lower scores indicate negative interactions a negative interaction that nevertheless receives a high score is a potential missing interaction to predict an interaction we have to provide a threshold t 0 1 for the computed scores scores higher than or equal to t are treated as positive interactions scores lower than t are treated as negative interactions hence when predicting an interaction four situations can arise depending on whether the interaction is positive or negative in the network and whether it was predicted as positive or not for these combinations the standard terminology is outlined in table 1 the number of predictions in each class depends on the threshold t when the threshold is set high the criterion for predicting a positive interaction is more severe fewer false positive predictions will be made at the cost of missing some true positive interactions if the threshold is set low however more interactions will be predicted as positives but these will be enriched with false positives since the value of the optimal threshold depends on the preferences of the user we evaluate the linear filtering method using metrics agnostic to its value to make the dependence on the threshold explicit we will use the notations tp t fp t fn t and tn t to denote the number of interactions in each class using this notation the false positive rate fpr and true positive rate tpr are defined as 30 tpr t tp t tp t fn t 31 fpr t fp t fp t tn t the tpr represents the fraction of true positives in the predicted interactions i e the fraction of interactions suggested by the method that were indeed interacting the fpr represents the fraction of interactions that have been wrongly deemed to occur i e the fractions of interactions suggested by the method that are not interacting in reality these two metrics evaluate the quality of the predicted interactions a slightly different and complementary view can be obtained by considering precision and recall defined as 32 precision t tp t tp t fp t 33 recall t tp t tp t fn t the precision indicates the fraction of positive interactions that have been found to occur i e the fraction of interactions that have been detected by the method recall is merely a different name for the true positive rate since all the metrics have trade offs depending on the threshold it is common practice to plot each of the two metrics for varying values of t in this way no choice of threshold needs to be made beforehand and the user can select a threshold based on an acceptable trade off between precision and recall the curve that plots the tpr as a function of the fpr is often called a roc receiver operator characteristic curve we refer to fawcett 2004 for an in depth discussion on the use of roc curves the area under this curve is called unsurprisingly the area under the roc curve auroc it is a value between zero and one values greater than 0 5 indicate that the method is better than random at selecting positive interactions the precision and recall are set out in a precision recall curve the area under this curve the aupr is also an important metric for diagnostic purposes the literature recommends to additionally use aupr for highly unbalanced datasets where negative observations greatly outnumber positive observations as is commonly the case in ecological data davis and goadrich 2006 sofaer et al 2018 this unbalancedness is the case here since our network is quite sparse with a density of 1 1 we will compute all performance metrics on the matrix f zoo hence our evaluation assesses the linear filtering method s true potential to find missing interactions in the network since every prediction is performed on a simulated negative interaction 2 4 2 null models for the information theoretic indices null models are commonly used in ecology to verify that the computed quantities in casu the information theoretic metrics and the performance of the linear filtering method do not arise by chance dormann et al 2009 for ecological networks one frequently uses topological methods in which random networks are generated according to some null model delmas et al 2018 these null models randomly generate networks with the same number of species and expected number of interactions but with vastly different topologies interestingly these null models correspond to specific parameterizations of the linear filtering method in this interpretation the score fij computed by eq 28 is used as the probability of an interaction between species i and j we will consider four standard null models for ecological networks each embodying a different type of bias in how the network is sampled type i fortuna and bascompte 2006 the interactions are sampled randomly every interaction is independently sampled from a bernoulli distribution with a probability of success equal to the density of the original network this model assumes that the interactions are distributed randomly between the bee and plant species the corresponding null distribution can be obtained using the linear filtering method setting α 1 0 α 2 0 α 3 0 and α 4 1 type ii bascompte et al 2003 the interactions are sampled according to the degree distribution of the bee and plant species this means that under this null model the probability of an interaction between bee species i and plant species j is m n p i b p j f 2 the corresponding null distribution can be obtained using the linear filtering method setting α 1 0 α 2 0 5 α 3 0 5 and α 4 0 type iii bee species the interactions are sampled according to the marginal distribution of the bee species i e the probability of an interaction between bee species i and plant species j is n m p i b under this null model the corresponding null distribution can be obtained using the linear filtering method setting α 1 0 α 2 1 α 3 0 and α 4 0 type iii plant species similarly the interactions are sampled according to the marginal distribution of the plant species i e the probability of an interaction between bee i and plant j is m n p j f under this null model the corresponding null distribution can be obtained using the linear filtering method setting α 1 0 α 2 0 α 3 1 and α 4 0 the four null models are illustrated in fig 4 we compute the information theoretic indices and the auroc using zero one out cross validation for 100 simulations according to the four different null models these are averaged and hence estimate the expected value of the quantities under the respective null distributions 3 results 3 1 information theoretic decomposition table 2 shows the information theoretic decomposition for the complete network and the bee and plant species separately note that the v component for the marginals is used to indicate the conditional entropy i e h b f and h f b respectively firstly we note that the d component of the information theoretic decomposition is quite low for both the bee and plant species and the network in general the number of interactions per bee species p b and the number of interactions per plant species p f are distributed relatively homogeneously over the different species this is not surprising even though certain bees species can interact with many different plant species e g apis mellifera interacts with 164 of the 452 plant species a large number of bee species however interact only with a single plant species likewise some plant species have many pollination partners e g centaurea solstitialis and calendula arvensis are each visited by 35 bee species while many plant species interact only with one or two bee species a deviation from the uniform distribution explains a little more than 10 of the information in the network the mutual information is symmetric since it quantifies how specific the interactions are between bee and plant species the mutual information component is roughly five times as large as the d component and 1 5 times the variation of information v component the specificity of the interactions explains more than half of the information of the network the remaining part of the information in the network is given by the v component the variation of information for the complete network this is 6 bits or 35 of the total information in the network it shows that the species are not completely specific learning the identity of the bee species only partially reduces the uncertainty of the identity of the plant species looking at the individual distributions of the bee and plant species we see that h b f 2 64 is higher than h f b 3 35 thus the plant species act in a more generalist way than the bee species this is also confirmed when looking at the relative fractions in the information decomposition 3 2 performance of the linear filtering method we apply the linear filtering method on the pollination network and compute the zero one out values we assess the performance using the roc and precision recall curves shown in fig 5 the roc curve is consistently above the first bisector dotted line indicating that for every threshold the linear filtering method performs better than random selection in detecting positive interactions the auroc is 0 8393 meaning that when randomly selecting a positive and a negative interaction in the network the linear filtering method gives a higher score to the positive interaction in 83 93 of the cases we see that the first part of the roc curve has a very steep increase at a threshold of about 10 5 about 20 of the positive interactions can be recovered with virtually no false positives lowering the threshold allows us to recover a more substantial fraction of the occurring interactions but a much larger number of false negatives will seep in for example a slightly lower threshold will recover about 70 of the interactions but will result in roughly 20 of mistakes any threshold will always result in errors one way or another diagnostic tools such as the roc and pr curves allow the field ecologist to select a suitable trade off a complementary picture can be obtained using the precision recall curve a strict cutoff of the score will have a nearly maximal precision all selected interactions are true positives but only a tiny fraction of the positive interactions are found a second local maximum is found at t 7 10 6 this recovers about 15 of the interactions in a top that has a bit less than 40 of the interactions correctly predicted as positive the aupr is 0 1179 which is substantially larger than the connectance density of 0 011 which would be the performance obtained by randomly scoring interactions 3 3 validation using null models table 3 shows the average of the indices and auroc computed on 100 simulations using the four null models the null model i distributes the interactions randomly it destroys all of the network structure as the auroc demonstrates which drops to about 50 it is no longer possible to detect missing interactions using the linear filtering method in the information theoretic decomposition this is reflected in the decrease of the d component this means that the marginals are now approximately uniformly distributed we also observe an increase in the i component this is because interactions are distributed randomly and species appear much more specialized under this null model the indices cannot distinguish between interactions that are specific in the actual network and specificity is caused by change due to reshuffling in the two instantiations of null model iii either the marginals of the bee or the plant species are used to sample a new network both seem to perform quite similarly suggesting the equal importance of both bee and plant species this structure can be explored by the linear filtering method though with a deteriorated performance compared to the original network here the average information decomposition matches the decomposition of the original network closely null model ii samples the interactions concerning both marginals it seems to result in worse performance for the linear filtering method than model iii as noted in the methods section this is likely because this null model does not adequately reproduce the original degree distributions of the original network 3 4 validation using web of life we also compared our results with the pollination networks from the web of life database for increasing thresholds of t 0 01 t 0 05 and t 0 1 we could recover all positive interactions in the florabeilles dataset we counted the number of false positives i e interactions predicted by our model not recovered in the florabeilles dataset based on the florabeilles dataset as these interactions might be missing we screened all plant pollinator interactions in web of life and recorded the number of confirmed interactions in table 4 several of the missing interactions detected by the linear filtering method could be found in the literature for example the four interactions confirmed using threshold t 0 1 contain apis mellifera interacting with the plants calendula arvensis petanidou 1991 scabiosa atropurpurea petanidou 1991 epilobium hirsutum bundgaard 2003 and rubus ulmifolius herrera 1988 4 discussion pollination networks and ecological networks in general are not random but are often highly structured bascompte and jordano 2013 holt and bonsall 2017 vázquez et al 2009 in the last decades research has found them to be nested bascompte et al 2003 bastolla et al 2009b staniczenko et al 2013 modular olesen et al 2007 and low dimensional eklöf et al 2013 ecologists have proposed numerous metrics to quantify this structure delmas et al 2018 information theory is a particularly attractive lens to look at species interaction networks since it provides a mathematically sound way to quantify how distributions the trophic levels of an interaction network are coupled in this case we can quantify the average promiscuity and specificity in a species interaction network we are not the first to explore the link between the metrics of information theory and ecological networks e g macarthur 1955 ulanowicz 2001 blüthgen et al 2006 dormann et al 2009 and ohlmann et al 2019 however we are the first to explore the information in the network using the relatively obscure balance equation of valverde albacete and peláez moreno 2010 an important distinction has to be made between modeling the network on the one hand and modeling the interactions in the network on the other hand this work only relates to the former the network is considered fixed we do not conjecture any generative process of how the network is realized either as an incomplete field sample or arising from species interacting depending on their abundances and traits as such our information theoretic indices as most metrics used in ecology can only give insight into the distribution of the interactions within the network only the use of null models can yield insight into how assumptions on the network engendering influence these metrics there is a tantalizing relation between the information theoretic decomposition of section 2 2 the ecological null models of section 2 4 2 and the linear filtering method of section 2 3 foremost many ecological structures can be explained by neutral effects canard et al 2012 rosindell et al 2012 i e species interactions only driven by relative abundances neutral effects are captured by the i component which captures the degree to which observed interactions deviate from what would be expected by a neutral assumption in statistical analyses one can use the null models ii and iii to test if observed effects could have arisen solely due to the relative abundance of one or two of the trophic levels similarly the linear filtering method allows for the filtering of any non neutral effects destroying parts of the network s information content for the florabeilles plant pollinator dataset we found that the structure is primarily dominated by specific interactions between species i component and to a lesser extent by the v component quantifying the generalist nature of the species the relatively low value of the d component shows that the marginal distributions are far from uniform some species have many more interacting partners than others our filtering method exploits this to find likely missing interactions by definition the mutual information component is due to specific interactions between species as such we cannot recover this connection using the marginal distributions as exploited by the linear filtering method to predict these interactions one needs external information such as species traits and phylogeny to build a predictive model santamaría and rodríguez gironés 2007 the linear filtering method was able to detect all true positive interactions present in the dataset with thresholds up to 0 1 all potential missing interactions put forward by our approach were validated with the web of life database up to 16 of these missing interactions were confirmed depending on how stringent one sets the threshold applying our method therefore allows for detecting missing interactions within a plant pollinator network as no additional features need to be assigned to either plant or pollinator species the method can be easily applied to documented plant pollinator networks or any other types of networks in pollinator ecology current standard protocols to document plant pollinator interactions typically include transects timed observations or both however both methods have their shortcomings most lead to documentation of a network that is distinct from the real network present due to sampling effects vázquez et al 2009 moreover gibson et al showed that the method used to construct plant pollinator networks i e transects or timed observations significantly affects network metrics gibson et al 2011 while transects are faster in generating vast quantities of data they are more prone to missing rare interactions timed observations on the other hand are less prone to missing rare observations even with the use of timed observations and a considerable amount of observation time there is still a substantial undersampling of the real network present chacoff et al 2012 this undersampling of the network could lead to false or obscured conclusions regarding the network structure since it may change network metrics dorado et al 2011 nielsen and bascompte 2007 classically rarefaction and species accumulation curve analyses are used to estimate the sampling completeness in mutualistic networks chacoff et al 2012 nielsen and bascompte 2007 several other methods have recently been developed to help in the assessment of sampling completeness macgregor et al 2017 and to provide guidance on the minimal sample size needed to estimate network metrics reliably casas et al 2018 however there is still a lack of methods that offer a guided and directed sampling method to enable a full sampling of the real network here we put forward a simple but effective way able to assist in a directed sampling design to complete interaction network datasets we investigated the non uniformity of the marginal and joint distributions of the interactions of a plant pollinator network with the use of information theory removing this structure by shuffling under null models deteriorates the performance of the linear filtering method illustrating the relationship between the structure of the network and the effectiveness of our methods these null models are intrinsically linked with the linear filtering method as they correspond to ignoring parts of the information in the network by setting specific parameters to zero by using all the information that can be extracted by the linear filtering method we were able to predict missing interactions which were validated with external databases a plenitude of methods exists that is capable of recovering missing edges in networks a similar model to infer missing pollination interactions called the interaction distribution was proposed by sorensen et al 2011 this method fits two dirichlet distributions to estimate the probability of a pollinator species visiting plant species on the one hand and the likelihood of a plant species being visited by a particular pollinator species on the other hand a similar recommender systems approach was used by desjardins proulx et al 2017 to find missing species using the popular k nearest neighbor method more general methods to detect missing links in networks are often based on bayesian reasoning guimerà and sales pardo 2009 newman 2018 the linear filtering method can be considered as the simplest possible of such methods as it only exploits the marginal interaction probabilities in a weighted sum in this work we provided a theoretical and empirical justification for using the linear filtering method as a reliable baseline 5 conclusion in this work we have shown how information theory can decompose the information content of a pollination network into the relative contribution of the species the specificity of the interactions and the redundancy or freedom of the interactions to the best of our knowledge we are the first to translate the information theoretical decomposition of valverde albacete and peláez moreno 2010 into an ecological context the difference in the importance of the species and the redundancy in the network can be exploited by the linear filtering method to find missing interactions the performance of this linear filtering method was confirmed by stringent performance evaluation as well as validation with external data availability the analysis of the network was done in ecologicalnetworks jl poisot et al 2019 a package in julia to study species interaction networks in which the information theoretic methods as well as the linear filtering method are implemented the pollination dataset and code will be made available upon acceptance declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests credit authorship contribution statement michiel stock conceptualization formal analysis methodology software supervision validation visualization writing original draft niels piot conceptualization methodology validation writing original draft sarah vanbesien data curation investigation validation bernard vaissière resources clémentine coiffait gombault resources guy smagghe supervision writing review editing bernard de baets supervision writing review editing acknowledgements m s is supported by the research foundation flanders fwo17 pdo 067 g s received support from the research foundation flanders via the eos program clips project 30947854 supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j ecolmodel 2020 109161 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
24793,in this paper we analyze the impact of data gaps in the context of ecotoxicology when parameterizing a species using a widely recognized theory the dynamic energy budget deb theory deb based models are used in many ecological domains and have been recognized as particularly useful in ecotoxicology however available datasets are often insufficient to accurately estimate all model parameters we utilized a data rich parameterized species that is widely used in laboratory tests danio rerio zebrafish we compared two versions old and new of deb parametrization methods when i removing datasets one by one to understand if one dataset was particularly relevant for parameter estimation and ii removing datasets cumulatively to test how many datasets were necessary to achieve a meaningful parametrization using the results of the newer version of the parametrization routine we checked how differences in parameter estimations could affect the modeled length and egg production of zebrafish finally we assessed the relevance of these differences in an ecotoxicological context for this purpose we applied five hypothetical stressors with different physiological modes of action to understand the impact of data gaps on estimating stressor effects on individual fish length and egg production our work shows that the new parametrization method is robust and efficient even when used with the minimum amount of data suggested by deb theory the parameters that are affected the most by data gaps are the maturity threshold parameters at the individual level data gaps affect mostly egg production however the link between data gaps and individual egg production is not always straightforward stressor effects can amplify or decrease differences between data rich and data poor scenarios but usually their effects are consistent within each scenario confirming deb models as a powerful tool in ecological and ecotoxicological studies we suggest careful consideration of the potential effects of data gaps when implementing deb based population models keywords dynamic energy budget model data gaps parametrization ecotoxicology 1 introduction dynamic energy budget deb theory describes the energy flow within an individual organism kooijman 2010 nisbet et al 2000 as with other metabolic theories it aims to capture the quantitative aspects of metabolism but unlike previous energy budget frameworks it is explicitly based on the principles of conservation of energy and matter sousa et al 2010 another key aspect is the independence of the reserve dynamics from food availability the reserve being the compartment in which assimilated energy is stored to subsequently fuel all metabolic needs this feature of deb theory provides the individual with some protection against environmental fluctuations and some control over its own metabolism sousa et al 2010 deb theory is generic with regard to species and species differences are reduced to differences in the set of parameter values sousa et al 2008 deb theory has been successfully tested for a collection of over 2000 species add my pet amp collection https www bio vu nl thb deb deblab add my pet index html which includes representatives of all larger animal phyla all chordate classes and all primate families augustine and kooijman 2019 van der meer et al 2014 the mean relative error mre is used to quantify the difference between the model predictions and the available data for each species the median of this value for the entire amp collection is less than 0 07 showing that deb models generally fit the data very well augustine and kooijman 2019 different models based on the deb framework have been published to answer both theoretical and applied questions including ecological and evolutionary issues aquaculture management ecotoxicology and climate research van der meer et al 2014 in particular deb based models are well suited for studying the effects of toxicants on individual organisms and can be coupled with population models to explore effects at higher levels of biological organization e g accolla et al 2019 galic et al 2018 martin et al 2013 vaugeois et al 2020 they incorporate toxicant effects on the energy flow within an individual organism making efficient use of available data from toxicity tests jager et al 2006 toxicant effects can be integrated into deb models nicknamed debtox see jager 2017 jager and zimmer 2012 by allowing a toxicant to impact five different metabolic pathways called physiological modes of action pmoas through the modification of specific deb parameters álvarez et al 2006 martin et al 2013 alternatively toxicant effects are represented through the addition of sub models that connect to the deb model klanjscek et al 2013 2012 methods based on deb theory have been systematically applied in ecotoxicology and have been included in international risk assessment guidance for their relevance in analyzing ecotoxicity data jager et al 2014 iso 2006 oecd 2006 moreover a recent scientific opinion released by efsa european food safety authority on tktd toxicokinetic toxicodynamic models specifically includes debtox as a model to deal with toxicant effects on growth and reproduction charles et al 2018 a better understanding of the influence of data gaps on debtox models could also improve their predictive power in ecological risk assessment as for any other biological model the calibration of deb models involves defining a set of parameters that allows for a good match between modeled values and observed data a major disadvantage of deb theory is that it is data intensive lika et al 2014 nisbet et al 2012 data availability is often a problem when dealing with complex models the actual performance of individuals depends strongly on environmental factors in particular temperature and food availability therefore having multiple datasets referring to different environmental conditions allows a better deb parametrization and finding a generalizable parameter set that best represents the species lika et al 2014 the types of data used in deb theory include zero variate and uni variate data the former are data points such as length at birth maximum length or age at death the latter are data functions such as body weight as a function of time respiration as a function of length or length as a function of weight lika et al 2014 marques et al 2018 when data are poor estimating all parameters becomes difficult and often data from closely related species or a best guess for needed data are used augustine and kooijman 2019 alternatively pseudo data may be used marques et al 2018 pseudo data are a set of parameter values for a generalized species i e typical values for a wide variety of animals lika et al 2011a they serve as prior knowledge on parameter values when the data do not contain that information marques et al 2018 and they can be used to define a reasonable area of parameter space lika et al 2014 the default weight coefficients for pseudo data are set an order of magnitude smaller than those for actual data to make sure that their contribution is small marques et al 2018 the completeness of data to estimate deb parameters is scored from 0 maximum body length only to 10 all aspects of energetics are fixed by data lika et al 2011a overall the mean level of completeness is 2 5 and the maximum is 6 illustrating the problem of lack of data lika et al 2014 however the process based philosophy of deb can still be useful for scrutinizing the results of experiments even when data are lacking as illustrated by entries for organisms such as dinosaurs or other extinct species and cryptic deep water fishes augustine and kooijman 2019 van der meer et al 2014 finally problems also arise when there are many datasets available but they come from numerous sources that refer to different and not always well described environmental conditions augustine and kooijman 2019 while initially much emphasis was put on the development of a consistent theory recently a lot of effort has been put toward the further development of parameter estimation methods and software marques et al 2018 morais et al 2019 and testing them on a rapidly growing number of species van der meer et al 2014 some work has been done on determining the importance of certain types of data for the parameterization process lika et al 2014 but it is not well known how data gaps may affect deb models the objective of this study was to look at the impacts of data gaps on the deb parameterization process this was done by taking the amp entry of a well described fish species danio rerio with level of completeness equal to 4 6 systematically removing data to introduce data gaps in the parameterization process and analyzing the resulting effects on parameters and individual properties length and egg production additionally we compared parameterization results when using the older version of the parameterization process lika et al 2011a 2011b and the newer version marques et al 2018 finally we assessed the relevance of these differences in an ecotoxicological context for this purpose we applied hypothetical stressors to understand the impact of data gaps on estimating stressor effects on the individual level traits stressor effects were implemented by modifying the five metabolic pathways pmoas targeting feeding somatic and maturity maintenance growth and reproduction jager 2017 martin et al 2013 2 methods in this section we provide a summary model description following the overview and design concepts of the odd overview design concepts and details protocol this protocol was originally designed for describing ibms grimm et al 2010 2006 but is helpful also to describe other types of models to give a comprehensive consistent and transparent overview of the modeling process e g meli et al 2014 deb parameterizations with and without data gaps were performed in matlab the life cycle implementation of danio rerio was performed in c and follows the rules detailed in the trace transparent and comprehensive model evaluation document of the model published by accolla et al 2019 and reported here in si model description 2 1 purpose the purpose of this model is to understand which data gaps mostly affect deb parameterization and the extent to which differences in deb parameter estimations result in different individual length and egg production predictions furthermore we compare the new amp routine marques et al 2018 with the previous approach lika et al 2011a 2011b finally we explore the effects of toxicants represented as physiological modes of action pmoas at the individual level for each parameter combination to reach our goals we chose zebrafish danio rerio as the modeled species at the individual level the metabolic processes are based on deb theory kooijman 2010 and are driven by temperature and resource availability 2 2 entities state variables and scales of the model once the deb parameters of d rerio were obtained through the matlab routine see submodel section we implemented a model in c describing the life cycle of one individual entities in the model were individuals and their environment we did not model males and females separately the individual was characterized by four state variables we did not consider aging defined within deb theory 1 structure expressed as length l cm 2 scaled reserve ue d cm2 which serves as an intermediate storage of energy between feeding and mobilization 3 scaled maturity uh d cm2 which regulates transitions between the three developmental stages embryo juvenile adult 4 cumulative reproduction ur d cm2 expressed as a scaled buffer of energy that is converted into eggs during reproduction events in addition we recorded the number of eggs produced we assumed laboratory conditions constant temperature and food and we modeled year round reproduction hazlerigg et al 2014 spence et al 2007 females produced clutches and spawning happened only if a certain number of eggs could be released literature shows that the size of clutches is highly variable spence and smith 2006 report clutches ranging from 1 to 700 eggs with a mean inter spawning interval of 1 5 days and a highly variable mean clutch size of 169 146 eggs spawned in each 24 h period eaton and farley 1974 reports that clutch size varied between 158 and 195 in our model we assigned a threshold of 155 eggs per clutch which allowed us to match the predicted average number of eggs produced per day as reported on the amp website 168 eggs environment in the life cycle model was defined by two forcing variables the temperature profile c and the amount of resource j both these variables were constant with t 26 0 c and unlimited food the model was not spatially explicit time was represented continuously ordinary differential equations however the model implementation was based on discrete hourly time steps we did not represent diurnal variations night day cycles 2 3 process overview and scheduling individual state variables were updated based on a set of differential equations accolla et al 2019 martin et al 2012 vaugeois et al 2020 reproduction was a discrete event and simulations did not last more than a typical lifespan of 4 years gerhard et al 2002 reports a mean life span of 42 months 2 4 design concepts 2 4 1 basic principles key processes in the model were based on dynamic energy budget theory kooijman 2010 which aims to capture the quantitative aspects of metabolism toxicological dynamics were represented following the debtox approach each generalized stressor affected a precise pmoa represented by a specific deb parameter álvarez et al 2006 martin et al 2013 2 4 2 stochasticity in order to introduce some variability among individuals a scatter term was added to one of the deb primary parameters following the method outlined in kooijman et al 1989 and implemented by martin et al 2012 more information can be found in si model description 2 5 initialization simulations started at the embryo stage with t 26 0 c and unlimited food 2 6 submodels 2 6 1 deb parameterizations 2 6 1 1 baseline model our goal being to reparameterize zebrafish for each new dataset we had to ensure that we were not starting the initial parameterization too close to the equilibrium point i e the parameter values of d rerio already estimated in the amp website for this reason the initial set of parameters to start the deb routine in matlab was obtained from the website https deb bolding bruggeman com this service estimates deb parameters for any species using information from the deb parameterized species on the amp website https www bio vu nl thb deb deblab add my pet and taxonomic relationships from the catalogue of life http www catalogueoflife org the method used to infer trait values is described in bruggeman et al 2009 the website illustrates the expected median values of deb parameters and the spread across an ensemble of up to 10 000 members we selected as a starting point i e the parameter values in the pars init m file the parameters of the genus danio instead of those of the species danio rerio nevertheless we copied from the original matlab file pars init m in the amp website those parameters that had been fixed by the authors who first parameterized danio rerio by imposing zero degrees of freedom f m κx κp κr ta k j p t sg ha we also defined the zoom factor z as equal to lm see also table 1 the zoom factor is a scaling parameter defined as the ratio of the maximum length of the species of interest to the maximum length of the reference species l m l m r e f where l m r e f is equal to 1 cm finally we gave to δm the value of the standard abj model the abj model is a general model used to represent the life cycle of fish which takes into account the metabolic acceleration that occurs in fish early life stages kooijman 2014 this acceleration in growth is called type m and takes place during a short period of time starting at birth and ending before puberty at a transition called metamorphosis in deb theory during this time fish are considered as v1 morphs their shape changes during growth and their assimilation surface is proportional to their volume v 1 before and after acceleration organisms are considered to be isomorph as in the standard deb model that is the organism does not change its shape during growth and the surface is proportional to v 2 3 once this new starting point was obtained we were able to run an estimation considering the whole dataset which represented our all data parameter estimation the same starting point was used to run the estimations with data gaps see next section for this all data scenario as well as for scenarios with data gaps we ran the matlab codes using both the old amp routine lika et al 2011a 2011b see si differences between new and old estimation methods and the new version marques et al 2018 when using the new method we followed a four step routine first we ran an estimation with number of steps equal to 500 starting from the initial parameter values this estimation generated a non convergent set of parameters we subsequently ran two estimations having as a starting point the result of the previous estimations using 500 steps finally we ran an estimation using 5000 steps this procedure always resulted in convergence 2 6 1 2 data gaps the data availability in amp for d rerio includes 23 different datasets of those 11 are univariate data and 12 are zero variate data in general zero variate data are reported with higher frequency than univariate data we created data gap scenarios in two ways one by one and cumulatively in the first case we deleted one of the 23 datasets for each deb parameterization therefore the parameterization was always based on 22 different datasets in the second case the order in which we deleted datasets was dictated by the frequency of the same datasets across all fish species actynopterigi present on amp figure 1 we started removing datasets from the least represented towards the most represented from the left of figure 1 to the right therefore in every deb parameterization the previous dataset s and a new one were deleted data gap scenarios were named with a number and an acronym as shown in figure 1 and tables 3 and 4 in which the acronym represents the piece of data that was removed either one by one or cumulatively in that scenario 2 6 1 3 parameterization starting point in the amp collection there are many species belonging to the same order cypriniformes and family cyprinidae of d rerio for this reason the service we used to estimate our parameterization starting point https deb bolding bruggeman com could easily find a good initial set of parameters however one could also have to parameterize a species that does not have phylogenetically closely related entries in the collection and for which it is harder to find an accurate set of initial parameters based on taxonomic considerations in this case the user will probably have to start from the generic pars init file which can be found in the library debtool debtool is a software package that can be used to apply deb theory in the analysis of eco physiological data https www bio vu nl thb deb deblab to test how much parameterization results could be affected by a starting point further from the solution we also ran a parameterization using the general abj pars init for this test we considered the first 9 sets of data from li to l ww plus lj 13 see si deb parameter estimates starting from the standard abj 2 6 1 4 analysis of deb parameter estimates we first estimated the 9 parameter values that were not fixed i e those with non zero degrees of freedom z v κ p m eg e h b e h j e h p and δm with the whole set of data then we compared this result with every deb parameterization obtained from the data gap scenarios generated from both data deletion methodologies in total 45 data gap scenarios were generated twenty three of them were scenarios in which datasets were removed one by one and 22 in which datasets were removed cumulatively for each data gap scenario we evaluated the change of the new parameter values with respect to the all data results equation 1 by calculating a symmetric squared error sse for each parameter we defined a negative positive sse as an underestimation overestimation of the parameter value in a data gap scenario by comparing it to the all data scenario we also calculated the symmetric mean squared error smse equation 2 within each data gap scenario by computing the average of the absolute value of all 9 parameter sses the smse and sses is a measure of the goodness of fit that takes into account that there is a multiplicative symmetry in the effects of the parameter change instead of an additive symmetry for more details on the features of these measures see marques et al 2018 in the following equations p is the parameter value all data if it refers to the parameterization with the whole dataset gap if it refers to one of the data gap scenarios and sign refers to the or sign of the subtraction positive if pgap is larger than p a l l d a t a negative if vice versa 1 s s e s i g n p g a p p a l l d a t a p a l l d a t a p g a p 2 p a l l d a t a 2 p g a p 2 2 smse 1 9 i 1 9 p a l l d a t a p g a p i 2 p a l l d a t a 2 p g a p i 2 2 6 2 individual level simulations every parameter set was used to model the life cycle of the organism with and without stressors we simulated hypothetical sublethal stressors by affecting five metabolic pathways jager 2017 jager et al 2006 martin et al 2013 jager 2017 recognizes five pmoas i reduction in the assimilation from food which is modeled by reducing ingestion efficiency ii increase in maintenance costs iii increase in the cost of growth iv reduction in egg production represented here as a reduction of conversion efficiency of reproduction buffer into offspring v increase in maturity maintenance costs for every mode of action we modified the corresponding deb parameters see also si model description table 2 shows which parameters were affected by the stress factor s 0 s 1 in our simulations the stress factor was always equal to 0 5 corresponding to a 50 increase or decrease in the relevant parameter value at the individual level we compared the maximum length and the total number of eggs produced per day in individuals simulated with different parameter sets we calculated the mean individual length and number of eggs over the last 2 years of simulations and then we averaged this value over 50 replicates we compared the results with the all data scenario by calculating the relative change 3 x g a p x a l l d a t a x a l l d a t a where x represents the average length or the average number of eggs produced by the organism to compare the effects of hypothetical stressors we calculated the relative change between the individual level traits in the stressed data gap scenarios and the stressed all data scenario 4 x g a p s x a l l d a t a s x a l l d a t a s where x represents the average length or the average number of eggs produced by the organism and s refers to the stressed condition in both cases with and without stressors we defined a negative relative change as an under prediction of the individual level characteristics length or egg number in a data gap scenario relative to the all data scenario conversely a positive relative change relative to the all data scenario was defined as an over prediction we also calculated the relative change due to the stressor within a scenario 5 ξ x s i x s i x i x i where i refers to a particular scenario all data or any of the data gap scenarios this was done to obtain the ratio 6 ξ x s g a p ξ x s a l l d a t a which represents the relative change in length or egg production due to stressor between data gap and all data scenarios within the same scenario if this value was less than one the effect of the stressor was under predicted in the data gap scenario if this value was greater than one the stressor effect was over predicted in the data gap scenario for all these ratios we defined a threshold of 10 such that a change larger than 10 was considered as significant for our purpose 3 results 3 1 analysis of deb parameter estimates both data deletion methods caused a parameter underestimation with respect to the all data scenario in the majority of the scenarios we evaluated negative values of sse figure 2 when datasets were removed one by one the individual sse values were mostly between 0 05 and 0 05 the maximum sse value 0 015 was obtained for e h p the maturity threshold for puberty when the dataset wet weight of embryo as a function of time dataset number 6 see figure 1 was removed the minimum value of 0 2 was obtained for parameter e h j the maturity threshold for metamorphosis for the same dataset we considered sse values between 0 2 and 0 2 as good parameter values are reasonably similar whereas values above 0 5 showed a large difference between parameter values when datasets were removed cumulatively the sse increased to higher values ranging from 0 26 for κ when data up to age of birth 18 were removed up to 1 for e h j when data up to maximum reproduction rate 19 were removed with the exception of v eg and δm sse values changed measurably parameters v eg p m and κ were the ones being stabilized by pseudo data all these parameters assumed the value of the pseudo data when too many datasets were removed showed by the plateau in figure 2 for datasets age at birth 18 maximum reproduction rate 19 maximum wet weight 20 length at puberty 21 and maximum age 22 the sse for p m was close to 1 also when we removed dataset age at puberty 17 parameter z was overestimated when data for age at birth maximum reproduction rate maximum wet weight length at puberty and maximum age were removed i e data gap scenarios 18 to 22 e h j was underestimated in the scenarios in which all metamorphosis data were removed i e data gap scenarios 13 to 22 underestimation of the parameters defining energy at birth and puberty started when we removed data for length at birth i e zero variate data 16 although the sse value for the parameter e h p energy at puberty was surprisingly low for data gap scenario 17 i e age at puberty and higher for data gap scenarios 16 and 18 22 we also calculated the sse with the new estimation method when starting deb parameterization from the standard abj values sse values were between 0 015 z and 0 14 p m see table 1 in si deb parameter estimates from the standard abj results of the differences between new and old parameterization methods are shown in si old versus new parameterization method the new method was more robust and therefore our individual level simulations were based on this method throughout 3 2 individual life history simulations 3 2 1 comparison among data gap scenarios in the following section we compare the individual level properties specifically length and egg production for the individual life history simulations we used only the parameterizations obtained from the most up to date method marques et al 2018 for the purpose of comparison the results for all data scenarios are referred to as baseline our baseline model reproduced the average length 5 332 cm and the daily average number of eggs produced by an individual 168 3 as predicted by the deb model and gave good predictions when compared to observed data see figures in si model predictions when we removed datasets cumulatively figure 3 we did not find significant changes in individual length until we removed dataset age of puberty 17 with the exception of dataset µmol of n consumed as a function of time in developing equation 3 that gave a mean length 10 7 larger than baseline when 17 or more datasets were removed individual length was under predicted by about 40 50 the daily average number of eggs produced was over predicted for almost all scenarios except for those in which the first two datasets were removed when datasets 17 22 were removed the number of eggs was under predicted by 20 100 parameter values for the scenario without data up to the maximum reproduction rate 19 resulted in particularly bad parameters especially those concerning the maturity thresholds the energy at birth e h b was of the order of 10 8 since the number of spawned eggs depends on the inverse of e h b we had very large values of the reproductive output furthermore the whole set of parameters was unrealistic so to properly model the life history and avoid numerical errors such as negative growth we had to modify the time step from 24 to 77 when we removed datasets one by one figure 4 we did not find any significant difference in average length the average daily number of eggs was slightly over predicted from 10 1 to 14 1 when we removed datasets survival over time 2 µmol of n consumed as a function of time in developing eggs 3 wet weight of the embryo as a function of time 6 gonado somatic index 14 wet weight as a function of length 15 length at puberty 21 and maximum length 23 the maximum over prediction 14 1 resulted when we removed dataset 15 wet weight as a function of length when we removed dataset maximum reproduction rate 19 we under predicted egg production by 15 3 3 2 2 comparison among data gap scenarios stressed conditions in this section we present the results of simulations in which we added hypothetical stressors we show the relative change in the individual level traits between the stressed data gap and the stressed all data scenarios as well as the ratio of stressor effects for the pmoa decrease in ingestion efficiency equation 6 results concerning the other pmoas were generally similar or less pronounced they can be found in tables 3 and 4 and are shown by the figures in si stressors and data gaps 3 2 2 1 decrease in ingestion efficiency when we removed data cumulatively and added a stressor on ingestion we found that length was not significantly different between the stressed all data scenario and stressed data gap scenarios 1 to 16 length at birth however when dataset age of puberty 17 was removed average lengths were substantially under predicted figure 5 top left side the top right figure shows the ratio of the relative change in length due to the stressor within the same scenario between all data and data gap scenarios this value was always close to 1 for scenarios 1 to 16 showing that the effect of the stressor was comparable among scenarios however this ratio was smaller than 0 9 for scenarios with data removal up to age of puberty 17 length at puberty 21 and maximum age 22 showing that the apparent impact of the stressor was under predicted relative to the all data case when we removed data up to the ultimate wet weight 20 instead we over predicted the effect of the stressor the average daily egg production was usually larger in the data gap scenarios highlighting an over prediction of reproductive output when data gaps were present compared to the stressed all data scenario exceptions were scenarios µmol of c consumed as a function of time in developing eggs and survival probability of larvae in starving conditions 1 and 2 all data removed up to the wet weight of the embryo as a function of time 6 egg diameter 7 and cumulative number of eggs over time 8 which did not show significant differences when a large number of datasets were removed from the age of puberty up to the maximum age 17 22 the difference with respect to the all data scenario was 99 100 no eggs were produced in the stressed data gap scenarios the ratio of the percentage changes due to stressor with and without data gaps was between 0 9 and 1 1 across all scenarios bottom right graph figure 6 shows the results obtained when we removed datasets one by one percentage differences in length compared to the all data scenario with and without stressor were always less than 10 the ratios of the percentage changes due to stressor with and without data gaps were between 0 97 and 1 07 indicating that stressor effects on length were neither under nor over predicted when datasets were removed the number of produced eggs under stress was significantly over predicted when removing data on survival probability over time under starvation 2 with a maximum value of 23 7 and on length of embryo over time age at metamorphosis and wet weight as a function of length respectively 11 12 and 15 in contrast the number of eggs was under predicted in scenarios 3 µmol of n consumed as a function of time in developing eggs and 19 maximum reproduction rate under predicted by 26 the ratio of the percentage changes due to stressor was between 0 99 and 1 02 table 3 and 4 summarize the results shown in figures 5 and 6 top and bottom left for this pmoa as well as the other four more details about the remaining four pmoas can be found in si table 3 and 4 in si stressor effects and data gaps show the relative change in length and egg production due to the different stressors with respect to the unstressed values within the same scenarios 4 discussion the estimation of deb model parameters based on the adjustment of the predicted values computed with the model to the observed data is not an easy task and many efforts have been made to improve the parameterization method morais et al 2019 data gaps are a major problem in ecological modeling and in many cases available datasets are insufficient to accurately identify all model parameters jager and zimmer 2012 lika et al 2014 our work shows that the deb estimation method is robust even with few datasets however differences in parameter sets can result in unexpected variations when describing the length and particularly the egg production of an individual both in the presence and absence of stressors egg production was about four to six times more impacted than length when cumulatively removing data reaching an over prediction of 59 with respect to the all data scenario with a decrease in ingestion efficiency when removing datasets one by one egg production was the only endpoint affected with the biggest under prediction of 26 with respect to the all data scenario with a decrease in ingestion efficiency 4 1 deb parameter estimates the two data deletion methods we used had a double goal we removed datasets one by one to understand if one dataset was particularly relevant for parameter estimation we also removed datasets cumulatively to determine the minimum number of datasets necessary to have a meaningful parametrization consequently the first methodology is similar to a sensitivity analysis that looks into the impact of one dataset in a data rich parameterization whereas with the second methodology we focus on the differences between data rich and data poor parameterizations our results showed that parameter estimation did not differ considerably from a very data rich estimation as long as we had a minimum number of zero variate data in our species length at birth age at puberty age at birth maximum reproduction rate maximum wet weight length at puberty maximum age maximum length and age or length at metamorphosis these datasets overlap with the list of data reported by lika et al 2014 as the minimum data needed to parameterize an abj model parameters representing maturity thresholds were the most sensitive to data gaps when we removed data one by one maturity at metamorphosis and at puberty e h j and e h p had the biggest sse values even if still reasonably low 0 2 maturity thresholds are not limited by pseudo data since they are size dependent in inter specific comparisons and thus can vary considerably among species therefore they are the most sensitive parameters to changes in datasets interestingly higher sse values resulted when we removed the dataset wet weight of embryo as a function of time 6 which is a rare dataset in the amp collection removing this dataset gave the biggest underestimations of parameters v κ p m e h j and δm and the biggest overestimations of parameters z and e h p this is probably due to the estimation method which adjusts the searching parameter space depending on data availability since the parameter space has a high dimension the same as the number of parameters it is hard to understand how the algorithm adjusts the search of parameter values depending on the available datasets therefore removing or adding a particular dataset can influence the overall parameterization process in unpredictable ways and our analysis points out that not all data types have the same impact sometimes it was possible to anchor changes in maturity levels to a specific data point such as the overestimation by 16 of e h p when removing the dataset age of puberty 17 see the excel table parameter values one by one in si however this was not always the case because these parameters are derived from both maturity and growth data in contrast the cumulative method confirmed how parameter estimates are unrealistic if we exclude information about zero variate data moreover since our model is an abj important differences were seen in the value of e h j when both age and length at metamorphosis were removed datasets 12 and 13 which are the only datasets allowing the estimation of metamorphosis thresholds nevertheless we found some unexpected behaviors removing data up to length at birth 16 for example affected the parameter e h p more than removing data up to length at birth and age of puberty 16 and 17 together a similar oscillating trend can be seen in other removal scenarios in the three maturity threshold parameters e h b e h j e h p as pointed out above energy thresholds are less constrained than most other parameters in the estimation process which could also explain the oscillating trend the more the estimation algorithm is forced to stay in a particular space to respect parameter constraints the less the final value of the parameter will change starting from data gap length at birth 16 the parameterization was no longer representative of the species and it is probably not meaningful to analyze such results in very data poor scenarios meanwhile this behavior suggests that information on particular life history traits such as length at birth and age at puberty could interact in unexpected ways during the parameterization process the large size of the amp collection provides the opportunity for additional exploration on this topic particularly using additional species for example one could remove one or more datasets starting from the minimal amount of data required to test how the overall parameterization would be affected and compare results across species the type of datasets on which the parameterization is based would likely influence the results of this exercise resulting in more or less sensitivity to data gaps moreover some data gaps could have different weights depending on the type of deb model std abj etc utilized in the parameterization for example in our case missing values on metamorphosis namely age and length at metamorphosis datasets 12 and 13 clearly caused an underestimation of e h j within the abj model since there was little information to anchor this parameter the new parameterization method gave much better results than the previously developed method when removing data one by one the old method was more prone to overestimating parameter values with respect to the all data scenario whereas the new method tends to underestimate them exploring the impacts of data gaps on other species with different datasets could help in determining whether this new algorithm consistently underestimates parameter values when utilizing a reduced dataset finally we showed that having the minimum amount of data required by deb theory lika et al 2014 allowed for good sse values stressing the importance of having those essential datasets when parameterizing an abj model 4 2 individual level simulations in the absence of stressors small differences in parameter values can result in important changes in egg production when comparing to the all data scenario usually ultimate length was not affected with the exception of very data poor scenarios in the cumulative removal case egg production was often over predicted even for some data gaps that did not markedly alter parameter values interestingly cumulatively removing datasets did not necessarily result in increasingly poorer parameter estimations removing up to the µmol of n consumed as a function of time in developing eggs dataset 3 had a greater impact on simulation results than removing further datasets from 4 to 8 this could be explained by the fact that scenario 3 had the lowest estimation of κ among all cumulative removals this is a key parameter in deb theory defining the amount of energy directed to maturation and reproduction the lower κ is the higher the fraction of energy allocated to reproduction we can cautiously speculate that in a data rich situation we need to pay particular attention to estimating this parameter well to correctly describe reproduction always keeping in mind that individual level results cannot be explained by κ alone when removing up to the length of the embryo as a function of time up to dataset 11 for example we had a slightly higher over prediction of egg production than in scenario 3 and we were still in a fairly data rich situation however in this case κ had a higher value than in the all data scenario and than in scenario 3 suggesting a reduction in reproduction the reason for this unexpected result in scenario 11 is probably explained by p m which was also overestimated suggesting more energy spent on somatic maintenance and less on reproduction furthermore maturity thresholds were poorly parameterized in this scenario the importance of κ is also shown by our results in the one by one removal strategy removing the dataset maximum reproduction rate 19 caused the greatest overestimation of κ leading to the most important decrease in egg production at the individual level however κ was only 8 5 higher than in the all data scenario suggesting that individual level outputs are particularly sensitive to this parameter removing other datasets caused an over prediction of egg production this usually occurred for those data gaps that underestimated parameters κ p m and e h j an example is given by the removal of the dataset wet weight of the embryo as a function of time 6 see previous section in this scenario κ and p m were about 20 lower than in the all data scenario causing a higher reproductive output however the largest increase in egg production resulted when wet weight as a function of length 15 was removed which hardly affected any individual parameter in summary with both deletion methods we could not reliably forecast the goodness of representation of individual length and egg number based on the sse values and we found differences in egg production larger than 20 even when the requirements of minimum data for abj models were satisfied according to lika et al 2014 reproductive endpoints in the individual level model seem particularly sensitive to parameter κ suggesting that data on maximum reproductive rate are important nevertheless the link between data on reproduction κ and egg production is not always straightforward a poor prediction of egg production can potentially affect population dynamics which may be relevant when considering the effects of stressors perturbing reproduction at the individual level can have very different population level impacts depending on the life history of the species considered and the system in which the species lives for instance different species can show different intensities of population decline for the same reduction of their reproduction at the individual level accolla et al 2019 linke gamenick et al 2000 moreover identical individual level impacts on reproduction within the same species can have different population level effects depending on the ecological features of the system by which the population is regulated vaugeois et al 2020 furthermore failing to correctly predict growth and reproduction rates could be linked to a misinterpretation of ages at birth and puberty which are key events in the life cycle defining transitions between life stages a mis estimation of the time spent in a particular life stage can potentially affect modeling results when considering effects of stressors because predation pressure is life stage dependent similar to perturbing reproduction individual level changes in other life history traits such as growth rate can have various consequences at the population level indeed for various reasons population level effects are rarely proportional to individual level effects forbes et al 2008 and the population level impacts of individual level stressor effects or their mis estimation are best quantified with the use of appropriate population models forbes and calow 2012 4 3 individual level simulations in the presence of stressors 4 3 1 comparison of stressor effects among all data and data gap scenarios since stressor effects were slightly different depending on the data gap we found that changes in the predictions of length or egg production with respect to the all data scenario were sometimes amplified or reduced in the presence of stressors for example the relative difference data gap all data on egg production between the unstressed and the stressed conditions changed across pmoas when removing data cumulatively up to the dry weight of the embryo 9 if we compare to the case without stressor we see that egg production was over predicted by almost double with a decreased ingestion efficiency about 30 less over predicted when increasing somatic or maturity maintenance and not over predicted at all with an increased overhead cost of growth this could potentially have important consequences when assessing the risk of a toxicant because differences due to data gaps sum up differently with stressors affecting different pmoas in our example applying a stress on ingestion would result in an important over prediction of egg production with respect to the all data simulations potentially leading to less conservative management measures similar differences were found when removing data one by one removing dataset 3 for example resulted in an over prediction of egg production without stress and an under prediction with a stressor affecting ingestion efficiency these results at the individual level suggest that different parameter sets could affect population level modeling results given the high fecundity of the species we studied large variability in reproductive outputs could have minimal effects at the population level forbes et al 2010 nevertheless the unpredictable individual level results we found both with and without stressors underline the importance of defining clear bounds in which model predictions could be used for ecological and ecotoxicological purposes the natural next step will be to implement these results in a population model to understand how differences due to data gaps propagate from the parametrization of individuals up to the population level 4 3 2 stressor effects within each scenario when we applied stressor effects results were usually consistent i e stressors affected length and egg production by similar percentages with respect to the unstressed case within each data gap and all data scenario see table 3 and 4 in si stressors and data gaps major differences were found only when we cumulatively removed more than 16 datasets in all pmoas and some minor differences occurred with the two pmoas affecting costs of growth and maturity maintenance see below all five pmoas resulted in higher effects on egg production than on length stressors affecting ingestion and somatic maintenance reduced both length and egg production in all scenarios because they caused a reduction of the energy allocated for growth and reproduction by decreasing ingestion or by increasing the costs to maintain the organism increasing the overhead costs of growth does not affect maximum size but does affect how long it takes to reach maximum size if it takes longer the organism will be smaller for more time and reproduce less stress increasing reproduction costs affects only the number of eggs produced by the organism and this was confirmed by our results length was never affected with the exception of cumulative removal scenarios with more than 16 data gaps finally increased maturity maintenance costs did not have measurable consequences on individual length and egg number nonetheless we found some differences across scenarios when we removed data with both methodologies and applied stressors increasing overhead costs of growth or maturity maintenance costs we found that egg production could be larger or smaller than in the unstressed scenario depending on the particular data gap this also explains why the ratio of the percentage change due to the stressor within the same scenario between all data and data gap scenarios was sometimes very high or very low see also table 1 to 4 in si stressors and data gaps for example egg production increased by 10 in the all data scenario when applying a stress on growth whereas it decreased by 10 4 when all data up to the egg dry weight 9 were removed cumulatively table 3 in si and decreased by almost 8 when only the data on length at birth 16 were removed table 4 in si unfortunately we could not find a common pattern among these data gaps when looking at the parameter values the difference in egg production is probably negligible for a species that produces as many eggs as danio rerio nevertheless it would be interesting to analyze how such differences interact with environmental factors at the population level 5 conclusions to conclude our work shows that the new parametrization method marques et al 2018 is robust and efficient even when used with the minimum amount of data suggested by lika et al 2014 for abj models the parameters that are affected the most by data gaps within the minimum data requirements are the maturity threshold parameters these parameters vary considerably from species to species and do not have pseudo data to stabilize them this leads to changes in egg production predictions and can potentially affect the results of deb based population models nevertheless the link between data gaps and individual egg production is not always straightforward and deserves further study when implementing a deb based population model the consequences of mis estimation of individual level responses such as growth or reproduction should be considered stressor effects can amplify or decrease differences between data rich and data poor scenarios but usually their effects are consistent within each scenario confirming deb models as a powerful tool in ecological and ecotoxicological studies authors statement chiara accolla maxime vaugeois pamela rueda cediel and valery forbes collaborated in the formulation of the idea of this project chiara accolla wrote the main manuscript and implemented the codes maxime vaugeois participated in the code implementation pamela rueda cediel helped with the figures adrian moore and goncalo marques gave important feedback maxime vaugeois pamela rueda cediel adrian moore goncalo marques and valery forbes participated in the writing of the manuscript purvaja marella helped running the parametrizations declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j ecolmodel 2020 109107 appendix supplementary materials image application 1 image application 2 image application 3 image application 4 image application 5 image application 6 image application 7 
24793,in this paper we analyze the impact of data gaps in the context of ecotoxicology when parameterizing a species using a widely recognized theory the dynamic energy budget deb theory deb based models are used in many ecological domains and have been recognized as particularly useful in ecotoxicology however available datasets are often insufficient to accurately estimate all model parameters we utilized a data rich parameterized species that is widely used in laboratory tests danio rerio zebrafish we compared two versions old and new of deb parametrization methods when i removing datasets one by one to understand if one dataset was particularly relevant for parameter estimation and ii removing datasets cumulatively to test how many datasets were necessary to achieve a meaningful parametrization using the results of the newer version of the parametrization routine we checked how differences in parameter estimations could affect the modeled length and egg production of zebrafish finally we assessed the relevance of these differences in an ecotoxicological context for this purpose we applied five hypothetical stressors with different physiological modes of action to understand the impact of data gaps on estimating stressor effects on individual fish length and egg production our work shows that the new parametrization method is robust and efficient even when used with the minimum amount of data suggested by deb theory the parameters that are affected the most by data gaps are the maturity threshold parameters at the individual level data gaps affect mostly egg production however the link between data gaps and individual egg production is not always straightforward stressor effects can amplify or decrease differences between data rich and data poor scenarios but usually their effects are consistent within each scenario confirming deb models as a powerful tool in ecological and ecotoxicological studies we suggest careful consideration of the potential effects of data gaps when implementing deb based population models keywords dynamic energy budget model data gaps parametrization ecotoxicology 1 introduction dynamic energy budget deb theory describes the energy flow within an individual organism kooijman 2010 nisbet et al 2000 as with other metabolic theories it aims to capture the quantitative aspects of metabolism but unlike previous energy budget frameworks it is explicitly based on the principles of conservation of energy and matter sousa et al 2010 another key aspect is the independence of the reserve dynamics from food availability the reserve being the compartment in which assimilated energy is stored to subsequently fuel all metabolic needs this feature of deb theory provides the individual with some protection against environmental fluctuations and some control over its own metabolism sousa et al 2010 deb theory is generic with regard to species and species differences are reduced to differences in the set of parameter values sousa et al 2008 deb theory has been successfully tested for a collection of over 2000 species add my pet amp collection https www bio vu nl thb deb deblab add my pet index html which includes representatives of all larger animal phyla all chordate classes and all primate families augustine and kooijman 2019 van der meer et al 2014 the mean relative error mre is used to quantify the difference between the model predictions and the available data for each species the median of this value for the entire amp collection is less than 0 07 showing that deb models generally fit the data very well augustine and kooijman 2019 different models based on the deb framework have been published to answer both theoretical and applied questions including ecological and evolutionary issues aquaculture management ecotoxicology and climate research van der meer et al 2014 in particular deb based models are well suited for studying the effects of toxicants on individual organisms and can be coupled with population models to explore effects at higher levels of biological organization e g accolla et al 2019 galic et al 2018 martin et al 2013 vaugeois et al 2020 they incorporate toxicant effects on the energy flow within an individual organism making efficient use of available data from toxicity tests jager et al 2006 toxicant effects can be integrated into deb models nicknamed debtox see jager 2017 jager and zimmer 2012 by allowing a toxicant to impact five different metabolic pathways called physiological modes of action pmoas through the modification of specific deb parameters álvarez et al 2006 martin et al 2013 alternatively toxicant effects are represented through the addition of sub models that connect to the deb model klanjscek et al 2013 2012 methods based on deb theory have been systematically applied in ecotoxicology and have been included in international risk assessment guidance for their relevance in analyzing ecotoxicity data jager et al 2014 iso 2006 oecd 2006 moreover a recent scientific opinion released by efsa european food safety authority on tktd toxicokinetic toxicodynamic models specifically includes debtox as a model to deal with toxicant effects on growth and reproduction charles et al 2018 a better understanding of the influence of data gaps on debtox models could also improve their predictive power in ecological risk assessment as for any other biological model the calibration of deb models involves defining a set of parameters that allows for a good match between modeled values and observed data a major disadvantage of deb theory is that it is data intensive lika et al 2014 nisbet et al 2012 data availability is often a problem when dealing with complex models the actual performance of individuals depends strongly on environmental factors in particular temperature and food availability therefore having multiple datasets referring to different environmental conditions allows a better deb parametrization and finding a generalizable parameter set that best represents the species lika et al 2014 the types of data used in deb theory include zero variate and uni variate data the former are data points such as length at birth maximum length or age at death the latter are data functions such as body weight as a function of time respiration as a function of length or length as a function of weight lika et al 2014 marques et al 2018 when data are poor estimating all parameters becomes difficult and often data from closely related species or a best guess for needed data are used augustine and kooijman 2019 alternatively pseudo data may be used marques et al 2018 pseudo data are a set of parameter values for a generalized species i e typical values for a wide variety of animals lika et al 2011a they serve as prior knowledge on parameter values when the data do not contain that information marques et al 2018 and they can be used to define a reasonable area of parameter space lika et al 2014 the default weight coefficients for pseudo data are set an order of magnitude smaller than those for actual data to make sure that their contribution is small marques et al 2018 the completeness of data to estimate deb parameters is scored from 0 maximum body length only to 10 all aspects of energetics are fixed by data lika et al 2011a overall the mean level of completeness is 2 5 and the maximum is 6 illustrating the problem of lack of data lika et al 2014 however the process based philosophy of deb can still be useful for scrutinizing the results of experiments even when data are lacking as illustrated by entries for organisms such as dinosaurs or other extinct species and cryptic deep water fishes augustine and kooijman 2019 van der meer et al 2014 finally problems also arise when there are many datasets available but they come from numerous sources that refer to different and not always well described environmental conditions augustine and kooijman 2019 while initially much emphasis was put on the development of a consistent theory recently a lot of effort has been put toward the further development of parameter estimation methods and software marques et al 2018 morais et al 2019 and testing them on a rapidly growing number of species van der meer et al 2014 some work has been done on determining the importance of certain types of data for the parameterization process lika et al 2014 but it is not well known how data gaps may affect deb models the objective of this study was to look at the impacts of data gaps on the deb parameterization process this was done by taking the amp entry of a well described fish species danio rerio with level of completeness equal to 4 6 systematically removing data to introduce data gaps in the parameterization process and analyzing the resulting effects on parameters and individual properties length and egg production additionally we compared parameterization results when using the older version of the parameterization process lika et al 2011a 2011b and the newer version marques et al 2018 finally we assessed the relevance of these differences in an ecotoxicological context for this purpose we applied hypothetical stressors to understand the impact of data gaps on estimating stressor effects on the individual level traits stressor effects were implemented by modifying the five metabolic pathways pmoas targeting feeding somatic and maturity maintenance growth and reproduction jager 2017 martin et al 2013 2 methods in this section we provide a summary model description following the overview and design concepts of the odd overview design concepts and details protocol this protocol was originally designed for describing ibms grimm et al 2010 2006 but is helpful also to describe other types of models to give a comprehensive consistent and transparent overview of the modeling process e g meli et al 2014 deb parameterizations with and without data gaps were performed in matlab the life cycle implementation of danio rerio was performed in c and follows the rules detailed in the trace transparent and comprehensive model evaluation document of the model published by accolla et al 2019 and reported here in si model description 2 1 purpose the purpose of this model is to understand which data gaps mostly affect deb parameterization and the extent to which differences in deb parameter estimations result in different individual length and egg production predictions furthermore we compare the new amp routine marques et al 2018 with the previous approach lika et al 2011a 2011b finally we explore the effects of toxicants represented as physiological modes of action pmoas at the individual level for each parameter combination to reach our goals we chose zebrafish danio rerio as the modeled species at the individual level the metabolic processes are based on deb theory kooijman 2010 and are driven by temperature and resource availability 2 2 entities state variables and scales of the model once the deb parameters of d rerio were obtained through the matlab routine see submodel section we implemented a model in c describing the life cycle of one individual entities in the model were individuals and their environment we did not model males and females separately the individual was characterized by four state variables we did not consider aging defined within deb theory 1 structure expressed as length l cm 2 scaled reserve ue d cm2 which serves as an intermediate storage of energy between feeding and mobilization 3 scaled maturity uh d cm2 which regulates transitions between the three developmental stages embryo juvenile adult 4 cumulative reproduction ur d cm2 expressed as a scaled buffer of energy that is converted into eggs during reproduction events in addition we recorded the number of eggs produced we assumed laboratory conditions constant temperature and food and we modeled year round reproduction hazlerigg et al 2014 spence et al 2007 females produced clutches and spawning happened only if a certain number of eggs could be released literature shows that the size of clutches is highly variable spence and smith 2006 report clutches ranging from 1 to 700 eggs with a mean inter spawning interval of 1 5 days and a highly variable mean clutch size of 169 146 eggs spawned in each 24 h period eaton and farley 1974 reports that clutch size varied between 158 and 195 in our model we assigned a threshold of 155 eggs per clutch which allowed us to match the predicted average number of eggs produced per day as reported on the amp website 168 eggs environment in the life cycle model was defined by two forcing variables the temperature profile c and the amount of resource j both these variables were constant with t 26 0 c and unlimited food the model was not spatially explicit time was represented continuously ordinary differential equations however the model implementation was based on discrete hourly time steps we did not represent diurnal variations night day cycles 2 3 process overview and scheduling individual state variables were updated based on a set of differential equations accolla et al 2019 martin et al 2012 vaugeois et al 2020 reproduction was a discrete event and simulations did not last more than a typical lifespan of 4 years gerhard et al 2002 reports a mean life span of 42 months 2 4 design concepts 2 4 1 basic principles key processes in the model were based on dynamic energy budget theory kooijman 2010 which aims to capture the quantitative aspects of metabolism toxicological dynamics were represented following the debtox approach each generalized stressor affected a precise pmoa represented by a specific deb parameter álvarez et al 2006 martin et al 2013 2 4 2 stochasticity in order to introduce some variability among individuals a scatter term was added to one of the deb primary parameters following the method outlined in kooijman et al 1989 and implemented by martin et al 2012 more information can be found in si model description 2 5 initialization simulations started at the embryo stage with t 26 0 c and unlimited food 2 6 submodels 2 6 1 deb parameterizations 2 6 1 1 baseline model our goal being to reparameterize zebrafish for each new dataset we had to ensure that we were not starting the initial parameterization too close to the equilibrium point i e the parameter values of d rerio already estimated in the amp website for this reason the initial set of parameters to start the deb routine in matlab was obtained from the website https deb bolding bruggeman com this service estimates deb parameters for any species using information from the deb parameterized species on the amp website https www bio vu nl thb deb deblab add my pet and taxonomic relationships from the catalogue of life http www catalogueoflife org the method used to infer trait values is described in bruggeman et al 2009 the website illustrates the expected median values of deb parameters and the spread across an ensemble of up to 10 000 members we selected as a starting point i e the parameter values in the pars init m file the parameters of the genus danio instead of those of the species danio rerio nevertheless we copied from the original matlab file pars init m in the amp website those parameters that had been fixed by the authors who first parameterized danio rerio by imposing zero degrees of freedom f m κx κp κr ta k j p t sg ha we also defined the zoom factor z as equal to lm see also table 1 the zoom factor is a scaling parameter defined as the ratio of the maximum length of the species of interest to the maximum length of the reference species l m l m r e f where l m r e f is equal to 1 cm finally we gave to δm the value of the standard abj model the abj model is a general model used to represent the life cycle of fish which takes into account the metabolic acceleration that occurs in fish early life stages kooijman 2014 this acceleration in growth is called type m and takes place during a short period of time starting at birth and ending before puberty at a transition called metamorphosis in deb theory during this time fish are considered as v1 morphs their shape changes during growth and their assimilation surface is proportional to their volume v 1 before and after acceleration organisms are considered to be isomorph as in the standard deb model that is the organism does not change its shape during growth and the surface is proportional to v 2 3 once this new starting point was obtained we were able to run an estimation considering the whole dataset which represented our all data parameter estimation the same starting point was used to run the estimations with data gaps see next section for this all data scenario as well as for scenarios with data gaps we ran the matlab codes using both the old amp routine lika et al 2011a 2011b see si differences between new and old estimation methods and the new version marques et al 2018 when using the new method we followed a four step routine first we ran an estimation with number of steps equal to 500 starting from the initial parameter values this estimation generated a non convergent set of parameters we subsequently ran two estimations having as a starting point the result of the previous estimations using 500 steps finally we ran an estimation using 5000 steps this procedure always resulted in convergence 2 6 1 2 data gaps the data availability in amp for d rerio includes 23 different datasets of those 11 are univariate data and 12 are zero variate data in general zero variate data are reported with higher frequency than univariate data we created data gap scenarios in two ways one by one and cumulatively in the first case we deleted one of the 23 datasets for each deb parameterization therefore the parameterization was always based on 22 different datasets in the second case the order in which we deleted datasets was dictated by the frequency of the same datasets across all fish species actynopterigi present on amp figure 1 we started removing datasets from the least represented towards the most represented from the left of figure 1 to the right therefore in every deb parameterization the previous dataset s and a new one were deleted data gap scenarios were named with a number and an acronym as shown in figure 1 and tables 3 and 4 in which the acronym represents the piece of data that was removed either one by one or cumulatively in that scenario 2 6 1 3 parameterization starting point in the amp collection there are many species belonging to the same order cypriniformes and family cyprinidae of d rerio for this reason the service we used to estimate our parameterization starting point https deb bolding bruggeman com could easily find a good initial set of parameters however one could also have to parameterize a species that does not have phylogenetically closely related entries in the collection and for which it is harder to find an accurate set of initial parameters based on taxonomic considerations in this case the user will probably have to start from the generic pars init file which can be found in the library debtool debtool is a software package that can be used to apply deb theory in the analysis of eco physiological data https www bio vu nl thb deb deblab to test how much parameterization results could be affected by a starting point further from the solution we also ran a parameterization using the general abj pars init for this test we considered the first 9 sets of data from li to l ww plus lj 13 see si deb parameter estimates starting from the standard abj 2 6 1 4 analysis of deb parameter estimates we first estimated the 9 parameter values that were not fixed i e those with non zero degrees of freedom z v κ p m eg e h b e h j e h p and δm with the whole set of data then we compared this result with every deb parameterization obtained from the data gap scenarios generated from both data deletion methodologies in total 45 data gap scenarios were generated twenty three of them were scenarios in which datasets were removed one by one and 22 in which datasets were removed cumulatively for each data gap scenario we evaluated the change of the new parameter values with respect to the all data results equation 1 by calculating a symmetric squared error sse for each parameter we defined a negative positive sse as an underestimation overestimation of the parameter value in a data gap scenario by comparing it to the all data scenario we also calculated the symmetric mean squared error smse equation 2 within each data gap scenario by computing the average of the absolute value of all 9 parameter sses the smse and sses is a measure of the goodness of fit that takes into account that there is a multiplicative symmetry in the effects of the parameter change instead of an additive symmetry for more details on the features of these measures see marques et al 2018 in the following equations p is the parameter value all data if it refers to the parameterization with the whole dataset gap if it refers to one of the data gap scenarios and sign refers to the or sign of the subtraction positive if pgap is larger than p a l l d a t a negative if vice versa 1 s s e s i g n p g a p p a l l d a t a p a l l d a t a p g a p 2 p a l l d a t a 2 p g a p 2 2 smse 1 9 i 1 9 p a l l d a t a p g a p i 2 p a l l d a t a 2 p g a p i 2 2 6 2 individual level simulations every parameter set was used to model the life cycle of the organism with and without stressors we simulated hypothetical sublethal stressors by affecting five metabolic pathways jager 2017 jager et al 2006 martin et al 2013 jager 2017 recognizes five pmoas i reduction in the assimilation from food which is modeled by reducing ingestion efficiency ii increase in maintenance costs iii increase in the cost of growth iv reduction in egg production represented here as a reduction of conversion efficiency of reproduction buffer into offspring v increase in maturity maintenance costs for every mode of action we modified the corresponding deb parameters see also si model description table 2 shows which parameters were affected by the stress factor s 0 s 1 in our simulations the stress factor was always equal to 0 5 corresponding to a 50 increase or decrease in the relevant parameter value at the individual level we compared the maximum length and the total number of eggs produced per day in individuals simulated with different parameter sets we calculated the mean individual length and number of eggs over the last 2 years of simulations and then we averaged this value over 50 replicates we compared the results with the all data scenario by calculating the relative change 3 x g a p x a l l d a t a x a l l d a t a where x represents the average length or the average number of eggs produced by the organism to compare the effects of hypothetical stressors we calculated the relative change between the individual level traits in the stressed data gap scenarios and the stressed all data scenario 4 x g a p s x a l l d a t a s x a l l d a t a s where x represents the average length or the average number of eggs produced by the organism and s refers to the stressed condition in both cases with and without stressors we defined a negative relative change as an under prediction of the individual level characteristics length or egg number in a data gap scenario relative to the all data scenario conversely a positive relative change relative to the all data scenario was defined as an over prediction we also calculated the relative change due to the stressor within a scenario 5 ξ x s i x s i x i x i where i refers to a particular scenario all data or any of the data gap scenarios this was done to obtain the ratio 6 ξ x s g a p ξ x s a l l d a t a which represents the relative change in length or egg production due to stressor between data gap and all data scenarios within the same scenario if this value was less than one the effect of the stressor was under predicted in the data gap scenario if this value was greater than one the stressor effect was over predicted in the data gap scenario for all these ratios we defined a threshold of 10 such that a change larger than 10 was considered as significant for our purpose 3 results 3 1 analysis of deb parameter estimates both data deletion methods caused a parameter underestimation with respect to the all data scenario in the majority of the scenarios we evaluated negative values of sse figure 2 when datasets were removed one by one the individual sse values were mostly between 0 05 and 0 05 the maximum sse value 0 015 was obtained for e h p the maturity threshold for puberty when the dataset wet weight of embryo as a function of time dataset number 6 see figure 1 was removed the minimum value of 0 2 was obtained for parameter e h j the maturity threshold for metamorphosis for the same dataset we considered sse values between 0 2 and 0 2 as good parameter values are reasonably similar whereas values above 0 5 showed a large difference between parameter values when datasets were removed cumulatively the sse increased to higher values ranging from 0 26 for κ when data up to age of birth 18 were removed up to 1 for e h j when data up to maximum reproduction rate 19 were removed with the exception of v eg and δm sse values changed measurably parameters v eg p m and κ were the ones being stabilized by pseudo data all these parameters assumed the value of the pseudo data when too many datasets were removed showed by the plateau in figure 2 for datasets age at birth 18 maximum reproduction rate 19 maximum wet weight 20 length at puberty 21 and maximum age 22 the sse for p m was close to 1 also when we removed dataset age at puberty 17 parameter z was overestimated when data for age at birth maximum reproduction rate maximum wet weight length at puberty and maximum age were removed i e data gap scenarios 18 to 22 e h j was underestimated in the scenarios in which all metamorphosis data were removed i e data gap scenarios 13 to 22 underestimation of the parameters defining energy at birth and puberty started when we removed data for length at birth i e zero variate data 16 although the sse value for the parameter e h p energy at puberty was surprisingly low for data gap scenario 17 i e age at puberty and higher for data gap scenarios 16 and 18 22 we also calculated the sse with the new estimation method when starting deb parameterization from the standard abj values sse values were between 0 015 z and 0 14 p m see table 1 in si deb parameter estimates from the standard abj results of the differences between new and old parameterization methods are shown in si old versus new parameterization method the new method was more robust and therefore our individual level simulations were based on this method throughout 3 2 individual life history simulations 3 2 1 comparison among data gap scenarios in the following section we compare the individual level properties specifically length and egg production for the individual life history simulations we used only the parameterizations obtained from the most up to date method marques et al 2018 for the purpose of comparison the results for all data scenarios are referred to as baseline our baseline model reproduced the average length 5 332 cm and the daily average number of eggs produced by an individual 168 3 as predicted by the deb model and gave good predictions when compared to observed data see figures in si model predictions when we removed datasets cumulatively figure 3 we did not find significant changes in individual length until we removed dataset age of puberty 17 with the exception of dataset µmol of n consumed as a function of time in developing equation 3 that gave a mean length 10 7 larger than baseline when 17 or more datasets were removed individual length was under predicted by about 40 50 the daily average number of eggs produced was over predicted for almost all scenarios except for those in which the first two datasets were removed when datasets 17 22 were removed the number of eggs was under predicted by 20 100 parameter values for the scenario without data up to the maximum reproduction rate 19 resulted in particularly bad parameters especially those concerning the maturity thresholds the energy at birth e h b was of the order of 10 8 since the number of spawned eggs depends on the inverse of e h b we had very large values of the reproductive output furthermore the whole set of parameters was unrealistic so to properly model the life history and avoid numerical errors such as negative growth we had to modify the time step from 24 to 77 when we removed datasets one by one figure 4 we did not find any significant difference in average length the average daily number of eggs was slightly over predicted from 10 1 to 14 1 when we removed datasets survival over time 2 µmol of n consumed as a function of time in developing eggs 3 wet weight of the embryo as a function of time 6 gonado somatic index 14 wet weight as a function of length 15 length at puberty 21 and maximum length 23 the maximum over prediction 14 1 resulted when we removed dataset 15 wet weight as a function of length when we removed dataset maximum reproduction rate 19 we under predicted egg production by 15 3 3 2 2 comparison among data gap scenarios stressed conditions in this section we present the results of simulations in which we added hypothetical stressors we show the relative change in the individual level traits between the stressed data gap and the stressed all data scenarios as well as the ratio of stressor effects for the pmoa decrease in ingestion efficiency equation 6 results concerning the other pmoas were generally similar or less pronounced they can be found in tables 3 and 4 and are shown by the figures in si stressors and data gaps 3 2 2 1 decrease in ingestion efficiency when we removed data cumulatively and added a stressor on ingestion we found that length was not significantly different between the stressed all data scenario and stressed data gap scenarios 1 to 16 length at birth however when dataset age of puberty 17 was removed average lengths were substantially under predicted figure 5 top left side the top right figure shows the ratio of the relative change in length due to the stressor within the same scenario between all data and data gap scenarios this value was always close to 1 for scenarios 1 to 16 showing that the effect of the stressor was comparable among scenarios however this ratio was smaller than 0 9 for scenarios with data removal up to age of puberty 17 length at puberty 21 and maximum age 22 showing that the apparent impact of the stressor was under predicted relative to the all data case when we removed data up to the ultimate wet weight 20 instead we over predicted the effect of the stressor the average daily egg production was usually larger in the data gap scenarios highlighting an over prediction of reproductive output when data gaps were present compared to the stressed all data scenario exceptions were scenarios µmol of c consumed as a function of time in developing eggs and survival probability of larvae in starving conditions 1 and 2 all data removed up to the wet weight of the embryo as a function of time 6 egg diameter 7 and cumulative number of eggs over time 8 which did not show significant differences when a large number of datasets were removed from the age of puberty up to the maximum age 17 22 the difference with respect to the all data scenario was 99 100 no eggs were produced in the stressed data gap scenarios the ratio of the percentage changes due to stressor with and without data gaps was between 0 9 and 1 1 across all scenarios bottom right graph figure 6 shows the results obtained when we removed datasets one by one percentage differences in length compared to the all data scenario with and without stressor were always less than 10 the ratios of the percentage changes due to stressor with and without data gaps were between 0 97 and 1 07 indicating that stressor effects on length were neither under nor over predicted when datasets were removed the number of produced eggs under stress was significantly over predicted when removing data on survival probability over time under starvation 2 with a maximum value of 23 7 and on length of embryo over time age at metamorphosis and wet weight as a function of length respectively 11 12 and 15 in contrast the number of eggs was under predicted in scenarios 3 µmol of n consumed as a function of time in developing eggs and 19 maximum reproduction rate under predicted by 26 the ratio of the percentage changes due to stressor was between 0 99 and 1 02 table 3 and 4 summarize the results shown in figures 5 and 6 top and bottom left for this pmoa as well as the other four more details about the remaining four pmoas can be found in si table 3 and 4 in si stressor effects and data gaps show the relative change in length and egg production due to the different stressors with respect to the unstressed values within the same scenarios 4 discussion the estimation of deb model parameters based on the adjustment of the predicted values computed with the model to the observed data is not an easy task and many efforts have been made to improve the parameterization method morais et al 2019 data gaps are a major problem in ecological modeling and in many cases available datasets are insufficient to accurately identify all model parameters jager and zimmer 2012 lika et al 2014 our work shows that the deb estimation method is robust even with few datasets however differences in parameter sets can result in unexpected variations when describing the length and particularly the egg production of an individual both in the presence and absence of stressors egg production was about four to six times more impacted than length when cumulatively removing data reaching an over prediction of 59 with respect to the all data scenario with a decrease in ingestion efficiency when removing datasets one by one egg production was the only endpoint affected with the biggest under prediction of 26 with respect to the all data scenario with a decrease in ingestion efficiency 4 1 deb parameter estimates the two data deletion methods we used had a double goal we removed datasets one by one to understand if one dataset was particularly relevant for parameter estimation we also removed datasets cumulatively to determine the minimum number of datasets necessary to have a meaningful parametrization consequently the first methodology is similar to a sensitivity analysis that looks into the impact of one dataset in a data rich parameterization whereas with the second methodology we focus on the differences between data rich and data poor parameterizations our results showed that parameter estimation did not differ considerably from a very data rich estimation as long as we had a minimum number of zero variate data in our species length at birth age at puberty age at birth maximum reproduction rate maximum wet weight length at puberty maximum age maximum length and age or length at metamorphosis these datasets overlap with the list of data reported by lika et al 2014 as the minimum data needed to parameterize an abj model parameters representing maturity thresholds were the most sensitive to data gaps when we removed data one by one maturity at metamorphosis and at puberty e h j and e h p had the biggest sse values even if still reasonably low 0 2 maturity thresholds are not limited by pseudo data since they are size dependent in inter specific comparisons and thus can vary considerably among species therefore they are the most sensitive parameters to changes in datasets interestingly higher sse values resulted when we removed the dataset wet weight of embryo as a function of time 6 which is a rare dataset in the amp collection removing this dataset gave the biggest underestimations of parameters v κ p m e h j and δm and the biggest overestimations of parameters z and e h p this is probably due to the estimation method which adjusts the searching parameter space depending on data availability since the parameter space has a high dimension the same as the number of parameters it is hard to understand how the algorithm adjusts the search of parameter values depending on the available datasets therefore removing or adding a particular dataset can influence the overall parameterization process in unpredictable ways and our analysis points out that not all data types have the same impact sometimes it was possible to anchor changes in maturity levels to a specific data point such as the overestimation by 16 of e h p when removing the dataset age of puberty 17 see the excel table parameter values one by one in si however this was not always the case because these parameters are derived from both maturity and growth data in contrast the cumulative method confirmed how parameter estimates are unrealistic if we exclude information about zero variate data moreover since our model is an abj important differences were seen in the value of e h j when both age and length at metamorphosis were removed datasets 12 and 13 which are the only datasets allowing the estimation of metamorphosis thresholds nevertheless we found some unexpected behaviors removing data up to length at birth 16 for example affected the parameter e h p more than removing data up to length at birth and age of puberty 16 and 17 together a similar oscillating trend can be seen in other removal scenarios in the three maturity threshold parameters e h b e h j e h p as pointed out above energy thresholds are less constrained than most other parameters in the estimation process which could also explain the oscillating trend the more the estimation algorithm is forced to stay in a particular space to respect parameter constraints the less the final value of the parameter will change starting from data gap length at birth 16 the parameterization was no longer representative of the species and it is probably not meaningful to analyze such results in very data poor scenarios meanwhile this behavior suggests that information on particular life history traits such as length at birth and age at puberty could interact in unexpected ways during the parameterization process the large size of the amp collection provides the opportunity for additional exploration on this topic particularly using additional species for example one could remove one or more datasets starting from the minimal amount of data required to test how the overall parameterization would be affected and compare results across species the type of datasets on which the parameterization is based would likely influence the results of this exercise resulting in more or less sensitivity to data gaps moreover some data gaps could have different weights depending on the type of deb model std abj etc utilized in the parameterization for example in our case missing values on metamorphosis namely age and length at metamorphosis datasets 12 and 13 clearly caused an underestimation of e h j within the abj model since there was little information to anchor this parameter the new parameterization method gave much better results than the previously developed method when removing data one by one the old method was more prone to overestimating parameter values with respect to the all data scenario whereas the new method tends to underestimate them exploring the impacts of data gaps on other species with different datasets could help in determining whether this new algorithm consistently underestimates parameter values when utilizing a reduced dataset finally we showed that having the minimum amount of data required by deb theory lika et al 2014 allowed for good sse values stressing the importance of having those essential datasets when parameterizing an abj model 4 2 individual level simulations in the absence of stressors small differences in parameter values can result in important changes in egg production when comparing to the all data scenario usually ultimate length was not affected with the exception of very data poor scenarios in the cumulative removal case egg production was often over predicted even for some data gaps that did not markedly alter parameter values interestingly cumulatively removing datasets did not necessarily result in increasingly poorer parameter estimations removing up to the µmol of n consumed as a function of time in developing eggs dataset 3 had a greater impact on simulation results than removing further datasets from 4 to 8 this could be explained by the fact that scenario 3 had the lowest estimation of κ among all cumulative removals this is a key parameter in deb theory defining the amount of energy directed to maturation and reproduction the lower κ is the higher the fraction of energy allocated to reproduction we can cautiously speculate that in a data rich situation we need to pay particular attention to estimating this parameter well to correctly describe reproduction always keeping in mind that individual level results cannot be explained by κ alone when removing up to the length of the embryo as a function of time up to dataset 11 for example we had a slightly higher over prediction of egg production than in scenario 3 and we were still in a fairly data rich situation however in this case κ had a higher value than in the all data scenario and than in scenario 3 suggesting a reduction in reproduction the reason for this unexpected result in scenario 11 is probably explained by p m which was also overestimated suggesting more energy spent on somatic maintenance and less on reproduction furthermore maturity thresholds were poorly parameterized in this scenario the importance of κ is also shown by our results in the one by one removal strategy removing the dataset maximum reproduction rate 19 caused the greatest overestimation of κ leading to the most important decrease in egg production at the individual level however κ was only 8 5 higher than in the all data scenario suggesting that individual level outputs are particularly sensitive to this parameter removing other datasets caused an over prediction of egg production this usually occurred for those data gaps that underestimated parameters κ p m and e h j an example is given by the removal of the dataset wet weight of the embryo as a function of time 6 see previous section in this scenario κ and p m were about 20 lower than in the all data scenario causing a higher reproductive output however the largest increase in egg production resulted when wet weight as a function of length 15 was removed which hardly affected any individual parameter in summary with both deletion methods we could not reliably forecast the goodness of representation of individual length and egg number based on the sse values and we found differences in egg production larger than 20 even when the requirements of minimum data for abj models were satisfied according to lika et al 2014 reproductive endpoints in the individual level model seem particularly sensitive to parameter κ suggesting that data on maximum reproductive rate are important nevertheless the link between data on reproduction κ and egg production is not always straightforward a poor prediction of egg production can potentially affect population dynamics which may be relevant when considering the effects of stressors perturbing reproduction at the individual level can have very different population level impacts depending on the life history of the species considered and the system in which the species lives for instance different species can show different intensities of population decline for the same reduction of their reproduction at the individual level accolla et al 2019 linke gamenick et al 2000 moreover identical individual level impacts on reproduction within the same species can have different population level effects depending on the ecological features of the system by which the population is regulated vaugeois et al 2020 furthermore failing to correctly predict growth and reproduction rates could be linked to a misinterpretation of ages at birth and puberty which are key events in the life cycle defining transitions between life stages a mis estimation of the time spent in a particular life stage can potentially affect modeling results when considering effects of stressors because predation pressure is life stage dependent similar to perturbing reproduction individual level changes in other life history traits such as growth rate can have various consequences at the population level indeed for various reasons population level effects are rarely proportional to individual level effects forbes et al 2008 and the population level impacts of individual level stressor effects or their mis estimation are best quantified with the use of appropriate population models forbes and calow 2012 4 3 individual level simulations in the presence of stressors 4 3 1 comparison of stressor effects among all data and data gap scenarios since stressor effects were slightly different depending on the data gap we found that changes in the predictions of length or egg production with respect to the all data scenario were sometimes amplified or reduced in the presence of stressors for example the relative difference data gap all data on egg production between the unstressed and the stressed conditions changed across pmoas when removing data cumulatively up to the dry weight of the embryo 9 if we compare to the case without stressor we see that egg production was over predicted by almost double with a decreased ingestion efficiency about 30 less over predicted when increasing somatic or maturity maintenance and not over predicted at all with an increased overhead cost of growth this could potentially have important consequences when assessing the risk of a toxicant because differences due to data gaps sum up differently with stressors affecting different pmoas in our example applying a stress on ingestion would result in an important over prediction of egg production with respect to the all data simulations potentially leading to less conservative management measures similar differences were found when removing data one by one removing dataset 3 for example resulted in an over prediction of egg production without stress and an under prediction with a stressor affecting ingestion efficiency these results at the individual level suggest that different parameter sets could affect population level modeling results given the high fecundity of the species we studied large variability in reproductive outputs could have minimal effects at the population level forbes et al 2010 nevertheless the unpredictable individual level results we found both with and without stressors underline the importance of defining clear bounds in which model predictions could be used for ecological and ecotoxicological purposes the natural next step will be to implement these results in a population model to understand how differences due to data gaps propagate from the parametrization of individuals up to the population level 4 3 2 stressor effects within each scenario when we applied stressor effects results were usually consistent i e stressors affected length and egg production by similar percentages with respect to the unstressed case within each data gap and all data scenario see table 3 and 4 in si stressors and data gaps major differences were found only when we cumulatively removed more than 16 datasets in all pmoas and some minor differences occurred with the two pmoas affecting costs of growth and maturity maintenance see below all five pmoas resulted in higher effects on egg production than on length stressors affecting ingestion and somatic maintenance reduced both length and egg production in all scenarios because they caused a reduction of the energy allocated for growth and reproduction by decreasing ingestion or by increasing the costs to maintain the organism increasing the overhead costs of growth does not affect maximum size but does affect how long it takes to reach maximum size if it takes longer the organism will be smaller for more time and reproduce less stress increasing reproduction costs affects only the number of eggs produced by the organism and this was confirmed by our results length was never affected with the exception of cumulative removal scenarios with more than 16 data gaps finally increased maturity maintenance costs did not have measurable consequences on individual length and egg number nonetheless we found some differences across scenarios when we removed data with both methodologies and applied stressors increasing overhead costs of growth or maturity maintenance costs we found that egg production could be larger or smaller than in the unstressed scenario depending on the particular data gap this also explains why the ratio of the percentage change due to the stressor within the same scenario between all data and data gap scenarios was sometimes very high or very low see also table 1 to 4 in si stressors and data gaps for example egg production increased by 10 in the all data scenario when applying a stress on growth whereas it decreased by 10 4 when all data up to the egg dry weight 9 were removed cumulatively table 3 in si and decreased by almost 8 when only the data on length at birth 16 were removed table 4 in si unfortunately we could not find a common pattern among these data gaps when looking at the parameter values the difference in egg production is probably negligible for a species that produces as many eggs as danio rerio nevertheless it would be interesting to analyze how such differences interact with environmental factors at the population level 5 conclusions to conclude our work shows that the new parametrization method marques et al 2018 is robust and efficient even when used with the minimum amount of data suggested by lika et al 2014 for abj models the parameters that are affected the most by data gaps within the minimum data requirements are the maturity threshold parameters these parameters vary considerably from species to species and do not have pseudo data to stabilize them this leads to changes in egg production predictions and can potentially affect the results of deb based population models nevertheless the link between data gaps and individual egg production is not always straightforward and deserves further study when implementing a deb based population model the consequences of mis estimation of individual level responses such as growth or reproduction should be considered stressor effects can amplify or decrease differences between data rich and data poor scenarios but usually their effects are consistent within each scenario confirming deb models as a powerful tool in ecological and ecotoxicological studies authors statement chiara accolla maxime vaugeois pamela rueda cediel and valery forbes collaborated in the formulation of the idea of this project chiara accolla wrote the main manuscript and implemented the codes maxime vaugeois participated in the code implementation pamela rueda cediel helped with the figures adrian moore and goncalo marques gave important feedback maxime vaugeois pamela rueda cediel adrian moore goncalo marques and valery forbes participated in the writing of the manuscript purvaja marella helped running the parametrizations declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j ecolmodel 2020 109107 appendix supplementary materials image application 1 image application 2 image application 3 image application 4 image application 5 image application 6 image application 7 
24794,several management strategies such as wastewater treatment plants wwtps have been increasingly used to mitigate nutrient pollution at watershed scale during the past few years however their cost efficiency in controlling nutrient load is still debatable this study quantified the annual cost of wwtp and constructed wetland based on process based modeling of nutrient dynamics within a typical lowland rural watershed in eastern china the evaluation results revealed that constructed wetland with macrophyte coverage increased by 10 and 15 can result in an identical reduction of nitrogen and phosphorus load by wwtp however a cost analysis revealed that the strategy of wwtp had a higher annual cost 65 538 yuan yr compared with the strategy of constructed wetland with an annual cost of 24 012 yuan yr the case study revealed that wwtps are not necessarily cost effective for controlling nutrient load from china s lowland rural areas this is because wwtp is costly while constructed wetland based on the widely distributed pond ecosystems in lowland areas can save considerable investments we concluded that an adequate cost effectiveness evaluation for management strategies in nutrient removal will benefit our decision making graphical abstract image graphical abstract keywords phosphorus nitrogen wetland wastewater treatment plants 1 introduction excessive nutrient in aquatic ecosystems has been a worldwide challenge for decades due to its consequence of eutrophication and harmful algal blooms ho et al 2019 huisman et al 2018 sinha et al 2019 nutrient dynamics and load into aquatic ecosystems has been highly concerned in water management practice zhu et al 2018 wastewater treatment plants wwtps have been widely encouraged applied for nutrient removal from domestic and industrial wastewater especially in the urban areas with large population atwm and langeveld 2017 both the clean water act by the united states us and the water framework directive by europe eu proposed a concise management for the treatment and discharge of wastewater kallis and butler 2001 kapp 2014 as a positive result of constructed wwtps in us and eu there are several successful cases of decreasing nutrient levels and eutrophication in receiving waters such as lake erie in 1970s bormans et al 2016 watson et al 2016 however nutrient controlling is still far from satisfactory especially in the developing countries with large population such as china by the end of 2013 china had built 3508 wwtps with a total wastewater treatment capacity of 1 48 108 m3 d i e 0 11 m3 d per capita zhang et al 2016 these wwtps implied a large amount of wastewater treatment and considerable reduction of nutrient load into the receiving water however the wastewater treatment capacity 0 11 m3 d per capita is far lower compared with usa and germany with a value of 0 37 and 0 89 m3 d per capita see table s1 for calculation details therefore harmful algal blooms are not mitigated in many chinese lakes such as lake taihu mao et al 2008 qi et al 2018 chaohu duan et al 2017 huang et al 2018b and dianchi li et al 2019 to further control nutrient load china s government attempted to bring the successful strategy of wwtps in urban areas to reduce nutrient load from rural areas during the past few years in the 13th five year 2016 2020 plan of national rural environment improvement in china released by the ministry of environmental protection of the people s republic of china 2016 81 400 villages were mentioned as the prior areas for pollution controlling constructing small scale wwtps was one of the most important strategies with substantial investment however it is widely known that the cost effectiveness of best management practices strongly depends on the location specific characteristics of the land on which they are applied panagopoulos et al 2011 2013 and a cost effectiveness evaluation of these developed wwtps is still lacking in particular there are still considerable knowledge gaps on the use of wwtps on nutrient reductions several debating questions included 1 is it worth our efforts to build these costly wwtps in china s lowland rural areas to reduce nutrients 2 are there any other cost effective strategies to control excessive nutrients as an alternative of small scale wwtps constructed wetland was recognized as another potential strategy for nutrient removal with a wide range of removal rate depending on wetland types ardón et al 2010 hansen et al 2018 land et al 2013 powers et al 2013 inspired by the high potential of wetlands in nutrient removal we proposed the hypothesis that wwtp is not the cost effective strategy for controlling nutrient load in lowland areas and constructed wetland may be an alternative to test the hypothesis this study investigated the cost efficiency of wwtps and its alternative strategy constructed wetland in reducing nitrogen n and phosphorus p load based on a dataset collected from a typical lowland watershed in eastern china two nutrient dynamic models specially developed for lowland areas were used to characterize the impacts of wwtp and constructed wetland on reducing nutrient load from the study area we concluded by discussing the mechanisms of wwtps and wetlands in altering n and p cycling within lowland rural watersheds as well as their implications in water management 2 material and methods 2 1 study area the study area polder jian is a typical lowland rural watershed with an area of 10 6 ha located at the lower reach of yangtze river basin china fig 1 it has several land use types of surface water 9 paddy land 50 1 dry land 21 7 and residential area 19 2 huang et al 2016 an artificial water transport network ditch pond has been constructed by local farmers such rural watershed without industrial pollution is widely distributed in the lowland areas of eastern china such as the watersheds of lakes poyang taihu and chaohu due to intensive agricultural activities these watersheds were considered as major n and p sources for their surrounding lakes and rivers a small scale wwtp with a daily treatment capacity of 30 m3 d is in construction since 2017 aiming to remove pollutants in wastewater from residential area 2 2 data collection to learn n and p dynamics in lowland rural watersheds a five year 2014 2018 dataset was collected based on an intensive monitoring program within a typical lowland rural watershed of polder jian the dataset included land use fertilization hydrological meteorological and water quality data table 1 the land use data were obtained from satellite image interpretation and drone based monitoring the fertilization data were collected from local farmers living in the lowland watershed the hydrological data were collected based on water level loggers hobo u20 and manual recording of pumping time the meteorological data were collected from china s national weather station of liyang near polder jian and a rain gauge hobo rg3 m at polder jian the water quality data were measured based on a water sampling program twice a month at w1 see fig 1 2 3 a modeling framework to evaluate cost effectiveness of management strategies in mitigating nutrient pollution a modeling framework was developed in this study to evaluate the cost efficiency of two management strategies wwtp and constructed wetland in nutrient removal fig 2 in the modeling framework a cost effectiveness index was proposed based on process based modeling using two existing nutrient dynamic models a nitrogen dynamic model ndp a phosphorus dynamic model pdp ndp and pdp aimed to estimate the study watershed s n and p flow as well as their response to wwtp and constructed wetland for cost effectiveness evaluation 2 3 1 nutrient dynamic models the process based models ndp and pdp used in this study were specifically developed to simulate hydrological and nutrient dynamics in lowland artificial watersheds polders huang et al 2018a huang et al 2016 they described the critical processes of the water and nutrient cycling within polder ecosystems as well as water and nutrient exchange between polders and their surrounding rivers different from the existing watershed models e g inca and swat ndp and pdp are well suited to describe the nutrient dynamics in lowland watersheds due to their following advantages 1 several artificial drainage processes such as irrigation flood and culvert drainage were described by ndp and pdp these processes significantly affect the water and nutrient exchange between the polder and its surrounding rivers 2 the water exchange between soil water surface water and groundwater were adequately described in ndp and pdp the water exchange process can significantly affect the n and p biogeochemical cycling within the polder conceptual models of ndp and pdp with their described processes were provided in fig s1 and fig s2 supporting information further details including governing equations parameters and validation results of ndp and pdp can be found in previous publications huang et al 2018a huang et al 2016 ndp and pdp were coupled in a software package that can be downloaded from http www escience cn people elake index html ndp and pdp have been calibrated and evaluated using a dataset obtained from the study area of polder jian huang et al 2018a huang and gao 2017 huang et al 2016 due to the ongoing monitoring program an update dataset table 1 was collected and support further calibration and validation of ndp and pdp with the following three subsequent steps 1 sensitivity analysis using one at a time method 2 parameter optimization using the artificial intelligence technique of genetic algorithms ga and 3 performance evaluation based on model fit metrics sensitivity analysis identified the sensitive parameters from the large parameter set in the model ga supported the global optimization of these sensitive parameters performance evaluation aimed to confirm the model fit of the calibrated model based on an independent dataset further technical details on these three steps can be found in huang et al 2018 the evaluation results based on the biweekly dataset showed that the seasonal dynamics of tn and tp were predicted with an acceptable performance of a coefficient of determination r2 value between 0 24 and 0 49 fig 3 notwithstanding the acceptable performance our models can be further improved to reduce model uncertainties compared with the results reported in the global overview of watershed modeling by wellen et al 2015 in particular our models did not well predict several peak values of tn and tp implying considerable uncertainties that were raised from data model structure and parameters from data perspective fertilization is a large n and p source for the studied area huang et al 2018a and thus required an accurate data of the fertilizer amount and fertilization date we collected these fertilization data from local farmers and did not consider its spatial differences fertilization exactly before a heavy rainfall event can lead to considerable tn and tp loss to surface water from model structure perspective several processes in biogeochemical cycling e g water sediment nutrient transport were not adequately described from model parameter perspective in case of more measured data a further parameter optimization can be implemented based on the global optimization method of genetic algorithm huang et al 2018a the above mentioned uncertainties required our considerable efforts to reduce in future 2 3 2 scenario simulation for nutrient load nutrient exchange between lowland polders and their surrounding rivers was caused by the pathways of irrigation seepage flood and culvert drainage all the nutrient flux of these pathways can be obtained from the simulation results of ndp and pdp therefore nutrient load of the studied polder is the difference between nutrient export through seepage flood and culvert drainage and import through irrigation to evaluate the impacts of wwtp and wetland on n and p load of the study area polder jian the following simulations were carried out using the developed models of ndp and pdp section 2 3 1 these simulations quantify the response of n and p dynamics to population increase wwtp and constructed wetland during a five year period 2014 2018 the annual n and p load from polder jian was calculated for these simulations with their model configurations given in table 2 sim base the benchmark simulation was carried out to compare with other simulations sim wwtp the simulation was compared with sim base to evaluate the impacts of wwtp on n and p load of the lowland rural watershed sim wetland i these simulations were compared with sim base to evaluate the potentials of constructed wetland on reducing n and p load of the lowland rural watershed we assumed that constructed wetland was implemented mainly by increasing macrophytes on surface water areas this measurement is feasible in eastern china s lowland rural areas where ditch pond ecosystems are widely distributed sim population i these simulations were compared with sim base to quantify the impacts of population growth on n and p load of the lowland rural watershed 2 3 3 cost effectiveness evaluation to better represent annual cost of management strategies for nutrient removal a cost effectiveness index ci yuan yr was proposed based on the scenario simulations in section 2 3 2 the index for wwtp and constructed wetland c i w w t p and c i w e t l a n d was estimated by the following equations with their variables and parameters listed in table 3 1 c i w w t p x w w t p p w w t p 1 p n y r n y r 2 c i w e t l a n d m a x c i w e t l a n d n c i w e t l a n d p 3 c i w e t l a n d n s n p w e t l a n d r t a r g e t n 4 c i w e t l a n d p s p p w e t l a n d r t a r g e t p the annual cost for wwtp c i w w t p was estimated by summing up the infrastructure cost xwwtp and running cost pwwtp xwwtp was obtained from the existing wwtps in the study area pwwtp was calculated based on the energy consumed by the wwtp a population growth of i will result in an increase of pwwtp by i based on a survey on wwtps for china s rural areas a wwtp can approximately run for 30 years nyr 30 the annual cost for constructed wetland c i w e t l a n d is calculated based on the maximum cost for controlling n and p load c i w e t l a n d n and c i w e t l a n d p management cost of macrophytes p w e t l a n d included the rent for the surface water area and the cost for harvesting macrophytes it was estimated based on the local labour cost surveyed in 2019 3 results 3 1 nutrient reduction by wwtp and wetland our simulation results showed that both wwtp and constructed wetland changed the n and p fluxes within the polder wwtp resulted in 2 8 reduction of tn load from 37 48 kg ha yr sim base to 36 44 kg ha yr sim wwtp and 4 6 reduction of tp load from 1 199 kg ha yr sim base to 1 143 kg ha yr sim wwtp the simulation results from sim wetland i showed that both tn and tp load reduction responded to macrophyte increase in a nearly linear pattern fig 4 a b because macrophytes were assumed to be harvested in autumn without decomposition to avoid nutrient pollution increasing macrophytes by 10 sim wetland 10 resulted in an identical tn reduction rate 2 8 of wwtp sim wwtp in fig 4a increasing macrophytes by 15 sim wetland 15 resulted in an identical tp reduction rate 4 6 of wwtp sim wwtp in fig 4b the potential of these two strategies in reducing tn and tp load was because they can reduce tp and tn concentration in the surface water areas fig 4c d simulation of tn and tp concentration in the surface water areas from sim wetland 10 fig 4c and sim wetland 15 fig 4d showed a larger change in summer due to n and p uptake by macrophytes 3 2 nutrient sources sinks and fates in the context of wwtp and constructed wetland to learn the sources sinks and fates of n and p within the lowland polder the n and p fluxes for critical components were calculated based on the simulation results from sim base fig 5 a the calculated fluxes showed that fertilization was the main n and p sources with survey values of 450 kg n ha yr and 72 kg p ha yr there was a strong n exchange between farmlands and atmosphere through denitrification 108 2 kg n ha yr volatilization 102 6 kg n ha yr and deposition 29 6 kg n ha yr n and p from residential area contributed a small fraction of n and p sources compared with other n and p sources such as fertilization and deposition the estimated n and p load of the watershed to its surrounding rivers had a rate of 42 2 kg n ha yr and 1 5 kg p ha yr through three pathways of seepage flood and culvert drainage based on the comparison between sim base sim wwtp sim wetland 10 and sim wetland 15 it is clear that both wwtp and wetland resulted in a significant reduction of tn and tp concentrations in surface area fig 4c d this tn and tp change resulted in a slight reduction of n and p load 1 0 1 1 kg n ha yr 0 05 0 06 kg n ha yr fig 5 however both wwtp and constructed wetland did not result in a large change of seepage because both wwtp and wetland did not change n and p cycling in paddy and dry lands where seepage occurred in their groundwater 3 3 nutrient reduction cost in case of population growth under current population conditions the strategy of wwtp with an annual cost of 65 538 yuan yr is much more costly than constructed wetland with an annual cost of 24 012 yuan yr fig 6 for tn controlling in the context of population growth the cost of constructed wetland increased much faster than the cost of wwtp in case that the population increased to 213 both of these two strategies had identical cost of 7 7 104 yuan yr fig 6a for tp controlling in the context of population growth the cost of constructed wetland increased much faster than the cost of wwtp in case that the population increased to 135 both of these two strategies had identical cost of 6 9 104 yuan yr fig 6b the different performance of these two strategies in nutrient controlling revealed that constructed wetland is much more effective in reducing tn than tp 4 discussion 4 1 why constructed wetland was cost effective in nutrient reduction compared with wwtp there is no doubt that wwtp can reduce nutrient load of the lowland rural watershed of polder jian fig 4 because wwtp have been demonstrated to be effective for n and p removal from sewage with high tn and tp lizarralde et al 2019 however the cost analysis in fig 6 revealed that wwtp was not a cost effective strategy due to its large investment during construction period compared with wwtp constructed wetland based on these ponds ecosystems can save considerable investment this is because ditch pond ecosystems in lowland areas were found to be natural wetlands and were important components for c n and p cycling holgerson and raymond 2016 yuan et al 2019 this conclusion is consistent with previous study debating that constructed wetlands have a strong potential for application in developing countries particularly by small rural communities kivaisi 2001 there are several pathways for n and p removal with surface water ponds that can partly explained the cost efficiency of constructed wetland compared with wwtp settling and sedimentation during the rainfall events runoff may be retained in these ditch pond ecosystems rather than drained into their surrounding rivers such water retention can profoundly enhance n and p removal through sedimentation and settling of particulate nutrient particularly under the stable hydrodynamic conditions of the lowland ditch pond ecosystems huang et al 2016 nutrient uptake these ditch pond ecosystems were covered by macrophytes the macrophytes can potentially remove n and p through uptake kim et al 2013 this component has a large contribution in the china s lowland rural watersheds because macrophytes in ditch ecosystems were harvested by local farmers for better flow conveyance such harvest can avoid decomposition pollution in the surface water area banks and frost 2017 denitrification compared with p n has an additional pathway sediment denitrification for its removal from the ditch pond ecosystems kuypers et al 2018 previous studies reported that macrophytes were able to increase denitrification rate 55 on average by providing additional carbon for denitrifiers alldred and baines stephen 2015 roley sarah et al 2018 this can well explain the higher efficiency of constructed wetland in tn removal than tp removal fig 6 4 2 implications for water management 4 2 1 potential of wetlands for nutrient removal in lowland rural areas regardless of the economic cost the ideal design of nutrient removal is the combination of wwtp and constructed wetland wetlands can be constructed to enhance the nutrient removal for the treated wastewater from wwtps hijosa valsero et al 2011 however limited investments were available for nutrient removal requiring us to choose a cost effective strategy rather than an ideal option the case study revealed that wwtps are not necessarily cost effective for nutrient controlling in china s lowland rural areas based on the difference of annual cost between wwtp and constructed wetland fig 6 we estimated that about 3 38 billion yuan yr can be saved by constructed wetland rather that wwtp in china s rural area apart from their value in nutrient removal turning ponds into wetlands can increase ecosystem services such as habitat biodiversity and landscape aesthetics pan and wang 2009 4 2 2 region specific strategies for nutrient removal in water management practices common failure in nutrient removal is the direct use of successful strategies based on previous experiences however the reality is that a successful management strategy in an ecosystem may not work in another ecosystem due to the different environmental conditions panagopoulos et al 2013 from this perspective we argued region specific strategies should be proposed based on the characteristics of the study watershed in this case study ponds are the widely distributed ecosystems in lowland rural areas they have been recognized as natural wetland and can thus be potentially used as a cost effective strategy for nutrient removal by simply increasing macrophytes moreover several typical macrophytes e g alternanthera philoxeroides in fig 1 have been found within the pond ecosystems therefore to save investments simple increase of macrophyte coverage can potentially enhance n and p retention in aquatic ecosystems alldred and baines stephen 2015 roley sarah et al 2018 in case of more available investments several advanced techniques in constructed wetland can also be taken into account 4 2 3 adequate cost effectiveness evaluation on the water management strategies before taking into action the case study demonstrated the value of assessing the economic cost of potential management strategies in nutrient removal therefore we strongly encouraged a prior evaluation for these management strategies before their applications to save resources for controlling nutrient the developed modelling framework section 2 3 as well as the calculation equations well showed the cost evaluation procedures for the water management strategies and are transferable to other watersheds with some changes of parameter values table 3 for the study area among the modelling framework aquatic ecosystem models aems offered a unique opportunity to quantify the n and p flux for each component fig 5 in the study polder and can thus support investigating the impacts of wwtp and wetland on watershed n and p dynamics aems have been increasingly developed for describing nutrient sources sinks and fates within a catchment fu et al 2019 however they are often poorly integrated into the decision making processes frassl et al 2019 this study implemented a simple connection between an existing nutrient model and a new developed cost analysis component such integration provided considerable supports in learning the cause effect relationships between nutrient dynamics and management strategies and evaluating the potential strategies by scenario simulation robson 2014 based on our cost effectiveness evaluation details section 2 3 3 it is clear that the evaluation results are sensitive to several parameters in table 3 for example the parameter of s n macrophyte area needed to remove 1 kg n related to nutrient uptake ability that differs according to macrophyte species and the parameter of p w e t l a n d management cost of macrophytes in the study area differs according to labor wage for the study area therefore to apply the modelling framework in other case studies we argue it is important necessary to justify the parameter values for the study areas 5 conclusions nutrient reduction efficiency of two management strategies wwtp and constructed wetland in a typical lowland rural watershed was evaluated by nutrient dynamic modeling we found that both wwtp and wetland can alter n and p cycling in the watershed s surface water from the perspective of nutrient controlling wwtp was not necessarily the cost effective option for lowland rural watersheds with low population density but can be a cost effective option in case of population growth constructed wetland based on the existing pond ecosystems was a cost effective strategy for nutrient removal in the study watershed although a specific case in this study is still inadequate to confirm the cost effectivity of a strategy the case study highlighted the value of a cost efficiency evaluation for potential strategies from both economic and mechanistic perspectives process based modeling was demonstrated to be useful in tracking n and p sources sinks and fates and can support us to obtain a cost effective strategy for nutrient reduction before taking actions credit authorship contribution statement jiacong huang conceptualization methodology writing original draft qiuwen chen conceptualization supervision jian peng writing review editing junfeng gao conceptualization supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the project was financially supported by youth innovation promotion association cas 2019313 national natural science foundation of china 41971138 china postdoctoral science foundation 2019m651891 and water resources science and technology program of jiangsu china 2018003 the authors would like to thank china meteorological data sharing service system for providing the measured data for model development supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j ecolmodel 2020 109123 appendix supplementary materials image application 1 
24794,several management strategies such as wastewater treatment plants wwtps have been increasingly used to mitigate nutrient pollution at watershed scale during the past few years however their cost efficiency in controlling nutrient load is still debatable this study quantified the annual cost of wwtp and constructed wetland based on process based modeling of nutrient dynamics within a typical lowland rural watershed in eastern china the evaluation results revealed that constructed wetland with macrophyte coverage increased by 10 and 15 can result in an identical reduction of nitrogen and phosphorus load by wwtp however a cost analysis revealed that the strategy of wwtp had a higher annual cost 65 538 yuan yr compared with the strategy of constructed wetland with an annual cost of 24 012 yuan yr the case study revealed that wwtps are not necessarily cost effective for controlling nutrient load from china s lowland rural areas this is because wwtp is costly while constructed wetland based on the widely distributed pond ecosystems in lowland areas can save considerable investments we concluded that an adequate cost effectiveness evaluation for management strategies in nutrient removal will benefit our decision making graphical abstract image graphical abstract keywords phosphorus nitrogen wetland wastewater treatment plants 1 introduction excessive nutrient in aquatic ecosystems has been a worldwide challenge for decades due to its consequence of eutrophication and harmful algal blooms ho et al 2019 huisman et al 2018 sinha et al 2019 nutrient dynamics and load into aquatic ecosystems has been highly concerned in water management practice zhu et al 2018 wastewater treatment plants wwtps have been widely encouraged applied for nutrient removal from domestic and industrial wastewater especially in the urban areas with large population atwm and langeveld 2017 both the clean water act by the united states us and the water framework directive by europe eu proposed a concise management for the treatment and discharge of wastewater kallis and butler 2001 kapp 2014 as a positive result of constructed wwtps in us and eu there are several successful cases of decreasing nutrient levels and eutrophication in receiving waters such as lake erie in 1970s bormans et al 2016 watson et al 2016 however nutrient controlling is still far from satisfactory especially in the developing countries with large population such as china by the end of 2013 china had built 3508 wwtps with a total wastewater treatment capacity of 1 48 108 m3 d i e 0 11 m3 d per capita zhang et al 2016 these wwtps implied a large amount of wastewater treatment and considerable reduction of nutrient load into the receiving water however the wastewater treatment capacity 0 11 m3 d per capita is far lower compared with usa and germany with a value of 0 37 and 0 89 m3 d per capita see table s1 for calculation details therefore harmful algal blooms are not mitigated in many chinese lakes such as lake taihu mao et al 2008 qi et al 2018 chaohu duan et al 2017 huang et al 2018b and dianchi li et al 2019 to further control nutrient load china s government attempted to bring the successful strategy of wwtps in urban areas to reduce nutrient load from rural areas during the past few years in the 13th five year 2016 2020 plan of national rural environment improvement in china released by the ministry of environmental protection of the people s republic of china 2016 81 400 villages were mentioned as the prior areas for pollution controlling constructing small scale wwtps was one of the most important strategies with substantial investment however it is widely known that the cost effectiveness of best management practices strongly depends on the location specific characteristics of the land on which they are applied panagopoulos et al 2011 2013 and a cost effectiveness evaluation of these developed wwtps is still lacking in particular there are still considerable knowledge gaps on the use of wwtps on nutrient reductions several debating questions included 1 is it worth our efforts to build these costly wwtps in china s lowland rural areas to reduce nutrients 2 are there any other cost effective strategies to control excessive nutrients as an alternative of small scale wwtps constructed wetland was recognized as another potential strategy for nutrient removal with a wide range of removal rate depending on wetland types ardón et al 2010 hansen et al 2018 land et al 2013 powers et al 2013 inspired by the high potential of wetlands in nutrient removal we proposed the hypothesis that wwtp is not the cost effective strategy for controlling nutrient load in lowland areas and constructed wetland may be an alternative to test the hypothesis this study investigated the cost efficiency of wwtps and its alternative strategy constructed wetland in reducing nitrogen n and phosphorus p load based on a dataset collected from a typical lowland watershed in eastern china two nutrient dynamic models specially developed for lowland areas were used to characterize the impacts of wwtp and constructed wetland on reducing nutrient load from the study area we concluded by discussing the mechanisms of wwtps and wetlands in altering n and p cycling within lowland rural watersheds as well as their implications in water management 2 material and methods 2 1 study area the study area polder jian is a typical lowland rural watershed with an area of 10 6 ha located at the lower reach of yangtze river basin china fig 1 it has several land use types of surface water 9 paddy land 50 1 dry land 21 7 and residential area 19 2 huang et al 2016 an artificial water transport network ditch pond has been constructed by local farmers such rural watershed without industrial pollution is widely distributed in the lowland areas of eastern china such as the watersheds of lakes poyang taihu and chaohu due to intensive agricultural activities these watersheds were considered as major n and p sources for their surrounding lakes and rivers a small scale wwtp with a daily treatment capacity of 30 m3 d is in construction since 2017 aiming to remove pollutants in wastewater from residential area 2 2 data collection to learn n and p dynamics in lowland rural watersheds a five year 2014 2018 dataset was collected based on an intensive monitoring program within a typical lowland rural watershed of polder jian the dataset included land use fertilization hydrological meteorological and water quality data table 1 the land use data were obtained from satellite image interpretation and drone based monitoring the fertilization data were collected from local farmers living in the lowland watershed the hydrological data were collected based on water level loggers hobo u20 and manual recording of pumping time the meteorological data were collected from china s national weather station of liyang near polder jian and a rain gauge hobo rg3 m at polder jian the water quality data were measured based on a water sampling program twice a month at w1 see fig 1 2 3 a modeling framework to evaluate cost effectiveness of management strategies in mitigating nutrient pollution a modeling framework was developed in this study to evaluate the cost efficiency of two management strategies wwtp and constructed wetland in nutrient removal fig 2 in the modeling framework a cost effectiveness index was proposed based on process based modeling using two existing nutrient dynamic models a nitrogen dynamic model ndp a phosphorus dynamic model pdp ndp and pdp aimed to estimate the study watershed s n and p flow as well as their response to wwtp and constructed wetland for cost effectiveness evaluation 2 3 1 nutrient dynamic models the process based models ndp and pdp used in this study were specifically developed to simulate hydrological and nutrient dynamics in lowland artificial watersheds polders huang et al 2018a huang et al 2016 they described the critical processes of the water and nutrient cycling within polder ecosystems as well as water and nutrient exchange between polders and their surrounding rivers different from the existing watershed models e g inca and swat ndp and pdp are well suited to describe the nutrient dynamics in lowland watersheds due to their following advantages 1 several artificial drainage processes such as irrigation flood and culvert drainage were described by ndp and pdp these processes significantly affect the water and nutrient exchange between the polder and its surrounding rivers 2 the water exchange between soil water surface water and groundwater were adequately described in ndp and pdp the water exchange process can significantly affect the n and p biogeochemical cycling within the polder conceptual models of ndp and pdp with their described processes were provided in fig s1 and fig s2 supporting information further details including governing equations parameters and validation results of ndp and pdp can be found in previous publications huang et al 2018a huang et al 2016 ndp and pdp were coupled in a software package that can be downloaded from http www escience cn people elake index html ndp and pdp have been calibrated and evaluated using a dataset obtained from the study area of polder jian huang et al 2018a huang and gao 2017 huang et al 2016 due to the ongoing monitoring program an update dataset table 1 was collected and support further calibration and validation of ndp and pdp with the following three subsequent steps 1 sensitivity analysis using one at a time method 2 parameter optimization using the artificial intelligence technique of genetic algorithms ga and 3 performance evaluation based on model fit metrics sensitivity analysis identified the sensitive parameters from the large parameter set in the model ga supported the global optimization of these sensitive parameters performance evaluation aimed to confirm the model fit of the calibrated model based on an independent dataset further technical details on these three steps can be found in huang et al 2018 the evaluation results based on the biweekly dataset showed that the seasonal dynamics of tn and tp were predicted with an acceptable performance of a coefficient of determination r2 value between 0 24 and 0 49 fig 3 notwithstanding the acceptable performance our models can be further improved to reduce model uncertainties compared with the results reported in the global overview of watershed modeling by wellen et al 2015 in particular our models did not well predict several peak values of tn and tp implying considerable uncertainties that were raised from data model structure and parameters from data perspective fertilization is a large n and p source for the studied area huang et al 2018a and thus required an accurate data of the fertilizer amount and fertilization date we collected these fertilization data from local farmers and did not consider its spatial differences fertilization exactly before a heavy rainfall event can lead to considerable tn and tp loss to surface water from model structure perspective several processes in biogeochemical cycling e g water sediment nutrient transport were not adequately described from model parameter perspective in case of more measured data a further parameter optimization can be implemented based on the global optimization method of genetic algorithm huang et al 2018a the above mentioned uncertainties required our considerable efforts to reduce in future 2 3 2 scenario simulation for nutrient load nutrient exchange between lowland polders and their surrounding rivers was caused by the pathways of irrigation seepage flood and culvert drainage all the nutrient flux of these pathways can be obtained from the simulation results of ndp and pdp therefore nutrient load of the studied polder is the difference between nutrient export through seepage flood and culvert drainage and import through irrigation to evaluate the impacts of wwtp and wetland on n and p load of the study area polder jian the following simulations were carried out using the developed models of ndp and pdp section 2 3 1 these simulations quantify the response of n and p dynamics to population increase wwtp and constructed wetland during a five year period 2014 2018 the annual n and p load from polder jian was calculated for these simulations with their model configurations given in table 2 sim base the benchmark simulation was carried out to compare with other simulations sim wwtp the simulation was compared with sim base to evaluate the impacts of wwtp on n and p load of the lowland rural watershed sim wetland i these simulations were compared with sim base to evaluate the potentials of constructed wetland on reducing n and p load of the lowland rural watershed we assumed that constructed wetland was implemented mainly by increasing macrophytes on surface water areas this measurement is feasible in eastern china s lowland rural areas where ditch pond ecosystems are widely distributed sim population i these simulations were compared with sim base to quantify the impacts of population growth on n and p load of the lowland rural watershed 2 3 3 cost effectiveness evaluation to better represent annual cost of management strategies for nutrient removal a cost effectiveness index ci yuan yr was proposed based on the scenario simulations in section 2 3 2 the index for wwtp and constructed wetland c i w w t p and c i w e t l a n d was estimated by the following equations with their variables and parameters listed in table 3 1 c i w w t p x w w t p p w w t p 1 p n y r n y r 2 c i w e t l a n d m a x c i w e t l a n d n c i w e t l a n d p 3 c i w e t l a n d n s n p w e t l a n d r t a r g e t n 4 c i w e t l a n d p s p p w e t l a n d r t a r g e t p the annual cost for wwtp c i w w t p was estimated by summing up the infrastructure cost xwwtp and running cost pwwtp xwwtp was obtained from the existing wwtps in the study area pwwtp was calculated based on the energy consumed by the wwtp a population growth of i will result in an increase of pwwtp by i based on a survey on wwtps for china s rural areas a wwtp can approximately run for 30 years nyr 30 the annual cost for constructed wetland c i w e t l a n d is calculated based on the maximum cost for controlling n and p load c i w e t l a n d n and c i w e t l a n d p management cost of macrophytes p w e t l a n d included the rent for the surface water area and the cost for harvesting macrophytes it was estimated based on the local labour cost surveyed in 2019 3 results 3 1 nutrient reduction by wwtp and wetland our simulation results showed that both wwtp and constructed wetland changed the n and p fluxes within the polder wwtp resulted in 2 8 reduction of tn load from 37 48 kg ha yr sim base to 36 44 kg ha yr sim wwtp and 4 6 reduction of tp load from 1 199 kg ha yr sim base to 1 143 kg ha yr sim wwtp the simulation results from sim wetland i showed that both tn and tp load reduction responded to macrophyte increase in a nearly linear pattern fig 4 a b because macrophytes were assumed to be harvested in autumn without decomposition to avoid nutrient pollution increasing macrophytes by 10 sim wetland 10 resulted in an identical tn reduction rate 2 8 of wwtp sim wwtp in fig 4a increasing macrophytes by 15 sim wetland 15 resulted in an identical tp reduction rate 4 6 of wwtp sim wwtp in fig 4b the potential of these two strategies in reducing tn and tp load was because they can reduce tp and tn concentration in the surface water areas fig 4c d simulation of tn and tp concentration in the surface water areas from sim wetland 10 fig 4c and sim wetland 15 fig 4d showed a larger change in summer due to n and p uptake by macrophytes 3 2 nutrient sources sinks and fates in the context of wwtp and constructed wetland to learn the sources sinks and fates of n and p within the lowland polder the n and p fluxes for critical components were calculated based on the simulation results from sim base fig 5 a the calculated fluxes showed that fertilization was the main n and p sources with survey values of 450 kg n ha yr and 72 kg p ha yr there was a strong n exchange between farmlands and atmosphere through denitrification 108 2 kg n ha yr volatilization 102 6 kg n ha yr and deposition 29 6 kg n ha yr n and p from residential area contributed a small fraction of n and p sources compared with other n and p sources such as fertilization and deposition the estimated n and p load of the watershed to its surrounding rivers had a rate of 42 2 kg n ha yr and 1 5 kg p ha yr through three pathways of seepage flood and culvert drainage based on the comparison between sim base sim wwtp sim wetland 10 and sim wetland 15 it is clear that both wwtp and wetland resulted in a significant reduction of tn and tp concentrations in surface area fig 4c d this tn and tp change resulted in a slight reduction of n and p load 1 0 1 1 kg n ha yr 0 05 0 06 kg n ha yr fig 5 however both wwtp and constructed wetland did not result in a large change of seepage because both wwtp and wetland did not change n and p cycling in paddy and dry lands where seepage occurred in their groundwater 3 3 nutrient reduction cost in case of population growth under current population conditions the strategy of wwtp with an annual cost of 65 538 yuan yr is much more costly than constructed wetland with an annual cost of 24 012 yuan yr fig 6 for tn controlling in the context of population growth the cost of constructed wetland increased much faster than the cost of wwtp in case that the population increased to 213 both of these two strategies had identical cost of 7 7 104 yuan yr fig 6a for tp controlling in the context of population growth the cost of constructed wetland increased much faster than the cost of wwtp in case that the population increased to 135 both of these two strategies had identical cost of 6 9 104 yuan yr fig 6b the different performance of these two strategies in nutrient controlling revealed that constructed wetland is much more effective in reducing tn than tp 4 discussion 4 1 why constructed wetland was cost effective in nutrient reduction compared with wwtp there is no doubt that wwtp can reduce nutrient load of the lowland rural watershed of polder jian fig 4 because wwtp have been demonstrated to be effective for n and p removal from sewage with high tn and tp lizarralde et al 2019 however the cost analysis in fig 6 revealed that wwtp was not a cost effective strategy due to its large investment during construction period compared with wwtp constructed wetland based on these ponds ecosystems can save considerable investment this is because ditch pond ecosystems in lowland areas were found to be natural wetlands and were important components for c n and p cycling holgerson and raymond 2016 yuan et al 2019 this conclusion is consistent with previous study debating that constructed wetlands have a strong potential for application in developing countries particularly by small rural communities kivaisi 2001 there are several pathways for n and p removal with surface water ponds that can partly explained the cost efficiency of constructed wetland compared with wwtp settling and sedimentation during the rainfall events runoff may be retained in these ditch pond ecosystems rather than drained into their surrounding rivers such water retention can profoundly enhance n and p removal through sedimentation and settling of particulate nutrient particularly under the stable hydrodynamic conditions of the lowland ditch pond ecosystems huang et al 2016 nutrient uptake these ditch pond ecosystems were covered by macrophytes the macrophytes can potentially remove n and p through uptake kim et al 2013 this component has a large contribution in the china s lowland rural watersheds because macrophytes in ditch ecosystems were harvested by local farmers for better flow conveyance such harvest can avoid decomposition pollution in the surface water area banks and frost 2017 denitrification compared with p n has an additional pathway sediment denitrification for its removal from the ditch pond ecosystems kuypers et al 2018 previous studies reported that macrophytes were able to increase denitrification rate 55 on average by providing additional carbon for denitrifiers alldred and baines stephen 2015 roley sarah et al 2018 this can well explain the higher efficiency of constructed wetland in tn removal than tp removal fig 6 4 2 implications for water management 4 2 1 potential of wetlands for nutrient removal in lowland rural areas regardless of the economic cost the ideal design of nutrient removal is the combination of wwtp and constructed wetland wetlands can be constructed to enhance the nutrient removal for the treated wastewater from wwtps hijosa valsero et al 2011 however limited investments were available for nutrient removal requiring us to choose a cost effective strategy rather than an ideal option the case study revealed that wwtps are not necessarily cost effective for nutrient controlling in china s lowland rural areas based on the difference of annual cost between wwtp and constructed wetland fig 6 we estimated that about 3 38 billion yuan yr can be saved by constructed wetland rather that wwtp in china s rural area apart from their value in nutrient removal turning ponds into wetlands can increase ecosystem services such as habitat biodiversity and landscape aesthetics pan and wang 2009 4 2 2 region specific strategies for nutrient removal in water management practices common failure in nutrient removal is the direct use of successful strategies based on previous experiences however the reality is that a successful management strategy in an ecosystem may not work in another ecosystem due to the different environmental conditions panagopoulos et al 2013 from this perspective we argued region specific strategies should be proposed based on the characteristics of the study watershed in this case study ponds are the widely distributed ecosystems in lowland rural areas they have been recognized as natural wetland and can thus be potentially used as a cost effective strategy for nutrient removal by simply increasing macrophytes moreover several typical macrophytes e g alternanthera philoxeroides in fig 1 have been found within the pond ecosystems therefore to save investments simple increase of macrophyte coverage can potentially enhance n and p retention in aquatic ecosystems alldred and baines stephen 2015 roley sarah et al 2018 in case of more available investments several advanced techniques in constructed wetland can also be taken into account 4 2 3 adequate cost effectiveness evaluation on the water management strategies before taking into action the case study demonstrated the value of assessing the economic cost of potential management strategies in nutrient removal therefore we strongly encouraged a prior evaluation for these management strategies before their applications to save resources for controlling nutrient the developed modelling framework section 2 3 as well as the calculation equations well showed the cost evaluation procedures for the water management strategies and are transferable to other watersheds with some changes of parameter values table 3 for the study area among the modelling framework aquatic ecosystem models aems offered a unique opportunity to quantify the n and p flux for each component fig 5 in the study polder and can thus support investigating the impacts of wwtp and wetland on watershed n and p dynamics aems have been increasingly developed for describing nutrient sources sinks and fates within a catchment fu et al 2019 however they are often poorly integrated into the decision making processes frassl et al 2019 this study implemented a simple connection between an existing nutrient model and a new developed cost analysis component such integration provided considerable supports in learning the cause effect relationships between nutrient dynamics and management strategies and evaluating the potential strategies by scenario simulation robson 2014 based on our cost effectiveness evaluation details section 2 3 3 it is clear that the evaluation results are sensitive to several parameters in table 3 for example the parameter of s n macrophyte area needed to remove 1 kg n related to nutrient uptake ability that differs according to macrophyte species and the parameter of p w e t l a n d management cost of macrophytes in the study area differs according to labor wage for the study area therefore to apply the modelling framework in other case studies we argue it is important necessary to justify the parameter values for the study areas 5 conclusions nutrient reduction efficiency of two management strategies wwtp and constructed wetland in a typical lowland rural watershed was evaluated by nutrient dynamic modeling we found that both wwtp and wetland can alter n and p cycling in the watershed s surface water from the perspective of nutrient controlling wwtp was not necessarily the cost effective option for lowland rural watersheds with low population density but can be a cost effective option in case of population growth constructed wetland based on the existing pond ecosystems was a cost effective strategy for nutrient removal in the study watershed although a specific case in this study is still inadequate to confirm the cost effectivity of a strategy the case study highlighted the value of a cost efficiency evaluation for potential strategies from both economic and mechanistic perspectives process based modeling was demonstrated to be useful in tracking n and p sources sinks and fates and can support us to obtain a cost effective strategy for nutrient reduction before taking actions credit authorship contribution statement jiacong huang conceptualization methodology writing original draft qiuwen chen conceptualization supervision jian peng writing review editing junfeng gao conceptualization supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the project was financially supported by youth innovation promotion association cas 2019313 national natural science foundation of china 41971138 china postdoctoral science foundation 2019m651891 and water resources science and technology program of jiangsu china 2018003 the authors would like to thank china meteorological data sharing service system for providing the measured data for model development supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j ecolmodel 2020 109123 appendix supplementary materials image application 1 
