index,text
26285,we present an open source computational tool for the 2d simulation of the diffusive logistic growth dlg model the r recovery module offers a complete environment for the simulation of forestry regeneration in conservation areas and includes a built in tool for calibration and validation of the model parameters through the use of standard and freely available satellite imagery it was implemented as an add on to the grass software a largely applied open source geographic information system gis to illustrate its application we present a complete case study of forest regeneration carried out in the espigão alto state park easp brazil from which we assess typical values of forest diffusion and growth rate parameters along with the prognostics of forest density status for the coming decades we observe that the r recovery tool can be advantageously applied by forestry managers and policy makers as a form of acquiring technical and scientifically based information for strategy development and decision making keywords forest growth diffusive logistic growth model environmental conservation grass module availability program name r recovery developer l a richit contact address luizaugustorichit gmail com year first available 2018 software required grass gis 7 0 or later program language c package size 144 kb source code availability https github com uffsenvmodelling r recovery cost free of charge 1 introduction the activity of suppressing forested areas for agriculture livestock and urbanization purpose is responsible for the loss of biodiversity environmental quality soil humidity water quality and quantity among others johnson et al 2013 parrotta and knowles 2001 vought et al 1995 yu et al 2011 forest regeneration takes place by means of a recolonization process which is begun by the remnants of the original forest this process can vary according to the characteristics of the vegetation the climatic conditions and the level of damage and degradation of the original forest cole et al 2014 fanta et al 1996 holl 1999 kilian 1998 wenhua 2004 researchers have proposed different methods to estimate the amount of time required for a forest to fully recover after suffering degradation while others have argued that the original environment cannot be achieved again nevertheless there is a consensus regarding the forest recovery time which requires anything from decades to centuries to stabilize should this be depending on the biome characteristics cole et al 2014 forest growth is described in the literature in numerous ways most of which are based on qualitative features such as seed dispersion benitez malvidojulieta 1998 clark et al 1999 higgins and richardson 1999 howe and smallwood 1982 soil characteristics elevation humidity species and successional stages of development havlicek and mitchell 2014 krumins and adams 2014 reyer et al 2015 uhl 1987 on the other hand the application of mathematical models to study forest growth is still at an early stage the models of forest growth were commonly associated with models of population growth in which a reactive term describes autorregulation and the balancing between offer and demand for different populations or resources cantrell and cosner 2004 skellam 1951 verhulst 1845 1847 a more elaborate model was employed by acevedo et al 2012 on the basis of the fisher kolmogorov equation fisher 1937 in which the logistic term was combined with diffusion to give rise to the diffusive logistic growth dlg model the authors successfully applied the dlg model to study forest regeneration for the period from 1951 to 1991 1992 in abandoned deforested areas on the island of puerto rico acevedo et al 2012 beyond its value of providing diagnostics prognostics and an useful management tool the model also permits the assessment of parameters of growth dynamics which can be of importance in the understanding of vegetation growth across different biomes while the logistic term provides the logistic autorregulation of growth the diffusive term describes the intrinsic colonization that takes place in forested areas cui and chen 1998 holmes et al 1994 the current status of forest growth models can be evaluated from the careful review presented by porté bartelink porté and bartelink 2002 which proposed a classification of vegetation growth models such a classification would only consider mathematical models of individual growth and thus would not include the dlg model which was proposed and applied for the first time after the publication by porté bartelink porté and bartelink 2002 nevertheless it makes clear that the work by acevedo et al 2012 proposes a different and somewhat complementary approach which changes the focus from growth models for individual trees to growth models for forests as a whole while growth models for individual trees require detailed costly and time consuming inventories forest growth models usually apply satellite imagery with in loco validation of certain key points thus allowing faster assessment of forest growth prognostics for environmental studies regarding the application of environmental models in general there has been an increasing effort from the modeling community to combine them with gis software in order to deliver readily applicable effective and easily operated tools in particular among the free and open source gis software the grass geographic resources analysis support system initiative has received considerable attention neteler et al 2012 there are numerous instances of modules working under this framework r water fea presented by vieux gauer vieuxbaxter and gauer 1994 and employed to simulate storm water runoff r sun presented by hofierka et al 2002 which estimates global radiation based on the work developed by the european solar radiation atlas esra more recent studies include that presented by tonini et al 2017 who created a module that evaluated the advances of an invasive species of pathogens the open source code r avaflow v1 developped by mergili et al 2017 to predict mass movement such as avalanches of rocks ice and debris the study presented by strigaro et al 2016 regarding the implementation of r glacio model to predict the glacial behavior under climate change with a case study of the rutor glacier in the italian alps the r diversity module present by rocchini et al rocchiniducciodelucchi et al 2013 to estimate the landscape diversity among others in this paper we present the r recovery module for grass gis the module implements the diffusive logistic growth equation and permits the calibration validation and application of the forest growth model for standard satellite imagery supported by gis softwares the module was written in c programming language using the grass libraries and follows the open source ideal of the software after presenting the working and operation of the module we present a case study that considers the process of forest recovery in the espigão alto state park located in the southern part of brazil from this we assess typical values of forest diffusion and growth rate parameters along with prognostics of forest density status for coming decades the paper is organized as follows section 2 presents the materials and methods applied in the implementation of the module section 3 presents the case study and illustrates the application of the module section 4 discusses the main findings and results obtained in this research and section 5 outlines the concluding remarks 2 materials and methods this section briefly presents the dlg model on the basis of refs acevedo et al 2012 richit et al 2017 and the discretization scheme for the equation the details on the implementation of the r recovery module are given and discussed later in the paper 2 1 a model of forest recovery reaction diffusion equations are commonly applied to population dynamics modeling since these two processes combined can handle the description of population movement mortality reproduction and regulation cantrell and cosner 2004 okubo 1980 an early study of the reaction diffusion equation aimed at modeling population dynamics was presented by fisher 1937 where the population growth is described by means of the diffusive logistic equation allen and linda 1983 bassan et al 1997 cantrell and cosner 2004 gurtin et al 1977 in this study we apply the diffusive logistic growth equation as presented by acevedo et al 2012 and further studied in ref richit et al 2017 to consider the problem of population dynamics as applied to forest recovery the model describes the dynamics of forest growth as a combination of both logistic growth which considers self limited population growth and diffusion which considers and quantifies colonization an illustrative scheme of the two processes and their combination is presented in fig 1 in the 2d formulation the diffusive logistic growth model equation can be written as 1 u t d u 2 u x 2 2 u y 2 r u 1 u k u u where u describes the forest density at a given instant t for a given position x y r u is the forest growth rate k u is the carrying capacity from the logistic term d u is the forest diffusion coefficient acevedo et al 2012 cantrell and cosner 2004 holmes et al 1994 skellam 1951 the classical logistic model describes the change of population on the basis of exponential growth with self regulation which plays a role in describing factors that inhibit population growth such as food shortages diseases or overcrowding chapra 2012 holmes et al 1994 this model equation can be written as follows 2 u t r u 1 u k u u the logistic model in eq 2 admits an analytical solution the solution predicts the population growth as a function of time as 3 u t k u u 0 u 0 k u 0 e r u t where u 0 is the initial population at arbitrary time t 0 thus analyzing the limit of eq 3 when t the population size tends to the carrying capacity k u despite u 0 k u or u 0 k u cantrell and cosner 2004 formally independent of the initial population size we have 4 l i m u t t k u to complete the diffusive logistic growth model with the inclusion of the diffusive term consider the dispersion of population as a brownian motion the flux of species u is represented by first fick s law as j d u u cengel and ghajar 2011 welty et al 2009 white 1999 analogously to the standard notion of heat flux or mass transfer fick s law is an empirical consideration regarding the transport of particles concentrations or individuals from a specified point in which the flux is taken as being proportional to the negative of the concentration gradient cantrell and cosner 2004 so applying the continuity equation of a quantity u fick s law as used to describe diffusion can be written as 5 u t j q where q q x y t is a source or sink term presuming that the density variable u follows a combination of diffusion and logistic growth we obtain the dlg model eq 1 in summary as written in the form of eq 1 the model asserts that population density u can be described as the combination of the incorporation of population by means of diffusion from the points of higher concentrations to those of lower concentrations and is restricted by the self regulation of the logistic term acevedo et al 2012 skellam 1951 verhulst 1845 1847 2 2 numerical scheme for dlg model following the procedures presented in acevedo et al 2012 we apply the crank nicolson scheme to discretize eq 1 considering this scheme the time derivative term is substituted by the first order forward difference approximation as 6 u t u x y t δ t u x y t δ t where δ t is the time increment between consecutive time steps finally the spatial derivatives at time t are approximated with the second order differences as 7 2 u x 2 u x δ x y t 2 u x y t u x δ x y t δ x 2 8 2 u y 2 u x y δ y t 2 u x y t u x y δ y t δ y 2 where δ x 2 and δ y 2 are position increments in the x and y directions respectively we assume the parameters d u k u and r u to be constants over time and space as presented in previous studies acevedo et al 2012 richit et al 2017 using the finite differences approximations given by eqs 6 8 to discretize the equation of the dlg model eq 1 applying the crank nicolson scheme crank and nicolson 1947 and some algebraic manipulation we can write the discretized model as 9 u i j k 1 u i j k d u δ t 2 δ y 2 u i 1 j k 1 2 u i j k 1 u i 1 j k 1 u i 1 j k 2 u i j k u i 1 j k d u δ t 2 δ x 2 u i j 1 k 1 2 u i j k 1 u i j 1 k 1 u i j 1 k 2 u i j k u i j 1 k r u δ t u i j k 1 u i j k k u where the value of the continuous function u x y t in a point x i y j of the grid at time step t k is represented as u i j k assuming δ x δ y and defining μ d u δ t 2 δ x 2 eq 9 can be rewritten as 10 1 4 μ u i j k 1 μ u i 1 j k 1 u i j 1 k 1 u i j 1 k 1 u i 1 j k 1 1 4 μ u i j k μ u i 1 j k u i j 1 k u i j 1 k u i 1 j k r u δ t 1 u i j k u i j k the application eq 10 for each element of the computational domain results in a linear system that can be written in matricial form as 11 a u k 1 b u k w k where w k is a vector that represents the discrete version of the logistic term from eq 1 a and b are the characteristic matrices of the linear system of equations and u k is the vector of forest density values for each element of the discretized grid this numerical approach benefits from the unconditional stability of the crank nicolson scheme and the next step is to solve the resulting linear system for each time step considering that u k at k th time step is known we therefore also know b u k and w k thus the solution of the linear system given by eq 11 requires the calculation of the unknown vector of densities for the next time step which is u k 1 to simplify the notation to the usual form a x b we write b k b u k w k so that a u k 1 b k the system has to be solved at every time step and the solution u k 1 is given by 12 u k 1 a 1 b k due to the computational burden of inverting large sparse matrices the solution of the system does not involve the actual calculation of the expression in eq 12 instead the solution is obtained numerically by means of the bi conjugate gradient stabilized method bicgstab for the model application we include the boundary conditions and initial values of the property u which can be obtained from the application of an adequate vegetation index for standard satellite images details on these procedures are also covered in the following section 2 3 the computational framework r recovery this section describes the functionality of the r recovery module including the pre processing procedures required for forest regeneration studies 2 3 1 computational domain boundary and initial conditions interactions among variables the module permits the definition of irregular computational domains so the user can delineate the exact area where the simulation will be performed the active cells belonging to the computational domain within a rectangular portion of a standard satellite image are indicated by means of an auxiliary matrix the objective is to permit that each simulation be tailored to the specific needs and characteristics of a given area of interest and situation the auxiliary matrix we referred to is a land use matrix which can be generated in the grass software and its purpose is to give the classification of each point of the mesh grid into classes during the mounting of the matrix of the linear system the auxiliary matrix permits the definition of two subclasses that will inform to the core routine the type of boundary condition to be implemented for each point belonging to a boundary of the computational domain suppose a pair of points representing forested areas are separated by a set of points that represent a river within the satellite image the auxiliary matrix permits two subclasses by means of which the user is allowed to inform whether or not the forest points from the two margins of the river will interact with each other in regard to the diffusion process this option can also be used in the cases when the forested area in question is crossed by roads highways railways or when it contains lakes weirs or dams similarly the user might opt to set large rivers as non flux boundary conditions between portions of forest along their banks the illustration of the interaction between borders is presented in fig 2 the case in which the boundaries do not interact with each other is set to behave according to a non flux condition on the density that is j 0 in the fick s law equation so that u 0 in both x and y directions 2 3 2 mathematical solution since a rectangular computational domain is not always the case in the forest growth simulation and since computational domains with irregular forms are admitted the next procedure in the module is to map the possibly fragmented grid of active cells to a linear system that represents the behavior of each active cell in the grid after the module maps the active cells in the grid to an element number in the matrix of the system the matrices a and b are created and stored in compressed row storage crs format matrix b is employed to generate vector b k at every time step by means of the multiplication b u k at each time step k vector b k is then applied to the solution of the linear system in eq 12 by means of bicgstab sleijpen et al 2010 van der vorsthenk 1992 2 3 3 model application the calibration and validation processes use two time lapse satellite images of the computational domain the time lapse should be chosen so that the forested area has enough time to recover later in the case study we will show that we consider time lapses of about ten years between the images which seem to work well for vegetal formations of the atlantic forest biome in brazil the earlier image will serve as the initial condition and the latter as the end condition in order to perform the calibration and validation of the model the images for the initial and end conditions are divided in half the left halves are used for the calibration and the right ones for the validation process thereby four sampled images are used to accomplish the verification of the model consistency the model must present consistent simulated end vegetation density values when compared to the real end ones 2 3 4 calibration of the model the calibration process was carried out using the two left halves of the split images the calibration consists of finding the optimal parameters d u and r u that evolve the model from the initial to the end condition in that time lapse the calibration of the model is implemented in the form of a succession of integrations of the system in which the optimal set of parameters is derived from the evaluation of the root mean squared error rmse between the simulated and the real end condition the term optimal is used here to denote the best solution within a matrix of parameter values for a given interval and resolution it can be observed in the calibration process that due to the complex dynamics and heterogeneity of the spatial density distribution the root mean squared error can be rather crude in the analysis of the best set of parameters thus we resort to the forest density histograms comparison the histograms of the simulated and real end conditions are compared to check whether the process of forest growth was properly captured by the model when the model is properly tuned the histograms should resemble each other it is also recommended to compare the similarity using histograms of the absolute values of the differences between the simulated and observed final vegetation density values for each histogram class bin thus for a given forest density frequency f i for class i the error function can be written as 13 e r r o r i 1 n f i p r e d f i o b s in which the superscripts represent observed o b s and simulated or predicted p r e d vegetation density values and the subscripts represent the index of class i for n classes the module is programmed for the automatic calibration of parameters so the user is not required to set the parameter ranges the ranges for the search of parameters are predefined to values adequate for most types of vegetation within these ranges the search is performed for increasingly finer resolutions in which the ranges of r u and d u are redefined to increasingly shorter intervals until a set of parameters that satisfies the error tolerance condition is found the process is illustrated in fig 3 2 3 5 validation of the model once the optimal parameters are found in the calibration process they are fixed and the model is run against the right halves of the split images as in the calibration process the rmse and the error function in eq 13 are derived from the comparison between the simulated and real forest density values to evaluate the model performance 2 3 6 input data and parameters declaration should the user previously know the values for the diffusion and growth rate parameters d u and r u the calibration step can be skipped and the parameters can be informed directly in the simulation inputs fields in addition to the parameters the auxiliary matrix containing the map of land use and the total simulation time are required if the parameters are not previously known then the application of r recovery will require the calibration of the model as described in the previous subsection fig 4 presents the module window for the simulation inputs in this example the parameters are known and are input in the manual mode the parameter values are set to k u 1 0 r u 0 03 e d u 2 33 and the total simulation time to 50 years the initial vegetation density is informed in the vegetation density raster map field while the land use classes are informed in the soil use raster map the output raster maps where the results of the simulation are stored are informed in the output density raster map field in this example fig 2 the land use is divided into 9 classes with classes 4 to 9 representing grid cells belonging to the computational domain and class 3 representing boundary elements that feature boundary flux that is they interact with elements from the other side of a river for example 2 3 7 considerations on vegetation density the vegetation density maps are applied in the calibration and validation processes and also in the simulations of forest growth prognostics to obtain the vegetation density maps from the standard satellite imagery we tested several methodologies available in the grass gis software such as the normalized difference vegetation index ndvi huete et al 1997 liu and huete 1995 pinty and verstraete 1992 atmospherically resistant vegetation index arvi kaufman and tanre 1992 soil adjusted vegetation index savi huete et al 1997 huete 1988 leaf area index lai carlson and ripley 1997 enhanced vegetation index evi huete et al 2002 among others the user ought to be familiar with the principles and workings of the vegetation index to be applied in such a way that the points of interest in the resulting vegetation index map can be properly selected and the regions not of interest disregarded furthermore before the outputs of a vegetation index are applied to the module their values have to be mapped to the working interval of the model which ranges from density zero no forest to the density informed by the carrying capacity k u which is generally set to unity thus the normalization of the output to the working interval of the model is an important step to preserve the integrity of the procedure and the meaning of the results as an example the evi features vegetation densities ranging approximately from 1 0 to 1 0 with negative values indicating areas with higher reflectance such as water or clouds and values greater than 1 0 indicating urban regions allen et al 2002 kaufman and tanre 1992 pettorelli et al 2005 cells featuring negative values for vegetation density or those with vegetation density values higher than unity are disregarded from the computational domain by the r recovery module since they do not contain vegetation growth as a consequence the normalization might be unnecessary in the case where the vegetation index output range matches the model working range but it must be kept in mind that the values should be normalized so that the calibration values of the parameters r u and d u can be compared across studies considering different vegetation indices 2 3 8 the interface the module features two tabs named simulation inputs and parameters calibration which are shown in fig 5 and fig 6 respectively the simulation inputs tab requires vegetation density raster maps for the initial conditions the land use raster map and the model parameters for simulation the parameters calibration tab requires vegetation density raster maps for both initial and end conditions for calibration and the land use raster map and total simulation time the interfaces follow the current standard found in the grass gis modules such that most of the inputs and procedures related to the application of the model are rather intuitive 3 case study forest regeneration in the espigão alto state park brazil in this section we illustrate the application of the module for the prognostics of forest growth in the espigão alto state park in brazil we begin with a brief description of the area under study and then we present the methodologies for the acquisitioning and processing of the satellite images and forest density maps employed in the calibration and validation processes 3 1 description of the area the espigão alto state park easp is a permanently conserved area located in the city of barracão rio grande do sul state brazil the park was created by decree nº 658 1949 containing an area that was originally 2 540 h a sema defap 2004 according to the classification applicable to permanent conservation units in brazil the easp belongs to an integral conservation unit meaning that any direct use of its natural resources is not permitted under brazilian law brasil 2000 the park was created to preserve ecosystems of considerable ecological importance and which are subject to scientific studies and environmental education initiatives due to recurrent invasions and reduction in size over time the current area of the easp now stands at 1 325 4 h a the clandestine occupation of the park came to an end in 2002 as a result of decree number 42 000 which determined the expropriation of plots of land pertaining to that park that were being illegally occupied for farming brasil 2002 the easp lies between latitudes 27 3 0 s and 27 4 5 s and longitudes 51 20 w and 51 40 w sema defap 2004 the easp location is shown on the map in fig 7 the soil from the region is classified as deep aluminoferric red oxisols formed from basalt and characterized by the presence of dark red clay and high cationic capacity this facilitates the absorption of nutrients by plants however the soil is naturally poor in potassium and nitrogen the climate in the region is classified as humid subtropical with hot summers and designated cfa according to köppen geiger kottek et al 2006 peelmurray et al 2007 precipitation varies according to the season and winter is usually the rainiest mean annual precipitation varies between 1 600 and 2 000 m m and the mean temperature is 18 c although summer temperatures are usually higher than 22 c and winter temperatures can fall to 4 c inmet 2017 the easp is inserted in the atlantic forest biome and it features a blend of subformations of mixed ombrophile forest araucaria forest and dense ombrophile forest broad leaved forest in primary and secondary stages of development sema defap 2004 the easp is regarded as a conservation unit of strategic importance for the conservation of the biodiversity of fauna and flora since it shelters a number of species threatened with extinction in addition the creation and maintenance of forest remnants in the region play an important role since the forests pertaining to the easp are considered as conservation priorities according to the brazilian ministry of the environment bme brasil 2007 according to the review carried out in 2007 by the bme the conservation unit itself is considered as an extremely high biodiversity conservation priority area and the riparian forests and the uruguay river banks from above its confluence are considered areas of priority classified as very high and even extremely high priority in some spots brasil 2007 due to invasions and clandestine occupations part of the easp has suffered a loss of green areas because of the removal of the native forests after expropriation such areas underwent a process of regeneration and replanting sema defap 2004 the main interest in using this area as a case study is due to the importance of being able to assess its regeneration potential and make prognostics on how the regeneration process will develop in the coming decades this information can be obtained by applying the module to the data presented in the next section 3 2 data images acquisition and processing following the procedures presented in ref acevedo et al 2012 taking into consideration the improvements proposed in ref richit et al 2017 and the remarks from subsection 2 3 7 we derive the evi values from the satellite images as described in ref solano et al 2010 to determine the vegetation index for the satellite images the landsat 5 tm scenes were acquired from the us geological survey site usgs 1994 usgs 2008 for the application in the case study the spectral bands of each image were converted from digital number dn to reflectance next the atmospheric and topographic corrections were applied by means of the grass gis modules i landsat toar and i topo corr then the spectral bands of the resulting images were converted to the vegetation index by means of the module i vi which implements the equation 14 e v i g n i r r e d n i r c 1 r e d c 2 b l u e l where n i r r e d b l u e represent the spectral bands near infrared red and blue respectively the parameters of the algorithm were set to the standard values solano et al 2010 as g 2 5 c 1 6 c 2 7 5 and l 1 the above procedure was applied to each image to generate the evi numeric matrix for scenes from the landsat 5 tm for1994 and 2008 usgs 1994 usgs 2008 these were used as the initial and final vegetation density conditions for the calibration and validation of the model the 2008 scene usgs 2008 was also used as the initial condition for the simulation of forest recovery the simulation was performed for a period of time 60 years in the future in order to allow a long term prognostic of forest growth 3 3 results this subsection presents the results from the application of the r recovery module first the calibration and validation of the model are presented and the parameter values from calibration are discussed as well as the results from the sensitivity evaluation then the model is applied to simulate forest growth in the easp with the current vegetation status in the park taken as the starting point 3 3 1 calibration of the model the land use map is required as an input to determine the computational area along with information regarding the classes representing regions pertaining to the computational domain an example of the land use map is shown in fig 8 c on this map all three classes are part of the computational domain and represent the classes of vegetation cover low medium and high vegetation density the rmse found in the calibration was 0 051 and its spatial distribution is shown in fig 9 the calibration process resulted in the pair of parameters r u 0 023 y r 1 and d u 0 895 m 2 y r 1 the sensitivity of the model to the parameters can be investigated by observing the behavior of the error for variations in the parameters it is known from previous studies that the model is less sensitive to the diffusion arameter d u and more sensitive to the growth parameter r u see acevedo et al 2012 richit et al 2017 3 3 2 validation of the model in order to validate the model the simulated end condition is compared to the real end condition obtained from the satellite image right side as shown in fig 8 a b the rmse found was 0 068 fig 9 right side shows the error map displaying the difference between the vegetation density values predicted by the model and those obtained from the satellite image positive values indicate areas where the model overestimated vegetation growth and negative values where growth was underestimated 3 3 3 simulation of forest growth in the easp due to the area reduction and human occupation endured by the easp up until the last decade some areas have had their vegetation suppressed and are currently undergoing a regeneration process that will span several decades in order to obtain a prognostic of vegetation growth and regeneration in the easp we apply the r recovery module considering a recent satellite image that features the current state of vegetation in the park as initial condition the vegetation density map was obtained by applying the enhanced vegetation index evi to the satellite images usgs 2008 within the area of the park as shown in fig 10 a according to the methodology described in section 3 2 additionally as a requirement for the application of the module the land use map was generated in the grass gis software in order to identify the classes of soil cover and allow the definition of the computational domain in this step of the procedure any parts of the maps that do not belong to the computational domain such as rivers and farming areas for instance can be straightforwardly identified and excluded in the simulation window the land use map for the easp is presented in fig 10 b for which the number of active cells in the computational domain is 15 020 in this case the entire park is part of the computational domain thus all classes are included in the simulation by informing their number in the soil use map classes field in the simulation window of the r recovery module in order to simulate forest growth for 60 years ahead we apply the parameters obtained in the calibration validation process presented in section 3 3 1 the graphical outputs 10 year time lapses were selected and are shown in fig 11 from the figures one can observe the expected evolution in the vegetation density profile over the coming decades 4 discussions regarding the limitations of the module we start from the conceptual model itself which presents assumptions that may intuitively be oversimplifications due to the complex dynamics of the forest regeneration process the main simplification seems to be the homogeneity of the constants k u d u and r u over the entire computational domain acevedo et al 2012 in his pioneering work raised questions involving the effects of homogeneity especially concerning the intrinsic growth rates r u and diffusion coefficients d u in which the authors observe that taking the parameters as functions of position can provide the incorporation of different factors that influence the growth process however in this case the calibration of the resulting model could be a major issue due to the quantity of parameters to be calibrated remember the assumption regarding the homogeneity reduces the model to an equation with three unknown parameters in summary the assumption regarding the homogeneity of parameters which allows the simplification of the model makes the problem simpler and still provides a good quantitative and qualitative description of the process of forest growth besides the complexity of the formulation is kept to a minimum which permits an intuitive understanding of the model a relevant consideration from the user s point of view is the calibration of the model considering that the formulation and numerical solution of the model is based on the homogeneity of the parameters k u d u and r u the optimal parameter values found after calibration refer to the minimization of an error function and require validation by the user based on visual inspection and comparison of the simulated and real end condition images in other words there are cases when the simulated end condition does not resemble the real one causes we have found for this are i the growth dynamics is too heterogeneous or is heavily impaired by factors not considered by the model such as fire or flooding in parts of the area under study consequently the simulated growth obtained by means of the model does not match that of the heterogeneous growth observed in the real case ii the objective function upon which the model is calibrated is set to give an optimal combination of parameters on the basis of the numerical outcomes of the model this means that the visual inspection and validation by the user is a fundamental part of the process therefore graphical validation by the user should not be dismissed since re calibration might be necessary in some cases however in most cases and also in the case study presented in the last section the calibration results for r u and d u were found to be adherent to the description of the forest recovery process in summary we have found by means of the application of the module that the model is able to accurately represent the dynamics of forest density evolution the tools offered by the module presented in this paper were built to be seamlessly integrated to and combined with those existing in the grass gis software this permits that all steps of the image treatment and modeling process are performed within the same software environment a more elaborated version of the dlg model and future versions of r recovery may incorporate the possibility of providing further inputs in order to improve the accuracy of the results these could include environmental factors such as soil depth solar availability soil type and or other environmental factors precipitation itself could be an important factor for the description of plant growth but in practice this analysis would make the problem stochastic and the scenario for forecasting would have to consider estimates of future precipitation which could pose new challenges due to the complex nature of the rainfall regime from the point of view of users and professionals involved in forestry management and environmental studies a fully available free and open source tool such as the r recovery can bridge the gap towards the synergy among professionals currently interested in this topic given the urgency of environmental conservation and recovery issues there are many professionals currently working in environmental areas in this context we find that the development of open platform tools that can provide scientifically based information and enhance the application of technical skills in the solution of environmental problems is a route to a more efficient and effective problem solving planning besides tools that operate over open platforms can provide widespread access and receive widespread contributions for further developments it is worth mentioning that a great deal of models appearing in the literature fall short in terms of applicability due to the lack of end user friendly interfaces or modules 5 final remarks in this paper we propose and present the module r recovery which is a full implementation of the diffusive logistic growth dlg model for the simulation of forest growth and regeneration the module was written in c programming language to be operated as an add on to the gis software grass it is fully compatible with existing modules in the grass software and provides the user with tools for the calibration validation and application of the dlg model as proposed in ref acevedo et al 2012 the results shown in this paper illustrate the use and the features of the module its interface and capabilities in making prognostics of forest growth and recovery further developments ameliorations and functionalities are encouraged the code for the module developed in this research is freely accessible and it can be obtained at https github com uffsenvmodelling r recovery acknowledgements the authors thank cnpq grant n 460329 2014 6 
26285,we present an open source computational tool for the 2d simulation of the diffusive logistic growth dlg model the r recovery module offers a complete environment for the simulation of forestry regeneration in conservation areas and includes a built in tool for calibration and validation of the model parameters through the use of standard and freely available satellite imagery it was implemented as an add on to the grass software a largely applied open source geographic information system gis to illustrate its application we present a complete case study of forest regeneration carried out in the espigão alto state park easp brazil from which we assess typical values of forest diffusion and growth rate parameters along with the prognostics of forest density status for the coming decades we observe that the r recovery tool can be advantageously applied by forestry managers and policy makers as a form of acquiring technical and scientifically based information for strategy development and decision making keywords forest growth diffusive logistic growth model environmental conservation grass module availability program name r recovery developer l a richit contact address luizaugustorichit gmail com year first available 2018 software required grass gis 7 0 or later program language c package size 144 kb source code availability https github com uffsenvmodelling r recovery cost free of charge 1 introduction the activity of suppressing forested areas for agriculture livestock and urbanization purpose is responsible for the loss of biodiversity environmental quality soil humidity water quality and quantity among others johnson et al 2013 parrotta and knowles 2001 vought et al 1995 yu et al 2011 forest regeneration takes place by means of a recolonization process which is begun by the remnants of the original forest this process can vary according to the characteristics of the vegetation the climatic conditions and the level of damage and degradation of the original forest cole et al 2014 fanta et al 1996 holl 1999 kilian 1998 wenhua 2004 researchers have proposed different methods to estimate the amount of time required for a forest to fully recover after suffering degradation while others have argued that the original environment cannot be achieved again nevertheless there is a consensus regarding the forest recovery time which requires anything from decades to centuries to stabilize should this be depending on the biome characteristics cole et al 2014 forest growth is described in the literature in numerous ways most of which are based on qualitative features such as seed dispersion benitez malvidojulieta 1998 clark et al 1999 higgins and richardson 1999 howe and smallwood 1982 soil characteristics elevation humidity species and successional stages of development havlicek and mitchell 2014 krumins and adams 2014 reyer et al 2015 uhl 1987 on the other hand the application of mathematical models to study forest growth is still at an early stage the models of forest growth were commonly associated with models of population growth in which a reactive term describes autorregulation and the balancing between offer and demand for different populations or resources cantrell and cosner 2004 skellam 1951 verhulst 1845 1847 a more elaborate model was employed by acevedo et al 2012 on the basis of the fisher kolmogorov equation fisher 1937 in which the logistic term was combined with diffusion to give rise to the diffusive logistic growth dlg model the authors successfully applied the dlg model to study forest regeneration for the period from 1951 to 1991 1992 in abandoned deforested areas on the island of puerto rico acevedo et al 2012 beyond its value of providing diagnostics prognostics and an useful management tool the model also permits the assessment of parameters of growth dynamics which can be of importance in the understanding of vegetation growth across different biomes while the logistic term provides the logistic autorregulation of growth the diffusive term describes the intrinsic colonization that takes place in forested areas cui and chen 1998 holmes et al 1994 the current status of forest growth models can be evaluated from the careful review presented by porté bartelink porté and bartelink 2002 which proposed a classification of vegetation growth models such a classification would only consider mathematical models of individual growth and thus would not include the dlg model which was proposed and applied for the first time after the publication by porté bartelink porté and bartelink 2002 nevertheless it makes clear that the work by acevedo et al 2012 proposes a different and somewhat complementary approach which changes the focus from growth models for individual trees to growth models for forests as a whole while growth models for individual trees require detailed costly and time consuming inventories forest growth models usually apply satellite imagery with in loco validation of certain key points thus allowing faster assessment of forest growth prognostics for environmental studies regarding the application of environmental models in general there has been an increasing effort from the modeling community to combine them with gis software in order to deliver readily applicable effective and easily operated tools in particular among the free and open source gis software the grass geographic resources analysis support system initiative has received considerable attention neteler et al 2012 there are numerous instances of modules working under this framework r water fea presented by vieux gauer vieuxbaxter and gauer 1994 and employed to simulate storm water runoff r sun presented by hofierka et al 2002 which estimates global radiation based on the work developed by the european solar radiation atlas esra more recent studies include that presented by tonini et al 2017 who created a module that evaluated the advances of an invasive species of pathogens the open source code r avaflow v1 developped by mergili et al 2017 to predict mass movement such as avalanches of rocks ice and debris the study presented by strigaro et al 2016 regarding the implementation of r glacio model to predict the glacial behavior under climate change with a case study of the rutor glacier in the italian alps the r diversity module present by rocchini et al rocchiniducciodelucchi et al 2013 to estimate the landscape diversity among others in this paper we present the r recovery module for grass gis the module implements the diffusive logistic growth equation and permits the calibration validation and application of the forest growth model for standard satellite imagery supported by gis softwares the module was written in c programming language using the grass libraries and follows the open source ideal of the software after presenting the working and operation of the module we present a case study that considers the process of forest recovery in the espigão alto state park located in the southern part of brazil from this we assess typical values of forest diffusion and growth rate parameters along with prognostics of forest density status for coming decades the paper is organized as follows section 2 presents the materials and methods applied in the implementation of the module section 3 presents the case study and illustrates the application of the module section 4 discusses the main findings and results obtained in this research and section 5 outlines the concluding remarks 2 materials and methods this section briefly presents the dlg model on the basis of refs acevedo et al 2012 richit et al 2017 and the discretization scheme for the equation the details on the implementation of the r recovery module are given and discussed later in the paper 2 1 a model of forest recovery reaction diffusion equations are commonly applied to population dynamics modeling since these two processes combined can handle the description of population movement mortality reproduction and regulation cantrell and cosner 2004 okubo 1980 an early study of the reaction diffusion equation aimed at modeling population dynamics was presented by fisher 1937 where the population growth is described by means of the diffusive logistic equation allen and linda 1983 bassan et al 1997 cantrell and cosner 2004 gurtin et al 1977 in this study we apply the diffusive logistic growth equation as presented by acevedo et al 2012 and further studied in ref richit et al 2017 to consider the problem of population dynamics as applied to forest recovery the model describes the dynamics of forest growth as a combination of both logistic growth which considers self limited population growth and diffusion which considers and quantifies colonization an illustrative scheme of the two processes and their combination is presented in fig 1 in the 2d formulation the diffusive logistic growth model equation can be written as 1 u t d u 2 u x 2 2 u y 2 r u 1 u k u u where u describes the forest density at a given instant t for a given position x y r u is the forest growth rate k u is the carrying capacity from the logistic term d u is the forest diffusion coefficient acevedo et al 2012 cantrell and cosner 2004 holmes et al 1994 skellam 1951 the classical logistic model describes the change of population on the basis of exponential growth with self regulation which plays a role in describing factors that inhibit population growth such as food shortages diseases or overcrowding chapra 2012 holmes et al 1994 this model equation can be written as follows 2 u t r u 1 u k u u the logistic model in eq 2 admits an analytical solution the solution predicts the population growth as a function of time as 3 u t k u u 0 u 0 k u 0 e r u t where u 0 is the initial population at arbitrary time t 0 thus analyzing the limit of eq 3 when t the population size tends to the carrying capacity k u despite u 0 k u or u 0 k u cantrell and cosner 2004 formally independent of the initial population size we have 4 l i m u t t k u to complete the diffusive logistic growth model with the inclusion of the diffusive term consider the dispersion of population as a brownian motion the flux of species u is represented by first fick s law as j d u u cengel and ghajar 2011 welty et al 2009 white 1999 analogously to the standard notion of heat flux or mass transfer fick s law is an empirical consideration regarding the transport of particles concentrations or individuals from a specified point in which the flux is taken as being proportional to the negative of the concentration gradient cantrell and cosner 2004 so applying the continuity equation of a quantity u fick s law as used to describe diffusion can be written as 5 u t j q where q q x y t is a source or sink term presuming that the density variable u follows a combination of diffusion and logistic growth we obtain the dlg model eq 1 in summary as written in the form of eq 1 the model asserts that population density u can be described as the combination of the incorporation of population by means of diffusion from the points of higher concentrations to those of lower concentrations and is restricted by the self regulation of the logistic term acevedo et al 2012 skellam 1951 verhulst 1845 1847 2 2 numerical scheme for dlg model following the procedures presented in acevedo et al 2012 we apply the crank nicolson scheme to discretize eq 1 considering this scheme the time derivative term is substituted by the first order forward difference approximation as 6 u t u x y t δ t u x y t δ t where δ t is the time increment between consecutive time steps finally the spatial derivatives at time t are approximated with the second order differences as 7 2 u x 2 u x δ x y t 2 u x y t u x δ x y t δ x 2 8 2 u y 2 u x y δ y t 2 u x y t u x y δ y t δ y 2 where δ x 2 and δ y 2 are position increments in the x and y directions respectively we assume the parameters d u k u and r u to be constants over time and space as presented in previous studies acevedo et al 2012 richit et al 2017 using the finite differences approximations given by eqs 6 8 to discretize the equation of the dlg model eq 1 applying the crank nicolson scheme crank and nicolson 1947 and some algebraic manipulation we can write the discretized model as 9 u i j k 1 u i j k d u δ t 2 δ y 2 u i 1 j k 1 2 u i j k 1 u i 1 j k 1 u i 1 j k 2 u i j k u i 1 j k d u δ t 2 δ x 2 u i j 1 k 1 2 u i j k 1 u i j 1 k 1 u i j 1 k 2 u i j k u i j 1 k r u δ t u i j k 1 u i j k k u where the value of the continuous function u x y t in a point x i y j of the grid at time step t k is represented as u i j k assuming δ x δ y and defining μ d u δ t 2 δ x 2 eq 9 can be rewritten as 10 1 4 μ u i j k 1 μ u i 1 j k 1 u i j 1 k 1 u i j 1 k 1 u i 1 j k 1 1 4 μ u i j k μ u i 1 j k u i j 1 k u i j 1 k u i 1 j k r u δ t 1 u i j k u i j k the application eq 10 for each element of the computational domain results in a linear system that can be written in matricial form as 11 a u k 1 b u k w k where w k is a vector that represents the discrete version of the logistic term from eq 1 a and b are the characteristic matrices of the linear system of equations and u k is the vector of forest density values for each element of the discretized grid this numerical approach benefits from the unconditional stability of the crank nicolson scheme and the next step is to solve the resulting linear system for each time step considering that u k at k th time step is known we therefore also know b u k and w k thus the solution of the linear system given by eq 11 requires the calculation of the unknown vector of densities for the next time step which is u k 1 to simplify the notation to the usual form a x b we write b k b u k w k so that a u k 1 b k the system has to be solved at every time step and the solution u k 1 is given by 12 u k 1 a 1 b k due to the computational burden of inverting large sparse matrices the solution of the system does not involve the actual calculation of the expression in eq 12 instead the solution is obtained numerically by means of the bi conjugate gradient stabilized method bicgstab for the model application we include the boundary conditions and initial values of the property u which can be obtained from the application of an adequate vegetation index for standard satellite images details on these procedures are also covered in the following section 2 3 the computational framework r recovery this section describes the functionality of the r recovery module including the pre processing procedures required for forest regeneration studies 2 3 1 computational domain boundary and initial conditions interactions among variables the module permits the definition of irregular computational domains so the user can delineate the exact area where the simulation will be performed the active cells belonging to the computational domain within a rectangular portion of a standard satellite image are indicated by means of an auxiliary matrix the objective is to permit that each simulation be tailored to the specific needs and characteristics of a given area of interest and situation the auxiliary matrix we referred to is a land use matrix which can be generated in the grass software and its purpose is to give the classification of each point of the mesh grid into classes during the mounting of the matrix of the linear system the auxiliary matrix permits the definition of two subclasses that will inform to the core routine the type of boundary condition to be implemented for each point belonging to a boundary of the computational domain suppose a pair of points representing forested areas are separated by a set of points that represent a river within the satellite image the auxiliary matrix permits two subclasses by means of which the user is allowed to inform whether or not the forest points from the two margins of the river will interact with each other in regard to the diffusion process this option can also be used in the cases when the forested area in question is crossed by roads highways railways or when it contains lakes weirs or dams similarly the user might opt to set large rivers as non flux boundary conditions between portions of forest along their banks the illustration of the interaction between borders is presented in fig 2 the case in which the boundaries do not interact with each other is set to behave according to a non flux condition on the density that is j 0 in the fick s law equation so that u 0 in both x and y directions 2 3 2 mathematical solution since a rectangular computational domain is not always the case in the forest growth simulation and since computational domains with irregular forms are admitted the next procedure in the module is to map the possibly fragmented grid of active cells to a linear system that represents the behavior of each active cell in the grid after the module maps the active cells in the grid to an element number in the matrix of the system the matrices a and b are created and stored in compressed row storage crs format matrix b is employed to generate vector b k at every time step by means of the multiplication b u k at each time step k vector b k is then applied to the solution of the linear system in eq 12 by means of bicgstab sleijpen et al 2010 van der vorsthenk 1992 2 3 3 model application the calibration and validation processes use two time lapse satellite images of the computational domain the time lapse should be chosen so that the forested area has enough time to recover later in the case study we will show that we consider time lapses of about ten years between the images which seem to work well for vegetal formations of the atlantic forest biome in brazil the earlier image will serve as the initial condition and the latter as the end condition in order to perform the calibration and validation of the model the images for the initial and end conditions are divided in half the left halves are used for the calibration and the right ones for the validation process thereby four sampled images are used to accomplish the verification of the model consistency the model must present consistent simulated end vegetation density values when compared to the real end ones 2 3 4 calibration of the model the calibration process was carried out using the two left halves of the split images the calibration consists of finding the optimal parameters d u and r u that evolve the model from the initial to the end condition in that time lapse the calibration of the model is implemented in the form of a succession of integrations of the system in which the optimal set of parameters is derived from the evaluation of the root mean squared error rmse between the simulated and the real end condition the term optimal is used here to denote the best solution within a matrix of parameter values for a given interval and resolution it can be observed in the calibration process that due to the complex dynamics and heterogeneity of the spatial density distribution the root mean squared error can be rather crude in the analysis of the best set of parameters thus we resort to the forest density histograms comparison the histograms of the simulated and real end conditions are compared to check whether the process of forest growth was properly captured by the model when the model is properly tuned the histograms should resemble each other it is also recommended to compare the similarity using histograms of the absolute values of the differences between the simulated and observed final vegetation density values for each histogram class bin thus for a given forest density frequency f i for class i the error function can be written as 13 e r r o r i 1 n f i p r e d f i o b s in which the superscripts represent observed o b s and simulated or predicted p r e d vegetation density values and the subscripts represent the index of class i for n classes the module is programmed for the automatic calibration of parameters so the user is not required to set the parameter ranges the ranges for the search of parameters are predefined to values adequate for most types of vegetation within these ranges the search is performed for increasingly finer resolutions in which the ranges of r u and d u are redefined to increasingly shorter intervals until a set of parameters that satisfies the error tolerance condition is found the process is illustrated in fig 3 2 3 5 validation of the model once the optimal parameters are found in the calibration process they are fixed and the model is run against the right halves of the split images as in the calibration process the rmse and the error function in eq 13 are derived from the comparison between the simulated and real forest density values to evaluate the model performance 2 3 6 input data and parameters declaration should the user previously know the values for the diffusion and growth rate parameters d u and r u the calibration step can be skipped and the parameters can be informed directly in the simulation inputs fields in addition to the parameters the auxiliary matrix containing the map of land use and the total simulation time are required if the parameters are not previously known then the application of r recovery will require the calibration of the model as described in the previous subsection fig 4 presents the module window for the simulation inputs in this example the parameters are known and are input in the manual mode the parameter values are set to k u 1 0 r u 0 03 e d u 2 33 and the total simulation time to 50 years the initial vegetation density is informed in the vegetation density raster map field while the land use classes are informed in the soil use raster map the output raster maps where the results of the simulation are stored are informed in the output density raster map field in this example fig 2 the land use is divided into 9 classes with classes 4 to 9 representing grid cells belonging to the computational domain and class 3 representing boundary elements that feature boundary flux that is they interact with elements from the other side of a river for example 2 3 7 considerations on vegetation density the vegetation density maps are applied in the calibration and validation processes and also in the simulations of forest growth prognostics to obtain the vegetation density maps from the standard satellite imagery we tested several methodologies available in the grass gis software such as the normalized difference vegetation index ndvi huete et al 1997 liu and huete 1995 pinty and verstraete 1992 atmospherically resistant vegetation index arvi kaufman and tanre 1992 soil adjusted vegetation index savi huete et al 1997 huete 1988 leaf area index lai carlson and ripley 1997 enhanced vegetation index evi huete et al 2002 among others the user ought to be familiar with the principles and workings of the vegetation index to be applied in such a way that the points of interest in the resulting vegetation index map can be properly selected and the regions not of interest disregarded furthermore before the outputs of a vegetation index are applied to the module their values have to be mapped to the working interval of the model which ranges from density zero no forest to the density informed by the carrying capacity k u which is generally set to unity thus the normalization of the output to the working interval of the model is an important step to preserve the integrity of the procedure and the meaning of the results as an example the evi features vegetation densities ranging approximately from 1 0 to 1 0 with negative values indicating areas with higher reflectance such as water or clouds and values greater than 1 0 indicating urban regions allen et al 2002 kaufman and tanre 1992 pettorelli et al 2005 cells featuring negative values for vegetation density or those with vegetation density values higher than unity are disregarded from the computational domain by the r recovery module since they do not contain vegetation growth as a consequence the normalization might be unnecessary in the case where the vegetation index output range matches the model working range but it must be kept in mind that the values should be normalized so that the calibration values of the parameters r u and d u can be compared across studies considering different vegetation indices 2 3 8 the interface the module features two tabs named simulation inputs and parameters calibration which are shown in fig 5 and fig 6 respectively the simulation inputs tab requires vegetation density raster maps for the initial conditions the land use raster map and the model parameters for simulation the parameters calibration tab requires vegetation density raster maps for both initial and end conditions for calibration and the land use raster map and total simulation time the interfaces follow the current standard found in the grass gis modules such that most of the inputs and procedures related to the application of the model are rather intuitive 3 case study forest regeneration in the espigão alto state park brazil in this section we illustrate the application of the module for the prognostics of forest growth in the espigão alto state park in brazil we begin with a brief description of the area under study and then we present the methodologies for the acquisitioning and processing of the satellite images and forest density maps employed in the calibration and validation processes 3 1 description of the area the espigão alto state park easp is a permanently conserved area located in the city of barracão rio grande do sul state brazil the park was created by decree nº 658 1949 containing an area that was originally 2 540 h a sema defap 2004 according to the classification applicable to permanent conservation units in brazil the easp belongs to an integral conservation unit meaning that any direct use of its natural resources is not permitted under brazilian law brasil 2000 the park was created to preserve ecosystems of considerable ecological importance and which are subject to scientific studies and environmental education initiatives due to recurrent invasions and reduction in size over time the current area of the easp now stands at 1 325 4 h a the clandestine occupation of the park came to an end in 2002 as a result of decree number 42 000 which determined the expropriation of plots of land pertaining to that park that were being illegally occupied for farming brasil 2002 the easp lies between latitudes 27 3 0 s and 27 4 5 s and longitudes 51 20 w and 51 40 w sema defap 2004 the easp location is shown on the map in fig 7 the soil from the region is classified as deep aluminoferric red oxisols formed from basalt and characterized by the presence of dark red clay and high cationic capacity this facilitates the absorption of nutrients by plants however the soil is naturally poor in potassium and nitrogen the climate in the region is classified as humid subtropical with hot summers and designated cfa according to köppen geiger kottek et al 2006 peelmurray et al 2007 precipitation varies according to the season and winter is usually the rainiest mean annual precipitation varies between 1 600 and 2 000 m m and the mean temperature is 18 c although summer temperatures are usually higher than 22 c and winter temperatures can fall to 4 c inmet 2017 the easp is inserted in the atlantic forest biome and it features a blend of subformations of mixed ombrophile forest araucaria forest and dense ombrophile forest broad leaved forest in primary and secondary stages of development sema defap 2004 the easp is regarded as a conservation unit of strategic importance for the conservation of the biodiversity of fauna and flora since it shelters a number of species threatened with extinction in addition the creation and maintenance of forest remnants in the region play an important role since the forests pertaining to the easp are considered as conservation priorities according to the brazilian ministry of the environment bme brasil 2007 according to the review carried out in 2007 by the bme the conservation unit itself is considered as an extremely high biodiversity conservation priority area and the riparian forests and the uruguay river banks from above its confluence are considered areas of priority classified as very high and even extremely high priority in some spots brasil 2007 due to invasions and clandestine occupations part of the easp has suffered a loss of green areas because of the removal of the native forests after expropriation such areas underwent a process of regeneration and replanting sema defap 2004 the main interest in using this area as a case study is due to the importance of being able to assess its regeneration potential and make prognostics on how the regeneration process will develop in the coming decades this information can be obtained by applying the module to the data presented in the next section 3 2 data images acquisition and processing following the procedures presented in ref acevedo et al 2012 taking into consideration the improvements proposed in ref richit et al 2017 and the remarks from subsection 2 3 7 we derive the evi values from the satellite images as described in ref solano et al 2010 to determine the vegetation index for the satellite images the landsat 5 tm scenes were acquired from the us geological survey site usgs 1994 usgs 2008 for the application in the case study the spectral bands of each image were converted from digital number dn to reflectance next the atmospheric and topographic corrections were applied by means of the grass gis modules i landsat toar and i topo corr then the spectral bands of the resulting images were converted to the vegetation index by means of the module i vi which implements the equation 14 e v i g n i r r e d n i r c 1 r e d c 2 b l u e l where n i r r e d b l u e represent the spectral bands near infrared red and blue respectively the parameters of the algorithm were set to the standard values solano et al 2010 as g 2 5 c 1 6 c 2 7 5 and l 1 the above procedure was applied to each image to generate the evi numeric matrix for scenes from the landsat 5 tm for1994 and 2008 usgs 1994 usgs 2008 these were used as the initial and final vegetation density conditions for the calibration and validation of the model the 2008 scene usgs 2008 was also used as the initial condition for the simulation of forest recovery the simulation was performed for a period of time 60 years in the future in order to allow a long term prognostic of forest growth 3 3 results this subsection presents the results from the application of the r recovery module first the calibration and validation of the model are presented and the parameter values from calibration are discussed as well as the results from the sensitivity evaluation then the model is applied to simulate forest growth in the easp with the current vegetation status in the park taken as the starting point 3 3 1 calibration of the model the land use map is required as an input to determine the computational area along with information regarding the classes representing regions pertaining to the computational domain an example of the land use map is shown in fig 8 c on this map all three classes are part of the computational domain and represent the classes of vegetation cover low medium and high vegetation density the rmse found in the calibration was 0 051 and its spatial distribution is shown in fig 9 the calibration process resulted in the pair of parameters r u 0 023 y r 1 and d u 0 895 m 2 y r 1 the sensitivity of the model to the parameters can be investigated by observing the behavior of the error for variations in the parameters it is known from previous studies that the model is less sensitive to the diffusion arameter d u and more sensitive to the growth parameter r u see acevedo et al 2012 richit et al 2017 3 3 2 validation of the model in order to validate the model the simulated end condition is compared to the real end condition obtained from the satellite image right side as shown in fig 8 a b the rmse found was 0 068 fig 9 right side shows the error map displaying the difference between the vegetation density values predicted by the model and those obtained from the satellite image positive values indicate areas where the model overestimated vegetation growth and negative values where growth was underestimated 3 3 3 simulation of forest growth in the easp due to the area reduction and human occupation endured by the easp up until the last decade some areas have had their vegetation suppressed and are currently undergoing a regeneration process that will span several decades in order to obtain a prognostic of vegetation growth and regeneration in the easp we apply the r recovery module considering a recent satellite image that features the current state of vegetation in the park as initial condition the vegetation density map was obtained by applying the enhanced vegetation index evi to the satellite images usgs 2008 within the area of the park as shown in fig 10 a according to the methodology described in section 3 2 additionally as a requirement for the application of the module the land use map was generated in the grass gis software in order to identify the classes of soil cover and allow the definition of the computational domain in this step of the procedure any parts of the maps that do not belong to the computational domain such as rivers and farming areas for instance can be straightforwardly identified and excluded in the simulation window the land use map for the easp is presented in fig 10 b for which the number of active cells in the computational domain is 15 020 in this case the entire park is part of the computational domain thus all classes are included in the simulation by informing their number in the soil use map classes field in the simulation window of the r recovery module in order to simulate forest growth for 60 years ahead we apply the parameters obtained in the calibration validation process presented in section 3 3 1 the graphical outputs 10 year time lapses were selected and are shown in fig 11 from the figures one can observe the expected evolution in the vegetation density profile over the coming decades 4 discussions regarding the limitations of the module we start from the conceptual model itself which presents assumptions that may intuitively be oversimplifications due to the complex dynamics of the forest regeneration process the main simplification seems to be the homogeneity of the constants k u d u and r u over the entire computational domain acevedo et al 2012 in his pioneering work raised questions involving the effects of homogeneity especially concerning the intrinsic growth rates r u and diffusion coefficients d u in which the authors observe that taking the parameters as functions of position can provide the incorporation of different factors that influence the growth process however in this case the calibration of the resulting model could be a major issue due to the quantity of parameters to be calibrated remember the assumption regarding the homogeneity reduces the model to an equation with three unknown parameters in summary the assumption regarding the homogeneity of parameters which allows the simplification of the model makes the problem simpler and still provides a good quantitative and qualitative description of the process of forest growth besides the complexity of the formulation is kept to a minimum which permits an intuitive understanding of the model a relevant consideration from the user s point of view is the calibration of the model considering that the formulation and numerical solution of the model is based on the homogeneity of the parameters k u d u and r u the optimal parameter values found after calibration refer to the minimization of an error function and require validation by the user based on visual inspection and comparison of the simulated and real end condition images in other words there are cases when the simulated end condition does not resemble the real one causes we have found for this are i the growth dynamics is too heterogeneous or is heavily impaired by factors not considered by the model such as fire or flooding in parts of the area under study consequently the simulated growth obtained by means of the model does not match that of the heterogeneous growth observed in the real case ii the objective function upon which the model is calibrated is set to give an optimal combination of parameters on the basis of the numerical outcomes of the model this means that the visual inspection and validation by the user is a fundamental part of the process therefore graphical validation by the user should not be dismissed since re calibration might be necessary in some cases however in most cases and also in the case study presented in the last section the calibration results for r u and d u were found to be adherent to the description of the forest recovery process in summary we have found by means of the application of the module that the model is able to accurately represent the dynamics of forest density evolution the tools offered by the module presented in this paper were built to be seamlessly integrated to and combined with those existing in the grass gis software this permits that all steps of the image treatment and modeling process are performed within the same software environment a more elaborated version of the dlg model and future versions of r recovery may incorporate the possibility of providing further inputs in order to improve the accuracy of the results these could include environmental factors such as soil depth solar availability soil type and or other environmental factors precipitation itself could be an important factor for the description of plant growth but in practice this analysis would make the problem stochastic and the scenario for forecasting would have to consider estimates of future precipitation which could pose new challenges due to the complex nature of the rainfall regime from the point of view of users and professionals involved in forestry management and environmental studies a fully available free and open source tool such as the r recovery can bridge the gap towards the synergy among professionals currently interested in this topic given the urgency of environmental conservation and recovery issues there are many professionals currently working in environmental areas in this context we find that the development of open platform tools that can provide scientifically based information and enhance the application of technical skills in the solution of environmental problems is a route to a more efficient and effective problem solving planning besides tools that operate over open platforms can provide widespread access and receive widespread contributions for further developments it is worth mentioning that a great deal of models appearing in the literature fall short in terms of applicability due to the lack of end user friendly interfaces or modules 5 final remarks in this paper we propose and present the module r recovery which is a full implementation of the diffusive logistic growth dlg model for the simulation of forest growth and regeneration the module was written in c programming language to be operated as an add on to the gis software grass it is fully compatible with existing modules in the grass software and provides the user with tools for the calibration validation and application of the dlg model as proposed in ref acevedo et al 2012 the results shown in this paper illustrate the use and the features of the module its interface and capabilities in making prognostics of forest growth and recovery further developments ameliorations and functionalities are encouraged the code for the module developed in this research is freely accessible and it can be obtained at https github com uffsenvmodelling r recovery acknowledgements the authors thank cnpq grant n 460329 2014 6 
26286,land use planners landscape architects and water resource managers are using green infrastructure gi designs in urban environments to promote ecosystem services including mitigation of storm water flooding and water quality degradation an expanded set of urban sustainability goals also includes increasing carbon sequestration songbird habitat reducing urban heat island effects and improvement of landscape aesthetics gi is conceptualized to improve water and ecosystem quality by reducing storm water runoff at the source but when properly designed may also benefit these expanded goals with the increasing use of gi in urban contexts there is an emerging need to facilitate participatory design and scenario evaluation to enable better communication between gi designers and groups impacted by these designs major barriers to this type of public participation is the complexity of both parameterizing operating visualizing and interpreting results of complex ecohydrological models at various watershed scales that are sufficient to address diverse ecosystem service goals this paper demonstrates a set of workflows to facilitate rapid and repeatable creation of gi landscape designs which are incorporated into complex models using web applications and services for this project we use the rhessys regional hydro ecologic simulation system ecohydrologic model to evaluate participatory gi landscape designs generated by stakeholders and decision makers but note that the workflow could be adapted to a set of other watershed models graphical abstract image 1 software availability name gi designer rhessys notebooks software required internet browser with html 5 support later versions are recommended program languages python javascript html c availability and cost the notebooks are available from github https github com leonard psu rhessys jupyter notebooks to use in any jupyter compute environment to use the jupyter notebooks with hydroshare an account is required that is free the notebooks are available by using hydroshare s discover search engine the green infrastructure gi notebook presented in this manuscript is available here leonard l l band l lin b miles 2018 green infrastructure designer with rhessys workflow hydroshare https doi org 10 4211 hs 3f7680cf83dc426e858d5b48cb95a565 the gi web application is here https hydroterremodels psu edu rhessys gi designer gi html ecohydrolib is available from https github com selimnairb ecohydrolib and rhessysworkflows from https github com selimnairb rhessysworkflows 1 introduction a growing paradigm in urban environmental management is the adaptation and enhancement of ecosystem services with green infrastructure gi to mitigate the effects of urbanization on stormwater flooding and water quality degradation urban heat islands air quality and other adverse impacts as hydrological and ecosystem models evolve to explicitly represent the influence of fine scale landscape form on the cycling and export of water carbon and nutrients public perceptions and preferences need to be included in the management process by which water resource managers and landscape planners design and implement new urban design and infrastructure often the public is not contemplating the important interaction between the water cycle and ecosystem processes with the design form architecture and engineered and management of their own property or neighborhood therefore environmental modeling software should be designed to address the strongly integrated set of ecosystem services affected by urban form and infrastructure and engage individuals and communities about their own role in improving water quality and mitigating extreme events at the parcel to watershed scales a major management barrier is the complexity of installing compiling and consuming datasets with watershed models using sufficient spatial resolution and process representation of the fine scale ecohydrological interactions of landscape design and structure the conventional process by many hydrological modelers is to prepare data as an offline process from multiple sources copy the processed datasets to the compute environment for modeling then process the watershed model results offline with calibration and visualization these processes are cumbersome and make it difficult for other professionals and the public to participate in improving the modeling process and critiquing watershed model results the research presented in this manuscript is a step towards improving these processes by using workflows to make data and model processes more efficient to evaluate watershed model results and engage other professionals and the public with gi landscape designs to evaluate their impact to ecohydrological interactions the premise of this paper has two components conceptualized to facilitate participatory design and evaluation of gi planning the participatory design assumes that stormwater engineers community groups and potentially individual residents would collaborate in the design of preferred gi with rapid evaluation of different design effectiveness for reaching stormwater restoration goals the first component is to engage and connect storm water engineers and property owners with rapid charrettes of applying gi landscape designs on their property using web based visualizations then connecting the gi design with agency regional plans as an integrated system within the local watershed to be used as inputs for ecohydrological modeling to rapidly evaluate the design efficacy of mitigating impacts in both the built and natural environments with the potential to iterate this process to balance desirable and effective modification of the landscape utilizing gi here we demonstrate using a spatially distributed ecohydrological model the regional hydro ecologic simulation system tague and band 2004 rhessys although the gi data workflows serve as a template for other models to adapt and consume the second component is to simplify and improve the modeling setup to consume fine resolution land information and gi in rhessys models by using multiple workflows and jupyter notebooks to share data code simulations and visualizations some of these workflows were provided by the hydroterre expert system to access essential terrestrial variables from national datasets within the united states of america leonard and duffy 2016a 2014 2013 we demonstrate using the gi web application within a jupyter notebook to setup a rhessys model at the dead run watershed located in baltimore maryland usa the rhessys notebooks serve as a prototype for beginner and expert users to modify their region of interest and data sources these notebooks are available to the community within the hydroshare collaborative environment cuahsi 2017 d g tarboton et al 2014 and as resources via github for other jupyter hosted environments the following are our primary research contributions we developed a new gi web application to allow web users to design gi with a plan and street view perspective users can assign custom gi attributes for their modeling requirements and estimate costs to share with other professionals and the public new gi model data workflows gi rhessys hydroterre were created to process the gi landscape designs from the gi web application into fine spatial resolution model parameter files these new workflows integrate the rhessys ecohydrological model and the hydroterre expert system to integrate gi data bundles with local custom spatial datasets e g terrain soils land cover supplied by the user or existing hydroterre national spatial data united states we demonstrate using a new gi rhessys jupyter notebook that uses the gi web application to create gi landscape designs these designs are processed using the gi rhessys hydroterre data workflows and then processed within the gi rhessys jupyter notebook the gi rhessys jupyter notebook uses the existing ecohydrolib and rhessys workflows to prepare data and execute the rhessys ecohydrological model using the user s gi landscape designs these notebooks serve to facilitate participatory use and evaluation of gi planning this article is structured with section 2 summarizing background information and the inspiration to combine gi with rhessys and workflows using a jupyter notebook section 3 discusses the back end web application design using multiple workflows section 4 describes and demonstrates the gi web application for modelers to create gi landscape designs finally section 5 demonstrates using the web application within jupyter notebooks to develop a new rhessys ecohydrological model 2 background 2 1 the role and requirements for green infrastructure gi the motivation for gi in urban areas is to manage and protect the water cycle without relying solely on conventional engineered solutions i e dams levees detention ponds here we operationally define gi to include various forms of soil vegetation and water storage treatments including tree canopy rain gardens bioswales permeable pavement that promote processes mimicking natural infiltration transpiration subsurface storage transport and processing of reduce and slow precipitation runoff land use planners landscape architects and water resource managers are using gi solutions to reduce storm water flow from impervious surfaces e g roofs roads parking lots in urbanized watersheds mitigate downstream water quality problems and urban heat islands while creating amenities for residents jefferson et al 2017 miles and band 2015a miles 2014 poff et al 1997 walsh et al 2005 to treat non point source storm water in older established urbanized areas i e baltimore maryland usa federal state and municipal agencies are working with residential and commercial owners to retrofit impervious surfaces for example rather than have roofs drain rainwater directly through gutters into roads and storm sewers the rainwater is stored using rain barrels and tanks to be slowly released between storms or routed to infiltrate into the ground using pervious surfaces including rain gardens bioswales and wetlands before the water is directed to roads and storm sewers many municipalities have also set goals to increase tree canopy cover to promote multiple ecosystem services pincetl et al 2013 in the examples used in this paper we emphasize landscape designed gi e g swale drains rain gardens and increased tree canopy coverage the goals of federal and local agencies are to improve water quality and reduce costs from flooding events and large engineering projects by mimicking the pre development hydrology by treating storm water runoff close to or at the origin united states environmental protection agency 2017 common goals in stormwater treatment include reduction in peak flows increased baseflow and reduction in nutrient and sediment loading therefore these agencies are interested in gi landscape design solutions that will address coupled ecosystem and hydrologic cycling ranging from the building site to the watershed scale meeting regulations and policies from a variety of government and non government organizations additional sustainability goals expressed by municipalities include a range of other ecosystem services benefiting from increased tree cover to better understand how successful gi landscape design solutions can be at reducing storm water runoff at a watershed scale potential designs need to be integrated so their combined effects can be evaluated doing so suggest the use of a distributed ecohydrologic model that couples water carbon and nutrient cycling along hydrologic flowpaths to scale up gi designs at the residential scale datasets with resolutions on the order of meters to target watersheds up to 10 square kilometers to evaluate the impact of reducing storm water overland flow and improve ecohydrology 2 2 rhessys and workflows for this project we used the regional hydroecological simulation system rhessys tague and band 2004 to evaluate gi landscape designs for three major reasons first rhessys has been developed to simulate hydrological and ecological processes at patches 1 10 m resolution hillslopes 10 100 m resolution watershed and basin scales it integrates the above ground canopy processes e g evapotranspiration and net canopy photosynthesis and below ground biological processes e g root function litter and soil organic matter decomposition nitrification and denitrification running and coughlan 1988 soil water distribution and routing beven and kirby 1979 wigmosta et al 1994 and topo climate extrapolation running et al 1987 the explicitly modeled ecohydrological interaction between vegetation and water provides the basis for landscape design in gi while rhessys was initially developed for research and prediction in unmanaged and rural watersheds it has been adapted for the study and forecast of ecohydrology of managed and urban watersheds bell et al 2017 martin et al 2017 mittman et al 2012 second rhessys simulates spatially distributed and coupled water carbon and nitrogen dynamics using a landscape hierarchical structure over nested patch hillslope and watershed scales this feature makes it possible to evaluate and monitor gi performance at fine spatial scale and their cumulative and emergent effect at the watershed landscape third instead of lumping at coarse spatial scale as other watershed models e g soil and water assessment tool swat hydrological simulation program fortran hspf water and solutes are spatially distributed within hillslopes and watersheds in rhessys this feature allows for simulating the distribution routing of rain storm water from impervious surfaces e g roofs and roads and the cycling of carbon and nutrients into designed gi in the landscape rhessys and other hydrological models i e pihm swat vic require modelers to prepare i e re project and transform i e generate model input files a diverse range of geospatial datasets from multiple agencies i e usgs noaa nasa epa usda or from local sources these processes are time consuming require a broad range of expertise and are often difficult to reproduce given the prospect of petabyte datasets automation is essential to process the diverse range of datasets for hydrological modeling leonard and duffy 2013 to prepare rhessys models that incorporate gi designs the ecohydrolib miles and band 2017a 2015b and rhessys workflows miles and band 2017b simplify the process of generating rhessys input from federal agencies or local datasets and generating model parameter files furthermore these python tool suites enable modelers to incorporate their own custom and often high resolution datasets to encourage outreach provenance and reproducibility this research incorporates rhessys and workflows using an interactive jupyter notebook computing environment with cloud resources for modelers to evaluate gi designs 2 3 hydroshare s interactive jupyter notebook computing environment hydroshare https www hydroshare org is an online collaborative infrastructure to share hydrologic data model and applications crawley et al 2017 horsburgh et al 2015 morsy et al 2017 d tarboton et al 2014 d g tarboton et al 2014 hydroshare manages data and model files as fully referenced resources which can be searched and cited a resource is defined as a set of digital content that includes the science data files or model files and their corresponding metadata information dcmi 2018 hydroshare 2018 oai 2018 hydroshare enables users to share hydrological models by using the jupyter http jupyter org notebook application a server client application that allows web users to edit and run notebook documents ipython within a web browser on a local or remote compute environment jupyter team 2017a b perez and granger 2007 this paper demonstrates using jupyter notebooks to prepare a rhessys hydrological model with gi landscape designs supported with data and model workflows using hydroshare s jupyter cloud compute environment supported by consortium of universities for the advancement of hydrologic science inc cuashi jupyter notebook documents are ideal to introduce new modelers other professionals i e landscape planners and stakeholders to all the steps with documentation and samples included in a single document in this case we use the jupyter notebook to prepare and execute a rhessys hydrological model on a remote cloud compute environment and include design and visualization of new gi we have developed a collection of rhessys jupyter notebooks prepared for different user skill levels i e beginner to expert and data requirements i e gage location based custom datasets and with national datasets that allows users to go through all the steps the notebooks can be accessed from github l leonard and band 2017 or by searching for rhessys resources via hydroshare users interested in learning how to use rhessys do not need to install rhessys and dependencies on their own compute environment using the hydroshare cloud environment rhessys notebooks are ready to be executed to create an ecohydrological model hence new users spend more time on learning rhessys capabilities while experienced modelers copy and edit an existing notebook for their study location to devote more time on analysis and refinement of model results furthermore after calibrating and refining ecohydrological model results the jupyter notebook is easy to share with other users by downloading the notebook in different file formats or by creating a hydroshare resource to share with the community for critique for example residents stakeholders and other professionals i e landscape architects can critique the gi design and evaluate ecohydrological performance in a collaborative web application before requesting gi design changes to meet the decision makers objectives examples of hydrologic research using hydroshare s jupyter notebook infrastructure include taudem with a notebook that processes terrain datasets castronova 2017 and an introduction to landlab to build numerical landscape models bandaragoda 2016 bandaragoda et al 2017 furthermore work by heidari et al 2018 is using the storm water management model swmm to study trees and rain gardens at the dead run watershed with the gi web application described in this article heidari et al 2018 outside of hydroshare water related research using jupyter notebooks include the evaluation of analytics solutions for steady interface flow where the aquifer extends below the sea bakker et al 2017 and end to end workflows for assessing sea surface temperature salinity and water levels predicted by coastal ocean models subramanian et al 2015 2 4 constraints incorporation of vegetation biodiversity e g tree species at fine spatial scale has been challenging for watershed models including rhessys which requires detailed information regarding vegetation species or plant functional type pft composition and density at high spatial resolution this type of information is difficult to acquire even with satellite imagery in current rhessys applications classifying vegetation as grass deciduous and evergreen is a common practice efforts have been made to further incorporate more diverse vegetation information in rhessys based on pft water use traits lin et al in review this feature will allow rhessys to better simulate gi that are customized with vegetation species and management purposes appropriate for different climates and designs pataki et al 2013 additionally this article focuses on the software development to create gi designs using web applications and workflows that are embedded within cloud distributed compute environments using jupyter notebooks that are easy to edit and share with the water resource and ecosystem management community as new data and model resources in this presentation we do not validate data inputs for example the appropriateness of placing gi by users or inputs such as sewer infrastructure however spatial restrictions in gi placement can be spatially represented within the core gis layers furthermore we do not demonstrate calibrating and validating gi rhessys model results used in the jupyter notebooks instead we leave these issues as an exercise for notebook modeler users or assume a previously developed and calibrated model in the case where users will design and test new gi 3 gi back end workflows this section describes the software architecture building blocks supporting the gi web application using multiple workflows required by the gi rhessys jupyter notebook to evaluate gi landscape designs with rhessys the gi web application consumes multiple software and data services section 3 1 to enable interactive hydrological modeling compute environments how the gi design web application is embedded into hydroshare s jupyter interactive environment is described in section 3 2 followed by how users gi designs are transformed by the gi rhessys hydroterre data workflows section 3 3 for rhessys model workflows and hydrological modeling section 3 4 to evaluate gi landscape designs hydrological impact 3 1 overview of main components there are three main components to use gi landscape designs within the hydroshare jupyter environment with rhessys as summarized in fig 1 the first component is the gi web applications currently hosted at penn state university psu the gi web application provides tools for users to design gi in both two dimension 2d and 2 5 dimension 2 5d environments the second component is hydroterre s services http hydroterre psu edu hosted at psu to place users gi designs within both national and custom datasets referred to as gi rhessys hydroterre data workflows in this manuscript to explain data origin the last component is hydroshare s jupyter compute environment supported by consortium of universities for the advancement of hydrologic science inc cuashi cuahsi 2017 in this manuscript we refer to these workflows both data and model and compute environment as gi rhessys jupyter notebook there are two gi web application versions the first is a standalone application for users to download gi rhessys hydroterre data bundles for their modeling goals using a personal desktop computer this version is available from psu at web link 1 1 https hydroterremodels psu edu rhessys gi designer gi html users create a gi design supported by map and data services from google maps google 2017 esri esri 2017 and hydroterre leonard and duffy 2016b 2013 2014 when the user is ready to evaluate their gi design the design is transformed by hydroterre data workflows to create gi data bundles these data bundles can be downloaded for offline use the second version of the gi web application is an embedded web application for cloud based modeling using jupyter notebooks web link 2 2 https hydroterremodels psu edu rhessys gi designer gi hs html users interact with the gi web application as she or he would with browser software i e chrome firefox etc inside the notebook the web user follows the same steps as he or she would with the standalone application however rather than downloading the data bundle for personal desktop use the gi data bundle is accessed by a gi rhessys jupyter notebook to create a rhessys hydrological model using cloud resources 3 2 hydroshare jupyter interactive compute environment one method within hydroshare to share data and models is to store jupyter notebooks within a resource jupyter notebooks are documents produced by the jupyter notebook application containing computer source code e g r python javascript and html and rich text elements e g tables figures that are executable documents for hydrological data and model analysis jupyter team 2017a b for example the gi rhessys jupyter notebook is available from hydroshare as a resource here leonard et al 2017 fig 2 summarizes how the gi web application interacts with the jupyter notebook environment when users open the gi notebook via hydroshare the user is accessing a cloud based compute environment dedicated compute nodes use openstack 2017 an open source cloud computing framework with jupyterhub jupyter team 2017a b to launch jupyter notebooks the purpose of jupyterhub is to manage authentication and spawn single user servers on demand for many users on a compute environment jupyter team 2017a b each user gets a complete jupyter notebook server to execute their data and modeling processes effectively the user has their own interactive linux based compute environment instance for hydrological modeling using a web browser embedded inside the gi rhessys jupyter notebook maintained by cuashi the gi web application hosted at psu is accessible using an html iframe object and javascript code the user creates gi designs as explained in sections 4 and 5 within the embedded gi web application using representational state transfer restful web services the gi design is processed using gi rhessys hydroterre workflows at psu section 3 3 with either national or custom data products html web links pointing to the location of the gi rhessys hydroterre data bundles are shared with the jupyter notebook within the gi rhessys jupyter notebook the python based ecohydrolib miles and band 2017a and rhessys workflows miles and band 2017b consume the gi data bundles data from federal agencies i e usgs and usda and with custom data as hydroshare resources i e dead run to generate a rhessys hydrological model using the user s gi landscape design generating a rhessys hydrological model within the jupyter notebook is explained in section 3 4 3 3 gi rhessys hydroterre data workflows once the user is ready to evaluate their gi landscape design using the gi rhessys jupyter notebook the design geometry and rhessys attributes are sent using restful services to the gi rhessys hydroterre data workflows within hydroterre compute resources the gi geometry point and polygon is converted to hydroterre s projection coordinate system albers conical equal area table 1 summarizes the input variables required to execute the gi rhessys hydroterre data workflows there are two gi rhessys hydroterre data workflows that transform the user s gi design into new datasets within the watershed boundary extent fig 2 the first workflow modifies land use datasets delineated by the user with polygon boundaries within the gi web application users assign pre defined rhessys land use classifications i e vegetation type and surface imperviousness to the land use geometry the main steps within the gi rhessys hydroterre land use data workflow include 1 converting gi design polygon geometry to a raster dataset 2 assign user defined land use values to the new raster dataset 3 merge the designed raster dataset with either custom land cover datasets or with a watershed scale clipped national landcover dataset the second gi rhessys hydroterre data workflow adjusts soil types outlined by the user with polygon shapes within the gi web application users assign nine rhessys soil properties table 1 to the soil geometry the main steps within the gi rhessys hydroterre soil data workflow include 1 erase gi design polygon geometry from custom soil dataset or watershed scale clipped national soil datasets i e ssurgo 2011 statsgo 2011 2 merge the erased dataset with the user defined soil dataset with both the land use and soil workflows the resultant datasets are zipped and a message is sent to the gi web application using a html callback indicating where the data bundle zip files are located these messages are kept as global variables within the gi web application for access by the gi rhessys jupyter notebook environment and rhessys workflows 3 4 ecohydrolib and rhessys model workflows the last components to evaluate gi designs with rhessys hydrological models within a gi rhessys jupyter notebook are to provide base model data required for the initial rhessys model setup not generated by the gi rhessys hydroterre data workflows and prepare the data for rhessys in fig 3 the dark green polygons indicate data sets provided by the gi rhessys hydroterre data workflows while the light green polygons designate the missing datasets elevation vegetation leaf area index soils and climate required by rhessys leaf area index lai is a primary ecosystem variable providing the surface area exchanging water carbon and energy with the atmosphere and needs to be assigned appropriate to vegetation type and structure within the gi rhessys jupyter notebooks the user can assign hydroshare resources to these missing datasets upload their own datasets or they can use ecohydrolib workflow scripts to retrieve data from federal agencies i e usda usgs nasa to use the ecohydrolib workflow scripts the user needs to define the bounding box or extent of the catchment that he or she is studying alternatively the user can specify a national hydrography datasets nhd streamflow gage to select upstream reaches to determine the catchment extent after the user has retrieved rhessys input datasets in the gi rhessys jupyter notebook she or he needs to prepare the rhessys model within the hydroshare compute environment rhessys uses a set of parameter files the worldfile describes the hierarchical structure of the watershed basin hillslope patch vegetation layer containing the full set of initial state variables and parameters terrain soils vegetation the flowtable stores a flow connectivity network linking patches within hillslopes to the stream network the rhessysworkflows python tool suite has been designed to simplify these steps required to build the model parameter files with three general categories the first category is to setup the rhessys environment by importing the rhessys source code via github rhessys 2017 and compiling the latest stable release of rhessys on the jupyter linux compute environment furthermore rhessys requires that all spatial data be stored in a geographic resources analysis support system grass neteler et al 2012 geographic information system gis mapset and this is achieved by creating a grass location using the digital elevation model dem the second category processes the watershed data using the grass mapset by importing and registering raster datasets i e landcover vegetation type and lai using ecohydrolib workflows due to the variety of climate data formats the user is responsible for supplying climate data either by uploading data using an existing hydroshare resource or using the north american land data assimilation system nldas nldas 2011 climate data supplied from the gi rhessys hydroterre data workflows l n leonard and band 2017 if the user has specified a streamflow gage the delineatewatershed tool delineates the watershed delineation is required so the watershed stream network be extracted and the watershed be hierarchically partitioned fig 3 illustrates hierarchy into hillslopes draining into each stream reach and ecosystem patches using the generatepatchmap tool miles and band 2017c after generating patches soil properties are assigned to each patch using the generatesoiltexturemap function the registerlandcoverreclassrules tool generates reclassification rules required to assign landcover properties to patches with the generatelandcovermaps software package miles and band 2017c the last category generates the rhessys input parameter files world and flowtable based on the grass gis mapset the first step is to use the generateworldtemplate package to create the initial state of the world the full domain both spatially and temporally within the watershed dynamics by assigning default values from data stored in the grass datasets next the createworldfile tool creates the world file using the template and fills out all required parameters for each hillslope and patch based on registered spatial data the createflowtable function creates a flow table to describe connectivity between the watershed patches and whether to include roads and rooftops optional with the watershed connectivity description the runlairead tool initializes vegetation carbon stores in the worldfile and the remaining step is to create a temporal event controller tec file to change rhessys land use parameters at certain times for example to simulate fire harvest urban and road developments and to control model output generation after finishing all these steps it is now possible to modify the parameter sets using the user s gi landscape design from the gi web application tool the next section focuses on the gi web application interface and user steps to create a gi landscape design 4 gi web application section 3 explained the back end components utilized by both the gi web application and gi rhessys jupyter notebook using multiple data and model workflows section 4 focuses on the gi web application by explaining the graphical user interface gui and visualization components for users to create gi designs that are processed by the back end workflows and to be used by gi rhessys jupyter notebooks section 4 1 summarizes the main components of the gi web application section 4 2 explains the gui layout and users steps to create gi designs section 4 3 explains the technical details of how the gi web application exchanges data with the gi rhessys jupyter notebook demonstrated in section 5 4 1 gi web application components the components to create gi features using the gi web application is summarized in fig 4 there are three key components to the gi web application the first component is the gi material dictionaries that allow users to add delete and edit gi features the second component uses these gi features in 2d and 2 5d visualizations and are used to assist users in placing gi and understand the visual impact of their landscape design the last element is gi tools that aid users to create or edit their own gi features share gi designs and estimate costs fig 4 section 3 blue dashed line explained the components for generating data required to evaluate gi designs using multiple workflows that are retrieved within hydroshares gi rhessys jupyter notebook interactive compute environment to initiate the gi rhessys hydroterre data workflows the gi web application is required to create the data structures shown in table 1 these data structures are retrieved sfrom the gi material dictionaries defined by users via the web interface table 2 summarizes the tree and surface gi type attributes stored in these material dictionaries attributes highlighted in green are identical to those in table 1 attributes highlighted in pink are used to estimate the costs for planting gi trees small medium or large with estimated labor costs and gi surface materials such as soil are calculated by volume depth is an attribute and area is calculated with gi geometry and estimated labor costs these costs are defined by the user and are expected to be location dependent attributes highlighted in yellow are for object descriptions with text and icons within the gi visualization tools the remaining attributes in table 2 are for creating unique keys within the material dictionaries for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article the remaining data type required in table 1 see section 3 is gi geometry there are two gi geometry types point and polygon point geometry represents the center positions of trees and shrubs as users add gi point materials to the map visualizations icons marked with sizes s small m medium l large are placed on the maps using latitude and longitude positions furthermore the tree attribute highlighted in orange is used to create circle geometry to represent the crown shape buffering is necessary to assign land use area changes polygon geometry is used to delineate surface gi types such as soils taller grasses lawn rain gardens and impervious surfaces to add delete and edit gi features users rely on the 2d and 2 5d visualizations to place gi in suitable locations google maps is the primary map for adding features at the map view as users add tree gi features billboards décoret et al 2003 that are called 2 5d geometry are placed within the google street view billboard sizes are dependent on the user s current tree size preference small medium large users are able to edit or delete gi features in either google visualization tool we note that at this time perspective visualization is limited to the area visible within google street view but ongoing work will incorporate point cloud generated views beyond the street view to provide additional datasets to aid users in placing gi for example terrain topography esri basemaps services are also available the remaining map visualization component is a gi rhessys hydroterre data workflow that provides national essential terrestrial variables etv and custom datasets from hydroterre the web mercator projection and coordinate system used by google maps differs from the hydroterre systems therefore gi features are re projected to albers conical equal area the remaining web application design categories are gi tools dedicated to enriching gi features and estimate costs to implement the user s gi landscape design as the user adds edits or removes gi features from the map views the gi inventory are coordinated with the gi spreadsheet view for example when the user replaces soil types in front of a home owner s property to retain and slow surface water movement the volume of material is calculated and the total cost of labor and materials is added to the gi spreadsheet all gi features such as tree sizes and material costs can be customized by the gi web application user to meet his or her local region differences for example material costs in city environments are often higher than outer urban environments another example is when labor costs are zero as the home owner is doing the project themselves or by volunteers the edit gi property tool enables web users to add their own gi features that meet the gi attributes described in table 2 once the user has finished her or his gi design the design can be saved to the user s desktop for sharing or to load back to the gi web application at another time otherwise users gi designs are not stored and are lost when users close the browser 4 2 using the web application to create gi landscape designs the gi web application available here web link 3 3 https hydroterremodels psu edu rhessys gi designer gi html as an independent service or here web link 4 4 https hydroterremodels psu edu rhessys gi designer gi hs html embedded within the gi rhessys jupyter notebook is shown in fig 5 the overarching strategy in designing the gi web application gui layout was to split the browser screen into two large panels to aid and simplify user creation of gi designs the left panel is designated to place gi in a 2d map environment and the right panel is dedicated to visualizing gi in a 2 5d environment with supporting gi tools to show how to create gi with the gi web application the dead run catchment located in baltimore county maryland usa is demonstrated using custom three meter high resolution datasets the watershed scale gi design has been prepared and uploaded as geodatabase layers further information about these designs is available from miles and band 2015a other locations within the conus are supported although at lower resolutions of 10 or 30 m constrained by national dataset properties unless additional local information is available we note that high resolution lidar terrain data and land cover are currently available or being developed by many municipalities and the chesapeake bay consortium has developed these data for the full chesapeake bay watershed http chesapeakeconservancy org in the dead run example the user specifies the dead run watershed location that is supported with a pre defined gi designed database by picking the location marked at fig 5a and the web visualizations will zoom to this location users can also use the google pegman fig 5b to change locations with all maps and the google streetview will zoom to the pegman location to pan hand icon the google map add gi point features balloon icon or create gi polygon surfaces shape icon the user picks from the toolbar located at fig 5c users specify gi properties such as size and depth at fig 5d that are assigned to gi materials chosen by the menu located at fig 5e for example to place a small honey locust tree the user clicks on small tree size fig 5d specifies the tree from the menu list fig 5e clicks on the shape icon fig 5c and the mouse cursor in the left panel changes from the hand icon to a cross icon now where the user clicks on the map left panel a tree icon marked with s for small is placed on the map fig 5f depending on the camera view in google streetview right panel the tree billboard will appear immediately fig 5g otherwise the user can rotate the camera view to see the tree in a perspective view to create surface gi users follow similar steps by specifying feature soil depth fig 5d selecting the shape icon fig 5c and clicking on the map multiple times to define the polygon vertices fig 5h however surface gi are not added to the google streetview due to the limitation of depth with the camera perspective and object occlusion as the user adds gi features quantities with estimated costs are automatically added to the gi spreadsheet located in fig 5i and shown in fig 6 a for users to modify the predefined gi properties such as costs that are kept in the gi dictionaries as discussed in section 3 1 the edit gi properties tool located in fig 5j and displayed in fig 6b is available to modify the existing gi or to create new custom gi to share existing and custom gi properties with other users or to save gi designs to user s local desktop for future use the save or load gi tool located in fig 5k and presented in fig 6c is offered to users once users have finished their gi landscape design the gi rhessys hydroterre workflows described in section 3 creates the gi datasets when users interact with the tools found by clicking the right panel tab at fig 5l the next section focuses on how the gi web application connects these gi datasets with the gi rhessys jupyter notebook 4 3 connecting gi designs with the gi rhessys jupyter notebook environment once the user has completed a gi landscape design he or she can download the resultant data bundles from gi rhessys hydroterre data workflows to their desktop by clicking on the download buttons shown in fig 6d within the gi rhessys jupyter notebook environment the user has the same capability however the process is cumbersome recall the hydroshare gi rhessys jupyter notebook environment is a web application providing access to a remote linux cloud compute environment when users click on the download buttons the browser will direct the zip file to the user s own compute environment not the remote linux environment hence users would need to upload the zip files to the hydroshare gi rhessys jupyter notebook environment requiring not only extra steps but be comfortable with the linux compute environment to avoid these extra steps browser window event listeners were added to the gi rhessys jupyter notebook to receive messages from the gi web application summarized in table 3 there are three general groups to post messages from the gi web application the first group contains the gi material dictionaries with default and any user custom gi as described in section 4 1 accordingly the gi attributes table 2 can be analyzed within the gi rhessys jupyter notebook the second group provides access to the gi geometry designed by the user both the first and second groups enable access to gi data useful for models that do not use spatial gis datasets for example the epa s swmm uses routing through a system of digitized drainage lines or pipes gironas et al 2010 and not derived from the elevation model that models such as rhessys use research by heidari et al 2018 use these gi javascript object notation json data structures with swmm and jupyter notebooks to analyze trees and rain gardens described elsewhere the last group provides uniform resource locator url locations of the workflow results by using wget free software foundation 2015 the data bundle results are downloaded within the gi rhessys jupyter notebook environment section 5 demonstrates these gi capabilities by executing a rhessys model at dead run watershed with the gi rhessys jupyter notebook compute environment 5 demonstration of using landscape designs within gi rhessys jupyter notebook the previous sections have discussed the gi web application for users to develop gi landscape designs and incorporate these designs with datasets required by rhessys using multiple workflows this section focuses on demonstrating the gi web application within the gi rhessys jupyter notebook compute environment to develop a rhessys model using the gi landscape design at dead run watershed from section 4 section 5 1 provides an overview of how users access the gi rhessys jupyter notebook and the last section demonstrates using the gi rhessys jupyter notebook to develop a rhessys hydrological model at dead run watershed 5 1 accessing gi rhessys jupyter notebooks there are two ways to access gi rhessys jupyter notebooks dependent on the user s confidence with computational environments the first method is to download the notebooks directly from hydroshare s github repository web link 5 5 https github com hydroshare hydroshare jupyterhub tree master notebooks where official notebooks supported by hydroshare are located with dependencies i e libraries additionally users can access the latest developer versions of gi rhessys jupyter notebooks using github from here l leonard and band 2017 the second method is to visit the hydroshare website at https www hydroshare org users are required to have an account with hydroshare to use these resources sign up is free once the user has logged into hydroshare click on discover at the top menu bar a keyword search of rhessys will return a list of resources available from hydroshare find and click on the resource titled green infrastructure designer with rhessys workflow a web page document appears describing details when the resource was created and last updated an abstract subject keywords how to cite the resource and a zipped bagit archive boyko et al 2014 containing the gi rhessys jupyter notebook users can download the gi rhessys jupyter notebook to their desktop using the zipped bagit archive if desired the main advantage of using the second method to access the gi rhessys jupyter notebook is the ability to use the open with button located top right of the web page document to open the notebook depending on the user s credentials the open with button will provide a list of different compute environments to use the gi rhessys jupyter notebook click on a jupyterhub assuming the user has valid credentials a new web page is loaded displaying a jupyter notebook within the jupyterhub compute environment a welcome page appears with instructions on how to open and use notebooks for users that downloaded gi rhessys jupyter notebooks from github or used the zipped bagit method it is feasible to upload jupyter notebooks within hydroshare s jupyterhub environment manually by using the jupyter tree click on jupyter icon to upload files in the users compute workspace users using their own jupyterhub environment will require dependencies such as the ecohydrolib and rhessys workflows python tools section 3 be installed instructions for installation are here miles and band 2017c the hydroshare jupyterhub compute environment has all the rhessys dependencies pre installed for all users the next section discusses how to open and use the gi rhessys jupyter notebook 5 2 using a jupyter notebook to create a rhessys model with gi designs the supplementary material provides the steps in opening and using the gi rhessys jupyter notebook to create gi landscape designs and a rhessys model briefly the user creates a gi landscape design fig 7 steps 1 3 using the gi web application explained in sections 4 2 and 4 3 next the user is ready to evaluate their gi landscape design in the gi rhessys jupyter notebook environment users need to import the echohydrolib and rhessys workflows section 3 4 into the gi rhessys jupyter notebook fig 7 step 4 and prepare the gi data fig 7 step 5 for the rhessys model assuming there were no errors reported the rhessys model is ready to be executed in the gi rhessys jupyter notebook compute environment as shown in fig 7 step 6 there is documentation on what the command options are for the runmodel tool that prepares and executes the rhessys model fig 7a the rhessys model using the jupyterhub compute environment took 45 min to finish the main reason the model compute time took a long time for a short simulation period the default value of 6 months defined in the rhessysworkflow python object which users can adjust is the high resolution three meter datasets used for defining the watershed properties and the resources allocated at the cloud site the last gi rhessys jupyter notebook code cell generates visualizations for rhessys variables as shown in fig 7b after executing all the notebook steps the user has generated a gi landscape design prepared data for the watershed location and created a rhessys hydrological model next steps for using the gi rhessys jupyter notebook include the development of workflows for calibrating the model parameters fig 7c with the runmodel tool trying different datasets from local or federal agencies or datasets from other hydroshare resources the gi rhessys jupyter notebook is easy to edit or clone for other research goals and a collection of different rhessys jupyter notebooks are available here l leonard and band 2017 with different data inputs and user skill level requirements after users have evaluated the gi rhessys jupyter notebook for the first time the learning curve to compare gi designs has been significantly reduced the authors encourage users to focus on creating ideal gi landscape designs with quantitative analysis with the rhessys ecohydrological model and share their data and model results within the hydroshare community as resources and public outreach 6 conclusion the provision and improvement of ecosystem services in urban areas depends on sound designs that integrate an understanding and forecast of the interactions of hydrologic ecological climate and social processes successful approaches to environmental restoration of the built environment are also increasingly seen as requiring some level of participation and input from residents and other stakeholders community participation in the use of advanced environmental models used to assess the benefits of different urban restoration designs requires both visualization and access or input to computational systems that are usually not available to non specialists our approach demonstrates how gi web applications can rapidly visualize gi landscape designs at the site scale for participatory planning between stormwater engineers and property owners in urban environments with pre defined and custom materials and automate the translation of user developed landscape designs to parameterize complex environmental models we demonstrated these capabilities at the dead run watershed in baltimore maryland usa a study site of the baltimore long term ecological research site http beslter org combining gi landscape designs at site locations with watershed scale designs user gi landscape designs are then transformed and integrated with custom and national datasets using gi rhessys hydroterre data workflows the data workflows generate data bundles and json data structures for hydrological modeling next we demonstrated the gi web application embedded inside a gi rhessys jupyter notebook hosted at hydroshare s cloud compute environment that consumes these data structures to create a rhessys ecohydrological model with gi landscape designs the documented rhessys jupyter notebooks serve as templates for beginner users to quickly learn how to prepare model inputs using ecohydrolib and rhessys workflows furthermore these notebooks are easy to clone and edit and the embedded workflows make the process for both beginner and expert users to rapidly create ecohydrological models at other watershed locations straightforward by using cloud compute resources to generate the rhessys ecohydrological models 7 future directions the next step with this research is to improve the visualization and data acquisition capabilities of the gi web application this includes workflows for users to upload sewer infrastructure computer aided designs and gi planning designs improving the gi material dictionaries with three dimensional trees and integrating google streetviews with lidar light detection and ranging captured topography and street infrastructure to create photo realistic gi landscape designs combined with simulated dynamics i e vegetation growth and extreme events from climate and hydrological model results furthermore new workflows and tools are necessary to validate gi landscape designs such as placing trees in suitable locations and quantifying whether the gi landscape designs meet local and state regulations using automation and hydro informatics and placement relative to utilities and other infrastructure we see a need from local agencies in multiple cities to use these web applications and services to interactively evaluate gi landscape designs with the public and other professions for immediate feedback on gi policies and implementations however spatially distributed and continuous time models like rhessys are computationally expensive and time consuming our vision is that the rhessys jupyter notebooks using compute resources such as hydroshare s jupyterhub will serve as templates and resources for agencies to access model results similar to a reference library as the data and models improve agencies access the latest improved model results with services such as the gi web application to evaluate proposed gi landscape designs and policies that are shared with other agencies and the community credits a number of scientists engineers and other professionals associated with the baltimore ecosystem study and with stormwater utilities in baltimore city and county portland or durham nc chicago and phoenix provided important feedback on conceptual design and initial versions of this software the research and development were a synthesis of information and methods developed by a set of nsf grants award no 1331813 collaborative research cybersees type 2 a new framework for crowd sourced green infrastructure design nsf cise awards 1148453 and 1148090 hydroshare nsf office of cyberinfrastructure award deb 1027188 long term ecological research program baltimore ecosystem study award dbi 1639145 socioeconomic synthesis center support for water science software institute award no 1239678 eager collaborative research interoperability testbed assessing a layered architecture for integration of existing capabilities award no 0940841 datanet federation consortium award no 1148090 collaborative research si2 ssi an interactive software infrastructure for sustaining collaborative innovation in the hydrologic sciences appendix a supplementary data the following is the supplementary data to this article gi supplementary docx gi supplementary docx appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 003 
26286,land use planners landscape architects and water resource managers are using green infrastructure gi designs in urban environments to promote ecosystem services including mitigation of storm water flooding and water quality degradation an expanded set of urban sustainability goals also includes increasing carbon sequestration songbird habitat reducing urban heat island effects and improvement of landscape aesthetics gi is conceptualized to improve water and ecosystem quality by reducing storm water runoff at the source but when properly designed may also benefit these expanded goals with the increasing use of gi in urban contexts there is an emerging need to facilitate participatory design and scenario evaluation to enable better communication between gi designers and groups impacted by these designs major barriers to this type of public participation is the complexity of both parameterizing operating visualizing and interpreting results of complex ecohydrological models at various watershed scales that are sufficient to address diverse ecosystem service goals this paper demonstrates a set of workflows to facilitate rapid and repeatable creation of gi landscape designs which are incorporated into complex models using web applications and services for this project we use the rhessys regional hydro ecologic simulation system ecohydrologic model to evaluate participatory gi landscape designs generated by stakeholders and decision makers but note that the workflow could be adapted to a set of other watershed models graphical abstract image 1 software availability name gi designer rhessys notebooks software required internet browser with html 5 support later versions are recommended program languages python javascript html c availability and cost the notebooks are available from github https github com leonard psu rhessys jupyter notebooks to use in any jupyter compute environment to use the jupyter notebooks with hydroshare an account is required that is free the notebooks are available by using hydroshare s discover search engine the green infrastructure gi notebook presented in this manuscript is available here leonard l l band l lin b miles 2018 green infrastructure designer with rhessys workflow hydroshare https doi org 10 4211 hs 3f7680cf83dc426e858d5b48cb95a565 the gi web application is here https hydroterremodels psu edu rhessys gi designer gi html ecohydrolib is available from https github com selimnairb ecohydrolib and rhessysworkflows from https github com selimnairb rhessysworkflows 1 introduction a growing paradigm in urban environmental management is the adaptation and enhancement of ecosystem services with green infrastructure gi to mitigate the effects of urbanization on stormwater flooding and water quality degradation urban heat islands air quality and other adverse impacts as hydrological and ecosystem models evolve to explicitly represent the influence of fine scale landscape form on the cycling and export of water carbon and nutrients public perceptions and preferences need to be included in the management process by which water resource managers and landscape planners design and implement new urban design and infrastructure often the public is not contemplating the important interaction between the water cycle and ecosystem processes with the design form architecture and engineered and management of their own property or neighborhood therefore environmental modeling software should be designed to address the strongly integrated set of ecosystem services affected by urban form and infrastructure and engage individuals and communities about their own role in improving water quality and mitigating extreme events at the parcel to watershed scales a major management barrier is the complexity of installing compiling and consuming datasets with watershed models using sufficient spatial resolution and process representation of the fine scale ecohydrological interactions of landscape design and structure the conventional process by many hydrological modelers is to prepare data as an offline process from multiple sources copy the processed datasets to the compute environment for modeling then process the watershed model results offline with calibration and visualization these processes are cumbersome and make it difficult for other professionals and the public to participate in improving the modeling process and critiquing watershed model results the research presented in this manuscript is a step towards improving these processes by using workflows to make data and model processes more efficient to evaluate watershed model results and engage other professionals and the public with gi landscape designs to evaluate their impact to ecohydrological interactions the premise of this paper has two components conceptualized to facilitate participatory design and evaluation of gi planning the participatory design assumes that stormwater engineers community groups and potentially individual residents would collaborate in the design of preferred gi with rapid evaluation of different design effectiveness for reaching stormwater restoration goals the first component is to engage and connect storm water engineers and property owners with rapid charrettes of applying gi landscape designs on their property using web based visualizations then connecting the gi design with agency regional plans as an integrated system within the local watershed to be used as inputs for ecohydrological modeling to rapidly evaluate the design efficacy of mitigating impacts in both the built and natural environments with the potential to iterate this process to balance desirable and effective modification of the landscape utilizing gi here we demonstrate using a spatially distributed ecohydrological model the regional hydro ecologic simulation system tague and band 2004 rhessys although the gi data workflows serve as a template for other models to adapt and consume the second component is to simplify and improve the modeling setup to consume fine resolution land information and gi in rhessys models by using multiple workflows and jupyter notebooks to share data code simulations and visualizations some of these workflows were provided by the hydroterre expert system to access essential terrestrial variables from national datasets within the united states of america leonard and duffy 2016a 2014 2013 we demonstrate using the gi web application within a jupyter notebook to setup a rhessys model at the dead run watershed located in baltimore maryland usa the rhessys notebooks serve as a prototype for beginner and expert users to modify their region of interest and data sources these notebooks are available to the community within the hydroshare collaborative environment cuahsi 2017 d g tarboton et al 2014 and as resources via github for other jupyter hosted environments the following are our primary research contributions we developed a new gi web application to allow web users to design gi with a plan and street view perspective users can assign custom gi attributes for their modeling requirements and estimate costs to share with other professionals and the public new gi model data workflows gi rhessys hydroterre were created to process the gi landscape designs from the gi web application into fine spatial resolution model parameter files these new workflows integrate the rhessys ecohydrological model and the hydroterre expert system to integrate gi data bundles with local custom spatial datasets e g terrain soils land cover supplied by the user or existing hydroterre national spatial data united states we demonstrate using a new gi rhessys jupyter notebook that uses the gi web application to create gi landscape designs these designs are processed using the gi rhessys hydroterre data workflows and then processed within the gi rhessys jupyter notebook the gi rhessys jupyter notebook uses the existing ecohydrolib and rhessys workflows to prepare data and execute the rhessys ecohydrological model using the user s gi landscape designs these notebooks serve to facilitate participatory use and evaluation of gi planning this article is structured with section 2 summarizing background information and the inspiration to combine gi with rhessys and workflows using a jupyter notebook section 3 discusses the back end web application design using multiple workflows section 4 describes and demonstrates the gi web application for modelers to create gi landscape designs finally section 5 demonstrates using the web application within jupyter notebooks to develop a new rhessys ecohydrological model 2 background 2 1 the role and requirements for green infrastructure gi the motivation for gi in urban areas is to manage and protect the water cycle without relying solely on conventional engineered solutions i e dams levees detention ponds here we operationally define gi to include various forms of soil vegetation and water storage treatments including tree canopy rain gardens bioswales permeable pavement that promote processes mimicking natural infiltration transpiration subsurface storage transport and processing of reduce and slow precipitation runoff land use planners landscape architects and water resource managers are using gi solutions to reduce storm water flow from impervious surfaces e g roofs roads parking lots in urbanized watersheds mitigate downstream water quality problems and urban heat islands while creating amenities for residents jefferson et al 2017 miles and band 2015a miles 2014 poff et al 1997 walsh et al 2005 to treat non point source storm water in older established urbanized areas i e baltimore maryland usa federal state and municipal agencies are working with residential and commercial owners to retrofit impervious surfaces for example rather than have roofs drain rainwater directly through gutters into roads and storm sewers the rainwater is stored using rain barrels and tanks to be slowly released between storms or routed to infiltrate into the ground using pervious surfaces including rain gardens bioswales and wetlands before the water is directed to roads and storm sewers many municipalities have also set goals to increase tree canopy cover to promote multiple ecosystem services pincetl et al 2013 in the examples used in this paper we emphasize landscape designed gi e g swale drains rain gardens and increased tree canopy coverage the goals of federal and local agencies are to improve water quality and reduce costs from flooding events and large engineering projects by mimicking the pre development hydrology by treating storm water runoff close to or at the origin united states environmental protection agency 2017 common goals in stormwater treatment include reduction in peak flows increased baseflow and reduction in nutrient and sediment loading therefore these agencies are interested in gi landscape design solutions that will address coupled ecosystem and hydrologic cycling ranging from the building site to the watershed scale meeting regulations and policies from a variety of government and non government organizations additional sustainability goals expressed by municipalities include a range of other ecosystem services benefiting from increased tree cover to better understand how successful gi landscape design solutions can be at reducing storm water runoff at a watershed scale potential designs need to be integrated so their combined effects can be evaluated doing so suggest the use of a distributed ecohydrologic model that couples water carbon and nutrient cycling along hydrologic flowpaths to scale up gi designs at the residential scale datasets with resolutions on the order of meters to target watersheds up to 10 square kilometers to evaluate the impact of reducing storm water overland flow and improve ecohydrology 2 2 rhessys and workflows for this project we used the regional hydroecological simulation system rhessys tague and band 2004 to evaluate gi landscape designs for three major reasons first rhessys has been developed to simulate hydrological and ecological processes at patches 1 10 m resolution hillslopes 10 100 m resolution watershed and basin scales it integrates the above ground canopy processes e g evapotranspiration and net canopy photosynthesis and below ground biological processes e g root function litter and soil organic matter decomposition nitrification and denitrification running and coughlan 1988 soil water distribution and routing beven and kirby 1979 wigmosta et al 1994 and topo climate extrapolation running et al 1987 the explicitly modeled ecohydrological interaction between vegetation and water provides the basis for landscape design in gi while rhessys was initially developed for research and prediction in unmanaged and rural watersheds it has been adapted for the study and forecast of ecohydrology of managed and urban watersheds bell et al 2017 martin et al 2017 mittman et al 2012 second rhessys simulates spatially distributed and coupled water carbon and nitrogen dynamics using a landscape hierarchical structure over nested patch hillslope and watershed scales this feature makes it possible to evaluate and monitor gi performance at fine spatial scale and their cumulative and emergent effect at the watershed landscape third instead of lumping at coarse spatial scale as other watershed models e g soil and water assessment tool swat hydrological simulation program fortran hspf water and solutes are spatially distributed within hillslopes and watersheds in rhessys this feature allows for simulating the distribution routing of rain storm water from impervious surfaces e g roofs and roads and the cycling of carbon and nutrients into designed gi in the landscape rhessys and other hydrological models i e pihm swat vic require modelers to prepare i e re project and transform i e generate model input files a diverse range of geospatial datasets from multiple agencies i e usgs noaa nasa epa usda or from local sources these processes are time consuming require a broad range of expertise and are often difficult to reproduce given the prospect of petabyte datasets automation is essential to process the diverse range of datasets for hydrological modeling leonard and duffy 2013 to prepare rhessys models that incorporate gi designs the ecohydrolib miles and band 2017a 2015b and rhessys workflows miles and band 2017b simplify the process of generating rhessys input from federal agencies or local datasets and generating model parameter files furthermore these python tool suites enable modelers to incorporate their own custom and often high resolution datasets to encourage outreach provenance and reproducibility this research incorporates rhessys and workflows using an interactive jupyter notebook computing environment with cloud resources for modelers to evaluate gi designs 2 3 hydroshare s interactive jupyter notebook computing environment hydroshare https www hydroshare org is an online collaborative infrastructure to share hydrologic data model and applications crawley et al 2017 horsburgh et al 2015 morsy et al 2017 d tarboton et al 2014 d g tarboton et al 2014 hydroshare manages data and model files as fully referenced resources which can be searched and cited a resource is defined as a set of digital content that includes the science data files or model files and their corresponding metadata information dcmi 2018 hydroshare 2018 oai 2018 hydroshare enables users to share hydrological models by using the jupyter http jupyter org notebook application a server client application that allows web users to edit and run notebook documents ipython within a web browser on a local or remote compute environment jupyter team 2017a b perez and granger 2007 this paper demonstrates using jupyter notebooks to prepare a rhessys hydrological model with gi landscape designs supported with data and model workflows using hydroshare s jupyter cloud compute environment supported by consortium of universities for the advancement of hydrologic science inc cuashi jupyter notebook documents are ideal to introduce new modelers other professionals i e landscape planners and stakeholders to all the steps with documentation and samples included in a single document in this case we use the jupyter notebook to prepare and execute a rhessys hydrological model on a remote cloud compute environment and include design and visualization of new gi we have developed a collection of rhessys jupyter notebooks prepared for different user skill levels i e beginner to expert and data requirements i e gage location based custom datasets and with national datasets that allows users to go through all the steps the notebooks can be accessed from github l leonard and band 2017 or by searching for rhessys resources via hydroshare users interested in learning how to use rhessys do not need to install rhessys and dependencies on their own compute environment using the hydroshare cloud environment rhessys notebooks are ready to be executed to create an ecohydrological model hence new users spend more time on learning rhessys capabilities while experienced modelers copy and edit an existing notebook for their study location to devote more time on analysis and refinement of model results furthermore after calibrating and refining ecohydrological model results the jupyter notebook is easy to share with other users by downloading the notebook in different file formats or by creating a hydroshare resource to share with the community for critique for example residents stakeholders and other professionals i e landscape architects can critique the gi design and evaluate ecohydrological performance in a collaborative web application before requesting gi design changes to meet the decision makers objectives examples of hydrologic research using hydroshare s jupyter notebook infrastructure include taudem with a notebook that processes terrain datasets castronova 2017 and an introduction to landlab to build numerical landscape models bandaragoda 2016 bandaragoda et al 2017 furthermore work by heidari et al 2018 is using the storm water management model swmm to study trees and rain gardens at the dead run watershed with the gi web application described in this article heidari et al 2018 outside of hydroshare water related research using jupyter notebooks include the evaluation of analytics solutions for steady interface flow where the aquifer extends below the sea bakker et al 2017 and end to end workflows for assessing sea surface temperature salinity and water levels predicted by coastal ocean models subramanian et al 2015 2 4 constraints incorporation of vegetation biodiversity e g tree species at fine spatial scale has been challenging for watershed models including rhessys which requires detailed information regarding vegetation species or plant functional type pft composition and density at high spatial resolution this type of information is difficult to acquire even with satellite imagery in current rhessys applications classifying vegetation as grass deciduous and evergreen is a common practice efforts have been made to further incorporate more diverse vegetation information in rhessys based on pft water use traits lin et al in review this feature will allow rhessys to better simulate gi that are customized with vegetation species and management purposes appropriate for different climates and designs pataki et al 2013 additionally this article focuses on the software development to create gi designs using web applications and workflows that are embedded within cloud distributed compute environments using jupyter notebooks that are easy to edit and share with the water resource and ecosystem management community as new data and model resources in this presentation we do not validate data inputs for example the appropriateness of placing gi by users or inputs such as sewer infrastructure however spatial restrictions in gi placement can be spatially represented within the core gis layers furthermore we do not demonstrate calibrating and validating gi rhessys model results used in the jupyter notebooks instead we leave these issues as an exercise for notebook modeler users or assume a previously developed and calibrated model in the case where users will design and test new gi 3 gi back end workflows this section describes the software architecture building blocks supporting the gi web application using multiple workflows required by the gi rhessys jupyter notebook to evaluate gi landscape designs with rhessys the gi web application consumes multiple software and data services section 3 1 to enable interactive hydrological modeling compute environments how the gi design web application is embedded into hydroshare s jupyter interactive environment is described in section 3 2 followed by how users gi designs are transformed by the gi rhessys hydroterre data workflows section 3 3 for rhessys model workflows and hydrological modeling section 3 4 to evaluate gi landscape designs hydrological impact 3 1 overview of main components there are three main components to use gi landscape designs within the hydroshare jupyter environment with rhessys as summarized in fig 1 the first component is the gi web applications currently hosted at penn state university psu the gi web application provides tools for users to design gi in both two dimension 2d and 2 5 dimension 2 5d environments the second component is hydroterre s services http hydroterre psu edu hosted at psu to place users gi designs within both national and custom datasets referred to as gi rhessys hydroterre data workflows in this manuscript to explain data origin the last component is hydroshare s jupyter compute environment supported by consortium of universities for the advancement of hydrologic science inc cuashi cuahsi 2017 in this manuscript we refer to these workflows both data and model and compute environment as gi rhessys jupyter notebook there are two gi web application versions the first is a standalone application for users to download gi rhessys hydroterre data bundles for their modeling goals using a personal desktop computer this version is available from psu at web link 1 1 https hydroterremodels psu edu rhessys gi designer gi html users create a gi design supported by map and data services from google maps google 2017 esri esri 2017 and hydroterre leonard and duffy 2016b 2013 2014 when the user is ready to evaluate their gi design the design is transformed by hydroterre data workflows to create gi data bundles these data bundles can be downloaded for offline use the second version of the gi web application is an embedded web application for cloud based modeling using jupyter notebooks web link 2 2 https hydroterremodels psu edu rhessys gi designer gi hs html users interact with the gi web application as she or he would with browser software i e chrome firefox etc inside the notebook the web user follows the same steps as he or she would with the standalone application however rather than downloading the data bundle for personal desktop use the gi data bundle is accessed by a gi rhessys jupyter notebook to create a rhessys hydrological model using cloud resources 3 2 hydroshare jupyter interactive compute environment one method within hydroshare to share data and models is to store jupyter notebooks within a resource jupyter notebooks are documents produced by the jupyter notebook application containing computer source code e g r python javascript and html and rich text elements e g tables figures that are executable documents for hydrological data and model analysis jupyter team 2017a b for example the gi rhessys jupyter notebook is available from hydroshare as a resource here leonard et al 2017 fig 2 summarizes how the gi web application interacts with the jupyter notebook environment when users open the gi notebook via hydroshare the user is accessing a cloud based compute environment dedicated compute nodes use openstack 2017 an open source cloud computing framework with jupyterhub jupyter team 2017a b to launch jupyter notebooks the purpose of jupyterhub is to manage authentication and spawn single user servers on demand for many users on a compute environment jupyter team 2017a b each user gets a complete jupyter notebook server to execute their data and modeling processes effectively the user has their own interactive linux based compute environment instance for hydrological modeling using a web browser embedded inside the gi rhessys jupyter notebook maintained by cuashi the gi web application hosted at psu is accessible using an html iframe object and javascript code the user creates gi designs as explained in sections 4 and 5 within the embedded gi web application using representational state transfer restful web services the gi design is processed using gi rhessys hydroterre workflows at psu section 3 3 with either national or custom data products html web links pointing to the location of the gi rhessys hydroterre data bundles are shared with the jupyter notebook within the gi rhessys jupyter notebook the python based ecohydrolib miles and band 2017a and rhessys workflows miles and band 2017b consume the gi data bundles data from federal agencies i e usgs and usda and with custom data as hydroshare resources i e dead run to generate a rhessys hydrological model using the user s gi landscape design generating a rhessys hydrological model within the jupyter notebook is explained in section 3 4 3 3 gi rhessys hydroterre data workflows once the user is ready to evaluate their gi landscape design using the gi rhessys jupyter notebook the design geometry and rhessys attributes are sent using restful services to the gi rhessys hydroterre data workflows within hydroterre compute resources the gi geometry point and polygon is converted to hydroterre s projection coordinate system albers conical equal area table 1 summarizes the input variables required to execute the gi rhessys hydroterre data workflows there are two gi rhessys hydroterre data workflows that transform the user s gi design into new datasets within the watershed boundary extent fig 2 the first workflow modifies land use datasets delineated by the user with polygon boundaries within the gi web application users assign pre defined rhessys land use classifications i e vegetation type and surface imperviousness to the land use geometry the main steps within the gi rhessys hydroterre land use data workflow include 1 converting gi design polygon geometry to a raster dataset 2 assign user defined land use values to the new raster dataset 3 merge the designed raster dataset with either custom land cover datasets or with a watershed scale clipped national landcover dataset the second gi rhessys hydroterre data workflow adjusts soil types outlined by the user with polygon shapes within the gi web application users assign nine rhessys soil properties table 1 to the soil geometry the main steps within the gi rhessys hydroterre soil data workflow include 1 erase gi design polygon geometry from custom soil dataset or watershed scale clipped national soil datasets i e ssurgo 2011 statsgo 2011 2 merge the erased dataset with the user defined soil dataset with both the land use and soil workflows the resultant datasets are zipped and a message is sent to the gi web application using a html callback indicating where the data bundle zip files are located these messages are kept as global variables within the gi web application for access by the gi rhessys jupyter notebook environment and rhessys workflows 3 4 ecohydrolib and rhessys model workflows the last components to evaluate gi designs with rhessys hydrological models within a gi rhessys jupyter notebook are to provide base model data required for the initial rhessys model setup not generated by the gi rhessys hydroterre data workflows and prepare the data for rhessys in fig 3 the dark green polygons indicate data sets provided by the gi rhessys hydroterre data workflows while the light green polygons designate the missing datasets elevation vegetation leaf area index soils and climate required by rhessys leaf area index lai is a primary ecosystem variable providing the surface area exchanging water carbon and energy with the atmosphere and needs to be assigned appropriate to vegetation type and structure within the gi rhessys jupyter notebooks the user can assign hydroshare resources to these missing datasets upload their own datasets or they can use ecohydrolib workflow scripts to retrieve data from federal agencies i e usda usgs nasa to use the ecohydrolib workflow scripts the user needs to define the bounding box or extent of the catchment that he or she is studying alternatively the user can specify a national hydrography datasets nhd streamflow gage to select upstream reaches to determine the catchment extent after the user has retrieved rhessys input datasets in the gi rhessys jupyter notebook she or he needs to prepare the rhessys model within the hydroshare compute environment rhessys uses a set of parameter files the worldfile describes the hierarchical structure of the watershed basin hillslope patch vegetation layer containing the full set of initial state variables and parameters terrain soils vegetation the flowtable stores a flow connectivity network linking patches within hillslopes to the stream network the rhessysworkflows python tool suite has been designed to simplify these steps required to build the model parameter files with three general categories the first category is to setup the rhessys environment by importing the rhessys source code via github rhessys 2017 and compiling the latest stable release of rhessys on the jupyter linux compute environment furthermore rhessys requires that all spatial data be stored in a geographic resources analysis support system grass neteler et al 2012 geographic information system gis mapset and this is achieved by creating a grass location using the digital elevation model dem the second category processes the watershed data using the grass mapset by importing and registering raster datasets i e landcover vegetation type and lai using ecohydrolib workflows due to the variety of climate data formats the user is responsible for supplying climate data either by uploading data using an existing hydroshare resource or using the north american land data assimilation system nldas nldas 2011 climate data supplied from the gi rhessys hydroterre data workflows l n leonard and band 2017 if the user has specified a streamflow gage the delineatewatershed tool delineates the watershed delineation is required so the watershed stream network be extracted and the watershed be hierarchically partitioned fig 3 illustrates hierarchy into hillslopes draining into each stream reach and ecosystem patches using the generatepatchmap tool miles and band 2017c after generating patches soil properties are assigned to each patch using the generatesoiltexturemap function the registerlandcoverreclassrules tool generates reclassification rules required to assign landcover properties to patches with the generatelandcovermaps software package miles and band 2017c the last category generates the rhessys input parameter files world and flowtable based on the grass gis mapset the first step is to use the generateworldtemplate package to create the initial state of the world the full domain both spatially and temporally within the watershed dynamics by assigning default values from data stored in the grass datasets next the createworldfile tool creates the world file using the template and fills out all required parameters for each hillslope and patch based on registered spatial data the createflowtable function creates a flow table to describe connectivity between the watershed patches and whether to include roads and rooftops optional with the watershed connectivity description the runlairead tool initializes vegetation carbon stores in the worldfile and the remaining step is to create a temporal event controller tec file to change rhessys land use parameters at certain times for example to simulate fire harvest urban and road developments and to control model output generation after finishing all these steps it is now possible to modify the parameter sets using the user s gi landscape design from the gi web application tool the next section focuses on the gi web application interface and user steps to create a gi landscape design 4 gi web application section 3 explained the back end components utilized by both the gi web application and gi rhessys jupyter notebook using multiple data and model workflows section 4 focuses on the gi web application by explaining the graphical user interface gui and visualization components for users to create gi designs that are processed by the back end workflows and to be used by gi rhessys jupyter notebooks section 4 1 summarizes the main components of the gi web application section 4 2 explains the gui layout and users steps to create gi designs section 4 3 explains the technical details of how the gi web application exchanges data with the gi rhessys jupyter notebook demonstrated in section 5 4 1 gi web application components the components to create gi features using the gi web application is summarized in fig 4 there are three key components to the gi web application the first component is the gi material dictionaries that allow users to add delete and edit gi features the second component uses these gi features in 2d and 2 5d visualizations and are used to assist users in placing gi and understand the visual impact of their landscape design the last element is gi tools that aid users to create or edit their own gi features share gi designs and estimate costs fig 4 section 3 blue dashed line explained the components for generating data required to evaluate gi designs using multiple workflows that are retrieved within hydroshares gi rhessys jupyter notebook interactive compute environment to initiate the gi rhessys hydroterre data workflows the gi web application is required to create the data structures shown in table 1 these data structures are retrieved sfrom the gi material dictionaries defined by users via the web interface table 2 summarizes the tree and surface gi type attributes stored in these material dictionaries attributes highlighted in green are identical to those in table 1 attributes highlighted in pink are used to estimate the costs for planting gi trees small medium or large with estimated labor costs and gi surface materials such as soil are calculated by volume depth is an attribute and area is calculated with gi geometry and estimated labor costs these costs are defined by the user and are expected to be location dependent attributes highlighted in yellow are for object descriptions with text and icons within the gi visualization tools the remaining attributes in table 2 are for creating unique keys within the material dictionaries for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article the remaining data type required in table 1 see section 3 is gi geometry there are two gi geometry types point and polygon point geometry represents the center positions of trees and shrubs as users add gi point materials to the map visualizations icons marked with sizes s small m medium l large are placed on the maps using latitude and longitude positions furthermore the tree attribute highlighted in orange is used to create circle geometry to represent the crown shape buffering is necessary to assign land use area changes polygon geometry is used to delineate surface gi types such as soils taller grasses lawn rain gardens and impervious surfaces to add delete and edit gi features users rely on the 2d and 2 5d visualizations to place gi in suitable locations google maps is the primary map for adding features at the map view as users add tree gi features billboards décoret et al 2003 that are called 2 5d geometry are placed within the google street view billboard sizes are dependent on the user s current tree size preference small medium large users are able to edit or delete gi features in either google visualization tool we note that at this time perspective visualization is limited to the area visible within google street view but ongoing work will incorporate point cloud generated views beyond the street view to provide additional datasets to aid users in placing gi for example terrain topography esri basemaps services are also available the remaining map visualization component is a gi rhessys hydroterre data workflow that provides national essential terrestrial variables etv and custom datasets from hydroterre the web mercator projection and coordinate system used by google maps differs from the hydroterre systems therefore gi features are re projected to albers conical equal area the remaining web application design categories are gi tools dedicated to enriching gi features and estimate costs to implement the user s gi landscape design as the user adds edits or removes gi features from the map views the gi inventory are coordinated with the gi spreadsheet view for example when the user replaces soil types in front of a home owner s property to retain and slow surface water movement the volume of material is calculated and the total cost of labor and materials is added to the gi spreadsheet all gi features such as tree sizes and material costs can be customized by the gi web application user to meet his or her local region differences for example material costs in city environments are often higher than outer urban environments another example is when labor costs are zero as the home owner is doing the project themselves or by volunteers the edit gi property tool enables web users to add their own gi features that meet the gi attributes described in table 2 once the user has finished her or his gi design the design can be saved to the user s desktop for sharing or to load back to the gi web application at another time otherwise users gi designs are not stored and are lost when users close the browser 4 2 using the web application to create gi landscape designs the gi web application available here web link 3 3 https hydroterremodels psu edu rhessys gi designer gi html as an independent service or here web link 4 4 https hydroterremodels psu edu rhessys gi designer gi hs html embedded within the gi rhessys jupyter notebook is shown in fig 5 the overarching strategy in designing the gi web application gui layout was to split the browser screen into two large panels to aid and simplify user creation of gi designs the left panel is designated to place gi in a 2d map environment and the right panel is dedicated to visualizing gi in a 2 5d environment with supporting gi tools to show how to create gi with the gi web application the dead run catchment located in baltimore county maryland usa is demonstrated using custom three meter high resolution datasets the watershed scale gi design has been prepared and uploaded as geodatabase layers further information about these designs is available from miles and band 2015a other locations within the conus are supported although at lower resolutions of 10 or 30 m constrained by national dataset properties unless additional local information is available we note that high resolution lidar terrain data and land cover are currently available or being developed by many municipalities and the chesapeake bay consortium has developed these data for the full chesapeake bay watershed http chesapeakeconservancy org in the dead run example the user specifies the dead run watershed location that is supported with a pre defined gi designed database by picking the location marked at fig 5a and the web visualizations will zoom to this location users can also use the google pegman fig 5b to change locations with all maps and the google streetview will zoom to the pegman location to pan hand icon the google map add gi point features balloon icon or create gi polygon surfaces shape icon the user picks from the toolbar located at fig 5c users specify gi properties such as size and depth at fig 5d that are assigned to gi materials chosen by the menu located at fig 5e for example to place a small honey locust tree the user clicks on small tree size fig 5d specifies the tree from the menu list fig 5e clicks on the shape icon fig 5c and the mouse cursor in the left panel changes from the hand icon to a cross icon now where the user clicks on the map left panel a tree icon marked with s for small is placed on the map fig 5f depending on the camera view in google streetview right panel the tree billboard will appear immediately fig 5g otherwise the user can rotate the camera view to see the tree in a perspective view to create surface gi users follow similar steps by specifying feature soil depth fig 5d selecting the shape icon fig 5c and clicking on the map multiple times to define the polygon vertices fig 5h however surface gi are not added to the google streetview due to the limitation of depth with the camera perspective and object occlusion as the user adds gi features quantities with estimated costs are automatically added to the gi spreadsheet located in fig 5i and shown in fig 6 a for users to modify the predefined gi properties such as costs that are kept in the gi dictionaries as discussed in section 3 1 the edit gi properties tool located in fig 5j and displayed in fig 6b is available to modify the existing gi or to create new custom gi to share existing and custom gi properties with other users or to save gi designs to user s local desktop for future use the save or load gi tool located in fig 5k and presented in fig 6c is offered to users once users have finished their gi landscape design the gi rhessys hydroterre workflows described in section 3 creates the gi datasets when users interact with the tools found by clicking the right panel tab at fig 5l the next section focuses on how the gi web application connects these gi datasets with the gi rhessys jupyter notebook 4 3 connecting gi designs with the gi rhessys jupyter notebook environment once the user has completed a gi landscape design he or she can download the resultant data bundles from gi rhessys hydroterre data workflows to their desktop by clicking on the download buttons shown in fig 6d within the gi rhessys jupyter notebook environment the user has the same capability however the process is cumbersome recall the hydroshare gi rhessys jupyter notebook environment is a web application providing access to a remote linux cloud compute environment when users click on the download buttons the browser will direct the zip file to the user s own compute environment not the remote linux environment hence users would need to upload the zip files to the hydroshare gi rhessys jupyter notebook environment requiring not only extra steps but be comfortable with the linux compute environment to avoid these extra steps browser window event listeners were added to the gi rhessys jupyter notebook to receive messages from the gi web application summarized in table 3 there are three general groups to post messages from the gi web application the first group contains the gi material dictionaries with default and any user custom gi as described in section 4 1 accordingly the gi attributes table 2 can be analyzed within the gi rhessys jupyter notebook the second group provides access to the gi geometry designed by the user both the first and second groups enable access to gi data useful for models that do not use spatial gis datasets for example the epa s swmm uses routing through a system of digitized drainage lines or pipes gironas et al 2010 and not derived from the elevation model that models such as rhessys use research by heidari et al 2018 use these gi javascript object notation json data structures with swmm and jupyter notebooks to analyze trees and rain gardens described elsewhere the last group provides uniform resource locator url locations of the workflow results by using wget free software foundation 2015 the data bundle results are downloaded within the gi rhessys jupyter notebook environment section 5 demonstrates these gi capabilities by executing a rhessys model at dead run watershed with the gi rhessys jupyter notebook compute environment 5 demonstration of using landscape designs within gi rhessys jupyter notebook the previous sections have discussed the gi web application for users to develop gi landscape designs and incorporate these designs with datasets required by rhessys using multiple workflows this section focuses on demonstrating the gi web application within the gi rhessys jupyter notebook compute environment to develop a rhessys model using the gi landscape design at dead run watershed from section 4 section 5 1 provides an overview of how users access the gi rhessys jupyter notebook and the last section demonstrates using the gi rhessys jupyter notebook to develop a rhessys hydrological model at dead run watershed 5 1 accessing gi rhessys jupyter notebooks there are two ways to access gi rhessys jupyter notebooks dependent on the user s confidence with computational environments the first method is to download the notebooks directly from hydroshare s github repository web link 5 5 https github com hydroshare hydroshare jupyterhub tree master notebooks where official notebooks supported by hydroshare are located with dependencies i e libraries additionally users can access the latest developer versions of gi rhessys jupyter notebooks using github from here l leonard and band 2017 the second method is to visit the hydroshare website at https www hydroshare org users are required to have an account with hydroshare to use these resources sign up is free once the user has logged into hydroshare click on discover at the top menu bar a keyword search of rhessys will return a list of resources available from hydroshare find and click on the resource titled green infrastructure designer with rhessys workflow a web page document appears describing details when the resource was created and last updated an abstract subject keywords how to cite the resource and a zipped bagit archive boyko et al 2014 containing the gi rhessys jupyter notebook users can download the gi rhessys jupyter notebook to their desktop using the zipped bagit archive if desired the main advantage of using the second method to access the gi rhessys jupyter notebook is the ability to use the open with button located top right of the web page document to open the notebook depending on the user s credentials the open with button will provide a list of different compute environments to use the gi rhessys jupyter notebook click on a jupyterhub assuming the user has valid credentials a new web page is loaded displaying a jupyter notebook within the jupyterhub compute environment a welcome page appears with instructions on how to open and use notebooks for users that downloaded gi rhessys jupyter notebooks from github or used the zipped bagit method it is feasible to upload jupyter notebooks within hydroshare s jupyterhub environment manually by using the jupyter tree click on jupyter icon to upload files in the users compute workspace users using their own jupyterhub environment will require dependencies such as the ecohydrolib and rhessys workflows python tools section 3 be installed instructions for installation are here miles and band 2017c the hydroshare jupyterhub compute environment has all the rhessys dependencies pre installed for all users the next section discusses how to open and use the gi rhessys jupyter notebook 5 2 using a jupyter notebook to create a rhessys model with gi designs the supplementary material provides the steps in opening and using the gi rhessys jupyter notebook to create gi landscape designs and a rhessys model briefly the user creates a gi landscape design fig 7 steps 1 3 using the gi web application explained in sections 4 2 and 4 3 next the user is ready to evaluate their gi landscape design in the gi rhessys jupyter notebook environment users need to import the echohydrolib and rhessys workflows section 3 4 into the gi rhessys jupyter notebook fig 7 step 4 and prepare the gi data fig 7 step 5 for the rhessys model assuming there were no errors reported the rhessys model is ready to be executed in the gi rhessys jupyter notebook compute environment as shown in fig 7 step 6 there is documentation on what the command options are for the runmodel tool that prepares and executes the rhessys model fig 7a the rhessys model using the jupyterhub compute environment took 45 min to finish the main reason the model compute time took a long time for a short simulation period the default value of 6 months defined in the rhessysworkflow python object which users can adjust is the high resolution three meter datasets used for defining the watershed properties and the resources allocated at the cloud site the last gi rhessys jupyter notebook code cell generates visualizations for rhessys variables as shown in fig 7b after executing all the notebook steps the user has generated a gi landscape design prepared data for the watershed location and created a rhessys hydrological model next steps for using the gi rhessys jupyter notebook include the development of workflows for calibrating the model parameters fig 7c with the runmodel tool trying different datasets from local or federal agencies or datasets from other hydroshare resources the gi rhessys jupyter notebook is easy to edit or clone for other research goals and a collection of different rhessys jupyter notebooks are available here l leonard and band 2017 with different data inputs and user skill level requirements after users have evaluated the gi rhessys jupyter notebook for the first time the learning curve to compare gi designs has been significantly reduced the authors encourage users to focus on creating ideal gi landscape designs with quantitative analysis with the rhessys ecohydrological model and share their data and model results within the hydroshare community as resources and public outreach 6 conclusion the provision and improvement of ecosystem services in urban areas depends on sound designs that integrate an understanding and forecast of the interactions of hydrologic ecological climate and social processes successful approaches to environmental restoration of the built environment are also increasingly seen as requiring some level of participation and input from residents and other stakeholders community participation in the use of advanced environmental models used to assess the benefits of different urban restoration designs requires both visualization and access or input to computational systems that are usually not available to non specialists our approach demonstrates how gi web applications can rapidly visualize gi landscape designs at the site scale for participatory planning between stormwater engineers and property owners in urban environments with pre defined and custom materials and automate the translation of user developed landscape designs to parameterize complex environmental models we demonstrated these capabilities at the dead run watershed in baltimore maryland usa a study site of the baltimore long term ecological research site http beslter org combining gi landscape designs at site locations with watershed scale designs user gi landscape designs are then transformed and integrated with custom and national datasets using gi rhessys hydroterre data workflows the data workflows generate data bundles and json data structures for hydrological modeling next we demonstrated the gi web application embedded inside a gi rhessys jupyter notebook hosted at hydroshare s cloud compute environment that consumes these data structures to create a rhessys ecohydrological model with gi landscape designs the documented rhessys jupyter notebooks serve as templates for beginner users to quickly learn how to prepare model inputs using ecohydrolib and rhessys workflows furthermore these notebooks are easy to clone and edit and the embedded workflows make the process for both beginner and expert users to rapidly create ecohydrological models at other watershed locations straightforward by using cloud compute resources to generate the rhessys ecohydrological models 7 future directions the next step with this research is to improve the visualization and data acquisition capabilities of the gi web application this includes workflows for users to upload sewer infrastructure computer aided designs and gi planning designs improving the gi material dictionaries with three dimensional trees and integrating google streetviews with lidar light detection and ranging captured topography and street infrastructure to create photo realistic gi landscape designs combined with simulated dynamics i e vegetation growth and extreme events from climate and hydrological model results furthermore new workflows and tools are necessary to validate gi landscape designs such as placing trees in suitable locations and quantifying whether the gi landscape designs meet local and state regulations using automation and hydro informatics and placement relative to utilities and other infrastructure we see a need from local agencies in multiple cities to use these web applications and services to interactively evaluate gi landscape designs with the public and other professions for immediate feedback on gi policies and implementations however spatially distributed and continuous time models like rhessys are computationally expensive and time consuming our vision is that the rhessys jupyter notebooks using compute resources such as hydroshare s jupyterhub will serve as templates and resources for agencies to access model results similar to a reference library as the data and models improve agencies access the latest improved model results with services such as the gi web application to evaluate proposed gi landscape designs and policies that are shared with other agencies and the community credits a number of scientists engineers and other professionals associated with the baltimore ecosystem study and with stormwater utilities in baltimore city and county portland or durham nc chicago and phoenix provided important feedback on conceptual design and initial versions of this software the research and development were a synthesis of information and methods developed by a set of nsf grants award no 1331813 collaborative research cybersees type 2 a new framework for crowd sourced green infrastructure design nsf cise awards 1148453 and 1148090 hydroshare nsf office of cyberinfrastructure award deb 1027188 long term ecological research program baltimore ecosystem study award dbi 1639145 socioeconomic synthesis center support for water science software institute award no 1239678 eager collaborative research interoperability testbed assessing a layered architecture for integration of existing capabilities award no 0940841 datanet federation consortium award no 1148090 collaborative research si2 ssi an interactive software infrastructure for sustaining collaborative innovation in the hydrologic sciences appendix a supplementary data the following is the supplementary data to this article gi supplementary docx gi supplementary docx appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 003 
26287,the determination of water quality along a river drainage network is of paramount importance for many environmental applications whereas several codes have been proposed to deal with this problem along a single reach of a river no simple solution has been proposed for a complex river network in this contribution a methodology is presented for preliminary screening of river water quality at the watershed scale and at steady state the solution process exploits the information contained within the watershed s space filling drainage network sfdn that is automatically computed by processing the raster map of the watershed s elevations to this end a visiting algorithm for non binary trees is coupled to a newton raphson method that locally solves the system of nonlinear water quality mass balances we believe that the resulting framework will be especially valuable for initial screening analyses in data poor watersheds a free code called q2t that implements the described methods is provided keywords water quality modeling dissolved oxygen nitrification channel networks rivers 1 introduction the ability to assess human impacts on water quality is a key aspect of sustainable river basin development by integrating human derived pollutant emissions with mathematical representations of the basin s transport regime and the pollutant s biochemical transformations computer based water quality models can serve as invaluable tools for guiding watershed management in a holistic cost effective and environmentally sustainable fashion chapra 2003 since the first dissolved oxygen model was developed in the early 20th century streeter and phelps 1925 a wide variety of river water quality frameworks have been created although a few simple back of the envelope approaches are available e g thomann and mueller 1987 welch et al 1989 chapra et al 2014 most currently available models e g ambrose et al 1988 qual2e 1987 bicknell et al 1997 pelletier et al 2006 chapra et al 2008 park et al 2008 etc are complex requiring copious amounts of field and laboratory input data for effective application at the watershed and regional scale regression based and process based water quality models for total phosphorus and total nitrogen have been devised e g spatially referenced regressions on watershed attributes smith et al 1997 wade et al 2002 sparrow is based on the assumption shared in this paper that spatial referencing of basin attributes in relation to the stream channel network greatly increases their statistical significance and model accuracy however due to its regional scale of application sparrow makes use of a statistical conceptualization of the transport processes along the network that is unsuitable to simulate dissolved oxygen distribution in response to different control scenarios similarly inca e g wade et al 2002 is a spatially semi distributed model that tracks the stores and fluxes of water nitrogen and phosphorus and that splits the water course into reaches with associated sub catchments and land based processes with a 1 km2 cell resolution depending on the level of detail required in a study data must be specified with various levels of precision when the focus is on strongly localised effects or time dependent pollution the use of sophisticated approaches that consider precisely measured local data diffusion of pollutants or backwater effects are necessary however when the emphasis is a preliminary simulation on a large space scale mass balances coupled to relevant chemical reactions play the most relevant role and simplified approaches are then legitimate river water quality problems are particularly severe in developing countries where fast growing population poor sanitary conditions and consumption of river water are often confounded by a lack of systematic and costly environmental monitoring and data collection chapra 2011 in addition almost everywhere multiple water uses e g hydropower production or agricultural irrigation lead to a modification of natural flow regimes which exacerbate water quality problems during periods of low flow for instance in order to preserve the ecological quality and integrity of rivers many countries have introduced legislation dictating a minimum flow usually called ecological flow or minimum admissible flow maf that must be maintained in a river system in dry periods an important constraint to the local value of maf is provided by the pollutant loads delivered to the river system because it is during low flow periods that the maximum pollutant concentrations invariably occur and the local maf value can be tuned so that these concentrations are kept below acceptable thresholds under these conditions both the river discharge and pollutant loads can be assumed to be at steady state and a simple framework integrating both water quantity and quality at the watershed scale results as a matter of fact in this situation water quality is the result of the interplay between maf and the pollutant loads entering along the channel network of the watershed often encompassing drainage areas of hundreds or even thousands of square kilometers accordingly the local value of maf can be explored on a case by case basis with a holistic approach that considers water quality throughout the watershed in this contribution we propose a novel simplified framework to address the problem of distributed modeling of water quality with specific attention to dissolved oxygen dynamics at steady state on a whole drainage network this framework is based on the idea that most processes governing the movement of water at the basin scale have in common a strong space variability and the possibility of being effectively simulated at a cell scale where cells represent the elementary units of the digital elevation model dem and are logically interrelated one to the others through the drainage network the methods described in this paper are implemented with a stand alone code q2t available at no cost for technical and scientific non commercial purposes q2t is a distributed nonlinear dissolved oxygen model that considers the dynamics of six variables carbonaceous bod organic nitrogen ammonia nitrogen nitrate nitrogen dissolved oxygen and sediment oxygen demand implemented for each cell of the watershed s channel network assuming local complete mixing the channel network is generated using the watershed s dem based on the approach originally presented by pilotti et al 1996 and pilotti and bacchi 1997 the information contained in the drainage network is then used to automatically compute the relevant topographic and geometric characteristics of the basin required by the model e g local flow direction local slopes local drained area local connectivity with other cells etc in the spirit of providing a simplified model that can be easily used at the watershed scale other geometric quantities e g the local cross section shape and width are computed by employing literature based geomorphological assumptions the watershed hydrology is simplified under the assumption of steady state conditions being primarily focused on water quality during drought periods we do not model the rainfall runoff transformation in the watershed and the local discharge is computed as the result of a simple discharge contribution per unit area that can be modulated throughout the watershed the flow in the river system is locally computed in uniform condition and the water temperature is computed by a local daily energy balance eventually many of the hydraulic and kinetic parameters local flow discharge velocity and depth gas transfer rates etc required by water quality models at the cell scale are evaluated in a simplified way minimizing the quantity of specific information required from the user in the following after describing the q2t model its performance is positively compared with the results provided for two simple comparisons with a more complex state of the art code the comparison shows that the pattern along the river of the investigated chemical quantities is accurately reproduced then the model is employed to simulate water quality in a drought period for the large upper watershed about 1480 km2 of the oglio river lombardy italy the oglio river flows through an area with thriving human activities and its watershed is a major source of pollutants to lake iseo the most endangered of the five large italian southern prealpine lakes e g pilotti et al 2013 this final example illustrates the level of complexity for which the q2t model has been devised 2 methods in the following section we present the methods that have been implemented in the q2t code to manage the large amount of data required at different sites in a basin for river water quality modelling these methods are not innovative in themselves but their use within a water quality model is we will show that most of the geometric information needed by a simplified water quality model such as the description of a branching stream system e g local slopes reach lengths reach connections drainage areas and the connectivity of each cell can be automatically obtained by the dem using these geomorphological quantities the other hydraulic local cross section width bed roughness local average flow velocity depth etc and hydrologic information local discharge can be evaluated in addition the use of the model requires knowledge of water quality model parameters whose values in data poor systems can be provided by the literature some basic meteorological data for the energy balance and the knowledge of the quantity of pollutants and the location of point and non point sources as well as artificial water diversions and intakes this information will be provided by the user finally the solution process of the mass and energy balances and chemical equations along the channel network takes advantage of efficient algorithms developed for non binary data structures 2 1 drainage networks the space filling drainage network sfdn is the connected set of paths in the direction of steepest descent on the surface of a basin see fig 1 a if we suppose that the motion of water on the surface of the basin is only controlled by gravity we could visualize the sfdn by following the motion of drops of water falling on the dem impermeable cells and flowing downward to the basin outlet where the paths converge and the surface runoff becomes channeled turbulent flow the sfdn is more properly called a channel network cn see fig 1b e accordingly the cn is a subset of the sfdn and these two networks represent an approximation of the path lines followed by water flowing on the surface of the basin the drainage networks are commonly computed by using a basin s raster digital elevation model dem whose cell resolution dictates the elementary space scale two stumbling blocks may hinder the derivation of a fully connected sfdn from a dem the presence of pits and of flat areas pits are depressed points in a dem where flowlines converge from all the adjacent cells flat areas are zones of zero gradient where the information available in the dem is not sufficient to determine the proper drainage direction the removal of pits and flat areas is a standard utility of several gis the q2t program works on an enhanced dem where all the depressions and flat areas have been eliminated and starting from any cell inside the watershed creates a single connected path that goes to the outlet using a d8 algorithm o callaghan and mark 1984 lin et al 2008 according to this algorithm the flow direction is selected with increments of 45 in spite of its simplicity this algorithm has some limitations particularly in areas of flow divergence e g wilson 2012 the sfdn is the resulting set of connected steepest directions that cover all the cells of the dem the cn can be derived from the sfdn using algorithms that conceptualise some physical mechanism if we accept that a channel is the result of the erosion exerted by the flowing stream e g horton 1945 it can be shown that the cn must be present wherever the drained area a along the sfdn is larger than a threshold that depends on the local erodibility and the rainfall regime the fixed contributing area algorithm reflects this dependence and assumes that a is spatially constant thus providing channel networks with a uniform drainage density see e g fig 1b however in term of erosive power a large drained area a might be compensated for by a small value of the local bed slope s b conversely as can be directly observed in steep valleys a large value of s b can induce channel formation at points where the upstream drained area is very small consequently the drainage network density is usually not uniformly distributed on the basin area as shown for instance in fig 1d in the past decades it has been observed that a fixed threshold contributing area algorithm gives reasonably satisfactory results for a channel network in rugged basins but fails for dems containing significant flat areas producing in such cases many spurious drainage lines to account for the local slope the slope dependent threshold contributing area algorithm has been proposed montgomery and dietrich 1992 montgomery and foufoula georgiou 1993 tribe 1992 maidment and djokic 2000 according to this approach implemented in q2t a channel may be identified starting from the point along the sfdn where the inequality 1 a c s b α is satisfied c km2 is a constant whose value theoretically depends on the soil and local climate but practically is tuned to reproduce the existing cn finally α is a dimensionless exponent with values typically in the range 1 6 2 if the exponent α is set to zero a fixed contributing area can be specified fig 1 shows some examples of cn obtained according to this criterion as a direct by product of this elaboration process a set of elementary topographic parameters of the dem e g elevation slope drained area interconnection with up to 8 surrounding cells are immediately available at each cell from a topological point of view both the sfdn and the cn are non binary trees whose root is the basin outlet accordingly the path lines followed by water flowing on the surface of the basin can be effectively represented by two tree like data structures from a computational point of view this implies that simple recursive tree traversal algorithms can be systematically employed to examine and or update or as it is commonly called visiting each node in the data structure using recursion it is possible with a few lines of code to write a general visiting algorithm along a tree of any level of complexity at least two different visiting algorithms can be devised depending on the order in which the nodes are visited considering a non binary tree as in fig 2 a where the dark green cell numbered 17 is the basin outlet and the pale green cells are the sources of the cn network a post order search algorithm explores the tree according to the numbering shown in each cell on the other hand a pre order algorithm follows the numbering of fig 2b where the outlet is numbered 1 to emphasise the relevance for water transfer modeling at the watershed scale it is interesting to observe that a post order algorithm would be suitable to compute supercritical flow profiles along the cn moving from upstream to downstream whereas the pre order algorithm would be suitable for subcritical flow profiles from downstream to upstream in this paper we assume that the motion in the cn is uniform and a post order algorithm is used for computing the drained area and other quantities e g local discharge and hydraulic quantities distance from the river sources conservative pollutant loads etc simply because it is better suited to the computation of cumulative properties simultaneously when visiting a node of the tree the system of equations governing the mass balance and the dissolved oxygen dynamic are solved accordingly based on the local quantities and input from upstream cells the local state variables and outputs for the downstream cell in the channel network can be computed each node of the tree represents a cell in the dem that in turn corresponds to a river reach length that is either d or 20 5 d where d is the linear resolution of the dem in this way the quantitative description of the two tree like structures along with suitable visiting algorithms provides a computationally efficient framework within which processes related to water movement in a watershed can be effectively modelled 2 2 computation of geometric hydrologic and hydraulic quantities to solve a water quality model various hydraulic e g local discharge bed roughness water depth and average velocity geometrical cross section shape and dimension and physical properties e g water temperature that govern reactant dynamics must be available along the cn although it would be possible to provide the required hydraulic information for each cell one by one where we make the working hypothesis that they are piecewise constant this would contradict the spirit of devising an effective and simple to apply modeling tool for initial screening of river pollution particularly in underdeveloped or developing countries most of this information is unavailable and in q2t we opted to provide empirical relations offering a first approximation for most of these quantities based on local characteristics such as the drained area and or of the distance from the source as far as the local discharge in the channel network is concerned in a complete water quality model this information would be provided by a distributed hydrologic model such a model could compute the local discharge based on the initial state of the system of the meteorological forcing functions and of the local parametric description of the underlying processes which could represent a challenge even in well monitored watersheds on the other hand if the watershed were partitioned into subcatchments a set of parametric lumped hydrological models could be used for each subcatchment in such a case however the variability of the discharge in the cn would be limited to the entry points of subcatchments into the cn and model parameters should equally be optimised from this perspective q2t is a steady state model mostly suitable for stable low flow periods it assumes that the watershed is hydrologically homogeneous and that a unitary discharge contribution q m3 km2 that can be regarded as a groundwater contribution can be used to compute the natural discharge q at each cell based on the local drained area a q qa in this way the discharge along the cn increases moving from upstream to the outlet to reproduce anthropogenic discharge both positive and negative i e a withdrawal at each cell along the cn the local value of q can be supplemented by a user specified input discharge this option could also be used to introduce limited space variability of the natural discharge contribution q in case of strong hydrological inhomogeneity of the watershed as far as the cn cross sections are concerned most natural river cross sections are much wider than deep so that their shape can be well approximated by a rectangular cross section at the same time the relationship between channel width and local discharge has been investigated in several studies e g leopold and maddock 1953 knighton 1998 montgomery and gran 2001 there is a general agreement that width varies approximately as a power of the discharge accordingly considering that discharge could scale linearly with the drained area a a relationship for the local channel width w of the form 2 w δ a ε can be postulated where δ and ε are empirical coefficients to be determined based on some observations for instance based on the river width measured at two cross sections say w 1 and w 2 where the drained area a is known a 1 and a 2 the parameter ε can be computed as 3 ε ln w 1 w 2 ln a 1 a 2 from which δ can then be obtained as w 1 a 1 ε as an alternative if ε 1 the width varies linearly with the drained area and if ε 0 the river width is constant the other fundamental parameters of flow needed along the cn are the local average flow depth h and velocity u if we assume uniform motion the chezy equation can be used 4 q 1 n w h 5 3 w 2 h 2 3 s b where n is the manning coefficient of roughness and s b m m is the local bed slope the manning coefficient is usually evaluated on a visual basis e g barnes 1967 and typically decreases moving from the source to the outlet of the cn due to the combined effect of slope width and discharge accordingly although a single constant value for the whole cn can sometimes be justified in q2t one can introduce a linear variation between a maximum and a minimum n value as a function of the drained area 2 3 nonlinear dissolved oxygen model for a stream network the q2t water quality component is a deterministic model that represents a one dimensional branched cn river as a series of well mixed reactors each associated with a cell fig 3 connected to the surrounding sfdn transport between cells is idealized as pure advection and each model state variable is subject to mass balance and reactions the model works with steady state inputs potentially located along each node of the cn these inputs can represent both point source reacting pollutants e g a wastewater treatment plant outfall and non point source i e distributed ground water upwelling to the river inputs three distinct types of mass balances are implemented within the model water conservative i e non reacting species and reacting chemical species the water mass balance is shown in fig 3b in volumetric terms the discharge q i m3 s at cell i varies with respect to the entering discharge from upstream q i 1 due to the increase of the watershed drained area a i km2 5 δ q i 1 q a i a i 1 where q m3 s km2 is the area specific discharge contribution from the watershed in addition there might be a flow increase from direct effluent discharges to the cell i m3 s this might include for instance a direct inflow to the channel cell from the groundwater the return of hydropower plant water or actual sewage pollutant loads and oxygen saturation can be associated to both q and i as will be detailed in the following and in the third example all these contributions are volumetrically lumped together within the water mass balance finally there might be a water withdrawal or diversion d m3 s which can be present due to agricultural hydropower or other water use the volumetric water balance incorporates all these flows as in 6 d v i d t q i 1 q a i a i 1 i i d i q i where v i m3 is the volume of water at cell i or considering that more than a one cn branch can converge at node i 7 d v i d t k q k q a i k a k i i d i q i where the summation includes all the cn cells that directly drain into cell i a second set of mass balances handles conservative constituents whose dynamics are governed only by dilution processes as was the case with the water balance one must consider the mass associated with the discharge coming from the upstream cell q i 1 c nr j i 1 where c nr j stands for the concentration of the jth non reacting species in the same way a load w may be associated with the external inflows i and q so that w nr j i i i c nr j i δ i c nr j q where the jth tracer concentration of the inflows is specified as a model input finally a water diversion d would represent an exit of the jth tracer dc nr j i from the cell i accordingly considering that more than one cn branch can converge at node i the mass balance of tracer j at cell i can be written as 8 d v i c n r j i d t w n r j i k q k c n r j k d i c n r j i q i c n r j i where the summation includes all the cn cells of index k that directly drain into cell i q2t gives the option of specifying the temperature t i i of any direct effluent discharge i to the ith cell if i consists of different contributions sewer outfalls return water from a hydropower plant etc t i i is the volume averaged temperature moreover as a first approximation the temperature of the area specific discharge contribution q can be specified as linearly dependent on the dem altitude the rationale behind this is that the temperature of the water from a shallow groundwater is related to the local average yearly temperature which in turn is controlled in part by elevation taylor and stefan 2009 the limits of this variation are set by the user based on the climatic situation of the investigated watershed the water temperature of the river at cell i t i is then computed by solving the mass averaged energy balance equation 9 d v i t i d t i i t i i k q k t k d i t i q i t i a φ ρ c p where φ represents both radiative and non radiative surface heat exchanges through the cell area a wl w is the cell width given by eq 2 and l is the cell length already discussed above and ρ and c p are the mass density and specific heat at constant pressure of water the surface heat exchange φ consists of 5 radiative and non radiative terms e g henderson sellers 1984 chapra 1997 10 φ φ s w φ l w φ b l w φ s h φ l h the term φ sw represents the net solar shortwave radiation wavelength range of 0 1 5 0 μm which depends on the solar altitude on scattering and absorption on reflection by the water surface and by possible shading exerted by trees or by the valley sides on the water surface all of these factors are determined as described in chapra et al 2008 the terms φ lw represents the atmospheric longwave radiation wavelength range of 5 100 μm that can be computed as chapra 1997 11 φ l w σ 1 r l t a i r 273 4 f e a i r where σ is the stefan boltzmann constant r l is a reflection coefficient t air is the air temperature and f e air accounts for atmospheric attenuation as a function of the air vapor pressure e air in turn the river water has a longwave back radiation 12 φ b l w ε σ t i s 273 4 where ε is the water emissivity whose value can be reckoned as 0 97 and t is is the temperature of the water surface at cell i in the following approximated as the average temperature of the river at cell i the non radiative fluxes are related to sensible heat φ sh and latent heat φ lh the sensible heat represents convective heat exchange between the water surface and the air 13 φ s h c 1 f v w t i t a i r where c 1 is the bowen s coefficient and f v w is usually given as an empirical function of the wind velocity v w at a fixed distance over the water surface chapra 1997 the latent heat φ lh is calculated as 14 φ l h f v w e s e a i r where e s is the saturation vapor pressure at the water surface whereas the local value of the different radiative fluxes can also be measured with suitable pyranometers e g pilotti et al 2013 the solution of equations 11 13 and 14 require among other quantities knowledge of the air temperature relative humidity and wind velocity at a given height on the water surface in order to have the air temperature at every elevation in the watershed in q2t we made the assumption of a linear variation as strictly true for a politropic dry air between a minimum value at the highest elevation and a maximum value at the watershed outlet if a reasonable estimate of these quantities is available eq 9 is a nonlinear equation whose numerical solution provides the average temperature of the water at cell i the chemically and biologically reactive compounds involved in the mass balances are listed in table 1 in addition sediment water fluxes are simulated via a nonlinear algebraic equation for sediment oxygen demand sod thus the major constituents and processes affecting organic sewage oxidation dynamics in a wastewater dominated river are included the water quality model s kinetic interactions reflect established theory in this area chapra 1997 and are depicted in fig 4 according to this theory two components of organic sewage typically contribute to oxygen depletion in natural waters organic carbon and reduced nitrogen the former is represented by carbonaceous bod cbod whereas the latter is represented by organic nitrogen and ammonia when dissolved oxygen is present the cbod is broken down or oxidized by bacteria which in the process consume oxygen at the same time organic n is converted to ammonia which then undergoes the nitrification which also consumes oxygen both processes lower the oxygen concentration creating an oxygen deficit which induces oxygen to enter the water from the atmosphere in an opposing process of reaeration the resulting oxygen level in the water is created by the balance between these two mechanisms for cases where waste discharges are low and reaeration is high e g high velocity shallow streams oxygen concentrations will be maintained at sufficiently elevated levels to support fish and other aerobic aquatic organisms conversely for sewage dominated rivers with low reaeration rates such as deeper slow moving rivers oxygen concentrations can be depleted to the point referred to as hypoxia where such organisms suffer at the extreme dissolved oxygen can be driven to zero called anoxia whereupon fish and other oxygen dependent organisms will die along with such ecological catastrophes a series of other processes emerge including the release of nutrients from the sediments phosphate and ammonia and nitrate oxidizes organic carbon via denitrification all these processes can be expressed mathematically for the concentrations this is done with the following mass balances as written for the ith cell eqs 15 19 for the dissolved oxygen flux into the sediments the sod flux this is based on an implicit non linear equation eq 20 derived by di toro et al 1990 from a steady state oxygen balance for the sediments underlying the ith cell the equations are carbonaceous bod c 1 15 v i d c 1 i d t w 1 i k q k c 1 k d i q i c 1 i c 5 i k s d c 5 i k d v i c 1 i c 4 i k s c d n c 4 i k s o d n k s o d n c 5 i k d n v i c 1 i v o c h i v i c 1 i j c h 4 i a b i organic nitrogen c 2 16 v i d c 2 i d t w 2 i k q k c 2 k d i q i c 2 i k h v i c 2 i v o n h i v i c 2 i ammonia nitrogen c 3 17 v i d c 3 i d t w 3 i k q k c 3 k d i q i c 3 i k h v i c 2 i c 5 1 k s n c 5 1 k n v i c 3 i j n h 4 i a b i nitrate nitrogen c 4 18 v i d c 4 i d t w 4 i k q k c 4 k d i q i c 4 i c 5 1 k s n c 5 1 k n v i c 3 i 1 r c n r o c c 4 i k s c d n c 4 i k s o d n k s o d n c 5 i k d n v i c 1 i dissolved oxygen c 5 19 v i d c 5 i d t w 5 i k q k c 5 k d i q i c 5 i c 5 1 k s d c 5 1 k d v i c 1 i r o n c 5 i k s n c 5 i k n v i c 3 i k a v i o s c 5 i c 6 i a b i sod c 6 20 c 6 i f c 1 i c 2 i c 5 i where a b i area m2 of the river bed at cell i t time day c j i concentration of constituent j in segment i mg l w j i loading of constituent j g day q i outflow from cell i m3 day d i water diversion out of cell i m3 day the meaning and typical values of the chemical constants that appear in the equations are listed in table 2 where due to convenience in connection to the typical values of chemical rates the unit of time is the day as already done above the summation in equations 15 19 is extended to all the cn cells of index k that directly drain into cell i note that all first order rates are corrected for temperature as in k t k 20 θ t 20 where k t is the rate at temperature t oc and θ is a constant that parameterizes the temperature impact for the rates in the current model suggested literature values chapra 1997 implemented in q2t are θ 1 07 except for reaeration k a and the cbod oxidation rate k d which are typically set to 1 024 and 1 047 respectively the term w j i in eqs 15 19 is related to the loading of pollutant j j 1 5 associated to the presence of a direct effluent discharge i to the cell and or to the area specific discharge contribution from the watershed actually groundwater feeding headwater streams might be depleted in oxygen relative to atmospheric partial pressures and enriched in pollutants as a result of respiration in the soils so that in q2t a background concentration can be associated to q whereas the first five equations are straightforward balances some explanation must be provided regarding the nonlinear equation 20 that governs sediment oxygen demand sod go2 m2 day as well as the methane j ch4 and ammonia j nh4 gains from the sediments these are calculated at steady state as a function of the model variables using a scheme originally developed by di toro et al 1990 in organically polluted waters organic carbon expressed in oxygen equivalents and organic nitrogen are delivered to the anaerobic sediments via settling from the overlying water the downward flux of organic carbon is j cbod 21 j c b o d v o c c 1 expressed in oxygen equivalents go2 m2 day and the input flux of nitrogen into the sediments j norg is specified in terms of the water variables as 22 j n o r g v o n c 2 where as above the meanings and values of the parameters are listed in table 2 within the anaerobic sediments the downward organic carbon and nitrogen fluxes are converted into dissolved methane and dissolved ammonia in addition when methane s solubility is exceeded gaseous methane is formed and assumed to escape to the atmosphere as bubbles the remaining dissolved forms then diffuse up to the aerobic layer where they are subject to oxidation the oxygen flux from the water required for these oxidations is the sediment oxygen demand any unoxidized dissolved methane and ammonia diffuses back up into the water cbod and ammonia pools finally the nitrate created via nitrification in the aerobic layer is denitrified into gaseous nitrogen which also consumes dissolved methane accordingly the sediment dynamic represents a feedback to the water through the methane j ch4 and ammonia j nh4 gains from the sediments these gains are strongly conditioned by the two mechanisms of methane bubble formation and anoxia the algorithm must handle these two distinct conditions of the water and the sediments regarding methane bubble formation this occurs in the anaerobic sediments when j cbod exceeds a threshold governed by methane s diffusivity and solubility here the methane solubility c s expressed in oxygen equivalents go2 m3 can be locally computed as 23 c s 100 1 h 10 1 024 20 t where h water depth m and t water temperature oc at each cell computed with eq 9 when the condition j cbod 2κ d c s is true where κ d is a methane diffusion mass transfer coefficient m d then bubbles do not form and the maximum possible sod due to cbod csod max is equal to the downward flux of cbod i e csod max j cbod if bubbles are formed j cbod 2κ d c s then the maximum carbonaceous sediment oxygen demand csod max 2 κ d c s j c b o d in summary 24 c s o d max j c b o d if j c b o d 2 κ d c s 2 κ d c s j c b o d if j c b o d 2 κ d c s with the first condition resolved the solution must reflect whether the water is oxic or anoxic anoxia occurs when the water oxygen concentration falls below some small zero or near zero value c 5 crit this criterion is expressed as c 5 c 5 crit depending on the oxygen concentration equation 20 governing the sod can be computed as a function of the downward carbon and organic nitrogen fluxes as 25 c 6 c s o d max 1 sech κ c c 5 c 6 r o n j n o r g 1 sech κ n c 5 c 6 if c 5 c 5 c r i t 0 if c 5 c 5 c r i t where κ c reaction velocity for methane oxidation m d j norg downward flux of organic nitrogen from the water to the sediments gn m2 day and κ n reaction velocity for ammonia oxidation m day the fluxes of dissolved methane and ammonia from the sediments back into the water can be computed as 26 j c h 4 a q j c h 4 c s o d max sech κ c c 5 c 6 if c 5 c 5 c r i t c s o d max if c 5 c 5 c r i t 27 j n h 4 j n o r g sech κ n c 5 c 6 if c 5 c 5 c r i t j n o r g if c 5 c 5 c r i t in addition although not strictly needed by the model it is also possible to compute the methane and the nitrogen gas fluxes that are released as in 28 j c h 4 g j c b o d c s o d max 29 j n 2 j n o r g 1 sech κ n c 5 c 6 if c 5 c 5 c r i t 0 if c 5 c 5 c r i t the calculation of the methane gas flux could be of interest as it is an important greenhouse gas notice that when no bubbles are formed csod max j cbod and eq 28 becomes j ch4 g 0 in the following we will consider the steady state results that can be obtained using daily average data to this purpose we observe that at steady state the model equations 9 and 15 20 reduce to a time independent set of non linear equations equation 9 decoupled from the system 15 20 provides the daily average temperature wheres eqs 15 20 are a system of 6 nonlinear simultaneous algebraic equations in the unknowns c j i 30 0 w 1 i k q k c 1 k d i q i c 1 i c 5 i k s d c 5 i k d v i c 1 i c 4 i k s c d n c 4 i k s o d n k s o d n c 5 i k d n v i c 1 i v o c h i v i c 1 i j c h 4 i a b i 31 0 w 2 i k q k c 2 k d i q i c 2 i k h v i c 2 i v o n h i v i c 2 i 32 0 w 3 i k q k c 3 k d i q i c 3 i k h v i c 2 i c 5 i k s n c 5 i k n v i c 3 i j n h 4 i a b i 33 0 w 4 i k q k c 4 k d i q i c 4 i c 5 i k s n c 5 i k n v i c 3 i 1 r c n r o c c 4 i k s c d n c 4 i k s o d n k s o d n c 5 i k d n v i c 1 i 34 0 w 5 i k q k c 5 k d i q i c 5 i c 5 1 k s d c 5 1 k d v i c 1 i r o n c 5 i k s n c 5 i k n v i c 3 i k a v i o s c 5 i c 6 i a b i 35 c 6 i c s o d max 1 sech κ c c 5 i c 6 i r o n j n o r g 1 sech κ n c 5 i c 6 i if c 5 i c 5 c r i t 0 if c 5 i c 5 c r i t this nonlinear system can be solved numerically for each element with the multi variable newton raphson method practically the problem is transformed into a sequence of linear systems of the type 36 j xt xt 1 j xt xt f xt where f is the system eqs 30 35 and j the jacobian of f both computed in the assumed solution vector xt in q2t the system 36 is solved with an efficient gradient conjugate method as with the differential equation approach the system s feed forward nature allows this to be done for each cell sequentially in the downstream direction according to the post order tree visiting algorithm in summary based on the computed geomorphologic information the localised steady state inputs and withdrawals i i and d i and of the other input data q2t solves eqs 7 and 9 to obtain the discharge and water temperature at each cell of the cn then eq 4 is solved with a scalar newton raphson method and all the hydraulic quantities are computed moving along the cn the system of equations 30 35 is then solved for each cell to obtain the concentration of the reacting variables at steady state and the results are propagated downstream using eq 8 3 examples in the following the q2t code is first compared with the solution provided by the qual2k software package chapra et al 2008 a well recognised standard for river water quality modeling example 1 presents a simple hypothetical river with a single pollutant source example 2 is an extension of example 1 with several point sources of varying quality and a withdrawal finally case 3 is an example of application of the code to a real watershed where a direct comparison with the results provided by qual2k would have been impossible the use of q2t model for a step by step reproduction of these cases is described in detail in the user s manual that is included as supplementary material to this paper in all comparisons of q2k and q2t we dropped all the mechanisms in q2k that were not included in q2t to make the tests meaningful for example we set plant biomass to zero in q2k these and other omissions were designed to verify that q2t yielded similar results to q2k when the latter was limited to the state variables and mechanisms employed in the simplified q2t such tests referred to formally as verification are an essential step in the software development process schlesinger et al 1979 office of technology assessment 1982 astm 1992 rykiel 1996 example 1 a first simple case is presented to illustrate the solution without complicating details it corresponds to a hypothetical basin made up of a straight sequence of cells with no confluences and arbitrarily set inputs the basin is covered by a 200 km long river discretised in 100 2 km long reaches to this purpose the cn has been extracted by q2t setting to 0 the drained threshold area c 0 in eq 1 the river slope is constant piecewise 1 3 10 4 m m for the first 80 km from the outlet and 1 7 10 4 m m upstream the river has a rectangular cross section with constant width w 20 m and manning s coefficient n 0 04 m 1 3 s the river water and effluent temperature are fixed at 20 c to this purpose q2t gives the possibility to turn off the terms of energy flux φ in eq 9 the coefficients of eq 30 35 are set to the default values suggested in table 2 with constant oxygen saturation concentration 9 092 mgo2 l and variable reaeration coefficient computed for each cell with the o connor dobbins formula table 2 the pollutant loads are summarized in table 3 at the cell centered at 199 km from outlet a first pollutant point source is used to set the river boundary condition the river is relatively clean until at 170 km from the outlet apoint source discharges both organic carbon cbod and reduced nitrogen organic n and ammonia into the river due to the simplified shape of this linear watershed the area specific discharge contribution from the watershed q is zero the computed spatial patterns of concentration are shown in fig 5 the results indicate that the point source discharge greatly increases the river s cbod and reduced nitrogen concentrations at x 170 km the cbod and ammonia nitrogen are then oxidized by bacteria to form carbon dioxide not shown and nitrate the net result is a classic dissolved oxygen sag that approaches hypoxia defined as oxygen concentration 3 mg l due to strong initial oxidation followed by subsequent recovery due to reaeration because both the cbod and organic nitrogen settle the oxygen sag is reinforced by sediment oxygen demand also notice in the bottom panel of fig 5 how the reaeration decreases at 170 and 80 km from the outlet due to changes in velocity and depth as the water flows downstream the first decrease is caused by the increased discharge in the channel and the second by the reduction of slope as shown in fig 5 the comparison with qual2k shows a perfect superimposition of the results example 2 case 2 is identical to case 1 as far as the geometry and cn upstream boundary condition and the point loading at x 170 km are concerned however at x 140 80 and 50 km from the outlet flows enter and leave the system these include a second point source input x 140 km a major outflow diversion x 80 km and a clean tributary inflow x 50 km the corresponding data are summarized in table 4 as in fig 6 because of the multiple sources the resulting spatial patterns are more complex than for the first case the results indicate that the large second point source x 140 causes oxygen levels pink shaded area in the third panel of fig 6 to become hypoxic i e to near zero oxygen concentrations detrimental to aerobic organisms due to the low oxygen the resulting inhibition of both carbon oxidation and nitrification tends to extend the hypoxic zone further downstream this condition is somewhat mitigated by the withdrawal at x 80 km which causes a decrease of the water depth and accordingly an increase of reaeration compensating for the consequences of the slope change this is indicated by the change of slope of the oxygen trend at x 80 the recovery is further alleviated in a more abrupt fashion by the introduction of the clean source at x 50 km also in this case the comparison with the results provided by qual2k is excellent this case illustrates the utility of the model to estimate the combined impacts of multiple sources and withdrawals to a linear river such a capability becomes more evident in the following section where the model framework is applied to an actual river network example 3 as a final example we use q2t to study water quality evolution along the 80 km long reach of the oglio river discretised based on a dem with 160 m of resolution the purpose of this example is to illustrate the complexity of a real case providing all the geometric and hydraulic data slopes cross section geometries local discharges connection with tributaries etc needed for such a spatially detailed description would be extremely challenging when using a traditional modeling approach in this example we show that a preliminary but detailed evaluation of water quality can be implemented quite easily with q2t this result is obtained by exploiting the geomorphologic data contained in the watershed s cn the mountain reach of the oglio river flows across valle camonica in lombardy north italy see fig 7 with an area of approximately 1480 km2 and an average elevation of 1600 m above sea level ranging from a minimum of 185 m to a maximum of 3554 m adamello glacier valle camonica is one of the largest valleys of the italian alpine area it is characterised by a relatively high population density 114 000 inhabitants and an elevated level of industry it is crossed by the oglio river for about 80 km and by several canals mostly for hydropower production this river is the main tributary of lake iseo and its temperature and pollutant load have a strong effect on the lake hydrodynamics and trophic state pilotti et al 2013 2014a b 2018 along its 80 km course many point sources from wastewater treatment plants and some untreated sewers enter the oglio river moreover its water is diverted at several points mainly for hydropower production in the following example the pollutants sources listed in table 5 are considered the location of these sources is shown as black points in fig 7 all the other tributaries carry zero contaminant load and we assume that are oxygen saturated according to the equations presented in table 2 the pre processed digital elevation model used in this example fig 7 has a space resolution of 160 m 160 m in the same figure the cn computed by q2t is shown as a black solid line obtained with parameters c 1 km2 and α 0 of eq 1 the depitting algorithm that produced the pre processed dem left some cells where the gradient is unrealistically small to deal with this problem which is an artefact of the quality of the original dem a user specified minimum slope can be introduced in q2t when the local slope in the dem is lower than this threshold the threshold is used in our case this minimum slope was set to 0 001 m m the average yearly discharge of the oglio river at the outlet b of fig 7 is about 50 m3 s in the following our goal is to provide a preliminary screening of the effects of sewage outfalls in the river during drought periods when natural flows are minimal this case typically represents the most severe situation and during these periods hydropower withdrawals from the river are not allowed the analysis of a discharge time series gauged at the watershed outlet that extends from 1933 provides the value of qd t where d is the duration in days and t the return period in years italian regulations suggest durations of 347 and 355 days and some values of q347 and q355 for different return period are shown in table 6 bacchi and pilotti 2000 in the following we will refer to q355 5 as this discharge corresponds to a drought situation that happens quite regularly in the watershed if snow melt and rainfall are both negligible the natural discharge reflects groundwater which is higher on the bottom of the valleys where however evapotranspiration is also higher accordingly although the validity of the assumption of a spatially uniform unitary discharge is not general for this preliminary screening we will assume that the value of q 355 5 is the result of a spatially uniform unitary discharge q 355 5 q 355 5 a 0 0097 m3 s km2 where a is the area of the watershed in km2 based on this unitary contribution q2t computes the discharge q j at each cell j of the drainage network as far as river width q 355 5 temperature of distributed q contribution and manning s coefficient are concerned q2t introduces some simplifying assumptions the width of the river is automatically computed as a function of the drained area according to eq 2 here we used ε 0 2 and δ 3 482 which were suggested by direct evaluations in the watershed they correspond to a width varying between 4 m in the upper part of the cn when the drained area is 1 km2 and 15 m at the basin outlet as far as the water temperature of the local unitary contribution q355 5 is concerned q2t provides a linear variation with elevation between a minimum value at the maximum elevation in the watershed and a maximum value at the watershed outlet considering that the average yearly temperature of the oglio watershed could range between 3 c at the summit of adamello glacier and 11 c at the outlet we made the assumpion that the linear variation can range between a minimum of 0 c and 11 c at the outlet the actual river temperature at each cell is provided by the balance of eq 9 and reflects the overall elevation of the drained watershed upstream as well as the other energy fluxes acting along the river course in this simulation we considered a typical summer day julian day 225 during which the actual daily averaged sw radiation at the ground is 245 w m2 and the average daily air temperature ranges between 2 c at the highest elevation and 23 c at the outlet with a relative humidity of 30 with these data the energy balance along the river provides a temperature at the outlet of 15 2 c in very good agreement with observations at august pilotti et al 2013 finally although q2t provides a linear variation of n with the drained area for simplicity sake a constant manning coefficient n 0 04 m 1 3s has been used based on this information the code automatically computes the tentative value of the local discharge in drought periods and the average velocity and depth according to chezy s equation eq 4 the following figures show the results from the output files produced by q2t as described in the companion tutorial of this example the upper panel of fig 8 shows the normalized altimetric profile of the river thalweg between point a station 82 km and b station 0 km of fig 7 the red line indicates the variation of the drained area along the reach under the assumption of constant unitary contribution the natural discharge varies along the river with the same pattern the blue line represents the river temperature t and has a more complex pattern superimposed on the rising pattern there are several abrupt t increases and drops these reflect the entrance of tributaries with water having a different temperature than the river for instance the drop at point i in the upper panel of fig 8 corresponds to the confluence of the cn with watershed i in fig 7 watershed i has an average elevation of 2332 m a s l whereas the average elevation of the watershed drained by the cn upstream of the confluence with i is 2219 m accordingly the drop is caused by the slightly colder temperature of the tributary waters from watershed i similarly but in the opposite direction at point ii there is the confluence with watershed ii see fig 7 with average elevation of 1653 m into the main river at a point where the average elevation of the drained watershed is 2066 m accordingly an abrupt temperature increase occurs slightly reinforced by the small pollutant outfall 2 in fact temperature variations can also be caused by heated effluents shown by vertical dashed lines in the upper panel of fig 8 for instance at point 1 in fig 8 one can observe that the mixing of 0 124 m3 s of sewage at 25 c data from table 5 with the natural discharge of 2 663 m3 s at 13 8 c data from q2t simulation causes an overall temperature rise of 0 51 c based on the computed velocity which varies locally as a function of the slope width and discharge it is possible to compute the transit time the cumulative pattern of which is shown in the panel b of fig 8 in dry period water takes almost 28 h to flow from point a to b with an average velocity of 0 8 m s from fig 9 it is evident that whereas the average normal depth keeps increasing from a to b albeit with strong local fluctuations due to the variations of slope the low flow normal velocity both computed with the chezy s equation 4 does not vary significantly moving from upstream to the outlet whereas moving downstream the local slope decreases and the cross section width increases this effect is compensated by the growth of the flow in panel a and b fig 10 shows the variation along the cn of the concentration of bod and of the different nitrogen species concentration increases when a point source discharges directly to the river but may decrease if an outfall is located on a tributary whose waters are cleaner than the ones of the main river this happens for outfall 11 as shown in fig 7 located on a lateral tributary of the selected ab reach of the cn and whose confluence with ab is shown at point 11j in figs 7 and 10 in general cbod concentration falls in correspondence to clean tributaries that dilute the polluted waters of the main river as clearly shown from the thin solid line in panel a indicating the decrease of cbod when only the outfall 1 is present along the cn in this case the cbod concentration would decrease in correspondence to each tributary confluence due to dilution with the incoming clean discharge panel c shows the variation of the oxygen and oxygen saturation concentration along the river in q2t the oxygen saturation varies as a function of water temperature and altitude according to the equations shown in table 2 this explains why the oxygen concentration in fresh clean water upstream of outfall 1 keeps increasing along the reach as the river elevation drops about 600 m see also fig 8 although q2t computes reaeration with o connor dobbins equation to make the discussion of the results simpler for this case the reaeration coefficient has been kept constant and equal to 20 d 1 as shown in panel d along with the sod dynamic notice how the unpolluted upper reaches have negligible sod in contrast beginning with outfall 1 considerable sod occurs due to settling of cbod and organic n the system of equations 30 35 is solved along the whole cn of the watershed and q2t provides the results along every path of the cn from an initial point to the outlet for instance if one selects point c of fig 7 as an initial point the results between points c and b are output these are shown in fig 11 upstream of point 11 oxygen tends to saturation and downstream of point 11j the pattern is the same shown in the first 15 kms of fig 10 with the same simplicity the whole cn can be explored with non trivial results occurring when different combination of inputs are present along the different branches that make up the overall cn of the watershed 4 discussion the increasing pressure exerted worldwide on water resources requires a growing awareness of the ecological implications of human activities high quality fresh water is a diminishing resource and every effort should be made to preserve it in the face of burgeoning human development in this spirit simple and effective water quality modeling tools can play a key role because they can be used to assess the implications of human exploitation and test different alternatives for water quality management and control however when large watersheds are considered the amount of data needed for a model set up can become daunting this is certainly true in developing countries where lack of funding can lead to the absence of institutional monitoring programs in these situations at least for initial screening purposes it is critical to exploit the information provided by all the available data sources some of these data inevitably require direct gauging for instance location and amount of point pollutant loads for others e g parameters of the chemical equations some reasonable literature guesses can be introduced finally a plethora of geomorphological information needed to model water transfer at the watershed scale can be derived by exploiting the informative content of digital elevation models which are available everywhere at low cost accordingly although it is not possible to eliminate the direct measurement and pre processing of field data the total effort can be considerably limited in this spirit the software q2t can be used to generate preliminary screening simulations guide future monitoring campaigns and inform more refined model development q2t reorganises the geomorphological information contained in a dem into efficient tree data structures sfdn and cn that naturally reproduce the watershed s drainage networks whereas other algorithms could be used to explore a drainage network in our experience a tree visiting algorithm provides a simple and powerful tool to simulate processes related to water transfer at the watershed scale the implemented water quality model that simulates the oxygen and nitrogen compounds dynamics at the dem cell scale in a flowing river has been positively compared with the outputs from a more complex model with two simple cases and then applied to a real and complex watershed the rationale of the comparison is the same as the basis of calibration of instruments that requires to compare the output of the unit under test here q2t with that provided by a standard here qual2k defined as a device with proven performance such comparisons are a formal part of sound software development called program verification fetzer 1988 1991 whereas the qual2k model is much more complex it is possible to generate simplified applications identical to case 1 and 2 by merely setting relevant qual2k model parameters and variables to zero the user guide and the companion material that is available for these two cases can be used to acquire proficiency with the simple application of q2t the valle camonica application a large alpine watershed in northern italy on the other hand demonstrates the type of insight and advantage that can be obtained with the proposed approach the model automatically accounts for the complex structure of the channel network that arises from the dem and the distributed nature of input data the topological and geometrical features of the network easily provide fundamental quantities drained area slope average elevation of each drained subcatchment etc within q2t basic equations e g chezy s law literature based relations e g the local channel width as a function of the drained area biochemical rate constants etc and assumptions related to watershed hydrology e g a unitary discharge contribution in drought periods provide throughout the cn a wealth of information local water depth velocity elevation travel times these in turn can then be employed to compute other coefficients and variables e g oxygen saturation reaeration coefficients water temperatures etc whose variation would be difficult to reproduce in more traditional modelling frameworks by quantifying and modeling this physically based complexity interesting effects emerge such as the increasing oxygen concentration upstream of the first pollutant outfall as the river s oxygen saturation continuously increases due to the rapid elevation drop see fig 10 panel c in conclusion in this example the model shows its capability to transmit the effects of contaminant sources originating in one location on water quality at other points downstream in the watershed accordingly it provides a straightforward way to single out the effects of local pollution sources from the effects of contaminants originating upstream along the cn undoubtedly other more complex approaches could reproduce point by point the complex situations that arise from the interplay of incoming discharges from the tributaries the residual local discharge coming from the slopes that directly drain at each cell and the pollutant outfalls however this would require a huge and costly information burden that could strongly limit or delay a holistic overview of water quality at the catchment scale as done by the algorithms implemented in q2t and would not be possible at all when there is a deficit of information the model presented in this paper and implemented in q2t is sufficiently simple that it could be exploited for preliminary screenings such as the ones to spot critical reaches along the cn that should be monitored during a planned measuring campaign or to explore the environmental consequences of different pollution and water diversion scenarios the output data produced by the model could be easily used e g in estimating the proportion of a cn where the concentration of a contaminant exceeds an established threshold in correspondence of different planned maf values the proposed approach could also be profitably employed by environmental authorities for an initial exploration of the environmental consequences of different water exploitation plans as well as industrial and civil waste permissions this could provide a simple method to contribute to the identification of the environmental flow required in a river for achieving a specific ecological target the framework s simplicity is based on several simplifying assumptions the model considers a steady state situation accordingly it cannot be used to study transient fluctuations such as daily variation nevertheless from the hydraulic point of view the steady state hypothesis is particularly appropriate for low flow periods when typically water quality is poorest and the computation of discharge based on a unitary contribution that mimics the effect of the river s aquifers is particularly valid moreover in these conditions the flow can be approximated as uniform at a local scale and its features can be reckoned based on the local values of slope cross section width and roughness in our steady state approach the groundwater temperature is provided as a function of elevation and the river water temperature is computed from a local heat balance another simplifying assumption is that the concentration is uniform across each cross section corresponding to full lateral and vertical mixing for this hypothesis to be true the cell dimension compared to the water depth and river width must be large accordingly contrary to other hydraulic uses of dem e g pilotti 2016 it would be wrong to use a very detailed dem for this type of application in the case of valle camonica where the typical width of the river is in the order of a dozen of meters during drought periods the square cell dimension is 160 m finally we have expressly designed the current framework for heavily polluted wastewater dominated systems of the type that are common in urbanizing developing countries such was the case in the early years of water quality modelling in north america and europe in the early twentieth century at that time the first models focused on untreated urban wastewater discharges of oxygen demanding pollutants first total bod and subsequently reduced nitrogen species as in our paper eutrophication was not the primary concern and plants were not modelled the major goal was to ameliorate anoxia and the attendant odors below raw wastewater point source discharges the current framework could readily be expanded to address further problems such as eutrophication ph metals etc but each of these additions would involve costly additional data collection although we anticipate adding such detail in the future doing it at this point would conflict with our primary goal of providing a free simple to use framework for preliminary water quality modelling of data poor river networks in developing countries it should be stressed that the application of our model should be viewed as a starting point for example given little or no observational data and literature values for the parameter the resulting model output would undoubtedly have high uncertainty nevertheless as pointed out above because it does incorporate proven scientific mechanisms and the drainage basin topology it would still be useful in a host of preliminary applications ranging from gross screening to informing subsequent data collection efforts most importantly it has the great upside of expediting a holistic basin wide perspective at the beginning of a study rather than as an afterthought or not at all by facilitating this global perspective at the outset we believe that subsequent efforts to refine the analysis and reduce uncertainty will be more efficiently realized in the long run coupled with the standard practices for model calibration and corroboration reckhow and chapra 1983 oreskes et al 1994 the framework could eventually be improved to the point that it could be used reliably for decision support users should reference the relevant literature on water quality model application to guide this evolution e g chapra 2003 as well as the key literature on uncertainty analysis that is of critical importance to such efforts omlin and reichert 1999 wagener et al 2001 etc this approach could be easily extended among many improvements an obvious extension would be to consider persistent non point waste sources that could be modelled along the sfdn with a similar approach and transferred to the cn for subsequent propagation to the watershed outlet pilotti and bacchi 1997 have shown how this can be done to model distributed soil erosion a process that is usually at the heart of nutrient export at the watershed scale 5 conclusions in this paper we have presented a steady state water quality model implemented in the free software q2t the water quality model is a deterministic distributed dissolved oxygen model that is based on the evolution of the bod model by streeter and phelps 1925 coupled with a simplified nitrogen cycle chapra 1997 and with an elegant analytical model of sediment water interactions di toro et al 1990 accordingly it considers the dynamics of six variables carbonaceous bod organic nitrogen ammonia nitrogen nitrate nitrogen dissolved oxygen and sediment oxygen demand the nonlinear equations of the chemical model are solved along the channel network that is automatically derived as a subset of the watershed s drainage network in turn obtained by the watershed s digital elevation model by application of a filtering criterion implemented in the software each cell of the channel network is regarded as a well mixed reactor where state variables are subject to mass balance and reactions the hydrological and hydraulic quantities needed by the model at each cell are automatically computed based on some simplified assumptions and transport between cells is idealized as pure advection the model works with steady state inputs potentially located along each node of the channel network these inputs can represent both point source reacting pollutants e g a wastewater treatment plant outfall and non point source i e local upwelling ground water in the river inputs with associated temperature oxygen and pollutant concentrations the model is positively tested with two geometrically simplified cases for which the space pattern of the chemical variables along the channel network are compared with those generated with the qual2k model finally as a third example the model is applied to a large watershed in the italian alps showing both its ease of application and its capability to transmit the effects of contaminant sources originating from one location on water quality at other points downstream in the watershed we believe that considering the simplified hypothesis used in its construction the model is particularly suitable to assess water quality objectives during critical pollution states characterised by low flows however the framework can serve as the starting point for broader applications such as more complex kinetic representations e g eutrophication pathogens etc and fully dynamic simulations we intend to proceed along these lines in future development of this scheme software availability the algorithm described in this paper has been implemented in a stand alone code q2t a name coming from the union of 2 keywords quality and tree like the topological structures used in the code and is freely available for technical and scientific non commercial purposes along with tutorials for reproduction of the three examples described herein the program is written in delphi s object pascal has a size of 3 7 mb and was tested under windows os versions 7 to 10 this code operates on a pre processed raster dem where all pits and flat areas have been eliminated such pre processing readily computed with most gis the program installation and application is straightforward and extensively described in the user guide which can be downloaded along with tutorials and the data needed to reproduce the three cases presented in the paper at http hydraulics unibs it hydraulics dati scaricabili water quality code conflicts of interest none acknowledgements we are grateful to three anonymous reviewers for their contributions to the improvement of this paper the participation of the second author scc was supported by the summer visiting professor program at the università degli studi di brescia italy 
26287,the determination of water quality along a river drainage network is of paramount importance for many environmental applications whereas several codes have been proposed to deal with this problem along a single reach of a river no simple solution has been proposed for a complex river network in this contribution a methodology is presented for preliminary screening of river water quality at the watershed scale and at steady state the solution process exploits the information contained within the watershed s space filling drainage network sfdn that is automatically computed by processing the raster map of the watershed s elevations to this end a visiting algorithm for non binary trees is coupled to a newton raphson method that locally solves the system of nonlinear water quality mass balances we believe that the resulting framework will be especially valuable for initial screening analyses in data poor watersheds a free code called q2t that implements the described methods is provided keywords water quality modeling dissolved oxygen nitrification channel networks rivers 1 introduction the ability to assess human impacts on water quality is a key aspect of sustainable river basin development by integrating human derived pollutant emissions with mathematical representations of the basin s transport regime and the pollutant s biochemical transformations computer based water quality models can serve as invaluable tools for guiding watershed management in a holistic cost effective and environmentally sustainable fashion chapra 2003 since the first dissolved oxygen model was developed in the early 20th century streeter and phelps 1925 a wide variety of river water quality frameworks have been created although a few simple back of the envelope approaches are available e g thomann and mueller 1987 welch et al 1989 chapra et al 2014 most currently available models e g ambrose et al 1988 qual2e 1987 bicknell et al 1997 pelletier et al 2006 chapra et al 2008 park et al 2008 etc are complex requiring copious amounts of field and laboratory input data for effective application at the watershed and regional scale regression based and process based water quality models for total phosphorus and total nitrogen have been devised e g spatially referenced regressions on watershed attributes smith et al 1997 wade et al 2002 sparrow is based on the assumption shared in this paper that spatial referencing of basin attributes in relation to the stream channel network greatly increases their statistical significance and model accuracy however due to its regional scale of application sparrow makes use of a statistical conceptualization of the transport processes along the network that is unsuitable to simulate dissolved oxygen distribution in response to different control scenarios similarly inca e g wade et al 2002 is a spatially semi distributed model that tracks the stores and fluxes of water nitrogen and phosphorus and that splits the water course into reaches with associated sub catchments and land based processes with a 1 km2 cell resolution depending on the level of detail required in a study data must be specified with various levels of precision when the focus is on strongly localised effects or time dependent pollution the use of sophisticated approaches that consider precisely measured local data diffusion of pollutants or backwater effects are necessary however when the emphasis is a preliminary simulation on a large space scale mass balances coupled to relevant chemical reactions play the most relevant role and simplified approaches are then legitimate river water quality problems are particularly severe in developing countries where fast growing population poor sanitary conditions and consumption of river water are often confounded by a lack of systematic and costly environmental monitoring and data collection chapra 2011 in addition almost everywhere multiple water uses e g hydropower production or agricultural irrigation lead to a modification of natural flow regimes which exacerbate water quality problems during periods of low flow for instance in order to preserve the ecological quality and integrity of rivers many countries have introduced legislation dictating a minimum flow usually called ecological flow or minimum admissible flow maf that must be maintained in a river system in dry periods an important constraint to the local value of maf is provided by the pollutant loads delivered to the river system because it is during low flow periods that the maximum pollutant concentrations invariably occur and the local maf value can be tuned so that these concentrations are kept below acceptable thresholds under these conditions both the river discharge and pollutant loads can be assumed to be at steady state and a simple framework integrating both water quantity and quality at the watershed scale results as a matter of fact in this situation water quality is the result of the interplay between maf and the pollutant loads entering along the channel network of the watershed often encompassing drainage areas of hundreds or even thousands of square kilometers accordingly the local value of maf can be explored on a case by case basis with a holistic approach that considers water quality throughout the watershed in this contribution we propose a novel simplified framework to address the problem of distributed modeling of water quality with specific attention to dissolved oxygen dynamics at steady state on a whole drainage network this framework is based on the idea that most processes governing the movement of water at the basin scale have in common a strong space variability and the possibility of being effectively simulated at a cell scale where cells represent the elementary units of the digital elevation model dem and are logically interrelated one to the others through the drainage network the methods described in this paper are implemented with a stand alone code q2t available at no cost for technical and scientific non commercial purposes q2t is a distributed nonlinear dissolved oxygen model that considers the dynamics of six variables carbonaceous bod organic nitrogen ammonia nitrogen nitrate nitrogen dissolved oxygen and sediment oxygen demand implemented for each cell of the watershed s channel network assuming local complete mixing the channel network is generated using the watershed s dem based on the approach originally presented by pilotti et al 1996 and pilotti and bacchi 1997 the information contained in the drainage network is then used to automatically compute the relevant topographic and geometric characteristics of the basin required by the model e g local flow direction local slopes local drained area local connectivity with other cells etc in the spirit of providing a simplified model that can be easily used at the watershed scale other geometric quantities e g the local cross section shape and width are computed by employing literature based geomorphological assumptions the watershed hydrology is simplified under the assumption of steady state conditions being primarily focused on water quality during drought periods we do not model the rainfall runoff transformation in the watershed and the local discharge is computed as the result of a simple discharge contribution per unit area that can be modulated throughout the watershed the flow in the river system is locally computed in uniform condition and the water temperature is computed by a local daily energy balance eventually many of the hydraulic and kinetic parameters local flow discharge velocity and depth gas transfer rates etc required by water quality models at the cell scale are evaluated in a simplified way minimizing the quantity of specific information required from the user in the following after describing the q2t model its performance is positively compared with the results provided for two simple comparisons with a more complex state of the art code the comparison shows that the pattern along the river of the investigated chemical quantities is accurately reproduced then the model is employed to simulate water quality in a drought period for the large upper watershed about 1480 km2 of the oglio river lombardy italy the oglio river flows through an area with thriving human activities and its watershed is a major source of pollutants to lake iseo the most endangered of the five large italian southern prealpine lakes e g pilotti et al 2013 this final example illustrates the level of complexity for which the q2t model has been devised 2 methods in the following section we present the methods that have been implemented in the q2t code to manage the large amount of data required at different sites in a basin for river water quality modelling these methods are not innovative in themselves but their use within a water quality model is we will show that most of the geometric information needed by a simplified water quality model such as the description of a branching stream system e g local slopes reach lengths reach connections drainage areas and the connectivity of each cell can be automatically obtained by the dem using these geomorphological quantities the other hydraulic local cross section width bed roughness local average flow velocity depth etc and hydrologic information local discharge can be evaluated in addition the use of the model requires knowledge of water quality model parameters whose values in data poor systems can be provided by the literature some basic meteorological data for the energy balance and the knowledge of the quantity of pollutants and the location of point and non point sources as well as artificial water diversions and intakes this information will be provided by the user finally the solution process of the mass and energy balances and chemical equations along the channel network takes advantage of efficient algorithms developed for non binary data structures 2 1 drainage networks the space filling drainage network sfdn is the connected set of paths in the direction of steepest descent on the surface of a basin see fig 1 a if we suppose that the motion of water on the surface of the basin is only controlled by gravity we could visualize the sfdn by following the motion of drops of water falling on the dem impermeable cells and flowing downward to the basin outlet where the paths converge and the surface runoff becomes channeled turbulent flow the sfdn is more properly called a channel network cn see fig 1b e accordingly the cn is a subset of the sfdn and these two networks represent an approximation of the path lines followed by water flowing on the surface of the basin the drainage networks are commonly computed by using a basin s raster digital elevation model dem whose cell resolution dictates the elementary space scale two stumbling blocks may hinder the derivation of a fully connected sfdn from a dem the presence of pits and of flat areas pits are depressed points in a dem where flowlines converge from all the adjacent cells flat areas are zones of zero gradient where the information available in the dem is not sufficient to determine the proper drainage direction the removal of pits and flat areas is a standard utility of several gis the q2t program works on an enhanced dem where all the depressions and flat areas have been eliminated and starting from any cell inside the watershed creates a single connected path that goes to the outlet using a d8 algorithm o callaghan and mark 1984 lin et al 2008 according to this algorithm the flow direction is selected with increments of 45 in spite of its simplicity this algorithm has some limitations particularly in areas of flow divergence e g wilson 2012 the sfdn is the resulting set of connected steepest directions that cover all the cells of the dem the cn can be derived from the sfdn using algorithms that conceptualise some physical mechanism if we accept that a channel is the result of the erosion exerted by the flowing stream e g horton 1945 it can be shown that the cn must be present wherever the drained area a along the sfdn is larger than a threshold that depends on the local erodibility and the rainfall regime the fixed contributing area algorithm reflects this dependence and assumes that a is spatially constant thus providing channel networks with a uniform drainage density see e g fig 1b however in term of erosive power a large drained area a might be compensated for by a small value of the local bed slope s b conversely as can be directly observed in steep valleys a large value of s b can induce channel formation at points where the upstream drained area is very small consequently the drainage network density is usually not uniformly distributed on the basin area as shown for instance in fig 1d in the past decades it has been observed that a fixed threshold contributing area algorithm gives reasonably satisfactory results for a channel network in rugged basins but fails for dems containing significant flat areas producing in such cases many spurious drainage lines to account for the local slope the slope dependent threshold contributing area algorithm has been proposed montgomery and dietrich 1992 montgomery and foufoula georgiou 1993 tribe 1992 maidment and djokic 2000 according to this approach implemented in q2t a channel may be identified starting from the point along the sfdn where the inequality 1 a c s b α is satisfied c km2 is a constant whose value theoretically depends on the soil and local climate but practically is tuned to reproduce the existing cn finally α is a dimensionless exponent with values typically in the range 1 6 2 if the exponent α is set to zero a fixed contributing area can be specified fig 1 shows some examples of cn obtained according to this criterion as a direct by product of this elaboration process a set of elementary topographic parameters of the dem e g elevation slope drained area interconnection with up to 8 surrounding cells are immediately available at each cell from a topological point of view both the sfdn and the cn are non binary trees whose root is the basin outlet accordingly the path lines followed by water flowing on the surface of the basin can be effectively represented by two tree like data structures from a computational point of view this implies that simple recursive tree traversal algorithms can be systematically employed to examine and or update or as it is commonly called visiting each node in the data structure using recursion it is possible with a few lines of code to write a general visiting algorithm along a tree of any level of complexity at least two different visiting algorithms can be devised depending on the order in which the nodes are visited considering a non binary tree as in fig 2 a where the dark green cell numbered 17 is the basin outlet and the pale green cells are the sources of the cn network a post order search algorithm explores the tree according to the numbering shown in each cell on the other hand a pre order algorithm follows the numbering of fig 2b where the outlet is numbered 1 to emphasise the relevance for water transfer modeling at the watershed scale it is interesting to observe that a post order algorithm would be suitable to compute supercritical flow profiles along the cn moving from upstream to downstream whereas the pre order algorithm would be suitable for subcritical flow profiles from downstream to upstream in this paper we assume that the motion in the cn is uniform and a post order algorithm is used for computing the drained area and other quantities e g local discharge and hydraulic quantities distance from the river sources conservative pollutant loads etc simply because it is better suited to the computation of cumulative properties simultaneously when visiting a node of the tree the system of equations governing the mass balance and the dissolved oxygen dynamic are solved accordingly based on the local quantities and input from upstream cells the local state variables and outputs for the downstream cell in the channel network can be computed each node of the tree represents a cell in the dem that in turn corresponds to a river reach length that is either d or 20 5 d where d is the linear resolution of the dem in this way the quantitative description of the two tree like structures along with suitable visiting algorithms provides a computationally efficient framework within which processes related to water movement in a watershed can be effectively modelled 2 2 computation of geometric hydrologic and hydraulic quantities to solve a water quality model various hydraulic e g local discharge bed roughness water depth and average velocity geometrical cross section shape and dimension and physical properties e g water temperature that govern reactant dynamics must be available along the cn although it would be possible to provide the required hydraulic information for each cell one by one where we make the working hypothesis that they are piecewise constant this would contradict the spirit of devising an effective and simple to apply modeling tool for initial screening of river pollution particularly in underdeveloped or developing countries most of this information is unavailable and in q2t we opted to provide empirical relations offering a first approximation for most of these quantities based on local characteristics such as the drained area and or of the distance from the source as far as the local discharge in the channel network is concerned in a complete water quality model this information would be provided by a distributed hydrologic model such a model could compute the local discharge based on the initial state of the system of the meteorological forcing functions and of the local parametric description of the underlying processes which could represent a challenge even in well monitored watersheds on the other hand if the watershed were partitioned into subcatchments a set of parametric lumped hydrological models could be used for each subcatchment in such a case however the variability of the discharge in the cn would be limited to the entry points of subcatchments into the cn and model parameters should equally be optimised from this perspective q2t is a steady state model mostly suitable for stable low flow periods it assumes that the watershed is hydrologically homogeneous and that a unitary discharge contribution q m3 km2 that can be regarded as a groundwater contribution can be used to compute the natural discharge q at each cell based on the local drained area a q qa in this way the discharge along the cn increases moving from upstream to the outlet to reproduce anthropogenic discharge both positive and negative i e a withdrawal at each cell along the cn the local value of q can be supplemented by a user specified input discharge this option could also be used to introduce limited space variability of the natural discharge contribution q in case of strong hydrological inhomogeneity of the watershed as far as the cn cross sections are concerned most natural river cross sections are much wider than deep so that their shape can be well approximated by a rectangular cross section at the same time the relationship between channel width and local discharge has been investigated in several studies e g leopold and maddock 1953 knighton 1998 montgomery and gran 2001 there is a general agreement that width varies approximately as a power of the discharge accordingly considering that discharge could scale linearly with the drained area a a relationship for the local channel width w of the form 2 w δ a ε can be postulated where δ and ε are empirical coefficients to be determined based on some observations for instance based on the river width measured at two cross sections say w 1 and w 2 where the drained area a is known a 1 and a 2 the parameter ε can be computed as 3 ε ln w 1 w 2 ln a 1 a 2 from which δ can then be obtained as w 1 a 1 ε as an alternative if ε 1 the width varies linearly with the drained area and if ε 0 the river width is constant the other fundamental parameters of flow needed along the cn are the local average flow depth h and velocity u if we assume uniform motion the chezy equation can be used 4 q 1 n w h 5 3 w 2 h 2 3 s b where n is the manning coefficient of roughness and s b m m is the local bed slope the manning coefficient is usually evaluated on a visual basis e g barnes 1967 and typically decreases moving from the source to the outlet of the cn due to the combined effect of slope width and discharge accordingly although a single constant value for the whole cn can sometimes be justified in q2t one can introduce a linear variation between a maximum and a minimum n value as a function of the drained area 2 3 nonlinear dissolved oxygen model for a stream network the q2t water quality component is a deterministic model that represents a one dimensional branched cn river as a series of well mixed reactors each associated with a cell fig 3 connected to the surrounding sfdn transport between cells is idealized as pure advection and each model state variable is subject to mass balance and reactions the model works with steady state inputs potentially located along each node of the cn these inputs can represent both point source reacting pollutants e g a wastewater treatment plant outfall and non point source i e distributed ground water upwelling to the river inputs three distinct types of mass balances are implemented within the model water conservative i e non reacting species and reacting chemical species the water mass balance is shown in fig 3b in volumetric terms the discharge q i m3 s at cell i varies with respect to the entering discharge from upstream q i 1 due to the increase of the watershed drained area a i km2 5 δ q i 1 q a i a i 1 where q m3 s km2 is the area specific discharge contribution from the watershed in addition there might be a flow increase from direct effluent discharges to the cell i m3 s this might include for instance a direct inflow to the channel cell from the groundwater the return of hydropower plant water or actual sewage pollutant loads and oxygen saturation can be associated to both q and i as will be detailed in the following and in the third example all these contributions are volumetrically lumped together within the water mass balance finally there might be a water withdrawal or diversion d m3 s which can be present due to agricultural hydropower or other water use the volumetric water balance incorporates all these flows as in 6 d v i d t q i 1 q a i a i 1 i i d i q i where v i m3 is the volume of water at cell i or considering that more than a one cn branch can converge at node i 7 d v i d t k q k q a i k a k i i d i q i where the summation includes all the cn cells that directly drain into cell i a second set of mass balances handles conservative constituents whose dynamics are governed only by dilution processes as was the case with the water balance one must consider the mass associated with the discharge coming from the upstream cell q i 1 c nr j i 1 where c nr j stands for the concentration of the jth non reacting species in the same way a load w may be associated with the external inflows i and q so that w nr j i i i c nr j i δ i c nr j q where the jth tracer concentration of the inflows is specified as a model input finally a water diversion d would represent an exit of the jth tracer dc nr j i from the cell i accordingly considering that more than one cn branch can converge at node i the mass balance of tracer j at cell i can be written as 8 d v i c n r j i d t w n r j i k q k c n r j k d i c n r j i q i c n r j i where the summation includes all the cn cells of index k that directly drain into cell i q2t gives the option of specifying the temperature t i i of any direct effluent discharge i to the ith cell if i consists of different contributions sewer outfalls return water from a hydropower plant etc t i i is the volume averaged temperature moreover as a first approximation the temperature of the area specific discharge contribution q can be specified as linearly dependent on the dem altitude the rationale behind this is that the temperature of the water from a shallow groundwater is related to the local average yearly temperature which in turn is controlled in part by elevation taylor and stefan 2009 the limits of this variation are set by the user based on the climatic situation of the investigated watershed the water temperature of the river at cell i t i is then computed by solving the mass averaged energy balance equation 9 d v i t i d t i i t i i k q k t k d i t i q i t i a φ ρ c p where φ represents both radiative and non radiative surface heat exchanges through the cell area a wl w is the cell width given by eq 2 and l is the cell length already discussed above and ρ and c p are the mass density and specific heat at constant pressure of water the surface heat exchange φ consists of 5 radiative and non radiative terms e g henderson sellers 1984 chapra 1997 10 φ φ s w φ l w φ b l w φ s h φ l h the term φ sw represents the net solar shortwave radiation wavelength range of 0 1 5 0 μm which depends on the solar altitude on scattering and absorption on reflection by the water surface and by possible shading exerted by trees or by the valley sides on the water surface all of these factors are determined as described in chapra et al 2008 the terms φ lw represents the atmospheric longwave radiation wavelength range of 5 100 μm that can be computed as chapra 1997 11 φ l w σ 1 r l t a i r 273 4 f e a i r where σ is the stefan boltzmann constant r l is a reflection coefficient t air is the air temperature and f e air accounts for atmospheric attenuation as a function of the air vapor pressure e air in turn the river water has a longwave back radiation 12 φ b l w ε σ t i s 273 4 where ε is the water emissivity whose value can be reckoned as 0 97 and t is is the temperature of the water surface at cell i in the following approximated as the average temperature of the river at cell i the non radiative fluxes are related to sensible heat φ sh and latent heat φ lh the sensible heat represents convective heat exchange between the water surface and the air 13 φ s h c 1 f v w t i t a i r where c 1 is the bowen s coefficient and f v w is usually given as an empirical function of the wind velocity v w at a fixed distance over the water surface chapra 1997 the latent heat φ lh is calculated as 14 φ l h f v w e s e a i r where e s is the saturation vapor pressure at the water surface whereas the local value of the different radiative fluxes can also be measured with suitable pyranometers e g pilotti et al 2013 the solution of equations 11 13 and 14 require among other quantities knowledge of the air temperature relative humidity and wind velocity at a given height on the water surface in order to have the air temperature at every elevation in the watershed in q2t we made the assumption of a linear variation as strictly true for a politropic dry air between a minimum value at the highest elevation and a maximum value at the watershed outlet if a reasonable estimate of these quantities is available eq 9 is a nonlinear equation whose numerical solution provides the average temperature of the water at cell i the chemically and biologically reactive compounds involved in the mass balances are listed in table 1 in addition sediment water fluxes are simulated via a nonlinear algebraic equation for sediment oxygen demand sod thus the major constituents and processes affecting organic sewage oxidation dynamics in a wastewater dominated river are included the water quality model s kinetic interactions reflect established theory in this area chapra 1997 and are depicted in fig 4 according to this theory two components of organic sewage typically contribute to oxygen depletion in natural waters organic carbon and reduced nitrogen the former is represented by carbonaceous bod cbod whereas the latter is represented by organic nitrogen and ammonia when dissolved oxygen is present the cbod is broken down or oxidized by bacteria which in the process consume oxygen at the same time organic n is converted to ammonia which then undergoes the nitrification which also consumes oxygen both processes lower the oxygen concentration creating an oxygen deficit which induces oxygen to enter the water from the atmosphere in an opposing process of reaeration the resulting oxygen level in the water is created by the balance between these two mechanisms for cases where waste discharges are low and reaeration is high e g high velocity shallow streams oxygen concentrations will be maintained at sufficiently elevated levels to support fish and other aerobic aquatic organisms conversely for sewage dominated rivers with low reaeration rates such as deeper slow moving rivers oxygen concentrations can be depleted to the point referred to as hypoxia where such organisms suffer at the extreme dissolved oxygen can be driven to zero called anoxia whereupon fish and other oxygen dependent organisms will die along with such ecological catastrophes a series of other processes emerge including the release of nutrients from the sediments phosphate and ammonia and nitrate oxidizes organic carbon via denitrification all these processes can be expressed mathematically for the concentrations this is done with the following mass balances as written for the ith cell eqs 15 19 for the dissolved oxygen flux into the sediments the sod flux this is based on an implicit non linear equation eq 20 derived by di toro et al 1990 from a steady state oxygen balance for the sediments underlying the ith cell the equations are carbonaceous bod c 1 15 v i d c 1 i d t w 1 i k q k c 1 k d i q i c 1 i c 5 i k s d c 5 i k d v i c 1 i c 4 i k s c d n c 4 i k s o d n k s o d n c 5 i k d n v i c 1 i v o c h i v i c 1 i j c h 4 i a b i organic nitrogen c 2 16 v i d c 2 i d t w 2 i k q k c 2 k d i q i c 2 i k h v i c 2 i v o n h i v i c 2 i ammonia nitrogen c 3 17 v i d c 3 i d t w 3 i k q k c 3 k d i q i c 3 i k h v i c 2 i c 5 1 k s n c 5 1 k n v i c 3 i j n h 4 i a b i nitrate nitrogen c 4 18 v i d c 4 i d t w 4 i k q k c 4 k d i q i c 4 i c 5 1 k s n c 5 1 k n v i c 3 i 1 r c n r o c c 4 i k s c d n c 4 i k s o d n k s o d n c 5 i k d n v i c 1 i dissolved oxygen c 5 19 v i d c 5 i d t w 5 i k q k c 5 k d i q i c 5 i c 5 1 k s d c 5 1 k d v i c 1 i r o n c 5 i k s n c 5 i k n v i c 3 i k a v i o s c 5 i c 6 i a b i sod c 6 20 c 6 i f c 1 i c 2 i c 5 i where a b i area m2 of the river bed at cell i t time day c j i concentration of constituent j in segment i mg l w j i loading of constituent j g day q i outflow from cell i m3 day d i water diversion out of cell i m3 day the meaning and typical values of the chemical constants that appear in the equations are listed in table 2 where due to convenience in connection to the typical values of chemical rates the unit of time is the day as already done above the summation in equations 15 19 is extended to all the cn cells of index k that directly drain into cell i note that all first order rates are corrected for temperature as in k t k 20 θ t 20 where k t is the rate at temperature t oc and θ is a constant that parameterizes the temperature impact for the rates in the current model suggested literature values chapra 1997 implemented in q2t are θ 1 07 except for reaeration k a and the cbod oxidation rate k d which are typically set to 1 024 and 1 047 respectively the term w j i in eqs 15 19 is related to the loading of pollutant j j 1 5 associated to the presence of a direct effluent discharge i to the cell and or to the area specific discharge contribution from the watershed actually groundwater feeding headwater streams might be depleted in oxygen relative to atmospheric partial pressures and enriched in pollutants as a result of respiration in the soils so that in q2t a background concentration can be associated to q whereas the first five equations are straightforward balances some explanation must be provided regarding the nonlinear equation 20 that governs sediment oxygen demand sod go2 m2 day as well as the methane j ch4 and ammonia j nh4 gains from the sediments these are calculated at steady state as a function of the model variables using a scheme originally developed by di toro et al 1990 in organically polluted waters organic carbon expressed in oxygen equivalents and organic nitrogen are delivered to the anaerobic sediments via settling from the overlying water the downward flux of organic carbon is j cbod 21 j c b o d v o c c 1 expressed in oxygen equivalents go2 m2 day and the input flux of nitrogen into the sediments j norg is specified in terms of the water variables as 22 j n o r g v o n c 2 where as above the meanings and values of the parameters are listed in table 2 within the anaerobic sediments the downward organic carbon and nitrogen fluxes are converted into dissolved methane and dissolved ammonia in addition when methane s solubility is exceeded gaseous methane is formed and assumed to escape to the atmosphere as bubbles the remaining dissolved forms then diffuse up to the aerobic layer where they are subject to oxidation the oxygen flux from the water required for these oxidations is the sediment oxygen demand any unoxidized dissolved methane and ammonia diffuses back up into the water cbod and ammonia pools finally the nitrate created via nitrification in the aerobic layer is denitrified into gaseous nitrogen which also consumes dissolved methane accordingly the sediment dynamic represents a feedback to the water through the methane j ch4 and ammonia j nh4 gains from the sediments these gains are strongly conditioned by the two mechanisms of methane bubble formation and anoxia the algorithm must handle these two distinct conditions of the water and the sediments regarding methane bubble formation this occurs in the anaerobic sediments when j cbod exceeds a threshold governed by methane s diffusivity and solubility here the methane solubility c s expressed in oxygen equivalents go2 m3 can be locally computed as 23 c s 100 1 h 10 1 024 20 t where h water depth m and t water temperature oc at each cell computed with eq 9 when the condition j cbod 2κ d c s is true where κ d is a methane diffusion mass transfer coefficient m d then bubbles do not form and the maximum possible sod due to cbod csod max is equal to the downward flux of cbod i e csod max j cbod if bubbles are formed j cbod 2κ d c s then the maximum carbonaceous sediment oxygen demand csod max 2 κ d c s j c b o d in summary 24 c s o d max j c b o d if j c b o d 2 κ d c s 2 κ d c s j c b o d if j c b o d 2 κ d c s with the first condition resolved the solution must reflect whether the water is oxic or anoxic anoxia occurs when the water oxygen concentration falls below some small zero or near zero value c 5 crit this criterion is expressed as c 5 c 5 crit depending on the oxygen concentration equation 20 governing the sod can be computed as a function of the downward carbon and organic nitrogen fluxes as 25 c 6 c s o d max 1 sech κ c c 5 c 6 r o n j n o r g 1 sech κ n c 5 c 6 if c 5 c 5 c r i t 0 if c 5 c 5 c r i t where κ c reaction velocity for methane oxidation m d j norg downward flux of organic nitrogen from the water to the sediments gn m2 day and κ n reaction velocity for ammonia oxidation m day the fluxes of dissolved methane and ammonia from the sediments back into the water can be computed as 26 j c h 4 a q j c h 4 c s o d max sech κ c c 5 c 6 if c 5 c 5 c r i t c s o d max if c 5 c 5 c r i t 27 j n h 4 j n o r g sech κ n c 5 c 6 if c 5 c 5 c r i t j n o r g if c 5 c 5 c r i t in addition although not strictly needed by the model it is also possible to compute the methane and the nitrogen gas fluxes that are released as in 28 j c h 4 g j c b o d c s o d max 29 j n 2 j n o r g 1 sech κ n c 5 c 6 if c 5 c 5 c r i t 0 if c 5 c 5 c r i t the calculation of the methane gas flux could be of interest as it is an important greenhouse gas notice that when no bubbles are formed csod max j cbod and eq 28 becomes j ch4 g 0 in the following we will consider the steady state results that can be obtained using daily average data to this purpose we observe that at steady state the model equations 9 and 15 20 reduce to a time independent set of non linear equations equation 9 decoupled from the system 15 20 provides the daily average temperature wheres eqs 15 20 are a system of 6 nonlinear simultaneous algebraic equations in the unknowns c j i 30 0 w 1 i k q k c 1 k d i q i c 1 i c 5 i k s d c 5 i k d v i c 1 i c 4 i k s c d n c 4 i k s o d n k s o d n c 5 i k d n v i c 1 i v o c h i v i c 1 i j c h 4 i a b i 31 0 w 2 i k q k c 2 k d i q i c 2 i k h v i c 2 i v o n h i v i c 2 i 32 0 w 3 i k q k c 3 k d i q i c 3 i k h v i c 2 i c 5 i k s n c 5 i k n v i c 3 i j n h 4 i a b i 33 0 w 4 i k q k c 4 k d i q i c 4 i c 5 i k s n c 5 i k n v i c 3 i 1 r c n r o c c 4 i k s c d n c 4 i k s o d n k s o d n c 5 i k d n v i c 1 i 34 0 w 5 i k q k c 5 k d i q i c 5 i c 5 1 k s d c 5 1 k d v i c 1 i r o n c 5 i k s n c 5 i k n v i c 3 i k a v i o s c 5 i c 6 i a b i 35 c 6 i c s o d max 1 sech κ c c 5 i c 6 i r o n j n o r g 1 sech κ n c 5 i c 6 i if c 5 i c 5 c r i t 0 if c 5 i c 5 c r i t this nonlinear system can be solved numerically for each element with the multi variable newton raphson method practically the problem is transformed into a sequence of linear systems of the type 36 j xt xt 1 j xt xt f xt where f is the system eqs 30 35 and j the jacobian of f both computed in the assumed solution vector xt in q2t the system 36 is solved with an efficient gradient conjugate method as with the differential equation approach the system s feed forward nature allows this to be done for each cell sequentially in the downstream direction according to the post order tree visiting algorithm in summary based on the computed geomorphologic information the localised steady state inputs and withdrawals i i and d i and of the other input data q2t solves eqs 7 and 9 to obtain the discharge and water temperature at each cell of the cn then eq 4 is solved with a scalar newton raphson method and all the hydraulic quantities are computed moving along the cn the system of equations 30 35 is then solved for each cell to obtain the concentration of the reacting variables at steady state and the results are propagated downstream using eq 8 3 examples in the following the q2t code is first compared with the solution provided by the qual2k software package chapra et al 2008 a well recognised standard for river water quality modeling example 1 presents a simple hypothetical river with a single pollutant source example 2 is an extension of example 1 with several point sources of varying quality and a withdrawal finally case 3 is an example of application of the code to a real watershed where a direct comparison with the results provided by qual2k would have been impossible the use of q2t model for a step by step reproduction of these cases is described in detail in the user s manual that is included as supplementary material to this paper in all comparisons of q2k and q2t we dropped all the mechanisms in q2k that were not included in q2t to make the tests meaningful for example we set plant biomass to zero in q2k these and other omissions were designed to verify that q2t yielded similar results to q2k when the latter was limited to the state variables and mechanisms employed in the simplified q2t such tests referred to formally as verification are an essential step in the software development process schlesinger et al 1979 office of technology assessment 1982 astm 1992 rykiel 1996 example 1 a first simple case is presented to illustrate the solution without complicating details it corresponds to a hypothetical basin made up of a straight sequence of cells with no confluences and arbitrarily set inputs the basin is covered by a 200 km long river discretised in 100 2 km long reaches to this purpose the cn has been extracted by q2t setting to 0 the drained threshold area c 0 in eq 1 the river slope is constant piecewise 1 3 10 4 m m for the first 80 km from the outlet and 1 7 10 4 m m upstream the river has a rectangular cross section with constant width w 20 m and manning s coefficient n 0 04 m 1 3 s the river water and effluent temperature are fixed at 20 c to this purpose q2t gives the possibility to turn off the terms of energy flux φ in eq 9 the coefficients of eq 30 35 are set to the default values suggested in table 2 with constant oxygen saturation concentration 9 092 mgo2 l and variable reaeration coefficient computed for each cell with the o connor dobbins formula table 2 the pollutant loads are summarized in table 3 at the cell centered at 199 km from outlet a first pollutant point source is used to set the river boundary condition the river is relatively clean until at 170 km from the outlet apoint source discharges both organic carbon cbod and reduced nitrogen organic n and ammonia into the river due to the simplified shape of this linear watershed the area specific discharge contribution from the watershed q is zero the computed spatial patterns of concentration are shown in fig 5 the results indicate that the point source discharge greatly increases the river s cbod and reduced nitrogen concentrations at x 170 km the cbod and ammonia nitrogen are then oxidized by bacteria to form carbon dioxide not shown and nitrate the net result is a classic dissolved oxygen sag that approaches hypoxia defined as oxygen concentration 3 mg l due to strong initial oxidation followed by subsequent recovery due to reaeration because both the cbod and organic nitrogen settle the oxygen sag is reinforced by sediment oxygen demand also notice in the bottom panel of fig 5 how the reaeration decreases at 170 and 80 km from the outlet due to changes in velocity and depth as the water flows downstream the first decrease is caused by the increased discharge in the channel and the second by the reduction of slope as shown in fig 5 the comparison with qual2k shows a perfect superimposition of the results example 2 case 2 is identical to case 1 as far as the geometry and cn upstream boundary condition and the point loading at x 170 km are concerned however at x 140 80 and 50 km from the outlet flows enter and leave the system these include a second point source input x 140 km a major outflow diversion x 80 km and a clean tributary inflow x 50 km the corresponding data are summarized in table 4 as in fig 6 because of the multiple sources the resulting spatial patterns are more complex than for the first case the results indicate that the large second point source x 140 causes oxygen levels pink shaded area in the third panel of fig 6 to become hypoxic i e to near zero oxygen concentrations detrimental to aerobic organisms due to the low oxygen the resulting inhibition of both carbon oxidation and nitrification tends to extend the hypoxic zone further downstream this condition is somewhat mitigated by the withdrawal at x 80 km which causes a decrease of the water depth and accordingly an increase of reaeration compensating for the consequences of the slope change this is indicated by the change of slope of the oxygen trend at x 80 the recovery is further alleviated in a more abrupt fashion by the introduction of the clean source at x 50 km also in this case the comparison with the results provided by qual2k is excellent this case illustrates the utility of the model to estimate the combined impacts of multiple sources and withdrawals to a linear river such a capability becomes more evident in the following section where the model framework is applied to an actual river network example 3 as a final example we use q2t to study water quality evolution along the 80 km long reach of the oglio river discretised based on a dem with 160 m of resolution the purpose of this example is to illustrate the complexity of a real case providing all the geometric and hydraulic data slopes cross section geometries local discharges connection with tributaries etc needed for such a spatially detailed description would be extremely challenging when using a traditional modeling approach in this example we show that a preliminary but detailed evaluation of water quality can be implemented quite easily with q2t this result is obtained by exploiting the geomorphologic data contained in the watershed s cn the mountain reach of the oglio river flows across valle camonica in lombardy north italy see fig 7 with an area of approximately 1480 km2 and an average elevation of 1600 m above sea level ranging from a minimum of 185 m to a maximum of 3554 m adamello glacier valle camonica is one of the largest valleys of the italian alpine area it is characterised by a relatively high population density 114 000 inhabitants and an elevated level of industry it is crossed by the oglio river for about 80 km and by several canals mostly for hydropower production this river is the main tributary of lake iseo and its temperature and pollutant load have a strong effect on the lake hydrodynamics and trophic state pilotti et al 2013 2014a b 2018 along its 80 km course many point sources from wastewater treatment plants and some untreated sewers enter the oglio river moreover its water is diverted at several points mainly for hydropower production in the following example the pollutants sources listed in table 5 are considered the location of these sources is shown as black points in fig 7 all the other tributaries carry zero contaminant load and we assume that are oxygen saturated according to the equations presented in table 2 the pre processed digital elevation model used in this example fig 7 has a space resolution of 160 m 160 m in the same figure the cn computed by q2t is shown as a black solid line obtained with parameters c 1 km2 and α 0 of eq 1 the depitting algorithm that produced the pre processed dem left some cells where the gradient is unrealistically small to deal with this problem which is an artefact of the quality of the original dem a user specified minimum slope can be introduced in q2t when the local slope in the dem is lower than this threshold the threshold is used in our case this minimum slope was set to 0 001 m m the average yearly discharge of the oglio river at the outlet b of fig 7 is about 50 m3 s in the following our goal is to provide a preliminary screening of the effects of sewage outfalls in the river during drought periods when natural flows are minimal this case typically represents the most severe situation and during these periods hydropower withdrawals from the river are not allowed the analysis of a discharge time series gauged at the watershed outlet that extends from 1933 provides the value of qd t where d is the duration in days and t the return period in years italian regulations suggest durations of 347 and 355 days and some values of q347 and q355 for different return period are shown in table 6 bacchi and pilotti 2000 in the following we will refer to q355 5 as this discharge corresponds to a drought situation that happens quite regularly in the watershed if snow melt and rainfall are both negligible the natural discharge reflects groundwater which is higher on the bottom of the valleys where however evapotranspiration is also higher accordingly although the validity of the assumption of a spatially uniform unitary discharge is not general for this preliminary screening we will assume that the value of q 355 5 is the result of a spatially uniform unitary discharge q 355 5 q 355 5 a 0 0097 m3 s km2 where a is the area of the watershed in km2 based on this unitary contribution q2t computes the discharge q j at each cell j of the drainage network as far as river width q 355 5 temperature of distributed q contribution and manning s coefficient are concerned q2t introduces some simplifying assumptions the width of the river is automatically computed as a function of the drained area according to eq 2 here we used ε 0 2 and δ 3 482 which were suggested by direct evaluations in the watershed they correspond to a width varying between 4 m in the upper part of the cn when the drained area is 1 km2 and 15 m at the basin outlet as far as the water temperature of the local unitary contribution q355 5 is concerned q2t provides a linear variation with elevation between a minimum value at the maximum elevation in the watershed and a maximum value at the watershed outlet considering that the average yearly temperature of the oglio watershed could range between 3 c at the summit of adamello glacier and 11 c at the outlet we made the assumpion that the linear variation can range between a minimum of 0 c and 11 c at the outlet the actual river temperature at each cell is provided by the balance of eq 9 and reflects the overall elevation of the drained watershed upstream as well as the other energy fluxes acting along the river course in this simulation we considered a typical summer day julian day 225 during which the actual daily averaged sw radiation at the ground is 245 w m2 and the average daily air temperature ranges between 2 c at the highest elevation and 23 c at the outlet with a relative humidity of 30 with these data the energy balance along the river provides a temperature at the outlet of 15 2 c in very good agreement with observations at august pilotti et al 2013 finally although q2t provides a linear variation of n with the drained area for simplicity sake a constant manning coefficient n 0 04 m 1 3s has been used based on this information the code automatically computes the tentative value of the local discharge in drought periods and the average velocity and depth according to chezy s equation eq 4 the following figures show the results from the output files produced by q2t as described in the companion tutorial of this example the upper panel of fig 8 shows the normalized altimetric profile of the river thalweg between point a station 82 km and b station 0 km of fig 7 the red line indicates the variation of the drained area along the reach under the assumption of constant unitary contribution the natural discharge varies along the river with the same pattern the blue line represents the river temperature t and has a more complex pattern superimposed on the rising pattern there are several abrupt t increases and drops these reflect the entrance of tributaries with water having a different temperature than the river for instance the drop at point i in the upper panel of fig 8 corresponds to the confluence of the cn with watershed i in fig 7 watershed i has an average elevation of 2332 m a s l whereas the average elevation of the watershed drained by the cn upstream of the confluence with i is 2219 m accordingly the drop is caused by the slightly colder temperature of the tributary waters from watershed i similarly but in the opposite direction at point ii there is the confluence with watershed ii see fig 7 with average elevation of 1653 m into the main river at a point where the average elevation of the drained watershed is 2066 m accordingly an abrupt temperature increase occurs slightly reinforced by the small pollutant outfall 2 in fact temperature variations can also be caused by heated effluents shown by vertical dashed lines in the upper panel of fig 8 for instance at point 1 in fig 8 one can observe that the mixing of 0 124 m3 s of sewage at 25 c data from table 5 with the natural discharge of 2 663 m3 s at 13 8 c data from q2t simulation causes an overall temperature rise of 0 51 c based on the computed velocity which varies locally as a function of the slope width and discharge it is possible to compute the transit time the cumulative pattern of which is shown in the panel b of fig 8 in dry period water takes almost 28 h to flow from point a to b with an average velocity of 0 8 m s from fig 9 it is evident that whereas the average normal depth keeps increasing from a to b albeit with strong local fluctuations due to the variations of slope the low flow normal velocity both computed with the chezy s equation 4 does not vary significantly moving from upstream to the outlet whereas moving downstream the local slope decreases and the cross section width increases this effect is compensated by the growth of the flow in panel a and b fig 10 shows the variation along the cn of the concentration of bod and of the different nitrogen species concentration increases when a point source discharges directly to the river but may decrease if an outfall is located on a tributary whose waters are cleaner than the ones of the main river this happens for outfall 11 as shown in fig 7 located on a lateral tributary of the selected ab reach of the cn and whose confluence with ab is shown at point 11j in figs 7 and 10 in general cbod concentration falls in correspondence to clean tributaries that dilute the polluted waters of the main river as clearly shown from the thin solid line in panel a indicating the decrease of cbod when only the outfall 1 is present along the cn in this case the cbod concentration would decrease in correspondence to each tributary confluence due to dilution with the incoming clean discharge panel c shows the variation of the oxygen and oxygen saturation concentration along the river in q2t the oxygen saturation varies as a function of water temperature and altitude according to the equations shown in table 2 this explains why the oxygen concentration in fresh clean water upstream of outfall 1 keeps increasing along the reach as the river elevation drops about 600 m see also fig 8 although q2t computes reaeration with o connor dobbins equation to make the discussion of the results simpler for this case the reaeration coefficient has been kept constant and equal to 20 d 1 as shown in panel d along with the sod dynamic notice how the unpolluted upper reaches have negligible sod in contrast beginning with outfall 1 considerable sod occurs due to settling of cbod and organic n the system of equations 30 35 is solved along the whole cn of the watershed and q2t provides the results along every path of the cn from an initial point to the outlet for instance if one selects point c of fig 7 as an initial point the results between points c and b are output these are shown in fig 11 upstream of point 11 oxygen tends to saturation and downstream of point 11j the pattern is the same shown in the first 15 kms of fig 10 with the same simplicity the whole cn can be explored with non trivial results occurring when different combination of inputs are present along the different branches that make up the overall cn of the watershed 4 discussion the increasing pressure exerted worldwide on water resources requires a growing awareness of the ecological implications of human activities high quality fresh water is a diminishing resource and every effort should be made to preserve it in the face of burgeoning human development in this spirit simple and effective water quality modeling tools can play a key role because they can be used to assess the implications of human exploitation and test different alternatives for water quality management and control however when large watersheds are considered the amount of data needed for a model set up can become daunting this is certainly true in developing countries where lack of funding can lead to the absence of institutional monitoring programs in these situations at least for initial screening purposes it is critical to exploit the information provided by all the available data sources some of these data inevitably require direct gauging for instance location and amount of point pollutant loads for others e g parameters of the chemical equations some reasonable literature guesses can be introduced finally a plethora of geomorphological information needed to model water transfer at the watershed scale can be derived by exploiting the informative content of digital elevation models which are available everywhere at low cost accordingly although it is not possible to eliminate the direct measurement and pre processing of field data the total effort can be considerably limited in this spirit the software q2t can be used to generate preliminary screening simulations guide future monitoring campaigns and inform more refined model development q2t reorganises the geomorphological information contained in a dem into efficient tree data structures sfdn and cn that naturally reproduce the watershed s drainage networks whereas other algorithms could be used to explore a drainage network in our experience a tree visiting algorithm provides a simple and powerful tool to simulate processes related to water transfer at the watershed scale the implemented water quality model that simulates the oxygen and nitrogen compounds dynamics at the dem cell scale in a flowing river has been positively compared with the outputs from a more complex model with two simple cases and then applied to a real and complex watershed the rationale of the comparison is the same as the basis of calibration of instruments that requires to compare the output of the unit under test here q2t with that provided by a standard here qual2k defined as a device with proven performance such comparisons are a formal part of sound software development called program verification fetzer 1988 1991 whereas the qual2k model is much more complex it is possible to generate simplified applications identical to case 1 and 2 by merely setting relevant qual2k model parameters and variables to zero the user guide and the companion material that is available for these two cases can be used to acquire proficiency with the simple application of q2t the valle camonica application a large alpine watershed in northern italy on the other hand demonstrates the type of insight and advantage that can be obtained with the proposed approach the model automatically accounts for the complex structure of the channel network that arises from the dem and the distributed nature of input data the topological and geometrical features of the network easily provide fundamental quantities drained area slope average elevation of each drained subcatchment etc within q2t basic equations e g chezy s law literature based relations e g the local channel width as a function of the drained area biochemical rate constants etc and assumptions related to watershed hydrology e g a unitary discharge contribution in drought periods provide throughout the cn a wealth of information local water depth velocity elevation travel times these in turn can then be employed to compute other coefficients and variables e g oxygen saturation reaeration coefficients water temperatures etc whose variation would be difficult to reproduce in more traditional modelling frameworks by quantifying and modeling this physically based complexity interesting effects emerge such as the increasing oxygen concentration upstream of the first pollutant outfall as the river s oxygen saturation continuously increases due to the rapid elevation drop see fig 10 panel c in conclusion in this example the model shows its capability to transmit the effects of contaminant sources originating in one location on water quality at other points downstream in the watershed accordingly it provides a straightforward way to single out the effects of local pollution sources from the effects of contaminants originating upstream along the cn undoubtedly other more complex approaches could reproduce point by point the complex situations that arise from the interplay of incoming discharges from the tributaries the residual local discharge coming from the slopes that directly drain at each cell and the pollutant outfalls however this would require a huge and costly information burden that could strongly limit or delay a holistic overview of water quality at the catchment scale as done by the algorithms implemented in q2t and would not be possible at all when there is a deficit of information the model presented in this paper and implemented in q2t is sufficiently simple that it could be exploited for preliminary screenings such as the ones to spot critical reaches along the cn that should be monitored during a planned measuring campaign or to explore the environmental consequences of different pollution and water diversion scenarios the output data produced by the model could be easily used e g in estimating the proportion of a cn where the concentration of a contaminant exceeds an established threshold in correspondence of different planned maf values the proposed approach could also be profitably employed by environmental authorities for an initial exploration of the environmental consequences of different water exploitation plans as well as industrial and civil waste permissions this could provide a simple method to contribute to the identification of the environmental flow required in a river for achieving a specific ecological target the framework s simplicity is based on several simplifying assumptions the model considers a steady state situation accordingly it cannot be used to study transient fluctuations such as daily variation nevertheless from the hydraulic point of view the steady state hypothesis is particularly appropriate for low flow periods when typically water quality is poorest and the computation of discharge based on a unitary contribution that mimics the effect of the river s aquifers is particularly valid moreover in these conditions the flow can be approximated as uniform at a local scale and its features can be reckoned based on the local values of slope cross section width and roughness in our steady state approach the groundwater temperature is provided as a function of elevation and the river water temperature is computed from a local heat balance another simplifying assumption is that the concentration is uniform across each cross section corresponding to full lateral and vertical mixing for this hypothesis to be true the cell dimension compared to the water depth and river width must be large accordingly contrary to other hydraulic uses of dem e g pilotti 2016 it would be wrong to use a very detailed dem for this type of application in the case of valle camonica where the typical width of the river is in the order of a dozen of meters during drought periods the square cell dimension is 160 m finally we have expressly designed the current framework for heavily polluted wastewater dominated systems of the type that are common in urbanizing developing countries such was the case in the early years of water quality modelling in north america and europe in the early twentieth century at that time the first models focused on untreated urban wastewater discharges of oxygen demanding pollutants first total bod and subsequently reduced nitrogen species as in our paper eutrophication was not the primary concern and plants were not modelled the major goal was to ameliorate anoxia and the attendant odors below raw wastewater point source discharges the current framework could readily be expanded to address further problems such as eutrophication ph metals etc but each of these additions would involve costly additional data collection although we anticipate adding such detail in the future doing it at this point would conflict with our primary goal of providing a free simple to use framework for preliminary water quality modelling of data poor river networks in developing countries it should be stressed that the application of our model should be viewed as a starting point for example given little or no observational data and literature values for the parameter the resulting model output would undoubtedly have high uncertainty nevertheless as pointed out above because it does incorporate proven scientific mechanisms and the drainage basin topology it would still be useful in a host of preliminary applications ranging from gross screening to informing subsequent data collection efforts most importantly it has the great upside of expediting a holistic basin wide perspective at the beginning of a study rather than as an afterthought or not at all by facilitating this global perspective at the outset we believe that subsequent efforts to refine the analysis and reduce uncertainty will be more efficiently realized in the long run coupled with the standard practices for model calibration and corroboration reckhow and chapra 1983 oreskes et al 1994 the framework could eventually be improved to the point that it could be used reliably for decision support users should reference the relevant literature on water quality model application to guide this evolution e g chapra 2003 as well as the key literature on uncertainty analysis that is of critical importance to such efforts omlin and reichert 1999 wagener et al 2001 etc this approach could be easily extended among many improvements an obvious extension would be to consider persistent non point waste sources that could be modelled along the sfdn with a similar approach and transferred to the cn for subsequent propagation to the watershed outlet pilotti and bacchi 1997 have shown how this can be done to model distributed soil erosion a process that is usually at the heart of nutrient export at the watershed scale 5 conclusions in this paper we have presented a steady state water quality model implemented in the free software q2t the water quality model is a deterministic distributed dissolved oxygen model that is based on the evolution of the bod model by streeter and phelps 1925 coupled with a simplified nitrogen cycle chapra 1997 and with an elegant analytical model of sediment water interactions di toro et al 1990 accordingly it considers the dynamics of six variables carbonaceous bod organic nitrogen ammonia nitrogen nitrate nitrogen dissolved oxygen and sediment oxygen demand the nonlinear equations of the chemical model are solved along the channel network that is automatically derived as a subset of the watershed s drainage network in turn obtained by the watershed s digital elevation model by application of a filtering criterion implemented in the software each cell of the channel network is regarded as a well mixed reactor where state variables are subject to mass balance and reactions the hydrological and hydraulic quantities needed by the model at each cell are automatically computed based on some simplified assumptions and transport between cells is idealized as pure advection the model works with steady state inputs potentially located along each node of the channel network these inputs can represent both point source reacting pollutants e g a wastewater treatment plant outfall and non point source i e local upwelling ground water in the river inputs with associated temperature oxygen and pollutant concentrations the model is positively tested with two geometrically simplified cases for which the space pattern of the chemical variables along the channel network are compared with those generated with the qual2k model finally as a third example the model is applied to a large watershed in the italian alps showing both its ease of application and its capability to transmit the effects of contaminant sources originating from one location on water quality at other points downstream in the watershed we believe that considering the simplified hypothesis used in its construction the model is particularly suitable to assess water quality objectives during critical pollution states characterised by low flows however the framework can serve as the starting point for broader applications such as more complex kinetic representations e g eutrophication pathogens etc and fully dynamic simulations we intend to proceed along these lines in future development of this scheme software availability the algorithm described in this paper has been implemented in a stand alone code q2t a name coming from the union of 2 keywords quality and tree like the topological structures used in the code and is freely available for technical and scientific non commercial purposes along with tutorials for reproduction of the three examples described herein the program is written in delphi s object pascal has a size of 3 7 mb and was tested under windows os versions 7 to 10 this code operates on a pre processed raster dem where all pits and flat areas have been eliminated such pre processing readily computed with most gis the program installation and application is straightforward and extensively described in the user guide which can be downloaded along with tutorials and the data needed to reproduce the three cases presented in the paper at http hydraulics unibs it hydraulics dati scaricabili water quality code conflicts of interest none acknowledgements we are grateful to three anonymous reviewers for their contributions to the improvement of this paper the participation of the second author scc was supported by the summer visiting professor program at the università degli studi di brescia italy 
26288,drainage system management relies on results from urban stormwater models errors in these models may have serious implications inaccurate field data or overly simplified models may cause the complex response of an urban basin to a rainfall event to be inadequately represented before undertaking expensive studies to gather and analyze additional data it is reasonable to understand what enhancement in model performance would result if individual uncertainties could be decreased this paper uses data collected during field monitoring campaigns to calibrate and validate a hydrologic and sediment transport model within the storm water management model swmm solid accumulation and disappearance rates were identified as factors generating the highest model sensitivity a generalized evaluation matrix is presented that considers both the uncertainty in input variables and the associated sensitivity in model response to inform model performance expectations and guide investments in model improvement toward actions with maximum benefit keywords model uncertainty sensitivity analysis model performance sediment build up and wash off swmm model 1 introduction urbanization typically leads to an increased percentage of impervious areas on catchment surfaces consequently the hydrologic and water quality responses of catchments become significantly altered hollis 1975 brabec et al 2002 pollutants generated from various anthropogenic activities and natural processes in urban environments accumulate on impervious surfaces during the dry weather period build up the kinetic energy of raindrops and the random motions created by the flowing water can mobilize pollutants and particles that have accumulated on impervious surfaces a phenomenon known as pollutant wash off wijesiri et al 2015 gorgoglione et al 2016a further the pollutants can be transported downstream it is therefore not surprising that urban stormwater is considered as a threat to the quality of receiving water bodies in urban areas han et al 2006 massoudieh et al 2010 cizek and hunt 2013 in this regard starting from the seventies several monitoring campaigns have been carried out to characterize the quality of stormwater runoff in urban areas and to analyze the build up wash off and transport phenomena of pollutants during wet periods in sewer systems sartor et al 1974 ellis and revitt 1982 saget et al 1996 vaze and chiew 2002 mckenzie and young 2013 a functional and comprehensive database of runoff quality is hard to collect mainly because of i high cost of the monitoring campaigns ii persistent dry weather that characterizes some areas and iii lack of reliability of the measurements due to occurrences of equipment malfunctions and or issues related to the equipment location gorgoglione et al 2016b in addition to monitoring campaigns accurate modeling tools aimed at the prediction of quantity and quality of drainage water are needed several simulation models have been developed e g swmm mike urban music etc these models consider pollutant build up and wash off processes as two independent sub models wijesiri et al 2016 the latter incorporate some conceptualizations and simplified assumptions to accommodate the combined effects of several influential factors into lumped parameters zoppou 2001 fiorentino et al 2006 chen and adams 2007 may and sivakumar 2009 mahbub et al 2011 iacobellis et al 2015 shorshani et al 2015 the use of a lumped approach for build up and wash off processes combined with inherent variability in site specific factors results in uncertainty in the modeling results together with the lack of accurate water quality data this modeling uncertainty can bias the interpretation of stormwater quality model outcomes furthermore it is worth noting that these models are characterized by a relatively large number of parameters for the process of prediction of the quantity and quality of drainage water it is always essential to assess the influence of various parameters on modeling results by performing a sensitivity analysis the level to which models accomplish their primary objective of satisfactorily representing real world processes is commonly judged by pairwise comparison of observed data and model output using well known statistics and with a graphical comparison of measured and simulated values as usually applied most quantitative goodness of fit indicators employ the simple difference to represent the discrepancy between measured and predicted data in an excellent overview engel et al 2007 described the content necessary to develop a model application protocol or modeling quality assurance plan which is needed to improve the scientific effectiveness of models and to increase the validity of model applications in light of research and regulatory implications regarding calibration and validation procedures engel et al 2007 highlighted the need to evaluate model goodness of fit and to evaluate the uncertainty in model results and observed data zamani and bombardelli 2014 however no universally accepted methods to assess model goodness of fit that incorporate both model uncertainty and sensitivity analysis are currently available based on these considerations this work initially presents the data collected during a monitoring campaign carried out within two residential areas located in folsom sacramento county california the rainfall and flow data collected were exploited to calibrate and validate the hydrologic model implemented in the storm water management model swmm aiming at having a hydrologic and water quality model with accuracy and reliability the first objective of this work is i to develop an efficient hydrologic and sediment transport model in swmm calibrated using the suspended solid data collected such robust and reliable models are essential for accomplishing the other two objectives of this work ii to understand the sources of uncertainty associated with sediment build up and wash off processes in urban areas and to analyze the impact that such uncertainty has on water quality model predictions and iii to develop a methodology able to assess and predict the accuracy and reliability of water quality models by considering model uncertainty and sensitivity the outcomes presented in this manuscript are expected to aid decision makers in assessing and quantifying their confidence in the predicted values which facilitates informed analysis communication and decision making the remainder of this paper is organized as follows in section 2 the study area and the observed data are described and the main features of swmm are summarized in section 3 the hydrologic and water quality modeling outcomes are reported in section 4 after presenting the results of the sensitivity analysis of swmm parameters and discussing the sources of uncertainty a method to assess the accuracy and reliability of water quality models is described the main conclusions are presented in section 5 it is worth remarking that this work represents the first part of a project that had the aim of building and testing at the watershed scale a new physically based model able to mathematically characterize and predict the sediment facilitated transport of pesticides from residential surfaces gorgoglione et al 2018 2 material and methods 2 1 description of the study area the study area includes two watersheds located in folsom sacramento county california the two basins called folsom 1 f1 and folsom 2 f2 in the current work are adjacent but physically separated by some terrain features and lack of connecting roadways f1 is located at marsh hawk drive between widgeon court and mcadoo drive the collection site is on marsh hawk drive on the opposite side and upstream from a large detention pond the drainage area with a surface equal to 21 634 ha possesses by 10 737 ha 49 6 of impervious surface consisting of 6 186 ha of pavement area 28 6 and 4 551 ha of structure area 21 the rest of the basin is covered by tree canopy 2 704 ha 12 5 and therefore is characterized as pervious area 8 193 ha 37 9 the separate drainage network has a 48 inch cement culvert pipe buried below a hillslope that discharges into a small depression f2 is located at the end of brock circle the drainage area with a surface equal to 30 202 ha features 15 832 ha 52 4 of impervious surface 9 862 ha of pavement area 32 7 and 5 970 ha of structure area 19 8 as for f1 the rest of the basin is covered by tree canopy 1 813 ha 6 0 and is characterized as pervious area 12 557 ha 41 6 also for f2 the drainage network is used only for stormwater and the flow comes out of a 48 inch cement culvert pipe and drops through a series of rock gabions in fig 1 the boundaries of the two watersheds and the respective discharge points are shown 2 2 equipment used for each catchment the equipment included a rain gauge an area velocity sensor and an autosampler installed at the discharge point of the drainage network in fig 2 the equipment installed at f2 is shown for discharge monitoring a hach sigma 950 flow meter hach american sigma loveland co was installed this has a portable datalogger that is completely self contained and that is suitable for recording and measuring flow in open channels and full pipes a bubbler depth sensor is used to directly measure the water depth in the runoff channel the 950 bubbler sensor can measure depths up to 3 58 m with a resolution of 0 033 m in addition the flow meter can measure the average velocity of the flow stream by using a submerged acoustic doppler velocity sensor and calculate flow based on the current level and the definition of discharge 1 wetted area velocity flow the velocity sensor is a velocity only wafer sensor hach american sigma loveland co it requires a minimum water depth of 0 02 cm can measure up to 6 10 m s with a resolution of 2 of the reading a tipping bucket rain gauge hach american sigma loveland co was installed at the end of a 3 m galvanized steel pole and recorded rainfall in 0 0254 cm increments water quality samples from both sites were collected using hach sigma 900 max autosamplers hach american sigma loveland co with the objective of obtaining sediment event mean concentration emc values smullen et al 1999 sample collection intervals were determined from this estimation and used to program the auto samplers to obtain flow weighted composite samples to ensure the ground was sufficiently wet to generate runoff 0 20 cm of rainfall was necessary to trigger sampling for the first storm event of the season in subsequent events during which the ground was assumed to be sufficiently wet rainfall in excess of 0 05 cm triggered sampling the pacing volumes used to program the auto samplers were based on collecting 50 samples of 400 ml each resulting in a total of 20 l however the autosamplers were actually programmed to collect 75 samples in case the storm duration or intensity was greater than predicted samples were typically smaller than 20 l because storm duration was shorter than expected or equipment malfunction occurred 2 3 experimental data collected the monitoring campaign produced records of rainfall flow rate and total suspended solids tss for storm events that occurred from 2007 to 2014 11 events at f1 and 20 events at f2 rainfall runoff data relating to all the events including total rainfall depth mm rainfall event duration hr lenght of antecedent dry weather day total runoff volume m3 runoff peak flow rate m3 s and total depth measured at the outfall m are summarized in table 1 the antecedent dry period is evaluated by counting days without precipitation since the previous storm the total depth is calculated by summing up all measurements of water depth during a rainfall event the most recent events in 2014 are characterized by the highest resolution 1 min all other events are characterized by 2 min or 15 min of resolution as expected rainfall runoff data collected at f1 and f2 show similar characteristics only two events at f2 6 dec 2007 and 26 oct 2010 are characterized by high values of runoff volume runoff peak and total depth it is assumed that these data may contain sampling errors neglecting these two intense events in f1 and f2 the mean values of runoff volume runoff peak and total depth are very similar they are equal to 4239 24 m3 0 32 m3 s 190 79 m at f1 and to 4876 17 m3 0 31 m3 s 81 55 m at f2 the monitoring campaign at folsom provided records of tss for 17 events by measuring tss event mean concentration emc defined as 2 e m c i 1 n c i v i v where v is the total runoff volume per event l v i is the runoff volume during time period i l c i is the pollutant concentration during time period i mg l and n is the total number of samples during a single storm event in this work the event mean load eml was considered for all sampled events as the sum of pollutant loads determined by multiplying each discrete concentration c i within an event with its corresponding runoff volume v i 3 e m l e m c v details of measured tss emc and the respective eml values are presented in table 2 for two events eml could not be calculated because corresponding flow data was lacking 2 4 correction to flow rate water depth relations data loggers at each monitoring site measured both water depth and velocity with flow rate calculated from these two measurements in open channel flow such as the flow occurring in drainage pipes monitored for this study a well defined relationship exists between water depth and flow under uniform flow conditions chow 1959 when evident this relationship becomes a sign that data loggers would be operating properly therefore establishing this correlation is always a reasonable first step in assuring the quality of the data collected bale et al 2017 for the watersheds f1 and f2 data from the on site data loggers did not always conform to this relationship in particular it was possible to identify two groups of data the first one seemed to follow a particular law the second one was a confused cluster of data points several factors contribute to the breakdown of this relationship in the dataset including erroneous velocity readings attributed to sensor malfunctions and depth readings caused by backwater resulting from downstream ponding a significant amount of effort was devoted to identifying and correcting these problems to generate reliable flow datasets initially the data points belonging to the confused cluster were neglected in this way a stage discharge relationship was developed to identify the trend of the first group of data points the best approximation to the flow rate and water depth data was obtained using a relationship described by two 2nd order polynomial laws one for each basin 4 q 0 2234 y 2 1 3945 y 1 6989 y 0 02 m at f 1 5 q 0 2107 y 2 1 4951 y 1 7541 y 0 02 m at f 2 where q is the flow lps and y is the water depth cm each data point was identified as corresponding to the respective hyetograph and a time during the rainfall event for all of the events the majority of the data points deviating significantly from eq 4 or 5 i e belonging to the confused cluster were from the end of the event since the flow meter relies on the doppler effect it is possible that these outliers result from errors arising from the lower concentration of reflectors i e suspended particles by the end of the event it is believed that the remaining portion of outliers arose from potential sensor malfunctions because they characterized peak flows that occurred while the hyetograph was equal to zero this methodology was initially applied to the events characterized by the highest resolution 1 min then to test its reliability it was extended to all events that are characterized by 2 min or 15 min of resolution in fig 3 measurements of water depth discharge curves for the most recent rainfall events 2014 for f1 fig 3a and f2 fig 3b after removing the outliers identified with the methodology explained above are shown the agreement among field data without outliers and the two regressions eqs 4 and 5 was evaluated through the coefficients of determination r2 defined in the supplementary materials which were 0 99 at f1 figs 3a and 0 95 at f2 fig 3b 2 5 modeling approach 2 5 1 brief description of swmm the epa s swmm is a comprehensive hydrologic and water quality simulation model developed primarily for urban areas huber and dickinson 1988 it obtains the hydrograph and the pollutograph for a storm event for single and long term events on the basis of rainfall and other meteorological inputs snow wind and system characteristics catchment conveyance and storage treatment rossman 2010 swmm has been designed in blocks or operating units each block can be used individually or in a cascade and an executive block coordinates their outputs the runoff block as well as the transport block were utilized for this study quality processes in the same unit include generation of surface runoff constituent loads through the build up of pollutants during dry weather and wash off during wet weather the mathematical representation of these two physical processes is explained in section 2 5 3 by using inlet hydrographs generated from the runoff unit the transport block executes the flow and pollutant routing through the drainage network in the transport unit flow routing is accomplished using the kinematic wave obtained when there is a balance between the gravitational and frictional forces in the momentum equation diffusive non inertia wave when pressure forces are important in addition to gravity and frictional forces in the momentum equation or dynamic wave methods obtained when all forces are important represented by the complete momentum equation in turn water quality processes include first order decay and simulation of resuspension and deposition within the sewer system gorgoglione 2016 2 5 2 swmm implementation the first step in implementing the model in swmm was to define watershed subcatchments and hydrologic connectivity standard methods for performing watershed and subcatchment delineation in an undeveloped landscape such as the use of geographic information system gis and digital elevation models dems are not completely effective in an urban environment because the natural topography and hydrologic flow paths are often altered by roads structures and stormwater collection systems therefore to correctly represent the contributing area and hydrologic connectivity in urbanized areas like the ones in folsom f1 and f2 additional data were needed a stormwater collection system map was obtained from the california department of pesticide regulation cdpr this collection map shows the locations of storm drains subsurface culverts outfalls and it also describes culvert characteristics including their diameter length and connectivity in fig 4 the two watersheds embedded in swmm with their respective drainage networks and discharge points are shown to simulate the runoff from urban surfaces the dynamic wave equation was chosen furthermore the water losses taken into account are represented by the depression storage on the impervious portion of the basin and the infiltration process the latter was modeled by evaluating for each subcatchment the percentage of impervious and pervious area obtained from the land use map the infiltration model utilized in this work was based on horton s equation whose parameter values have been chosen according to the representative values reported in the literature in relation to soil type horton 1940 2 5 3 sediment build up wash off and transport swmm supports simulation of sediment accumulation on impervious surfaces during dry weather periods tss build up within a land use category is described by a mass per unit of subcatchment area the amount of build up is a function of the number of dry weather days antecedent to the rainfall event based on the assumptions of alley and smith 1981 the build up function follows a growth law that asymptotically approaches a maximum limit 6 m a d a d p a c c u d i s p a p i m p 1 e d i s p d a d p where m a d a d p represents the pollutant build up during the antecedent dry period kg ha d i s p is the parameter that measures the disappearance of accumulated solids due to the action of wind or vehicular traffic 1 d p i m p is the impervious area fraction a c c u is the parameter that characterizes the solids build up rate kg ha d a c c u d i s p a p i m p represents the maximum asymptotic limit of the build up curve the pollutant wash off over different land uses takes place during wet periods and it is described by the jewell and adrian 1978 differential equation 7 d m d t d t a r r a i t w a s h m a t where d m d t d t is the wash off load rate kg h a r r a is the wash off coefficient mm 1 i t is the runoff rate mm h w a s h is the wash off exponent a parameter that controls the influence of rainfall intensity on the amount of leached pollutants swmm calculates the spatial and temporal trend of pollutant concentrations in the drainage network assuming that the conduits behave as ideal completely mixed flow reactors cmfrs the control volume the reactor volume coincides with the conduit volume inside the reactor the mathematical balance is obtained from a macroscopic material mass balance di modugno et al 2015 8 d v c d t q i n c i n q o u t c o u t k v c o u t in which v represents the water volume in the conduit reactor calculated at each time step m3 q in is the inflow in the conduit m3 s c in is the sediment concentration at the inlet of the conduit mg l q out is the outflow to the conduit m3 s c out is the sediment concentration in volume v at the outlet of the conduit mg l and k is the decay coefficient s 1 2 6 uncertainty analysis in the context of modeling two main types of uncertainties δ can be identified that influence the uncertainty of model outcomes δ results loucks and beek 2005 asme ptc committee 2009 i model uncertainty δ model and ii measurement uncertainty δ meas as follows 9 δ r e s u l t s δ m o d e l δ m e a s in particular δ model is identified as the sum of three types of uncertainty asme ptc committee 2009 i theoretical model uncertainty δ theor model the uncertainty due to simplifications and approximations involved in the mathematical representations ii parameter uncertainty δ param the uncertainty in the simulation results related to the estimation of the model parameters and iii numerical schemes and solutions uncertainty δ ns the uncertainty associated with the numerical schemes and solutions of the equations as follows 10 δ m o d e l δ t h e o r m o d e l δ p a r a m δ n s any misunderstanding of these factors will lead to untrustworthy outputs which could cause incorrect interpretations of the pollutant build up and wash off processes uusitalo et al 2015 in this work the uncertainty of model parameter values δ p a r a m is considered the main source of uncertainty in stormwater quality model outputs it is assumed that the potential uncertainty in the numerical schemes and solutions by swmm is controlled and we consider that theoretical model is valid as it also has low uncertainty therefore the parameter uncertainty is addressed and analyzed in detail on the basis of the on site data regarding rainfall collected during the monitoring campaign a low uncertainty associated with d adp is hypothesized on the other hand for the other four parameters accu disp arra and wash a wide range of variation was evaluated from the scientific literature 3 results 3 1 hydrologic simulations eight parameters of the runoff block were used to calibrate the hydraulic hydrologic model the depth of depression storage on impervious dstore imperv and pervious dstore perv portions of the subcatchment manning s coefficient for overland flow over the impervious n imperv and pervious n perv portions of the subcatchment the percent of impervious area without depression storage zero imperv and the infiltration parameters of horton s equation the range of variation and the values chosen for these parameters are shown in table 3 asce 1992 rossman 2010 the calibration of the two models employed an iterative process by varying the parameters in table 3 working within the established range and comparing numerically and statistically in table 4 and graphically in fig 5 the simulation with the measured hydrograph the calibration was performed until a good fit was obtained the most recent events characterized by the highest resolution 1 min were used for the calibration process all the other events characterized by 2 min or 15 min of resolution were used for the validation process the goodness of fit indicators used in table 4 to evaluate the model performance are described in the supplementary materials agreement between measured and simulated hydrographs for almost all simulations was very satisfactory poor performance was found for only two of the 31 simulations for the rainfall events of 6 dec 2007 and 26 oct 2010 at f2 for such events the simulated flow is one order of magnitude lower than the measured one unless parameter values outside their suitable physical ranges are used these events were considered intense events by taking into account their runoff volume table 1 3 2 sediment transport simulations four parameters of the runoff block were identified for the calibration of the sediment transport model for the build up function eq 6 the parameter that characterizes the solids build up rate accu and the parameter that identifies the disappearance of accumulated sediments due to the action of the wind or vehicular traffic disp for the wash off function eq 7 the wash off coefficient arra and the wash off exponent wash since a particle size distribution is lacking in this study with the aim of assigning a realistic value to the decay coefficient k of the transport formula eq 8 a range of particle sizes common in urban areas was assumed particle settling velocities were calculated using stokes law and divided by the mean value of the water depth at f1 and f2 the outcomes obtained from the simulations launched with this range of k values suggested that in the drainage network of the two watersheds the sediment resuspension and the settling processes would be counterbalanced e g in each conduit the vertical motion of the particles would be negligible compared to the horizontal one for this reason k was assumed to be zero and was not considered for the calibration process the values of the sediment transport parameters obtained from the calibration and their range of variation are presented in table 5 the range of variation considered for each parameter was defined using 11 previous studies that involve one or more urban basins similar to f1 and f2 baffaut and delleur 1990 warwick and tadepalli 1991 gaume et al 1998 papiri 2000 ciaponi et al 2002 temprano et al 2006 cambez et al 2008 rossman 2009 candela et al 2012 mambretti and sanfilippo 2012 di modugno et al 2015 as we did for the hydraulic hydrologic model calibration was performed via an iterative process by adjusting the parameters listed in table 5 the events characterized by the highest resolution were used for the calibration all the others were used for validating the model the results of the calibration process are shown in figs 6 and 7 which compare the observed and simulated emls and the correlation between sediment load observations and simulations respectively the coefficient of determination r 2 and pearson coefficient r were evaluated and they are respectively equal to 0 62 and 0 79 these goodness of fit indicators were evaluated for all the events with tss eml data excluding the two intense events monitored at f2 the agreement among measured and simulated emls is not as good as that of the hydrologic model results however this is frequently observed in sediment related studies julien 2002 parker 2004 garcìa 2008 4 discussion 4 1 impact of parameter uncertainty on model results considering the range of variation of each parameter found in the literature and reported in table 5 a sensitivity analysis was executed on their mean values to provide a general assessment of model accuracy this information is valuable to those who make design decisions the analysis was performed by adjusting one parameter at a time in fixed percentage increments the corresponding difference in tss emc and tss peak was calculated and plotted fig 8 based on eqs 6 and 7 the parameters considered for this analysis are accu disp d adp arra and wash starting from the set of parameters characterized by the mean value that they can assume per subcatchment one parameter was varied at a time by using the chosen value of change in parameter reported in table 6 the parameters that showed greater sensitivity were accu and disp since the latter is present in both the exponential and maximum build up a c c u d i s p a p i m p factors in eq 6 it is not surprising that disp is one of parameters to which outcomes are the most sensitive whereas arra and wash also have important impacts on tss peak their impact on tss emc are less significant variations in d adp over a range of 80 do not produce significant changes in the tss peak or tss emc 4 2 coupling sensitivity and uncertainty analysis after analyzing the sensitivity of modeling outcomes to water quality parameters this study presents a methodology for combining sensitivity and uncertainty analysis this approach exploits the results of the sensitivity analysis to decrease the model uncertainty aiming to assess and predict the overall accuracy and reliability of water quality models the outcomes of the sensitivity analysis support the classification of the five water quality parameters into three different classes low medium and high fig 9 in addition in fig 9 the interval of change in parameter with respect to the parameter limits proposed in table 5 in which each of these parameters shows its sensitivity level with the corresponding change in emc are presented for the purpose of this study among these three classes the one characterized by medium sensitivity arra was excluded since it represents an average condition that reveals information that can be located among the results obtained from the classes considered although the sensitivity analysis was performed to assess the influence of the five water quality parameters on both tss emc and peak only the tss emc is considered further because tss samples during the monitoring campaign conducted in f1 and f2 were collected as composites to proceed with the combination of sensitivity and uncertainty of the water quality parameters the extreme values of the interval of change in parameter were considered being the most representative of the entire range of variation clearly the interval of change in parameter is not the same for all the parameters since i the range of variation is different from one parameter to another and ii within these ranges at least an 80 variation was considered since disp varies between 73 the entire range of its variation was considered in particular in table 6 the change in parameter with the corresponding change in emc that were taken into account are shown to evaluate the accuracy of the model simulations three widely used goodness of fit indicators nash sutcliffe efficiency nse root mean square error rmse and kling gupta efficiency kge were assessed in particular an indicator ratio was evaluated by dividing each indicator by its corresponding value when the set of parameters took on their mean values table 7 to better understand how these ratios were calculated we can consider as an example n s e r a t i o calculated for d a d p 80 11 n s e r a t i o n s e a d p 80 n s e m e a n v a l u e s f o r d a d p 80 where n s e a d p 80 represents the nse between observed and simulated tss values obtained by varying the antecedent dry period value by 80 and keeping the value of all the other variables equal to the mean value of their range of variation n s e m e a n v a l u e s is the nse between observed and simulated tss values obtained by keeping the value of all the variables equal to the mean value of their range of variation the closer these ratios are to 1 the better the performance of the water quality model in table 7 these ratios obtained by considering the lower change in parameter value chosen and highlighted in table 6 are shown since the trend of the sensitivity plots fig 8 is not symmetric the same evaluations were also assessed by considering the upper change in parameter value chosen whereas these results are very close to the ones obtained with the lower value for brevity only the latter are reported in the following table by coupling parameter sensitivity and uncertainty in the input variables a model assessment matrix was developed to assist users in making appropriate model performance conclusions and to guide investments in model improvement fig 10 the matrix presents conclusions for overall model performance that include model accuracy goodness of fit and uncertainty of the model response in the model assessment matrix four general cases based on qualitative indications of sensitivity and uncertainty of the water quality variables are presented when parameters have low variability the model accuracy goodness of fit can be confidently concluded because the standard of comparison is relatively certain and a good very good uncertainty of the model response is obtained in particular by coupling this low parameter variability with low parameter sensitivity good satisfactory uncertainty of the model response is obtained in swmm this is the case of the parameter d adp that characterizes the build up process although there are no parameters characterized by high sensitivity and low variability in our implementation of swmm it is reasonable to assume that low variability combined with high sensitivity would produce intermediate model uncertainty for the case with high parameter variability and high parameter sensitivity model uncertainty will be large and overall model performance will be poor in swmm this is the case of the parameters accu and disp that regulate the build up process high parameter variability may produce an intermediate degree of uncertainty in the model response when it is coupled with low parameter sensitivity in swmm the parameter wash that characterizes the wash off process belongs to this case in addition to making forward predictions of expected model uncertainty given various variability sensitivity combinations fig 10 can be used to direct investments to reduce model uncertainties improved measurement or independent assessments of parameters with high variability and high sensitivity will produce the greatest return on investment while improvements in parameter measurement to reduce variability and uncertainty will be the least valuable for cases where both the range of parameter variability and model sensitivity to that parameter are low although model sensitivity is routine considered in model formulation the combination of input parameter variability and sensitivity offers improved insights regarding model performance and potential for improvements 5 conclusions the data collected during a monitoring campaign carried out within two residential areas located in folsom sacramento county california have been presented in this work and used as an opportunity to present a framework for the assessment of model uncertainty datasets encompass eight years which is unusual the rainfall and flow data collected were exploited to calibrate and validate the hydrologic model implemented in swmm because the methodology evaluated and eventually identified possible outliers among flow measurements an accurate and reliable hydrologic model was developed the numerical comparison between observed and simulated hydrographs was made by evaluating r 2 rmse and nse for each rainfall event by means of this accurate model an efficient sediment transport model was calibrated and validated in swmm by using the total suspended solid data collected during the monitoring campaign the agreement between measured and simulations of this water quality model was estimated by evaluating the coefficient of determination r 2 and pearson coefficient r an in depth understanding of the possible sources of uncertainty has outlined key knowledge required to analyze the impact that such uncertainty associated with sediment build up and wash off processes in urban areas has on water quality model predictions furthermore considering the range of variation of each parameter a sensitivity analysis was executed on their mean values to provide a general assessment of model accuracy accu and disp were the parameters that showed greater sensitivity by coupling uncertainty and sensitivity analysis and combining them with an underlying reliable sediment transport model a methodology that aimed to assess and predict the overall accuracy and reliability of water quality models was developed a model assessment matrix was created to assist users in making appropriate model performance conclusions it is characterized by four general cases based on qualitative indications of sensitivity and uncertainty of the sediment transport parameters for instance it was found that for the case with high parameter variability coupled with high parameter sensitivity in swmm this is the case of accu and disp an unsatisfactory uncertainty of the model response may be obtained these together indicate low accuracy and overall poor model performance in terms of uncertainty of the model response as a result the matrix presents appropriate conclusions for overall model performance that include model accuracy goodness of fit and uncertainty of the model response software availability storm water management model swmm is freely downloadable from https www epa gov water research storm water management model swmm downloads swmm was first developed in 1971 and has undergone several major upgrades since then in this study we are using the version 5 0 swmm 5 the development of swmm 5 was pursued under a cooperative research and development agreement between the water supply and water resources division of the u s environmental protection agency and the consulting engineering firm of camp dresser mckee inc the project team consisted of the following individuals lewis rossman contact information unknown trent schade contact information unknown daniel sullivan contact information unknown from us epa robert dickinson e mail robert dickinson gmail com address 9340 pontiac drive tampa florida 33626 phone 813 712 0664 carl chan e mail chancc cdm com address 151 north delaware st suite 1520 indianapolis in 46204 edward burgess e mail burgesseh cdm com address 8805 governor s hill drive suite 260 cincinnati oh 45249 from cdm swmm 5 is designed to run under the windows operating system of an ibm intel compatible personal computer the program language is c and the size is 5 mb conflicts of interest the authors declare that there is no conflict of interests regarding the publication of this paper acknowledgments this research was supported by the california department of pesticide regulation under contract number 14 c0029 with t m y and f a b as pis the project officer dr yuzhou luo contributed valuable modeling insight and guidance during the development of this work michael ensminger and robert budd from dpr provided assistance with information and data related to the dpr monitoring programs author contributions a g f a b and t m y conjointly designed and undertook the presented modeling research b j l p l r o and d l h collected and provided the monitoring data for flow and suspended solids from the f1 and f2 monitoring sites all authors have read and approved the final manuscript appendix a supplementary data the following is the supplementary data to this article supplementary materials supplementary materials appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 022 
26288,drainage system management relies on results from urban stormwater models errors in these models may have serious implications inaccurate field data or overly simplified models may cause the complex response of an urban basin to a rainfall event to be inadequately represented before undertaking expensive studies to gather and analyze additional data it is reasonable to understand what enhancement in model performance would result if individual uncertainties could be decreased this paper uses data collected during field monitoring campaigns to calibrate and validate a hydrologic and sediment transport model within the storm water management model swmm solid accumulation and disappearance rates were identified as factors generating the highest model sensitivity a generalized evaluation matrix is presented that considers both the uncertainty in input variables and the associated sensitivity in model response to inform model performance expectations and guide investments in model improvement toward actions with maximum benefit keywords model uncertainty sensitivity analysis model performance sediment build up and wash off swmm model 1 introduction urbanization typically leads to an increased percentage of impervious areas on catchment surfaces consequently the hydrologic and water quality responses of catchments become significantly altered hollis 1975 brabec et al 2002 pollutants generated from various anthropogenic activities and natural processes in urban environments accumulate on impervious surfaces during the dry weather period build up the kinetic energy of raindrops and the random motions created by the flowing water can mobilize pollutants and particles that have accumulated on impervious surfaces a phenomenon known as pollutant wash off wijesiri et al 2015 gorgoglione et al 2016a further the pollutants can be transported downstream it is therefore not surprising that urban stormwater is considered as a threat to the quality of receiving water bodies in urban areas han et al 2006 massoudieh et al 2010 cizek and hunt 2013 in this regard starting from the seventies several monitoring campaigns have been carried out to characterize the quality of stormwater runoff in urban areas and to analyze the build up wash off and transport phenomena of pollutants during wet periods in sewer systems sartor et al 1974 ellis and revitt 1982 saget et al 1996 vaze and chiew 2002 mckenzie and young 2013 a functional and comprehensive database of runoff quality is hard to collect mainly because of i high cost of the monitoring campaigns ii persistent dry weather that characterizes some areas and iii lack of reliability of the measurements due to occurrences of equipment malfunctions and or issues related to the equipment location gorgoglione et al 2016b in addition to monitoring campaigns accurate modeling tools aimed at the prediction of quantity and quality of drainage water are needed several simulation models have been developed e g swmm mike urban music etc these models consider pollutant build up and wash off processes as two independent sub models wijesiri et al 2016 the latter incorporate some conceptualizations and simplified assumptions to accommodate the combined effects of several influential factors into lumped parameters zoppou 2001 fiorentino et al 2006 chen and adams 2007 may and sivakumar 2009 mahbub et al 2011 iacobellis et al 2015 shorshani et al 2015 the use of a lumped approach for build up and wash off processes combined with inherent variability in site specific factors results in uncertainty in the modeling results together with the lack of accurate water quality data this modeling uncertainty can bias the interpretation of stormwater quality model outcomes furthermore it is worth noting that these models are characterized by a relatively large number of parameters for the process of prediction of the quantity and quality of drainage water it is always essential to assess the influence of various parameters on modeling results by performing a sensitivity analysis the level to which models accomplish their primary objective of satisfactorily representing real world processes is commonly judged by pairwise comparison of observed data and model output using well known statistics and with a graphical comparison of measured and simulated values as usually applied most quantitative goodness of fit indicators employ the simple difference to represent the discrepancy between measured and predicted data in an excellent overview engel et al 2007 described the content necessary to develop a model application protocol or modeling quality assurance plan which is needed to improve the scientific effectiveness of models and to increase the validity of model applications in light of research and regulatory implications regarding calibration and validation procedures engel et al 2007 highlighted the need to evaluate model goodness of fit and to evaluate the uncertainty in model results and observed data zamani and bombardelli 2014 however no universally accepted methods to assess model goodness of fit that incorporate both model uncertainty and sensitivity analysis are currently available based on these considerations this work initially presents the data collected during a monitoring campaign carried out within two residential areas located in folsom sacramento county california the rainfall and flow data collected were exploited to calibrate and validate the hydrologic model implemented in the storm water management model swmm aiming at having a hydrologic and water quality model with accuracy and reliability the first objective of this work is i to develop an efficient hydrologic and sediment transport model in swmm calibrated using the suspended solid data collected such robust and reliable models are essential for accomplishing the other two objectives of this work ii to understand the sources of uncertainty associated with sediment build up and wash off processes in urban areas and to analyze the impact that such uncertainty has on water quality model predictions and iii to develop a methodology able to assess and predict the accuracy and reliability of water quality models by considering model uncertainty and sensitivity the outcomes presented in this manuscript are expected to aid decision makers in assessing and quantifying their confidence in the predicted values which facilitates informed analysis communication and decision making the remainder of this paper is organized as follows in section 2 the study area and the observed data are described and the main features of swmm are summarized in section 3 the hydrologic and water quality modeling outcomes are reported in section 4 after presenting the results of the sensitivity analysis of swmm parameters and discussing the sources of uncertainty a method to assess the accuracy and reliability of water quality models is described the main conclusions are presented in section 5 it is worth remarking that this work represents the first part of a project that had the aim of building and testing at the watershed scale a new physically based model able to mathematically characterize and predict the sediment facilitated transport of pesticides from residential surfaces gorgoglione et al 2018 2 material and methods 2 1 description of the study area the study area includes two watersheds located in folsom sacramento county california the two basins called folsom 1 f1 and folsom 2 f2 in the current work are adjacent but physically separated by some terrain features and lack of connecting roadways f1 is located at marsh hawk drive between widgeon court and mcadoo drive the collection site is on marsh hawk drive on the opposite side and upstream from a large detention pond the drainage area with a surface equal to 21 634 ha possesses by 10 737 ha 49 6 of impervious surface consisting of 6 186 ha of pavement area 28 6 and 4 551 ha of structure area 21 the rest of the basin is covered by tree canopy 2 704 ha 12 5 and therefore is characterized as pervious area 8 193 ha 37 9 the separate drainage network has a 48 inch cement culvert pipe buried below a hillslope that discharges into a small depression f2 is located at the end of brock circle the drainage area with a surface equal to 30 202 ha features 15 832 ha 52 4 of impervious surface 9 862 ha of pavement area 32 7 and 5 970 ha of structure area 19 8 as for f1 the rest of the basin is covered by tree canopy 1 813 ha 6 0 and is characterized as pervious area 12 557 ha 41 6 also for f2 the drainage network is used only for stormwater and the flow comes out of a 48 inch cement culvert pipe and drops through a series of rock gabions in fig 1 the boundaries of the two watersheds and the respective discharge points are shown 2 2 equipment used for each catchment the equipment included a rain gauge an area velocity sensor and an autosampler installed at the discharge point of the drainage network in fig 2 the equipment installed at f2 is shown for discharge monitoring a hach sigma 950 flow meter hach american sigma loveland co was installed this has a portable datalogger that is completely self contained and that is suitable for recording and measuring flow in open channels and full pipes a bubbler depth sensor is used to directly measure the water depth in the runoff channel the 950 bubbler sensor can measure depths up to 3 58 m with a resolution of 0 033 m in addition the flow meter can measure the average velocity of the flow stream by using a submerged acoustic doppler velocity sensor and calculate flow based on the current level and the definition of discharge 1 wetted area velocity flow the velocity sensor is a velocity only wafer sensor hach american sigma loveland co it requires a minimum water depth of 0 02 cm can measure up to 6 10 m s with a resolution of 2 of the reading a tipping bucket rain gauge hach american sigma loveland co was installed at the end of a 3 m galvanized steel pole and recorded rainfall in 0 0254 cm increments water quality samples from both sites were collected using hach sigma 900 max autosamplers hach american sigma loveland co with the objective of obtaining sediment event mean concentration emc values smullen et al 1999 sample collection intervals were determined from this estimation and used to program the auto samplers to obtain flow weighted composite samples to ensure the ground was sufficiently wet to generate runoff 0 20 cm of rainfall was necessary to trigger sampling for the first storm event of the season in subsequent events during which the ground was assumed to be sufficiently wet rainfall in excess of 0 05 cm triggered sampling the pacing volumes used to program the auto samplers were based on collecting 50 samples of 400 ml each resulting in a total of 20 l however the autosamplers were actually programmed to collect 75 samples in case the storm duration or intensity was greater than predicted samples were typically smaller than 20 l because storm duration was shorter than expected or equipment malfunction occurred 2 3 experimental data collected the monitoring campaign produced records of rainfall flow rate and total suspended solids tss for storm events that occurred from 2007 to 2014 11 events at f1 and 20 events at f2 rainfall runoff data relating to all the events including total rainfall depth mm rainfall event duration hr lenght of antecedent dry weather day total runoff volume m3 runoff peak flow rate m3 s and total depth measured at the outfall m are summarized in table 1 the antecedent dry period is evaluated by counting days without precipitation since the previous storm the total depth is calculated by summing up all measurements of water depth during a rainfall event the most recent events in 2014 are characterized by the highest resolution 1 min all other events are characterized by 2 min or 15 min of resolution as expected rainfall runoff data collected at f1 and f2 show similar characteristics only two events at f2 6 dec 2007 and 26 oct 2010 are characterized by high values of runoff volume runoff peak and total depth it is assumed that these data may contain sampling errors neglecting these two intense events in f1 and f2 the mean values of runoff volume runoff peak and total depth are very similar they are equal to 4239 24 m3 0 32 m3 s 190 79 m at f1 and to 4876 17 m3 0 31 m3 s 81 55 m at f2 the monitoring campaign at folsom provided records of tss for 17 events by measuring tss event mean concentration emc defined as 2 e m c i 1 n c i v i v where v is the total runoff volume per event l v i is the runoff volume during time period i l c i is the pollutant concentration during time period i mg l and n is the total number of samples during a single storm event in this work the event mean load eml was considered for all sampled events as the sum of pollutant loads determined by multiplying each discrete concentration c i within an event with its corresponding runoff volume v i 3 e m l e m c v details of measured tss emc and the respective eml values are presented in table 2 for two events eml could not be calculated because corresponding flow data was lacking 2 4 correction to flow rate water depth relations data loggers at each monitoring site measured both water depth and velocity with flow rate calculated from these two measurements in open channel flow such as the flow occurring in drainage pipes monitored for this study a well defined relationship exists between water depth and flow under uniform flow conditions chow 1959 when evident this relationship becomes a sign that data loggers would be operating properly therefore establishing this correlation is always a reasonable first step in assuring the quality of the data collected bale et al 2017 for the watersheds f1 and f2 data from the on site data loggers did not always conform to this relationship in particular it was possible to identify two groups of data the first one seemed to follow a particular law the second one was a confused cluster of data points several factors contribute to the breakdown of this relationship in the dataset including erroneous velocity readings attributed to sensor malfunctions and depth readings caused by backwater resulting from downstream ponding a significant amount of effort was devoted to identifying and correcting these problems to generate reliable flow datasets initially the data points belonging to the confused cluster were neglected in this way a stage discharge relationship was developed to identify the trend of the first group of data points the best approximation to the flow rate and water depth data was obtained using a relationship described by two 2nd order polynomial laws one for each basin 4 q 0 2234 y 2 1 3945 y 1 6989 y 0 02 m at f 1 5 q 0 2107 y 2 1 4951 y 1 7541 y 0 02 m at f 2 where q is the flow lps and y is the water depth cm each data point was identified as corresponding to the respective hyetograph and a time during the rainfall event for all of the events the majority of the data points deviating significantly from eq 4 or 5 i e belonging to the confused cluster were from the end of the event since the flow meter relies on the doppler effect it is possible that these outliers result from errors arising from the lower concentration of reflectors i e suspended particles by the end of the event it is believed that the remaining portion of outliers arose from potential sensor malfunctions because they characterized peak flows that occurred while the hyetograph was equal to zero this methodology was initially applied to the events characterized by the highest resolution 1 min then to test its reliability it was extended to all events that are characterized by 2 min or 15 min of resolution in fig 3 measurements of water depth discharge curves for the most recent rainfall events 2014 for f1 fig 3a and f2 fig 3b after removing the outliers identified with the methodology explained above are shown the agreement among field data without outliers and the two regressions eqs 4 and 5 was evaluated through the coefficients of determination r2 defined in the supplementary materials which were 0 99 at f1 figs 3a and 0 95 at f2 fig 3b 2 5 modeling approach 2 5 1 brief description of swmm the epa s swmm is a comprehensive hydrologic and water quality simulation model developed primarily for urban areas huber and dickinson 1988 it obtains the hydrograph and the pollutograph for a storm event for single and long term events on the basis of rainfall and other meteorological inputs snow wind and system characteristics catchment conveyance and storage treatment rossman 2010 swmm has been designed in blocks or operating units each block can be used individually or in a cascade and an executive block coordinates their outputs the runoff block as well as the transport block were utilized for this study quality processes in the same unit include generation of surface runoff constituent loads through the build up of pollutants during dry weather and wash off during wet weather the mathematical representation of these two physical processes is explained in section 2 5 3 by using inlet hydrographs generated from the runoff unit the transport block executes the flow and pollutant routing through the drainage network in the transport unit flow routing is accomplished using the kinematic wave obtained when there is a balance between the gravitational and frictional forces in the momentum equation diffusive non inertia wave when pressure forces are important in addition to gravity and frictional forces in the momentum equation or dynamic wave methods obtained when all forces are important represented by the complete momentum equation in turn water quality processes include first order decay and simulation of resuspension and deposition within the sewer system gorgoglione 2016 2 5 2 swmm implementation the first step in implementing the model in swmm was to define watershed subcatchments and hydrologic connectivity standard methods for performing watershed and subcatchment delineation in an undeveloped landscape such as the use of geographic information system gis and digital elevation models dems are not completely effective in an urban environment because the natural topography and hydrologic flow paths are often altered by roads structures and stormwater collection systems therefore to correctly represent the contributing area and hydrologic connectivity in urbanized areas like the ones in folsom f1 and f2 additional data were needed a stormwater collection system map was obtained from the california department of pesticide regulation cdpr this collection map shows the locations of storm drains subsurface culverts outfalls and it also describes culvert characteristics including their diameter length and connectivity in fig 4 the two watersheds embedded in swmm with their respective drainage networks and discharge points are shown to simulate the runoff from urban surfaces the dynamic wave equation was chosen furthermore the water losses taken into account are represented by the depression storage on the impervious portion of the basin and the infiltration process the latter was modeled by evaluating for each subcatchment the percentage of impervious and pervious area obtained from the land use map the infiltration model utilized in this work was based on horton s equation whose parameter values have been chosen according to the representative values reported in the literature in relation to soil type horton 1940 2 5 3 sediment build up wash off and transport swmm supports simulation of sediment accumulation on impervious surfaces during dry weather periods tss build up within a land use category is described by a mass per unit of subcatchment area the amount of build up is a function of the number of dry weather days antecedent to the rainfall event based on the assumptions of alley and smith 1981 the build up function follows a growth law that asymptotically approaches a maximum limit 6 m a d a d p a c c u d i s p a p i m p 1 e d i s p d a d p where m a d a d p represents the pollutant build up during the antecedent dry period kg ha d i s p is the parameter that measures the disappearance of accumulated solids due to the action of wind or vehicular traffic 1 d p i m p is the impervious area fraction a c c u is the parameter that characterizes the solids build up rate kg ha d a c c u d i s p a p i m p represents the maximum asymptotic limit of the build up curve the pollutant wash off over different land uses takes place during wet periods and it is described by the jewell and adrian 1978 differential equation 7 d m d t d t a r r a i t w a s h m a t where d m d t d t is the wash off load rate kg h a r r a is the wash off coefficient mm 1 i t is the runoff rate mm h w a s h is the wash off exponent a parameter that controls the influence of rainfall intensity on the amount of leached pollutants swmm calculates the spatial and temporal trend of pollutant concentrations in the drainage network assuming that the conduits behave as ideal completely mixed flow reactors cmfrs the control volume the reactor volume coincides with the conduit volume inside the reactor the mathematical balance is obtained from a macroscopic material mass balance di modugno et al 2015 8 d v c d t q i n c i n q o u t c o u t k v c o u t in which v represents the water volume in the conduit reactor calculated at each time step m3 q in is the inflow in the conduit m3 s c in is the sediment concentration at the inlet of the conduit mg l q out is the outflow to the conduit m3 s c out is the sediment concentration in volume v at the outlet of the conduit mg l and k is the decay coefficient s 1 2 6 uncertainty analysis in the context of modeling two main types of uncertainties δ can be identified that influence the uncertainty of model outcomes δ results loucks and beek 2005 asme ptc committee 2009 i model uncertainty δ model and ii measurement uncertainty δ meas as follows 9 δ r e s u l t s δ m o d e l δ m e a s in particular δ model is identified as the sum of three types of uncertainty asme ptc committee 2009 i theoretical model uncertainty δ theor model the uncertainty due to simplifications and approximations involved in the mathematical representations ii parameter uncertainty δ param the uncertainty in the simulation results related to the estimation of the model parameters and iii numerical schemes and solutions uncertainty δ ns the uncertainty associated with the numerical schemes and solutions of the equations as follows 10 δ m o d e l δ t h e o r m o d e l δ p a r a m δ n s any misunderstanding of these factors will lead to untrustworthy outputs which could cause incorrect interpretations of the pollutant build up and wash off processes uusitalo et al 2015 in this work the uncertainty of model parameter values δ p a r a m is considered the main source of uncertainty in stormwater quality model outputs it is assumed that the potential uncertainty in the numerical schemes and solutions by swmm is controlled and we consider that theoretical model is valid as it also has low uncertainty therefore the parameter uncertainty is addressed and analyzed in detail on the basis of the on site data regarding rainfall collected during the monitoring campaign a low uncertainty associated with d adp is hypothesized on the other hand for the other four parameters accu disp arra and wash a wide range of variation was evaluated from the scientific literature 3 results 3 1 hydrologic simulations eight parameters of the runoff block were used to calibrate the hydraulic hydrologic model the depth of depression storage on impervious dstore imperv and pervious dstore perv portions of the subcatchment manning s coefficient for overland flow over the impervious n imperv and pervious n perv portions of the subcatchment the percent of impervious area without depression storage zero imperv and the infiltration parameters of horton s equation the range of variation and the values chosen for these parameters are shown in table 3 asce 1992 rossman 2010 the calibration of the two models employed an iterative process by varying the parameters in table 3 working within the established range and comparing numerically and statistically in table 4 and graphically in fig 5 the simulation with the measured hydrograph the calibration was performed until a good fit was obtained the most recent events characterized by the highest resolution 1 min were used for the calibration process all the other events characterized by 2 min or 15 min of resolution were used for the validation process the goodness of fit indicators used in table 4 to evaluate the model performance are described in the supplementary materials agreement between measured and simulated hydrographs for almost all simulations was very satisfactory poor performance was found for only two of the 31 simulations for the rainfall events of 6 dec 2007 and 26 oct 2010 at f2 for such events the simulated flow is one order of magnitude lower than the measured one unless parameter values outside their suitable physical ranges are used these events were considered intense events by taking into account their runoff volume table 1 3 2 sediment transport simulations four parameters of the runoff block were identified for the calibration of the sediment transport model for the build up function eq 6 the parameter that characterizes the solids build up rate accu and the parameter that identifies the disappearance of accumulated sediments due to the action of the wind or vehicular traffic disp for the wash off function eq 7 the wash off coefficient arra and the wash off exponent wash since a particle size distribution is lacking in this study with the aim of assigning a realistic value to the decay coefficient k of the transport formula eq 8 a range of particle sizes common in urban areas was assumed particle settling velocities were calculated using stokes law and divided by the mean value of the water depth at f1 and f2 the outcomes obtained from the simulations launched with this range of k values suggested that in the drainage network of the two watersheds the sediment resuspension and the settling processes would be counterbalanced e g in each conduit the vertical motion of the particles would be negligible compared to the horizontal one for this reason k was assumed to be zero and was not considered for the calibration process the values of the sediment transport parameters obtained from the calibration and their range of variation are presented in table 5 the range of variation considered for each parameter was defined using 11 previous studies that involve one or more urban basins similar to f1 and f2 baffaut and delleur 1990 warwick and tadepalli 1991 gaume et al 1998 papiri 2000 ciaponi et al 2002 temprano et al 2006 cambez et al 2008 rossman 2009 candela et al 2012 mambretti and sanfilippo 2012 di modugno et al 2015 as we did for the hydraulic hydrologic model calibration was performed via an iterative process by adjusting the parameters listed in table 5 the events characterized by the highest resolution were used for the calibration all the others were used for validating the model the results of the calibration process are shown in figs 6 and 7 which compare the observed and simulated emls and the correlation between sediment load observations and simulations respectively the coefficient of determination r 2 and pearson coefficient r were evaluated and they are respectively equal to 0 62 and 0 79 these goodness of fit indicators were evaluated for all the events with tss eml data excluding the two intense events monitored at f2 the agreement among measured and simulated emls is not as good as that of the hydrologic model results however this is frequently observed in sediment related studies julien 2002 parker 2004 garcìa 2008 4 discussion 4 1 impact of parameter uncertainty on model results considering the range of variation of each parameter found in the literature and reported in table 5 a sensitivity analysis was executed on their mean values to provide a general assessment of model accuracy this information is valuable to those who make design decisions the analysis was performed by adjusting one parameter at a time in fixed percentage increments the corresponding difference in tss emc and tss peak was calculated and plotted fig 8 based on eqs 6 and 7 the parameters considered for this analysis are accu disp d adp arra and wash starting from the set of parameters characterized by the mean value that they can assume per subcatchment one parameter was varied at a time by using the chosen value of change in parameter reported in table 6 the parameters that showed greater sensitivity were accu and disp since the latter is present in both the exponential and maximum build up a c c u d i s p a p i m p factors in eq 6 it is not surprising that disp is one of parameters to which outcomes are the most sensitive whereas arra and wash also have important impacts on tss peak their impact on tss emc are less significant variations in d adp over a range of 80 do not produce significant changes in the tss peak or tss emc 4 2 coupling sensitivity and uncertainty analysis after analyzing the sensitivity of modeling outcomes to water quality parameters this study presents a methodology for combining sensitivity and uncertainty analysis this approach exploits the results of the sensitivity analysis to decrease the model uncertainty aiming to assess and predict the overall accuracy and reliability of water quality models the outcomes of the sensitivity analysis support the classification of the five water quality parameters into three different classes low medium and high fig 9 in addition in fig 9 the interval of change in parameter with respect to the parameter limits proposed in table 5 in which each of these parameters shows its sensitivity level with the corresponding change in emc are presented for the purpose of this study among these three classes the one characterized by medium sensitivity arra was excluded since it represents an average condition that reveals information that can be located among the results obtained from the classes considered although the sensitivity analysis was performed to assess the influence of the five water quality parameters on both tss emc and peak only the tss emc is considered further because tss samples during the monitoring campaign conducted in f1 and f2 were collected as composites to proceed with the combination of sensitivity and uncertainty of the water quality parameters the extreme values of the interval of change in parameter were considered being the most representative of the entire range of variation clearly the interval of change in parameter is not the same for all the parameters since i the range of variation is different from one parameter to another and ii within these ranges at least an 80 variation was considered since disp varies between 73 the entire range of its variation was considered in particular in table 6 the change in parameter with the corresponding change in emc that were taken into account are shown to evaluate the accuracy of the model simulations three widely used goodness of fit indicators nash sutcliffe efficiency nse root mean square error rmse and kling gupta efficiency kge were assessed in particular an indicator ratio was evaluated by dividing each indicator by its corresponding value when the set of parameters took on their mean values table 7 to better understand how these ratios were calculated we can consider as an example n s e r a t i o calculated for d a d p 80 11 n s e r a t i o n s e a d p 80 n s e m e a n v a l u e s f o r d a d p 80 where n s e a d p 80 represents the nse between observed and simulated tss values obtained by varying the antecedent dry period value by 80 and keeping the value of all the other variables equal to the mean value of their range of variation n s e m e a n v a l u e s is the nse between observed and simulated tss values obtained by keeping the value of all the variables equal to the mean value of their range of variation the closer these ratios are to 1 the better the performance of the water quality model in table 7 these ratios obtained by considering the lower change in parameter value chosen and highlighted in table 6 are shown since the trend of the sensitivity plots fig 8 is not symmetric the same evaluations were also assessed by considering the upper change in parameter value chosen whereas these results are very close to the ones obtained with the lower value for brevity only the latter are reported in the following table by coupling parameter sensitivity and uncertainty in the input variables a model assessment matrix was developed to assist users in making appropriate model performance conclusions and to guide investments in model improvement fig 10 the matrix presents conclusions for overall model performance that include model accuracy goodness of fit and uncertainty of the model response in the model assessment matrix four general cases based on qualitative indications of sensitivity and uncertainty of the water quality variables are presented when parameters have low variability the model accuracy goodness of fit can be confidently concluded because the standard of comparison is relatively certain and a good very good uncertainty of the model response is obtained in particular by coupling this low parameter variability with low parameter sensitivity good satisfactory uncertainty of the model response is obtained in swmm this is the case of the parameter d adp that characterizes the build up process although there are no parameters characterized by high sensitivity and low variability in our implementation of swmm it is reasonable to assume that low variability combined with high sensitivity would produce intermediate model uncertainty for the case with high parameter variability and high parameter sensitivity model uncertainty will be large and overall model performance will be poor in swmm this is the case of the parameters accu and disp that regulate the build up process high parameter variability may produce an intermediate degree of uncertainty in the model response when it is coupled with low parameter sensitivity in swmm the parameter wash that characterizes the wash off process belongs to this case in addition to making forward predictions of expected model uncertainty given various variability sensitivity combinations fig 10 can be used to direct investments to reduce model uncertainties improved measurement or independent assessments of parameters with high variability and high sensitivity will produce the greatest return on investment while improvements in parameter measurement to reduce variability and uncertainty will be the least valuable for cases where both the range of parameter variability and model sensitivity to that parameter are low although model sensitivity is routine considered in model formulation the combination of input parameter variability and sensitivity offers improved insights regarding model performance and potential for improvements 5 conclusions the data collected during a monitoring campaign carried out within two residential areas located in folsom sacramento county california have been presented in this work and used as an opportunity to present a framework for the assessment of model uncertainty datasets encompass eight years which is unusual the rainfall and flow data collected were exploited to calibrate and validate the hydrologic model implemented in swmm because the methodology evaluated and eventually identified possible outliers among flow measurements an accurate and reliable hydrologic model was developed the numerical comparison between observed and simulated hydrographs was made by evaluating r 2 rmse and nse for each rainfall event by means of this accurate model an efficient sediment transport model was calibrated and validated in swmm by using the total suspended solid data collected during the monitoring campaign the agreement between measured and simulations of this water quality model was estimated by evaluating the coefficient of determination r 2 and pearson coefficient r an in depth understanding of the possible sources of uncertainty has outlined key knowledge required to analyze the impact that such uncertainty associated with sediment build up and wash off processes in urban areas has on water quality model predictions furthermore considering the range of variation of each parameter a sensitivity analysis was executed on their mean values to provide a general assessment of model accuracy accu and disp were the parameters that showed greater sensitivity by coupling uncertainty and sensitivity analysis and combining them with an underlying reliable sediment transport model a methodology that aimed to assess and predict the overall accuracy and reliability of water quality models was developed a model assessment matrix was created to assist users in making appropriate model performance conclusions it is characterized by four general cases based on qualitative indications of sensitivity and uncertainty of the sediment transport parameters for instance it was found that for the case with high parameter variability coupled with high parameter sensitivity in swmm this is the case of accu and disp an unsatisfactory uncertainty of the model response may be obtained these together indicate low accuracy and overall poor model performance in terms of uncertainty of the model response as a result the matrix presents appropriate conclusions for overall model performance that include model accuracy goodness of fit and uncertainty of the model response software availability storm water management model swmm is freely downloadable from https www epa gov water research storm water management model swmm downloads swmm was first developed in 1971 and has undergone several major upgrades since then in this study we are using the version 5 0 swmm 5 the development of swmm 5 was pursued under a cooperative research and development agreement between the water supply and water resources division of the u s environmental protection agency and the consulting engineering firm of camp dresser mckee inc the project team consisted of the following individuals lewis rossman contact information unknown trent schade contact information unknown daniel sullivan contact information unknown from us epa robert dickinson e mail robert dickinson gmail com address 9340 pontiac drive tampa florida 33626 phone 813 712 0664 carl chan e mail chancc cdm com address 151 north delaware st suite 1520 indianapolis in 46204 edward burgess e mail burgesseh cdm com address 8805 governor s hill drive suite 260 cincinnati oh 45249 from cdm swmm 5 is designed to run under the windows operating system of an ibm intel compatible personal computer the program language is c and the size is 5 mb conflicts of interest the authors declare that there is no conflict of interests regarding the publication of this paper acknowledgments this research was supported by the california department of pesticide regulation under contract number 14 c0029 with t m y and f a b as pis the project officer dr yuzhou luo contributed valuable modeling insight and guidance during the development of this work michael ensminger and robert budd from dpr provided assistance with information and data related to the dpr monitoring programs author contributions a g f a b and t m y conjointly designed and undertook the presented modeling research b j l p l r o and d l h collected and provided the monitoring data for flow and suspended solids from the f1 and f2 monitoring sites all authors have read and approved the final manuscript appendix a supplementary data the following is the supplementary data to this article supplementary materials supplementary materials appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 022 
26289,this study introduces a novel framework for land change simulation that combines the traditional land transformation model ltm with data clustering tools for the purposes of conducting land change simulations of large areas e g continental scale and over multiple time steps this framework called ltm cluster subsets massive land use datasets which are presented to the artificial neural network based ltm ltm cluster uses the k means clustering algorithm implemented within the spark high performance compute environment to illustrate the framework we use three case studies in the united states which vary in simulation extents cell size time intervals number of inputs and quantity of urban change findings indicate consistent and substantial improvements in accuracy performance for all three case studies compared to the traditional ltm model implemented without input clustering specifically the percent correct match the area under the operating characteristics curve and the error rate improved on average of 9 11 and 4 these results confirm that ltm cluster has high reliability when handling large datasets future studies should expand on the framework by exploring other clustering methods and algorithms keywords clustering parallel processing spark environment land use change abbreviations ltm land transformation model ltm cluster land transformation model cluster framework hpc high performance computing ml machine learning ann artificial neural network lcs land change science pcm percent correct match toc total operating characteristic auc area under the curve er error rate mse mean square error 1 introduction land change science is now challenged with a massive quantity of freely available high resolution land cover data that is generated by a multitude of satellite and airborne platforms many with continental or global coverage advancements in computational methods denning and lewis 2017 have facilitated land change modeling toward larger spatial extents and longer time frames e g pijanowski et al 2014 tayyebi et al 2013 numerous models have been developed to simulate land change at a variety of extents liu and phinn 2003 yang et al 2008 omrani et al 2015 shafizadeh moghadam et al 2015 basse et al 2016 azari et al 2016 shafizadeh moghadam et al 2017a b c among these machine learning based ml models pijanowski et al 2014 have proven their potential in quantifying complex relationships and interactions among drivers while improving land change forecasts shafizadeh moghadam et al 2017a b tayyebi et al 2014b omrani et al 2017a b model fitting over large areas has commonly relied on smaller training samples besides dividing the area into multiple sub regions and simulating these separately is also common e g see pijanowski et al 2014 we propose an alternative method that relies on data partitioning algorithms the aim of this research is to introduce a novel data driven framework called ltm cluster to address the aforementioned modeling challenges by splitting the input data into clusters using a partitioning algorithm prior to machine learning which we implement in the big data high performance computing hpc environment called spark we address the following research questions 1 does clustering prior to learning scheme coupled with the ltm improve the model s goodness of fit during calibration training and validation testing 2 do these techniques work equally well in areas that differ in spatial extent cell size and quantity of land change 3 what is the gain in computational efficiency of porting traditional ml algorithms to spark to address these questions we compare the performance of the traditional artificial neural network based ann land transformation model ltm pijanowski et al 2014 and the proposed ltm cluster framework using spark enabled k means clustering algorithms our research builds upon a few studies that utilize data clustering prior to a learning scheme as an alternative for handling large datasets ayyadurai and jayanthi 2012 to ensure that the approach has broad applicability we tested our scheme on three diverse land use change datasets from the usa the study sites vary in size number of inputs cell resolution time interval as well as in number of observations and quantity of change i e urban gain 2 literature review 2 1 generating a scalable and automated framework for large extent dataset using clustering scalability and automation are important aspects when modeling land change clustering data prior to model fitting can support both aspects the objective of data clustering is to determine groupings of data values jain et al 1999 berkhin 2006 jain 2010 han et al 2011 a clustering algorithm divides n number of objects into k groups based on some metric of similarity similarity metrics can be generated from one or many variables i e dimensions or alternatively a collection of patterns an ideal cluster places a portion of the n data into a compact group isolated from other groups clustering is often considered as an unsupervised task because no training with specific labels is provided the process of clustering involves four steps jain et al 1999 feature extraction pattern proximity determination i e establishing distance values between pairs clustering i e grouping and abstraction e g selecting from the clusters a small subsample statistically representing the complete dataset a variety of clustering methods have been introduced among which some focus on 1 centroid models e g k means k medians k medoids 2 connectivity models which define distances between objects often called hierarchical which can be top down or bottom up and 3 data distribution models i e those that are grouped according to statistical distributions we use the k means clustering technique because of its ease of use and well known performance kanungo et al 2002 2 2 leveraging ml algorithms in hpc with spark for larger datasets a variety of technologies are available to ensure that computation storage and data workflow are optimized and that scalability can be achieved for potentially larger scale modeling these computational scaling technologies include the operationalization of the mapreduce concept bello orgaz et al 2016 which partitions data into units for parallel processing processed units are then mapped back into the larger dataset for further analyses the apache hadoop computation platform is designed to utilize the mapreduce concept by providing a hpc environment including massive storage a distributed file system and advanced processing power spark is an open source implementation of an hpc framework that is optimized for the use of r python and java when big data analysis is required hashem et al 2015 it uses a more complex distributed clustering model i e not the dual mapreduce model where data are loaded once into memory and numerous operations can be performed on it spark is especially proficient at executing ml algorithms that require iterative learning xin et al 2013 given these properties we leverage the spark framework capabilities to implement the ltm cluster framework with the k means clustering and the ann based ltm 2 3 extending ltm model with spark several supervised ml methods are frequently used in land change modeling common ones are support vector machines svm random forest rf and artificial neural networks ann svm and rf are computationally expensive and are not well suited for simulations with large datasets tuzel et al 2007 in contrast to anns that are more efficient pijanowski et al 2014 basse et al 2014 anns search for patterns in data iteratively using learning algorithms that mimic the way that neurons parallel process information in the mammalian brain zhang et al 1998 ltm uses a form of ann to simulate land use change pijanowski et al 2014 tayyebi et al 2014b ltm has been widely applied and has displayed high accuracy pijanowski et al 2005 2006 tang et al 2005 olson et al 2008 wiley et al 2010 ray and pijanowski 2010 ray et al 2010 pijanowski and robinson 2011 pijanowski et al 2011 moore et al 2012 tayyebi et al 2013 tayyebi et al 2014a b omrani et al 2017a b shafizadeh moghadam et al 2017a the literature reports that the multi layer perceptron a form of an ann outperforms traditional statistical models e g logistic and multinomial regression regression and classification trees feng et al 2016 omrani 2015 charif et al 2017 omrani et al 2017a b shafizadeh moghadam et al 2017b support vector machines and random forests shafizadeh moghadam et al 2017c however with the increasing availability of data new model applications are appearing along with new challenges especially in the area of model calibration a common practice in land change model calibration with large datasets is to use only a small portion of input data e g 5 or 10 for model building e g pijanowski et al 2014 basse et al 2014 2016 or even lower e g mustafa et al 2018 approaches to select input data include random sampling mustafa et al 2018 stratified random sampling omrani 2015 omrani et al 2017 basse et al 2014 2016 liu and feng 2016 shafizadeh moghadam et al 2017b c charif et al 2017 etc however these sampling strategies introduce a bias as the performance of ml models depends on the distribution of both the input e g distance to the nearest road and the output maps i e a map of land change developed from calculating the map difference ideally model building and calibration should include the entire range of input data but existing sampling methods discard a lot of information leading to lower accuracy in predictions pijanowski et al 2014 another shortcoming of data sampling is when fitted models are used to forecast future land change scenarios values beyond the original input data used for calibration become more prevalent e g development has to proceed further from roads as most land for development proximate to roads has already been converted which can induce forecasting errors basse et al 2016 in addition if land change modeling is implemented with increasing spatial extents longer time frames and greater number of spatial inputs a scalable computing framework cf tayyebi et al 2016 pijanowski et al 2014 is needed to deal with these calibration challenges 2 4 data and study area we used land use datasets from three case studies 1 muskegon county michigan 2 lynnfield massachusetts boston and 3 south eastern wisconsin table 1 hereafter as muskegon boston and sewi the muskegon case study is located in the west central lower peninsula of michigan united states the region is currently dominated by forest in the northeast agriculture in the center and urban areas in the southwest it covers 1 292 km2 and urban nearly doubled in area between 1978 and 1998 time the boston case study consists of the town of lynnfield which is located in the northeastern massachusetts in 1971 lynnfield was composed of dense and sparse urban development 36 of the town mixed deciduous and coniferous forest 43 wetlands 6 and open land 12 by 1999 lynnfield was composed of dense and sparse urban development 45 mixed deciduous and coniferous forest 36 wetlands 6 and open land 10 the sewi case study is situated in the state of wisconsin in the north central part of the united states sewi comprises seven counties kenosha milwaukee ozaukee racine walworth washington and waukesha counties pijanowski et al 2006 pijanowski and robinson 2011 sewi is currently dominated by agriculture urban and forest which accounted for more than 86 of the landscape in 2011 47 27 and 12 between 2001 and 2011 the percentage of urban areas increased from 24 to 27 whereas agriculture and forest decreased by 2 and 0 3 respectively more than 60 of lost agriculture contributed to urban gain during the 10 year period sewi has undergone remarkable urbanization between 2001 and 2011 the urban expansion rate was 10 and densification rate was 1 5 for 2001 2011 we summarized the characteristic of each case studies in table 1 and additional information can be found in the literature blanchard et al 2015 tayyebi et al 2014b for each dataset we determined the difference between urban gain and non urban persistence between two time periods fig 1 we excluded the urban class in the initial time because it is impossible for this urban class to have any urban gain or non urban persistence across two time points furthermore a set of variables was defined for each cell serving as driving factors table 2 appendix two models i e ltm and ltm cluster were developed using six variables in 1978 for muskegon eight variables in 1998 for boston and sixteen variables in 1990 for sewi as inputs and urban change maps between two time periods 1978 1998 in muskegon 1971 1999 in boston 1990 2000 in sewi as outputs the cells of land use have a spatial resolution of 100 2 and 30 m in muskegon boston and sewi respectively our repository provides the three datasets and related r code omrani et al 2018 3 methods the ltm cluster modeling framework is composed of three phases 1 generating clustering using k means with spark 2 ltm modeling using hpc and 3 model accuracy assessment fig 2 summarizes the workflow the approach is implemented in the r programming language r 3 5 1 team 2013 within the spark environment zaharia et al 2010 to execute ltm cluster we used parallel processing including the following steps fig 3 split the dataset s into learning l and testing t subsets using stratified random sampling the l set is used for model calibration and the t set is used for assessing its performance see section 3 3 we selected 70 of the data to be used for training and the remaining 30 to assess model accuracy i e testing divide l into k clusters lk based on an advanced version of k means algorithm divide t into k clusters tk by assigning cells from t to the closest centroid of lk divide s into k clusters sk by assigning cells from s to the closest centroid of lk calibrate and validate ltm cluster based on lk and tk clusters respectively this is done by running ltm model for each cluster simulate urban change for each sk based on ltm cluster that is calibrated on lk 3 1 clustering using k means with spark due to its good performance the k means algorithm is chosen jain 2010 murray et al 2017 for the clustering prior to the learning scheme the number of clusters is determined by maximizing the average silhouette coefficient chiang and mirkin 2010 instead of a trial and error learning approach e g basse et al 2014 the silhouette coefficient is the average distance between all points in a cluster compared with the average distance between a point and its distance from the nearest cluster the k means algorithm randomly selects k observations as centers of groups and then calculates the euclidean distances in the feature space between each observation and the centers of all clusters next it assigns to each observation a group k based on the nearest e g shortest euclidean distance cluster center centers are iteratively updated until a number of groups is reached and all observations are assigned to a unique cluster because k means cannot handle large datasets due to an excess of computation we employed spark based k means an advanced version of the standard k means algorithm to address the computational burden spark based k means performs parallel calculations of distances across all data pairs gopalani and arora 2015 meng et al 2016 while requiring one parameter k determined through the average silhouette coefficient to segments the data into k clusters fig 3 shows the clustering prior to learning scheme using an advanced k means approach that ultimately minimizes the challenges of a large dataset while improving the ltm s performance due to the cutting edge spark based programming techniques this approach is suitable for exploring large datasets that cannot be stored in a computer s main memory studies highlight the fast computation of spark based k means compared to alternatives e g basic k means algorithm singh and reddy 2015 zaharia et al 2010 we also computed the average silhouette value for each value of k and for each region in the k means clustering routine a silhouette value rousseeuw 1987 measures cohesion of each value to its own cluster compared to another cluster silhouette values range from 1 perfect match to its own cluster and 1 fits better in another cluster a silhouette value of 0 means that the value is located in multi dimensional space and using a standardized euclidean distance precisely between two clusters finally to determine how the best value of k varies spatially we saved the k for the largest silhouette coefficient average for each cell and then mapped that for each study simulation area 3 2 land transformation model the land transformation model ltm uses ann to derive relationships between land change and multiple explanatory variables pijanowski et al 2014 ltm uses a multi layer perceptron with one hidden layer with a back propagation training method minimizing the mean square error mse the hidden layer is composed of a set of hidden units called neurons hidden layers in ann allow for the modeling of any nonlinear function schmidhuber 2015 and can approximate the relationship between influential factors i e predictor variables and outcomes e g a change no change categorical variable several studies have shown that ltm performs well in forecasting an outcome variable pijanowski et al 2005 omrani et al 2013 2015 more formally an outcome variable y contains categorical values in a specified set 1 2 c the variable y is expressed as a function of the input x x 1 x 2 x q and described by the following formula 1 p y k x ψ j 1 p v j k φ i 1 q w i j x i w 0 j v 0 k where ω ij and v jk are weights assigned to the connections between the input layer and the hidden layer and between the hidden layer and the output layer respectively ω 0j and v 0k are biases or threshold values in the activation of a unit φ is an activation function applied to the weighted sum of the output of the preceding layer i e the input layer ψ is an activation function applied to each output unit to the weighted sum of the activations of the hidden layer an ann generates outputs contained within the 0 1 interval but not equal to 0 or 1 a suitable choice for ψ e g a softmax or a sigmoid function maps the real axis to the interval 0 1 ann s output is a composition that correspond to a set of values summing to unity land use conversion are determined using the maximum probability strategy which assigns each observation to a unique class of land use from the ann s outputs pijanowski et al 2014 shafizadeh moghadam et al 2017c detailed descriptions of the ltm are available in pijanowski et al 2014 3 3 model calibration metrics to validate calibration performance from the ltm and ltm cluster models we measure the model s accuracy with four goodness of fit metrics first we created confusion matrices table 3 between the observed and predicted land uses and quantified the location errors pontius et al 2008 we generated error maps that illustrates configuration and location of all errors i e misses false alarm and correctly simulated values i e hits and correct rejections second we used percent correct match pcm pijanowski et al 2014 the total operating characteristic curve toc pontius and si 2014 the area under the curve auc pontius and batchu 2003 and the error rate er bradley et al 2016 to quantify the mismatch between the observed and simulated values pcm specifies the percentage of cells for the land use class correctly classified by a model true positives 100 urban change cells toc overcomes the limitations of the relative operating characteristic roc given that the roc fails in cases where some types of error are more important than others dodd and pepe 2003 toc evaluates the performance of non binary model output by varying the decision threshold between 0 and 1 pontius and si 2014 from the toc we extract the auc for each case study finally we use the er metric to measure the incorrect inclusion and incorrect omission of a class table 3 appendix larger values of pcm and auc as well as smaller value for er correspond to better model goodness of fit 4 results the silhouette coefficients for each k means calculation per study area are shown in fig 4 based on the results the goodness of fit of the ltm cluster k 3 is superior to that obtained in tayyebi et al 2014 and the traditionally parameterized ltm k 1 table 4 we found that the difference between applying clustering prior to the learning scheme versus not applying is significant based on the wilcoxon signed rank test of paired calibration statistics p 0 01 for the muskegon case study we also note there is an increase in performance in the pcm values from 69 for the ltm to 82 for ltm cluster table 4 maps of the best value of k based on the largest silhouette coefficient average are shown in fig 5 note that each region produces a different spatial pattern of k as it ranges from 1 to 3 designated as clusters 1 2 and 3 in muskegon the entire eastern portion of this county had the best value of k 1 the western portion had a k 2 interestingly following the us 31 highway corridor north south and m 36 highway east west and the more rural areas had a value of k 3 gray for boston the spatial pattern of k across the range of 1 through 3 selected again based on the largest silhouette coefficient averages is less distinct than muskegon k values of 1 for example are shown as small patches arranged at the edges of the 1990 map for urban cluster 3 i e k 3 locations are all located in the upper left corner of the region finally for sewi the map is dominated by k 1 and k 3 locations with k 3 following highway corridors and proximity to large 1971 urban patches and locations of k 1 being in the most rural areas in 1971 the error maps figs 6 8 show the spatial pattern of agreement and disagreement which we created by overlaying the reference change maps and the urban simulated maps the er rates for the ltm and ltm cluster were 37 and 31 33 and 32 and 24 and 22 for the muskegon boston and sewi simulations the ltm cluster displayed a performance improvement of 19 3 and 4 compared to the traditionally parameterized ltm model for the er calibration statistic for the muskegon boston and sewi simulations auc values also show improvements in accuracy for all three case study locations as well overall the results indicate that the ltm cluster framework provides an improvement in accuracy compared to the ltm model visualization of toc curves figs 9 11 also confirm that ltm cluster outperforms the traditionally configured ltm finally we compared the computing time across the ltm and ltm cluster model both models were implemented on a server dell poweredge r930 dual intel r xeon r cpu e7 8891 v4 2 80 ghz and a memory of 512 gb 16 32 gb ddr4 table 5 shows that ltm cluster model is faster than the ltm model for each dataset computation time decreased by 19 2 and 6 for the muskegon boston and sewi dataset 5 discussion we extended the ltm using prior clustering scheme and the spark environment we used spark rather than hadoop or mapreduce because spark has emerged as a popular choice to implement large scale ml applications on large datasets due to its ease in accommodating iterative learning processes zaharia et al 2010 2012 our key findings are first that the clustering prior to learning scheme is an easy and effective instrument to improve the model s performance and supports the processing of large land use datasets and second the improvements through ltm cluster compared to ltm are statistically significant the strengths and disadvantages of the clustering prior to learning scheme are as follows 1 data clustering improves the performance of the model since data in each cluster are homogenous i e have similar characteristics each cluster from the learning set is used to create a sub model as the ltm cluster each having its own parameters in addition we demonstrated that ltm cluster comprising several sub models is superior to the basic ltm based on a single model in terms of several performance metrics i e pcm toc and auc 2 clustering prior to learning scheme reduces the error rate table 4 3 dividing data across multiple clusters allows the model to scale up because data chunks from clusters are more easily maintained or processed in hpc each cluster is manageable in size and can be analyzed independently on separate processing units although the results show that our approach is promising there are still some limitations to our approach 1 the spark based k means algorithm requires one user defined parameter this is also the case for the majority of clustering approaches a drawback is the difficulty in determining an optimal number of clusters k multiple techniques exist to make that determination and most of them are based on finding the values of k which balance the search of minimizing the intra cluster distance and maximizing the inter cluster distances there is however no consensus on the best technique the choice of k may also often rely on the researchers expert opinion and interpreting the goodness of fit of a model in our application silhouette indices guided our choice in an objective manner rousseeuw 1987 the three datasets presented local maxima for k 3 clusters and larger values of k which vary with the studied dataset we found that allowing a larger number of clusters generated many clusters with small numbers of cells we believe that the ltm cluster framework can be extended in several respects besides the spark based k means prior learning clustering process other clustering approaches can be explored include dbscan birant and kut 2007 shen et al 2016 isodata abbas et al 2016 kim and liang 2017 hierarchical clustering liu et al 2017 and spherical k means tunali et al 2016 2 we used a different set of inputs across the case studies despite the model improvement compared to the traditional implementation of the ltm future studies should consider additional economic and social factors such as income levels employment rates and accessibility hagenauer and helbich 2018 which could greatly improve the model performance 6 conclusions this study introduced a framework with an innovative model calibration sampling scheme into ml based land change modeling our approach called the ltm cluster framework is based on clustering using a k means algorithm its application before the calibration step addresses the unmanageable size of the data by splitting it into clusters i e groups with similar input values for that purpose we implemented this framework in the spark computing environment for large scale data parallel processing we compared the performance of ltm cluster to the traditional ltm using numerous metrics for three case studies with high variability results provide clear evidence that applying the clustering prior to learning scheme is significantly superior compared to the basic ltm independently of the considered performance measure the percent correct match the area under the curve and the error rate improved on average by 9 11 and 4 respectively with ltm cluster over all three datasets more research is needed to address some of the limitations of the method including the determination of the appropriate number of clusters testing this method in other case studies of varying size and complexity together with other ml algorithms may be useful for a more in depth model evaluation findings from this study suggest that the ltm cluster framework successfully improves the prediction accuracy and that clustering prior to learning scheme may be valuable for other applications beyond land change modeling funding this work was part of the smart boundary and smart ca projects funded by the national research fund luxembourg fnr and liser research institute software and data availability the name of the software tool prototype introduced in this research is ltm cluster the developers are the authors of the study for the conceptualization hichem omrani generated the implementation of the tool in r please contact the first author for further information year first available october 2018 software required download the r software from the internet www r project org availability the datasets and r codes of the developed model are available on the mendeley repository dx doi org 10 17632 xnxrhw4fhv 2 omrani et al 2018 datasets are land use data inputs from three study areas described in this research the datasets and the tool can be used to perform a cross model comparison or for other purposes users are welcome to send their feedback to the corresponding author hichem omrani liser lu acknowledgements this research work has been partially carried out at the center for global soundscapes at purdue university the authors thank the four reviewers as well the editor for their valuable comments appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 004 appendix table 2 driving factors table 2 muskegon 1978 boston 1971 sewi 1990 1 distance to road distance to transport distance to road 2 distance to rivers distance to water distance to stream 3 distance to urban distance to urban distance to wetland 4 distance to lake distance to non urban distance to urban 5 distance to highways density of transport distance to forest 6 distance to water density of water distance to park 7 density of urban distance to agriculture 8 density of non urban distance to shrub 9 density of wetland 10 density of urban 11 density of shrub 12 density of forest 13 density of agriculture 14 elevation 15 slope 16 aspect note distances refer to euclidean distances in km slope is in elevation is an angle between 0 and 90 and density is a value between 0 and 1 table 3 confusion matrix from the testing set t table 3 simulated map observed map non urban persistence urban gain non urban persistence crs fas urban gain misses hits total sum of non urban persistence crs misses sum of urban gain hits fas note hits or true positive crs correct rejections or true negative misses or false positive fas false alarms or false negative pcm for urban gain hits total of urban gain er error rate fa misses cardinality t for the ltm model er for the ltm cluster fak missesk sum cardinality tk where k is the index of cluster fak and missesk are computed from the confusion matrix based on tk subset and sum cardinality tk cardinality t 
26289,this study introduces a novel framework for land change simulation that combines the traditional land transformation model ltm with data clustering tools for the purposes of conducting land change simulations of large areas e g continental scale and over multiple time steps this framework called ltm cluster subsets massive land use datasets which are presented to the artificial neural network based ltm ltm cluster uses the k means clustering algorithm implemented within the spark high performance compute environment to illustrate the framework we use three case studies in the united states which vary in simulation extents cell size time intervals number of inputs and quantity of urban change findings indicate consistent and substantial improvements in accuracy performance for all three case studies compared to the traditional ltm model implemented without input clustering specifically the percent correct match the area under the operating characteristics curve and the error rate improved on average of 9 11 and 4 these results confirm that ltm cluster has high reliability when handling large datasets future studies should expand on the framework by exploring other clustering methods and algorithms keywords clustering parallel processing spark environment land use change abbreviations ltm land transformation model ltm cluster land transformation model cluster framework hpc high performance computing ml machine learning ann artificial neural network lcs land change science pcm percent correct match toc total operating characteristic auc area under the curve er error rate mse mean square error 1 introduction land change science is now challenged with a massive quantity of freely available high resolution land cover data that is generated by a multitude of satellite and airborne platforms many with continental or global coverage advancements in computational methods denning and lewis 2017 have facilitated land change modeling toward larger spatial extents and longer time frames e g pijanowski et al 2014 tayyebi et al 2013 numerous models have been developed to simulate land change at a variety of extents liu and phinn 2003 yang et al 2008 omrani et al 2015 shafizadeh moghadam et al 2015 basse et al 2016 azari et al 2016 shafizadeh moghadam et al 2017a b c among these machine learning based ml models pijanowski et al 2014 have proven their potential in quantifying complex relationships and interactions among drivers while improving land change forecasts shafizadeh moghadam et al 2017a b tayyebi et al 2014b omrani et al 2017a b model fitting over large areas has commonly relied on smaller training samples besides dividing the area into multiple sub regions and simulating these separately is also common e g see pijanowski et al 2014 we propose an alternative method that relies on data partitioning algorithms the aim of this research is to introduce a novel data driven framework called ltm cluster to address the aforementioned modeling challenges by splitting the input data into clusters using a partitioning algorithm prior to machine learning which we implement in the big data high performance computing hpc environment called spark we address the following research questions 1 does clustering prior to learning scheme coupled with the ltm improve the model s goodness of fit during calibration training and validation testing 2 do these techniques work equally well in areas that differ in spatial extent cell size and quantity of land change 3 what is the gain in computational efficiency of porting traditional ml algorithms to spark to address these questions we compare the performance of the traditional artificial neural network based ann land transformation model ltm pijanowski et al 2014 and the proposed ltm cluster framework using spark enabled k means clustering algorithms our research builds upon a few studies that utilize data clustering prior to a learning scheme as an alternative for handling large datasets ayyadurai and jayanthi 2012 to ensure that the approach has broad applicability we tested our scheme on three diverse land use change datasets from the usa the study sites vary in size number of inputs cell resolution time interval as well as in number of observations and quantity of change i e urban gain 2 literature review 2 1 generating a scalable and automated framework for large extent dataset using clustering scalability and automation are important aspects when modeling land change clustering data prior to model fitting can support both aspects the objective of data clustering is to determine groupings of data values jain et al 1999 berkhin 2006 jain 2010 han et al 2011 a clustering algorithm divides n number of objects into k groups based on some metric of similarity similarity metrics can be generated from one or many variables i e dimensions or alternatively a collection of patterns an ideal cluster places a portion of the n data into a compact group isolated from other groups clustering is often considered as an unsupervised task because no training with specific labels is provided the process of clustering involves four steps jain et al 1999 feature extraction pattern proximity determination i e establishing distance values between pairs clustering i e grouping and abstraction e g selecting from the clusters a small subsample statistically representing the complete dataset a variety of clustering methods have been introduced among which some focus on 1 centroid models e g k means k medians k medoids 2 connectivity models which define distances between objects often called hierarchical which can be top down or bottom up and 3 data distribution models i e those that are grouped according to statistical distributions we use the k means clustering technique because of its ease of use and well known performance kanungo et al 2002 2 2 leveraging ml algorithms in hpc with spark for larger datasets a variety of technologies are available to ensure that computation storage and data workflow are optimized and that scalability can be achieved for potentially larger scale modeling these computational scaling technologies include the operationalization of the mapreduce concept bello orgaz et al 2016 which partitions data into units for parallel processing processed units are then mapped back into the larger dataset for further analyses the apache hadoop computation platform is designed to utilize the mapreduce concept by providing a hpc environment including massive storage a distributed file system and advanced processing power spark is an open source implementation of an hpc framework that is optimized for the use of r python and java when big data analysis is required hashem et al 2015 it uses a more complex distributed clustering model i e not the dual mapreduce model where data are loaded once into memory and numerous operations can be performed on it spark is especially proficient at executing ml algorithms that require iterative learning xin et al 2013 given these properties we leverage the spark framework capabilities to implement the ltm cluster framework with the k means clustering and the ann based ltm 2 3 extending ltm model with spark several supervised ml methods are frequently used in land change modeling common ones are support vector machines svm random forest rf and artificial neural networks ann svm and rf are computationally expensive and are not well suited for simulations with large datasets tuzel et al 2007 in contrast to anns that are more efficient pijanowski et al 2014 basse et al 2014 anns search for patterns in data iteratively using learning algorithms that mimic the way that neurons parallel process information in the mammalian brain zhang et al 1998 ltm uses a form of ann to simulate land use change pijanowski et al 2014 tayyebi et al 2014b ltm has been widely applied and has displayed high accuracy pijanowski et al 2005 2006 tang et al 2005 olson et al 2008 wiley et al 2010 ray and pijanowski 2010 ray et al 2010 pijanowski and robinson 2011 pijanowski et al 2011 moore et al 2012 tayyebi et al 2013 tayyebi et al 2014a b omrani et al 2017a b shafizadeh moghadam et al 2017a the literature reports that the multi layer perceptron a form of an ann outperforms traditional statistical models e g logistic and multinomial regression regression and classification trees feng et al 2016 omrani 2015 charif et al 2017 omrani et al 2017a b shafizadeh moghadam et al 2017b support vector machines and random forests shafizadeh moghadam et al 2017c however with the increasing availability of data new model applications are appearing along with new challenges especially in the area of model calibration a common practice in land change model calibration with large datasets is to use only a small portion of input data e g 5 or 10 for model building e g pijanowski et al 2014 basse et al 2014 2016 or even lower e g mustafa et al 2018 approaches to select input data include random sampling mustafa et al 2018 stratified random sampling omrani 2015 omrani et al 2017 basse et al 2014 2016 liu and feng 2016 shafizadeh moghadam et al 2017b c charif et al 2017 etc however these sampling strategies introduce a bias as the performance of ml models depends on the distribution of both the input e g distance to the nearest road and the output maps i e a map of land change developed from calculating the map difference ideally model building and calibration should include the entire range of input data but existing sampling methods discard a lot of information leading to lower accuracy in predictions pijanowski et al 2014 another shortcoming of data sampling is when fitted models are used to forecast future land change scenarios values beyond the original input data used for calibration become more prevalent e g development has to proceed further from roads as most land for development proximate to roads has already been converted which can induce forecasting errors basse et al 2016 in addition if land change modeling is implemented with increasing spatial extents longer time frames and greater number of spatial inputs a scalable computing framework cf tayyebi et al 2016 pijanowski et al 2014 is needed to deal with these calibration challenges 2 4 data and study area we used land use datasets from three case studies 1 muskegon county michigan 2 lynnfield massachusetts boston and 3 south eastern wisconsin table 1 hereafter as muskegon boston and sewi the muskegon case study is located in the west central lower peninsula of michigan united states the region is currently dominated by forest in the northeast agriculture in the center and urban areas in the southwest it covers 1 292 km2 and urban nearly doubled in area between 1978 and 1998 time the boston case study consists of the town of lynnfield which is located in the northeastern massachusetts in 1971 lynnfield was composed of dense and sparse urban development 36 of the town mixed deciduous and coniferous forest 43 wetlands 6 and open land 12 by 1999 lynnfield was composed of dense and sparse urban development 45 mixed deciduous and coniferous forest 36 wetlands 6 and open land 10 the sewi case study is situated in the state of wisconsin in the north central part of the united states sewi comprises seven counties kenosha milwaukee ozaukee racine walworth washington and waukesha counties pijanowski et al 2006 pijanowski and robinson 2011 sewi is currently dominated by agriculture urban and forest which accounted for more than 86 of the landscape in 2011 47 27 and 12 between 2001 and 2011 the percentage of urban areas increased from 24 to 27 whereas agriculture and forest decreased by 2 and 0 3 respectively more than 60 of lost agriculture contributed to urban gain during the 10 year period sewi has undergone remarkable urbanization between 2001 and 2011 the urban expansion rate was 10 and densification rate was 1 5 for 2001 2011 we summarized the characteristic of each case studies in table 1 and additional information can be found in the literature blanchard et al 2015 tayyebi et al 2014b for each dataset we determined the difference between urban gain and non urban persistence between two time periods fig 1 we excluded the urban class in the initial time because it is impossible for this urban class to have any urban gain or non urban persistence across two time points furthermore a set of variables was defined for each cell serving as driving factors table 2 appendix two models i e ltm and ltm cluster were developed using six variables in 1978 for muskegon eight variables in 1998 for boston and sixteen variables in 1990 for sewi as inputs and urban change maps between two time periods 1978 1998 in muskegon 1971 1999 in boston 1990 2000 in sewi as outputs the cells of land use have a spatial resolution of 100 2 and 30 m in muskegon boston and sewi respectively our repository provides the three datasets and related r code omrani et al 2018 3 methods the ltm cluster modeling framework is composed of three phases 1 generating clustering using k means with spark 2 ltm modeling using hpc and 3 model accuracy assessment fig 2 summarizes the workflow the approach is implemented in the r programming language r 3 5 1 team 2013 within the spark environment zaharia et al 2010 to execute ltm cluster we used parallel processing including the following steps fig 3 split the dataset s into learning l and testing t subsets using stratified random sampling the l set is used for model calibration and the t set is used for assessing its performance see section 3 3 we selected 70 of the data to be used for training and the remaining 30 to assess model accuracy i e testing divide l into k clusters lk based on an advanced version of k means algorithm divide t into k clusters tk by assigning cells from t to the closest centroid of lk divide s into k clusters sk by assigning cells from s to the closest centroid of lk calibrate and validate ltm cluster based on lk and tk clusters respectively this is done by running ltm model for each cluster simulate urban change for each sk based on ltm cluster that is calibrated on lk 3 1 clustering using k means with spark due to its good performance the k means algorithm is chosen jain 2010 murray et al 2017 for the clustering prior to the learning scheme the number of clusters is determined by maximizing the average silhouette coefficient chiang and mirkin 2010 instead of a trial and error learning approach e g basse et al 2014 the silhouette coefficient is the average distance between all points in a cluster compared with the average distance between a point and its distance from the nearest cluster the k means algorithm randomly selects k observations as centers of groups and then calculates the euclidean distances in the feature space between each observation and the centers of all clusters next it assigns to each observation a group k based on the nearest e g shortest euclidean distance cluster center centers are iteratively updated until a number of groups is reached and all observations are assigned to a unique cluster because k means cannot handle large datasets due to an excess of computation we employed spark based k means an advanced version of the standard k means algorithm to address the computational burden spark based k means performs parallel calculations of distances across all data pairs gopalani and arora 2015 meng et al 2016 while requiring one parameter k determined through the average silhouette coefficient to segments the data into k clusters fig 3 shows the clustering prior to learning scheme using an advanced k means approach that ultimately minimizes the challenges of a large dataset while improving the ltm s performance due to the cutting edge spark based programming techniques this approach is suitable for exploring large datasets that cannot be stored in a computer s main memory studies highlight the fast computation of spark based k means compared to alternatives e g basic k means algorithm singh and reddy 2015 zaharia et al 2010 we also computed the average silhouette value for each value of k and for each region in the k means clustering routine a silhouette value rousseeuw 1987 measures cohesion of each value to its own cluster compared to another cluster silhouette values range from 1 perfect match to its own cluster and 1 fits better in another cluster a silhouette value of 0 means that the value is located in multi dimensional space and using a standardized euclidean distance precisely between two clusters finally to determine how the best value of k varies spatially we saved the k for the largest silhouette coefficient average for each cell and then mapped that for each study simulation area 3 2 land transformation model the land transformation model ltm uses ann to derive relationships between land change and multiple explanatory variables pijanowski et al 2014 ltm uses a multi layer perceptron with one hidden layer with a back propagation training method minimizing the mean square error mse the hidden layer is composed of a set of hidden units called neurons hidden layers in ann allow for the modeling of any nonlinear function schmidhuber 2015 and can approximate the relationship between influential factors i e predictor variables and outcomes e g a change no change categorical variable several studies have shown that ltm performs well in forecasting an outcome variable pijanowski et al 2005 omrani et al 2013 2015 more formally an outcome variable y contains categorical values in a specified set 1 2 c the variable y is expressed as a function of the input x x 1 x 2 x q and described by the following formula 1 p y k x ψ j 1 p v j k φ i 1 q w i j x i w 0 j v 0 k where ω ij and v jk are weights assigned to the connections between the input layer and the hidden layer and between the hidden layer and the output layer respectively ω 0j and v 0k are biases or threshold values in the activation of a unit φ is an activation function applied to the weighted sum of the output of the preceding layer i e the input layer ψ is an activation function applied to each output unit to the weighted sum of the activations of the hidden layer an ann generates outputs contained within the 0 1 interval but not equal to 0 or 1 a suitable choice for ψ e g a softmax or a sigmoid function maps the real axis to the interval 0 1 ann s output is a composition that correspond to a set of values summing to unity land use conversion are determined using the maximum probability strategy which assigns each observation to a unique class of land use from the ann s outputs pijanowski et al 2014 shafizadeh moghadam et al 2017c detailed descriptions of the ltm are available in pijanowski et al 2014 3 3 model calibration metrics to validate calibration performance from the ltm and ltm cluster models we measure the model s accuracy with four goodness of fit metrics first we created confusion matrices table 3 between the observed and predicted land uses and quantified the location errors pontius et al 2008 we generated error maps that illustrates configuration and location of all errors i e misses false alarm and correctly simulated values i e hits and correct rejections second we used percent correct match pcm pijanowski et al 2014 the total operating characteristic curve toc pontius and si 2014 the area under the curve auc pontius and batchu 2003 and the error rate er bradley et al 2016 to quantify the mismatch between the observed and simulated values pcm specifies the percentage of cells for the land use class correctly classified by a model true positives 100 urban change cells toc overcomes the limitations of the relative operating characteristic roc given that the roc fails in cases where some types of error are more important than others dodd and pepe 2003 toc evaluates the performance of non binary model output by varying the decision threshold between 0 and 1 pontius and si 2014 from the toc we extract the auc for each case study finally we use the er metric to measure the incorrect inclusion and incorrect omission of a class table 3 appendix larger values of pcm and auc as well as smaller value for er correspond to better model goodness of fit 4 results the silhouette coefficients for each k means calculation per study area are shown in fig 4 based on the results the goodness of fit of the ltm cluster k 3 is superior to that obtained in tayyebi et al 2014 and the traditionally parameterized ltm k 1 table 4 we found that the difference between applying clustering prior to the learning scheme versus not applying is significant based on the wilcoxon signed rank test of paired calibration statistics p 0 01 for the muskegon case study we also note there is an increase in performance in the pcm values from 69 for the ltm to 82 for ltm cluster table 4 maps of the best value of k based on the largest silhouette coefficient average are shown in fig 5 note that each region produces a different spatial pattern of k as it ranges from 1 to 3 designated as clusters 1 2 and 3 in muskegon the entire eastern portion of this county had the best value of k 1 the western portion had a k 2 interestingly following the us 31 highway corridor north south and m 36 highway east west and the more rural areas had a value of k 3 gray for boston the spatial pattern of k across the range of 1 through 3 selected again based on the largest silhouette coefficient averages is less distinct than muskegon k values of 1 for example are shown as small patches arranged at the edges of the 1990 map for urban cluster 3 i e k 3 locations are all located in the upper left corner of the region finally for sewi the map is dominated by k 1 and k 3 locations with k 3 following highway corridors and proximity to large 1971 urban patches and locations of k 1 being in the most rural areas in 1971 the error maps figs 6 8 show the spatial pattern of agreement and disagreement which we created by overlaying the reference change maps and the urban simulated maps the er rates for the ltm and ltm cluster were 37 and 31 33 and 32 and 24 and 22 for the muskegon boston and sewi simulations the ltm cluster displayed a performance improvement of 19 3 and 4 compared to the traditionally parameterized ltm model for the er calibration statistic for the muskegon boston and sewi simulations auc values also show improvements in accuracy for all three case study locations as well overall the results indicate that the ltm cluster framework provides an improvement in accuracy compared to the ltm model visualization of toc curves figs 9 11 also confirm that ltm cluster outperforms the traditionally configured ltm finally we compared the computing time across the ltm and ltm cluster model both models were implemented on a server dell poweredge r930 dual intel r xeon r cpu e7 8891 v4 2 80 ghz and a memory of 512 gb 16 32 gb ddr4 table 5 shows that ltm cluster model is faster than the ltm model for each dataset computation time decreased by 19 2 and 6 for the muskegon boston and sewi dataset 5 discussion we extended the ltm using prior clustering scheme and the spark environment we used spark rather than hadoop or mapreduce because spark has emerged as a popular choice to implement large scale ml applications on large datasets due to its ease in accommodating iterative learning processes zaharia et al 2010 2012 our key findings are first that the clustering prior to learning scheme is an easy and effective instrument to improve the model s performance and supports the processing of large land use datasets and second the improvements through ltm cluster compared to ltm are statistically significant the strengths and disadvantages of the clustering prior to learning scheme are as follows 1 data clustering improves the performance of the model since data in each cluster are homogenous i e have similar characteristics each cluster from the learning set is used to create a sub model as the ltm cluster each having its own parameters in addition we demonstrated that ltm cluster comprising several sub models is superior to the basic ltm based on a single model in terms of several performance metrics i e pcm toc and auc 2 clustering prior to learning scheme reduces the error rate table 4 3 dividing data across multiple clusters allows the model to scale up because data chunks from clusters are more easily maintained or processed in hpc each cluster is manageable in size and can be analyzed independently on separate processing units although the results show that our approach is promising there are still some limitations to our approach 1 the spark based k means algorithm requires one user defined parameter this is also the case for the majority of clustering approaches a drawback is the difficulty in determining an optimal number of clusters k multiple techniques exist to make that determination and most of them are based on finding the values of k which balance the search of minimizing the intra cluster distance and maximizing the inter cluster distances there is however no consensus on the best technique the choice of k may also often rely on the researchers expert opinion and interpreting the goodness of fit of a model in our application silhouette indices guided our choice in an objective manner rousseeuw 1987 the three datasets presented local maxima for k 3 clusters and larger values of k which vary with the studied dataset we found that allowing a larger number of clusters generated many clusters with small numbers of cells we believe that the ltm cluster framework can be extended in several respects besides the spark based k means prior learning clustering process other clustering approaches can be explored include dbscan birant and kut 2007 shen et al 2016 isodata abbas et al 2016 kim and liang 2017 hierarchical clustering liu et al 2017 and spherical k means tunali et al 2016 2 we used a different set of inputs across the case studies despite the model improvement compared to the traditional implementation of the ltm future studies should consider additional economic and social factors such as income levels employment rates and accessibility hagenauer and helbich 2018 which could greatly improve the model performance 6 conclusions this study introduced a framework with an innovative model calibration sampling scheme into ml based land change modeling our approach called the ltm cluster framework is based on clustering using a k means algorithm its application before the calibration step addresses the unmanageable size of the data by splitting it into clusters i e groups with similar input values for that purpose we implemented this framework in the spark computing environment for large scale data parallel processing we compared the performance of ltm cluster to the traditional ltm using numerous metrics for three case studies with high variability results provide clear evidence that applying the clustering prior to learning scheme is significantly superior compared to the basic ltm independently of the considered performance measure the percent correct match the area under the curve and the error rate improved on average by 9 11 and 4 respectively with ltm cluster over all three datasets more research is needed to address some of the limitations of the method including the determination of the appropriate number of clusters testing this method in other case studies of varying size and complexity together with other ml algorithms may be useful for a more in depth model evaluation findings from this study suggest that the ltm cluster framework successfully improves the prediction accuracy and that clustering prior to learning scheme may be valuable for other applications beyond land change modeling funding this work was part of the smart boundary and smart ca projects funded by the national research fund luxembourg fnr and liser research institute software and data availability the name of the software tool prototype introduced in this research is ltm cluster the developers are the authors of the study for the conceptualization hichem omrani generated the implementation of the tool in r please contact the first author for further information year first available october 2018 software required download the r software from the internet www r project org availability the datasets and r codes of the developed model are available on the mendeley repository dx doi org 10 17632 xnxrhw4fhv 2 omrani et al 2018 datasets are land use data inputs from three study areas described in this research the datasets and the tool can be used to perform a cross model comparison or for other purposes users are welcome to send their feedback to the corresponding author hichem omrani liser lu acknowledgements this research work has been partially carried out at the center for global soundscapes at purdue university the authors thank the four reviewers as well the editor for their valuable comments appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 004 appendix table 2 driving factors table 2 muskegon 1978 boston 1971 sewi 1990 1 distance to road distance to transport distance to road 2 distance to rivers distance to water distance to stream 3 distance to urban distance to urban distance to wetland 4 distance to lake distance to non urban distance to urban 5 distance to highways density of transport distance to forest 6 distance to water density of water distance to park 7 density of urban distance to agriculture 8 density of non urban distance to shrub 9 density of wetland 10 density of urban 11 density of shrub 12 density of forest 13 density of agriculture 14 elevation 15 slope 16 aspect note distances refer to euclidean distances in km slope is in elevation is an angle between 0 and 90 and density is a value between 0 and 1 table 3 confusion matrix from the testing set t table 3 simulated map observed map non urban persistence urban gain non urban persistence crs fas urban gain misses hits total sum of non urban persistence crs misses sum of urban gain hits fas note hits or true positive crs correct rejections or true negative misses or false positive fas false alarms or false negative pcm for urban gain hits total of urban gain er error rate fa misses cardinality t for the ltm model er for the ltm cluster fak missesk sum cardinality tk where k is the index of cluster fak and missesk are computed from the confusion matrix based on tk subset and sum cardinality tk cardinality t 
