index,text
25475,predicting coastal erosion requires an accurate morphodynamic model xbeach has been widely adopted to simulate storm induced coastal erosion because of the large number of model input parameters and their uncertainties sensitivity analysis is a crucial step toward reliable results this requires running a large number of computationally expensive simulations here we adopt a computationally cost effective approach based on the non intrusive polynomial chaos expansion method to quantify the sensitivity of xbeach to its input parameters the method is applied to the coastal erosion event caused by hurricane sandy along the barrier islands of new jersey in the united states the results show a spatial variation of the model sensitivity with increasing boundary conditions parameters interaction is found to decrease as the magnitude of boundary conditions increases causing a reduction in the nonlinearity of the model behavior and consequently leading to a reduction in the uncertainty of the model output keywords sensitivity uncertainty polynomial chaos expansions coastal erosion sediment transport x beach data availability data will be made available on request 1 introduction worldwide coastal areas are prone to storm induced beach erosion due to storm waves for example the united states coastline experienced hurricanes katrina irene sandy and dorian which caused coastal erosion in several states thus extra attention has been raised to quantify the vulnerability of coastal systems to erosion during extreme wave events using numerical models such as xbeach van thiel de vries 2009 mccall et al 2010 hartanto et al 2011 elsayed and oumeraci 2018 cohn et al 2019 simmons and splinter 2022 xbeach is an open source depth averaged numerical model that simulates morphodynamic bed evolution resulting from storm wave and surge conditions roelvink et al 2009 mccall et al 2010 splinter and palmsten 2012 kalligeris et al 2020 the model was first implemented by roelvink et al 2009 to simulate coastal erosion under the four different erosion regimes sallenger 2000 including swash collision overwash and inundation these regimes exert a range of forcings on coastal regions and are categorized by a limiting ratio of the dune height to water level during the storm a swashing regime exerts the least force and impact on coastal regions as the wave does not reach up to the dune and is limited to the berm and beach regions as the hydrodynamic forcings increase the dune starts to erode but is not yet overtopped this is referred to as the collision regime in overwash and inundation regimes transitioning into more adverse effects takes place waves start to overtop the dune and in some cases the water level inundates the dune xbeach is populated with many input variables and empirical parameters to capture the complex physical processes that govern the hydro and morphodynamics during extreme wave events this poses in turn a challenge in calibrating the model parameters thus different sensitivity analysis methods were used in previous studies to select the most important input parameters for calibration roelvink et al 2009 found that xbeach is sensitive to wave height and period and storm surge more than input parameters such as roller parameter and wet and dry slope of avalanching vousdoukas et al 2011 concluded that the model sensitivity to input parameters is directly proportional to the beach slope vousdoukas et al 2012 found that the model is sensitive to wave asymmetry and skewness the wet slope of avalanching the turbulence model and wave current interaction splinter and palmsten 2012 concluded that the model is more sensitive to the parameters of wave dissipation skewness and asymmetry than the foreshore beach profile slope the aforementioned studies conducted their sensitivity analysis based on the one at a time approach which varies one parameter while keeping the others constant this approach neglects the interactions between the input parameters which could lead to conflicting conclusions among different studies a more sound approach is the mont carlo approach which varies all the variables simultaneously but requires a large number of simulations for an accurate sensitivity analysis a robust approach was recently adopted by simmons et al 2017 2019 and biolchi et al 2022 who used the generalized likelihood uncertainty estimation glue method beven and binley 1992 and varied all the model parameters simultaneously their findings showed that the model is more sensitive to parameters that control the waves skewness and asymmetry bed friction and breaking wave dissipation though their methodology is sound and the results are valid the main drawback of the glue approach is the need for a large number of simulations e g 15 000 for each site or event rendering the method computationally expensive also the glue method does not provide the proportionality of the model sensitivity to the parameters as shown in section 3 to address the aforementioned shortcomings in sensitivity analysis we adopt a new approach based on the non intrusive polynomial chaos expansion nipce method nipce is an uncertainty quantification approach that is based on representing the output values as a function of the input parameters nipce can be used to generate a surrogate model that quantifies the variance and uncertainty in the model output with respect to the corresponding input wang et al 2020 illustrated the strength of pce compared to the sobol method a monte carlo based method in evaluating the effect of parameter interactions on hydrological models using a smaller number of simulations e g 1 000 000 versus 600 simulations many other studies shahsavani and grimvall 2011 hu et al 2015 fan et al 2016 kerrou et al 2017 christelis and hughes 2018 goeury et al 2022 koo et al 2020 ayyad et al 2021 used the pce methods in the sensitivity analysis of hydrodynamic hydrological and hydraulic models yet to the authors knowledge the use of a robust and cost effective sensitivity analysis method based on the pce in the field of coastal erosion modeling has not been introduced here we implement the nipce method to quantify the sensitivity of xbeach to its parameters and their interactions furthermore we show the effect of site characteristics and boundary conditions hydraulic forcing on the model sensitivity we present and discuss the results for the barrier islands of new jersey in the u s using hurricane sandy the study findings help to better understand the response of the xbeach model to changes in its parameters and efficiently calibrate the model using a relatively small number of simulations in section 2 we show the study area and the selected extreme event section 3 presents the used methods including the governing equations of the xbeach model selected calibration parameters sampling approach and nipce method the results are presented and discussed in section 4 finally conclusions and recommendations are presented in section 5 2 study area and boundary conditions the study area covers the barrier islands of the state of new jersey in the us the 2015 2019 census of the office of coastal management at the us national oceanic and atmospheric administration noaa estimated the total population of nj to be 8 9 million of which 7 15 million people live in coastal portions of the state ache et al 2015 the islands are the main line of defense of the back bays and they extend from cape may in the south to point pleasant in the north they comprise approximately 200 km fig 1 of the atlantic new jersey shoreline which extends 270 km in total hapke et al 2010 the boundary condition for xbeach is extracted from hurricane sandy on october 29 2012 the hurricane made landfall near atlantic city as shown in fig 1 based on noaa buoy s 44025 readings which is located off the new jersey coast the hurricane caused a maximum storm surge and significant wave heights of 2 6 and 9 85 m respectively this resulted in 38 casualties and 29 4 billion in losses statewide bilinski et al 2015 the dune system in nj experienced vertical losses from 2 to 6 m and volume losses between 25 and 150 m 3 m sopkin et al 2014 the northern part of island beach state park experienced the most destructive changes in dunes with an average vertical loss of 2 7 m and a maximum of 7 m sopkin et al 2014 fig 1 shows the sediment losses across the chosen profiles above the mean sea level msl we perform the sensitivity analysis for multiple sites along the study area one dimensional simulations are performed along 12 cross shore profiles as shown in fig 1 measurements of pre and post storm beach and dune elevation profiles are available from the coastal research center at stockton university under the new jersey beach profile network njbpn program the measurements extend from the back toe of the dunes to water depths ranging from 3 7 to 5 6 m for the pre storm measurements while post storm measurements extend from 1 to 4 5 m all the depths are with respect to the navd88 vertical datum we use the pre storm profile measurements to generate the computational grid for xbeach given that the measurements extend only to water depths as mentioned above we use the best available bathymetric data to build a continuous computational domain to a water depth of 15 m which is sufficiently deep to meet the conditions required by wave theories used by the model to generate waves at the offshore boundary of the grid the selected best available bathymetric data are from the 1 m resolution coned topo bathymetric model for new jersey and delaware collected by the u s geological survey usgs coastal and marine geology program in collaboration with the usgs national geospatial program ngp and noaa the computational grid has a spatial resolution of 10 m at the offshore boundary and gradually decreases to 1 m at a water depth of 8 m shoreward the grid between the water depth of 8 m and the land boundary has a constant resolution of 1 m the temporal peak water level and significant wave height at buoy 44025 are shown in fig 2 both the water level and wave height are larger on the right side of the hurricane landfall the total water level and wave characteristics at the offshore boundary of the xbeach model are obtained from a regional scale advanced circulation model adcirc luettich et al 1992 westerink et al 1994 coupled with the simulating waves nearshore model swan booij et al 1999 ris et al 1999 the computational mesh of the adcirc swan model is based on the mesh developed by the u s federal emergency management agency s region ii coastal storm surge study which was previously validated for both tropical and extratropical cyclones fema 2014 and was implemented in previous hurricane hazard studies e g marsooli et al 2021 and ayyad et al 2022 the mesh covers the western north atlantic ocean with a high resolution in the coastal waters of new jersey the mesh resolution is about 80 m in water depths smaller than 8 m and gradually increases to 8 km at a water depth of 50 m for the spectra mesh of the swan model we use 36 equally distributed directional bins and 37 frequencies logarithmically distributed between 0 033 and 1 hz meteorological forcing including the surface pressure and 10 m wind speed and direction is based on the reanalysis data from the oceanweather inc www oceanweather com tidal forcing is based on eight major tidal constituents k1 k2 m2 n2 o1 p1 q1 and s2 fig 2 compares the measured and calculated wave characteristics at the location of buoy 44025 which is the closest station to the study area with measurements 3 methods 3 1 xbeach xbeach is an open source depth averaged numerical model that simulates hydrodynamic and morphodynamic processes in coastal regions on a spatial scale of kilometers and temporal scale of storms roelvink et al 2009 mccall et al 2010 splinter and palmsten 2012 kalligeris et al 2020 xbeach runs in different modes including hydrostatic surfbeat and non hydrostatic in this work we employ the 1 d surfbeat version roelvink et al 2010 of xbeach which simulates short waves on a wave group scale by solving the short wave action balance equation given by 1 a t c x a x c y a y c θ a θ d w d f d v σ and roller energy balance equation is given by 2 d e r d t e r t e r c c o s θ x e r c s i n θ y s d where a is the wave action a s w σ σ is the intrinsic wave frequency s w is the wave energy density in each directional bin θ is the angle of incidence with respect to the x axis c x c y and c θ are wave action propagation speeds in x y and directional space and d w d f and d v are short wave energy dissipation due to wave breaking bottom friction and vegetation respectively e r is the roller kinetic energy c is wave celerity s is the loss of wave motion due to breaking and d is the dissipation of breaking waves xbeach simulates a variety of processes that include wave current interaction depth averaged velocities and variations of bed and suspended sediment transport making the calibration of xbeach a tedious task thus performing a sensitivity analysis for some of the important parameters is the traditional approach for calibrating xbeach based on the model developers recommendations and a literature review we choose six parameters shown in table 1 for calibration and sensitivity analysis in the present study although the governing equations of xbeach and its parameters have been described in detail in the literature we briefly discuss the selected model input parameters and their role in modeling there is a general agreement in the literature that the model is most sensitive to the f a c u a parameter elsayed and oumeraci 2017 vousdoukas et al 2012 biolchi et al 2022 vousdoukas et al 2011 nederhoff 2014 the f a c u a parameter is a calibration parameter introduced by van thiel de vries 2009 that sets the value for two other parameters f s k and f a s they calibrate the discretization of the wave skewness and asymmetry coefficients s k and a s respectively s k and a s are used in flow velocity calculations in the advection diffusion equation as below 3 u a f s k s k f a s a s u r m s where u r m s is the root mean squared velocity elsayed and oumeraci 2017 developed a relationship between the calibrated value of f a c u a and beach slope from closure depth to the maximum wave run up elevation they concluded that larger slopes require larger values of f a c u a for calibration this suggests less sensitivity to f a c u a at steeper slopes being directly related to the morphological processes the w e t s l p parameter defines the critical avalanching slope below water level to model the processes of wet sand slumping if the wet bed slope at a computational cell exceeds the slope set by the w e t s l p parameter the avalanched volume of sand in that cell will be redistributed to the adjacent cells to bring the slope back down to w e t s l p the w e t s l p parameter is a commonly used parameter for calibration and sensitivity analysis of xbeach harley et al 2011 vousdoukas et al 2012 bugajny et al 2013 armaroli et al 2013 palmsten and splinter 2016 gharagozlou et al 2020 biolchi et al 2022 vousdoukas et al 2012 bugajny et al 2013 and palmsten and splinter 2016 showed that the model is sensitive to w e t s l p and should be considered in the calibration on the other hand the glue analysis by simmons et al 2017 2019 and biolchi et al 2022 showed that the model is least sensitive to w e t s l p when the other parameters are simultaneously changing g a m m a is an important parameter in the wave breaking modeling process as it sets the maximum wave height in the roelvink wave breaking model roelvink 1993 which is given by 4 h m a x γ h δ h r m s where γ is the breaker index h is water depth δ is a non dimensional fraction and h r m s is the root mean square wave height many studies have considered g a m m a in the process of xbeach calibration biolchi et al 2022 harley et al 2011 armaroli et al 2013 in the wave dissipation process i e in the breaker model of xbeach the ratio of the maximum wave height to the water depth is controlled by a parameter so called g a m m a x and is usually considered for sensitivity analysis and calibration simmons et al 2019 harley et al 2011 biolchi et al 2022 during calibration a poorly chosen g a m m a x is attributed to overestimating the eroded volume harley et al 2011 armaroli et al 2013 a l p h a and b e t a are parameters that control the roelvink wave breaking and roller models respectively a l p h a represents a wave dissipation coefficient that is used in the calculation of wave energy dissipation due to breaking and is commonly considered a calibrating parameter for xbeach lashley et al 2018 kalligeris et al 2020 an increase in a l p h a will result in waves with less energy higher dissipation and thus lower erosion rates baldock et al 1998 the roller model in xbeach controls the transfer of momentum from breaking waves shoreward and into the water column causing wave setup return flows and alongshore currents roelvink et al 2010 svendsen 1984 b e t a is a breaker slope coefficient that is used to control the shoreward shift in the wave induced setup return flow and alongshore current such that an increase in b e t a causes a decrease in the shoreward shift kalligeris et al 2020 this is because b e t a is directly included in the roller energy dissipation as shown below 5 d r 2 β g c g e r where d r is the roller dissipated energy g is the gravitational constant and c g is the wave group velocity following wave breaking the rollers apply shear stress to the water column and presented as 6 τ roller ρ g r l β s where ρ is water density r is the roller area and l and β s are the length and slope of the breaking wave in one dimensional simulations increasing b e t a in the roller models causes a shoreward flux of mass and momentum this results in an offshore shift in wave setup and undertow return flow forcings svendsen 1984 faria et al 2000 3 2 non intrusive polynomial chaos expansions nipce the nipce approach is implemented to evaluate the sensitivity of xbeach model output represented by the eroded volume v to xbeach input parameters table 1 and their interactions while varying the hydraulic forcings and site characteristics the nipce approach represents the realizations model output as a function of xbeach input parameters x and their associated stochastic variations represented by the random variable ξ by implementing the stochastic galerkin projection the nipce describes a stochastic process by generating surrogate models that consist of nipce coefficients in this manner and as illustrated by reagana et al 2003 hosder et al 2006 mola et al 2011 konda et al 2010 thacker et al 2012 jacquelin et al 2015 thacker et al 2015 yildirim and karniadakis 2015 gauchi et al 2017 and ayyad et al 2021 the implementation of the nipce allows for the decomposition of the output as follows 7 v x ξ i 0 p v i x ψ i ξ where v is the output here the volume of erosion v i x is the deterministic component ψ i ξ is the random basis function that corresponds to v i x and p 1 is the number of output modes and is given by 8 p 1 n p n p where p is the polynomial chaos order and n is the number of xbeach input parameters chosen to be varied in this form of the nipce the choice of the basis function depends on the random sampling function used to generate the p 1 combinations of the chosen xbeach input parameters as shown in simmons et al 2019 and since the choice of a certain value from the sampling ranges of the chosen xbeach input parameters does not have a higher probability of one value over the other we randomly selected the xbeach input parameters x using the uniform distribution given by 9 x i a i ξ i b i where a i and b i are respectively the upper and lower bounds of the xbeach input parameter x i since the xbeach input parameters are uniformly distributed then the basis function to be used is the legendre polynomial which can be expressed in the rodrigues or ivory jacobi formula as follows sudret 2008 hampton and doostan 2015 hadigol and doostan 2018 wang et al 2018 2021 ayyad et al 2021 10 l p ξ 1 ξ n 1 2 p p p ξ 1 c 1 ξ k c n 1 2 ξ t ξ 1 p for 1 ξ n 1 where k 1 n c k p and c k s are integers if we let p 1 be the number of realizations n then the polynomial expansion coefficients v i x are determined by rewriting eq 7 in the matrix form hosder et al 2006 as 11 v 1 v 2 v n ψ 0 ξ 1 ψ 1 ξ 1 ψ p ξ 1 ψ 0 ξ 2 ψ 1 ξ 2 ψ p ξ 2 ψ 0 ξ n ψ 1 ξ n ψ p ξ n v 0 v 1 v p the system of eqs 11 is a form of the nipce that is convergent in the mean square sense ghanem and spanos 2003 choi et al 2004 if n equals to p 1 then the matrix could be directly inverted to find the expansion coefficients v i however these sets of v i s generated by this solution are dependent on the choice of the sampling vector hosder et al 2006 thus different samples of the x s will result in a different set of α i s for the solution to be independent of the sampling set a larger number of realizations should be used i e n p 1 ghanem and spanos 2003 choi et al 2004 wang et al 2021 and ayyad et al 2021 the system of eqs 11 would then be solved by minimizing the errors in the least squares sense sampling from the independent variables space should be done with care as it has a significant impact on computing the polynomial coefficients l p wang et al 2021 the sampling technique used should guarantee that all portions of the independent variables space chosen ranges in table 1 are sampled traditionally the random sampling method was used to achieve this sampling criterion thus to assure the independence of v i s and consequently the convergence of the nipce it is required to have a very large number of samples n 10 000 therefore we use the latin hypercube sampling lhs to reduce the number of samples while assuring the coverage of the entire sampling space choi et al 2004 mola et al 2011 ayyad et al 2021 after solving eq 11 the nipce coefficients can be used to analytically approximate statistical moments of the realizations such as the mean μ 12 μ v 0 x and standard deviation σ 13 σ 2 i 1 p v i 2 x ψ i ξ 2 where 2 is the l 2 norm i e euclidean norm validation of the nipce should seek an answer to the following question how many polynomial orders p and sampling combinations n are needed so that the mean and standard deviation from the nipce eqs 12 and 13 are independent of n the answer is usually found by running a large number of monte carlo simulations 10 000 and then comparing the mean and standard deviation from those monte carlo simulations with those calculated from eqs 12 and 13 pettit et al 2010 ayyad et al 2021 an alternative approach to the computationally expensive monte carlo method is a recently adopted method for validating the pce so called the leave one out loo cross validation error ϵ l o o molinaro et al 2005 blatman 2009 blatman and sudret 2010 2013 marelli and sudret 2015 al ghosoun et al 2021 in this approach one builds n models m p c i such that each model uses a reduced sample χ x i x j j 1 n j i then calculate the square error between the resultants v from the main model m x calculated using all the samples n and those calculated from the model m p c i using reduced sample n 1 14 ϵ l o o i 1 n m x i m p c i x i 2 i 1 n m x i μ ˆ y 2 molinaro et al 2005 blatman 2009 blatman and sudret 2010 marelli and sudret 2015 yang et al 2017 and tran and kim 2021 considered that a perfect model has ϵ l o o error of an order less than o 1 0 4 slim et al 2017 considered that a good model has an error of order less than o 1 0 2 3 3 pce implementation to xbeach we use the lhs technique to sample three sets of 600 combinations of the selected input parameters for each study site i e each cross shore profile in fig 1 two sets were sampled from the sensitivity sampling ranges shown in table 1 with a constant sediment size d50 of 0 5 mm for set 1 and 0 25 mm for set 2 the aim of these two sets is to separate the effects of input parameters from the effects of the sediment size that varies from one site to another the sensitivity ranges were selected based on preliminary runs by varying the parameters one at a time and choosing a range where the model results perturbed around the post storm measured profiles sample set 3 uses an expanded range for the input parameters to show the effectiveness of the lhs in capturing a suitable set of parameters from the sampling space of parameters the expanded ranges were chosen based on preliminary runs where each parameter was varied separately up to a point that realistic model outputs were obtained for example the upper limit of the range assigned to f a c u a is 0 4 since larger values led to unrealistic volumes of erosion the actual sediment size at each profile flynn 1999 lemke and miller 2018 is used in sample set 3 the d50 varies between 0 2 and 0 3 mm among the profiles on the left side of the hurricane sandy landfall and between 0 3 mm to 0 6 mm on the right side of the landfall the calculated v sensitivities from sample sets 1 and 2 of simulations will help understand the effect of site characteristics d50 and slope and boundary conditions surge and wave height on the sensitivity of xbeach to the selected parameters the calculated v i x vector will contain different components of the sensitivity of xbeach output v to its parameters the first order components that represent v x i explain the sensitivity of xbeach output to i th xbeach parameter i e whether the parameter has a decreasing or increasing effect on the model output and which parameter has the most effect on the output the second order component 2 v x i x j explains the interaction between the i th and j th parameters which represents the non linearity introduced to the model output due to the interaction between every two parameters using these measures of sensitivity we will use sample sets 1 and 2 to show how the sensitivity of xbeach responds to changes in boundary conditions and site characteristics we use the beach slope to quantify the effect of site characteristics on the model sensitivity to its parameters sampling set 1 the slope will help in understanding the wave breaking regime through the surf similarity parameter ξ battjes 1975 as follows 15 ξ tan α h 0 l 0 1 2 where t a n α is the beach slope h 0 and l 0 respectively the temporally averaged deep water wave height and length during the storm deep water is taken at the deep end location of the computational grid calculating the beach slope is a subjective matter and depends on what is being considered for analysis for example if a shore structure is considered then the swashing region slope slope of the structure or seabed leading to the structure is considered as a representative slope in calculating ξ here we consider the slope to be the bed slope from the upper swashing limit to the depth where waves interact with the seabed depth of interaction d d for a given profile is determined by examining the final bed elevations from all simulations for that profile and identifying the most seaward water depth where no bed elevation change has taken place we adopted the method proposed by stockdon et al 2006 to find the upper swashing elevation as the following 16 u η ˆ 2 η t where η t is the time series of the shoreline elevation taken from the last wetted computational cell on the beach dune and η ˆ is the maximum wave setup calculated as the temporal mean of η t the slope will then be calculated as the slope of the best linear line fitted to the elevation of points between d and u i e d z b u where z b is the average modeled bed elevation during the storm the calculated slope and surf similarity parameter at each profile are shown in fig 3 the profile slope and consequently the surf similarity parameter increase from the left side to the right side of the landfall i e south to north suggesting rougher wave breaking regimes on the right side of the landfall 3 4 xbeach calibration the calibration of xbeach has been considered by many of the sensitivity analysis studies cited earlier in most of these studies the brier skill score bss is considered to measure the model s accuracy the bss is a measure of model skill that quantifies the model s ability to reproduce the morphological evolution during a certain event and is formulated as 17 b s s 1 σ z 0 z m 2 σ z 0 z b 2 where z m is the final modeled profile elevation z o and z b are the post and pre storm measured profile elevations observed respectively a perfect bss score would be 1 which indicates that the model perfectly simulated the morphological changes van rijn et al 2003 classified the model performance based on the bss such that results are considered to be bad if bss 0 poor if 0 bss 0 3 fair if 0 3 bss 0 6 good if 0 6 bss 0 8 and excellent if 0 8 bss 1 although most studies used the bss they adopted different calibration approaches for example elsayed and oumeraci 2017 calibrated each profile separately by identifying a parameter set with the best bss vousdoukas et al 2012 chose the parameter set that produced the best bss across different beach profiles and events for the same site callaghan et al 2013 and de vet et al 2015 concluded that different sallenger regimes can influence calibration simmons et al 2017 2019 used the glue method to combine the likelihood of parameter sets with bss exceeding a behavioral bss limit across multiple boundary conditions here we perform the calibration process for each profile using the results of the sensitivity analysis based on sample set 3 which considers the actual sediment size for each site to account for equifinality we determine the calibrated value for a particular parameter by averaging the values of that parameter used in the top 10 simulations that had the highest bss scores then we re run a simulation using the averaged values to ensure the accuracy of the model is preserved 4 results and discussion 4 1 pce validation we calculated the ϵ l o o error eq 14 to quantify the accuracy of the nipce model for sets 1 and 2 at each profile fig 4 shows that the ϵ l o o error is in order of 10 4 at all study sites which is within the acceptable range the low errors confirm that the chosen number of simulations n 600 is enough to cover the uncertainty in the model output and thus the chosen polynomial order p 2 can capture the model behavior therefore the results from the nipce model can be considered reliable for sensitivity and uncertainty quantification 4 2 sensitivity analysis 4 2 1 effect of boundary conditions the results presented and discussed here are based on sample set 1 which accounts for the sensitivity of the model to its parameter while using a constant d50 among all study sites i e removing the effects of sediment size on the model fig 5 shows the absolute value of nipce s first order component of the vector v i x i e v x i this component describes the magnitude of the effect of each model input parameter e g f a c u a on the model output i e the volume of erosion a positive value of v x i represented by solid lines means a direct relationship between the model input parameter and the output i e an increase decrease in the parameter results in an increase decrease in the simulated volume of erosion a negative value of v x i represented by dashed lines means an inverse relationship i e an increase decrease in the parameter results in a decrease increase in the volume of erosion the results show that the f a c u a parameter has the largest effect on the model output when compared to the effects of the other parameters because of its significant effect on the calculated net cross shore sediment transport this finding is consistent with the literature roelvink et al 2010 vousdoukas et al 2012 bugajny et al 2013 nederhoff 2014 elsayed and oumeraci 2017 as shown in eq 3 f a c u a is used in the calculations of flow velocity u a which is directly related to the shear stress calculations in the model thus higher values for f a c u a would increase the flow velocity u a and consequently the shear stress which in turn increases onshore sediment transport on the other hand the model sensitivity to f a c u a decreases from the left to the right side of hurricane sandy s landfall location which is accompanied by an increase in storm surge and wave height as shown in fig 3b this suggests higher sensitivity of the model to non linearity of the waves represented by f a c u a under lower storm impact regimes which is consistent with the findings of elsayed and oumeraci 2017 simmons et al 2019 and biolchi et al 2022 the effect of profile slope on xbeach sensitivity to f a c u a can be inferred from figs 3a and 5 the effect of f a c u a on the model is more pronounced fig 5 when the slope is lower fig 3 a i e the model is more sensitive to f a c u a in case of steep profiles than mild ones this elucidates that beach slope has direct control of the onshore sediment transport induced by the wave skewness and asymmetry which is consistent with past studies by vousdoukas et al 2011 nederhoff 2014 and elsayed and oumeraci 2017 although our results show a strong sensitivity to f a c u a a previous study harley et al 2016 found that f a c u a is not the most significant parameter this can be due to the sampling approach that is adopted in harley et al 2016 where f a c u a was sampled within a small range between 0 1 and 0 15 the limited number of sampled values resulted in missing the behavior of the model in the rest of the sampling space this shows the effect that the sampling techniques have on sensitivity analysis leading to conflicting conclusions in understanding the model a reliable model calibration requires a sampling space that entirely covers all potential values of the model parameters which is done in the present study by adopting the lhs technique in the wave breaker model g a m m a controls the maximum wave height such that larger values of g a m m a delay wave breaking this results in larger waves reaching the shoreline which could cause more erosion roelvink 1993 fig 5 shows a direct relation represented by the solid line between the g a m m a parameter and the model output the plot shows that xbeach sensitivity to g a m m a is ranked second after f a c u a and is directly proportional with the boundary conditions and profile slope as concluded from figs 3 and 5 this is attributed to the fact that the resultant morphological change is very sensitive to the location of wave breaking relative to the shoreline i e whether waves break closer or farther from the shoreline due to larger or smaller values of g a m m a these findings agree with simmons et al 2017 2019 and biolchi et al 2022 where they showed a higher sensitivity to g a m m a at the overwash than the collision condition in xbeach s dissipation formula xbeach a l p h a controls the dissipated wave energy due to depth induced wave breaking lashley et al 2018 an increase in a l p h a results in more intense wave breaking resulting in a decrease in the waves energy before they reach the shoreline thus the erosion rates above the msl would decrease baldock et al 1998 this is consistent with our results in fig 5 where the model is shown to be inversely proportional to a l p h a represented by the dashed line figs 3 and 5 show that the sensitivity of xbeach to a l p h a increases as the boundary condition and beach slope increase as waves become larger i e intense storm regimes their dissipated energy increases resulting in a more pronounced effect of a l p h a i e an increase in the model sensitivity to a l p h a also a steeper slope reflective beach causes larger waves to reach the surf zone this causes larger energy dissipation when they break therefore increasing the sensitivity of the output to a l p h a thus the steeper the slope fig 3a the more important information passes from the dissipation model to xbeach i e a higher sensitivity to a l p h a the roller model in xbeach is governed by b e t a which is shown to have an inverse relation with the eroded volume of xbeach as presented in fig 5 larger values of b e t a cause more roller wave energy dissipation and less erosion above msl this is attributed to its direct involvement in the calculation of the dissipated roller energy eq 5 the increase in b e t a leads to a shoreward flux of mass and momentum that in turn results in an offshore shift in wave setup and undertow return flow forcings svendsen 1984 faria et al 2000 kalligeris et al 2020 rafati et al 2021 this shifting in forcings causes underwater bars to become more prominent which increases the wave energy dissipation due to depth induced breaking similar to g a m m a and a l p h a b e t a exhibits an increasing sensitivity as the boundary conditions and slope increase as shown in figs 3 and 5 as waves become larger their roller shear stress applied to the water column increases this in return causes the model to become more sensitive to b e t a due to the direct usage of a l p h a and b e t a in calculating the dissipated energy respectively from depth induced breaking waves and rollers these parameters are intertwined physically and mathematically in the model this can be depicted in fig 5 as the sensitivity of the model to a l p h a and b e t a is approximately the same fig 5 shows that the g a m m a x is the second least sensitive parameter this parameter determines the ratio of maximum wave height to water depth thus increasing the g a m m a x results in more energetic waves reaching the shoreline which causes more erosion this behavior is successfully captured by nipce where fig 5 shows a direct relation of xbeach output to g a m m a x represented by the solid line although harley et al 2011 and armaroli et al 2013 found that g a m m a x is a key parameter in the calibration process simmons et al 2017 2019 and biolchi et al 2022 and our results show a subtle impact of this parameter on the model output this is attributed to the effects of interactions among different parameters when varied alone e g in the preliminary simulations in the present study g a m m a x had a considerable effect on the model output however when g a m m a x and other parameters were simultaneously varied the model output showed a subtle sensitivity to g a m m a x unlike other parameters considered in this study g a m m a x is not directly used in the governing equations but it rather sets an upper limit to the g a m m a parameter therefore as shown in fig 5 the sensitivity of the model to g a m m a x does not respond to changes in the boundary conditions or slope our results show that the model is least sensitive to the w e t s l p parameter as shown in fig 5 this parameter represents the maximum slope of wet sand particles that can be reached after an avalanche is initiated this finding contradicts our preliminary runs mentioned in section 3 3 where w e t s l p caused considerable erosion when it was independently varied while the other parameters were kept constant this finding is in agreement with studies that changed the parameters simultaneously during the calibration process simmons et al 2017 2019 biolchi et al 2022 other studies vousdoukas et al 2012 bugajny et al 2013 palmsten and splinter 2016 that are in disagreement with our results neglected the interactions among parameters due to either the use of the one at a time sensitivity approach or choosing discrete values of the parameters rather than sampling a sufficiently large number of samples point discrete sampling from the parameter range would miss some aspects of the model behavior as the whole sampling space is not considered this suggests that interactions between different parameters are important when calibrating the model although parameters interaction is a key part of sensitivity analysis other model characteristics e g mesh size could play an important role in its behavior in simmons et al 2017 and our sensitivity analysis the effect of mesh size was not considered however gharagozlou et al 2020 found that the xbeach sensitivity to w e t s l p changes with the mesh spacing size in the cross shore direction this suggests that the w e t s l p is sensitive to the characteristics of the mesh size rather than boundary conditions and profile slope the results presented and discussed above reveal the capabilities of the introduced nipce method compared to the traditional sensitivity analysis adopted in the literature first the nipce method accounts for simultaneous changes in multiple parameters and in turn resolves the interactions among the parameters which is quantified in section 4 2 3 the interactions among parameters could weaken or strengthen the sensitivity of the model to some parameters e g the w e t s l p parameter as discussed earlier second the nipce method can determine whether the model output has a direct or inverse relationship with a parameter for example our analysis shows that while the calculated volume of erosion is inversely related to f a c u a e g an increase in f a c u a leads to a decrease in erosion it is directly related to g a m m a traditional sensitivity methods as well as the most recently adopted glue method simmons et al 2019 biolchi et al 2022 show no distinction in the proportionality direct inverse effect of g a m m a and f a c u a on the model output eroded volume 4 2 2 effect of sediment median size as shown in the previous section boundary conditions and profile slope affect xbeach s sensitivity to its parameters therefore changing its behavior however sediment size is considered a key parameter in coastal sediment transport such that its increase would results in less erosion van rijn 2011 here we consider the effect of sediment size on the behavior of xbeach this is important as the modeler might not be certain about the sediment size in the model to that end we show the sensitivity of the model to its parameters using the first order derivative i e v x i from sets 1 d50 0 5 mm and 2 d50 0 25 mm in fig 6 our results suggest that changing d50 from 0 5 mm to 0 25 mm has a subtle and consistent effect on the sensitivity of f a c u a and a l p h a but no effect on the other parameters from eq 3 f a c u a is used in calculating the flow velocity which is directly related to the shear stress applied to the sediment in the model as sediment size increases it naturally retards the transport process this results in a reduction in the model sensitivity to parameters that govern the shear stress applied to the sediment i e f a c u a this is captured by the nipce method as shown in fig 6a where the sensitivity of xbeach to f a c u a has slightly reduced when the sediment size is doubled therefore bigger changes in f a c u a are required to transport the sediment this suggests that with larger sediment sizes the model would perform better for larger smaller values of f a c u a i e shoreward seaward sediment transport this justifies the need for separate profile calibration for a site with varying sediment sizes a l p h a also showed a subtle but consistent change in sensitivity however contrary to f a c u a the sensitivity of a l p h a increased when the sediment size increased from 0 25 mm to 0 5 mm as shown in fig 6b this suggests that for larger sediment sizes the model is more sensitive to changes in the wave breaking dissipation formula i e changes in a l p h a during an extreme event a profile with a larger sediment size will evolve into a steeper sloped profile steeper profiles have narrower surf zones hence the energy of the incoming waves would dissipate within a smaller surf zone causing more erosion dean and dalrymple 2004 therefore small changes in the dissipation process is more prominent in a narrower surf zone steeper slope larger sediment than in a wider surf zone milder slope smaller sediment the rest of the parameters do not show a consistent change in xbeach sensitivity when changing the sediment size however the small to no change in the sensitivity when changing the sediment size could be because of the model s insensitivity to changes in the sediment size the sensitivity of xbeach to sediment size is not considered in this study nonetheless previous studies have shown that the sediment size has a negligible effect on the model output for example rafati et al 2021 did not report a change in the model behavior for d50 of 0 15 mm and 0 3 mm brandenburg 2010 concluded that the output of xbeach is not sensitive to changes in sediment size as it does to the beach slope and boundary conditions the results of mccarroll et al 2021 also show that xbeach is more sensitive to wave height and period than sediment size while previous studies found a negligible effect of the sediment size on the model output we showed that the effect of sediment size on the sensitivity of the model to its parameters is also negligible compared to the boundary conditions and profile slope 4 2 3 parameter interactions and nonlinearity a literature review shows that to the authors knowledge the effects of interactions among the xbeach parameters on the model performance have not been investigated yet because of the limits of the sensitivity analysis methods adopted in previous studies the nipce method in the present study allows us for the first time to quantify these effects throughout the second order component of the nipce vector 2 v x i x j fig 7 this vector quantifies different aspects of the model s response to its parameters interactions the sign of this vector shows whether the interaction between two particular parameters has an accumulation or cancellation effect on the model performance a negative value represented by the dashed line indicates that the interaction between the two parameters cancels out the effect of each parameter on the model output while a positive sign represented by solid lines indicates that the interaction builds upon each parameter s effect such information may be obtained from the first order components of the sensitivity analysis as shown in fig 5 however a linear superposition of the first order components will not resolve the non linearity introduced to the model due to the interactions among the parameters which is resolved by the magnitude of 2 v x i x j finally using 2 v x i x j one can compare how interactions between different parameters have an impact on uncertainty in the model output by examining the magnitude of 2 v f a c u a g a m m a shown in fig 7 one can conclude that f a c u a and g a m m a have opposite effects on the model and thus they are expected to level out each other s effects this conclusion may be also depicted from the sign and magnitude of v f a c u a and v g a m m a from fig 5 however the first order component of sensitivity does not quantify the nonlinearity in the interaction between the parameters therefore it does not reliably explain the uncertainty in the model behavior as the modeler varies g a m m a and f a c u a simultaneously for example if the magnitude of 2 v f a c u a g a m m a is very small a 10 increase in f a c u a and g a m m a will cause respectively 10 decrease and increase in the simulated volume of erosion resulting in almost no change in the eroded volume however since the second derivative is showing large values fig 7 then this interaction is highly nonlinear and increases the uncertainty in the model output when the modeler varies f a c u a and g a m m a simultaneously by comparing the other second order derivatives to 2 v f a c u a g a m m a it can be concluded that the effect of f a c u a g a m m a interaction introduces more non linearity and uncertainty in the model behavior than for example the f a c u a a l p h a interaction the f a c u a parameter shows higher interactions with g a m m a followed with a l p h a and b e t a the strong interaction between f a c u a and g a m m a 2 v f a c u a g a m m a suggests that the results of the governing equations for wave nonlinearity and wave energy dissipation are highly interacting with each other causing the model to behave nonlinearly this can be traced back in the model through h r m s root mean square wave height and u r m s root mean square flow velocity yet this strong interaction extends further in the numerical implementation of the model therefore tracing the interaction between parameters is extremely complicated and is out of the scope of this work however thanks to the robustness of the nipce and the ability of the legendre polynomial to capture complicated relationships between the xbeach output and the realizations we can qualitatively assess the interactions through fig 7 the quantified interactions among the model parameters may be considered as guidelines for modelers to understand that for example the effect of varying f a c u a and g a m m a on the model output is larger than the effect of varying f a c u a and a l p h a given that fig 7a shows 2 v f a c u a g a m m a 2 v f a c u a a l p h a therefore the f a c u a g a m m a interaction causes more nonlinearity than the f a c u a a l p h a interaction and therefore introduces larger uncertainty in the model output the most important nonlinear interaction between different parameters is f a c u a g a m m a followed by f a c u a a l p h a f a c u a b e t a and g a m m a b e t a these interactions decrease with increasing boundary conditions except for g a m m a b e t a interaction suggesting that the effect of nonlinear interactions among parameters reduces with larger boundary conditions this is an unexpected conclusion as larger waves generally mean bigger disturbance to shorelines and morphodynamic processes and thus larger uncertainty this can be explained through the behavior of the model in fig 5 as the boundary conditions increase the sensitivity of the model to other parameters other than f a c u a increases causing the sensitivity of the model to be shared across different parameters 4 3 xbeach calibration findings from sensitivity analysis inform the model calibration which aims to select the combination of model parameters that results in reliable predictions according to the nipce validation presented earlier in fig 4 a sample size of 600 simulations is sufficient for nipce to capture the model behavior here we use the results of the 600 simulations from sample set 3 which used the actual sediment grain sizes to determine the calibrated values of the input parameters we calculated a calibrated value for each parameter by averaging the parameter s values used in the top 10 simulations that had the highest bss scores the bss scores of all simulations of sample set 3 as well as the calibrated model using the averaged top 10 simulations parameters are shown in fig 8 the minimum bss across all calibrated profiles is 0 8 except for profile 246 which behaved fairly with a minimum bss of 0 47 variation in the bss scores represented by the height of the box plot along the shoreline decreases as boundary forcings and model sensitivity increase i e from profile 212 to 156 as the model sensitivity to a parameter increases the parameter becomes more important in the calibration process thus the modeler should reduce the uncertainty in the choice of suitable calibration ranges for that parameter this can be done by running preliminary simulations to assure that the selected ranges are reasonable for calibration for example in section 3 3 before choosing the calibration ranges preliminary runs aided in restricting the upper limit of f a c u a to 0 4 where values above 0 4 showed a big deviation from the post measured profile wrong choices in a highly sensitive parameter as f a c u a will cause more errors in the simulations this reduction in the uncertainty can be depicted in fig 9 where the spread uncertainty in the parameters values used in the top 10 simulations is resembled by the box plot fig 9 shows that f a c u a has a smaller spread thus the modeler should be more certain about the choice of f a c u a compared to other parameters therefore uncertainty in f a c u a would cause larger errors in the model outputs than for example uncertainty in the w e t s l p in the same manner as the sensitivity of the model to a parameter increases fig 5 the uncertainty in selecting a proper value for that parameter decreases spread shown in fig 9 as discussed in section 4 2 1 the sensitivity of the model is directly proportional to the boundary conditions this poses a challenge as the increase in the boundary forcings would make the selection of a correct value of the parameter difficult this can be inferred from fig 9 as the spread of the values used in calculating the calibrated parameters decreases i e the height of box plots decreases with the increase in the boundary condition 5 summary and conclusions this study introduced a new approach to sensitivity analysis and calibration of morphodynamic models the non intrusive polynomial chaos expansions nipce was implemented to quantify the sensitivity of the xbeach model to six parameters f a c u a g a m m a a l p h a b e t a g a m m a x and w e t s l p the method was applied to the coastal erosion event induced by hurricane sandy at a series of cross shore profiles along the new jersey barrier islands the latin hypercube sampling lhs method was adopted to generate three sets of 600 unique combinations of the parameters two of the sample sets had two different constant sediment grain sizes across all profiles to study the effect of sediment size on the sensitivity of the model the nipce was validated using the leave one out cross validation error the results of the nipce application to a sample set with constant d50 showed a relation between the model sensitivity to the selected parameters and the boundary conditions the sensitivity of the model output volume of erosion to the dissipation and roller model parameters g a m m a b e t a and a l p h a increases as the wave height and the cross shore profile slope increase on the contrary the sensitivity to the wave non linearity parameter f a c u a decreases as boundary conditions and the profile slope increase the model did not show any significant sensitivity to the parameters w e t s l p and g a m m a x comparing the results of the nipce from the two sample sets with constant d50 showed the effect of changing sediment grain size on the model sensitivity to the selected parameters the model sensitivity showed a consistent but a minor change in the sensitivity of f a c u a and a l p h a when changing the sediment size other parameters showed no change in the model s sensitivity this may be attributed to the choice of parameters as all the selected parameters in this study influence the wave physics in the model but are not included in the sediment transport governing equations the sensitivity of the model to parameters that are directly related to sediment transport e g maximum shield s stress parameter may respond to changes in sediment characteristics such as d50 which remains to be investigated in future studies this study quantified for the first time the effects of interactions between parameters on the sensitivity of the model this was done through the second order component of the nipce which explains the non linearity introduced to the model by the interaction between two parameters the results show that the model s non linearity decreases with increasing boundary conditions and site characteristics it was shown that the strongest interaction among the selected parameters in this study is the f a c u a g a m m a interaction followed by f a c u a a l p h a b e t a g a m m a and f a c u a b e t a the interaction between other parameters shows negligible effects on the model set 3 was sampled using larger ranges of parameters to demonstrate the capabilities of lhs in creating a set of parameters combinations with a small sample size e g 600 in this study for efficient model calibration excellent bss scores were obtained for the simulations that were conducted using the averaged parameters of the top 10 simulations with the highest bss scores bss 0 8 the variation in the parameters used in the top 10 simulations is smaller for f a c u a than other parameters which is due to the stronger sensitivity of the model to f a c u a except for f a c u a the variation in the parameters used in the top 10 simulations reduces as boundary conditions increase finally the results of this study are subject to a certain extreme event specific site characteristics and the selected parameters and their ranges similar analyses under different environmental conditions e g other storm events and study areas would help to get a better and perhaps universal insight into the sensitivity of the model to its parameters declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests reza marsooli reports financial support was provided by noaa new jersey sea grant consortium acknowledgments this publication is the result of work sponsored by the new jersey sea grant with funds from the national oceanic and atmospheric administration noaa office of sea grant u s department of commerce under noaa grant na18oar4170087 and the new jersey sea grant consortium united states the statements findings conclusions and recommendations are those of the author s and do not necessarily reflect the views of new jersey sea grant or the u s department of commerce njsg 22 1000 we thank dr andrew cox from oceanweather inc for providing us with the meteorological reanalysis data for hurricane sandy we would like to thank the three anonymous reviewers for their comments and suggestions which helped us to improve the quality of this paper software availability the software used in this paper xbeach is an open source software that can be accessed and downloaded at https oss deltares nl web xbeach 
25475,predicting coastal erosion requires an accurate morphodynamic model xbeach has been widely adopted to simulate storm induced coastal erosion because of the large number of model input parameters and their uncertainties sensitivity analysis is a crucial step toward reliable results this requires running a large number of computationally expensive simulations here we adopt a computationally cost effective approach based on the non intrusive polynomial chaos expansion method to quantify the sensitivity of xbeach to its input parameters the method is applied to the coastal erosion event caused by hurricane sandy along the barrier islands of new jersey in the united states the results show a spatial variation of the model sensitivity with increasing boundary conditions parameters interaction is found to decrease as the magnitude of boundary conditions increases causing a reduction in the nonlinearity of the model behavior and consequently leading to a reduction in the uncertainty of the model output keywords sensitivity uncertainty polynomial chaos expansions coastal erosion sediment transport x beach data availability data will be made available on request 1 introduction worldwide coastal areas are prone to storm induced beach erosion due to storm waves for example the united states coastline experienced hurricanes katrina irene sandy and dorian which caused coastal erosion in several states thus extra attention has been raised to quantify the vulnerability of coastal systems to erosion during extreme wave events using numerical models such as xbeach van thiel de vries 2009 mccall et al 2010 hartanto et al 2011 elsayed and oumeraci 2018 cohn et al 2019 simmons and splinter 2022 xbeach is an open source depth averaged numerical model that simulates morphodynamic bed evolution resulting from storm wave and surge conditions roelvink et al 2009 mccall et al 2010 splinter and palmsten 2012 kalligeris et al 2020 the model was first implemented by roelvink et al 2009 to simulate coastal erosion under the four different erosion regimes sallenger 2000 including swash collision overwash and inundation these regimes exert a range of forcings on coastal regions and are categorized by a limiting ratio of the dune height to water level during the storm a swashing regime exerts the least force and impact on coastal regions as the wave does not reach up to the dune and is limited to the berm and beach regions as the hydrodynamic forcings increase the dune starts to erode but is not yet overtopped this is referred to as the collision regime in overwash and inundation regimes transitioning into more adverse effects takes place waves start to overtop the dune and in some cases the water level inundates the dune xbeach is populated with many input variables and empirical parameters to capture the complex physical processes that govern the hydro and morphodynamics during extreme wave events this poses in turn a challenge in calibrating the model parameters thus different sensitivity analysis methods were used in previous studies to select the most important input parameters for calibration roelvink et al 2009 found that xbeach is sensitive to wave height and period and storm surge more than input parameters such as roller parameter and wet and dry slope of avalanching vousdoukas et al 2011 concluded that the model sensitivity to input parameters is directly proportional to the beach slope vousdoukas et al 2012 found that the model is sensitive to wave asymmetry and skewness the wet slope of avalanching the turbulence model and wave current interaction splinter and palmsten 2012 concluded that the model is more sensitive to the parameters of wave dissipation skewness and asymmetry than the foreshore beach profile slope the aforementioned studies conducted their sensitivity analysis based on the one at a time approach which varies one parameter while keeping the others constant this approach neglects the interactions between the input parameters which could lead to conflicting conclusions among different studies a more sound approach is the mont carlo approach which varies all the variables simultaneously but requires a large number of simulations for an accurate sensitivity analysis a robust approach was recently adopted by simmons et al 2017 2019 and biolchi et al 2022 who used the generalized likelihood uncertainty estimation glue method beven and binley 1992 and varied all the model parameters simultaneously their findings showed that the model is more sensitive to parameters that control the waves skewness and asymmetry bed friction and breaking wave dissipation though their methodology is sound and the results are valid the main drawback of the glue approach is the need for a large number of simulations e g 15 000 for each site or event rendering the method computationally expensive also the glue method does not provide the proportionality of the model sensitivity to the parameters as shown in section 3 to address the aforementioned shortcomings in sensitivity analysis we adopt a new approach based on the non intrusive polynomial chaos expansion nipce method nipce is an uncertainty quantification approach that is based on representing the output values as a function of the input parameters nipce can be used to generate a surrogate model that quantifies the variance and uncertainty in the model output with respect to the corresponding input wang et al 2020 illustrated the strength of pce compared to the sobol method a monte carlo based method in evaluating the effect of parameter interactions on hydrological models using a smaller number of simulations e g 1 000 000 versus 600 simulations many other studies shahsavani and grimvall 2011 hu et al 2015 fan et al 2016 kerrou et al 2017 christelis and hughes 2018 goeury et al 2022 koo et al 2020 ayyad et al 2021 used the pce methods in the sensitivity analysis of hydrodynamic hydrological and hydraulic models yet to the authors knowledge the use of a robust and cost effective sensitivity analysis method based on the pce in the field of coastal erosion modeling has not been introduced here we implement the nipce method to quantify the sensitivity of xbeach to its parameters and their interactions furthermore we show the effect of site characteristics and boundary conditions hydraulic forcing on the model sensitivity we present and discuss the results for the barrier islands of new jersey in the u s using hurricane sandy the study findings help to better understand the response of the xbeach model to changes in its parameters and efficiently calibrate the model using a relatively small number of simulations in section 2 we show the study area and the selected extreme event section 3 presents the used methods including the governing equations of the xbeach model selected calibration parameters sampling approach and nipce method the results are presented and discussed in section 4 finally conclusions and recommendations are presented in section 5 2 study area and boundary conditions the study area covers the barrier islands of the state of new jersey in the us the 2015 2019 census of the office of coastal management at the us national oceanic and atmospheric administration noaa estimated the total population of nj to be 8 9 million of which 7 15 million people live in coastal portions of the state ache et al 2015 the islands are the main line of defense of the back bays and they extend from cape may in the south to point pleasant in the north they comprise approximately 200 km fig 1 of the atlantic new jersey shoreline which extends 270 km in total hapke et al 2010 the boundary condition for xbeach is extracted from hurricane sandy on october 29 2012 the hurricane made landfall near atlantic city as shown in fig 1 based on noaa buoy s 44025 readings which is located off the new jersey coast the hurricane caused a maximum storm surge and significant wave heights of 2 6 and 9 85 m respectively this resulted in 38 casualties and 29 4 billion in losses statewide bilinski et al 2015 the dune system in nj experienced vertical losses from 2 to 6 m and volume losses between 25 and 150 m 3 m sopkin et al 2014 the northern part of island beach state park experienced the most destructive changes in dunes with an average vertical loss of 2 7 m and a maximum of 7 m sopkin et al 2014 fig 1 shows the sediment losses across the chosen profiles above the mean sea level msl we perform the sensitivity analysis for multiple sites along the study area one dimensional simulations are performed along 12 cross shore profiles as shown in fig 1 measurements of pre and post storm beach and dune elevation profiles are available from the coastal research center at stockton university under the new jersey beach profile network njbpn program the measurements extend from the back toe of the dunes to water depths ranging from 3 7 to 5 6 m for the pre storm measurements while post storm measurements extend from 1 to 4 5 m all the depths are with respect to the navd88 vertical datum we use the pre storm profile measurements to generate the computational grid for xbeach given that the measurements extend only to water depths as mentioned above we use the best available bathymetric data to build a continuous computational domain to a water depth of 15 m which is sufficiently deep to meet the conditions required by wave theories used by the model to generate waves at the offshore boundary of the grid the selected best available bathymetric data are from the 1 m resolution coned topo bathymetric model for new jersey and delaware collected by the u s geological survey usgs coastal and marine geology program in collaboration with the usgs national geospatial program ngp and noaa the computational grid has a spatial resolution of 10 m at the offshore boundary and gradually decreases to 1 m at a water depth of 8 m shoreward the grid between the water depth of 8 m and the land boundary has a constant resolution of 1 m the temporal peak water level and significant wave height at buoy 44025 are shown in fig 2 both the water level and wave height are larger on the right side of the hurricane landfall the total water level and wave characteristics at the offshore boundary of the xbeach model are obtained from a regional scale advanced circulation model adcirc luettich et al 1992 westerink et al 1994 coupled with the simulating waves nearshore model swan booij et al 1999 ris et al 1999 the computational mesh of the adcirc swan model is based on the mesh developed by the u s federal emergency management agency s region ii coastal storm surge study which was previously validated for both tropical and extratropical cyclones fema 2014 and was implemented in previous hurricane hazard studies e g marsooli et al 2021 and ayyad et al 2022 the mesh covers the western north atlantic ocean with a high resolution in the coastal waters of new jersey the mesh resolution is about 80 m in water depths smaller than 8 m and gradually increases to 8 km at a water depth of 50 m for the spectra mesh of the swan model we use 36 equally distributed directional bins and 37 frequencies logarithmically distributed between 0 033 and 1 hz meteorological forcing including the surface pressure and 10 m wind speed and direction is based on the reanalysis data from the oceanweather inc www oceanweather com tidal forcing is based on eight major tidal constituents k1 k2 m2 n2 o1 p1 q1 and s2 fig 2 compares the measured and calculated wave characteristics at the location of buoy 44025 which is the closest station to the study area with measurements 3 methods 3 1 xbeach xbeach is an open source depth averaged numerical model that simulates hydrodynamic and morphodynamic processes in coastal regions on a spatial scale of kilometers and temporal scale of storms roelvink et al 2009 mccall et al 2010 splinter and palmsten 2012 kalligeris et al 2020 xbeach runs in different modes including hydrostatic surfbeat and non hydrostatic in this work we employ the 1 d surfbeat version roelvink et al 2010 of xbeach which simulates short waves on a wave group scale by solving the short wave action balance equation given by 1 a t c x a x c y a y c θ a θ d w d f d v σ and roller energy balance equation is given by 2 d e r d t e r t e r c c o s θ x e r c s i n θ y s d where a is the wave action a s w σ σ is the intrinsic wave frequency s w is the wave energy density in each directional bin θ is the angle of incidence with respect to the x axis c x c y and c θ are wave action propagation speeds in x y and directional space and d w d f and d v are short wave energy dissipation due to wave breaking bottom friction and vegetation respectively e r is the roller kinetic energy c is wave celerity s is the loss of wave motion due to breaking and d is the dissipation of breaking waves xbeach simulates a variety of processes that include wave current interaction depth averaged velocities and variations of bed and suspended sediment transport making the calibration of xbeach a tedious task thus performing a sensitivity analysis for some of the important parameters is the traditional approach for calibrating xbeach based on the model developers recommendations and a literature review we choose six parameters shown in table 1 for calibration and sensitivity analysis in the present study although the governing equations of xbeach and its parameters have been described in detail in the literature we briefly discuss the selected model input parameters and their role in modeling there is a general agreement in the literature that the model is most sensitive to the f a c u a parameter elsayed and oumeraci 2017 vousdoukas et al 2012 biolchi et al 2022 vousdoukas et al 2011 nederhoff 2014 the f a c u a parameter is a calibration parameter introduced by van thiel de vries 2009 that sets the value for two other parameters f s k and f a s they calibrate the discretization of the wave skewness and asymmetry coefficients s k and a s respectively s k and a s are used in flow velocity calculations in the advection diffusion equation as below 3 u a f s k s k f a s a s u r m s where u r m s is the root mean squared velocity elsayed and oumeraci 2017 developed a relationship between the calibrated value of f a c u a and beach slope from closure depth to the maximum wave run up elevation they concluded that larger slopes require larger values of f a c u a for calibration this suggests less sensitivity to f a c u a at steeper slopes being directly related to the morphological processes the w e t s l p parameter defines the critical avalanching slope below water level to model the processes of wet sand slumping if the wet bed slope at a computational cell exceeds the slope set by the w e t s l p parameter the avalanched volume of sand in that cell will be redistributed to the adjacent cells to bring the slope back down to w e t s l p the w e t s l p parameter is a commonly used parameter for calibration and sensitivity analysis of xbeach harley et al 2011 vousdoukas et al 2012 bugajny et al 2013 armaroli et al 2013 palmsten and splinter 2016 gharagozlou et al 2020 biolchi et al 2022 vousdoukas et al 2012 bugajny et al 2013 and palmsten and splinter 2016 showed that the model is sensitive to w e t s l p and should be considered in the calibration on the other hand the glue analysis by simmons et al 2017 2019 and biolchi et al 2022 showed that the model is least sensitive to w e t s l p when the other parameters are simultaneously changing g a m m a is an important parameter in the wave breaking modeling process as it sets the maximum wave height in the roelvink wave breaking model roelvink 1993 which is given by 4 h m a x γ h δ h r m s where γ is the breaker index h is water depth δ is a non dimensional fraction and h r m s is the root mean square wave height many studies have considered g a m m a in the process of xbeach calibration biolchi et al 2022 harley et al 2011 armaroli et al 2013 in the wave dissipation process i e in the breaker model of xbeach the ratio of the maximum wave height to the water depth is controlled by a parameter so called g a m m a x and is usually considered for sensitivity analysis and calibration simmons et al 2019 harley et al 2011 biolchi et al 2022 during calibration a poorly chosen g a m m a x is attributed to overestimating the eroded volume harley et al 2011 armaroli et al 2013 a l p h a and b e t a are parameters that control the roelvink wave breaking and roller models respectively a l p h a represents a wave dissipation coefficient that is used in the calculation of wave energy dissipation due to breaking and is commonly considered a calibrating parameter for xbeach lashley et al 2018 kalligeris et al 2020 an increase in a l p h a will result in waves with less energy higher dissipation and thus lower erosion rates baldock et al 1998 the roller model in xbeach controls the transfer of momentum from breaking waves shoreward and into the water column causing wave setup return flows and alongshore currents roelvink et al 2010 svendsen 1984 b e t a is a breaker slope coefficient that is used to control the shoreward shift in the wave induced setup return flow and alongshore current such that an increase in b e t a causes a decrease in the shoreward shift kalligeris et al 2020 this is because b e t a is directly included in the roller energy dissipation as shown below 5 d r 2 β g c g e r where d r is the roller dissipated energy g is the gravitational constant and c g is the wave group velocity following wave breaking the rollers apply shear stress to the water column and presented as 6 τ roller ρ g r l β s where ρ is water density r is the roller area and l and β s are the length and slope of the breaking wave in one dimensional simulations increasing b e t a in the roller models causes a shoreward flux of mass and momentum this results in an offshore shift in wave setup and undertow return flow forcings svendsen 1984 faria et al 2000 3 2 non intrusive polynomial chaos expansions nipce the nipce approach is implemented to evaluate the sensitivity of xbeach model output represented by the eroded volume v to xbeach input parameters table 1 and their interactions while varying the hydraulic forcings and site characteristics the nipce approach represents the realizations model output as a function of xbeach input parameters x and their associated stochastic variations represented by the random variable ξ by implementing the stochastic galerkin projection the nipce describes a stochastic process by generating surrogate models that consist of nipce coefficients in this manner and as illustrated by reagana et al 2003 hosder et al 2006 mola et al 2011 konda et al 2010 thacker et al 2012 jacquelin et al 2015 thacker et al 2015 yildirim and karniadakis 2015 gauchi et al 2017 and ayyad et al 2021 the implementation of the nipce allows for the decomposition of the output as follows 7 v x ξ i 0 p v i x ψ i ξ where v is the output here the volume of erosion v i x is the deterministic component ψ i ξ is the random basis function that corresponds to v i x and p 1 is the number of output modes and is given by 8 p 1 n p n p where p is the polynomial chaos order and n is the number of xbeach input parameters chosen to be varied in this form of the nipce the choice of the basis function depends on the random sampling function used to generate the p 1 combinations of the chosen xbeach input parameters as shown in simmons et al 2019 and since the choice of a certain value from the sampling ranges of the chosen xbeach input parameters does not have a higher probability of one value over the other we randomly selected the xbeach input parameters x using the uniform distribution given by 9 x i a i ξ i b i where a i and b i are respectively the upper and lower bounds of the xbeach input parameter x i since the xbeach input parameters are uniformly distributed then the basis function to be used is the legendre polynomial which can be expressed in the rodrigues or ivory jacobi formula as follows sudret 2008 hampton and doostan 2015 hadigol and doostan 2018 wang et al 2018 2021 ayyad et al 2021 10 l p ξ 1 ξ n 1 2 p p p ξ 1 c 1 ξ k c n 1 2 ξ t ξ 1 p for 1 ξ n 1 where k 1 n c k p and c k s are integers if we let p 1 be the number of realizations n then the polynomial expansion coefficients v i x are determined by rewriting eq 7 in the matrix form hosder et al 2006 as 11 v 1 v 2 v n ψ 0 ξ 1 ψ 1 ξ 1 ψ p ξ 1 ψ 0 ξ 2 ψ 1 ξ 2 ψ p ξ 2 ψ 0 ξ n ψ 1 ξ n ψ p ξ n v 0 v 1 v p the system of eqs 11 is a form of the nipce that is convergent in the mean square sense ghanem and spanos 2003 choi et al 2004 if n equals to p 1 then the matrix could be directly inverted to find the expansion coefficients v i however these sets of v i s generated by this solution are dependent on the choice of the sampling vector hosder et al 2006 thus different samples of the x s will result in a different set of α i s for the solution to be independent of the sampling set a larger number of realizations should be used i e n p 1 ghanem and spanos 2003 choi et al 2004 wang et al 2021 and ayyad et al 2021 the system of eqs 11 would then be solved by minimizing the errors in the least squares sense sampling from the independent variables space should be done with care as it has a significant impact on computing the polynomial coefficients l p wang et al 2021 the sampling technique used should guarantee that all portions of the independent variables space chosen ranges in table 1 are sampled traditionally the random sampling method was used to achieve this sampling criterion thus to assure the independence of v i s and consequently the convergence of the nipce it is required to have a very large number of samples n 10 000 therefore we use the latin hypercube sampling lhs to reduce the number of samples while assuring the coverage of the entire sampling space choi et al 2004 mola et al 2011 ayyad et al 2021 after solving eq 11 the nipce coefficients can be used to analytically approximate statistical moments of the realizations such as the mean μ 12 μ v 0 x and standard deviation σ 13 σ 2 i 1 p v i 2 x ψ i ξ 2 where 2 is the l 2 norm i e euclidean norm validation of the nipce should seek an answer to the following question how many polynomial orders p and sampling combinations n are needed so that the mean and standard deviation from the nipce eqs 12 and 13 are independent of n the answer is usually found by running a large number of monte carlo simulations 10 000 and then comparing the mean and standard deviation from those monte carlo simulations with those calculated from eqs 12 and 13 pettit et al 2010 ayyad et al 2021 an alternative approach to the computationally expensive monte carlo method is a recently adopted method for validating the pce so called the leave one out loo cross validation error ϵ l o o molinaro et al 2005 blatman 2009 blatman and sudret 2010 2013 marelli and sudret 2015 al ghosoun et al 2021 in this approach one builds n models m p c i such that each model uses a reduced sample χ x i x j j 1 n j i then calculate the square error between the resultants v from the main model m x calculated using all the samples n and those calculated from the model m p c i using reduced sample n 1 14 ϵ l o o i 1 n m x i m p c i x i 2 i 1 n m x i μ ˆ y 2 molinaro et al 2005 blatman 2009 blatman and sudret 2010 marelli and sudret 2015 yang et al 2017 and tran and kim 2021 considered that a perfect model has ϵ l o o error of an order less than o 1 0 4 slim et al 2017 considered that a good model has an error of order less than o 1 0 2 3 3 pce implementation to xbeach we use the lhs technique to sample three sets of 600 combinations of the selected input parameters for each study site i e each cross shore profile in fig 1 two sets were sampled from the sensitivity sampling ranges shown in table 1 with a constant sediment size d50 of 0 5 mm for set 1 and 0 25 mm for set 2 the aim of these two sets is to separate the effects of input parameters from the effects of the sediment size that varies from one site to another the sensitivity ranges were selected based on preliminary runs by varying the parameters one at a time and choosing a range where the model results perturbed around the post storm measured profiles sample set 3 uses an expanded range for the input parameters to show the effectiveness of the lhs in capturing a suitable set of parameters from the sampling space of parameters the expanded ranges were chosen based on preliminary runs where each parameter was varied separately up to a point that realistic model outputs were obtained for example the upper limit of the range assigned to f a c u a is 0 4 since larger values led to unrealistic volumes of erosion the actual sediment size at each profile flynn 1999 lemke and miller 2018 is used in sample set 3 the d50 varies between 0 2 and 0 3 mm among the profiles on the left side of the hurricane sandy landfall and between 0 3 mm to 0 6 mm on the right side of the landfall the calculated v sensitivities from sample sets 1 and 2 of simulations will help understand the effect of site characteristics d50 and slope and boundary conditions surge and wave height on the sensitivity of xbeach to the selected parameters the calculated v i x vector will contain different components of the sensitivity of xbeach output v to its parameters the first order components that represent v x i explain the sensitivity of xbeach output to i th xbeach parameter i e whether the parameter has a decreasing or increasing effect on the model output and which parameter has the most effect on the output the second order component 2 v x i x j explains the interaction between the i th and j th parameters which represents the non linearity introduced to the model output due to the interaction between every two parameters using these measures of sensitivity we will use sample sets 1 and 2 to show how the sensitivity of xbeach responds to changes in boundary conditions and site characteristics we use the beach slope to quantify the effect of site characteristics on the model sensitivity to its parameters sampling set 1 the slope will help in understanding the wave breaking regime through the surf similarity parameter ξ battjes 1975 as follows 15 ξ tan α h 0 l 0 1 2 where t a n α is the beach slope h 0 and l 0 respectively the temporally averaged deep water wave height and length during the storm deep water is taken at the deep end location of the computational grid calculating the beach slope is a subjective matter and depends on what is being considered for analysis for example if a shore structure is considered then the swashing region slope slope of the structure or seabed leading to the structure is considered as a representative slope in calculating ξ here we consider the slope to be the bed slope from the upper swashing limit to the depth where waves interact with the seabed depth of interaction d d for a given profile is determined by examining the final bed elevations from all simulations for that profile and identifying the most seaward water depth where no bed elevation change has taken place we adopted the method proposed by stockdon et al 2006 to find the upper swashing elevation as the following 16 u η ˆ 2 η t where η t is the time series of the shoreline elevation taken from the last wetted computational cell on the beach dune and η ˆ is the maximum wave setup calculated as the temporal mean of η t the slope will then be calculated as the slope of the best linear line fitted to the elevation of points between d and u i e d z b u where z b is the average modeled bed elevation during the storm the calculated slope and surf similarity parameter at each profile are shown in fig 3 the profile slope and consequently the surf similarity parameter increase from the left side to the right side of the landfall i e south to north suggesting rougher wave breaking regimes on the right side of the landfall 3 4 xbeach calibration the calibration of xbeach has been considered by many of the sensitivity analysis studies cited earlier in most of these studies the brier skill score bss is considered to measure the model s accuracy the bss is a measure of model skill that quantifies the model s ability to reproduce the morphological evolution during a certain event and is formulated as 17 b s s 1 σ z 0 z m 2 σ z 0 z b 2 where z m is the final modeled profile elevation z o and z b are the post and pre storm measured profile elevations observed respectively a perfect bss score would be 1 which indicates that the model perfectly simulated the morphological changes van rijn et al 2003 classified the model performance based on the bss such that results are considered to be bad if bss 0 poor if 0 bss 0 3 fair if 0 3 bss 0 6 good if 0 6 bss 0 8 and excellent if 0 8 bss 1 although most studies used the bss they adopted different calibration approaches for example elsayed and oumeraci 2017 calibrated each profile separately by identifying a parameter set with the best bss vousdoukas et al 2012 chose the parameter set that produced the best bss across different beach profiles and events for the same site callaghan et al 2013 and de vet et al 2015 concluded that different sallenger regimes can influence calibration simmons et al 2017 2019 used the glue method to combine the likelihood of parameter sets with bss exceeding a behavioral bss limit across multiple boundary conditions here we perform the calibration process for each profile using the results of the sensitivity analysis based on sample set 3 which considers the actual sediment size for each site to account for equifinality we determine the calibrated value for a particular parameter by averaging the values of that parameter used in the top 10 simulations that had the highest bss scores then we re run a simulation using the averaged values to ensure the accuracy of the model is preserved 4 results and discussion 4 1 pce validation we calculated the ϵ l o o error eq 14 to quantify the accuracy of the nipce model for sets 1 and 2 at each profile fig 4 shows that the ϵ l o o error is in order of 10 4 at all study sites which is within the acceptable range the low errors confirm that the chosen number of simulations n 600 is enough to cover the uncertainty in the model output and thus the chosen polynomial order p 2 can capture the model behavior therefore the results from the nipce model can be considered reliable for sensitivity and uncertainty quantification 4 2 sensitivity analysis 4 2 1 effect of boundary conditions the results presented and discussed here are based on sample set 1 which accounts for the sensitivity of the model to its parameter while using a constant d50 among all study sites i e removing the effects of sediment size on the model fig 5 shows the absolute value of nipce s first order component of the vector v i x i e v x i this component describes the magnitude of the effect of each model input parameter e g f a c u a on the model output i e the volume of erosion a positive value of v x i represented by solid lines means a direct relationship between the model input parameter and the output i e an increase decrease in the parameter results in an increase decrease in the simulated volume of erosion a negative value of v x i represented by dashed lines means an inverse relationship i e an increase decrease in the parameter results in a decrease increase in the volume of erosion the results show that the f a c u a parameter has the largest effect on the model output when compared to the effects of the other parameters because of its significant effect on the calculated net cross shore sediment transport this finding is consistent with the literature roelvink et al 2010 vousdoukas et al 2012 bugajny et al 2013 nederhoff 2014 elsayed and oumeraci 2017 as shown in eq 3 f a c u a is used in the calculations of flow velocity u a which is directly related to the shear stress calculations in the model thus higher values for f a c u a would increase the flow velocity u a and consequently the shear stress which in turn increases onshore sediment transport on the other hand the model sensitivity to f a c u a decreases from the left to the right side of hurricane sandy s landfall location which is accompanied by an increase in storm surge and wave height as shown in fig 3b this suggests higher sensitivity of the model to non linearity of the waves represented by f a c u a under lower storm impact regimes which is consistent with the findings of elsayed and oumeraci 2017 simmons et al 2019 and biolchi et al 2022 the effect of profile slope on xbeach sensitivity to f a c u a can be inferred from figs 3a and 5 the effect of f a c u a on the model is more pronounced fig 5 when the slope is lower fig 3 a i e the model is more sensitive to f a c u a in case of steep profiles than mild ones this elucidates that beach slope has direct control of the onshore sediment transport induced by the wave skewness and asymmetry which is consistent with past studies by vousdoukas et al 2011 nederhoff 2014 and elsayed and oumeraci 2017 although our results show a strong sensitivity to f a c u a a previous study harley et al 2016 found that f a c u a is not the most significant parameter this can be due to the sampling approach that is adopted in harley et al 2016 where f a c u a was sampled within a small range between 0 1 and 0 15 the limited number of sampled values resulted in missing the behavior of the model in the rest of the sampling space this shows the effect that the sampling techniques have on sensitivity analysis leading to conflicting conclusions in understanding the model a reliable model calibration requires a sampling space that entirely covers all potential values of the model parameters which is done in the present study by adopting the lhs technique in the wave breaker model g a m m a controls the maximum wave height such that larger values of g a m m a delay wave breaking this results in larger waves reaching the shoreline which could cause more erosion roelvink 1993 fig 5 shows a direct relation represented by the solid line between the g a m m a parameter and the model output the plot shows that xbeach sensitivity to g a m m a is ranked second after f a c u a and is directly proportional with the boundary conditions and profile slope as concluded from figs 3 and 5 this is attributed to the fact that the resultant morphological change is very sensitive to the location of wave breaking relative to the shoreline i e whether waves break closer or farther from the shoreline due to larger or smaller values of g a m m a these findings agree with simmons et al 2017 2019 and biolchi et al 2022 where they showed a higher sensitivity to g a m m a at the overwash than the collision condition in xbeach s dissipation formula xbeach a l p h a controls the dissipated wave energy due to depth induced wave breaking lashley et al 2018 an increase in a l p h a results in more intense wave breaking resulting in a decrease in the waves energy before they reach the shoreline thus the erosion rates above the msl would decrease baldock et al 1998 this is consistent with our results in fig 5 where the model is shown to be inversely proportional to a l p h a represented by the dashed line figs 3 and 5 show that the sensitivity of xbeach to a l p h a increases as the boundary condition and beach slope increase as waves become larger i e intense storm regimes their dissipated energy increases resulting in a more pronounced effect of a l p h a i e an increase in the model sensitivity to a l p h a also a steeper slope reflective beach causes larger waves to reach the surf zone this causes larger energy dissipation when they break therefore increasing the sensitivity of the output to a l p h a thus the steeper the slope fig 3a the more important information passes from the dissipation model to xbeach i e a higher sensitivity to a l p h a the roller model in xbeach is governed by b e t a which is shown to have an inverse relation with the eroded volume of xbeach as presented in fig 5 larger values of b e t a cause more roller wave energy dissipation and less erosion above msl this is attributed to its direct involvement in the calculation of the dissipated roller energy eq 5 the increase in b e t a leads to a shoreward flux of mass and momentum that in turn results in an offshore shift in wave setup and undertow return flow forcings svendsen 1984 faria et al 2000 kalligeris et al 2020 rafati et al 2021 this shifting in forcings causes underwater bars to become more prominent which increases the wave energy dissipation due to depth induced breaking similar to g a m m a and a l p h a b e t a exhibits an increasing sensitivity as the boundary conditions and slope increase as shown in figs 3 and 5 as waves become larger their roller shear stress applied to the water column increases this in return causes the model to become more sensitive to b e t a due to the direct usage of a l p h a and b e t a in calculating the dissipated energy respectively from depth induced breaking waves and rollers these parameters are intertwined physically and mathematically in the model this can be depicted in fig 5 as the sensitivity of the model to a l p h a and b e t a is approximately the same fig 5 shows that the g a m m a x is the second least sensitive parameter this parameter determines the ratio of maximum wave height to water depth thus increasing the g a m m a x results in more energetic waves reaching the shoreline which causes more erosion this behavior is successfully captured by nipce where fig 5 shows a direct relation of xbeach output to g a m m a x represented by the solid line although harley et al 2011 and armaroli et al 2013 found that g a m m a x is a key parameter in the calibration process simmons et al 2017 2019 and biolchi et al 2022 and our results show a subtle impact of this parameter on the model output this is attributed to the effects of interactions among different parameters when varied alone e g in the preliminary simulations in the present study g a m m a x had a considerable effect on the model output however when g a m m a x and other parameters were simultaneously varied the model output showed a subtle sensitivity to g a m m a x unlike other parameters considered in this study g a m m a x is not directly used in the governing equations but it rather sets an upper limit to the g a m m a parameter therefore as shown in fig 5 the sensitivity of the model to g a m m a x does not respond to changes in the boundary conditions or slope our results show that the model is least sensitive to the w e t s l p parameter as shown in fig 5 this parameter represents the maximum slope of wet sand particles that can be reached after an avalanche is initiated this finding contradicts our preliminary runs mentioned in section 3 3 where w e t s l p caused considerable erosion when it was independently varied while the other parameters were kept constant this finding is in agreement with studies that changed the parameters simultaneously during the calibration process simmons et al 2017 2019 biolchi et al 2022 other studies vousdoukas et al 2012 bugajny et al 2013 palmsten and splinter 2016 that are in disagreement with our results neglected the interactions among parameters due to either the use of the one at a time sensitivity approach or choosing discrete values of the parameters rather than sampling a sufficiently large number of samples point discrete sampling from the parameter range would miss some aspects of the model behavior as the whole sampling space is not considered this suggests that interactions between different parameters are important when calibrating the model although parameters interaction is a key part of sensitivity analysis other model characteristics e g mesh size could play an important role in its behavior in simmons et al 2017 and our sensitivity analysis the effect of mesh size was not considered however gharagozlou et al 2020 found that the xbeach sensitivity to w e t s l p changes with the mesh spacing size in the cross shore direction this suggests that the w e t s l p is sensitive to the characteristics of the mesh size rather than boundary conditions and profile slope the results presented and discussed above reveal the capabilities of the introduced nipce method compared to the traditional sensitivity analysis adopted in the literature first the nipce method accounts for simultaneous changes in multiple parameters and in turn resolves the interactions among the parameters which is quantified in section 4 2 3 the interactions among parameters could weaken or strengthen the sensitivity of the model to some parameters e g the w e t s l p parameter as discussed earlier second the nipce method can determine whether the model output has a direct or inverse relationship with a parameter for example our analysis shows that while the calculated volume of erosion is inversely related to f a c u a e g an increase in f a c u a leads to a decrease in erosion it is directly related to g a m m a traditional sensitivity methods as well as the most recently adopted glue method simmons et al 2019 biolchi et al 2022 show no distinction in the proportionality direct inverse effect of g a m m a and f a c u a on the model output eroded volume 4 2 2 effect of sediment median size as shown in the previous section boundary conditions and profile slope affect xbeach s sensitivity to its parameters therefore changing its behavior however sediment size is considered a key parameter in coastal sediment transport such that its increase would results in less erosion van rijn 2011 here we consider the effect of sediment size on the behavior of xbeach this is important as the modeler might not be certain about the sediment size in the model to that end we show the sensitivity of the model to its parameters using the first order derivative i e v x i from sets 1 d50 0 5 mm and 2 d50 0 25 mm in fig 6 our results suggest that changing d50 from 0 5 mm to 0 25 mm has a subtle and consistent effect on the sensitivity of f a c u a and a l p h a but no effect on the other parameters from eq 3 f a c u a is used in calculating the flow velocity which is directly related to the shear stress applied to the sediment in the model as sediment size increases it naturally retards the transport process this results in a reduction in the model sensitivity to parameters that govern the shear stress applied to the sediment i e f a c u a this is captured by the nipce method as shown in fig 6a where the sensitivity of xbeach to f a c u a has slightly reduced when the sediment size is doubled therefore bigger changes in f a c u a are required to transport the sediment this suggests that with larger sediment sizes the model would perform better for larger smaller values of f a c u a i e shoreward seaward sediment transport this justifies the need for separate profile calibration for a site with varying sediment sizes a l p h a also showed a subtle but consistent change in sensitivity however contrary to f a c u a the sensitivity of a l p h a increased when the sediment size increased from 0 25 mm to 0 5 mm as shown in fig 6b this suggests that for larger sediment sizes the model is more sensitive to changes in the wave breaking dissipation formula i e changes in a l p h a during an extreme event a profile with a larger sediment size will evolve into a steeper sloped profile steeper profiles have narrower surf zones hence the energy of the incoming waves would dissipate within a smaller surf zone causing more erosion dean and dalrymple 2004 therefore small changes in the dissipation process is more prominent in a narrower surf zone steeper slope larger sediment than in a wider surf zone milder slope smaller sediment the rest of the parameters do not show a consistent change in xbeach sensitivity when changing the sediment size however the small to no change in the sensitivity when changing the sediment size could be because of the model s insensitivity to changes in the sediment size the sensitivity of xbeach to sediment size is not considered in this study nonetheless previous studies have shown that the sediment size has a negligible effect on the model output for example rafati et al 2021 did not report a change in the model behavior for d50 of 0 15 mm and 0 3 mm brandenburg 2010 concluded that the output of xbeach is not sensitive to changes in sediment size as it does to the beach slope and boundary conditions the results of mccarroll et al 2021 also show that xbeach is more sensitive to wave height and period than sediment size while previous studies found a negligible effect of the sediment size on the model output we showed that the effect of sediment size on the sensitivity of the model to its parameters is also negligible compared to the boundary conditions and profile slope 4 2 3 parameter interactions and nonlinearity a literature review shows that to the authors knowledge the effects of interactions among the xbeach parameters on the model performance have not been investigated yet because of the limits of the sensitivity analysis methods adopted in previous studies the nipce method in the present study allows us for the first time to quantify these effects throughout the second order component of the nipce vector 2 v x i x j fig 7 this vector quantifies different aspects of the model s response to its parameters interactions the sign of this vector shows whether the interaction between two particular parameters has an accumulation or cancellation effect on the model performance a negative value represented by the dashed line indicates that the interaction between the two parameters cancels out the effect of each parameter on the model output while a positive sign represented by solid lines indicates that the interaction builds upon each parameter s effect such information may be obtained from the first order components of the sensitivity analysis as shown in fig 5 however a linear superposition of the first order components will not resolve the non linearity introduced to the model due to the interactions among the parameters which is resolved by the magnitude of 2 v x i x j finally using 2 v x i x j one can compare how interactions between different parameters have an impact on uncertainty in the model output by examining the magnitude of 2 v f a c u a g a m m a shown in fig 7 one can conclude that f a c u a and g a m m a have opposite effects on the model and thus they are expected to level out each other s effects this conclusion may be also depicted from the sign and magnitude of v f a c u a and v g a m m a from fig 5 however the first order component of sensitivity does not quantify the nonlinearity in the interaction between the parameters therefore it does not reliably explain the uncertainty in the model behavior as the modeler varies g a m m a and f a c u a simultaneously for example if the magnitude of 2 v f a c u a g a m m a is very small a 10 increase in f a c u a and g a m m a will cause respectively 10 decrease and increase in the simulated volume of erosion resulting in almost no change in the eroded volume however since the second derivative is showing large values fig 7 then this interaction is highly nonlinear and increases the uncertainty in the model output when the modeler varies f a c u a and g a m m a simultaneously by comparing the other second order derivatives to 2 v f a c u a g a m m a it can be concluded that the effect of f a c u a g a m m a interaction introduces more non linearity and uncertainty in the model behavior than for example the f a c u a a l p h a interaction the f a c u a parameter shows higher interactions with g a m m a followed with a l p h a and b e t a the strong interaction between f a c u a and g a m m a 2 v f a c u a g a m m a suggests that the results of the governing equations for wave nonlinearity and wave energy dissipation are highly interacting with each other causing the model to behave nonlinearly this can be traced back in the model through h r m s root mean square wave height and u r m s root mean square flow velocity yet this strong interaction extends further in the numerical implementation of the model therefore tracing the interaction between parameters is extremely complicated and is out of the scope of this work however thanks to the robustness of the nipce and the ability of the legendre polynomial to capture complicated relationships between the xbeach output and the realizations we can qualitatively assess the interactions through fig 7 the quantified interactions among the model parameters may be considered as guidelines for modelers to understand that for example the effect of varying f a c u a and g a m m a on the model output is larger than the effect of varying f a c u a and a l p h a given that fig 7a shows 2 v f a c u a g a m m a 2 v f a c u a a l p h a therefore the f a c u a g a m m a interaction causes more nonlinearity than the f a c u a a l p h a interaction and therefore introduces larger uncertainty in the model output the most important nonlinear interaction between different parameters is f a c u a g a m m a followed by f a c u a a l p h a f a c u a b e t a and g a m m a b e t a these interactions decrease with increasing boundary conditions except for g a m m a b e t a interaction suggesting that the effect of nonlinear interactions among parameters reduces with larger boundary conditions this is an unexpected conclusion as larger waves generally mean bigger disturbance to shorelines and morphodynamic processes and thus larger uncertainty this can be explained through the behavior of the model in fig 5 as the boundary conditions increase the sensitivity of the model to other parameters other than f a c u a increases causing the sensitivity of the model to be shared across different parameters 4 3 xbeach calibration findings from sensitivity analysis inform the model calibration which aims to select the combination of model parameters that results in reliable predictions according to the nipce validation presented earlier in fig 4 a sample size of 600 simulations is sufficient for nipce to capture the model behavior here we use the results of the 600 simulations from sample set 3 which used the actual sediment grain sizes to determine the calibrated values of the input parameters we calculated a calibrated value for each parameter by averaging the parameter s values used in the top 10 simulations that had the highest bss scores the bss scores of all simulations of sample set 3 as well as the calibrated model using the averaged top 10 simulations parameters are shown in fig 8 the minimum bss across all calibrated profiles is 0 8 except for profile 246 which behaved fairly with a minimum bss of 0 47 variation in the bss scores represented by the height of the box plot along the shoreline decreases as boundary forcings and model sensitivity increase i e from profile 212 to 156 as the model sensitivity to a parameter increases the parameter becomes more important in the calibration process thus the modeler should reduce the uncertainty in the choice of suitable calibration ranges for that parameter this can be done by running preliminary simulations to assure that the selected ranges are reasonable for calibration for example in section 3 3 before choosing the calibration ranges preliminary runs aided in restricting the upper limit of f a c u a to 0 4 where values above 0 4 showed a big deviation from the post measured profile wrong choices in a highly sensitive parameter as f a c u a will cause more errors in the simulations this reduction in the uncertainty can be depicted in fig 9 where the spread uncertainty in the parameters values used in the top 10 simulations is resembled by the box plot fig 9 shows that f a c u a has a smaller spread thus the modeler should be more certain about the choice of f a c u a compared to other parameters therefore uncertainty in f a c u a would cause larger errors in the model outputs than for example uncertainty in the w e t s l p in the same manner as the sensitivity of the model to a parameter increases fig 5 the uncertainty in selecting a proper value for that parameter decreases spread shown in fig 9 as discussed in section 4 2 1 the sensitivity of the model is directly proportional to the boundary conditions this poses a challenge as the increase in the boundary forcings would make the selection of a correct value of the parameter difficult this can be inferred from fig 9 as the spread of the values used in calculating the calibrated parameters decreases i e the height of box plots decreases with the increase in the boundary condition 5 summary and conclusions this study introduced a new approach to sensitivity analysis and calibration of morphodynamic models the non intrusive polynomial chaos expansions nipce was implemented to quantify the sensitivity of the xbeach model to six parameters f a c u a g a m m a a l p h a b e t a g a m m a x and w e t s l p the method was applied to the coastal erosion event induced by hurricane sandy at a series of cross shore profiles along the new jersey barrier islands the latin hypercube sampling lhs method was adopted to generate three sets of 600 unique combinations of the parameters two of the sample sets had two different constant sediment grain sizes across all profiles to study the effect of sediment size on the sensitivity of the model the nipce was validated using the leave one out cross validation error the results of the nipce application to a sample set with constant d50 showed a relation between the model sensitivity to the selected parameters and the boundary conditions the sensitivity of the model output volume of erosion to the dissipation and roller model parameters g a m m a b e t a and a l p h a increases as the wave height and the cross shore profile slope increase on the contrary the sensitivity to the wave non linearity parameter f a c u a decreases as boundary conditions and the profile slope increase the model did not show any significant sensitivity to the parameters w e t s l p and g a m m a x comparing the results of the nipce from the two sample sets with constant d50 showed the effect of changing sediment grain size on the model sensitivity to the selected parameters the model sensitivity showed a consistent but a minor change in the sensitivity of f a c u a and a l p h a when changing the sediment size other parameters showed no change in the model s sensitivity this may be attributed to the choice of parameters as all the selected parameters in this study influence the wave physics in the model but are not included in the sediment transport governing equations the sensitivity of the model to parameters that are directly related to sediment transport e g maximum shield s stress parameter may respond to changes in sediment characteristics such as d50 which remains to be investigated in future studies this study quantified for the first time the effects of interactions between parameters on the sensitivity of the model this was done through the second order component of the nipce which explains the non linearity introduced to the model by the interaction between two parameters the results show that the model s non linearity decreases with increasing boundary conditions and site characteristics it was shown that the strongest interaction among the selected parameters in this study is the f a c u a g a m m a interaction followed by f a c u a a l p h a b e t a g a m m a and f a c u a b e t a the interaction between other parameters shows negligible effects on the model set 3 was sampled using larger ranges of parameters to demonstrate the capabilities of lhs in creating a set of parameters combinations with a small sample size e g 600 in this study for efficient model calibration excellent bss scores were obtained for the simulations that were conducted using the averaged parameters of the top 10 simulations with the highest bss scores bss 0 8 the variation in the parameters used in the top 10 simulations is smaller for f a c u a than other parameters which is due to the stronger sensitivity of the model to f a c u a except for f a c u a the variation in the parameters used in the top 10 simulations reduces as boundary conditions increase finally the results of this study are subject to a certain extreme event specific site characteristics and the selected parameters and their ranges similar analyses under different environmental conditions e g other storm events and study areas would help to get a better and perhaps universal insight into the sensitivity of the model to its parameters declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests reza marsooli reports financial support was provided by noaa new jersey sea grant consortium acknowledgments this publication is the result of work sponsored by the new jersey sea grant with funds from the national oceanic and atmospheric administration noaa office of sea grant u s department of commerce under noaa grant na18oar4170087 and the new jersey sea grant consortium united states the statements findings conclusions and recommendations are those of the author s and do not necessarily reflect the views of new jersey sea grant or the u s department of commerce njsg 22 1000 we thank dr andrew cox from oceanweather inc for providing us with the meteorological reanalysis data for hurricane sandy we would like to thank the three anonymous reviewers for their comments and suggestions which helped us to improve the quality of this paper software availability the software used in this paper xbeach is an open source software that can be accessed and downloaded at https oss deltares nl web xbeach 
25476,this r package technically gathers the methods developed and lessons learnt over recent years for estimating discharge of ungauged outlets in a runoff runoff type approach discharge is both the input and output it uses the observed discharge of neighbouring gauged basins and a geomorphology based deconvolution convolution modelling approach the continuous hydrological modelling is based on the description of the hydro geomorphometry of the river drainage network that can be easily described for any outlet considered an inversion of this model allows us to deconvolute the observed discharge series and to estimate the signal of the water flowing into rivers from the hillslope namely the net rainfall the transfer of this net rainfall to a targeted ungauged basin enables the simulation of the discharge series there the methodology has been tested in several hydro climatic contexts and with this package we aim to encourage further evaluation and improvement as well as subsequent operational applications keywords prediction in ungauged basin pub streamflow observation hydrogeomorphology inverse modelling r package data availability code and data are freely available as described in the paper software availability name of software transfr version 1 0 0 developer alban de lavenne maintainer alban de lavenne contact email alban delavenne inrae fr year of first release 2021 software required r and fortran compiler program language r and fortran program size 2 2 mo license gpl 2 availability and cost open source software available at https cran r project org package transfr download free of charge 1 introduction streamflow time series and their derived signatures and metrics mcmillan 2020 represent essential information for the relevant management of water dynamics resources and hazards however it is often not measured for the exact location where decisions need to be made ceola et al 2016 cudennec et al 2020 dixon et al 2020 and thus the location the basin and its outlet is referred to as ungauged major methodological progress has been made over the past two decades from the perspective of predictions in ungauged basins pub blöschl et al 2013 hrachowitz et al 2013 of discharge series or related signatures approaches often rely on a rainfall runoff model and on a method summarised by the term regionalisation which makes use of the calibration at gauged locations to predict the hydrological behaviour at ungauged target locations different tools and softwares have been developed to help the implementation of rainfall runoff models in data scarce regions see e g gibbs et al 2012 archfield et al 2013b pumo et al 2017 ouarda et al 2018 jafarzadegan et al 2020 alternative runoff runoff modelling approaches less frequent than rainfall runoff approaches have also been developed for the prediction of discharge statistics or discharge time series in ungauged basins they essentially rely on geostatistical spatial interpolation skøien and blöschl 2007 isaak et al 2014 müller and thompson 2015 farmer 2016 transformation functions applied to hydrographs andréassian et al 2012 or hydraulic routing tewolde and smithers 2007 song et al 2011 some of these approaches were complemented by the development of r packages such as rtop skøien et al 2014 and atakrig hu and huang 2020 in comparison with the rainfall runoff approach runoff runoff modelling approaches do not require an explicit modelling of the hydrological response to meteorological forcing data on the other hand they do not explicitly take into account the specificity of this forcing at an ungauged location and rely on the density of stream gauges however by relying directly on discharge observations and not on model simulations they often provide more robust predictions than other regionalisation techniques in densely instrumented regions laaha et al 2012 archfield et al 2013a viglione et al 2013 we present a runoff runoff approach that differs from these geostatistically driven approaches relying mainly on geomorphological descriptions that are easily provided at any point it aims at transferring the observed discharge series themselves from one or several gauged locations to an ungauged target location the approach relies on a four step rationale i building of a hydrogeomorphology based transfer function ii estimation of net rainfall series by deconvolution of observed discharge series for one or several gauged basins iii inter basin transfer of net rainfall series towards the ungauged target basin iv simulation of discharge series of the ungauged target basin via convolution with its own hydrogeomorphology based transfer function here we present an open source tool delivered as an r package called transfr which enables the implementation of this pub approach the shape of every hydrograph results from the convolution of the dynamics of water flowing from hillslopes into the river drainage network net rainfall and the overall converging across the drainage network of the considered basin down to the outlet a geometric characterisation of the drainage network of a basin is accessible whether the basin is gauged or not and provides valuable information on the underlying hydrological dynamics see e g rigon et al 2016 a robust transfer function of the water dynamics across the drainage network can be deduced from such a characterisation of the hydro geomorphometry and an average velocity based on the unit hydrograph theory it can be used to support mathematical convolution see e g sherman 1932 dooge 1959 rinaldo and rodriguez iturbe 1996 cudennec 2007 moussa 2008 grimaldi et al 2011 andrieu et al 2021 inversion of such transfer functions allows one to deconvolute the observed discharge series of a gauged location and estimate the net rainfall series articulating inverse and direct modelling over neighbouring gauged basins and ungauged basins makes it possible to use a runoff runoff approach in order to address the pub challenge the hydrological modelling approach has been built and improved over time the original idea was introduced theoretically by cudennec 2000 and implemented in the following years the inverse modelling at the core being lately published by boudhraâ et al 2018 the methodology was first implemented on a few basins in semi arid tunisia at the event timescale boudhraâ et al 2006 2009 then in dense configurations of neighbouring and nesting basins in temperate oceanic brittany france de lavenne et al 2015 and in snow influenced québec canada ecrepont et al 2019 on a continuous time basis it has been compared with another approach well received by the community namely top kriging skøien and blöschl 2007 and similar performance was observed de lavenne et al 2016 this evaluation was made on a large set of 389 sub basins of the french loire river basin 117 500 km 2 and a higher robustness was achieved by combining the information of several gauged basins beyond pseudo ungauged applications and analyses simulations for truly ungauged basins have been performed including for multiple outlets flowing into coastal bays de lavenne and cudennec 2014 2019 and the potential has been pointed out for water quality modelling that often relies on a good estimate of discharge for the estimation of nutrient fluxes recently the approach was implemented in a web processing service wps named simfen for operational perspectives in the french brittany region dallery et al 2020 with an accompanying web interface https bit do simfen in the spirit of open service delivery and chaining of web services bera et al 2015 squividant et al 2015 pecora and lins 2020 this package has been initially developed to be the computational backend of this wps this package aims to provide a new tool for water management modelling in order to spatially evaluate the resource at any point in a territory by directly valuing the gauges of its rivers making the package available aims to i open new applications by end users in particular water and basin managers public authorities and engaged citizens see e g nardi et al 2021 ii get feedback and facilitate new bridges between science practices and investigations on the method and iii value and explore genericity and flexibility in various hydrological regions and contexts hall et al 2022 we have chosen to implement the method in the r programming language thus relying on a growing community of developers and users in the field of hydrology slater et al 2019 astagneau et al 2021 2 theory this paper does not provide an exhaustive description of the theoretical aspects of the method implemented in the package as it has already been described in previous studies see introduction however our aim is to support an efficient implementation of this r package with some reference points on the conceptualisation and on the way how it is translated into this programming language the implementation of the methods can be summarised in four steps fig 1 presented in the following sections 2 1 step 1 a hydrogeomorphology based transfer function modelling the first step is to build a hydrological model of the transfer function type for every basin considered either gauged or ungauged based on hydrogeomorphometry the hydrological processes that we aim to describe are limited to the water transfer within the drainage network and do not address any explicit description of the behaviour of hillslopes for any geographic point within the basin the flow path and its length called hydraulic length x c within this drainage network to the outlet can be extracted from a digital elevation model dem assuming an average velocity allows us to estimate a travel time along this flow path applying this rationale to a grid of points within the basin makes it possible to estimate probability density functions of the hydraulic length and hence of the travel time which can then be used as a unit hydrograph u h sherman 1932 streamflow velocity u c is thus the only parameter that needs to be estimated either from observation in the case of a gauged basin or from indirect assessment in the case of an ungauged basin previous studies have indeed shown that this velocity can be easily regionalised see e g eq 1 1 u c a x c b where x c is the mean hydraulic length and calibrated with a 4 38 1 0 4 and b 0 69 in the work of de lavenne et al 2016 over the french loire river basin even if this calibration has been done over a wide range of hydroclimatic conditions these values of a and b might need to be re calibrated locally de lavenne et al 2016 proposed to estimate this velocity u c from the ratio of mean hydraulic length x c over mean rising time of the basin 2 2 step 2 net rainfall of gauged basins by deconvolution the second step is to assess the water flowing into the drainage network from hillslopes namely the net rainfall r n instead of estimating r n from a direct modelling of hillslope response to rainfall which remains highly non linear and difficult to quantify by field measurements it is done by an inverse modelling of the transfer function u h developed in step 1 for a gauged basin we estimate the net rainfall that best reconstitutes the observed discharge this inversion is solved by minimising the following expression tarantola and valette 1982 menke 1984 2 q q o b s t c q o b s 1 q q o b s r n r n a p t c r n a p 1 r n r n a p where q is the output of the model r n a p is initial a priori information about r n and c r n a p and c q o b s are covariance matrices for vectors r n a p and q o b s respectively and superscript t is the matrix transpose r n a p is estimated from the specific discharge of q o b s delayed by a lag time which can be estimated using x c u c a maximum likelihood solution is used to solve this inversion eq 3 see tarantola and valette 1982 menke 1984 for generic aspects and boudhraâ et al 2018 for a detailed application as we are using a lumped modelling approach we focus on the temporal dynamics of this net rainfall but spatial heterogeneity is not explicitly considered 3 r n r n a p c r n a p u h t u h c r n a p u h t c q o b s 1 q o b s u h r n a p 2 3 step 3 net rainfall of ungauged basins by spatial interpolation the third step is to estimate the net rainfall of the targeted ungauged basin i by combining the net rainfalls of neighbouring gauged basins j a weighting strategy can be used in order to increase the weight λ i j of the gauged basins for which a higher hydrological similarity is assumed to exist with the ungauged one estimating this similarity is a rather complex question but spatial proximity approaches generally appear to be efficient in large sample studies and in data rich countries and therefore it is the one currently implemented in this tool see also the discussion on gauging station density in section 6 de lavenne et al 2016 advised computing a distance between basins using a rescaled ghosh distance d ghosh 1951 gottschalk 1993 gottschalk et al 2011 considering the n closest basins in a given radius number of donors and radius parameterised by the user around the targeted ungauged basin their net rainfall time series are averaged using an inverse distance weighting strategy and the outcome is considered to be the net rainfall time series of the target basin eq 4 and 5 an optimum value of n 5 is implemented by default based on the work of oudin et al 2008 4 r n i j 1 n λ i j r n j 5 λ i j 1 d i j 1 k 1 n 1 d k j with k 1 n λ k j 1 2 4 step 4 discharge simulation by convolution the discharge at the ungauged outlet can finally be simulated by convolution eq 6 between the assessed net rainfall r n i and the transfer function u h i of the corresponding basin based on its hydrogeomorphometry and the estimated streamflow velocity step 1 6 q s i m i t 0 t r n i t τ u h i τ d τ 3 the transfr package 3 1 input data requirement to start using this r package three kinds of inputs are required fig 2 1 georeferenced vector layer describing the location of all gauged and ungauged basins it could be the basin delineation outlet or centroid however basin delineation enables a better assessment of the distances between them de lavenne et al 2016 2 observed discharge time series of gauged basins 3 matrices of the hydraulic lengths for all gauged and ungauged basins derived from an analysis of a dem see section 2 1 to link space and time attributes it is required to group inputs 1 and 2 into one r object of class stars pebesma 2020 users should refer to the stars package documentation to create this object from their own input format possibly using st read and st as stars functions a vignette is also provided with the r package to assist the user in this step it is further advised to add the units of these inputs hydraulic length and discharge using the units r function possibly with the units r package by pebesma et al 2016 this package does not provide functions for analysing the flow path length from a dem it needs to be prepared beforehand by the user several gis softwares offer possibilities to extract hydraulic length from a dem such as grass toolkits jasiewicz and metz 2011 whitebox gat or whiteboxtools lindsay 2016 and taudem tarboton 1997 if no function to compute hydraulic length is available it can be calculated as the total flow path length to the outlet minus the distance to the river drainage network fig 3 functions to compute these two flow path distances are more commonly proposed by gis softwares a vignette is provided with the r package to present how to compute hydraulic length using the whitebox r package lindsay 2016 3 2 data and code organisation the logic of data organisation is that the user creates a spatio temporal object of class transfr and each function of the package will add new attributes to this object the need to gather all the data in one object is justified by the fact that basins cannot be processed independently in step 3 the data set as a whole is required with both space and time attributes see section 2 3 this object approach also aims to reduce the number of arguments needed functions can generally be executed with only one input argument the transfr object since it will automatically retrieve the required attributes manage a large number of basins more easily functions will apply the data processing in a loop on every basin automatically with some possibilities of parallelisation avoid potential misuse of the package with the risk of mixing spatio temporal attributes between basins as illustrated in fig 1 each of the four modelling steps theoretically presented in section 2 can be implemented using different r functions table 1 the main precaution is to follow these steps and to run each function in the correct sequence in order to increment the object with the results needed for the next function fig 4 in order to graphically check the results at each of these steps a plot function is also available for objects of class transfr most functions are written as r methods so that they can also be used with a class object different than transfr object if the user does not want to follow the proposed object organisation all functions use only r scripts except the function hdist which calls fortran code 3 3 performance evaluation of the transfr package the transfr package allows to easily implement a leave one out evaluation also called cross validation over the user s data set it will automatically consider each gauged basin as being ungauged one after another in order to provide simulations that can be compared with observations the resulting performance can be used to evaluate the performance of the approach for truly ungauged basins in the study area this feature can also be used to evaluate different settings in the implementation of the package or to evaluate the performance gain of any new development users are advised to check the help of the mixr function and its cv argument 3 4 how to get started with transfr the transfr package is provided with a few examples and data sets that can be used for testing the package before using the user s own data vignettes are also provided to illustrate the implementation of the package using these data sets see also the following section 5 and to help with input data preparation we advise the user to carefully read the help of each function presented in table 1 this documentation can be directly accessed from r using the help function examples on how to use each function are also provided in this documentation 3 5 download and installation the package can be downloaded from the cran https cran r project org package transfr the package comes with two example data sets the oudon and blavet data sets which contain all the inputs needed to test the package and reproduce the analysis made in this paper installation should be performed from r or from an integrated development environment e g rstudio rkward eclipse it is available for the most common operating systems windows mac os linux 4 default arguments of functions most functions are provided with some default value of arguments in order to enable fast implementation of the different functions in this section we aim to provide additional information on how they have been defined and in that way we encourage users to carefully check whether these default values fit their own data set we focus on the most important ones that could affect the quality of the simulation 4 1 warm up and cool down periods inversion can potentially create oscillations mostly at the beginning and to some extent at the end of the simulated net rainfall time series these oscillations are not realistic they are the result of the minimisation of errors during the inversion procedure these oscillations thus depend on inversion parameters typically the quantification of the variance describing the discharge and the net rainfall for more details see de lavenne 2013 the package solves this issue by cutting the simulated net rainfall at the beginning warm up period and at the end cool down period of the time series fig 5 default values are provided and have been defined empirically but users are encouraged to check whether these default values need to be updated for their data set 4 2 optimisation of inversion computation time of the inversion mainly depends on the size of the matrix involved eq 3 boudhraâ et al 2018 the two dimensions of these matrices are both defined by the amount of time steps when the length of the studied period increases inversion can quickly become unmanageable for a standard computer in terms of memory or computation time the package solves this issue by splitting the period into sub periods and by aggregating the results of the inversion for each sub period this irremediably generates a multiplication of warm up and cool down periods with a potential expansion of the oscillation issues previously described section 4 1 fig 6 compares three different settings of the inversion and simply do a scatter plot to compare if they are similar it enables to emphasise two points i the numerical optimisation which consists in splitting a long period into small sub periods does not have any impact on the final result ii warm up periods are necessary for stable results and users should pay attention to this aspect in addition this splitting allows the computation of the inversion to be parallelised since the inversion of each sub period can be computed independently the default values for the warm up and cool down periods are proposed in the package and should cover a wide range of situations however users are encouraged to check the absence of oscillations by following an analysis similar to fig 6 4 3 spatial sampling for the ghosh distance in step 3 section 2 3 using the rescaled ghosh distance de lavenne et al 2016 involves the generation of sample points following a regular grid to compute the distance between two surface features the basin areas because the ghosh distance relies on the computation of all the possible distances between each of these points increasing the number of points can be computationally demanding it has thus been implemented in fortran to reduce computation time computation time can also be reduced by decreasing the number of generated points a spatial density of 10 pts km 2 seems to offer stable performance fig 7 however this density should also be considered in relation to the size of the basin skøien et al 2014 4 4 handling of missing values one weakness of the approach is its sensitivity to missing values in the time series of observed discharge these gaps are quite common and the package has been designed to deal with it the inversion is split into sub periods without missing data however it is important to note that this will enlarge the gaps of missing values in the simulated net rainfall due to warm up and cool down periods see section 4 1 two options are then proposed to average these incomplete time series of net rainfall from the different donors i the package can try to fill the gaps by looking for another donor at each time step or ii it can try to keep the same donors throughout the simulation the first approach favours stability in the number of donors throughout the simulation period while the second favours stability in the choice of the donors themselves this second approach might however lead to a reduced number of donors during periods of missing data users are encouraged to check the help of the mixr function in order to understand how to implement these options 5 example of the use of transfr package 5 1 step 0 data preparation the blavet example data set provides hourly discharge observations for six basins claie and blavet sub basins with their respective basin delineation and map of hydraulic length the documentation of the package has an additional description of the example data sets a transfr object needs to be created first with the function as transfr this object will also be used to gather all the basin attributes and intermediate results from the different steps see section 3 2 a stars object organising both time series observed discharge and spatial localisation georeferenced basin delineation of each basin the first two inputs presented in section 3 1 is directly provided by the blavet example data set the maps of hydraulic length are supplied as a list organised in the same order as in the stars object all basins are gauged however in this example we will use the first three as gauged basins and the last three as pseudo ungauged basins 5 2 step 1 a geomorphology based model the streamflow velocity is the unique parameter of the transfer function that needs to be estimated users can input this velocity parameter manually or estimate the velocity from one of the two regressions implemented in the package based on the literature indeed de lavenne et al 2016 proposed a regression between this velocity and the mean hydraulic length of the basin this regression has been adjusted using a data set of 389 basins under different hydroclimatic conditions within the french loire river basin and thus enables a first reasonable estimate of this parameter for various contexts the transfr package also proposed a similar second regression which only differs on the training data set it has been optimised in the context of the french brittany peninsula over 85 basins de lavenne 2013 and thus it is the one that is preferred for this illustrative example this velocity estimation is used to estimate all the streamflow velocities of gauged and ungauged basins using the function velocity if the input of velocity is a transfr object the velocity will be computed for every basin within the transfr object once this velocity parameter is estimated by one of these two regressions the unit hydrograph can be computed see section 2 1 if the input of uh is a transfr object the unit hydrograph will be computed for each basin by automatically retrieving the corresponding velocity and hydraulic lengths see fig 8 5 3 step 2 net rainfall of gauged basins by inverse modelling the net rainfall of a gauged basin is estimated by inverse modelling which requires an a priori estimate of the net rainfall see section 2 2 we will estimate this a priori using the specific discharge simply delayed by a lag time see fig 5 using both functions lagtime and rapriori once all the needed inputs are saved in the transfr object unit hydrograph discharge time series and the a priori on net rainfall the deconvolution on each gauged basin can be run with the function inversion 5 4 step 3 net rainfall of ungauged basins the net rainfall of an ungauged basin is an averaged value of net rainfalls of neighbouring gauged basins this average can be weighted by the inverse of the distance between the gauged basins and the ungauged basin see section 2 3 the function hdist should be used to compute the distance matrix between basins different options are proposed to compute this distance matrix such as distance between the centroids outlet or a combination of both following lebecherel et al 2016 however de lavenne et al 2016 recommend using a rescaled ghosh distance this distance matrix is then used as an input for the function mixr to estimate the net rainfall time series at every ungauged location 5 5 step 4 discharge simulation finally the function convolution will retrieve two attributes from the transfr object the unit hydrograph and the simulated net rainfall to provide the discharge simulation for each ungauged basin in this example illustrated by fig 9 it is possible to compare discharge simulation and discharge observation because pseudo ungauged basins are considered see section 5 1 note that the beginning and the end of the simulation are cut because of the warm up and cool down periods needed for the inversion see section 4 1 6 limitations and perspectives of the methodology because the modelling approach focuses only on the transfer function within the river drainage network the hydrological behaviours of hillslopes are not explicitly described this concerns any production of surface flow sub surface flow or any groundwater contribution to river discharge it is assumed that the hillslope behaviour of an ungauged basin can be approximated from the behaviour of the surrounding basins through a weighted average however this behaviour may quickly change in space e g due to geology and thus invalidate an approach based on spatial proximity one perspective would be to investigate new hydrological distances that would take into account physiographic differences between basins oudin et al 2010 loritz et al 2018 2019 another assumption that may affect the performance of the simulations concerns the spatial homogeneity of meteorological inputs on one hand the approach does not require rainfall or temperature input data but on the other hand it assumes that the surrounding basins provide a reliable estimate of these inputs for ungauged basins in regions where high spatial meteorological variability is observed e g in mountainous regions this assumption may be invalidated this is why climatic descriptors often appear as important drivers of the hydrological similarity in large sample studies with contrasting climate regions jehn et al 2020 pool et al 2021 one way to improve the methodology would be to incorporate rainfall information and evapotranspiration estimation into the current modelling framework or into the computation of hydrological distances the approach relies on the geographical proximity of stream gauges to the ungauged basins several studies have demonstrated the positive impact of gauging station density on the performance of regionalisation techniques e g for topological kriging parajka et al 2005 or for regionalisation of rainfall runoff models lebecherel et al 2016 neri et al 2020 although the geographical proximity approach often appears as the most efficient approach in well gauged regions oudin et al 2008 it might not be the case in poorly gauged regions pool et al 2021 indeed even in well gauged regions the performance of regionalisation is not strictly driven by the spatial proximity to a monitoring station especially in dry regions patil and stieglitz 2012 as demonstrated by de lavenne et al 2016 the performance of our approach mainly depends on basin size with larger basins having better performance and on how this hydrological distance is estimated the core of the hydrological model i e the geomorphology based transfer function aims at describing the transfer dynamics of each basin so as to take into account the specificity of each one in order to correctly capture this transfer dynamic the modelling time step should preferably be smaller than the response time of the basins in previous studies in which the methodology has been applied see introduction the hourly time step has always been preferred in order to correctly capture the dynamics of storm events and take full advantage of the methodology s potential however the transfr package has been built to be flexible enough to handle any modelling time step it can be thus refined or enlarged according to the studied basins and the available data based on this flexibility another perspective of this work would be to evaluate the potential of this approach to capture the shape of the hydrograph during extreme events flood and drought in comparison with a rainfall runoff modelling approach the modelling principle implemented distinguishes the hillslopes from the river drainage network see e g robinson et al 1995 with the objective of explicitly modelling only the latter thus we assume that the velocity parameter describes only the flow dynamics within the river drainage network at the level of the whole basin without being influenced by the dynamics of the hillslopes however this distinction is tenuous when looking at the aggregating discharge signal so the proposed regionalisation methods de lavenne 2013 de lavenne et al 2016 based on rising time analysis are not able to ensure a rigorous distinction although the estimated values are reasonable and provide a satisfactory performance a perspective of this work would be to propose new estimates and regionalisation of this velocity parameter that would better address this distinction between hillslopes and river drainage network 7 summary the transfr package was designed to ease the implementation of the methodology developed over the past years for prediction in ungauged basins using geomorphology based models the package includes a series of functions to create and use geomorphology based transfer functions perform inverse modelling analyse hydrological distances between basins and preview the results the package will be maintained and further developed in the coming years to include more options and functionalities this package is already running as a computational backend of the simfen web service dallery et al 2020 for the brittany region in france which provides a user friendly interface for this modelling including an automatic harvesting of discharge series from the public database and an online basin delineation facility this package aims to complement this web service for more expert users and for research needs it offers the possibility of expertise and improvement of the code it allows an implementation on a large sample of basins with potential comparisons with other approaches finally it opens up possibilities for implementation in any region with user specific input data thanks to its generic principles feedback or contributions are therefore welcome and can be sent to the authors credit authorship contribution statement alban de lavenne conceptualization methodology software writing original draft writing review editing visualization tom loree software data curation review editing hervé squividant software data curation review editing christophe cudennec conceptualization methodology writing review editing project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the conseil régional de bretagne france and agence de l eau loire bretagne france appendix r code of figures a 1 fig 5 of the manuscript a 2 fig 6 of the manuscript 
25476,this r package technically gathers the methods developed and lessons learnt over recent years for estimating discharge of ungauged outlets in a runoff runoff type approach discharge is both the input and output it uses the observed discharge of neighbouring gauged basins and a geomorphology based deconvolution convolution modelling approach the continuous hydrological modelling is based on the description of the hydro geomorphometry of the river drainage network that can be easily described for any outlet considered an inversion of this model allows us to deconvolute the observed discharge series and to estimate the signal of the water flowing into rivers from the hillslope namely the net rainfall the transfer of this net rainfall to a targeted ungauged basin enables the simulation of the discharge series there the methodology has been tested in several hydro climatic contexts and with this package we aim to encourage further evaluation and improvement as well as subsequent operational applications keywords prediction in ungauged basin pub streamflow observation hydrogeomorphology inverse modelling r package data availability code and data are freely available as described in the paper software availability name of software transfr version 1 0 0 developer alban de lavenne maintainer alban de lavenne contact email alban delavenne inrae fr year of first release 2021 software required r and fortran compiler program language r and fortran program size 2 2 mo license gpl 2 availability and cost open source software available at https cran r project org package transfr download free of charge 1 introduction streamflow time series and their derived signatures and metrics mcmillan 2020 represent essential information for the relevant management of water dynamics resources and hazards however it is often not measured for the exact location where decisions need to be made ceola et al 2016 cudennec et al 2020 dixon et al 2020 and thus the location the basin and its outlet is referred to as ungauged major methodological progress has been made over the past two decades from the perspective of predictions in ungauged basins pub blöschl et al 2013 hrachowitz et al 2013 of discharge series or related signatures approaches often rely on a rainfall runoff model and on a method summarised by the term regionalisation which makes use of the calibration at gauged locations to predict the hydrological behaviour at ungauged target locations different tools and softwares have been developed to help the implementation of rainfall runoff models in data scarce regions see e g gibbs et al 2012 archfield et al 2013b pumo et al 2017 ouarda et al 2018 jafarzadegan et al 2020 alternative runoff runoff modelling approaches less frequent than rainfall runoff approaches have also been developed for the prediction of discharge statistics or discharge time series in ungauged basins they essentially rely on geostatistical spatial interpolation skøien and blöschl 2007 isaak et al 2014 müller and thompson 2015 farmer 2016 transformation functions applied to hydrographs andréassian et al 2012 or hydraulic routing tewolde and smithers 2007 song et al 2011 some of these approaches were complemented by the development of r packages such as rtop skøien et al 2014 and atakrig hu and huang 2020 in comparison with the rainfall runoff approach runoff runoff modelling approaches do not require an explicit modelling of the hydrological response to meteorological forcing data on the other hand they do not explicitly take into account the specificity of this forcing at an ungauged location and rely on the density of stream gauges however by relying directly on discharge observations and not on model simulations they often provide more robust predictions than other regionalisation techniques in densely instrumented regions laaha et al 2012 archfield et al 2013a viglione et al 2013 we present a runoff runoff approach that differs from these geostatistically driven approaches relying mainly on geomorphological descriptions that are easily provided at any point it aims at transferring the observed discharge series themselves from one or several gauged locations to an ungauged target location the approach relies on a four step rationale i building of a hydrogeomorphology based transfer function ii estimation of net rainfall series by deconvolution of observed discharge series for one or several gauged basins iii inter basin transfer of net rainfall series towards the ungauged target basin iv simulation of discharge series of the ungauged target basin via convolution with its own hydrogeomorphology based transfer function here we present an open source tool delivered as an r package called transfr which enables the implementation of this pub approach the shape of every hydrograph results from the convolution of the dynamics of water flowing from hillslopes into the river drainage network net rainfall and the overall converging across the drainage network of the considered basin down to the outlet a geometric characterisation of the drainage network of a basin is accessible whether the basin is gauged or not and provides valuable information on the underlying hydrological dynamics see e g rigon et al 2016 a robust transfer function of the water dynamics across the drainage network can be deduced from such a characterisation of the hydro geomorphometry and an average velocity based on the unit hydrograph theory it can be used to support mathematical convolution see e g sherman 1932 dooge 1959 rinaldo and rodriguez iturbe 1996 cudennec 2007 moussa 2008 grimaldi et al 2011 andrieu et al 2021 inversion of such transfer functions allows one to deconvolute the observed discharge series of a gauged location and estimate the net rainfall series articulating inverse and direct modelling over neighbouring gauged basins and ungauged basins makes it possible to use a runoff runoff approach in order to address the pub challenge the hydrological modelling approach has been built and improved over time the original idea was introduced theoretically by cudennec 2000 and implemented in the following years the inverse modelling at the core being lately published by boudhraâ et al 2018 the methodology was first implemented on a few basins in semi arid tunisia at the event timescale boudhraâ et al 2006 2009 then in dense configurations of neighbouring and nesting basins in temperate oceanic brittany france de lavenne et al 2015 and in snow influenced québec canada ecrepont et al 2019 on a continuous time basis it has been compared with another approach well received by the community namely top kriging skøien and blöschl 2007 and similar performance was observed de lavenne et al 2016 this evaluation was made on a large set of 389 sub basins of the french loire river basin 117 500 km 2 and a higher robustness was achieved by combining the information of several gauged basins beyond pseudo ungauged applications and analyses simulations for truly ungauged basins have been performed including for multiple outlets flowing into coastal bays de lavenne and cudennec 2014 2019 and the potential has been pointed out for water quality modelling that often relies on a good estimate of discharge for the estimation of nutrient fluxes recently the approach was implemented in a web processing service wps named simfen for operational perspectives in the french brittany region dallery et al 2020 with an accompanying web interface https bit do simfen in the spirit of open service delivery and chaining of web services bera et al 2015 squividant et al 2015 pecora and lins 2020 this package has been initially developed to be the computational backend of this wps this package aims to provide a new tool for water management modelling in order to spatially evaluate the resource at any point in a territory by directly valuing the gauges of its rivers making the package available aims to i open new applications by end users in particular water and basin managers public authorities and engaged citizens see e g nardi et al 2021 ii get feedback and facilitate new bridges between science practices and investigations on the method and iii value and explore genericity and flexibility in various hydrological regions and contexts hall et al 2022 we have chosen to implement the method in the r programming language thus relying on a growing community of developers and users in the field of hydrology slater et al 2019 astagneau et al 2021 2 theory this paper does not provide an exhaustive description of the theoretical aspects of the method implemented in the package as it has already been described in previous studies see introduction however our aim is to support an efficient implementation of this r package with some reference points on the conceptualisation and on the way how it is translated into this programming language the implementation of the methods can be summarised in four steps fig 1 presented in the following sections 2 1 step 1 a hydrogeomorphology based transfer function modelling the first step is to build a hydrological model of the transfer function type for every basin considered either gauged or ungauged based on hydrogeomorphometry the hydrological processes that we aim to describe are limited to the water transfer within the drainage network and do not address any explicit description of the behaviour of hillslopes for any geographic point within the basin the flow path and its length called hydraulic length x c within this drainage network to the outlet can be extracted from a digital elevation model dem assuming an average velocity allows us to estimate a travel time along this flow path applying this rationale to a grid of points within the basin makes it possible to estimate probability density functions of the hydraulic length and hence of the travel time which can then be used as a unit hydrograph u h sherman 1932 streamflow velocity u c is thus the only parameter that needs to be estimated either from observation in the case of a gauged basin or from indirect assessment in the case of an ungauged basin previous studies have indeed shown that this velocity can be easily regionalised see e g eq 1 1 u c a x c b where x c is the mean hydraulic length and calibrated with a 4 38 1 0 4 and b 0 69 in the work of de lavenne et al 2016 over the french loire river basin even if this calibration has been done over a wide range of hydroclimatic conditions these values of a and b might need to be re calibrated locally de lavenne et al 2016 proposed to estimate this velocity u c from the ratio of mean hydraulic length x c over mean rising time of the basin 2 2 step 2 net rainfall of gauged basins by deconvolution the second step is to assess the water flowing into the drainage network from hillslopes namely the net rainfall r n instead of estimating r n from a direct modelling of hillslope response to rainfall which remains highly non linear and difficult to quantify by field measurements it is done by an inverse modelling of the transfer function u h developed in step 1 for a gauged basin we estimate the net rainfall that best reconstitutes the observed discharge this inversion is solved by minimising the following expression tarantola and valette 1982 menke 1984 2 q q o b s t c q o b s 1 q q o b s r n r n a p t c r n a p 1 r n r n a p where q is the output of the model r n a p is initial a priori information about r n and c r n a p and c q o b s are covariance matrices for vectors r n a p and q o b s respectively and superscript t is the matrix transpose r n a p is estimated from the specific discharge of q o b s delayed by a lag time which can be estimated using x c u c a maximum likelihood solution is used to solve this inversion eq 3 see tarantola and valette 1982 menke 1984 for generic aspects and boudhraâ et al 2018 for a detailed application as we are using a lumped modelling approach we focus on the temporal dynamics of this net rainfall but spatial heterogeneity is not explicitly considered 3 r n r n a p c r n a p u h t u h c r n a p u h t c q o b s 1 q o b s u h r n a p 2 3 step 3 net rainfall of ungauged basins by spatial interpolation the third step is to estimate the net rainfall of the targeted ungauged basin i by combining the net rainfalls of neighbouring gauged basins j a weighting strategy can be used in order to increase the weight λ i j of the gauged basins for which a higher hydrological similarity is assumed to exist with the ungauged one estimating this similarity is a rather complex question but spatial proximity approaches generally appear to be efficient in large sample studies and in data rich countries and therefore it is the one currently implemented in this tool see also the discussion on gauging station density in section 6 de lavenne et al 2016 advised computing a distance between basins using a rescaled ghosh distance d ghosh 1951 gottschalk 1993 gottschalk et al 2011 considering the n closest basins in a given radius number of donors and radius parameterised by the user around the targeted ungauged basin their net rainfall time series are averaged using an inverse distance weighting strategy and the outcome is considered to be the net rainfall time series of the target basin eq 4 and 5 an optimum value of n 5 is implemented by default based on the work of oudin et al 2008 4 r n i j 1 n λ i j r n j 5 λ i j 1 d i j 1 k 1 n 1 d k j with k 1 n λ k j 1 2 4 step 4 discharge simulation by convolution the discharge at the ungauged outlet can finally be simulated by convolution eq 6 between the assessed net rainfall r n i and the transfer function u h i of the corresponding basin based on its hydrogeomorphometry and the estimated streamflow velocity step 1 6 q s i m i t 0 t r n i t τ u h i τ d τ 3 the transfr package 3 1 input data requirement to start using this r package three kinds of inputs are required fig 2 1 georeferenced vector layer describing the location of all gauged and ungauged basins it could be the basin delineation outlet or centroid however basin delineation enables a better assessment of the distances between them de lavenne et al 2016 2 observed discharge time series of gauged basins 3 matrices of the hydraulic lengths for all gauged and ungauged basins derived from an analysis of a dem see section 2 1 to link space and time attributes it is required to group inputs 1 and 2 into one r object of class stars pebesma 2020 users should refer to the stars package documentation to create this object from their own input format possibly using st read and st as stars functions a vignette is also provided with the r package to assist the user in this step it is further advised to add the units of these inputs hydraulic length and discharge using the units r function possibly with the units r package by pebesma et al 2016 this package does not provide functions for analysing the flow path length from a dem it needs to be prepared beforehand by the user several gis softwares offer possibilities to extract hydraulic length from a dem such as grass toolkits jasiewicz and metz 2011 whitebox gat or whiteboxtools lindsay 2016 and taudem tarboton 1997 if no function to compute hydraulic length is available it can be calculated as the total flow path length to the outlet minus the distance to the river drainage network fig 3 functions to compute these two flow path distances are more commonly proposed by gis softwares a vignette is provided with the r package to present how to compute hydraulic length using the whitebox r package lindsay 2016 3 2 data and code organisation the logic of data organisation is that the user creates a spatio temporal object of class transfr and each function of the package will add new attributes to this object the need to gather all the data in one object is justified by the fact that basins cannot be processed independently in step 3 the data set as a whole is required with both space and time attributes see section 2 3 this object approach also aims to reduce the number of arguments needed functions can generally be executed with only one input argument the transfr object since it will automatically retrieve the required attributes manage a large number of basins more easily functions will apply the data processing in a loop on every basin automatically with some possibilities of parallelisation avoid potential misuse of the package with the risk of mixing spatio temporal attributes between basins as illustrated in fig 1 each of the four modelling steps theoretically presented in section 2 can be implemented using different r functions table 1 the main precaution is to follow these steps and to run each function in the correct sequence in order to increment the object with the results needed for the next function fig 4 in order to graphically check the results at each of these steps a plot function is also available for objects of class transfr most functions are written as r methods so that they can also be used with a class object different than transfr object if the user does not want to follow the proposed object organisation all functions use only r scripts except the function hdist which calls fortran code 3 3 performance evaluation of the transfr package the transfr package allows to easily implement a leave one out evaluation also called cross validation over the user s data set it will automatically consider each gauged basin as being ungauged one after another in order to provide simulations that can be compared with observations the resulting performance can be used to evaluate the performance of the approach for truly ungauged basins in the study area this feature can also be used to evaluate different settings in the implementation of the package or to evaluate the performance gain of any new development users are advised to check the help of the mixr function and its cv argument 3 4 how to get started with transfr the transfr package is provided with a few examples and data sets that can be used for testing the package before using the user s own data vignettes are also provided to illustrate the implementation of the package using these data sets see also the following section 5 and to help with input data preparation we advise the user to carefully read the help of each function presented in table 1 this documentation can be directly accessed from r using the help function examples on how to use each function are also provided in this documentation 3 5 download and installation the package can be downloaded from the cran https cran r project org package transfr the package comes with two example data sets the oudon and blavet data sets which contain all the inputs needed to test the package and reproduce the analysis made in this paper installation should be performed from r or from an integrated development environment e g rstudio rkward eclipse it is available for the most common operating systems windows mac os linux 4 default arguments of functions most functions are provided with some default value of arguments in order to enable fast implementation of the different functions in this section we aim to provide additional information on how they have been defined and in that way we encourage users to carefully check whether these default values fit their own data set we focus on the most important ones that could affect the quality of the simulation 4 1 warm up and cool down periods inversion can potentially create oscillations mostly at the beginning and to some extent at the end of the simulated net rainfall time series these oscillations are not realistic they are the result of the minimisation of errors during the inversion procedure these oscillations thus depend on inversion parameters typically the quantification of the variance describing the discharge and the net rainfall for more details see de lavenne 2013 the package solves this issue by cutting the simulated net rainfall at the beginning warm up period and at the end cool down period of the time series fig 5 default values are provided and have been defined empirically but users are encouraged to check whether these default values need to be updated for their data set 4 2 optimisation of inversion computation time of the inversion mainly depends on the size of the matrix involved eq 3 boudhraâ et al 2018 the two dimensions of these matrices are both defined by the amount of time steps when the length of the studied period increases inversion can quickly become unmanageable for a standard computer in terms of memory or computation time the package solves this issue by splitting the period into sub periods and by aggregating the results of the inversion for each sub period this irremediably generates a multiplication of warm up and cool down periods with a potential expansion of the oscillation issues previously described section 4 1 fig 6 compares three different settings of the inversion and simply do a scatter plot to compare if they are similar it enables to emphasise two points i the numerical optimisation which consists in splitting a long period into small sub periods does not have any impact on the final result ii warm up periods are necessary for stable results and users should pay attention to this aspect in addition this splitting allows the computation of the inversion to be parallelised since the inversion of each sub period can be computed independently the default values for the warm up and cool down periods are proposed in the package and should cover a wide range of situations however users are encouraged to check the absence of oscillations by following an analysis similar to fig 6 4 3 spatial sampling for the ghosh distance in step 3 section 2 3 using the rescaled ghosh distance de lavenne et al 2016 involves the generation of sample points following a regular grid to compute the distance between two surface features the basin areas because the ghosh distance relies on the computation of all the possible distances between each of these points increasing the number of points can be computationally demanding it has thus been implemented in fortran to reduce computation time computation time can also be reduced by decreasing the number of generated points a spatial density of 10 pts km 2 seems to offer stable performance fig 7 however this density should also be considered in relation to the size of the basin skøien et al 2014 4 4 handling of missing values one weakness of the approach is its sensitivity to missing values in the time series of observed discharge these gaps are quite common and the package has been designed to deal with it the inversion is split into sub periods without missing data however it is important to note that this will enlarge the gaps of missing values in the simulated net rainfall due to warm up and cool down periods see section 4 1 two options are then proposed to average these incomplete time series of net rainfall from the different donors i the package can try to fill the gaps by looking for another donor at each time step or ii it can try to keep the same donors throughout the simulation the first approach favours stability in the number of donors throughout the simulation period while the second favours stability in the choice of the donors themselves this second approach might however lead to a reduced number of donors during periods of missing data users are encouraged to check the help of the mixr function in order to understand how to implement these options 5 example of the use of transfr package 5 1 step 0 data preparation the blavet example data set provides hourly discharge observations for six basins claie and blavet sub basins with their respective basin delineation and map of hydraulic length the documentation of the package has an additional description of the example data sets a transfr object needs to be created first with the function as transfr this object will also be used to gather all the basin attributes and intermediate results from the different steps see section 3 2 a stars object organising both time series observed discharge and spatial localisation georeferenced basin delineation of each basin the first two inputs presented in section 3 1 is directly provided by the blavet example data set the maps of hydraulic length are supplied as a list organised in the same order as in the stars object all basins are gauged however in this example we will use the first three as gauged basins and the last three as pseudo ungauged basins 5 2 step 1 a geomorphology based model the streamflow velocity is the unique parameter of the transfer function that needs to be estimated users can input this velocity parameter manually or estimate the velocity from one of the two regressions implemented in the package based on the literature indeed de lavenne et al 2016 proposed a regression between this velocity and the mean hydraulic length of the basin this regression has been adjusted using a data set of 389 basins under different hydroclimatic conditions within the french loire river basin and thus enables a first reasonable estimate of this parameter for various contexts the transfr package also proposed a similar second regression which only differs on the training data set it has been optimised in the context of the french brittany peninsula over 85 basins de lavenne 2013 and thus it is the one that is preferred for this illustrative example this velocity estimation is used to estimate all the streamflow velocities of gauged and ungauged basins using the function velocity if the input of velocity is a transfr object the velocity will be computed for every basin within the transfr object once this velocity parameter is estimated by one of these two regressions the unit hydrograph can be computed see section 2 1 if the input of uh is a transfr object the unit hydrograph will be computed for each basin by automatically retrieving the corresponding velocity and hydraulic lengths see fig 8 5 3 step 2 net rainfall of gauged basins by inverse modelling the net rainfall of a gauged basin is estimated by inverse modelling which requires an a priori estimate of the net rainfall see section 2 2 we will estimate this a priori using the specific discharge simply delayed by a lag time see fig 5 using both functions lagtime and rapriori once all the needed inputs are saved in the transfr object unit hydrograph discharge time series and the a priori on net rainfall the deconvolution on each gauged basin can be run with the function inversion 5 4 step 3 net rainfall of ungauged basins the net rainfall of an ungauged basin is an averaged value of net rainfalls of neighbouring gauged basins this average can be weighted by the inverse of the distance between the gauged basins and the ungauged basin see section 2 3 the function hdist should be used to compute the distance matrix between basins different options are proposed to compute this distance matrix such as distance between the centroids outlet or a combination of both following lebecherel et al 2016 however de lavenne et al 2016 recommend using a rescaled ghosh distance this distance matrix is then used as an input for the function mixr to estimate the net rainfall time series at every ungauged location 5 5 step 4 discharge simulation finally the function convolution will retrieve two attributes from the transfr object the unit hydrograph and the simulated net rainfall to provide the discharge simulation for each ungauged basin in this example illustrated by fig 9 it is possible to compare discharge simulation and discharge observation because pseudo ungauged basins are considered see section 5 1 note that the beginning and the end of the simulation are cut because of the warm up and cool down periods needed for the inversion see section 4 1 6 limitations and perspectives of the methodology because the modelling approach focuses only on the transfer function within the river drainage network the hydrological behaviours of hillslopes are not explicitly described this concerns any production of surface flow sub surface flow or any groundwater contribution to river discharge it is assumed that the hillslope behaviour of an ungauged basin can be approximated from the behaviour of the surrounding basins through a weighted average however this behaviour may quickly change in space e g due to geology and thus invalidate an approach based on spatial proximity one perspective would be to investigate new hydrological distances that would take into account physiographic differences between basins oudin et al 2010 loritz et al 2018 2019 another assumption that may affect the performance of the simulations concerns the spatial homogeneity of meteorological inputs on one hand the approach does not require rainfall or temperature input data but on the other hand it assumes that the surrounding basins provide a reliable estimate of these inputs for ungauged basins in regions where high spatial meteorological variability is observed e g in mountainous regions this assumption may be invalidated this is why climatic descriptors often appear as important drivers of the hydrological similarity in large sample studies with contrasting climate regions jehn et al 2020 pool et al 2021 one way to improve the methodology would be to incorporate rainfall information and evapotranspiration estimation into the current modelling framework or into the computation of hydrological distances the approach relies on the geographical proximity of stream gauges to the ungauged basins several studies have demonstrated the positive impact of gauging station density on the performance of regionalisation techniques e g for topological kriging parajka et al 2005 or for regionalisation of rainfall runoff models lebecherel et al 2016 neri et al 2020 although the geographical proximity approach often appears as the most efficient approach in well gauged regions oudin et al 2008 it might not be the case in poorly gauged regions pool et al 2021 indeed even in well gauged regions the performance of regionalisation is not strictly driven by the spatial proximity to a monitoring station especially in dry regions patil and stieglitz 2012 as demonstrated by de lavenne et al 2016 the performance of our approach mainly depends on basin size with larger basins having better performance and on how this hydrological distance is estimated the core of the hydrological model i e the geomorphology based transfer function aims at describing the transfer dynamics of each basin so as to take into account the specificity of each one in order to correctly capture this transfer dynamic the modelling time step should preferably be smaller than the response time of the basins in previous studies in which the methodology has been applied see introduction the hourly time step has always been preferred in order to correctly capture the dynamics of storm events and take full advantage of the methodology s potential however the transfr package has been built to be flexible enough to handle any modelling time step it can be thus refined or enlarged according to the studied basins and the available data based on this flexibility another perspective of this work would be to evaluate the potential of this approach to capture the shape of the hydrograph during extreme events flood and drought in comparison with a rainfall runoff modelling approach the modelling principle implemented distinguishes the hillslopes from the river drainage network see e g robinson et al 1995 with the objective of explicitly modelling only the latter thus we assume that the velocity parameter describes only the flow dynamics within the river drainage network at the level of the whole basin without being influenced by the dynamics of the hillslopes however this distinction is tenuous when looking at the aggregating discharge signal so the proposed regionalisation methods de lavenne 2013 de lavenne et al 2016 based on rising time analysis are not able to ensure a rigorous distinction although the estimated values are reasonable and provide a satisfactory performance a perspective of this work would be to propose new estimates and regionalisation of this velocity parameter that would better address this distinction between hillslopes and river drainage network 7 summary the transfr package was designed to ease the implementation of the methodology developed over the past years for prediction in ungauged basins using geomorphology based models the package includes a series of functions to create and use geomorphology based transfer functions perform inverse modelling analyse hydrological distances between basins and preview the results the package will be maintained and further developed in the coming years to include more options and functionalities this package is already running as a computational backend of the simfen web service dallery et al 2020 for the brittany region in france which provides a user friendly interface for this modelling including an automatic harvesting of discharge series from the public database and an online basin delineation facility this package aims to complement this web service for more expert users and for research needs it offers the possibility of expertise and improvement of the code it allows an implementation on a large sample of basins with potential comparisons with other approaches finally it opens up possibilities for implementation in any region with user specific input data thanks to its generic principles feedback or contributions are therefore welcome and can be sent to the authors credit authorship contribution statement alban de lavenne conceptualization methodology software writing original draft writing review editing visualization tom loree software data curation review editing hervé squividant software data curation review editing christophe cudennec conceptualization methodology writing review editing project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the conseil régional de bretagne france and agence de l eau loire bretagne france appendix r code of figures a 1 fig 5 of the manuscript a 2 fig 6 of the manuscript 
25477,while global sensitivity analysis gsa has been widely implemented for crop models few studies have assessed the sensitivity of model parameters at the regional scale to fully understand the overall sensitivity of parameters in a specific area this study carried out a regional scale global sensitivity analysis rgsa by selecting multiple stations through grids within the study area and integrated gsa results obtained by extended fourier amplitude sensitivity test efast from these stations by statistical indicators to quantify parameter sensitivity at the regional scale taking the yangtze river basin yrb as the study area rgsa was implemented for the oryza model for paddy rice under two water and nitrogen situations wncs and four levels of number of stations for sensitivity analysis nssa the results suggested that the applicable area for the results of sensitive and insensitive parameters partitioning sensitivity ranks and sensitivity indices at a specific station were not larger than grid sizes of 8 8 4 4 and 2 2 around it respectively thus the nssa needed for sensitive parameter identification sensitivity ranking and sensitivity indices variability analysis at regional scale increased sequentially it was recommended that a suitable number of stations should be chosen for analysis according to the sensitivity analysis purpose at the regional scale since wnc will affect the results of rgsa such analysis was recommended to be performed separately for different wncs highly sensitive parameters of the oryza model for paddy rice in yzb were screened out through rgsa which were dvrj dvrp and rgrlmx model users can adopt the rgsa workflow used in this study for parameter sensitivity assessment of other crop models keywords global sensitivity analysis oryza model screening ranking regional scale data availability data will be made available on request 1 introduction sensitivity analysis sa aims to characterize the impact of changes in the model input e g parameters input data and initial states etc on the model outputs sarrazin et al 2016 sa can identify parameters that greatly affect model output and thus guide model calibration model users can have better insight into a specific model by sa to improve the model sa is usually distinguished into two types local sensitivity analysis lsa and global sensitivity analysis gsa lsa focuses on the local impact of uncertain factors on model outputs and is carried out by computing partial derivatives of the output variables with respect to the input factors bert et al 2007 pogson et al 2012 wallach et al 2019 contrary to lsa gsa can consider the full domain of uncertainty of the uncertain model factors gsa is usually implemented with monte carlo sampling shapiro 2003 several sets of parameter values are first sampled and then used for model simulation then parameter sensitivity is quantified by an index computed based on model outputs compared with lsa gsa can analyse several parameters at the same time while quantifying the interactions among different parameters therefore gsa has been widely used in the study of sa of environmental models such as crop models and hydrological models liang et al 2017 sarrazin et al 2016 suárez rey et al 2019 tan et al 2021 crop models are one of the most widely used environmental models which can simulate the growth of crops under different environmental conditions and field management practices muller and martre 2019 a crop model usually consists of many equations or modules that simulate crop growth as well as the transport and transfer of water and nutrients in the field bouman et al 2001 jones et al 2003 li et al 2017 mccown et al 1996 raes 2007 a crop model also contains many parameters the values of some parameters can be obtained directly from field surveys or experiments such as parameters related to soil properties and field management meanwhile the values of some parameters need to be estimated indirectly by comparing the model outputs and their corresponding observations which is the so called calibration these parameters are mainly those related to crop cultivar ma et al 2020 tan and duan 2019 tan et al 2021 wallach et al 2011 since a crop model usually has many parameters related to crop cultivar it is time and labour intensive to calibrate all of these cultivar parameters therefore it will be helpful to screen out parameters with high sensitivity to model outputs for calibration while parameters that contribute little can be fixed saltelli et al 2019 previous studies found that parameter variation range tan et al 2017 wang et al 2013 and parameter sample size becker et al 2018 sarrazin et al 2016 wang et al 2013 will affect the results of gsa for crop models in addition the environmental conditions set in the crop model e g water condition nitrogen supply and meteorological factors etc dejonge et al 2012 liang et al 2017 zhao et al 2014 and the model structure lu et al 2021 stella et al 2014 also influence the gsa results some studies found that the sensitivity of the same parameter varies at different stations suggesting that parameter sensitivity has spatial variability he et al 2015 lu et al 2021 tan et al 2020 zhao et al 2014 therefore gsa results obtained from a specific station might not be suitable for other stations for studies at the field scale the gsa result of a single station can provide enough information for model calibration however for studies at the regional scale crop models need to be calibrated at different stations within the study area gsa results from a single station might lead to the misidentification of sensitive parameters at other stations and provide incorrect information for model calibration thus for crop model application at the regional scale it is essential to replace traditional single station analysis at the field scale with multiple stations analysis at the regional scale to obtain a result that can reflect the overall sensitivity of the model parameters within the study area and provide a more accurate guide for model calibration the gsa of the crop model in most of the existing studies is implemented at a single station level although some studies analysed parameter sensitivity at several stations the objectives of these studies were to investigate the effect of meteorological factors on gsa he et al 2015 lu et al 2021 zhao et al 2014 to the best of our knowledge few studies implement gsa at multiple stations within a specific area and assess the overall parameter sensitivity within the area to assess the parameter sensitivity of crop models at the regional scale two problems need to be solved 1 construct indicators to quantify the parameter sensitivity at the regional scale indicators used in previous studies can only be used for sensitivity quantification in single analyses at the field scale such as the first order and total order sensitivity indices computed by variance based methods saltelli et al 1999 the elementary effect computed by the method developed by morris 1991 and the standardized regression coefficient computed by the regression based method tan et al 2017 when the scale of the analysis is upgraded it is necessary to comprehensively consider the variations in parameter sensitivity among different stations within the area and construct suitable indicators to reflect the overall sensitivity of parameters in the area 2 determine the number of stations for sensitivity analysis nssa selecting too few stations may not fully reflect the actual parameter sensitivity in the area while selecting too many stations will increase the simulation times for a specific area how to select stations for analysis and specify the nssa is the second problem that needs to be solved the oryza model is a widely used crop model designed for the simulation of rice growth belder et al 2007 dossou yovo et al 2020 sudhir et al 2012 tan et al 2016 2017 and 2020 investigated the parameter sensitivity of the oryza model under different parameter variation ranges and contrasting weather types tan et al 2016 2017 2020 however few studies have taken a large scale rice cropping area as the study area and assessed the parameter sensitivity of the oryza model within the area besides comparison of the parameter sensitivity of the oryza model under different water and nitrogen conditions wncs were rarely reported in this study a multiple station gsa was implemented in the yangtze river basin yrb in china and the results from each station were integrated with statistical indicators to quantify the overall parameter sensitivity of the oryza model within the area this kind of analysis was called regional scale global sensitivity analysis rgsa in this study the main objectives of this study are 1 to propose indicators for the rgsa of crop models 2 to investigate the effect of the nssa on the rgsa under different wncs when the stations are selected from the grid in the study area and 3 to assess the parameter sensitivity of the oryza model for paddy rice in the yrb and screen out the sensitive parameters in this area 2 materials and methods this section introduces the oryza model section 2 1 and yrb section 2 2 first then the statistical indicators and workflow for rgsa are explained section 2 3 finally the experiment that carried out to explore the effect of the nssa on the rgsa under different wncs is introduced section 2 4 2 1 model description the oryza model was chosen for rgsa in this study this model can simulate the growth and development of a rice crop in situations of potential production and water and nitrogen limitations bouman et al 2001 for the potential production condition ppc the growth of rice occurs in conditions with an ample supply of water and nitrogen growth rates are determined by varietal characteristics and weather conditions solar radiation and temperature no need to set the irrigation and nitrogen fertilization modes for water and nitrogen limited condition wnlc the irrigation and nitrogen fertilization modes need to be set in the model the growth of rice is affected by the water and nitrogen supply in paddy fields the growth rate of rice might be reduction by water and nitrogen stress rice growth simulation is implemented by the oryza1 module in the model the oryza1 module can be divided into three parts 1 phenological development simulation which simulates the physiological age of rice the physiological age of rice is expressed as the development stage dvs in the model and is simulated based on radiation and temperature 2 leaf area index lai simulation lai is simulated based on daily temperature and leaf biomass 3 biomass simulation which simulates the biomass of leaves stems and storage organs of rice at different dvss the algorithms used in biomass simulation are basically the functions of dvs the simulation of biomass is also affected by solar radiation and lai in addition the biomass is simulated along with the computation of nitrogen uptake and rice yield rice growth simulation is determined by the value of cultivar parameters in the model tan et al 2016 screened 20 cultivar parameters related to rice growth tan et al 2016 these 20 parameters were used for rgsa in this study in addition a water and nitrogen balance simulation module called swnp was constructed for the oryza model to simulate the transformation and transfer of water and nitrogen in paddy fields for irrigation and drainage control a technique using three thresholds is incorporated to control the irrigation and drainage of paddy fields guo 1997 wu et al 2019 yu and cui 2022 the three thresholds include the minimum fitting threshold h min or θ min the maximum fitting depth h max and the maximum allowable depth of impoundment h p daily irrigation is triggered and automatically fills h max when the ponded water depth pwd is less than h min or the soil water content is less than θ min daily drainage is triggered when pwd is above h p the three thresholds can be set as different values in the returning green early tillering later tillering jointing booting heading flowering milky ripening and yellow ripening stages yu and cui 2022 for fertilization control nitrogen is applied when the pwd is not lower than a depth threshold called fpwd during the period of fertilization irrigation will be triggered if pwd is lower than fpwd after the pwd reaches fpwd fertilization is triggered under ppc the rice growth simulation is the same between the improved model with the swnp module and the original oryza model under wnlc the simulation of irrigation drainage and nitrogen fertilization control is closer to actual water and management practices in the improved model compared with the original model in this study we implemented the rgsa for the improved oryza model 2 2 study areas and datasets the yrb in china was selected as the study area for rgsa of the oryza model the yrb contains two rice based cropping systems single cropping rice and double cropping rice systems single cropping rice systems are usually planted with middle rice which is usually transplanted in mid to late may or early june and harvested in mid to late september double cropping systems are planted with early rice and late rice early rice is usually transplanted in late april or early may and harvested in mid to early july late rice is transplanted after early rice is harvested and late rice is generally harvested in mid to late october in this study 41 single cropping rice system stations and 19 double cropping rice system stations were extracted from the agrometeorology station datasets on the national weather data centre of china http data cma cn and from the national irrigation experiment information service platform of china http ggsy jsgg com cn 8080 eisflex fig 1 fig 1 shows that single cropping rice systems stations are primarily located in the north of the yangtze river and the middle and upper reaches of the yrb while the double cropping rice systems stations are located in the south of the middle and lower reaches of the yrb basic information of the 41 single cropping rice system stations and 19 double cropping rice system stations are shown in table s1 see supplementary material 2 3 workflow for regional scale global sensitivity analysis the workflow for rgsa is introduced as follows 1 selecting stations for sensitivity analysis the study area is first divided into several grids with equal areas e g fig 2 for grids containing multiple stations one station is selected for analysis from each grid when there are multiple stations in a grid the station closest to the centroid of this grid is chosen 2 implementing gsa at each station within the study area 3 computing the statistical indicators for the sensitivity assessment at the regional scale based on the gsa results from the selected stations within the area the extended fourier amplitude sensitivity test efast was selected for the gsa at each station efast is a variance based method which is widely used in sensitivity analysis lu et al 2021 saltelli et al 1999 wang et al 2013 the implement of efast needs to decompose the total variance of model output firstly by equation 1 ren et al 2022 1 v y i 1 n v i i 1 n j i n v i j i 1 n j i n k j n v i j k v 1 2 n 2 v i v e y x i x i 3 v i j v e y x i x i x j x j v i v j 4 v i j k e y x i x i x j x j x k x k v i v j v k where y and v y denote the model output and its total variance respectively v i denotes the variance allocated to each parameter x i which is the variance of the conditional expectation of y given that the parameter x i has a fixed value x i v i j v i j k v 1 2 n denote the variance allocated to interactions among parameters then two sensitivity indices first order sensitivity index and total sensitivity index are computed the first order sensitivity index measures the contribution of single parameter x i to the uncertainty of output which is defined by equation 5 the total sensitivity index measures the sum of all contributions involving parameter x i to the uncertainty of output which is defined by equation 6 since the total sensitivity index takes into account of the interactions between parameter x i and the other parameters it was selected for the rgsa in this study 5 s i v i v y 6 t s i v y v i v y where s i and t s i are the first order sensitivity index and the total sensitivity index respectively v i denotes the sum of all the contributions that do not include parameter x i to the uncertainty of output s i and t s i are both in the range 0 1 low values indicate negligible effects and high values close to 1 denote more important effects vanuytrecht et al 2014 besides two statistical indicators were proposed for rgsa which are introduced as follows ① regional sensitivity score rss for gsa implemented at a single station the sensitive and insensitive parameter is usually partitioned by setting a screening threshold st sarrazin et al 2016 parameters whose sensitivity indices si are greater than st are screened as sensitive parameters and those less than st are screened as insensitive parameters when the analysis is for multiple stations at the regional scale parameters may be sensitive at some stations and not at others in this study the rss was used to quantify the sensitivity of a parameter at the regional scale equations 7 and 8 whether a parameter is sensitive or insensitive is first determined at each station if the parameter is sensitive at a specific station the sensitivity score ss of this parameter at this station is 1 otherwise it is 0 then the average ss of the parameter at each station is computed as its rss the rss is a statistical indicator that varies between 0 and 1 which indicates how many stations within an area have a parameter showing sensitive a parameter with higher rss means it appears sensitive at more stations and thus is considered to have higher sensitivity within the area the si used in equation 7 was the total sensitivity index computed by efast method in this study 7 s s i j 0 s i i j s t 1 s i i j s t 8 r s s i j n s s i j n where s s i j and s i i j are the ss and si of the ith parameter at the jth station r s s i is the rss of the ith parameter and n is the number of stations for sensitivity analysis within an area according to previous studies the input factors that have an si below 0 01 are often considered noninfluential for the variance based sa cosenza et al 2013 shin et al 2013 tang et al 2007 vanrolleghem et al 2015 therefore st used in equation 7 was set as 0 01 in this study ② regional rank index rri for parameters whose sis at all stations are greater than the st their rsss are all 1 thus the rank of sensitivity of these parameters cannot be distinguished by rss in this study the rri was proposed to quantify the rank of parameters at the regional scale equations 9 and 10 and act as a complement to the rss at each station the parameters were ranked according to their si from high to low and then the rank index ri of each parameter was computed according to equation 9 to quantify the relative sensitivity of the parameter a parameter with a higher ri at a specific station indicates that it has a higher sensitivity rank at this station then the average ri of the parameter is computed as its rri the ri is a deformation of the sensitivity ranking which can help users to analyse the relative sensitivity of parameters more intuitively 9 r i i j m r i j m 10 r r i i j n r i i j n where r i i j and r i j are the ri and sensitivity rank of the ith parameter at the jth station m is the number of parameters and r r i i is the rri of the ith parameter 2 4 experiment of regional scale global sensitivity analysis rgsa was implemented for the improved oryza model within the yzb based on the workflow explained above the input factors are the 20 parameters listed in table 1 and the model output is the simulated rice yield t ha 1 the prior distributions of model parameters are usually assumed to be uniform distribution in previous studies li and ren 2019 lu et al 2021 suárez rey et al 2019 tan et al 2016 according to helton 1993 the assumptions of uniform distribution are often adequate to determine which input factors have the greatest influence on the output variable helton 1993 and sensitivity results are generally less dependent on the actual distributions assigned to the input variables than they are on the ranges chosen for the variables helton 1993 thus the prior distributions of parameters were all assumed to be uniform distributions in this study the variation in each parameter was set as 30 perturbation of its default value according to tan et al 2016 tan et al 2017 the parameter sample size was set as 20 000 based on the knowledge of total sensitivity indices convergence the histograms of the 20 000 values of parameters are shown in fig s4 see supplementary material to investigate the effect of nssa on the results of rgsa under different wncs three levels of nssa and two wncs were considered in this study the level of nssa was determined by dividing the study area with grids of different sizes fig 2 the larger the grid is the greater the number of stations in a grid and thus the nssa of the whole area is smaller in this study the size of the grid is expressed as the product of longitude and latitude three levels of grid size were set which were l1 8 8 l2 4 4 and l3 2 2 corresponding to the three levels of nssa in addition the results obtained from selecting all the stations in the area la were compared with those from the l1 l3 levels the two wncs considered in the experiments are ppc and wnlc under the wnlc the irrigation and nitrogen fertilization modes were set the same among all the stations within yzb the irrigation mode was keeping the field continuously flooding except during the later stage of tillering and yellow ripening wu et al 2019 the nitrogen application level was set as 180 kg ha 1 n and was divided into two splits base fertilizer tf tiller fertilizer pf 5 5 the yzb contains two kinds of rice cropping systems and the double cropping rice system contains two types of rice early rice and late rice therefore the analysis was carried out for the single cropping rice systems planting middle rice scrs m double cropping rice systems planting early rice dcrs e and double cropping rice district planting late rice dcrs l within the yrb respectively the total number of stations of dcrs e and dcrs l were the same in yzb and were different from that of scrs m there are 41 stations of scrs m and the numbers of stations of l1 l2 and l3 were 5 11 and 27 respectively the number of stations of dcrs e and dcrs l is both 19 for dcrs e and dcrs l the numbers of stations of l1 l2 and l3 were 2 4 and 11 respectively the meteorological factors of each station were obtained from the national weather data centre of china http data cma cn while the soil property data were obtained from the soil information service platform of china http www soilinfo cn map index aspx the emergence and transplanting date of rice at each station were set according to the observations tan et al 2016 found that the weather types of a specific station do not affect the identification of sensitive parameters tan et al 2016 to decrease the simulation times daily meteorological factors from a year that represented the normal temperature condition of each station were extracted for rice growth simulation at each station the representative year is chosen based on the annual accumulated temperature in the rice growing season with a 50 probability of exceedance over 59 years 1959 2017 the annual accumulated temperature in the rice growing season of stations within yzb are shown in fig s1 see supplementary material and the accumulated temperature within the rice growing season in the representative year at all stations are shown in figs s2 and s3 see supplementary material 3 results at each station rice yields were simulated with the 20 000 parameter sets in the representative year the histograms of the simulated yield are shown in fig s5 s6 and s7 see supplementary material then the total sensitivity indices of parameters at each station were computed finally the rss and rri were computed based on the results of stations within yzb meanwhile the sensitivity indices distribution sid stella et al 2014 of parameters were constructed based on the sis at stations within the area 3 1 the regional sensitivity score of parameters in the oryza model fig 3 shows that for scrs m the rsss of dvrj dvri dvrp dvrr rgrlmx asla flv0 5 flv0 75 spgf and wgrmx under the two wncs did not change significantly with increasing nssa and were generally greater than 0 9 the rsss of bsla csla dsla slamax drlv1 0 drlv1 6 and drlv2 1 under the two wncs did not change with the increase in nssa but were generally less than 0 1 the rss of fst1 0 under ppc varied from 0 20 to 0 55 as the nssa increased in addition the rss of fstr decreased while that of rgrlmn increased significantly when the wnc changed from ppc to wnlc for dcrs e the rsss of dvrj dvri dvrp dvrr rgrlmx spgf and wgrmx under the two wncs did not change significantly with increasing nssa and were all equal to 1 0 the rsss of bsla csla dsla and drlv1 0 under the two wncs did not change with the increase in nssa and were generally less than 0 1 the rsss of fst1 0 and fstr under ppc changed significantly as nssa increased the rsss of asla slamax flv0 5 and flv0 75 under wnlc changed significantly with increasing nssa similar to the results of scrs m the rss of fstr decreased while that of rgrlmn increased significantly when the wnc changed from ppc to wnlc for dcrs l the rsss of dvrj dvri dvrp dvrr rgrlmx asla flv0 5 flv0 75 spgf and wgrmx under the two wncs did not change significantly with increasing nssa and were all equal to 1 0 the rsss of bsla dsla and drlv1 0 under the two wncs did not change with the increase in nssa and were generally less than 0 1 the rss of the fstr under ppc changed significantly as the nssa increased the rss of rgrlmn csla drlv1 0 and drlv1 6 under wnlc changed significantly with increasing nssa in addition the rss of the rgrlmn increased significantly when the wnc changed from ppc to wnlc for parameters with the highest and lowest rsss the changes in nssa had little effects on the value of their rsss under the two wncs however for parameters with moderate rsss their rsss changed obviously as the nssa increases because the sis of these parameters was close to st 0 01 the judgements of sensitivity of these parameters were not stable among different stations which led to obvious changes in rsss as the nssa increased in contrast the sis of parameters with higher rsss were much higher than the st while the sis of parameters with lower rsss were always less than the st which led to the judgement of sensitivity of these two kinds of parameters stable among different stations in addition fig 3 also shows that the rsss of some parameters were affected by the wncs however the effects of wncs on the rsss of these parameters did not show a consistent pattern among scrs m dcrs e and dcrs l this might be because the sensitivity of the model parameters was affected by both the wncs and climate conditions there were differences in climatic conditions at different stations see fig s1 s3 so the effects of wncs on rss were not consistent among scrs m dcrs e and dcrs l 3 2 the regional rank indices of parameters in the oryza model fig s8 s9 and s10 are the ris of some model parameters at stations within the yzb see supplementary material which showed the spatial variation of parameters sensitivity rank for each parameter the spatial distributions of ris were different between the two wncs besides regardless of the wnc the spatial distributions of ris were various among different parameters and none of the parameters showed a clear pattern of spatial variation within the area fig 4 shows the rris of parameters with higher rsss for each combination of rice cropping systems and wncs the rris of the parameters gradually stabilized at a certain level with the increase in nssa for scrs m the rris of the selected parameters under the two wncs basically reached a stable level when nssa increased to the l2 level the rris of dvrp and dvrr were much higher than those of the other parameters under ppc while the rri of dvrj was much higher than those of the other parameters under wnlc for dcrs e the rris of the parameters reached a stable level when the nssa increased to the l3 level under ppc but reached a stable level when the nssa increased to the l2 level under wnlc the rris of dvrj dvrp and rgrlmx were much higher than those of the other parameters under the ppc while the rris of dvrj and rgrlmx were much higher than those of the other parameters under the wnlc for the dcrs l the rri of the parameters under the two wnc reached a stable level when the nssa increased to the l3 level the dvrj had the highest rri among all the selected parameters under the two wnc the variations of rris of parameters among different nssa levels were attributed to the variations of sensitivity ranks of these parameters among different stations within yzb see fig s8 s9 and s10 for example the ris of dvrj under wnlc were basically greater than 0 9 among stations planting the middle rice see fig s8 b and thus lead to the rri of dvrj had little changes as the increase of nssa see fig 4 d on the contrary the ris of dvrj under ppc had specific large variation among stations planting the early rice see fig s9 a and thus the rri of dvrj fluctuated significantly with nssa increasing see fig 4 b the rris of the selected parameters were different between the two wncs indicating that wncs will affect the sensitivity ranking of model parameters the rris of dvrp and dvrr decreased while that of dvrj increased when the situation changed from ppc to wnlc this was because dvrp and dvrr determine the length of the panicle formation phase and grain filling phase respectively while dvrj affects the development stages before and after transplanting in the oryza model rice yield is simulated based on the biomass produced at the grain filling phase and is limited by a variable called the maximum total grain weight pwrr the longer the grain filling phase is the greater the amount of biomass generated at this phase and the greater the rice yield however the rice yield is not allowed to be greater than the pwrr in the model the pwrr is computed based on the spikelet number simulated at the panicle formation phase the longer the panicle formation phase is the greater the spikelet number and the greater the pwrr therefore under ppc rice yield was primarily affected by the length of the panicle formation phase and grain filling phase thus dvrp and dvrr had a more dominant role in yield simulation which was consistent with the study by tan et al 2016 tan et al 2016 however under wnlc the growth of rice is also affected by the water and nitrogen supply in this study nitrogen fertilization was applied at transplanting and approximately 10 days after transplanting the changes in dvs during this period affect the nitrogen demand and nitrogen stress suffered by rice during this period and thus affect rice yield therefore the value of dvrj had a greater effect on the yield simulation in wnlc than in ppc and thus the rri of dvrj increased 3 3 the sensitivity index distribution of parameters in the oryza model fig s11 s12 and s13 are the sis of some model parameters at stations within yzb see supplementary material for each parameter the spatial distributions of sis were also different between the two wncs and regardless of the wnc the spatial distributions of sis were different among parameters none of the parameters showed a clear pattern of spatial variation within the area fig 5 shows the sid of four parameters with higher rris for scrs m the sid range of dvrp increased as the nssa increased in ppc while those of dvrr dvrj and rgrlmx all stabilized when nssa increased to the l3 level in the same situation in the wnlc the sid range of rgrlmx increased as the nssa increased while those of the dvrj dvrp and dvrr all stabilized when the nssa increased to the l3 level for dcrs e the sid range of dvrj and dvrr both reached stability when nssa increased to the l3 level in the ppc while those of rgrlmx and dvrp did not reach stability as the nssa increased in the wnlc the sid ranges of the dvrj dvrp and dvri all stabilized when the nssa increased to the l3 level while those of the rgrlmx did not stabilize as the nssa increased for the dcrs l the four selected parameters stabilized when the nssa increased to the l3 level in the two wncs fig 5 illustrates that the sids of the parameters will change as the nssa increases in general when the nssa reaches the l3 level increasing the nssa basically has little effect on the sids of the parameters the variations of sids of parameters among different nssa levels were attributed to the variations of si of these parameters among different stations within yzb see fig s11 s12 and s13 for example the sis of rgrlmx under ppc were basically at 0 2 0 4 stations planting the early rice see fig s12 i and thus the sid range of rgrlmx had little changes as the increase of nssa see fig 5 c on the contrary the sis of dvrj under ppc had specific large variation among stations planting the early rice see fig s12 a and thus the range of sid increased significantly when nssa changed from l2 to l3 see fig 5 c 4 discussion 4 1 strategies of parameter sensitivity assessment at the regional scale while gsa has been widely implemented for crop models few studies have assessed the sensitivity of model parameters at the regional scale the objective of rgsa is to fully understand the overall sensitivity of model parameters in a specific study area the results of rgsa can help model users to identify the sensitive parameter in a specific area and guide the model calibration at multiple stations within the area on the other hand the analysis results can help users to capture the key parameters in the model and thus be a basis for studying the spatial variation of parameters and the upscale strategies of crop models according to previous studies screening and ranking are two purposes of gsa saltelli 2008 sarrazin et al 2016 screening refers to the identification of parameters that have no influence or a great influence on the model output ranking describes the ordering of the input factors according to their relative influence on the model output the two indicators rss and rri were constructed to achieve the purposes of screening and ranking at the regional scale respectively the values of rss and rri were affected by the following two factors 1 the first factor is the setting of st which affects the value of rss commonly an increase in st leads to a decrease in rss vice versa several studies consider 0 01 as the threshold to partition the sensitive and insensitive parameters for the variance based sa cosenza et al 2013 shin et al 2013 tang et al 2007 vanrolleghem et al 2015 while some studies report a screening threshold value below 0 01 nossent et al 2011 sarrazin et al 2016 given the results of previous studies it can be speculated that the accurate value of st might be model and gsa method specific however calculating the exact value of st is time consuming sarrazin et al 2016 and is far beyond the objective of this study thus the st was set as 0 01 in accordance with most of the previous studies 2 the second factor is the selection of stations for analysis since the rss and rri are the average of ss and ri at each station it was necessary to select stations from evenly space grids see fig 2 a b and c if the selection of station is uneven e g number of station in one district within the study area is large and the other is small the judgement of sensitivity may be closer to that of area with higher number of stations which might lead to the misidentification of overall sensitive parameters within the whole study area therefore the computation of rss and rri should be based on stations selected from evenly space grids fig s8 s10 illustrated that the sensitivity rank of a specific parameter was not always the same across stations within an area so do the sensitivity index fig s11 s13 the spatial variation of sensitivity rank and sensitivity index were both different among parameters and wncs thus using statistical indicators like rss and rri to quantified the overall sensitivity of stations within the area would be easier to operate and was more convenient for model calibration at multiple stations within an area based on the sensitivity analysis results although the evaluation of parameter sensitivity based on rss or rri might not be the same as the results at each station in the study area this judgement of sensitive parameters and the sensitivity ranking were suitable for most stations in the study area which could reflect the overall sensitivity of parameters among stations within the study area stations for rgsa were selected from grids having the same area within the study area in this study this was different from some previous studies where only a limited number of stations are selected for gsa based on climatic zones or climatic characteristics he et al 2015 lu et al 2021 zhao et al 2014 the station selection methods of these previous studies were often used to analyse the influence of climatic conditions on parameter sensitivity but it might not be suitable for evaluating the overall sensitivity of parameters at the regional scale because the areas of different climatic zones might vary within the study area the weight of the zone area cannot be taken into account by directly averaging the sa results from stations in each zone and the final results may not represent the overall sensitivity of the study area some studies selected the stations for gsa by dividing grids in the study area confalonieri et al 2010 stella et al 2014 for example stella et al divided the area of european countries by a 25 km 25 km grid and selected a station from the grid with the highest crop presence for gsa the station selection methods mentioned above are actually both a local analysis of stations in the area differing from the above methods the method used in this study tried to selected stations covered the whole range of the study area as much as possible so that the results of rgsa can more fully reflect the overall sensitivity of the parameter in the area however this approach may increase the nssa compared with the local analysis method therefore the number of stations needed for reasonable rgsa results was further investigated in this study the rgsa results under the four levels of nssa were compared for parameters with higher rsss their rsss were hardly affected by the nssa for parameters with moderate rss their rss changed obviously with the increase in nssa but was always much lower than that of parameters with higher rss therefore the nssa of the l1 level was sufficient for distinguishing sensitive and insensitive parameters at the regional scale fig 4 shows that the rri of most parameters remained stable when the nssa reached the l2 level the rris of a few parameters were stable only when the nssa reached the l3 level only a small number of parameters still changed significantly when the nssa changed from l3 to l4 therefore the nssa should at least reach the l2 level to obtain stable parameter sensitivity ranking results at the regional scale fig 5 shows that in most cases when the nssa reached the l3 level increasing the number of stations for analysis had little effect on the range of sid therefore the nssa should at least reach the l3 level to obtain stable sid results at the regional scale in general the rss rri and sid all have a threshold of nssa that makes them stable when the nssa reached these thresholds increasing the number of stations for analysis had little impact on the rss rri and sid according to the results of this study the nssa needed to obtain stable rss rri and sid increased sequentially this phenomenon was similar to the phenomenon that the parameter sample size needed to reach convergence of screening ranking and sensitivity indices by increasing sequentially sarrazin et al 2016 the changes of rss rri and sid were due to the spatial variation of parameters sensitivity ranks or sensitivity indices the three nssa levels all corresponded to a grid size for station selection and one station was extracted from one grid for rgsa see fig 2 when the rss rri and sid remain stable it means that the corresponded grid size of the nssa is small enough to capture the spatial variability of sensitivity therefore given the results obtained above it could be roughly determined that the size of applicable area for results of sensitive and insensitive parameters partition at a specific station was not greater than 8 8 the size of applicable area for result of parameter sensitivity ranks at a specific station was roughly not greater than 4 4 the size of applicable area for result of sensitivity indices at a specific station was roughly not greater than 2 2 the results also suggested that a suitable number of stations be chosen for analysis according to the sensitivity analysis purpose at the regional scale to reduce the number of model simulations it was not always necessary to choose a large number of stations for analysis to obtain a stable sid for the purpose of identifying the sensitive and insensitive parameters of the crop model in the region the results suggested that the grid area should be no more than 8 8 for the purpose of sensitive parameter ranking the study found that the grid area should be no more than 4 4 for the purpose of constructing the sids and analysing the variation of the sensitivity indices the results recommend that the grid area should be no more than 2 2 4 2 identification of the sensitive parameters at the regional scale the sensitive parameters of a crop model within an area can be identified based on the rss and rri parameters that influenced the model outputs could be screened out based on rss first then the ranks of the sensitive parameters were further identified by computing the rri and parameters with high sensitivity were screened out during the model calibration more attention could be devoted to the calibration of high sensitivity parameters figs 3 and 4 show that the rss and rri almost reached stable when nssa increased to l3 level and results under l3 and la level were basically the same therefore the identification of the sensitive parameters in the improved oryza model at regional scale are based on the rss and rri under la level the progress is illustrated below the sensitive parameters of the oryza model in the yrb were first screened out fig 3 illustrates that the rss of the parameters showed polarity nearly half of the parameters had rsss above 0 8 while nearly half of the parameters had rsss below 0 4 only a few parameters had rsss between 0 4 and 0 8 therefore parameters with rsss not lower than 0 8 were identified as sensitive parameters in the study area that is parameters showing sensitivity in at least 80 of stations were defined as sensitive parameters in the study area parameters that were always sensitive under ppc and wnlc for the scrs m dcrs e and dcrs l were dvrj dvri dvrp dvrr rgrlmx asla flv0 75 spgf and wgrmx flv0 5 was only insensitive under wnlc for dcrs e rgrlmn was sensitive under wnlc for scrs m dcrs e and dcrs l fstr showed sensitivity under ppc for scrs m and dcrs e twelve sensitive parameters were screened out based on the rss the ranks of these parameters were then analysed by rri fig 4 shows that the rri can generally be divided into two layers the rris of parameters in the same layer were close or overlapped each other while the rris of parameters of the two different layers were quite different the boundaries of the upper layer and lower layer were approximately 0 7 thus 0 7 was selected as the threshold for partitioning high sensitivity and low sensitivity parameters in this study based on the rri from the la level parameters with rri greater than 0 7 were identified as high sensitivity parameters under ppc high sensitivity parameters for scrs m were dvrj dvrp dvrr rgrlmx and wgrmx high sensitivity parameters in dcrde and dcrdl were dvrj dvri dvrp dvrr rgrlmx spgf and wgrmx under wnlc the high sensitivity parameters for scrs m and dcrs e were dvrj dvri dvrp dvrr and rgrlmx the high sensitivity parameters for dcrs l were dvrj dvri dvrp and rgrlmx the identification of sensitive parameters and sensitivity ranks were different between the two wnc the results suggest that the rgsa of the crop model should be implemented for different wncs among the twelve sensitive parameters dvrj dvrp and rgrlmx were always the high sensitivity parameters under the two wncs for scrs m dcrs e and dcrs l which should be paid more attention during the calibration of the oryza model in the yrb 5 conclusion this study took the yrb as the study area and assessed the sensitivity of the parameters in the oryza model for paddy rice in this area two statistical indicators rss and rri were proposed to quantify the parameter sensitivity and rank the sensitive parameters at the regional scale in addition the sid was constructed to investigate the variation of parameters sis in the area then the influence of the nssa on the rss rri and sid was analysed the results indicated that the applicable area for the results of sensitive and insensitive parameters partitioning sensitivity ranks and sensitivity indices at a specific station were not larger than grid sizes of 8 8 4 4 and 2 2 around it respectively therefore the nssa needed to obtain stable rss rri and sid increased sequentially meaning that the nssa needed for sensitive parameter identification sensitivity ranking and analysing the variation of si increased sequentially for example it was not necessary to select a large number of stations to achieve a stable sid if the objective was only to identify the sensitive parameters since wnc will affect the results of rgsa the results suggest that such analysis should be performed separately for different wncs finally based on the rss and rri the highly sensitive parameters of the oryza model for paddy rice in the yzb dvrj dvrp and rgrlmx were screened out the results above provide a clear explanation of the parameter sensitivity assessment of the crop model at the regional scale and investigate factors that affect the analysis results which can provide guidelines for the rgsa of other crop models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financially supported by the nsfc mwr ctgc joint yangtze river water science research project no u2040213 and the financial program of hubei province china 2021 218 006 001 we are grateful to the anonymous reviewers for valuable and constructive comments appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105575 appendix a abbreviations abbreviation description dcrs e double cropping rice systems planting early rice dcrs l double cropping rice systems planting late rice dvs development stage efast extended fourier amplitude sensitivity test gsa global sensitivity analysis lai leaf area index lsa local sensitivity analysis nssa number of stations for sensitivity analysis ppc potential production condition pwd ponded water depth rgsa regional scale global sensitivity analysis ri rank index rri regional rank index rss regional sensitivity score sa sensitivity analysis scrs m single cropping rice systems planting middle rice si sensitivity index sid sensitivity index distribution ss sensitivity score st screening threshold wncs water and nitrogen conditions wnlc water and nitrogen limited condition yrb yangtze river basin 
25477,while global sensitivity analysis gsa has been widely implemented for crop models few studies have assessed the sensitivity of model parameters at the regional scale to fully understand the overall sensitivity of parameters in a specific area this study carried out a regional scale global sensitivity analysis rgsa by selecting multiple stations through grids within the study area and integrated gsa results obtained by extended fourier amplitude sensitivity test efast from these stations by statistical indicators to quantify parameter sensitivity at the regional scale taking the yangtze river basin yrb as the study area rgsa was implemented for the oryza model for paddy rice under two water and nitrogen situations wncs and four levels of number of stations for sensitivity analysis nssa the results suggested that the applicable area for the results of sensitive and insensitive parameters partitioning sensitivity ranks and sensitivity indices at a specific station were not larger than grid sizes of 8 8 4 4 and 2 2 around it respectively thus the nssa needed for sensitive parameter identification sensitivity ranking and sensitivity indices variability analysis at regional scale increased sequentially it was recommended that a suitable number of stations should be chosen for analysis according to the sensitivity analysis purpose at the regional scale since wnc will affect the results of rgsa such analysis was recommended to be performed separately for different wncs highly sensitive parameters of the oryza model for paddy rice in yzb were screened out through rgsa which were dvrj dvrp and rgrlmx model users can adopt the rgsa workflow used in this study for parameter sensitivity assessment of other crop models keywords global sensitivity analysis oryza model screening ranking regional scale data availability data will be made available on request 1 introduction sensitivity analysis sa aims to characterize the impact of changes in the model input e g parameters input data and initial states etc on the model outputs sarrazin et al 2016 sa can identify parameters that greatly affect model output and thus guide model calibration model users can have better insight into a specific model by sa to improve the model sa is usually distinguished into two types local sensitivity analysis lsa and global sensitivity analysis gsa lsa focuses on the local impact of uncertain factors on model outputs and is carried out by computing partial derivatives of the output variables with respect to the input factors bert et al 2007 pogson et al 2012 wallach et al 2019 contrary to lsa gsa can consider the full domain of uncertainty of the uncertain model factors gsa is usually implemented with monte carlo sampling shapiro 2003 several sets of parameter values are first sampled and then used for model simulation then parameter sensitivity is quantified by an index computed based on model outputs compared with lsa gsa can analyse several parameters at the same time while quantifying the interactions among different parameters therefore gsa has been widely used in the study of sa of environmental models such as crop models and hydrological models liang et al 2017 sarrazin et al 2016 suárez rey et al 2019 tan et al 2021 crop models are one of the most widely used environmental models which can simulate the growth of crops under different environmental conditions and field management practices muller and martre 2019 a crop model usually consists of many equations or modules that simulate crop growth as well as the transport and transfer of water and nutrients in the field bouman et al 2001 jones et al 2003 li et al 2017 mccown et al 1996 raes 2007 a crop model also contains many parameters the values of some parameters can be obtained directly from field surveys or experiments such as parameters related to soil properties and field management meanwhile the values of some parameters need to be estimated indirectly by comparing the model outputs and their corresponding observations which is the so called calibration these parameters are mainly those related to crop cultivar ma et al 2020 tan and duan 2019 tan et al 2021 wallach et al 2011 since a crop model usually has many parameters related to crop cultivar it is time and labour intensive to calibrate all of these cultivar parameters therefore it will be helpful to screen out parameters with high sensitivity to model outputs for calibration while parameters that contribute little can be fixed saltelli et al 2019 previous studies found that parameter variation range tan et al 2017 wang et al 2013 and parameter sample size becker et al 2018 sarrazin et al 2016 wang et al 2013 will affect the results of gsa for crop models in addition the environmental conditions set in the crop model e g water condition nitrogen supply and meteorological factors etc dejonge et al 2012 liang et al 2017 zhao et al 2014 and the model structure lu et al 2021 stella et al 2014 also influence the gsa results some studies found that the sensitivity of the same parameter varies at different stations suggesting that parameter sensitivity has spatial variability he et al 2015 lu et al 2021 tan et al 2020 zhao et al 2014 therefore gsa results obtained from a specific station might not be suitable for other stations for studies at the field scale the gsa result of a single station can provide enough information for model calibration however for studies at the regional scale crop models need to be calibrated at different stations within the study area gsa results from a single station might lead to the misidentification of sensitive parameters at other stations and provide incorrect information for model calibration thus for crop model application at the regional scale it is essential to replace traditional single station analysis at the field scale with multiple stations analysis at the regional scale to obtain a result that can reflect the overall sensitivity of the model parameters within the study area and provide a more accurate guide for model calibration the gsa of the crop model in most of the existing studies is implemented at a single station level although some studies analysed parameter sensitivity at several stations the objectives of these studies were to investigate the effect of meteorological factors on gsa he et al 2015 lu et al 2021 zhao et al 2014 to the best of our knowledge few studies implement gsa at multiple stations within a specific area and assess the overall parameter sensitivity within the area to assess the parameter sensitivity of crop models at the regional scale two problems need to be solved 1 construct indicators to quantify the parameter sensitivity at the regional scale indicators used in previous studies can only be used for sensitivity quantification in single analyses at the field scale such as the first order and total order sensitivity indices computed by variance based methods saltelli et al 1999 the elementary effect computed by the method developed by morris 1991 and the standardized regression coefficient computed by the regression based method tan et al 2017 when the scale of the analysis is upgraded it is necessary to comprehensively consider the variations in parameter sensitivity among different stations within the area and construct suitable indicators to reflect the overall sensitivity of parameters in the area 2 determine the number of stations for sensitivity analysis nssa selecting too few stations may not fully reflect the actual parameter sensitivity in the area while selecting too many stations will increase the simulation times for a specific area how to select stations for analysis and specify the nssa is the second problem that needs to be solved the oryza model is a widely used crop model designed for the simulation of rice growth belder et al 2007 dossou yovo et al 2020 sudhir et al 2012 tan et al 2016 2017 and 2020 investigated the parameter sensitivity of the oryza model under different parameter variation ranges and contrasting weather types tan et al 2016 2017 2020 however few studies have taken a large scale rice cropping area as the study area and assessed the parameter sensitivity of the oryza model within the area besides comparison of the parameter sensitivity of the oryza model under different water and nitrogen conditions wncs were rarely reported in this study a multiple station gsa was implemented in the yangtze river basin yrb in china and the results from each station were integrated with statistical indicators to quantify the overall parameter sensitivity of the oryza model within the area this kind of analysis was called regional scale global sensitivity analysis rgsa in this study the main objectives of this study are 1 to propose indicators for the rgsa of crop models 2 to investigate the effect of the nssa on the rgsa under different wncs when the stations are selected from the grid in the study area and 3 to assess the parameter sensitivity of the oryza model for paddy rice in the yrb and screen out the sensitive parameters in this area 2 materials and methods this section introduces the oryza model section 2 1 and yrb section 2 2 first then the statistical indicators and workflow for rgsa are explained section 2 3 finally the experiment that carried out to explore the effect of the nssa on the rgsa under different wncs is introduced section 2 4 2 1 model description the oryza model was chosen for rgsa in this study this model can simulate the growth and development of a rice crop in situations of potential production and water and nitrogen limitations bouman et al 2001 for the potential production condition ppc the growth of rice occurs in conditions with an ample supply of water and nitrogen growth rates are determined by varietal characteristics and weather conditions solar radiation and temperature no need to set the irrigation and nitrogen fertilization modes for water and nitrogen limited condition wnlc the irrigation and nitrogen fertilization modes need to be set in the model the growth of rice is affected by the water and nitrogen supply in paddy fields the growth rate of rice might be reduction by water and nitrogen stress rice growth simulation is implemented by the oryza1 module in the model the oryza1 module can be divided into three parts 1 phenological development simulation which simulates the physiological age of rice the physiological age of rice is expressed as the development stage dvs in the model and is simulated based on radiation and temperature 2 leaf area index lai simulation lai is simulated based on daily temperature and leaf biomass 3 biomass simulation which simulates the biomass of leaves stems and storage organs of rice at different dvss the algorithms used in biomass simulation are basically the functions of dvs the simulation of biomass is also affected by solar radiation and lai in addition the biomass is simulated along with the computation of nitrogen uptake and rice yield rice growth simulation is determined by the value of cultivar parameters in the model tan et al 2016 screened 20 cultivar parameters related to rice growth tan et al 2016 these 20 parameters were used for rgsa in this study in addition a water and nitrogen balance simulation module called swnp was constructed for the oryza model to simulate the transformation and transfer of water and nitrogen in paddy fields for irrigation and drainage control a technique using three thresholds is incorporated to control the irrigation and drainage of paddy fields guo 1997 wu et al 2019 yu and cui 2022 the three thresholds include the minimum fitting threshold h min or θ min the maximum fitting depth h max and the maximum allowable depth of impoundment h p daily irrigation is triggered and automatically fills h max when the ponded water depth pwd is less than h min or the soil water content is less than θ min daily drainage is triggered when pwd is above h p the three thresholds can be set as different values in the returning green early tillering later tillering jointing booting heading flowering milky ripening and yellow ripening stages yu and cui 2022 for fertilization control nitrogen is applied when the pwd is not lower than a depth threshold called fpwd during the period of fertilization irrigation will be triggered if pwd is lower than fpwd after the pwd reaches fpwd fertilization is triggered under ppc the rice growth simulation is the same between the improved model with the swnp module and the original oryza model under wnlc the simulation of irrigation drainage and nitrogen fertilization control is closer to actual water and management practices in the improved model compared with the original model in this study we implemented the rgsa for the improved oryza model 2 2 study areas and datasets the yrb in china was selected as the study area for rgsa of the oryza model the yrb contains two rice based cropping systems single cropping rice and double cropping rice systems single cropping rice systems are usually planted with middle rice which is usually transplanted in mid to late may or early june and harvested in mid to late september double cropping systems are planted with early rice and late rice early rice is usually transplanted in late april or early may and harvested in mid to early july late rice is transplanted after early rice is harvested and late rice is generally harvested in mid to late october in this study 41 single cropping rice system stations and 19 double cropping rice system stations were extracted from the agrometeorology station datasets on the national weather data centre of china http data cma cn and from the national irrigation experiment information service platform of china http ggsy jsgg com cn 8080 eisflex fig 1 fig 1 shows that single cropping rice systems stations are primarily located in the north of the yangtze river and the middle and upper reaches of the yrb while the double cropping rice systems stations are located in the south of the middle and lower reaches of the yrb basic information of the 41 single cropping rice system stations and 19 double cropping rice system stations are shown in table s1 see supplementary material 2 3 workflow for regional scale global sensitivity analysis the workflow for rgsa is introduced as follows 1 selecting stations for sensitivity analysis the study area is first divided into several grids with equal areas e g fig 2 for grids containing multiple stations one station is selected for analysis from each grid when there are multiple stations in a grid the station closest to the centroid of this grid is chosen 2 implementing gsa at each station within the study area 3 computing the statistical indicators for the sensitivity assessment at the regional scale based on the gsa results from the selected stations within the area the extended fourier amplitude sensitivity test efast was selected for the gsa at each station efast is a variance based method which is widely used in sensitivity analysis lu et al 2021 saltelli et al 1999 wang et al 2013 the implement of efast needs to decompose the total variance of model output firstly by equation 1 ren et al 2022 1 v y i 1 n v i i 1 n j i n v i j i 1 n j i n k j n v i j k v 1 2 n 2 v i v e y x i x i 3 v i j v e y x i x i x j x j v i v j 4 v i j k e y x i x i x j x j x k x k v i v j v k where y and v y denote the model output and its total variance respectively v i denotes the variance allocated to each parameter x i which is the variance of the conditional expectation of y given that the parameter x i has a fixed value x i v i j v i j k v 1 2 n denote the variance allocated to interactions among parameters then two sensitivity indices first order sensitivity index and total sensitivity index are computed the first order sensitivity index measures the contribution of single parameter x i to the uncertainty of output which is defined by equation 5 the total sensitivity index measures the sum of all contributions involving parameter x i to the uncertainty of output which is defined by equation 6 since the total sensitivity index takes into account of the interactions between parameter x i and the other parameters it was selected for the rgsa in this study 5 s i v i v y 6 t s i v y v i v y where s i and t s i are the first order sensitivity index and the total sensitivity index respectively v i denotes the sum of all the contributions that do not include parameter x i to the uncertainty of output s i and t s i are both in the range 0 1 low values indicate negligible effects and high values close to 1 denote more important effects vanuytrecht et al 2014 besides two statistical indicators were proposed for rgsa which are introduced as follows ① regional sensitivity score rss for gsa implemented at a single station the sensitive and insensitive parameter is usually partitioned by setting a screening threshold st sarrazin et al 2016 parameters whose sensitivity indices si are greater than st are screened as sensitive parameters and those less than st are screened as insensitive parameters when the analysis is for multiple stations at the regional scale parameters may be sensitive at some stations and not at others in this study the rss was used to quantify the sensitivity of a parameter at the regional scale equations 7 and 8 whether a parameter is sensitive or insensitive is first determined at each station if the parameter is sensitive at a specific station the sensitivity score ss of this parameter at this station is 1 otherwise it is 0 then the average ss of the parameter at each station is computed as its rss the rss is a statistical indicator that varies between 0 and 1 which indicates how many stations within an area have a parameter showing sensitive a parameter with higher rss means it appears sensitive at more stations and thus is considered to have higher sensitivity within the area the si used in equation 7 was the total sensitivity index computed by efast method in this study 7 s s i j 0 s i i j s t 1 s i i j s t 8 r s s i j n s s i j n where s s i j and s i i j are the ss and si of the ith parameter at the jth station r s s i is the rss of the ith parameter and n is the number of stations for sensitivity analysis within an area according to previous studies the input factors that have an si below 0 01 are often considered noninfluential for the variance based sa cosenza et al 2013 shin et al 2013 tang et al 2007 vanrolleghem et al 2015 therefore st used in equation 7 was set as 0 01 in this study ② regional rank index rri for parameters whose sis at all stations are greater than the st their rsss are all 1 thus the rank of sensitivity of these parameters cannot be distinguished by rss in this study the rri was proposed to quantify the rank of parameters at the regional scale equations 9 and 10 and act as a complement to the rss at each station the parameters were ranked according to their si from high to low and then the rank index ri of each parameter was computed according to equation 9 to quantify the relative sensitivity of the parameter a parameter with a higher ri at a specific station indicates that it has a higher sensitivity rank at this station then the average ri of the parameter is computed as its rri the ri is a deformation of the sensitivity ranking which can help users to analyse the relative sensitivity of parameters more intuitively 9 r i i j m r i j m 10 r r i i j n r i i j n where r i i j and r i j are the ri and sensitivity rank of the ith parameter at the jth station m is the number of parameters and r r i i is the rri of the ith parameter 2 4 experiment of regional scale global sensitivity analysis rgsa was implemented for the improved oryza model within the yzb based on the workflow explained above the input factors are the 20 parameters listed in table 1 and the model output is the simulated rice yield t ha 1 the prior distributions of model parameters are usually assumed to be uniform distribution in previous studies li and ren 2019 lu et al 2021 suárez rey et al 2019 tan et al 2016 according to helton 1993 the assumptions of uniform distribution are often adequate to determine which input factors have the greatest influence on the output variable helton 1993 and sensitivity results are generally less dependent on the actual distributions assigned to the input variables than they are on the ranges chosen for the variables helton 1993 thus the prior distributions of parameters were all assumed to be uniform distributions in this study the variation in each parameter was set as 30 perturbation of its default value according to tan et al 2016 tan et al 2017 the parameter sample size was set as 20 000 based on the knowledge of total sensitivity indices convergence the histograms of the 20 000 values of parameters are shown in fig s4 see supplementary material to investigate the effect of nssa on the results of rgsa under different wncs three levels of nssa and two wncs were considered in this study the level of nssa was determined by dividing the study area with grids of different sizes fig 2 the larger the grid is the greater the number of stations in a grid and thus the nssa of the whole area is smaller in this study the size of the grid is expressed as the product of longitude and latitude three levels of grid size were set which were l1 8 8 l2 4 4 and l3 2 2 corresponding to the three levels of nssa in addition the results obtained from selecting all the stations in the area la were compared with those from the l1 l3 levels the two wncs considered in the experiments are ppc and wnlc under the wnlc the irrigation and nitrogen fertilization modes were set the same among all the stations within yzb the irrigation mode was keeping the field continuously flooding except during the later stage of tillering and yellow ripening wu et al 2019 the nitrogen application level was set as 180 kg ha 1 n and was divided into two splits base fertilizer tf tiller fertilizer pf 5 5 the yzb contains two kinds of rice cropping systems and the double cropping rice system contains two types of rice early rice and late rice therefore the analysis was carried out for the single cropping rice systems planting middle rice scrs m double cropping rice systems planting early rice dcrs e and double cropping rice district planting late rice dcrs l within the yrb respectively the total number of stations of dcrs e and dcrs l were the same in yzb and were different from that of scrs m there are 41 stations of scrs m and the numbers of stations of l1 l2 and l3 were 5 11 and 27 respectively the number of stations of dcrs e and dcrs l is both 19 for dcrs e and dcrs l the numbers of stations of l1 l2 and l3 were 2 4 and 11 respectively the meteorological factors of each station were obtained from the national weather data centre of china http data cma cn while the soil property data were obtained from the soil information service platform of china http www soilinfo cn map index aspx the emergence and transplanting date of rice at each station were set according to the observations tan et al 2016 found that the weather types of a specific station do not affect the identification of sensitive parameters tan et al 2016 to decrease the simulation times daily meteorological factors from a year that represented the normal temperature condition of each station were extracted for rice growth simulation at each station the representative year is chosen based on the annual accumulated temperature in the rice growing season with a 50 probability of exceedance over 59 years 1959 2017 the annual accumulated temperature in the rice growing season of stations within yzb are shown in fig s1 see supplementary material and the accumulated temperature within the rice growing season in the representative year at all stations are shown in figs s2 and s3 see supplementary material 3 results at each station rice yields were simulated with the 20 000 parameter sets in the representative year the histograms of the simulated yield are shown in fig s5 s6 and s7 see supplementary material then the total sensitivity indices of parameters at each station were computed finally the rss and rri were computed based on the results of stations within yzb meanwhile the sensitivity indices distribution sid stella et al 2014 of parameters were constructed based on the sis at stations within the area 3 1 the regional sensitivity score of parameters in the oryza model fig 3 shows that for scrs m the rsss of dvrj dvri dvrp dvrr rgrlmx asla flv0 5 flv0 75 spgf and wgrmx under the two wncs did not change significantly with increasing nssa and were generally greater than 0 9 the rsss of bsla csla dsla slamax drlv1 0 drlv1 6 and drlv2 1 under the two wncs did not change with the increase in nssa but were generally less than 0 1 the rss of fst1 0 under ppc varied from 0 20 to 0 55 as the nssa increased in addition the rss of fstr decreased while that of rgrlmn increased significantly when the wnc changed from ppc to wnlc for dcrs e the rsss of dvrj dvri dvrp dvrr rgrlmx spgf and wgrmx under the two wncs did not change significantly with increasing nssa and were all equal to 1 0 the rsss of bsla csla dsla and drlv1 0 under the two wncs did not change with the increase in nssa and were generally less than 0 1 the rsss of fst1 0 and fstr under ppc changed significantly as nssa increased the rsss of asla slamax flv0 5 and flv0 75 under wnlc changed significantly with increasing nssa similar to the results of scrs m the rss of fstr decreased while that of rgrlmn increased significantly when the wnc changed from ppc to wnlc for dcrs l the rsss of dvrj dvri dvrp dvrr rgrlmx asla flv0 5 flv0 75 spgf and wgrmx under the two wncs did not change significantly with increasing nssa and were all equal to 1 0 the rsss of bsla dsla and drlv1 0 under the two wncs did not change with the increase in nssa and were generally less than 0 1 the rss of the fstr under ppc changed significantly as the nssa increased the rss of rgrlmn csla drlv1 0 and drlv1 6 under wnlc changed significantly with increasing nssa in addition the rss of the rgrlmn increased significantly when the wnc changed from ppc to wnlc for parameters with the highest and lowest rsss the changes in nssa had little effects on the value of their rsss under the two wncs however for parameters with moderate rsss their rsss changed obviously as the nssa increases because the sis of these parameters was close to st 0 01 the judgements of sensitivity of these parameters were not stable among different stations which led to obvious changes in rsss as the nssa increased in contrast the sis of parameters with higher rsss were much higher than the st while the sis of parameters with lower rsss were always less than the st which led to the judgement of sensitivity of these two kinds of parameters stable among different stations in addition fig 3 also shows that the rsss of some parameters were affected by the wncs however the effects of wncs on the rsss of these parameters did not show a consistent pattern among scrs m dcrs e and dcrs l this might be because the sensitivity of the model parameters was affected by both the wncs and climate conditions there were differences in climatic conditions at different stations see fig s1 s3 so the effects of wncs on rss were not consistent among scrs m dcrs e and dcrs l 3 2 the regional rank indices of parameters in the oryza model fig s8 s9 and s10 are the ris of some model parameters at stations within the yzb see supplementary material which showed the spatial variation of parameters sensitivity rank for each parameter the spatial distributions of ris were different between the two wncs besides regardless of the wnc the spatial distributions of ris were various among different parameters and none of the parameters showed a clear pattern of spatial variation within the area fig 4 shows the rris of parameters with higher rsss for each combination of rice cropping systems and wncs the rris of the parameters gradually stabilized at a certain level with the increase in nssa for scrs m the rris of the selected parameters under the two wncs basically reached a stable level when nssa increased to the l2 level the rris of dvrp and dvrr were much higher than those of the other parameters under ppc while the rri of dvrj was much higher than those of the other parameters under wnlc for dcrs e the rris of the parameters reached a stable level when the nssa increased to the l3 level under ppc but reached a stable level when the nssa increased to the l2 level under wnlc the rris of dvrj dvrp and rgrlmx were much higher than those of the other parameters under the ppc while the rris of dvrj and rgrlmx were much higher than those of the other parameters under the wnlc for the dcrs l the rri of the parameters under the two wnc reached a stable level when the nssa increased to the l3 level the dvrj had the highest rri among all the selected parameters under the two wnc the variations of rris of parameters among different nssa levels were attributed to the variations of sensitivity ranks of these parameters among different stations within yzb see fig s8 s9 and s10 for example the ris of dvrj under wnlc were basically greater than 0 9 among stations planting the middle rice see fig s8 b and thus lead to the rri of dvrj had little changes as the increase of nssa see fig 4 d on the contrary the ris of dvrj under ppc had specific large variation among stations planting the early rice see fig s9 a and thus the rri of dvrj fluctuated significantly with nssa increasing see fig 4 b the rris of the selected parameters were different between the two wncs indicating that wncs will affect the sensitivity ranking of model parameters the rris of dvrp and dvrr decreased while that of dvrj increased when the situation changed from ppc to wnlc this was because dvrp and dvrr determine the length of the panicle formation phase and grain filling phase respectively while dvrj affects the development stages before and after transplanting in the oryza model rice yield is simulated based on the biomass produced at the grain filling phase and is limited by a variable called the maximum total grain weight pwrr the longer the grain filling phase is the greater the amount of biomass generated at this phase and the greater the rice yield however the rice yield is not allowed to be greater than the pwrr in the model the pwrr is computed based on the spikelet number simulated at the panicle formation phase the longer the panicle formation phase is the greater the spikelet number and the greater the pwrr therefore under ppc rice yield was primarily affected by the length of the panicle formation phase and grain filling phase thus dvrp and dvrr had a more dominant role in yield simulation which was consistent with the study by tan et al 2016 tan et al 2016 however under wnlc the growth of rice is also affected by the water and nitrogen supply in this study nitrogen fertilization was applied at transplanting and approximately 10 days after transplanting the changes in dvs during this period affect the nitrogen demand and nitrogen stress suffered by rice during this period and thus affect rice yield therefore the value of dvrj had a greater effect on the yield simulation in wnlc than in ppc and thus the rri of dvrj increased 3 3 the sensitivity index distribution of parameters in the oryza model fig s11 s12 and s13 are the sis of some model parameters at stations within yzb see supplementary material for each parameter the spatial distributions of sis were also different between the two wncs and regardless of the wnc the spatial distributions of sis were different among parameters none of the parameters showed a clear pattern of spatial variation within the area fig 5 shows the sid of four parameters with higher rris for scrs m the sid range of dvrp increased as the nssa increased in ppc while those of dvrr dvrj and rgrlmx all stabilized when nssa increased to the l3 level in the same situation in the wnlc the sid range of rgrlmx increased as the nssa increased while those of the dvrj dvrp and dvrr all stabilized when the nssa increased to the l3 level for dcrs e the sid range of dvrj and dvrr both reached stability when nssa increased to the l3 level in the ppc while those of rgrlmx and dvrp did not reach stability as the nssa increased in the wnlc the sid ranges of the dvrj dvrp and dvri all stabilized when the nssa increased to the l3 level while those of the rgrlmx did not stabilize as the nssa increased for the dcrs l the four selected parameters stabilized when the nssa increased to the l3 level in the two wncs fig 5 illustrates that the sids of the parameters will change as the nssa increases in general when the nssa reaches the l3 level increasing the nssa basically has little effect on the sids of the parameters the variations of sids of parameters among different nssa levels were attributed to the variations of si of these parameters among different stations within yzb see fig s11 s12 and s13 for example the sis of rgrlmx under ppc were basically at 0 2 0 4 stations planting the early rice see fig s12 i and thus the sid range of rgrlmx had little changes as the increase of nssa see fig 5 c on the contrary the sis of dvrj under ppc had specific large variation among stations planting the early rice see fig s12 a and thus the range of sid increased significantly when nssa changed from l2 to l3 see fig 5 c 4 discussion 4 1 strategies of parameter sensitivity assessment at the regional scale while gsa has been widely implemented for crop models few studies have assessed the sensitivity of model parameters at the regional scale the objective of rgsa is to fully understand the overall sensitivity of model parameters in a specific study area the results of rgsa can help model users to identify the sensitive parameter in a specific area and guide the model calibration at multiple stations within the area on the other hand the analysis results can help users to capture the key parameters in the model and thus be a basis for studying the spatial variation of parameters and the upscale strategies of crop models according to previous studies screening and ranking are two purposes of gsa saltelli 2008 sarrazin et al 2016 screening refers to the identification of parameters that have no influence or a great influence on the model output ranking describes the ordering of the input factors according to their relative influence on the model output the two indicators rss and rri were constructed to achieve the purposes of screening and ranking at the regional scale respectively the values of rss and rri were affected by the following two factors 1 the first factor is the setting of st which affects the value of rss commonly an increase in st leads to a decrease in rss vice versa several studies consider 0 01 as the threshold to partition the sensitive and insensitive parameters for the variance based sa cosenza et al 2013 shin et al 2013 tang et al 2007 vanrolleghem et al 2015 while some studies report a screening threshold value below 0 01 nossent et al 2011 sarrazin et al 2016 given the results of previous studies it can be speculated that the accurate value of st might be model and gsa method specific however calculating the exact value of st is time consuming sarrazin et al 2016 and is far beyond the objective of this study thus the st was set as 0 01 in accordance with most of the previous studies 2 the second factor is the selection of stations for analysis since the rss and rri are the average of ss and ri at each station it was necessary to select stations from evenly space grids see fig 2 a b and c if the selection of station is uneven e g number of station in one district within the study area is large and the other is small the judgement of sensitivity may be closer to that of area with higher number of stations which might lead to the misidentification of overall sensitive parameters within the whole study area therefore the computation of rss and rri should be based on stations selected from evenly space grids fig s8 s10 illustrated that the sensitivity rank of a specific parameter was not always the same across stations within an area so do the sensitivity index fig s11 s13 the spatial variation of sensitivity rank and sensitivity index were both different among parameters and wncs thus using statistical indicators like rss and rri to quantified the overall sensitivity of stations within the area would be easier to operate and was more convenient for model calibration at multiple stations within an area based on the sensitivity analysis results although the evaluation of parameter sensitivity based on rss or rri might not be the same as the results at each station in the study area this judgement of sensitive parameters and the sensitivity ranking were suitable for most stations in the study area which could reflect the overall sensitivity of parameters among stations within the study area stations for rgsa were selected from grids having the same area within the study area in this study this was different from some previous studies where only a limited number of stations are selected for gsa based on climatic zones or climatic characteristics he et al 2015 lu et al 2021 zhao et al 2014 the station selection methods of these previous studies were often used to analyse the influence of climatic conditions on parameter sensitivity but it might not be suitable for evaluating the overall sensitivity of parameters at the regional scale because the areas of different climatic zones might vary within the study area the weight of the zone area cannot be taken into account by directly averaging the sa results from stations in each zone and the final results may not represent the overall sensitivity of the study area some studies selected the stations for gsa by dividing grids in the study area confalonieri et al 2010 stella et al 2014 for example stella et al divided the area of european countries by a 25 km 25 km grid and selected a station from the grid with the highest crop presence for gsa the station selection methods mentioned above are actually both a local analysis of stations in the area differing from the above methods the method used in this study tried to selected stations covered the whole range of the study area as much as possible so that the results of rgsa can more fully reflect the overall sensitivity of the parameter in the area however this approach may increase the nssa compared with the local analysis method therefore the number of stations needed for reasonable rgsa results was further investigated in this study the rgsa results under the four levels of nssa were compared for parameters with higher rsss their rsss were hardly affected by the nssa for parameters with moderate rss their rss changed obviously with the increase in nssa but was always much lower than that of parameters with higher rss therefore the nssa of the l1 level was sufficient for distinguishing sensitive and insensitive parameters at the regional scale fig 4 shows that the rri of most parameters remained stable when the nssa reached the l2 level the rris of a few parameters were stable only when the nssa reached the l3 level only a small number of parameters still changed significantly when the nssa changed from l3 to l4 therefore the nssa should at least reach the l2 level to obtain stable parameter sensitivity ranking results at the regional scale fig 5 shows that in most cases when the nssa reached the l3 level increasing the number of stations for analysis had little effect on the range of sid therefore the nssa should at least reach the l3 level to obtain stable sid results at the regional scale in general the rss rri and sid all have a threshold of nssa that makes them stable when the nssa reached these thresholds increasing the number of stations for analysis had little impact on the rss rri and sid according to the results of this study the nssa needed to obtain stable rss rri and sid increased sequentially this phenomenon was similar to the phenomenon that the parameter sample size needed to reach convergence of screening ranking and sensitivity indices by increasing sequentially sarrazin et al 2016 the changes of rss rri and sid were due to the spatial variation of parameters sensitivity ranks or sensitivity indices the three nssa levels all corresponded to a grid size for station selection and one station was extracted from one grid for rgsa see fig 2 when the rss rri and sid remain stable it means that the corresponded grid size of the nssa is small enough to capture the spatial variability of sensitivity therefore given the results obtained above it could be roughly determined that the size of applicable area for results of sensitive and insensitive parameters partition at a specific station was not greater than 8 8 the size of applicable area for result of parameter sensitivity ranks at a specific station was roughly not greater than 4 4 the size of applicable area for result of sensitivity indices at a specific station was roughly not greater than 2 2 the results also suggested that a suitable number of stations be chosen for analysis according to the sensitivity analysis purpose at the regional scale to reduce the number of model simulations it was not always necessary to choose a large number of stations for analysis to obtain a stable sid for the purpose of identifying the sensitive and insensitive parameters of the crop model in the region the results suggested that the grid area should be no more than 8 8 for the purpose of sensitive parameter ranking the study found that the grid area should be no more than 4 4 for the purpose of constructing the sids and analysing the variation of the sensitivity indices the results recommend that the grid area should be no more than 2 2 4 2 identification of the sensitive parameters at the regional scale the sensitive parameters of a crop model within an area can be identified based on the rss and rri parameters that influenced the model outputs could be screened out based on rss first then the ranks of the sensitive parameters were further identified by computing the rri and parameters with high sensitivity were screened out during the model calibration more attention could be devoted to the calibration of high sensitivity parameters figs 3 and 4 show that the rss and rri almost reached stable when nssa increased to l3 level and results under l3 and la level were basically the same therefore the identification of the sensitive parameters in the improved oryza model at regional scale are based on the rss and rri under la level the progress is illustrated below the sensitive parameters of the oryza model in the yrb were first screened out fig 3 illustrates that the rss of the parameters showed polarity nearly half of the parameters had rsss above 0 8 while nearly half of the parameters had rsss below 0 4 only a few parameters had rsss between 0 4 and 0 8 therefore parameters with rsss not lower than 0 8 were identified as sensitive parameters in the study area that is parameters showing sensitivity in at least 80 of stations were defined as sensitive parameters in the study area parameters that were always sensitive under ppc and wnlc for the scrs m dcrs e and dcrs l were dvrj dvri dvrp dvrr rgrlmx asla flv0 75 spgf and wgrmx flv0 5 was only insensitive under wnlc for dcrs e rgrlmn was sensitive under wnlc for scrs m dcrs e and dcrs l fstr showed sensitivity under ppc for scrs m and dcrs e twelve sensitive parameters were screened out based on the rss the ranks of these parameters were then analysed by rri fig 4 shows that the rri can generally be divided into two layers the rris of parameters in the same layer were close or overlapped each other while the rris of parameters of the two different layers were quite different the boundaries of the upper layer and lower layer were approximately 0 7 thus 0 7 was selected as the threshold for partitioning high sensitivity and low sensitivity parameters in this study based on the rri from the la level parameters with rri greater than 0 7 were identified as high sensitivity parameters under ppc high sensitivity parameters for scrs m were dvrj dvrp dvrr rgrlmx and wgrmx high sensitivity parameters in dcrde and dcrdl were dvrj dvri dvrp dvrr rgrlmx spgf and wgrmx under wnlc the high sensitivity parameters for scrs m and dcrs e were dvrj dvri dvrp dvrr and rgrlmx the high sensitivity parameters for dcrs l were dvrj dvri dvrp and rgrlmx the identification of sensitive parameters and sensitivity ranks were different between the two wnc the results suggest that the rgsa of the crop model should be implemented for different wncs among the twelve sensitive parameters dvrj dvrp and rgrlmx were always the high sensitivity parameters under the two wncs for scrs m dcrs e and dcrs l which should be paid more attention during the calibration of the oryza model in the yrb 5 conclusion this study took the yrb as the study area and assessed the sensitivity of the parameters in the oryza model for paddy rice in this area two statistical indicators rss and rri were proposed to quantify the parameter sensitivity and rank the sensitive parameters at the regional scale in addition the sid was constructed to investigate the variation of parameters sis in the area then the influence of the nssa on the rss rri and sid was analysed the results indicated that the applicable area for the results of sensitive and insensitive parameters partitioning sensitivity ranks and sensitivity indices at a specific station were not larger than grid sizes of 8 8 4 4 and 2 2 around it respectively therefore the nssa needed to obtain stable rss rri and sid increased sequentially meaning that the nssa needed for sensitive parameter identification sensitivity ranking and analysing the variation of si increased sequentially for example it was not necessary to select a large number of stations to achieve a stable sid if the objective was only to identify the sensitive parameters since wnc will affect the results of rgsa the results suggest that such analysis should be performed separately for different wncs finally based on the rss and rri the highly sensitive parameters of the oryza model for paddy rice in the yzb dvrj dvrp and rgrlmx were screened out the results above provide a clear explanation of the parameter sensitivity assessment of the crop model at the regional scale and investigate factors that affect the analysis results which can provide guidelines for the rgsa of other crop models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financially supported by the nsfc mwr ctgc joint yangtze river water science research project no u2040213 and the financial program of hubei province china 2021 218 006 001 we are grateful to the anonymous reviewers for valuable and constructive comments appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105575 appendix a abbreviations abbreviation description dcrs e double cropping rice systems planting early rice dcrs l double cropping rice systems planting late rice dvs development stage efast extended fourier amplitude sensitivity test gsa global sensitivity analysis lai leaf area index lsa local sensitivity analysis nssa number of stations for sensitivity analysis ppc potential production condition pwd ponded water depth rgsa regional scale global sensitivity analysis ri rank index rri regional rank index rss regional sensitivity score sa sensitivity analysis scrs m single cropping rice systems planting middle rice si sensitivity index sid sensitivity index distribution ss sensitivity score st screening threshold wncs water and nitrogen conditions wnlc water and nitrogen limited condition yrb yangtze river basin 
25478,landslide susceptibility maps lsms are among the essential layers for landslide risk management in landslide prone regions this study proposes raster based landslide susceptibility mapping using widely used compensatory multi attribute decision making madm methods including ahp saw ahp topsis and ahp vikor in the razmian region iran the results showed that the range of area under the curve auc values varied from 0 793 to 0 833 therefore all lsms derived from the compensatory madm methods are within the acceptable and excellent acceptance range so the accuracy of madm methods is acceptable for topsis and vikor v 0 0 and excellent for other methods validation results also showed that vikor0 7 is the best method to produce the lsm based on the quality sum qs index and auc values in general pixel based landslide susceptibility mapping using compensatory madm methods in combination with the group decision making approach in ahp is recommended for landslide risk management and land use planning graphical abstract image 1 keywords compensatory madm methods consistency ratio decision matrix group decision making pixel based geo processing data availability data will be made available on request 1 introduction landslides are one of the natural geohazards in the mountainous regions of iran which cause significant human and financial annual losses talaei 2014 azarafza et al 2018 eslami et al 2019 bordbar et al 2022 this phenomenon has also occurred in many parts of the world and in addition to human and financial losses has led to land degradation and environmental damages roy et al 2019 li et al 2022 sim et al 2022 therefore landslide risk management is essential to reduce these damages dai et al 2002 cheung 2021 in this regard landslide susceptibility map lsms is important for landslide risk management and land use planning maes et al 2018 shano et al 2022 this map shows the hazard levels of landslide occurrence in the form of hazard zones in the study area castellanos abella and van westen 2008 ahmad et al 2022 in general landslide susceptibility mapping is based on considering causal factors and landslide inventory map selecting and assessing lsm production methods and finally selecting the best method huang and zhao 2018 wubalem 2021 trinh et al 2022 these factors can be classified into main groups including meteorological hydrological topographic geological and natural anthropogenic components kayastha et al 2013 zhang et al 2017 balogun et al 2022 the literature review shows that slope angle and aspect precipitation distance to fault road and stream land use altitude and lithology are widely used causal factors to produce lsms pradhan and lee 2010 pourghasemi et al 2013 chen et al 2018 lv et al 2022 zhang et al 2022 also lsms can be produced by quantitative or qualitative methods guzzetti et al 1999 however choosing suitable methods and data plays a crucial role in the reliability and accuracy of lsms trinh et al 2022 since the qualitative methods of landslide susceptibility mapping depend on the experience and expertise of experts quantitative methods have been strongly developed aleotti and chowdhury 1999 these methods are divided into four main groups called statistical deterministic heuristic and landslide inventory based probabilistic techniques akgun 2012 in this regard the hybrid multi attribute decision making madm methods used in this study are classified into the heuristic group tomashevskii and tomashevskii 2019 these methods have been widely used to map the hazard susceptibility of some phenomena such as landslides el jazouli et al 2019 salehpour jam et al 2021a floods arabameri et al 2019b akay 2021 vojtek et al 2021 land degradation and desertification salehpour jam and karimpour reihan 2016 sadhasivam et al 2020 deforestation bera et al 2021 earthquake jena and pradhan 2020 jena et al 2020 and drought palchaudhuri and biswas 2016 abdullah et al 2021 in this study the most widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor were used to produce lsms the analytic hierarchy process ahp is one of the compensatory madm methods proposed by saaty 1980 it is also a widely used method to produce lsms that has been used 1 alone yalcin et al 2011 kayastha et al 2013 chen et al 2016 2 in combination with other madm methods roodposhti et al 2014 salehpour jam et al 2021a or 3 in combination with non madm methods mondal and maiti 2013 rabby and li 2020 also the simple additive weighting saw the technique for order of preference by similarity to ideal solution topsis and the viekriterijumsko kompromisno rangiranje vikor methods are other compensatory madm methods that were proposed by fishburn 1967 hwang and yoon 1981 and opricovic 1998 respectively these methods have been used in combination with ahp as hybrid methods for the production of lsms salehpour jam et al 2021a these methods have also been used to prepare multi hazard maps mirzaei et al 2018 sheikh et al 2019 in general gis based madm methods have a raster based approach in their computational process to produce hazard susceptibility maps of flood landslide land degradation earthquake and other hazards yalcin 2008 mahmoody vanolya and jelokhani niaraki 2021 mostafa mousavi et al 2022 these methods also require alternatives and relevant values to create a decision matrix in their decision making process tzeng and huang 2011 the literature review indicates that the alternatives have been used in 4 types for calculations of madm techniques 1 cell pixel based considering each cell of causal factor as an alternative feizizadeh et al 2014 roodposhti et al 2014 liu et al 2022 2 fishnet or grid based creating a fishnet of rectangular cells so that each of its cells serves as an alternative for madm techniques accordingly the average values of the cells of each causal factor in each of the cells of this grid participate in the calculations salehpour jam et al 2021a 3 reclassification creating homogeneous units as alternatives for madm techniques by classifying layers of causal factors and their intersections kayastha et al 2013 meena et al 2019 balogun et al 2022 and 4 homogeneous unit creating homogeneous units based on the intersection of primary classified layers as alternatives for madm techniques therefore the values of the causal factors are calculated for each of them karimpour reyhan et al 2007 salehpour jam et al 2017 this study aimed to evaluate the performance of widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor to produce lsm in the razmian region qazvin province iran also the performance of the vikor method was assessed based on the different weights for the strategy of the majority of criteria or vs 0 0 0 1 1 0 in previous studies the simultaneous evaluation of the widely used hybrid compensatory madm methods has been less considered in pixel based geo processings to produce lsms also in the vikor method the v coefficient varies from zero to one which is usually recommended to be 0 5 opricovic and tzeng 2004 tzeng and huang 2011 in this regard in some previous studies the use of the proposed value of v 0 5 instead of using different values of v has led to the introduction of the vikor method as a less efficient or undesirable method in some studies of producing hazard susceptibility maps arabameri et al 2019b akay 2021 in other words this default value may not be the best value to use in the vikor method therefore in this study finding the best value of the weight for the strategy of the majority of criteria is examined and assessed to produce lsms in this study an assessment of the efficiency of the methods was conceptualized in six different steps including 1 preparation of causal layers 2 landslide inventory mapping 3 normalization of indicators using the min max normalization method for the saw method and the vector normalization method for the topsis and vikor methods 4 calculating the weights of the indicators using the group decision making method in ahp 5 raster based calculation of the indices including saw vikor and topsis and finally 6 assessment of the efficiency of the methods using the verification methods including the quality sum qs index and the receiver operating characteristic roc curves 2 materials and methods 2 1 study area razmian region is a part of the shahroud river basin in the north of qazvin province and covers an area of 1023 3 square kilometers it is located between 49 59 40 to 50 30 30 east longitude and 36 20 57 to 36 41 3 north latitude and includes 87 villages based on data provided by the statistics center of iran the villages of yaroud rajaei dasht hir akujan and the shahrestan e sofla have the highest population in this region respectively this mountainous region has a minimum and maximum altitude of 740 and 3888 m respectively fig 1 the mean annual rainfall of the region is 521 mm fig 3 there are different types of land uses including rangeland dry farming forest bare land and irrigated farming in this area rangeland and bare land have a maximum and minimum area of 72546 1 and 781 05 ha respectively also most of the irrigated lands are extended in the vicinity of the hydrological network of the region especially the shahroud river fig 4 2 2 methodology the most widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor were used to produce lsms based on the raster based process in this study this process was conceptualized in six different steps fig 2 2 2 1 preparation of causal layers in this study causal factors were classified into two categories 1 causal factors with primary values the initial values of these thematic maps were used directly to create lsms using madm methods this group includes rainfall map slope angle and slope aspect maps and distance maps 2 causal factors with secondary values these are thematic reclassified maps that participate in lsm production with secondary values instead of initial values this group includes the lithological altitude and land use maps in general these thematic layers and the pixels of these raster layers were used as attributes and alternatives to compensatory madm methods respectively 2 2 1 1 slope angle and slope aspect maps these layers which have been widely used in landslide susceptibility studies katz et al 2014 chen et al 2018 salehpour jam et al 2021a play an important role in slope stability analysis zakaria et al 2018 in this regard slope angle and slope aspect maps were created separately using arcgis 10 8 software and the dem layer prepared by the national cartographic center of iran fig 3 the spatial resolution of the dem layer is 10 m 2 2 1 2 rainfall map this layer is one of the main layers for lsm production pourghasemi et al 2013 roodposhti et al 2014 roy et al 2019 the mean annual rainfall layer was created using the dem layer and the related gradient equation by the raster calculator of arcgis 10 8 software fig 3 the rainfall gradient was created during the period 1985 to 2019 using the mean annual rainfall statistical data of 12 meteorological stations after reconstructing the relevant statistical defects equation 1 these data are related to the meteorological stations of the meteorological organization and the ministry of energy 1 y 0 21 x 146 26 where x is the elevation in meters and y is the average annual rainfall in millimeters 2 2 1 3 distance maps distance maps including distance to road distance to fault and distance to stream play an important role in landslide susceptibility studies yalcin et al 2011 ahmed 2015 zhu et al 2019 lv et al 2022 all distance maps were created using the euclidean distance tool by arcgis 10 8 software for the study area the maximum calculated distance to the road fault and stream are 4395 5 3781 3 and 3861 8 m respectively fig 3 in this study the road and stream layers prepared by the national cartographic center of iran and the fault layer prepared by the geological survey and mineral exploration of iran were used 2 2 1 4 lithological map a lithological map is a keymap for lsm production yalcin et al 2011 chen et al 2018 pourghasemi et al 2021 in this study geological maps of the geological survey and mineral explorations of iran with a map scale of 1 100 000 were used in this regard 4 geological sheets covering the study area including qazvin javaherdeh jirandeh and ramsar were merged and digitized using arcgis 10 5 tools qazvin and javaherdeh geological sheets cover most of the study area finally a classification map of landslide susceptibility of rock units based on the score obtained from the classification performed by peyrowan and shariat jafari 2013 was created this schematic reclassified map has values from 1 to 5 which indicates the susceptibility to landslide occurrence from very low to very high respectively fig 4 2 2 1 5 altitude and land use these maps are among the most widely used maps for lsm production pradhan et al 2010 pham et al 2016 zhou et al 2018 zhang et al 2022 in this study the land use layer prepared by the natural resources and watershed management organization of iran was used there are a variety of land uses in this area including bare land dry farming low density forest irrigated farming and rangeland in this regard bare land and rangeland with an area of 781 0 and 72546 1 ha have the minimum and maximum area respectively also agriculture is one of the anthropogenic land use activities in the region so irrigated farming and dry farming have an area of 7437 5 and 13383 9 ha respectively in this study 7 altitude classes including 500 1000 1000 1500 1500 2000 2000 2500 2500 3000 3000 3500 and 3500 4000 based on the minimum and maximum height of the area were considered fig 4 this reclassified map was created using arcgis 10 8 software based on the dem layer prepared by the national cartographic center of iran the spatial resolution of the dem layer is 10 m it is necessary to consider these classes due to the relationship between precipitation and altitude and the presence of frost in the region pourghasemi et al 2013 ozdemir 2020 salehpour jam et al 2021a in this study new values were assigned to different altitude classes and types of land uses based on weights obtained from the group decision making method in ahp in this regard after filling in the pairwise comparison questionnaires of the criteria by 16 experts the consistency ratio cr of judgments was calculated using equation 2 based on the consistency index ci and the random consistency index ri saaty 2012 also the geometric mean was used to aggregate the judgments in a comparison matrix saaty and peniwati 2013 when the cr value is less than or equal to 0 1 the consistency of the c r c i r i judgments is acceptable saaty 2012 2 c r c i r i c i λ max n n 1 the ci value was calculated based on equation 3 saaty 2012 the ri value was also obtained based on the number of criteria from table 1 saaty 2000 3 c i λ max n n 1 where λmax is the largest eigenvalue and n is the number of criteria 2 2 2 landslide inventory map in the present study rotational landslides were considered in the study area also the area and location of landslides are two features considered to assess the accuracy of models and select the best model using the verification methods in this study in this regard the landslide distribution map was prepared using the landslide map created by the natural resources and watershed management organization nrwmo of iran the interpretation of aerial photos 1 20 000 scale of the national cartographic center ncc of iran landsat copernicus maxar technologies and cnes airbus images in the historical imagery of google earth and finally an investigation of geologic map prepared by the geological survey and mineral exploration of iran 1 100 000 scale and field surveys this map is presented in fig 1 2 2 3 compensatory madm methods in this study the most widely used compensatory madm methods including ahp saw ahp topsis and ahp vikor were used to produce lsms based on the principles of madm methods these methods need a decision matrix in their decision making process this matrix contains the values of indicators for the various alternatives specified to produce lsms in this study the pixels or cells of these layers were considered as alternatives for madm methods 2 2 3 1 normalization of indicators normalization of indicators is necessary to compare their values based on the dimensionless nature of the normalized data papathanasiou and ploskas 2018a in this study two normalization methods including the min max normalization method for the saw method equations 4 and 5 and the vector normalization method for the topsis and vikor methods equations 6 and 7 were used to scale the data in the range of 0 1 4 p i j x i j min x i j max x i j min x i j 5 n i j max x i j x i j max x i j min x i j 6 p i j x i j i 1 m x i j 2 7 n i j 1 x i j i 1 m x i j 2 where pij is the normalized values for positive indicators nij is the normalized values for negative indicators and xij is the value of each indicator for each alternative 2 2 3 2 calculating the weights of the indicators in this study the group decision making method in ahp was used to determine the weights of indicators the analytic hierarchy process with the participation of 16 experts is described in section 2 2 1 2 2 2 2 3 3 calculating the saw index the saw index is a weighted normalized index to prioritize alternatives based on indicators equation 8 this index is an integrated index calculated by the weighted mean method using normalized values obtained from the min max normalization method and weights of indicators obtained from the ahp method papathanasiou and ploskas 2018b 8 v i j i 1 n w j x i j where vij is the saw index wj is the weight of the jth indicator and xij is the normalized value of the jth indicator for ith alternative 2 2 3 4 calculating the topsis index to calculate the topsis index first the positive a and negative a ideal solutions were calculated based on equations 9 and 10 respectively hwang and yoon 1981 9 a v 1 v 2 v n max i v i j j s b min i v i j j s c 10 a v 1 v 2 v n min i v i j j s b max i v i j j s c where sb and sc denote the set of benefit criteria and set of cost criteria respectively then the euclidean distance of each alternative from the positive di and negative di ideal solutions was calculated equations 11 and 12 finally the topsis index ri was calculated as a relative closeness value equation 13 11 d i j 1 n v i j v j 2 i i 12 d i j 1 n v i j v j 2 i i 13 r i d i d i d i f o r i i 2 2 3 5 calculating the vikor index to calculate the vikor index first the positive fi and negative fi ideal solutions were determined based on equations 14 and 15 respectively opricovic 1998 14 f i max f i j j 1 2 m 15 f i min f i j j 1 2 m where fij is the value of the desired alternative for each indicator in the weighted standardized decision matrix then the utility values sj and the regret values rj of the indicators were calculated based on equations 16 and 17 16 s j i 1 n w i f i f i j f i f i 17 r j max w i f i f i j f i f i finally the vikor index qj was calculated equation 18 18 q j v s j s s s 1 v r j r r r where s max sj s min sj r max rj r min rj and v is the weight for the strategy of the majority of criteria and varies from 0 to 1 2 2 3 6 performance assessment in this study the verification methods including the quality sum qs index and the receiver operating characteristic roc curves were used to assess the accuracy of compensatory madm methods to produce lsms in this regard the dr and qs indices are presented in equations 19 and 20 respectively gee 1992 19 d r s i a i i n s i i n a i 20 q s 1 n d r 1 2 s where dr is density ratio qs is quality sum index si is the total area of landslides in each hazard zone ai is the area of each hazard zone in the landslide susceptibility map and s is the ratio of landslide area in each hazard zone to total area the area under the curve auc values for different rocs were also calculated by ibm spss statistics 22 after plotting the true positive rate tpr against the false positive rate fpr metz 1978 pradhan 2011 tien bui et al 2020 3 results the causal factors affecting the landslide occurrence are presented as schematic layers in figs 3 and 4 the pairwise comparison matrix and the cr values obtained from the ahp method are presented in table 2 in this regard the cr values obtained from group decision making by ahp for the decision matrices related to types of land use altitude classes and causal factors are equal to 0 088 0 059 and 0 054 respectively given that the cr values are less than 0 1 the judgments are consistent and therefore the weights obtained from the ahp method are acceptable saaty 2012 the weights of the indicators are presented in table 2 the results show that irrigated farming and dry farming are respectively the most important land uses that cause landslide occurrence the results also show that among the 9 selected indicators for the production of lsms indicators of lithology and distance to road have the maximum and minimum weight from the perspective of experts also indicators of lithology slope degree land use and slope aspect have the first to fourth importance respectively table 2 the utility and regret values of the indicators and the relative closeness to the positive and negative ideal solutions are presented in figs 5 and 6 respectively also the limit values of di di sj and rj are shown in figs 5 and 6 lsms produced by compensatory madm methods are presented in figs 7 and 8 in this regard in addition to lsms produced by saw and topsis methods 11 lsms were produced using the vikor method based on different values of v 0 0 0 1 1 0 the range of all indices varies from zero to one also a higher value of each index means more susceptibility to landslide occurrence roc curves and auc values related to the compensatory madm methods are presented in figs 9 and 10 the results showed that the range of auc values varies from 0 793 to 0 833 in this regard topsis and vikor v 0 7 methods have the minimum and maximum auc values respectively also the range of auc values calculated to assess the performance of the vikor method varies from 0 795 to 0 833 based on different values of v so that vs of 0 0 and 0 7 have the minimum and maximum values fig 10 based on the acceptance limits determined by hosmer et al 2013 the ranges of auc values from 0 5 to 0 7 0 7 to 0 8 0 8 to 0 9 and equal to or more than 0 9 represent poor acceptable excellent and outstanding discriminations respectively therefore all lsms derived from the compensatory madm methods are within the acceptable and excellent acceptance range so the accuracy of madm methods is acceptable for topsis and vikor v 0 0 and excellent for other methods accordingly the vikor method v 0 7 is the best method to produce the lsm in this study the qs and dr indices were also used to compare the accuracy of the methods these indices are among the most widely used indices to determine the accuracy of methods eslami et al 2019 roy et al 2019 rabby et al 2020 the dr values of different classes of susceptibility to landslide obtained from compensatory madm methods are presented in fig 11 also the qs index values and the related computational parameters are presented in table 3 and fig 12 validation results show that the range of qs values varies from 0 632 to 0 820 table 3 fig 12 accordingly the vikor method v 0 7 is the best method for producing lsm among the madm methods also the dr values of different landslide susceptibility classes produced by the vikor method v 0 7 have an uptrend from very low class i to very high class v landslide susceptibility zones fig 11 this uptrend reflects the good ability of the model to create lsm pradhan and lee 2010 in general the lsm produced by the vikor method v 0 7 shows that 65839 34 ha 64 34 of the area have very low to low hazard potential 34732 94 ha 33 94 of the area have moderate hazard potential and 1753 86 ha 1 71 of the area have high to very high hazard potential table 3 4 discussion in this study the most widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor were used to produce lsms based on the raster based process consistent with the results of this study factors of lithology slope degree land use and slope aspect have been introduced as the most important factors for the production of lsms yalcin et al 2011 pourghasemi et al 2012 bahrami et al 2020 salehpour jam et al 2021a given that indicators and alternatives create the decision matrix table in madm methods papathanasiou and ploskas 2018a initiatives to create alternatives that are used in the decision making process play an important role in using these methods to prepare lsms in this regard various initiatives have been used in previous studies 1 overlaying and crossing thematic layers to create alternatives and extract values from causal layers karimpour reyhan et al 2007 2 using a grid layer with specific cell size as spatial alternatives to extract values from causal layers salehpour jam et al 2021a and 3 reclassification of causal layers based on ahp weights and aggregating them pourghasemi et al 2013 in this study the pixels of the causal layers were considered as alternatives to the compensatory madm methods accordingly all the formulas of these methods were applied in arcgis 10 8 software using the raster calculator and cell and zonal statistics tools in this regard raster based calculations were performed on 10232600 pixels as alternatives for madm methods assessment of compensatory madm methods used to produce lsms using the landslide inventory map is important from two aspects 1 comparing the accuracy of methods and choosing the best method and 2 the possibility of using different coefficients in methods such as vikor and selecting the best coefficient in each method in the second case the lack of a suitable indicator to assess methods that have coefficients with a range of zero to one such as vikor v 0 0 0 1 1 0 has led to consider a default value for this coefficient the vikor method is a widely used madm method to create hazard maps arabameri et al 2019a 2019b ghaleno et al 2020 akay 2021 malakar and rai 2022 in the vikor method the v coefficient or the weight for the strategy of the majority of criteria varies from zero to one which is usually recommended to be 0 5 opricovic and tzeng 2004 tzeng and huang 2011 accordingly the vikor method based on v0 5 v 0 5 may not be selected as the best method in the validation stage hoseinzade et al 2021 salehpour jam et al 2021a in this regard the use of the proposed value of v 0 5 instead of using different values of v has led to the introduction of the vikor method as a less efficient or undesirable method in some studies of producing hazard susceptibility maps arabameri et al 2019b akay 2021 therefore in this study due to the possibility of validation of madm methods using auc and qs methods the vikor method was evaluated based on different vs 0 0 0 1 1 0 in this study all lsms derived from the compensatory madm methods are within the acceptable and excellent acceptance range so the accuracy of madm methods is acceptable for topsis and vikor v 0 0 and excellent for other methods accordingly the vikor method v 0 7 is the best method to produce the lsm among the madm methods consistent with these results salehpour jam et al 2021a showed that the vikor model has excellent accuracy to produce lsms in the alamut watershed in tehran province iran based on the auc validation index also the vikor method has been introduced as a good method to create other hazard susceptibility maps khosravi et al 2019 sari 2021 ma et al 2022 in addition the saw method also called the integrated index method iim salehpour jam et al 2021a has been introduced as a simple and efficient method for producing lsms eskandari et al 2016 hadi et al 2018 ashournejad et al 2019 ercanoglu et al 2021 also although the saw method is a simple additive weighting method based on standardizing decision matrix data determining the weights of indicators and aggregating indicators based on the weighted average method papathanasiou and ploskas 2018a it is widely used in environmental assessments johnston et al 2013 abrams et al 2018 mosaffaie et al 2021 salehpour jam et al 2021b in this study the ahp method was used to determine the weights of indicators in combination with other compensatory madm methods as hybrid models although the ahp method is a widely used method for determining the weights of indicators kayastha et al 2013 myronidis et al 2016 devara et al 2021 chanu and bakimchandra 2022 determining the weights based on the expert judgment or opinion is a disadvantage for this method nefeslioglu et al 2013 pant et al 2022 accordingly the use of group decision making in the analytical hierarchy process has been strongly recommended saaty 2012 based on lsm produced by the vikor method v 0 7 36486 79 ha of the area have moderate to very high hazard potential table 3 therefore landslide risk management is strongly recommended for this region in general the production of lsms and determining hazard zones are crucial for landslide risk management and land use planning fell et al 2008 oliveira et al 2017 lópez et al 2021 also it is recommended to pay attention to both reactive and proactive approaches based on causal analysis for landslide risk management salehpour jam et al 2021c 5 conclusion this study proposes raster based landslide susceptibility mapping using widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor also different values of the weight for the strategy of the majority of criteria or vs 0 0 0 1 1 0 in the vikor method were used and assessed to produce lsms the results showed that all lsms derived from the compensatory madm methods are within the acceptable and excellent acceptance range so the accuracy of madm methods is acceptable for topsis and vikor v 0 0 and excellent for other methods also the vikor method v 0 7 is the best method to produce the lsm this indicates that the default value of the v coefficient suggested for the vikor method v 0 5 is not the best value for this coefficient therefore the production of lsms based on different values of the v coefficient and their assessment is strongly recommended also based on the presence of moderate to very high landslide hazard zones in the study area landslide risk management and attention to the proactive and reactive approaches to improve the state and reduce the relevant damages are recommended in this study considering cells related to the layers of causal factors as alternatives for madm methods made it possible to perform pixel based geo processings also producing lsms based on causal factors and mathematical formulas in arcgis software are other strengths of these methods however in hybrid compensatory madm methods determining the weights of causal factors and their importance strongly depends on the expert view accordingly group decision making in the ahp to determine the weights of causal layers is strongly recommended declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study has been conducted as a research project with code 04 29 29 008 970158 in the soil conservation and watershed management research institute scwmri agricultural research education and extension organization areeo tehran iran we are thankful for the financial support of scwmri and the natural resources and watershed management administration of qazvin province the authors are also grateful to the anonymous referees for their valuable suggestions and comments 
25478,landslide susceptibility maps lsms are among the essential layers for landslide risk management in landslide prone regions this study proposes raster based landslide susceptibility mapping using widely used compensatory multi attribute decision making madm methods including ahp saw ahp topsis and ahp vikor in the razmian region iran the results showed that the range of area under the curve auc values varied from 0 793 to 0 833 therefore all lsms derived from the compensatory madm methods are within the acceptable and excellent acceptance range so the accuracy of madm methods is acceptable for topsis and vikor v 0 0 and excellent for other methods validation results also showed that vikor0 7 is the best method to produce the lsm based on the quality sum qs index and auc values in general pixel based landslide susceptibility mapping using compensatory madm methods in combination with the group decision making approach in ahp is recommended for landslide risk management and land use planning graphical abstract image 1 keywords compensatory madm methods consistency ratio decision matrix group decision making pixel based geo processing data availability data will be made available on request 1 introduction landslides are one of the natural geohazards in the mountainous regions of iran which cause significant human and financial annual losses talaei 2014 azarafza et al 2018 eslami et al 2019 bordbar et al 2022 this phenomenon has also occurred in many parts of the world and in addition to human and financial losses has led to land degradation and environmental damages roy et al 2019 li et al 2022 sim et al 2022 therefore landslide risk management is essential to reduce these damages dai et al 2002 cheung 2021 in this regard landslide susceptibility map lsms is important for landslide risk management and land use planning maes et al 2018 shano et al 2022 this map shows the hazard levels of landslide occurrence in the form of hazard zones in the study area castellanos abella and van westen 2008 ahmad et al 2022 in general landslide susceptibility mapping is based on considering causal factors and landslide inventory map selecting and assessing lsm production methods and finally selecting the best method huang and zhao 2018 wubalem 2021 trinh et al 2022 these factors can be classified into main groups including meteorological hydrological topographic geological and natural anthropogenic components kayastha et al 2013 zhang et al 2017 balogun et al 2022 the literature review shows that slope angle and aspect precipitation distance to fault road and stream land use altitude and lithology are widely used causal factors to produce lsms pradhan and lee 2010 pourghasemi et al 2013 chen et al 2018 lv et al 2022 zhang et al 2022 also lsms can be produced by quantitative or qualitative methods guzzetti et al 1999 however choosing suitable methods and data plays a crucial role in the reliability and accuracy of lsms trinh et al 2022 since the qualitative methods of landslide susceptibility mapping depend on the experience and expertise of experts quantitative methods have been strongly developed aleotti and chowdhury 1999 these methods are divided into four main groups called statistical deterministic heuristic and landslide inventory based probabilistic techniques akgun 2012 in this regard the hybrid multi attribute decision making madm methods used in this study are classified into the heuristic group tomashevskii and tomashevskii 2019 these methods have been widely used to map the hazard susceptibility of some phenomena such as landslides el jazouli et al 2019 salehpour jam et al 2021a floods arabameri et al 2019b akay 2021 vojtek et al 2021 land degradation and desertification salehpour jam and karimpour reihan 2016 sadhasivam et al 2020 deforestation bera et al 2021 earthquake jena and pradhan 2020 jena et al 2020 and drought palchaudhuri and biswas 2016 abdullah et al 2021 in this study the most widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor were used to produce lsms the analytic hierarchy process ahp is one of the compensatory madm methods proposed by saaty 1980 it is also a widely used method to produce lsms that has been used 1 alone yalcin et al 2011 kayastha et al 2013 chen et al 2016 2 in combination with other madm methods roodposhti et al 2014 salehpour jam et al 2021a or 3 in combination with non madm methods mondal and maiti 2013 rabby and li 2020 also the simple additive weighting saw the technique for order of preference by similarity to ideal solution topsis and the viekriterijumsko kompromisno rangiranje vikor methods are other compensatory madm methods that were proposed by fishburn 1967 hwang and yoon 1981 and opricovic 1998 respectively these methods have been used in combination with ahp as hybrid methods for the production of lsms salehpour jam et al 2021a these methods have also been used to prepare multi hazard maps mirzaei et al 2018 sheikh et al 2019 in general gis based madm methods have a raster based approach in their computational process to produce hazard susceptibility maps of flood landslide land degradation earthquake and other hazards yalcin 2008 mahmoody vanolya and jelokhani niaraki 2021 mostafa mousavi et al 2022 these methods also require alternatives and relevant values to create a decision matrix in their decision making process tzeng and huang 2011 the literature review indicates that the alternatives have been used in 4 types for calculations of madm techniques 1 cell pixel based considering each cell of causal factor as an alternative feizizadeh et al 2014 roodposhti et al 2014 liu et al 2022 2 fishnet or grid based creating a fishnet of rectangular cells so that each of its cells serves as an alternative for madm techniques accordingly the average values of the cells of each causal factor in each of the cells of this grid participate in the calculations salehpour jam et al 2021a 3 reclassification creating homogeneous units as alternatives for madm techniques by classifying layers of causal factors and their intersections kayastha et al 2013 meena et al 2019 balogun et al 2022 and 4 homogeneous unit creating homogeneous units based on the intersection of primary classified layers as alternatives for madm techniques therefore the values of the causal factors are calculated for each of them karimpour reyhan et al 2007 salehpour jam et al 2017 this study aimed to evaluate the performance of widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor to produce lsm in the razmian region qazvin province iran also the performance of the vikor method was assessed based on the different weights for the strategy of the majority of criteria or vs 0 0 0 1 1 0 in previous studies the simultaneous evaluation of the widely used hybrid compensatory madm methods has been less considered in pixel based geo processings to produce lsms also in the vikor method the v coefficient varies from zero to one which is usually recommended to be 0 5 opricovic and tzeng 2004 tzeng and huang 2011 in this regard in some previous studies the use of the proposed value of v 0 5 instead of using different values of v has led to the introduction of the vikor method as a less efficient or undesirable method in some studies of producing hazard susceptibility maps arabameri et al 2019b akay 2021 in other words this default value may not be the best value to use in the vikor method therefore in this study finding the best value of the weight for the strategy of the majority of criteria is examined and assessed to produce lsms in this study an assessment of the efficiency of the methods was conceptualized in six different steps including 1 preparation of causal layers 2 landslide inventory mapping 3 normalization of indicators using the min max normalization method for the saw method and the vector normalization method for the topsis and vikor methods 4 calculating the weights of the indicators using the group decision making method in ahp 5 raster based calculation of the indices including saw vikor and topsis and finally 6 assessment of the efficiency of the methods using the verification methods including the quality sum qs index and the receiver operating characteristic roc curves 2 materials and methods 2 1 study area razmian region is a part of the shahroud river basin in the north of qazvin province and covers an area of 1023 3 square kilometers it is located between 49 59 40 to 50 30 30 east longitude and 36 20 57 to 36 41 3 north latitude and includes 87 villages based on data provided by the statistics center of iran the villages of yaroud rajaei dasht hir akujan and the shahrestan e sofla have the highest population in this region respectively this mountainous region has a minimum and maximum altitude of 740 and 3888 m respectively fig 1 the mean annual rainfall of the region is 521 mm fig 3 there are different types of land uses including rangeland dry farming forest bare land and irrigated farming in this area rangeland and bare land have a maximum and minimum area of 72546 1 and 781 05 ha respectively also most of the irrigated lands are extended in the vicinity of the hydrological network of the region especially the shahroud river fig 4 2 2 methodology the most widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor were used to produce lsms based on the raster based process in this study this process was conceptualized in six different steps fig 2 2 2 1 preparation of causal layers in this study causal factors were classified into two categories 1 causal factors with primary values the initial values of these thematic maps were used directly to create lsms using madm methods this group includes rainfall map slope angle and slope aspect maps and distance maps 2 causal factors with secondary values these are thematic reclassified maps that participate in lsm production with secondary values instead of initial values this group includes the lithological altitude and land use maps in general these thematic layers and the pixels of these raster layers were used as attributes and alternatives to compensatory madm methods respectively 2 2 1 1 slope angle and slope aspect maps these layers which have been widely used in landslide susceptibility studies katz et al 2014 chen et al 2018 salehpour jam et al 2021a play an important role in slope stability analysis zakaria et al 2018 in this regard slope angle and slope aspect maps were created separately using arcgis 10 8 software and the dem layer prepared by the national cartographic center of iran fig 3 the spatial resolution of the dem layer is 10 m 2 2 1 2 rainfall map this layer is one of the main layers for lsm production pourghasemi et al 2013 roodposhti et al 2014 roy et al 2019 the mean annual rainfall layer was created using the dem layer and the related gradient equation by the raster calculator of arcgis 10 8 software fig 3 the rainfall gradient was created during the period 1985 to 2019 using the mean annual rainfall statistical data of 12 meteorological stations after reconstructing the relevant statistical defects equation 1 these data are related to the meteorological stations of the meteorological organization and the ministry of energy 1 y 0 21 x 146 26 where x is the elevation in meters and y is the average annual rainfall in millimeters 2 2 1 3 distance maps distance maps including distance to road distance to fault and distance to stream play an important role in landslide susceptibility studies yalcin et al 2011 ahmed 2015 zhu et al 2019 lv et al 2022 all distance maps were created using the euclidean distance tool by arcgis 10 8 software for the study area the maximum calculated distance to the road fault and stream are 4395 5 3781 3 and 3861 8 m respectively fig 3 in this study the road and stream layers prepared by the national cartographic center of iran and the fault layer prepared by the geological survey and mineral exploration of iran were used 2 2 1 4 lithological map a lithological map is a keymap for lsm production yalcin et al 2011 chen et al 2018 pourghasemi et al 2021 in this study geological maps of the geological survey and mineral explorations of iran with a map scale of 1 100 000 were used in this regard 4 geological sheets covering the study area including qazvin javaherdeh jirandeh and ramsar were merged and digitized using arcgis 10 5 tools qazvin and javaherdeh geological sheets cover most of the study area finally a classification map of landslide susceptibility of rock units based on the score obtained from the classification performed by peyrowan and shariat jafari 2013 was created this schematic reclassified map has values from 1 to 5 which indicates the susceptibility to landslide occurrence from very low to very high respectively fig 4 2 2 1 5 altitude and land use these maps are among the most widely used maps for lsm production pradhan et al 2010 pham et al 2016 zhou et al 2018 zhang et al 2022 in this study the land use layer prepared by the natural resources and watershed management organization of iran was used there are a variety of land uses in this area including bare land dry farming low density forest irrigated farming and rangeland in this regard bare land and rangeland with an area of 781 0 and 72546 1 ha have the minimum and maximum area respectively also agriculture is one of the anthropogenic land use activities in the region so irrigated farming and dry farming have an area of 7437 5 and 13383 9 ha respectively in this study 7 altitude classes including 500 1000 1000 1500 1500 2000 2000 2500 2500 3000 3000 3500 and 3500 4000 based on the minimum and maximum height of the area were considered fig 4 this reclassified map was created using arcgis 10 8 software based on the dem layer prepared by the national cartographic center of iran the spatial resolution of the dem layer is 10 m it is necessary to consider these classes due to the relationship between precipitation and altitude and the presence of frost in the region pourghasemi et al 2013 ozdemir 2020 salehpour jam et al 2021a in this study new values were assigned to different altitude classes and types of land uses based on weights obtained from the group decision making method in ahp in this regard after filling in the pairwise comparison questionnaires of the criteria by 16 experts the consistency ratio cr of judgments was calculated using equation 2 based on the consistency index ci and the random consistency index ri saaty 2012 also the geometric mean was used to aggregate the judgments in a comparison matrix saaty and peniwati 2013 when the cr value is less than or equal to 0 1 the consistency of the c r c i r i judgments is acceptable saaty 2012 2 c r c i r i c i λ max n n 1 the ci value was calculated based on equation 3 saaty 2012 the ri value was also obtained based on the number of criteria from table 1 saaty 2000 3 c i λ max n n 1 where λmax is the largest eigenvalue and n is the number of criteria 2 2 2 landslide inventory map in the present study rotational landslides were considered in the study area also the area and location of landslides are two features considered to assess the accuracy of models and select the best model using the verification methods in this study in this regard the landslide distribution map was prepared using the landslide map created by the natural resources and watershed management organization nrwmo of iran the interpretation of aerial photos 1 20 000 scale of the national cartographic center ncc of iran landsat copernicus maxar technologies and cnes airbus images in the historical imagery of google earth and finally an investigation of geologic map prepared by the geological survey and mineral exploration of iran 1 100 000 scale and field surveys this map is presented in fig 1 2 2 3 compensatory madm methods in this study the most widely used compensatory madm methods including ahp saw ahp topsis and ahp vikor were used to produce lsms based on the principles of madm methods these methods need a decision matrix in their decision making process this matrix contains the values of indicators for the various alternatives specified to produce lsms in this study the pixels or cells of these layers were considered as alternatives for madm methods 2 2 3 1 normalization of indicators normalization of indicators is necessary to compare their values based on the dimensionless nature of the normalized data papathanasiou and ploskas 2018a in this study two normalization methods including the min max normalization method for the saw method equations 4 and 5 and the vector normalization method for the topsis and vikor methods equations 6 and 7 were used to scale the data in the range of 0 1 4 p i j x i j min x i j max x i j min x i j 5 n i j max x i j x i j max x i j min x i j 6 p i j x i j i 1 m x i j 2 7 n i j 1 x i j i 1 m x i j 2 where pij is the normalized values for positive indicators nij is the normalized values for negative indicators and xij is the value of each indicator for each alternative 2 2 3 2 calculating the weights of the indicators in this study the group decision making method in ahp was used to determine the weights of indicators the analytic hierarchy process with the participation of 16 experts is described in section 2 2 1 2 2 2 2 3 3 calculating the saw index the saw index is a weighted normalized index to prioritize alternatives based on indicators equation 8 this index is an integrated index calculated by the weighted mean method using normalized values obtained from the min max normalization method and weights of indicators obtained from the ahp method papathanasiou and ploskas 2018b 8 v i j i 1 n w j x i j where vij is the saw index wj is the weight of the jth indicator and xij is the normalized value of the jth indicator for ith alternative 2 2 3 4 calculating the topsis index to calculate the topsis index first the positive a and negative a ideal solutions were calculated based on equations 9 and 10 respectively hwang and yoon 1981 9 a v 1 v 2 v n max i v i j j s b min i v i j j s c 10 a v 1 v 2 v n min i v i j j s b max i v i j j s c where sb and sc denote the set of benefit criteria and set of cost criteria respectively then the euclidean distance of each alternative from the positive di and negative di ideal solutions was calculated equations 11 and 12 finally the topsis index ri was calculated as a relative closeness value equation 13 11 d i j 1 n v i j v j 2 i i 12 d i j 1 n v i j v j 2 i i 13 r i d i d i d i f o r i i 2 2 3 5 calculating the vikor index to calculate the vikor index first the positive fi and negative fi ideal solutions were determined based on equations 14 and 15 respectively opricovic 1998 14 f i max f i j j 1 2 m 15 f i min f i j j 1 2 m where fij is the value of the desired alternative for each indicator in the weighted standardized decision matrix then the utility values sj and the regret values rj of the indicators were calculated based on equations 16 and 17 16 s j i 1 n w i f i f i j f i f i 17 r j max w i f i f i j f i f i finally the vikor index qj was calculated equation 18 18 q j v s j s s s 1 v r j r r r where s max sj s min sj r max rj r min rj and v is the weight for the strategy of the majority of criteria and varies from 0 to 1 2 2 3 6 performance assessment in this study the verification methods including the quality sum qs index and the receiver operating characteristic roc curves were used to assess the accuracy of compensatory madm methods to produce lsms in this regard the dr and qs indices are presented in equations 19 and 20 respectively gee 1992 19 d r s i a i i n s i i n a i 20 q s 1 n d r 1 2 s where dr is density ratio qs is quality sum index si is the total area of landslides in each hazard zone ai is the area of each hazard zone in the landslide susceptibility map and s is the ratio of landslide area in each hazard zone to total area the area under the curve auc values for different rocs were also calculated by ibm spss statistics 22 after plotting the true positive rate tpr against the false positive rate fpr metz 1978 pradhan 2011 tien bui et al 2020 3 results the causal factors affecting the landslide occurrence are presented as schematic layers in figs 3 and 4 the pairwise comparison matrix and the cr values obtained from the ahp method are presented in table 2 in this regard the cr values obtained from group decision making by ahp for the decision matrices related to types of land use altitude classes and causal factors are equal to 0 088 0 059 and 0 054 respectively given that the cr values are less than 0 1 the judgments are consistent and therefore the weights obtained from the ahp method are acceptable saaty 2012 the weights of the indicators are presented in table 2 the results show that irrigated farming and dry farming are respectively the most important land uses that cause landslide occurrence the results also show that among the 9 selected indicators for the production of lsms indicators of lithology and distance to road have the maximum and minimum weight from the perspective of experts also indicators of lithology slope degree land use and slope aspect have the first to fourth importance respectively table 2 the utility and regret values of the indicators and the relative closeness to the positive and negative ideal solutions are presented in figs 5 and 6 respectively also the limit values of di di sj and rj are shown in figs 5 and 6 lsms produced by compensatory madm methods are presented in figs 7 and 8 in this regard in addition to lsms produced by saw and topsis methods 11 lsms were produced using the vikor method based on different values of v 0 0 0 1 1 0 the range of all indices varies from zero to one also a higher value of each index means more susceptibility to landslide occurrence roc curves and auc values related to the compensatory madm methods are presented in figs 9 and 10 the results showed that the range of auc values varies from 0 793 to 0 833 in this regard topsis and vikor v 0 7 methods have the minimum and maximum auc values respectively also the range of auc values calculated to assess the performance of the vikor method varies from 0 795 to 0 833 based on different values of v so that vs of 0 0 and 0 7 have the minimum and maximum values fig 10 based on the acceptance limits determined by hosmer et al 2013 the ranges of auc values from 0 5 to 0 7 0 7 to 0 8 0 8 to 0 9 and equal to or more than 0 9 represent poor acceptable excellent and outstanding discriminations respectively therefore all lsms derived from the compensatory madm methods are within the acceptable and excellent acceptance range so the accuracy of madm methods is acceptable for topsis and vikor v 0 0 and excellent for other methods accordingly the vikor method v 0 7 is the best method to produce the lsm in this study the qs and dr indices were also used to compare the accuracy of the methods these indices are among the most widely used indices to determine the accuracy of methods eslami et al 2019 roy et al 2019 rabby et al 2020 the dr values of different classes of susceptibility to landslide obtained from compensatory madm methods are presented in fig 11 also the qs index values and the related computational parameters are presented in table 3 and fig 12 validation results show that the range of qs values varies from 0 632 to 0 820 table 3 fig 12 accordingly the vikor method v 0 7 is the best method for producing lsm among the madm methods also the dr values of different landslide susceptibility classes produced by the vikor method v 0 7 have an uptrend from very low class i to very high class v landslide susceptibility zones fig 11 this uptrend reflects the good ability of the model to create lsm pradhan and lee 2010 in general the lsm produced by the vikor method v 0 7 shows that 65839 34 ha 64 34 of the area have very low to low hazard potential 34732 94 ha 33 94 of the area have moderate hazard potential and 1753 86 ha 1 71 of the area have high to very high hazard potential table 3 4 discussion in this study the most widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor were used to produce lsms based on the raster based process consistent with the results of this study factors of lithology slope degree land use and slope aspect have been introduced as the most important factors for the production of lsms yalcin et al 2011 pourghasemi et al 2012 bahrami et al 2020 salehpour jam et al 2021a given that indicators and alternatives create the decision matrix table in madm methods papathanasiou and ploskas 2018a initiatives to create alternatives that are used in the decision making process play an important role in using these methods to prepare lsms in this regard various initiatives have been used in previous studies 1 overlaying and crossing thematic layers to create alternatives and extract values from causal layers karimpour reyhan et al 2007 2 using a grid layer with specific cell size as spatial alternatives to extract values from causal layers salehpour jam et al 2021a and 3 reclassification of causal layers based on ahp weights and aggregating them pourghasemi et al 2013 in this study the pixels of the causal layers were considered as alternatives to the compensatory madm methods accordingly all the formulas of these methods were applied in arcgis 10 8 software using the raster calculator and cell and zonal statistics tools in this regard raster based calculations were performed on 10232600 pixels as alternatives for madm methods assessment of compensatory madm methods used to produce lsms using the landslide inventory map is important from two aspects 1 comparing the accuracy of methods and choosing the best method and 2 the possibility of using different coefficients in methods such as vikor and selecting the best coefficient in each method in the second case the lack of a suitable indicator to assess methods that have coefficients with a range of zero to one such as vikor v 0 0 0 1 1 0 has led to consider a default value for this coefficient the vikor method is a widely used madm method to create hazard maps arabameri et al 2019a 2019b ghaleno et al 2020 akay 2021 malakar and rai 2022 in the vikor method the v coefficient or the weight for the strategy of the majority of criteria varies from zero to one which is usually recommended to be 0 5 opricovic and tzeng 2004 tzeng and huang 2011 accordingly the vikor method based on v0 5 v 0 5 may not be selected as the best method in the validation stage hoseinzade et al 2021 salehpour jam et al 2021a in this regard the use of the proposed value of v 0 5 instead of using different values of v has led to the introduction of the vikor method as a less efficient or undesirable method in some studies of producing hazard susceptibility maps arabameri et al 2019b akay 2021 therefore in this study due to the possibility of validation of madm methods using auc and qs methods the vikor method was evaluated based on different vs 0 0 0 1 1 0 in this study all lsms derived from the compensatory madm methods are within the acceptable and excellent acceptance range so the accuracy of madm methods is acceptable for topsis and vikor v 0 0 and excellent for other methods accordingly the vikor method v 0 7 is the best method to produce the lsm among the madm methods consistent with these results salehpour jam et al 2021a showed that the vikor model has excellent accuracy to produce lsms in the alamut watershed in tehran province iran based on the auc validation index also the vikor method has been introduced as a good method to create other hazard susceptibility maps khosravi et al 2019 sari 2021 ma et al 2022 in addition the saw method also called the integrated index method iim salehpour jam et al 2021a has been introduced as a simple and efficient method for producing lsms eskandari et al 2016 hadi et al 2018 ashournejad et al 2019 ercanoglu et al 2021 also although the saw method is a simple additive weighting method based on standardizing decision matrix data determining the weights of indicators and aggregating indicators based on the weighted average method papathanasiou and ploskas 2018a it is widely used in environmental assessments johnston et al 2013 abrams et al 2018 mosaffaie et al 2021 salehpour jam et al 2021b in this study the ahp method was used to determine the weights of indicators in combination with other compensatory madm methods as hybrid models although the ahp method is a widely used method for determining the weights of indicators kayastha et al 2013 myronidis et al 2016 devara et al 2021 chanu and bakimchandra 2022 determining the weights based on the expert judgment or opinion is a disadvantage for this method nefeslioglu et al 2013 pant et al 2022 accordingly the use of group decision making in the analytical hierarchy process has been strongly recommended saaty 2012 based on lsm produced by the vikor method v 0 7 36486 79 ha of the area have moderate to very high hazard potential table 3 therefore landslide risk management is strongly recommended for this region in general the production of lsms and determining hazard zones are crucial for landslide risk management and land use planning fell et al 2008 oliveira et al 2017 lópez et al 2021 also it is recommended to pay attention to both reactive and proactive approaches based on causal analysis for landslide risk management salehpour jam et al 2021c 5 conclusion this study proposes raster based landslide susceptibility mapping using widely used hybrid compensatory madm methods including ahp saw ahp topsis and ahp vikor also different values of the weight for the strategy of the majority of criteria or vs 0 0 0 1 1 0 in the vikor method were used and assessed to produce lsms the results showed that all lsms derived from the compensatory madm methods are within the acceptable and excellent acceptance range so the accuracy of madm methods is acceptable for topsis and vikor v 0 0 and excellent for other methods also the vikor method v 0 7 is the best method to produce the lsm this indicates that the default value of the v coefficient suggested for the vikor method v 0 5 is not the best value for this coefficient therefore the production of lsms based on different values of the v coefficient and their assessment is strongly recommended also based on the presence of moderate to very high landslide hazard zones in the study area landslide risk management and attention to the proactive and reactive approaches to improve the state and reduce the relevant damages are recommended in this study considering cells related to the layers of causal factors as alternatives for madm methods made it possible to perform pixel based geo processings also producing lsms based on causal factors and mathematical formulas in arcgis software are other strengths of these methods however in hybrid compensatory madm methods determining the weights of causal factors and their importance strongly depends on the expert view accordingly group decision making in the ahp to determine the weights of causal layers is strongly recommended declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study has been conducted as a research project with code 04 29 29 008 970158 in the soil conservation and watershed management research institute scwmri agricultural research education and extension organization areeo tehran iran we are thankful for the financial support of scwmri and the natural resources and watershed management administration of qazvin province the authors are also grateful to the anonymous referees for their valuable suggestions and comments 
25479,in rivers coastal seas and transitional waters sediment transport processes involve a variety of interacting factors that dynamically vary over time and space the flow dynamics within these highly heterogeneous natural systems influences the spatial patterns of erosion and deposition of the bed sediments which in turn shapes and conditions the bottom morphology by taking advantage of the established modelling framework in both two and three dimensions for unstructured meshes proposed within the telemac mascaret system the new module gaia provides a code structure for solving sediment transport and morphological evolution problems by a clear treatment of sedimentary processes that happen in the water column in the bed structure and at the water bed interface gaia efficiently manages the spatial and temporal variability of sediment size classes properties and transport modes for two and three dimensions in addition this module can easily be expanded and customised to particular requirements by modifying user friendly easy to read and well documented fortran 90 subroutines keywords telemac mascaret sediment transport morphodynamics hydraulics numerical modelling hydrodynamics data availability data will be made available on request 1 introduction in rivers coastal seas and transitional estuaries lagoons etc waters sediment transport processes involve a variety of interacting factors that dynamically vary over time and space according to the spatial and temporal scale being considered the flow dynamics within these highly heterogeneous natural systems influences both sedimentary processes and spatial patterns of erosion and deposition of the bed sediments which in turn shapes and conditions the environment for instance over long timescales if forcing conditions and sediment supply remain relatively stable the morphology of the system will ultimately reach a quasi equilibrium however over shorter scales this balance can be altered by both natural causes and human pressures masselink et al 2011 rhoads 2020 consequently understanding and predicting the sediment dynamics and related hydromorphological changes is important for evaluating the modification of sedimentary features or for assessing the departure from naturalness of the body of water moreover these adjustments can also be intrinsically related with biological e g bacteria vegetation benthic communities or chemical processes e g pollutants polymers among others larsen et al 2021 siviglia and crosato 2016 before the 1970s solutions to most engineering problems involving sediment transport processes were determined through field investigations and or physical scale models in laboratory wu 2007 from the early 70s advancements in computer technology and methods for the solution of scientific and engineering problems propelled the application of mechanistic models to predict sediment transport processes mathematical tools describing flow sediment transport and morphological change processes in a water body consist of a set of partial differential algebraic equations pdes completed with suitable initial and boundary conditions and closure relationships analytical or closed form solutions of these mechanistic models for the prediction of sediment transport processes provide a quantitative means of rapidly describing systematic trends associated with the phenomenon nevertheless they cannot be expected to provide accurate solutions to problems involving for example complex geometries time varying boundary conditions etc to overcome these limitations numerical solutions are often the most efficient and practical methods for predicting sediment transport in complex natural systems james et al 2010 thanks to the effort of several research institutes from both the industry and academia a number of computational models are currently available and widely applicable to practical cases involving sediment transport processes these models have been developed for different spatial dimensions flow states sediment size classes and transport modes wu 2007 their solution relies on the numerical approximation used for the solution of the pdes where according to the numerical method and nature of sub spaces used in the approximation the given problem is reduced to an algebraic problem having finite dimension which can be solved from a selection of convenient algorithms quarteroni and valli 2009 a coupling strategy based on the flow field is generally adopted this procedure usually includes defining an updating strategy of the flow and sediment transport variables the numerical bed updating schemes and morphological acceleration techniques roelvink 2011 the numerical simulation of sediment transport processes in river systems has increased dramatically over the past several decades kaveh et al 2019 physical processes observed in morphological structures commonly found in alluvial systems such as bar units bifurcations etc rhoads 2020 can be efficiently captured with standard computational resources open source software packages like delft3d lesser et al 2004 iric shimizu et al 2020 basement vanzo et al 2021 hec ras 2d sediment usace 2022 srh 2d lai and gaeuman 2020 and telemac mascaret villaret et al 2013 among others are able to capture the dynamics of river systems at different space and time scales many of the numerical tools developed for coastal and transitional waters derive from methods applied for the solution of river processes roelvink 2011 according to their dimensionality these models resolve variations in flow and transport along the horizontal directions for two dimensional models and along the horizontal and vertical directions for three dimensional models such models can be applied to both small scale and macro scale problems for different types of flow and wave models roelvink 2011 typical examples of computational models currently available to solve coastal and transitional water problems are adcirc mirabito et al 2011 coawst warner et al 2010 mars3d grasso et al 2018 roms sherwood et al 2018 and xbeach roelvink et al 2009 the telemac mascaret modelling system offers an established modelling framework in both two 2d and three dimensions 3d for unstructured meshes it is an integrated set of open source modular fortran 90 subroutines which provide the software architecture necessary for the numerical solution of the governing equations namely the data structure the algebraic operations and the building and solving phases nheili et al 2016 simulations can be launched in both serial mode or on multiple processors using distributed memory architecture mpi it also benefits from a python wrapper and a fortran api application program interface goeury et al 2017 allowing full control of a running simulation this software interface can be used for implementing efficient calibration algorithms based on data assimilation poncot et al 2017 and to perform sensitivity analysis and uncertainty quantification mouradi et al 2016 pre post processing and analysis tools are available through a set of python scripts included in the system s distribution audouin et al 2019 and by means of third party software such as bluekenue paraview and others in light of this the sediment transport and bed evolution module sisyphe of the telemac mascaret modelling system has been developed for more than 25 years latteux and tanguy 1990 originally based on the finite element structure used to solve the shallow water equations despite its robustness flexibility and capability of dealing with a large number of river cordier et al 2019 dutta et al 2017 mendoza et al 2017 coastal brown and davies 2009 robins et al 2014 van den eynde et al 2010 and estuarine santoro et al 2019 2017 giardino et al 2009 sediment transport and morphodynamics problems villaret et al 2013 a number of issues arose regarding the improvement of the treatment of graded and mixed cohesive and non cohesive sediments as well as the full compatibility between 2d and 3d processes gaia building upon the sisyphe module is able to model complex sediment and morphodynamic processes in coastal areas rivers lakes and estuaries accounting for spatial and temporal variability of sediment size classes uniform graded or mixed properties cohesive and non cohesive and transport modes suspended bedload or both processes simultaneously the generalised framework used for bed layering enables any combination of multiple size classes for both non cohesive and cohesive sediment to be modelled simultaneously compatibility is ensured between an active layer model an approach traditionally adopted for non cohesive sediment and the presence of different classes of fine sediments and consolidation processes although invisible to the end user suspended sediment transport processes are dealt with by the hydrodynamic modules of the telemac mascaret modelling system telemac 2d or telemac 3d while near bed bedload and processes in the bottom layer are handled by gaia this allows a clearer treatment of sedimentary processes that happen in the water column in the bed structure and at the water bed interface the forcing effects from waves are computed from the coupled wave propagation module tomawac benoit et al 1996 gaia can also be coupled with the modules for sediment dredging nestor open telemac mascaret 2022a and water quality waqtel open telemac mascaret 2022c 2 sediment transport processes numerical simulations of sediment transport processes in rivers coastal seas and transitional waters can be performed by integrating several modules accounting for different physical mechanisms acting according their characteristic time response relevant mechanisms driving morphological changes are i hydrodynamics i i sediment transport with predictors for the sediment transport capacity and i i i bed evolution with a conservation law for sediment mass tassi et al 2008 2 1 hydrodynamics 2 1 1 currents the hydrodynamic modules telemac 2d hervouet 2007 and telemac 3d open telemac mascaret 2022b provide the flow field needed to compute sediment transport and morphological processes in 2d and 3d spatial dimensions respectively telemac 2d solves the two dimensional depth averaged shallow water equations which implicitly assume hydrostatic pressure distribution constant fluid density and depth averaged velocity components vreugdenhil 2013 lane 1998 telemac 3d solves the three dimensional free surface navier stokes equations based on the pressure decomposition this module allows consideration of either a hydrostatic or non hydrostatic pressure hypothesis the hydrostatic approximation eliminates the need to solve the three dimensional poisson equation for the dynamic pressure thereby decreasing computational resources conversely in many cases the dynamic pressure and the vertical acceleration are not negligible and the non hydrostatic approach is needed to effectively capture relevant sedimentary processes density variations in buoyant flows mainly affect the flow dynamics through the gravity term the difference in density δ ρ ρ ρ 0 is a deviation of the water density ρ kg m 3 with respect to some reference value ρ 0 kg m 3 in telemac 3d it is given by an equation of state that may be dependent on the temperature the salinity and or the sediment concentration for both 2d and 3d models a number of relationships must be specified to close the governing equations the classical squared function dependency on depth averaged velocity is used to parameterise the bed resistance with strickler manning chézy haaland or nikuradse dimensionless friction coefficients the boussinesq approximation is used for the turbulence parameterisation with eddy viscosity values calculated by using one of the options available for turbulence closure models such as constant viscosity elder k ϵ smagorinski mixing length k ω or spalart allmaras in addition turbulence closure models can be enhanced for simulating a broader range of flow fields by using the available damping functions such as the munk anderson winterwerp and kranenburg 2002 viollet viollet 1987 or a user defined function telemac 2d uses the finite element and finite volume methods for solving the governing equations with an unstructured triangular mesh discretisation for telemac 3d the computational domain is discretised with a triangular element mesh over an horizontal plane as for telemac 2d followed by extruding each triangle along the vertical direction into linear prismatic columns spanning the water column from the bottom to the free surface each column is composed of a fixed number of prismatic elements whose vertical spacing can be adjusted accordingly e g to increase the resolution near the bottom and the free surface to take into account the domain movement along the vertical direction a technique combining arbitrary lagrangian eulerian ale and sigma transformation approaches is used in telemac 3d decoene and gerbeau 2009 in addition both telemac 2d and telemac 3d incorporate radiation stress terms to the momentum equations which are relevant for an accurate representation of coastal processes when the influence of the waves is considered the wave radiation stresses and their corresponding gradients are computed within the wave model tomawac and interpolated in space and time in the 2d and 3d flow models 2 1 2 waves propagation in gaia the influence of waves on sediment transport processes can be included using the wave generated oscillatory or orbital velocity u w m s roelvink 2011 soulsby 1997 according to the wave s characteristics and assuming the validity of the linear waves theory either i regular monochromatic or i i irregular spectral waves can be considered following soulsby and smallman 1986 the latter method calculates the rms orbital velocity u r m s produced by all the waves in a jonswap spectrum and then converts it to a monochromatic orbital velocity u w 2 u r m s the former method computes the maximum wave orbital velocity as follows u w h s ω 2 sinh k h where h is the water depth m h s is the significant wave height m ω 2 π t p is the intrinsic angular frequency 1 s with t p the wave period s k 2 π l is the wavenumber 1 m with l the wavelength m both h s and t p are computed from the waves module tomawac benoit et al 1996 the wavenumber is calculated from the dispersion relationship ω 2 g k tanh k h with g the acceleration due to gravity m s 2 the bed shear stress due to waves alone τ w n m 2 is calculated as function of u w by using a friction factor f w accounting for the influence of waves τ w 1 2 ρ f w u w 2 the wave friction factor f w is computed according to soulsby 1997 as follows f w f w a 0 k s where a 0 u w ω m is the semi orbital excursion and k s m the bed roughness in gaia the expression proposed by swart 1976 has been implemented f w exp 6 0 5 2 a 0 k s 0 19 if a 0 k s 1 59 0 30 otherwise in coastal seas the effect of waves superimposed onto a mean current can have an impact on the sediment transport and morphodynamic processes as the bed shear stress varies through a wave cycle the following quantities are computed in gaia for sediment transport calculations soulsby 2012 1997 the mean shear stress value τ m n m 2 computed over the wave cycle 1 τ m τ b 1 1 2 τ w τ b τ w 3 2 with τ b the bed shear stress due to current alone n m 2 the maximum bed shear stress τ m a x n m 2 during the wave cycle is computed as follows 2 τ m a x τ m τ w cos ϕ w c 2 τ w sin ϕ w c 2 1 2 with ϕ w c the angle between the current and wave directions the root mean square value τ r m s n m 2 taken over the wave cycle is computed by the expression 3 τ r m s τ m 2 1 2 τ w 2 1 2 2 2 sediment transport processes in the water column suspended sediment particles being transported by the flow at a given time and maintained in temporary suspension above the bottom by the action of upward moving turbulent eddies are commonly called suspended load garcia 2008 the equation describing mass conservation of suspended sediment is the advection diffusion equation ade that is valid only for dilute suspensions of particles that are not too coarse within gaia the solution of the ade completed with appropriate boundary and initial conditions is computed by telemac 2d or telemac 3d for 2d and 3d cases respectively the solution procedure remains invisible to the user since the physical parameters are provided by the gaia steering file two advantages of this procedure are evident i to stay up to date with the numerical schemes and algorithm developments in the hydrodynamics modules for the solution of the advection terms and i i for a clearer distinction between sediment transport processes happening in the water column in the near bed and in the bed structure for example in cases where exchanges with the bottom are not required such as suspended sediment transport over a rigid bed 2 3 sediment transport processes in the bottom 2 3 1 bedload transport sediment particles which are transported in direct contact with the bottom or next to the bed without being affected by the fluid turbulence are commonly called bedload in gaia bedload fluxes are computed in terms of dry mass transport rate per unit width without pores kg ms 4 q m b ρ s q b with q b q b x q b y q b cos α q b sin α above q b is the vector of volumetric transport rate per unit width without pores m 2 s with components q b x q b y along the x and y directions respectively and module q b α is the angle between the sediment transport vector and the downstream direction x axis and ρ s the sediment density kg m 3 the non dimensional sediment transport rate φ b is expressed by wu 2007 5 φ b q b g s 1 d 3 with s ρ s ρ the relative density ρ the water density kg m 3 d the sand grain diameter m and g the gravity acceleration constant m s 2 different choices of empirical formulae for computing φ b can be selected by the user by default corresponding to the meyer peter and müller formula in gaia available sediment transport rate formulas are shown in table 1 other sediment transport formulas can easily be implemented by the user within the fortran 90 code three key aspects must usually be considered for computing the magnitude and direction of the bedload abad et al 2008 namely the effect of the local bed slope secondary flow effects on the direction of the bed shear stress also referred as to helical flows and the bed shear stress partitioned into components affected by skin friction and drag force from bedforms also known as form drag gaia includes methods for evaluating these three aspects the angle α between the sediment transport direction and the x axis direction will deviate from that of the shear stress due to the combined action of a transverse slope and secondary currents in a cartesian coordinate system the relation of 95 is 6 tan α sin δ 1 f θ z b y cos δ 1 f θ z b x above the terms z b x and z b y represent respectively the transverse and longitudinal slopes z b the bottom position above datum m and δ the angle between the sediment transport vector and the flow direction the sediment shape function f θ is a function weighting the influence of the transverse bed slope expressed as a function of the non dimensional shear stress or shields parameter θ it can be computed according to i koch and flokstra 1980 f θ 3 2 θ and i i talmon et al 1995 f θ β 2 θ with the empirical coefficient β 2 the default value is β 2 0 85 but an optimal value of β 2 1 6 was found for the calibration of numerical experiments of dunes and bars in a laboratory channel mendoza et al 2017 in curved channels the direction of the sediment transport will no longer coincide with the direction of the bed shear stress for two dimensional simulations the effect of secondary flows can be accounted from the equation 7 δ tan 1 v u tan 1 a r s h where h the water depth m u v the components of the depth averaged velocity field along the x and y cartesian directions m s respectively r s the local radius of curvature m and a the spiral flow coefficient above the second term accounts for the effect of the spiral motion on the sediment flux due to the effect of secondary currents in gaia the default value of a 7 as originally proposed by engelund 1974 two methods are proposed in gaia for the correction of the magnitude of the sediment transport the method proposed by koch and flokstra 1980 is based on the modification of the bed load transport rate by a factor that acts as a diffusion term in the bed evolution equation 8 q b q b 1 β z b s q b 1 β z b x cos α z b y sin α where q b is the modified bedload transport rate s is the flow direction and β is an empirical factor accounting for the streamwise bed slope effect 1 3 by default the correction proposed by soulsby 1997 is based on the modification of the critical shields parameter and is therefore only valid for threshold bedload formulas τ β c r τ c r cos ψ sin χ cos 2 χ tan 2 ϕ sin 2 ψ sin 2 χ tan ϕ where τ β c r is the corrected critical shields number for a sloping bed n m 2 τ c r n m 2 is the critical shields number for a flat horizontal bed ϕ is the angle of repose of the sediment χ is the bed slope angle with the horizontal and ψ is the angle between the flow and the bed slope directions the total bed shear stress is due to skin friction and bedform drag van rijn et al 1993 but only the component due to skin friction acts on bedload garcia 2008 the shear stress due to skin friction is expressed as 9 τ μ τ b where τ b 0 5 ρ c f u 2 v 2 is the total bed shear stress and μ is the friction factor 10 μ c f c f where c f is the friction coefficient due to form drag plus skin friction specified in the hydrodynamics module and c f is the friction coefficient due only to skin friction which is computed as 11 c f 2 κ ln 11 036 h k s 2 where κ is the von kármán coefficient assumed to be equal to 0 40 by default the roughness height k s α k s d 50 m the coefficient α k s is a calibration parameter and d 50 the median diameter of the sediment material m in coastal and transitional water zones the presence of ripples can be taken into account to compute the friction factor μ van rijn et al 1993 for this option a bedform predictor is used to calculate the bedform roughness k r m in order to account for the effect of ripples both k r and k s defined below influence the transport rates van rijn 2007a it is assumed that 12 μ c f 0 75 c r 0 25 c f where the quadratic friction c r due to bedforms is calculated as a function of k r see eq 13 a natural sediment bed is generally covered with bedforms with wavelength λ d m and height η d m in most cases large scale models do not resolve the small to medium scale bedforms such as ripples or mega ripples which need therefore to be parameterised by increasing the friction coefficient to determine the bed roughness two options are available in gaia by imposing the friction coefficient based on friction laws in this case the values of the friction coefficients are provided by telemac 2d or telemac 3d by computing the value of the bed roughness as a function of flow and sediment parameters using a bed roughness predictor three different options are implemented in gaia to predict the total bed roughness the bed is assumed to be flat k s k s α k s d 50 with α k s equal to 3 by default the bed is assumed to be covered by ripples for currents only the ripple bed roughness is function of the mobility parameter ψ see 107 13 k r d 50 85 65 tanh 0 015 ψ 150 for ψ 250 20 d 50 otherwise with ψ u 2 s 1 g d 50 and u the norm of the velocity vector for waves and combined waves and currents bedform dimensions are calculated as a function of wave parameters following the method of wiberg and harris 1994 the wave induced bedform bed roughness k r is calculated as a function of the wave induced bedform height η r 14 k r max k s η r then k s k s k r for currents only without wave effects the total bed roughness predictor of 107 may be used huybrechts et al 2010 here the total bed roughness can be decomposed into a grain roughness k s m a small scale ripple roughness k r m a mega ripple component k m r m and a dune roughness k d m 15 k s k s k r 2 k m r 2 k d 2 both small scale ripples and grain roughness have an influence on the sediment transport laws while the mega ripples and dune roughness only contribute to the hydrodynamic model total friction in eq 15 the general expression for the mega ripple roughness k m r is given by 16 k m r 0 00002 f t s h 1 exp 0 05 ψ 550 ψ with f t s d 50 1 5 d s a n d for d 50 1 5 d s a n d 1 0 otherwise with d s a n d 0 000062 m lastly the general expression for the dune roughness is k d 0 00008 f t s h 1 exp 0 02 ψ 600 ψ further details and information about the bed roughness predictor can be found in van rijn 2007a 2 3 2 bed stratigraphy for sand graded distributions two different approaches are proposed one based on the classical active layer formulation of hirano 1971 and blom 2008 and the other based on a continuous grain sorting along the vertical direction cvsm merkel and kopmann 2012 merkel 2017 both approaches consider the concept of an active layer which supplies material that can be eroded or deposited as bedload or suspended load its thickness can be specified by the user or can be calculated by six different formulations e g hunziker 1995 günter 1971 fredsoe and deigaard 1992 van rijn 1993 wong and parker 2006 malcherek 2007 and others the default value is equal to 3 d 50 with d 50 the median diameter of sediment material contained in the active layer for the approach based on the classical hirano formulation the bed model can be discretised by a constant number of layers along the vertical direction since layers are allowed to be emptied the utilised number of layers at each mesh node can vary during a numerical simulation when more than one sediment class is specified the following cases arise i for a given initial bed stratification i e through a given number of layers n l a y an active layer is added inside this stratification at the beginning of the simulation in this case the total number of layers is n l a y 1 i i if the initial bed stratification is not provided the sediment bed is thus subdivided in two layers the active layer and a substrate layer located directly below in this case the total number of layers is equal to 2 to maintain the active layer thickness throughout the numerical simulation at each time step the following procedures are performed in the case of erosion the sediment mass is taken from the active layer therefore the sediment flux is transferred from the substratum first non empty layer below the active layer to the active layer note that the rigid bed algorithm is applied to the active layer i e only the sediment mass in the active layer is available at the given time step this is important as bedload transport rate and or the rate of entrainment for suspension are computed using the sediment composition available in the active layer if the erosion during the time step exceeds the mass of sediment available in the top layer this layer is fully eroded and a new erosion flux is computed using the composition of the layer underneath that is now the surface layer in the case of deposition the increased thickness generates a sediment flux from the active layer towards the first substratum layer the cvsm approach is based on the work of blom 2003 and blom et al 2008 and was adapted to gaia merkel and kopmann 2012 merkel 2017 the main idea is to keep the vertical grain sorting profile independently from the active layer sediment distribution the vertical grain sorting profile is stored for each sediment class with a vertical discretisation adapted to the profile at each time step the vertical sorting profile is changed in case of sedimentation a new vertical profile layer is added in case of erosion the vertical sediment stratification is changed for each eroded sediment class by using a modified version of the line generalisation algorithm proposed by douglas and peucker 1973 the maximal number of vertical profile layers can be kept from the vertical sediment stratification the sediment distribution of the active layer is calculated at each time step therefore this mixing process does not disturb the vertical sediment stratification 2 3 3 mixed sediments the hirano bed model algorithm introduced in the previous section has been modified to account for the presence of mud or sand mud mixtures mixed sediment consists of a mixture of n n c o 1 classes of non cohesive sediment sand and or gravel with n c o 1 classes of fine cohesive sediment non cohesive sediments are assumed to be transported by bedload and or suspension while cohesive sediment is transported only by suspension for mixed sediments the layer thickness results from the ratio of the volume of cohesive and non cohesive sediment contained in each layer if the cohesive sediment volume is less than the non cohesive sediment porosity default value equal to 40 the layer thickness only depends on the volume of non cohesive sediment in this case it is assumed that the cohesive sediment is contained within the pore space volume between the non cohesive sediment grains conversely if the cohesive sediment volume is larger than the non cohesive sediment porosity the layer thickness is computed from the non cohesive sediment volume plus the cohesive sediment volume minus the interstitial volume between non cohesive sediment classes the presence of high concentrations of cohesive sediment in the bed are known to prevent bedload transport from occurring van ledden et al 2004 therefore in gaia bedload transport is only computed if the mass fraction of cohesive sediment in the active layer is 30 2 3 4 consolidation processes consolidation processes are based on the semi empirical formulation proposed by villaret and walther 2008 which uses the iso pycnal and first order kinetics formulations consolidation of mud deposits is modelled using a layer discretisation where the first layer corresponds to the freshest deposit while the lower layer is the most consolidated layer sediment deposition from the water column is added directly to the first layer a rate or flux of consolidation is computed for each layer and for each class of cohesive sediment separately the values of the computed fluxes depend on the availability of each class in the layer considered in the case of mixed sediment the presence of non cohesive sediment in the stratigraphy of the mixture is considered to not alter the cohesive sediment consolidation 2 4 sediment exchanges at the water bed interface a unified framework has been developed for modelling the bed exchange processes at the water bottom interface of both cohesive and non cohesive sediment in 2d and 3d that eliminates unnecessary code duplication 2 4 1 erosion rate the erosion of sediment classes from the bed per unit area per unit time e kg sm 2 is computed by using the same formulation for both 2d and 3d simulations wu 2007 for cohesive sediment the erosion rate is computed for each sediment class following partheniades 1965 assuming the surface erosion rate is a linear function of the dimensionless excess shear stress 17 e m τ b τ c e τ c e with m the erodibility coefficient kg sm 2 and τ c e the critical shear stress for erosion n m 2 for non cohesive sediment classes the erosion rate is calculated for each size class as a function of the equilibrium sediment concentration at the interface between suspended load and bedload c b g l and the settling velocity of the sediment particles w s m s 18 e w s c b available formulations of the equilibrium sediment concentration at the interface are zyserman and fredsøe 1994 bijker 1968 van rijn 1993 and knaapen and kelly 2011 in gaia settling velocity values for non cohesive sediment particles are computed from the stokes zanke and van rijn formulations wu 2007 19 w s λ d 50 2 g 18 ν d 50 1 0 4 10 ν d 50 1 0 01 λ g d 50 3 ν 2 1 1 0 4 d 50 1 0 3 1 1 λ g d 50 otherwise where λ ρ s ρ ρ with ρ s the sediment density kg m 3 ρ the water density kg m 3 and ν is the water kinematic viscosity m 2 s assumed to be equal to 10 6 m 2 s by default constant or variable e g as a function of the sediment concentration particle settling velocity values can be also provided by the user for non cohesive sediments erosion is initiated when the bed shear stress due to currents and optionally waves exceeds a critical threshold for initiation of erosion of cohesive sediment it is assumed that all the cohesive sediment classes have the same combined mechanical behaviour and therefore the same value of critical shear stress is used for all classes nevertheless since the erosion flux depends on the relative availability of each sediment class the erosion fluxes can differ between classes when considering a mixture of sediment classes in the bed the composition in the surface active layer is taken into consideration when computing both the critical shear stress for erosion and the erosion flux this is achieved by combining the critical shear stresses for erosion for all the sediment classes cohesive and non cohesive according to le hir et al 2011 if the mass of cohesive sediment as a fraction of the mixture is 50 then the critical shear stress and erosion flux for cohesive sediment alone is used if the mass of cohesive sediment as a fraction of the mixture is 30 then the critical shear stress for non cohesive sediment is used and the erosion flux for non cohesive sediment is used if the mass of cohesive sediment as a fraction of the mixture is 30 and 50 then the values are interpolated between the previous values the erosion flux determined above is then distributed among the non cohesive and cohesive sediment classes according to their respective mass fractions in the bed mixture 2 4 2 deposition flux the deposition flux d kg sm 2 for each sediment class in suspension is calculated as the product of its class specific settling velocity w s and the suspended sediment concentration at the interface between suspended load and bedload c b g l computed from the respective coupled 2d or 3d suspended transport model wu 2007 20 d w s c b 1 τ b τ c d with τ c d the critical shear stress for deposition n m 2 by default erosion and deposition are allowed to occur simultaneously winterwerp et al 2012 this paradigm implies that sediment deposition takes place at all times regardless of the value of the bottom shear stress this is performed in gaia by assuming a very large value e g τ c d 1000 nm 2 the critical shear stress value can also be provided by the user 2 5 bed evolution the bed evolution is computed by solving the mass conservation equation for sediment or exner equation garcia 2008 expressed in terms of mass where bedload suspended load or both sediment transport modes can be considered simultaneously in its simplest form by considering bedload and suspended sediment transport of one class of non cohesive sediment this equation reads 21 1 λ ρ s z b t q m b d e where λ is the porosity of the bed material at the bed surface z b is the bed elevation above datum m and is the divergence operator numerical computation of sediment fluxes in dry mass as the conservative variable in exner equation minimises roundoff error particularly for the mass transfer algorithms used for the bed layer model in gaia a morphological factor that modifies the values of sediment flux for bedload transport and the erosion and deposition fluxes for suspended transport is implemented this simple accelerator is based on a constant scale factor applied to the divergence operator of the exner eq 21 for the case of bedload transport morgan et al 2020 roelvink 2006 when suspended transport processes are considered the constant scale factor is applied to the net exchange flux term at the bed water interface 3 overview of solution methods 3 1 numerical methods for the solution of processes in the water column the solution of the ade equation for sediments is performed by telemac 2d or telemac 3d where different numerical schemes are implemented the following methods are available method of characteristics e g see hervouet 2007 for its application in the telemac mascaret system which is not mass conservative but has the property of not generating new extrema monotone scheme supg streamline upwind petrov galerkin method brooks and hugues 1982 hervouet 2007 residual distribution method which is mass conservative and monotonous it includes different schemes 1 the n scheme roe 1987 2 the positive streamwise invariant psi scheme struijs 1994 3 the predictor corrector scheme ricchiuto and abgrall 2010 pavan et al 2016 4 the locally implicit psi lips scheme pavan 2016 edge based residual distribution method that is also mass conservative and monotonous it includes 1 n edge based residual distribution nerd scheme hervouet et al 2011 2 element by element residual distributive iterative advection eria scheme hervouet et al 2017 lips nerd and eria are suitable schemes for problems accounting for wetting and drying processes 3 1 1 initial conditions the spatial distribution of initial condition values of concentration can be specified for each sediment class expressed in g l 3 1 2 inflow boundary conditions at inflow boundaries concentration values can be specified i by imposing a fixed value i i computed according a near bed equilibrium concentration formula and i i i by providing time varying concentration values through an external ascii file for the 3d case the vertical profile of suspended sediments can be adopted assuming a constant rouse or user provided concentration distribution along the vertical direction 3 1 3 outflow boundary conditions at the outflow boundary the suspended load concentration gradient along the flow direction is set to zero 3 1 4 solid wall boundary conditions along banks and islands the no flux boundary condition is imposed in this case the suspended load concentration gradient is set to zero 22 c n 0 with n the coordinate in the direction normal to the boundary 3 2 numerical methods for the solution of processes in the bottom the exner equation 21 can be solved with two different approaches for simplicity we define below q q m b 1 a finite element centred n scheme as described in hervouet et al 2011 in a first step fluxes are computed with a distributive scheme and in a second step the positivity of the erodible layer is ensured by a limitation of the previously computed fluxes 2 a finite volume scheme built on a dual mesh as proposed by bristeau and coussin 2001 by defining q i j the solid discharge at the interface between adjacent elements i and j of the dual mesh and q i and q j the solid discharge computed by a bedload sediment transport formula see eq 5 at elements i and j respectively then for the centred scheme q i j q i q j 2 for the upwind scheme the decentering is chosen according to the sign of the solid discharge value projected at the interface between two cells q p r o j q p r o j i j n x i j q x i j n y i j q y i j with q x i j and q y i j the average of the solid discharge components on each side of the interface n x i j and n y i j are the components of the normal at the element s interface the flux at the interface is therefore calculated as q i j q i if q p r o j i j 0 q j if q p r o j i j 0 3 2 1 inflow boundary conditions at inflow boundaries the sediment discharge must be given at each node two different cases can be specified i equilibrium or i i user defined time series of bedload sediment discharge values constant or variable 3 2 2 outflow boundary conditions at the outflow boundary bedload does not require any particular boundary condition 3 2 3 solid wall boundary conditions at solid boundaries the bedload transport rate is set to zero 3 3 coupling strategy between different modules the solution of the discretised sediment transport equations is solved in gaia in a synchronous decoupled way meaning that flow and sediment transport processes are decoupled by using this approach the pseudocode of the implemented algorithm for the solution of the morphodynamics problem for bedload transport in the telemac mascaret modelling system is given in algorithm 1 in the following δ t t 2 d is the time step for hydrodynamics telemac 2d δ t g a i a is the time step for sediment transport and morphodynamics gaia δ t t o m is the time step for waves tomawac c p t 2 d g a i a is the coupling period for telemac 2d and gaia c p t 2 d t o m is the coupling period for telemac 2d and tomawac n i t is the number of iterations and t 0 and t f are respectively the initial time and total simulation time in algorithm 1 telemac2d wac and gaia are the main subroutines of modules telemac 2d tomawac and gaia respectively a similar procedure is executed when coupling with telemac 3d 4 examples hereafter five examples are provided to illustrate the capabilities of gaia at reproducing sediment transport and morphological processes in rivers coastal seas and transitional waters 4 1 flume experiment of a river bend the flume experiment proposed by yen and lee 1995 was chosen to assess the ability of telemac 2d gaia to reproduce the bed evolution in an alluvial channel bend under unsteady flow conditions this case might be relevant for river training projects in bends sediment transport is not only longitudinal but also transverse due to secondary flow typically cut bank erosion occurs at the outer bank and point bar deposition with finer material occurs at the inner bend in the experiment the final bottom evolution and the sediment sorting were measured along transverse cross sections located at points of observed maximum erosion and deposition around a bend in a flume data from run 4 of the laboratory experiment of yen and lee 1995 were used to validate the gaia results numerical simulations were performed by implementing the wu et al 2000 bedload transport formulation including the hiding exposure effects five sediment classes were considered with values ranging from 0 31 to 3 36 mm five vertical layers were adopted for the bottom stratigraphy the active layer thickness was computed with the formula of malcherek 2007 the constant critical shields parameter equal to 0 045 and a bed porosity equal to 0 375 were adopted slope effects were calculated according to talmon 1992 and soulsby et al 1996 secondary currents effects were incorporated in the simulations after engelund 1974 with the engelund parameter equal to 7 fig 1 left shows a comparison between contour levels of measured and simulated normalised bottom evolution and the normalised mean diameter at 90 around the bend fig 1 right numerical results show that the maximum deposition occurred more upstream approx 50 leading to a gentler slope at the 90 cross section in comparison with observations fig 2 shows the comparison between the measured and the simulated normalised bottom evolution for the maximum deposition at 90 and maximum erosion at 180 around the bend the simulated bed levels along the 180 cross section are in good agreement with the measured data simulation results follow the expected behaviour in an alluvial channel bend 4 2 suspended sediment distribution in the vertical direction according to the law of the wall von kármán 1930 for steady state fully mixed shallow water flows the shape of the vertical flow velocity profile is logarithmic the vertical turbulence in this situation may be accurately represented using a mixing length formula prandtl 1926 and with the assumption of a linear decrease in shear stress between bed and surface leads to a parabolic shaped eddy diffusivity profile if a single class of sediment with uniform settling velocity and infinite supply is then introduced the resultant steady state suspended concentration profile will follow the shape of the analytical rouse profile von kármán 1937 written as 23 c z c a h a z h z a w s β κ u where c z is the concentration g l at height z m above the bed c a is the concentration g l at reference height a m h is the height of the water column m w s is the settling velocity m s u is the bed shear velocity m s κ is the von kármán constant equal to 0 4 and β is the prandtl number representing the ratio between the turbulent transport for the sediment and the momentum assumed here to be equal to 1 a simple test case was developed to assess the ability of telemac 3d gaia to reproduce the rouse profile and consequently to solve suspended sediment transport problems the model geometry comprised a rectangular channel 5 km 0 5 km with a water depth of 10 m divided into 21 sigma planes bed friction was prescribed using a nikuradse roughness length of 0 01 m and the lateral boundaries were prescribed with no friction the vertical turbulence model was chosen to be consistent with the law of the wall assumption i e mixing length model at the entrance to the channel a discharge of 5000 m 3 s was applied generating a depth mean flow speed of 1 m s sediment was introduced with a constant concentration through the vertical of 0 1 g l and with a settling velocity of 0 001 m s during the simulation the combined effects of friction vertical turbulence and settling velocity led to the gradual development downstream of equilibrium profiles for eddy diffusivity k z flow velocity u and suspended concentration c fig 3 the results validate the ability of telemac 3d gaia at reproducing the rouse profile and in doing so also validate the modelled flow velocity profile mixing length turbulence model and settling velocity calculations 4 3 middle lower rhine river reach for a 46 5 km long stretch of the middle lower rhine river between neuss and duisburg in germany rh km 730 776 5 a coupled telemac 2d gaia model was built to investigate artificial bedload supply measures this case might be relevant for projects related to navigability and fluvial transportation the model was calibrated for a period of 6 years of the natural hydrograph from 2000 07 11 to 2006 06 22 the computational domain was discretised with a mesh of approximately 260 000 nodes and typical element sizes in the range 3 17 m in the main channel and up to 40 m in the floodplains this discretisation allowed an appropriate reproduction of the groyne geometry as well as the analysis of artificial bedload supply bed evolution and bedload transport fig 4 shows the river topography inside the model boundaries the large amplitude bends in this river stretch strongly influence hydrodynamics and morphodynamics processes furthermore a tendency for long term erosion is observed erosion mitigation is applied using sediment management operations including artificial bedload supply as well as dredging and disposal activities all sediment management actions during the simulation period were considered in the model by coupling with the nestor module numerical simulations were performed with a hydrodynamic time step of 4 s a coupling period equal to 10 and a morphological factor of 4 the adopted parameterisations for friction and turbulence were nikuradse friction law and elder turbulence model respectively ten sediment classes were defined with the hirano multi layer model consisting of 3 layers and a constant active layer thickness equal to 0 1 m in the numerical simulation suspended load was assumed to be negligible relative to bedload and hence only bedload transport was considered the meyer peter and müller transport formula was adopted with the karim holly and yang hiding exposure formulation karim et al 1987 the soulsby and talmon slope effect formulation was also included secondary currents were accounted for using the morphodynamic approach of engelund with the radius of curvature provided in an additional file for the model calibration a moving average of the river bottom was calculated considering only the mean channel and a moving longitudinal section of 1100 m the averaged simulated bottom evolution is generally in a good agreement with the measurements see fig 5 nevertheless in the last 10 km of the stretch the numerical model predicts aggregation instead of erosion tendencies fig 6 presents the comparison of the simulated and measured river bottom at three chosen cross sections the cross sections rh km 738 1 and 743 1 are located before and after the nearly 180 bend while the cross section rh km 761 1 is positioned before the last bend in the model the positions of the cross sections are shown in fig 4 the cross sections 738 1 and 761 1 show a typical outer bank erosion and point bar deposition profile despite some discrepancies between observed and simulated bed evolution results at section 743 1 predictions of the riverbed evolution are globally well reproduced by the numerical model fig 7 illustrates the simulated river bottom and mean diameter at the sharp bend near rh km 741 typically at the outer banks the river bottom consists of coarser material while finer material is located at the inner banks and middle grounds which is also well represented by the numerical model 4 4 the gironde estuary the macrotidal gironde estuary is located in south west france covering a surface of 635 km 2 from the bay of biscay to 170 km landward see fig 8 this transitional water body is characterised by a complex geomorphology high suspended particulate matter spm concentrations of up to 20 g l and a heterogeneous bed composition understanding the flow sediment dynamics and morphological changes of this coastal estuarine system would be important for a more efficient planning of ship routes and management of underkeel clearance in navigation channels for example numerical simulations of this transitional water body were performed by coupling telemac 3d with gaia for a time period spanning 90 days salinity was considered as a passive tracer with 35 psu imposed on the offshore boundary and 0 psu for both tributaries namely the garonne and dordogne rivers the prandtl mixing length model combined with a damping function lehfeldt and bloss 1988 was selected for the turbulence closure relationship the computational domain was extended offshore up to 70 km from the mouth and about 200 km alongshore such that the boundary condition for salinity was far enough from the mouth so it did not influence the salinity within the estuary inland the model extended up to 180 km to the limit of the tidal propagation tides and surges were imposed at the offshore open boundary the tidal signal was extracted from the north east atlantic tidal model of legos huybrechts et al 2012 for the simulated period flowrates were imposed at both garonne and dordogne rivers from a time series data provided by banque hydro 2020 the digital elevation model containing the bathymetric information was provided by the port of bordeaux the horizontal mesh comprised 58 250 nodes with typical element sizes ranging from 4 km on the offshore boundary to 30 50 m inside the tributaries the mesh discretisation along the vertical direction consisted of 9 planes spaced using a geometric progression with a higher mesh density near the bottom the mesh was refined along the navigation channel in order to have at least 5 nodes along the channel width both non cohesive and cohesive sediments were included in the simulation the coarser grained non cohesive sediment was considered to be transported by bedload using the meyer peter müller formula chini and villaret 2007 for the finer cohesive sediment two classes were considered the process of consolidation was included in the bed model using a multi layer approach with experimental data van 2012 used to calibrate the consolidation transfer rate coefficient orseau et al 2021 the erodibility coefficient m was set to 0 0015 kg s m 2 for settling velocity time and space varying values were computed for both mud classes as a function of the spm and salinity see van maanen and sottolichio 2018 the bed roughness was estimated using the van rijn 2007b formula according to the flow characteristics and predominant sediment diameter the sediment composition was estimated from the available data two simulations were performed considering high and low river discharges corresponding to winter and summer periods respectively the ability of the model to reproduce the spm pattern at observation stations p2 p3 and p5 is illustrated in figs 9 and 10 see fig 8 for locations the main variability is captured well by the model although the spm levels are slightly underestimated during the neap tide combined with low flow rate results from the numerical model were further used to identify the location of the turbidity maximum values during winter and summer periods see fig 11 4 5 the gironde estuary s mouth located on the sw atlantic coast façade the 20 km wide tidal mouth of the gironde estuary opens to the atlantic ocean in this zone the combined influence of waves and tides has led to the development of a complex hydro sedimentary dynamics where the net seaward transport varies with the tidal amplitude allen and castaing 1973 at the estuary mouth see fig 8 strong tidal currents and high energy atlantic swells induce important displacements of coarse grained sediment deposits and severe erosion along its coastline howa 1997 these processes lead to the development of numerous sandbanks in the intertidal and subtidal areas stéphan et al 2019 to be able to capture the flow sediment interactions at the estuary s mouth all relevant physical drivers were considered in a three way coupled simulation telemac 3d tomawac gaia i e tides waves wind atmospheric pressure and river discharges the computational domain was discretised with a finite element mesh consisting of 106 000 nodes and 207 000 elements covering a surface of approximately 15 000 m 2 along the vertical direction 5 layers were used typical element sizes varied from 3 km on the offshore boundary down to 50 meters in specific areas of the mesh to capture the morphology of channels and shoals of the estuary bathymetric information in the study area was incorporated into the finite element mesh from a digital elevation model built from a dataset collected during a field campaign performed in 2015 shom 2016 the initial bathymetry is showed in fig 12 a hydrodynamics forcing accounting for wave and current interactions was implemented by coupling the modules telemac 3d and tomawac along the offshore boundary harmonically derived water levels were prescribed using 33 tidal constituents obtained from interpolation of the values available in the global model fes 2012 carrère et al 2013 daily flow rates of river discharges were extracted from banque hydro 2020 and aquitaine 2020 ranging from 35 m 3 s to 2000 m 3 s in the dordogne river and from 70 m 3 s to 6000 m 3 s in the garonne river the boundary conditions for the wave model were extracted from a hindcast data set for the assessment of sea states climatologies homere developed by ifremer boudière et al 2013 they consisted of significant wave height wave period direction and spread in addition the model was forced with time series of atmospheric pressure fields from the fifth generation ecmwf atmospheric reanalysis model era5 downloaded from the copernicus climate change service c3s climate data store hersbach et al 2020 for gaia the representative sand diameter selected for the study area was d 50 0 30 mm the characteristics of the ocean floor louazel et al 2021 were used to determine the bottom friction for waves and hydrodynamic propagation the sediment transport formulation transpor2004 walstra et al 2005 was implemented in gaia the interaction due to the combined wave current forcing was considered bedform roughness length for waves and current were computed and sediment transport fluxes accounting for the influence of both suspended and bedload transport were computed time steps were set equal to 10 s for both telemac 3d and tomawac modules for gaia a morphological acceleration factor equal to 5 was used hydrodynamics was validated between 2009 and 2010 against 9 tidal gauges and waves propagation and generation were validated using data from a measurement campaign of 2 months in the estuary s mouth where waves meet strong currents the sediment transport formula was calibrated with a simulation of one year and was compared to the mean annual longshore sediment transport deduced from the shoreline evolution during the last decade results are not shown here morphodynamic simulations were performed for a period spanning 5 years bed evolution results fig 12 b show i the presence of a morphological stable zone in the area surrounding the coast for the considered time period with bed evolution values comprise within the range 0 5 m and 0 5 m i i sediment accretion of approximately 5 5 million m 3 for 5 average years see fig 12 c comparable to the 4 6 million m 3 resulting from a bathymetric differential in the polygon indicated in fig 12 a i i i a migration of an oblique sandbar in the central zone of fig 12 b oriented towards the northwest direction related to the wave induced littoral drift and i v a general tendency to sediment erosion at the main and secondary channels at the estuary s inlet 5 conclusions and outlook gaia the new code structure available in the telemac mascaret modelling system provides an integrated framework for solving sediment transport and morphological evolution problems developments have been undertaken by considering the large number of variables to be considered when dealing with rivers coastal seas and transitional water applications namely different spatial dimensions two and three dimensional flow states steady quasi steady unsteady flow regimes subcritical critical supercritical sediment characteristics cohesive non cohesive mixed sediment classes uniform graded and transport modes bedload suspended load total load and states equilibrium non equilibrium continuous developments performed in the telemac mascaret system allow access to efficient numerical solvers serial or multiple processors distributed memory architectures and easy to use api wrappers allowing a full control of numerical simulations in addition gaia can easily be expanded and customised to particular requirements by modifying user friendly easy to read and well documented fortran 90 files moreover theoretical aspects and validation test cases are documented and continually updated so that the quality of the source code remains assured in progress model improvements are i the use of highly exploitable and transparent techniques for coupling sediment transport with for example pollutant transport models to better quantify the transport and fate of chemicals in the aquatic environment i i the improvement of strategies for morphodynamic updating e g the enhancement of the various ways of accelerating the computation of sediment and morphological changes and i i i the application of stochastic methods in morphodynamic modelling in particular regarding uncertainty quantification data assimilation and automatic calibration techniques 6 software availability the telemac mascaret modelling system is distributed under gnu general public license gpl version 3 29 june 2007 the telemac mascaret modelling system can be downloaded from the gitlab repository https gitlab pam retd fr otm telemac mascaret additional information new releases user s forum installation notes etc can be accessed from the telemac mascaret s webpage http www opentelemac org user s manuals theoretical guides and miscellaneous information are available from the wiki http wiki opentelemac org declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are indebted to their former co workers yoann audouin and agnès leroy for helpful discussions and numerical informatics work essentially contributing to the development of the gaia module the authors would like to acknowledge the contributions of boris glander regarding the development of the sediment dredging and dumping module nestor sébastien bourban is kindly acknowledged for providing advice and support during the manuscript writing we sincerely appreciate the reviewers comments and suggestions which have been very helpful in improving the manuscript this research was partially supported by edf r d france through the project plateforme hydro environnementale 2 phe2 
25479,in rivers coastal seas and transitional waters sediment transport processes involve a variety of interacting factors that dynamically vary over time and space the flow dynamics within these highly heterogeneous natural systems influences the spatial patterns of erosion and deposition of the bed sediments which in turn shapes and conditions the bottom morphology by taking advantage of the established modelling framework in both two and three dimensions for unstructured meshes proposed within the telemac mascaret system the new module gaia provides a code structure for solving sediment transport and morphological evolution problems by a clear treatment of sedimentary processes that happen in the water column in the bed structure and at the water bed interface gaia efficiently manages the spatial and temporal variability of sediment size classes properties and transport modes for two and three dimensions in addition this module can easily be expanded and customised to particular requirements by modifying user friendly easy to read and well documented fortran 90 subroutines keywords telemac mascaret sediment transport morphodynamics hydraulics numerical modelling hydrodynamics data availability data will be made available on request 1 introduction in rivers coastal seas and transitional estuaries lagoons etc waters sediment transport processes involve a variety of interacting factors that dynamically vary over time and space according to the spatial and temporal scale being considered the flow dynamics within these highly heterogeneous natural systems influences both sedimentary processes and spatial patterns of erosion and deposition of the bed sediments which in turn shapes and conditions the environment for instance over long timescales if forcing conditions and sediment supply remain relatively stable the morphology of the system will ultimately reach a quasi equilibrium however over shorter scales this balance can be altered by both natural causes and human pressures masselink et al 2011 rhoads 2020 consequently understanding and predicting the sediment dynamics and related hydromorphological changes is important for evaluating the modification of sedimentary features or for assessing the departure from naturalness of the body of water moreover these adjustments can also be intrinsically related with biological e g bacteria vegetation benthic communities or chemical processes e g pollutants polymers among others larsen et al 2021 siviglia and crosato 2016 before the 1970s solutions to most engineering problems involving sediment transport processes were determined through field investigations and or physical scale models in laboratory wu 2007 from the early 70s advancements in computer technology and methods for the solution of scientific and engineering problems propelled the application of mechanistic models to predict sediment transport processes mathematical tools describing flow sediment transport and morphological change processes in a water body consist of a set of partial differential algebraic equations pdes completed with suitable initial and boundary conditions and closure relationships analytical or closed form solutions of these mechanistic models for the prediction of sediment transport processes provide a quantitative means of rapidly describing systematic trends associated with the phenomenon nevertheless they cannot be expected to provide accurate solutions to problems involving for example complex geometries time varying boundary conditions etc to overcome these limitations numerical solutions are often the most efficient and practical methods for predicting sediment transport in complex natural systems james et al 2010 thanks to the effort of several research institutes from both the industry and academia a number of computational models are currently available and widely applicable to practical cases involving sediment transport processes these models have been developed for different spatial dimensions flow states sediment size classes and transport modes wu 2007 their solution relies on the numerical approximation used for the solution of the pdes where according to the numerical method and nature of sub spaces used in the approximation the given problem is reduced to an algebraic problem having finite dimension which can be solved from a selection of convenient algorithms quarteroni and valli 2009 a coupling strategy based on the flow field is generally adopted this procedure usually includes defining an updating strategy of the flow and sediment transport variables the numerical bed updating schemes and morphological acceleration techniques roelvink 2011 the numerical simulation of sediment transport processes in river systems has increased dramatically over the past several decades kaveh et al 2019 physical processes observed in morphological structures commonly found in alluvial systems such as bar units bifurcations etc rhoads 2020 can be efficiently captured with standard computational resources open source software packages like delft3d lesser et al 2004 iric shimizu et al 2020 basement vanzo et al 2021 hec ras 2d sediment usace 2022 srh 2d lai and gaeuman 2020 and telemac mascaret villaret et al 2013 among others are able to capture the dynamics of river systems at different space and time scales many of the numerical tools developed for coastal and transitional waters derive from methods applied for the solution of river processes roelvink 2011 according to their dimensionality these models resolve variations in flow and transport along the horizontal directions for two dimensional models and along the horizontal and vertical directions for three dimensional models such models can be applied to both small scale and macro scale problems for different types of flow and wave models roelvink 2011 typical examples of computational models currently available to solve coastal and transitional water problems are adcirc mirabito et al 2011 coawst warner et al 2010 mars3d grasso et al 2018 roms sherwood et al 2018 and xbeach roelvink et al 2009 the telemac mascaret modelling system offers an established modelling framework in both two 2d and three dimensions 3d for unstructured meshes it is an integrated set of open source modular fortran 90 subroutines which provide the software architecture necessary for the numerical solution of the governing equations namely the data structure the algebraic operations and the building and solving phases nheili et al 2016 simulations can be launched in both serial mode or on multiple processors using distributed memory architecture mpi it also benefits from a python wrapper and a fortran api application program interface goeury et al 2017 allowing full control of a running simulation this software interface can be used for implementing efficient calibration algorithms based on data assimilation poncot et al 2017 and to perform sensitivity analysis and uncertainty quantification mouradi et al 2016 pre post processing and analysis tools are available through a set of python scripts included in the system s distribution audouin et al 2019 and by means of third party software such as bluekenue paraview and others in light of this the sediment transport and bed evolution module sisyphe of the telemac mascaret modelling system has been developed for more than 25 years latteux and tanguy 1990 originally based on the finite element structure used to solve the shallow water equations despite its robustness flexibility and capability of dealing with a large number of river cordier et al 2019 dutta et al 2017 mendoza et al 2017 coastal brown and davies 2009 robins et al 2014 van den eynde et al 2010 and estuarine santoro et al 2019 2017 giardino et al 2009 sediment transport and morphodynamics problems villaret et al 2013 a number of issues arose regarding the improvement of the treatment of graded and mixed cohesive and non cohesive sediments as well as the full compatibility between 2d and 3d processes gaia building upon the sisyphe module is able to model complex sediment and morphodynamic processes in coastal areas rivers lakes and estuaries accounting for spatial and temporal variability of sediment size classes uniform graded or mixed properties cohesive and non cohesive and transport modes suspended bedload or both processes simultaneously the generalised framework used for bed layering enables any combination of multiple size classes for both non cohesive and cohesive sediment to be modelled simultaneously compatibility is ensured between an active layer model an approach traditionally adopted for non cohesive sediment and the presence of different classes of fine sediments and consolidation processes although invisible to the end user suspended sediment transport processes are dealt with by the hydrodynamic modules of the telemac mascaret modelling system telemac 2d or telemac 3d while near bed bedload and processes in the bottom layer are handled by gaia this allows a clearer treatment of sedimentary processes that happen in the water column in the bed structure and at the water bed interface the forcing effects from waves are computed from the coupled wave propagation module tomawac benoit et al 1996 gaia can also be coupled with the modules for sediment dredging nestor open telemac mascaret 2022a and water quality waqtel open telemac mascaret 2022c 2 sediment transport processes numerical simulations of sediment transport processes in rivers coastal seas and transitional waters can be performed by integrating several modules accounting for different physical mechanisms acting according their characteristic time response relevant mechanisms driving morphological changes are i hydrodynamics i i sediment transport with predictors for the sediment transport capacity and i i i bed evolution with a conservation law for sediment mass tassi et al 2008 2 1 hydrodynamics 2 1 1 currents the hydrodynamic modules telemac 2d hervouet 2007 and telemac 3d open telemac mascaret 2022b provide the flow field needed to compute sediment transport and morphological processes in 2d and 3d spatial dimensions respectively telemac 2d solves the two dimensional depth averaged shallow water equations which implicitly assume hydrostatic pressure distribution constant fluid density and depth averaged velocity components vreugdenhil 2013 lane 1998 telemac 3d solves the three dimensional free surface navier stokes equations based on the pressure decomposition this module allows consideration of either a hydrostatic or non hydrostatic pressure hypothesis the hydrostatic approximation eliminates the need to solve the three dimensional poisson equation for the dynamic pressure thereby decreasing computational resources conversely in many cases the dynamic pressure and the vertical acceleration are not negligible and the non hydrostatic approach is needed to effectively capture relevant sedimentary processes density variations in buoyant flows mainly affect the flow dynamics through the gravity term the difference in density δ ρ ρ ρ 0 is a deviation of the water density ρ kg m 3 with respect to some reference value ρ 0 kg m 3 in telemac 3d it is given by an equation of state that may be dependent on the temperature the salinity and or the sediment concentration for both 2d and 3d models a number of relationships must be specified to close the governing equations the classical squared function dependency on depth averaged velocity is used to parameterise the bed resistance with strickler manning chézy haaland or nikuradse dimensionless friction coefficients the boussinesq approximation is used for the turbulence parameterisation with eddy viscosity values calculated by using one of the options available for turbulence closure models such as constant viscosity elder k ϵ smagorinski mixing length k ω or spalart allmaras in addition turbulence closure models can be enhanced for simulating a broader range of flow fields by using the available damping functions such as the munk anderson winterwerp and kranenburg 2002 viollet viollet 1987 or a user defined function telemac 2d uses the finite element and finite volume methods for solving the governing equations with an unstructured triangular mesh discretisation for telemac 3d the computational domain is discretised with a triangular element mesh over an horizontal plane as for telemac 2d followed by extruding each triangle along the vertical direction into linear prismatic columns spanning the water column from the bottom to the free surface each column is composed of a fixed number of prismatic elements whose vertical spacing can be adjusted accordingly e g to increase the resolution near the bottom and the free surface to take into account the domain movement along the vertical direction a technique combining arbitrary lagrangian eulerian ale and sigma transformation approaches is used in telemac 3d decoene and gerbeau 2009 in addition both telemac 2d and telemac 3d incorporate radiation stress terms to the momentum equations which are relevant for an accurate representation of coastal processes when the influence of the waves is considered the wave radiation stresses and their corresponding gradients are computed within the wave model tomawac and interpolated in space and time in the 2d and 3d flow models 2 1 2 waves propagation in gaia the influence of waves on sediment transport processes can be included using the wave generated oscillatory or orbital velocity u w m s roelvink 2011 soulsby 1997 according to the wave s characteristics and assuming the validity of the linear waves theory either i regular monochromatic or i i irregular spectral waves can be considered following soulsby and smallman 1986 the latter method calculates the rms orbital velocity u r m s produced by all the waves in a jonswap spectrum and then converts it to a monochromatic orbital velocity u w 2 u r m s the former method computes the maximum wave orbital velocity as follows u w h s ω 2 sinh k h where h is the water depth m h s is the significant wave height m ω 2 π t p is the intrinsic angular frequency 1 s with t p the wave period s k 2 π l is the wavenumber 1 m with l the wavelength m both h s and t p are computed from the waves module tomawac benoit et al 1996 the wavenumber is calculated from the dispersion relationship ω 2 g k tanh k h with g the acceleration due to gravity m s 2 the bed shear stress due to waves alone τ w n m 2 is calculated as function of u w by using a friction factor f w accounting for the influence of waves τ w 1 2 ρ f w u w 2 the wave friction factor f w is computed according to soulsby 1997 as follows f w f w a 0 k s where a 0 u w ω m is the semi orbital excursion and k s m the bed roughness in gaia the expression proposed by swart 1976 has been implemented f w exp 6 0 5 2 a 0 k s 0 19 if a 0 k s 1 59 0 30 otherwise in coastal seas the effect of waves superimposed onto a mean current can have an impact on the sediment transport and morphodynamic processes as the bed shear stress varies through a wave cycle the following quantities are computed in gaia for sediment transport calculations soulsby 2012 1997 the mean shear stress value τ m n m 2 computed over the wave cycle 1 τ m τ b 1 1 2 τ w τ b τ w 3 2 with τ b the bed shear stress due to current alone n m 2 the maximum bed shear stress τ m a x n m 2 during the wave cycle is computed as follows 2 τ m a x τ m τ w cos ϕ w c 2 τ w sin ϕ w c 2 1 2 with ϕ w c the angle between the current and wave directions the root mean square value τ r m s n m 2 taken over the wave cycle is computed by the expression 3 τ r m s τ m 2 1 2 τ w 2 1 2 2 2 sediment transport processes in the water column suspended sediment particles being transported by the flow at a given time and maintained in temporary suspension above the bottom by the action of upward moving turbulent eddies are commonly called suspended load garcia 2008 the equation describing mass conservation of suspended sediment is the advection diffusion equation ade that is valid only for dilute suspensions of particles that are not too coarse within gaia the solution of the ade completed with appropriate boundary and initial conditions is computed by telemac 2d or telemac 3d for 2d and 3d cases respectively the solution procedure remains invisible to the user since the physical parameters are provided by the gaia steering file two advantages of this procedure are evident i to stay up to date with the numerical schemes and algorithm developments in the hydrodynamics modules for the solution of the advection terms and i i for a clearer distinction between sediment transport processes happening in the water column in the near bed and in the bed structure for example in cases where exchanges with the bottom are not required such as suspended sediment transport over a rigid bed 2 3 sediment transport processes in the bottom 2 3 1 bedload transport sediment particles which are transported in direct contact with the bottom or next to the bed without being affected by the fluid turbulence are commonly called bedload in gaia bedload fluxes are computed in terms of dry mass transport rate per unit width without pores kg ms 4 q m b ρ s q b with q b q b x q b y q b cos α q b sin α above q b is the vector of volumetric transport rate per unit width without pores m 2 s with components q b x q b y along the x and y directions respectively and module q b α is the angle between the sediment transport vector and the downstream direction x axis and ρ s the sediment density kg m 3 the non dimensional sediment transport rate φ b is expressed by wu 2007 5 φ b q b g s 1 d 3 with s ρ s ρ the relative density ρ the water density kg m 3 d the sand grain diameter m and g the gravity acceleration constant m s 2 different choices of empirical formulae for computing φ b can be selected by the user by default corresponding to the meyer peter and müller formula in gaia available sediment transport rate formulas are shown in table 1 other sediment transport formulas can easily be implemented by the user within the fortran 90 code three key aspects must usually be considered for computing the magnitude and direction of the bedload abad et al 2008 namely the effect of the local bed slope secondary flow effects on the direction of the bed shear stress also referred as to helical flows and the bed shear stress partitioned into components affected by skin friction and drag force from bedforms also known as form drag gaia includes methods for evaluating these three aspects the angle α between the sediment transport direction and the x axis direction will deviate from that of the shear stress due to the combined action of a transverse slope and secondary currents in a cartesian coordinate system the relation of 95 is 6 tan α sin δ 1 f θ z b y cos δ 1 f θ z b x above the terms z b x and z b y represent respectively the transverse and longitudinal slopes z b the bottom position above datum m and δ the angle between the sediment transport vector and the flow direction the sediment shape function f θ is a function weighting the influence of the transverse bed slope expressed as a function of the non dimensional shear stress or shields parameter θ it can be computed according to i koch and flokstra 1980 f θ 3 2 θ and i i talmon et al 1995 f θ β 2 θ with the empirical coefficient β 2 the default value is β 2 0 85 but an optimal value of β 2 1 6 was found for the calibration of numerical experiments of dunes and bars in a laboratory channel mendoza et al 2017 in curved channels the direction of the sediment transport will no longer coincide with the direction of the bed shear stress for two dimensional simulations the effect of secondary flows can be accounted from the equation 7 δ tan 1 v u tan 1 a r s h where h the water depth m u v the components of the depth averaged velocity field along the x and y cartesian directions m s respectively r s the local radius of curvature m and a the spiral flow coefficient above the second term accounts for the effect of the spiral motion on the sediment flux due to the effect of secondary currents in gaia the default value of a 7 as originally proposed by engelund 1974 two methods are proposed in gaia for the correction of the magnitude of the sediment transport the method proposed by koch and flokstra 1980 is based on the modification of the bed load transport rate by a factor that acts as a diffusion term in the bed evolution equation 8 q b q b 1 β z b s q b 1 β z b x cos α z b y sin α where q b is the modified bedload transport rate s is the flow direction and β is an empirical factor accounting for the streamwise bed slope effect 1 3 by default the correction proposed by soulsby 1997 is based on the modification of the critical shields parameter and is therefore only valid for threshold bedload formulas τ β c r τ c r cos ψ sin χ cos 2 χ tan 2 ϕ sin 2 ψ sin 2 χ tan ϕ where τ β c r is the corrected critical shields number for a sloping bed n m 2 τ c r n m 2 is the critical shields number for a flat horizontal bed ϕ is the angle of repose of the sediment χ is the bed slope angle with the horizontal and ψ is the angle between the flow and the bed slope directions the total bed shear stress is due to skin friction and bedform drag van rijn et al 1993 but only the component due to skin friction acts on bedload garcia 2008 the shear stress due to skin friction is expressed as 9 τ μ τ b where τ b 0 5 ρ c f u 2 v 2 is the total bed shear stress and μ is the friction factor 10 μ c f c f where c f is the friction coefficient due to form drag plus skin friction specified in the hydrodynamics module and c f is the friction coefficient due only to skin friction which is computed as 11 c f 2 κ ln 11 036 h k s 2 where κ is the von kármán coefficient assumed to be equal to 0 40 by default the roughness height k s α k s d 50 m the coefficient α k s is a calibration parameter and d 50 the median diameter of the sediment material m in coastal and transitional water zones the presence of ripples can be taken into account to compute the friction factor μ van rijn et al 1993 for this option a bedform predictor is used to calculate the bedform roughness k r m in order to account for the effect of ripples both k r and k s defined below influence the transport rates van rijn 2007a it is assumed that 12 μ c f 0 75 c r 0 25 c f where the quadratic friction c r due to bedforms is calculated as a function of k r see eq 13 a natural sediment bed is generally covered with bedforms with wavelength λ d m and height η d m in most cases large scale models do not resolve the small to medium scale bedforms such as ripples or mega ripples which need therefore to be parameterised by increasing the friction coefficient to determine the bed roughness two options are available in gaia by imposing the friction coefficient based on friction laws in this case the values of the friction coefficients are provided by telemac 2d or telemac 3d by computing the value of the bed roughness as a function of flow and sediment parameters using a bed roughness predictor three different options are implemented in gaia to predict the total bed roughness the bed is assumed to be flat k s k s α k s d 50 with α k s equal to 3 by default the bed is assumed to be covered by ripples for currents only the ripple bed roughness is function of the mobility parameter ψ see 107 13 k r d 50 85 65 tanh 0 015 ψ 150 for ψ 250 20 d 50 otherwise with ψ u 2 s 1 g d 50 and u the norm of the velocity vector for waves and combined waves and currents bedform dimensions are calculated as a function of wave parameters following the method of wiberg and harris 1994 the wave induced bedform bed roughness k r is calculated as a function of the wave induced bedform height η r 14 k r max k s η r then k s k s k r for currents only without wave effects the total bed roughness predictor of 107 may be used huybrechts et al 2010 here the total bed roughness can be decomposed into a grain roughness k s m a small scale ripple roughness k r m a mega ripple component k m r m and a dune roughness k d m 15 k s k s k r 2 k m r 2 k d 2 both small scale ripples and grain roughness have an influence on the sediment transport laws while the mega ripples and dune roughness only contribute to the hydrodynamic model total friction in eq 15 the general expression for the mega ripple roughness k m r is given by 16 k m r 0 00002 f t s h 1 exp 0 05 ψ 550 ψ with f t s d 50 1 5 d s a n d for d 50 1 5 d s a n d 1 0 otherwise with d s a n d 0 000062 m lastly the general expression for the dune roughness is k d 0 00008 f t s h 1 exp 0 02 ψ 600 ψ further details and information about the bed roughness predictor can be found in van rijn 2007a 2 3 2 bed stratigraphy for sand graded distributions two different approaches are proposed one based on the classical active layer formulation of hirano 1971 and blom 2008 and the other based on a continuous grain sorting along the vertical direction cvsm merkel and kopmann 2012 merkel 2017 both approaches consider the concept of an active layer which supplies material that can be eroded or deposited as bedload or suspended load its thickness can be specified by the user or can be calculated by six different formulations e g hunziker 1995 günter 1971 fredsoe and deigaard 1992 van rijn 1993 wong and parker 2006 malcherek 2007 and others the default value is equal to 3 d 50 with d 50 the median diameter of sediment material contained in the active layer for the approach based on the classical hirano formulation the bed model can be discretised by a constant number of layers along the vertical direction since layers are allowed to be emptied the utilised number of layers at each mesh node can vary during a numerical simulation when more than one sediment class is specified the following cases arise i for a given initial bed stratification i e through a given number of layers n l a y an active layer is added inside this stratification at the beginning of the simulation in this case the total number of layers is n l a y 1 i i if the initial bed stratification is not provided the sediment bed is thus subdivided in two layers the active layer and a substrate layer located directly below in this case the total number of layers is equal to 2 to maintain the active layer thickness throughout the numerical simulation at each time step the following procedures are performed in the case of erosion the sediment mass is taken from the active layer therefore the sediment flux is transferred from the substratum first non empty layer below the active layer to the active layer note that the rigid bed algorithm is applied to the active layer i e only the sediment mass in the active layer is available at the given time step this is important as bedload transport rate and or the rate of entrainment for suspension are computed using the sediment composition available in the active layer if the erosion during the time step exceeds the mass of sediment available in the top layer this layer is fully eroded and a new erosion flux is computed using the composition of the layer underneath that is now the surface layer in the case of deposition the increased thickness generates a sediment flux from the active layer towards the first substratum layer the cvsm approach is based on the work of blom 2003 and blom et al 2008 and was adapted to gaia merkel and kopmann 2012 merkel 2017 the main idea is to keep the vertical grain sorting profile independently from the active layer sediment distribution the vertical grain sorting profile is stored for each sediment class with a vertical discretisation adapted to the profile at each time step the vertical sorting profile is changed in case of sedimentation a new vertical profile layer is added in case of erosion the vertical sediment stratification is changed for each eroded sediment class by using a modified version of the line generalisation algorithm proposed by douglas and peucker 1973 the maximal number of vertical profile layers can be kept from the vertical sediment stratification the sediment distribution of the active layer is calculated at each time step therefore this mixing process does not disturb the vertical sediment stratification 2 3 3 mixed sediments the hirano bed model algorithm introduced in the previous section has been modified to account for the presence of mud or sand mud mixtures mixed sediment consists of a mixture of n n c o 1 classes of non cohesive sediment sand and or gravel with n c o 1 classes of fine cohesive sediment non cohesive sediments are assumed to be transported by bedload and or suspension while cohesive sediment is transported only by suspension for mixed sediments the layer thickness results from the ratio of the volume of cohesive and non cohesive sediment contained in each layer if the cohesive sediment volume is less than the non cohesive sediment porosity default value equal to 40 the layer thickness only depends on the volume of non cohesive sediment in this case it is assumed that the cohesive sediment is contained within the pore space volume between the non cohesive sediment grains conversely if the cohesive sediment volume is larger than the non cohesive sediment porosity the layer thickness is computed from the non cohesive sediment volume plus the cohesive sediment volume minus the interstitial volume between non cohesive sediment classes the presence of high concentrations of cohesive sediment in the bed are known to prevent bedload transport from occurring van ledden et al 2004 therefore in gaia bedload transport is only computed if the mass fraction of cohesive sediment in the active layer is 30 2 3 4 consolidation processes consolidation processes are based on the semi empirical formulation proposed by villaret and walther 2008 which uses the iso pycnal and first order kinetics formulations consolidation of mud deposits is modelled using a layer discretisation where the first layer corresponds to the freshest deposit while the lower layer is the most consolidated layer sediment deposition from the water column is added directly to the first layer a rate or flux of consolidation is computed for each layer and for each class of cohesive sediment separately the values of the computed fluxes depend on the availability of each class in the layer considered in the case of mixed sediment the presence of non cohesive sediment in the stratigraphy of the mixture is considered to not alter the cohesive sediment consolidation 2 4 sediment exchanges at the water bed interface a unified framework has been developed for modelling the bed exchange processes at the water bottom interface of both cohesive and non cohesive sediment in 2d and 3d that eliminates unnecessary code duplication 2 4 1 erosion rate the erosion of sediment classes from the bed per unit area per unit time e kg sm 2 is computed by using the same formulation for both 2d and 3d simulations wu 2007 for cohesive sediment the erosion rate is computed for each sediment class following partheniades 1965 assuming the surface erosion rate is a linear function of the dimensionless excess shear stress 17 e m τ b τ c e τ c e with m the erodibility coefficient kg sm 2 and τ c e the critical shear stress for erosion n m 2 for non cohesive sediment classes the erosion rate is calculated for each size class as a function of the equilibrium sediment concentration at the interface between suspended load and bedload c b g l and the settling velocity of the sediment particles w s m s 18 e w s c b available formulations of the equilibrium sediment concentration at the interface are zyserman and fredsøe 1994 bijker 1968 van rijn 1993 and knaapen and kelly 2011 in gaia settling velocity values for non cohesive sediment particles are computed from the stokes zanke and van rijn formulations wu 2007 19 w s λ d 50 2 g 18 ν d 50 1 0 4 10 ν d 50 1 0 01 λ g d 50 3 ν 2 1 1 0 4 d 50 1 0 3 1 1 λ g d 50 otherwise where λ ρ s ρ ρ with ρ s the sediment density kg m 3 ρ the water density kg m 3 and ν is the water kinematic viscosity m 2 s assumed to be equal to 10 6 m 2 s by default constant or variable e g as a function of the sediment concentration particle settling velocity values can be also provided by the user for non cohesive sediments erosion is initiated when the bed shear stress due to currents and optionally waves exceeds a critical threshold for initiation of erosion of cohesive sediment it is assumed that all the cohesive sediment classes have the same combined mechanical behaviour and therefore the same value of critical shear stress is used for all classes nevertheless since the erosion flux depends on the relative availability of each sediment class the erosion fluxes can differ between classes when considering a mixture of sediment classes in the bed the composition in the surface active layer is taken into consideration when computing both the critical shear stress for erosion and the erosion flux this is achieved by combining the critical shear stresses for erosion for all the sediment classes cohesive and non cohesive according to le hir et al 2011 if the mass of cohesive sediment as a fraction of the mixture is 50 then the critical shear stress and erosion flux for cohesive sediment alone is used if the mass of cohesive sediment as a fraction of the mixture is 30 then the critical shear stress for non cohesive sediment is used and the erosion flux for non cohesive sediment is used if the mass of cohesive sediment as a fraction of the mixture is 30 and 50 then the values are interpolated between the previous values the erosion flux determined above is then distributed among the non cohesive and cohesive sediment classes according to their respective mass fractions in the bed mixture 2 4 2 deposition flux the deposition flux d kg sm 2 for each sediment class in suspension is calculated as the product of its class specific settling velocity w s and the suspended sediment concentration at the interface between suspended load and bedload c b g l computed from the respective coupled 2d or 3d suspended transport model wu 2007 20 d w s c b 1 τ b τ c d with τ c d the critical shear stress for deposition n m 2 by default erosion and deposition are allowed to occur simultaneously winterwerp et al 2012 this paradigm implies that sediment deposition takes place at all times regardless of the value of the bottom shear stress this is performed in gaia by assuming a very large value e g τ c d 1000 nm 2 the critical shear stress value can also be provided by the user 2 5 bed evolution the bed evolution is computed by solving the mass conservation equation for sediment or exner equation garcia 2008 expressed in terms of mass where bedload suspended load or both sediment transport modes can be considered simultaneously in its simplest form by considering bedload and suspended sediment transport of one class of non cohesive sediment this equation reads 21 1 λ ρ s z b t q m b d e where λ is the porosity of the bed material at the bed surface z b is the bed elevation above datum m and is the divergence operator numerical computation of sediment fluxes in dry mass as the conservative variable in exner equation minimises roundoff error particularly for the mass transfer algorithms used for the bed layer model in gaia a morphological factor that modifies the values of sediment flux for bedload transport and the erosion and deposition fluxes for suspended transport is implemented this simple accelerator is based on a constant scale factor applied to the divergence operator of the exner eq 21 for the case of bedload transport morgan et al 2020 roelvink 2006 when suspended transport processes are considered the constant scale factor is applied to the net exchange flux term at the bed water interface 3 overview of solution methods 3 1 numerical methods for the solution of processes in the water column the solution of the ade equation for sediments is performed by telemac 2d or telemac 3d where different numerical schemes are implemented the following methods are available method of characteristics e g see hervouet 2007 for its application in the telemac mascaret system which is not mass conservative but has the property of not generating new extrema monotone scheme supg streamline upwind petrov galerkin method brooks and hugues 1982 hervouet 2007 residual distribution method which is mass conservative and monotonous it includes different schemes 1 the n scheme roe 1987 2 the positive streamwise invariant psi scheme struijs 1994 3 the predictor corrector scheme ricchiuto and abgrall 2010 pavan et al 2016 4 the locally implicit psi lips scheme pavan 2016 edge based residual distribution method that is also mass conservative and monotonous it includes 1 n edge based residual distribution nerd scheme hervouet et al 2011 2 element by element residual distributive iterative advection eria scheme hervouet et al 2017 lips nerd and eria are suitable schemes for problems accounting for wetting and drying processes 3 1 1 initial conditions the spatial distribution of initial condition values of concentration can be specified for each sediment class expressed in g l 3 1 2 inflow boundary conditions at inflow boundaries concentration values can be specified i by imposing a fixed value i i computed according a near bed equilibrium concentration formula and i i i by providing time varying concentration values through an external ascii file for the 3d case the vertical profile of suspended sediments can be adopted assuming a constant rouse or user provided concentration distribution along the vertical direction 3 1 3 outflow boundary conditions at the outflow boundary the suspended load concentration gradient along the flow direction is set to zero 3 1 4 solid wall boundary conditions along banks and islands the no flux boundary condition is imposed in this case the suspended load concentration gradient is set to zero 22 c n 0 with n the coordinate in the direction normal to the boundary 3 2 numerical methods for the solution of processes in the bottom the exner equation 21 can be solved with two different approaches for simplicity we define below q q m b 1 a finite element centred n scheme as described in hervouet et al 2011 in a first step fluxes are computed with a distributive scheme and in a second step the positivity of the erodible layer is ensured by a limitation of the previously computed fluxes 2 a finite volume scheme built on a dual mesh as proposed by bristeau and coussin 2001 by defining q i j the solid discharge at the interface between adjacent elements i and j of the dual mesh and q i and q j the solid discharge computed by a bedload sediment transport formula see eq 5 at elements i and j respectively then for the centred scheme q i j q i q j 2 for the upwind scheme the decentering is chosen according to the sign of the solid discharge value projected at the interface between two cells q p r o j q p r o j i j n x i j q x i j n y i j q y i j with q x i j and q y i j the average of the solid discharge components on each side of the interface n x i j and n y i j are the components of the normal at the element s interface the flux at the interface is therefore calculated as q i j q i if q p r o j i j 0 q j if q p r o j i j 0 3 2 1 inflow boundary conditions at inflow boundaries the sediment discharge must be given at each node two different cases can be specified i equilibrium or i i user defined time series of bedload sediment discharge values constant or variable 3 2 2 outflow boundary conditions at the outflow boundary bedload does not require any particular boundary condition 3 2 3 solid wall boundary conditions at solid boundaries the bedload transport rate is set to zero 3 3 coupling strategy between different modules the solution of the discretised sediment transport equations is solved in gaia in a synchronous decoupled way meaning that flow and sediment transport processes are decoupled by using this approach the pseudocode of the implemented algorithm for the solution of the morphodynamics problem for bedload transport in the telemac mascaret modelling system is given in algorithm 1 in the following δ t t 2 d is the time step for hydrodynamics telemac 2d δ t g a i a is the time step for sediment transport and morphodynamics gaia δ t t o m is the time step for waves tomawac c p t 2 d g a i a is the coupling period for telemac 2d and gaia c p t 2 d t o m is the coupling period for telemac 2d and tomawac n i t is the number of iterations and t 0 and t f are respectively the initial time and total simulation time in algorithm 1 telemac2d wac and gaia are the main subroutines of modules telemac 2d tomawac and gaia respectively a similar procedure is executed when coupling with telemac 3d 4 examples hereafter five examples are provided to illustrate the capabilities of gaia at reproducing sediment transport and morphological processes in rivers coastal seas and transitional waters 4 1 flume experiment of a river bend the flume experiment proposed by yen and lee 1995 was chosen to assess the ability of telemac 2d gaia to reproduce the bed evolution in an alluvial channel bend under unsteady flow conditions this case might be relevant for river training projects in bends sediment transport is not only longitudinal but also transverse due to secondary flow typically cut bank erosion occurs at the outer bank and point bar deposition with finer material occurs at the inner bend in the experiment the final bottom evolution and the sediment sorting were measured along transverse cross sections located at points of observed maximum erosion and deposition around a bend in a flume data from run 4 of the laboratory experiment of yen and lee 1995 were used to validate the gaia results numerical simulations were performed by implementing the wu et al 2000 bedload transport formulation including the hiding exposure effects five sediment classes were considered with values ranging from 0 31 to 3 36 mm five vertical layers were adopted for the bottom stratigraphy the active layer thickness was computed with the formula of malcherek 2007 the constant critical shields parameter equal to 0 045 and a bed porosity equal to 0 375 were adopted slope effects were calculated according to talmon 1992 and soulsby et al 1996 secondary currents effects were incorporated in the simulations after engelund 1974 with the engelund parameter equal to 7 fig 1 left shows a comparison between contour levels of measured and simulated normalised bottom evolution and the normalised mean diameter at 90 around the bend fig 1 right numerical results show that the maximum deposition occurred more upstream approx 50 leading to a gentler slope at the 90 cross section in comparison with observations fig 2 shows the comparison between the measured and the simulated normalised bottom evolution for the maximum deposition at 90 and maximum erosion at 180 around the bend the simulated bed levels along the 180 cross section are in good agreement with the measured data simulation results follow the expected behaviour in an alluvial channel bend 4 2 suspended sediment distribution in the vertical direction according to the law of the wall von kármán 1930 for steady state fully mixed shallow water flows the shape of the vertical flow velocity profile is logarithmic the vertical turbulence in this situation may be accurately represented using a mixing length formula prandtl 1926 and with the assumption of a linear decrease in shear stress between bed and surface leads to a parabolic shaped eddy diffusivity profile if a single class of sediment with uniform settling velocity and infinite supply is then introduced the resultant steady state suspended concentration profile will follow the shape of the analytical rouse profile von kármán 1937 written as 23 c z c a h a z h z a w s β κ u where c z is the concentration g l at height z m above the bed c a is the concentration g l at reference height a m h is the height of the water column m w s is the settling velocity m s u is the bed shear velocity m s κ is the von kármán constant equal to 0 4 and β is the prandtl number representing the ratio between the turbulent transport for the sediment and the momentum assumed here to be equal to 1 a simple test case was developed to assess the ability of telemac 3d gaia to reproduce the rouse profile and consequently to solve suspended sediment transport problems the model geometry comprised a rectangular channel 5 km 0 5 km with a water depth of 10 m divided into 21 sigma planes bed friction was prescribed using a nikuradse roughness length of 0 01 m and the lateral boundaries were prescribed with no friction the vertical turbulence model was chosen to be consistent with the law of the wall assumption i e mixing length model at the entrance to the channel a discharge of 5000 m 3 s was applied generating a depth mean flow speed of 1 m s sediment was introduced with a constant concentration through the vertical of 0 1 g l and with a settling velocity of 0 001 m s during the simulation the combined effects of friction vertical turbulence and settling velocity led to the gradual development downstream of equilibrium profiles for eddy diffusivity k z flow velocity u and suspended concentration c fig 3 the results validate the ability of telemac 3d gaia at reproducing the rouse profile and in doing so also validate the modelled flow velocity profile mixing length turbulence model and settling velocity calculations 4 3 middle lower rhine river reach for a 46 5 km long stretch of the middle lower rhine river between neuss and duisburg in germany rh km 730 776 5 a coupled telemac 2d gaia model was built to investigate artificial bedload supply measures this case might be relevant for projects related to navigability and fluvial transportation the model was calibrated for a period of 6 years of the natural hydrograph from 2000 07 11 to 2006 06 22 the computational domain was discretised with a mesh of approximately 260 000 nodes and typical element sizes in the range 3 17 m in the main channel and up to 40 m in the floodplains this discretisation allowed an appropriate reproduction of the groyne geometry as well as the analysis of artificial bedload supply bed evolution and bedload transport fig 4 shows the river topography inside the model boundaries the large amplitude bends in this river stretch strongly influence hydrodynamics and morphodynamics processes furthermore a tendency for long term erosion is observed erosion mitigation is applied using sediment management operations including artificial bedload supply as well as dredging and disposal activities all sediment management actions during the simulation period were considered in the model by coupling with the nestor module numerical simulations were performed with a hydrodynamic time step of 4 s a coupling period equal to 10 and a morphological factor of 4 the adopted parameterisations for friction and turbulence were nikuradse friction law and elder turbulence model respectively ten sediment classes were defined with the hirano multi layer model consisting of 3 layers and a constant active layer thickness equal to 0 1 m in the numerical simulation suspended load was assumed to be negligible relative to bedload and hence only bedload transport was considered the meyer peter and müller transport formula was adopted with the karim holly and yang hiding exposure formulation karim et al 1987 the soulsby and talmon slope effect formulation was also included secondary currents were accounted for using the morphodynamic approach of engelund with the radius of curvature provided in an additional file for the model calibration a moving average of the river bottom was calculated considering only the mean channel and a moving longitudinal section of 1100 m the averaged simulated bottom evolution is generally in a good agreement with the measurements see fig 5 nevertheless in the last 10 km of the stretch the numerical model predicts aggregation instead of erosion tendencies fig 6 presents the comparison of the simulated and measured river bottom at three chosen cross sections the cross sections rh km 738 1 and 743 1 are located before and after the nearly 180 bend while the cross section rh km 761 1 is positioned before the last bend in the model the positions of the cross sections are shown in fig 4 the cross sections 738 1 and 761 1 show a typical outer bank erosion and point bar deposition profile despite some discrepancies between observed and simulated bed evolution results at section 743 1 predictions of the riverbed evolution are globally well reproduced by the numerical model fig 7 illustrates the simulated river bottom and mean diameter at the sharp bend near rh km 741 typically at the outer banks the river bottom consists of coarser material while finer material is located at the inner banks and middle grounds which is also well represented by the numerical model 4 4 the gironde estuary the macrotidal gironde estuary is located in south west france covering a surface of 635 km 2 from the bay of biscay to 170 km landward see fig 8 this transitional water body is characterised by a complex geomorphology high suspended particulate matter spm concentrations of up to 20 g l and a heterogeneous bed composition understanding the flow sediment dynamics and morphological changes of this coastal estuarine system would be important for a more efficient planning of ship routes and management of underkeel clearance in navigation channels for example numerical simulations of this transitional water body were performed by coupling telemac 3d with gaia for a time period spanning 90 days salinity was considered as a passive tracer with 35 psu imposed on the offshore boundary and 0 psu for both tributaries namely the garonne and dordogne rivers the prandtl mixing length model combined with a damping function lehfeldt and bloss 1988 was selected for the turbulence closure relationship the computational domain was extended offshore up to 70 km from the mouth and about 200 km alongshore such that the boundary condition for salinity was far enough from the mouth so it did not influence the salinity within the estuary inland the model extended up to 180 km to the limit of the tidal propagation tides and surges were imposed at the offshore open boundary the tidal signal was extracted from the north east atlantic tidal model of legos huybrechts et al 2012 for the simulated period flowrates were imposed at both garonne and dordogne rivers from a time series data provided by banque hydro 2020 the digital elevation model containing the bathymetric information was provided by the port of bordeaux the horizontal mesh comprised 58 250 nodes with typical element sizes ranging from 4 km on the offshore boundary to 30 50 m inside the tributaries the mesh discretisation along the vertical direction consisted of 9 planes spaced using a geometric progression with a higher mesh density near the bottom the mesh was refined along the navigation channel in order to have at least 5 nodes along the channel width both non cohesive and cohesive sediments were included in the simulation the coarser grained non cohesive sediment was considered to be transported by bedload using the meyer peter müller formula chini and villaret 2007 for the finer cohesive sediment two classes were considered the process of consolidation was included in the bed model using a multi layer approach with experimental data van 2012 used to calibrate the consolidation transfer rate coefficient orseau et al 2021 the erodibility coefficient m was set to 0 0015 kg s m 2 for settling velocity time and space varying values were computed for both mud classes as a function of the spm and salinity see van maanen and sottolichio 2018 the bed roughness was estimated using the van rijn 2007b formula according to the flow characteristics and predominant sediment diameter the sediment composition was estimated from the available data two simulations were performed considering high and low river discharges corresponding to winter and summer periods respectively the ability of the model to reproduce the spm pattern at observation stations p2 p3 and p5 is illustrated in figs 9 and 10 see fig 8 for locations the main variability is captured well by the model although the spm levels are slightly underestimated during the neap tide combined with low flow rate results from the numerical model were further used to identify the location of the turbidity maximum values during winter and summer periods see fig 11 4 5 the gironde estuary s mouth located on the sw atlantic coast façade the 20 km wide tidal mouth of the gironde estuary opens to the atlantic ocean in this zone the combined influence of waves and tides has led to the development of a complex hydro sedimentary dynamics where the net seaward transport varies with the tidal amplitude allen and castaing 1973 at the estuary mouth see fig 8 strong tidal currents and high energy atlantic swells induce important displacements of coarse grained sediment deposits and severe erosion along its coastline howa 1997 these processes lead to the development of numerous sandbanks in the intertidal and subtidal areas stéphan et al 2019 to be able to capture the flow sediment interactions at the estuary s mouth all relevant physical drivers were considered in a three way coupled simulation telemac 3d tomawac gaia i e tides waves wind atmospheric pressure and river discharges the computational domain was discretised with a finite element mesh consisting of 106 000 nodes and 207 000 elements covering a surface of approximately 15 000 m 2 along the vertical direction 5 layers were used typical element sizes varied from 3 km on the offshore boundary down to 50 meters in specific areas of the mesh to capture the morphology of channels and shoals of the estuary bathymetric information in the study area was incorporated into the finite element mesh from a digital elevation model built from a dataset collected during a field campaign performed in 2015 shom 2016 the initial bathymetry is showed in fig 12 a hydrodynamics forcing accounting for wave and current interactions was implemented by coupling the modules telemac 3d and tomawac along the offshore boundary harmonically derived water levels were prescribed using 33 tidal constituents obtained from interpolation of the values available in the global model fes 2012 carrère et al 2013 daily flow rates of river discharges were extracted from banque hydro 2020 and aquitaine 2020 ranging from 35 m 3 s to 2000 m 3 s in the dordogne river and from 70 m 3 s to 6000 m 3 s in the garonne river the boundary conditions for the wave model were extracted from a hindcast data set for the assessment of sea states climatologies homere developed by ifremer boudière et al 2013 they consisted of significant wave height wave period direction and spread in addition the model was forced with time series of atmospheric pressure fields from the fifth generation ecmwf atmospheric reanalysis model era5 downloaded from the copernicus climate change service c3s climate data store hersbach et al 2020 for gaia the representative sand diameter selected for the study area was d 50 0 30 mm the characteristics of the ocean floor louazel et al 2021 were used to determine the bottom friction for waves and hydrodynamic propagation the sediment transport formulation transpor2004 walstra et al 2005 was implemented in gaia the interaction due to the combined wave current forcing was considered bedform roughness length for waves and current were computed and sediment transport fluxes accounting for the influence of both suspended and bedload transport were computed time steps were set equal to 10 s for both telemac 3d and tomawac modules for gaia a morphological acceleration factor equal to 5 was used hydrodynamics was validated between 2009 and 2010 against 9 tidal gauges and waves propagation and generation were validated using data from a measurement campaign of 2 months in the estuary s mouth where waves meet strong currents the sediment transport formula was calibrated with a simulation of one year and was compared to the mean annual longshore sediment transport deduced from the shoreline evolution during the last decade results are not shown here morphodynamic simulations were performed for a period spanning 5 years bed evolution results fig 12 b show i the presence of a morphological stable zone in the area surrounding the coast for the considered time period with bed evolution values comprise within the range 0 5 m and 0 5 m i i sediment accretion of approximately 5 5 million m 3 for 5 average years see fig 12 c comparable to the 4 6 million m 3 resulting from a bathymetric differential in the polygon indicated in fig 12 a i i i a migration of an oblique sandbar in the central zone of fig 12 b oriented towards the northwest direction related to the wave induced littoral drift and i v a general tendency to sediment erosion at the main and secondary channels at the estuary s inlet 5 conclusions and outlook gaia the new code structure available in the telemac mascaret modelling system provides an integrated framework for solving sediment transport and morphological evolution problems developments have been undertaken by considering the large number of variables to be considered when dealing with rivers coastal seas and transitional water applications namely different spatial dimensions two and three dimensional flow states steady quasi steady unsteady flow regimes subcritical critical supercritical sediment characteristics cohesive non cohesive mixed sediment classes uniform graded and transport modes bedload suspended load total load and states equilibrium non equilibrium continuous developments performed in the telemac mascaret system allow access to efficient numerical solvers serial or multiple processors distributed memory architectures and easy to use api wrappers allowing a full control of numerical simulations in addition gaia can easily be expanded and customised to particular requirements by modifying user friendly easy to read and well documented fortran 90 files moreover theoretical aspects and validation test cases are documented and continually updated so that the quality of the source code remains assured in progress model improvements are i the use of highly exploitable and transparent techniques for coupling sediment transport with for example pollutant transport models to better quantify the transport and fate of chemicals in the aquatic environment i i the improvement of strategies for morphodynamic updating e g the enhancement of the various ways of accelerating the computation of sediment and morphological changes and i i i the application of stochastic methods in morphodynamic modelling in particular regarding uncertainty quantification data assimilation and automatic calibration techniques 6 software availability the telemac mascaret modelling system is distributed under gnu general public license gpl version 3 29 june 2007 the telemac mascaret modelling system can be downloaded from the gitlab repository https gitlab pam retd fr otm telemac mascaret additional information new releases user s forum installation notes etc can be accessed from the telemac mascaret s webpage http www opentelemac org user s manuals theoretical guides and miscellaneous information are available from the wiki http wiki opentelemac org declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are indebted to their former co workers yoann audouin and agnès leroy for helpful discussions and numerical informatics work essentially contributing to the development of the gaia module the authors would like to acknowledge the contributions of boris glander regarding the development of the sediment dredging and dumping module nestor sébastien bourban is kindly acknowledged for providing advice and support during the manuscript writing we sincerely appreciate the reviewers comments and suggestions which have been very helpful in improving the manuscript this research was partially supported by edf r d france through the project plateforme hydro environnementale 2 phe2 
