index,text
26380,optimal land use allocation with the intention of ecosystem services provision and biodiversity conservation is one of the key challenges in agricultural management optimization techniques have been especially prevalent for solving land use problems however there is no guideline supporting the selection of an appropriate method to enhance the applicability of optimization techniques for real world case studies this study provides an overview of optimization methods used for targeting land use decisions in agricultural areas we explore their relative abilities for the integration of stakeholders and the identification of ecosystem service trade offs since these are especially pertinent to land use planners finally we provide recommendations for the use of the different optimization methods for example scalarization methods e g reference point methods tabu search are particularly useful for a priori or interactive stakeholder integration whereas pareto based approaches e g evolutionary algorithms are appropriate for trade off analyses and a posteriori stakeholder involvement keywords agricultural land use allocation multi criteria decision analysis mcda multi criteria optimization stakeholder integration trade off analysis constraint handling 1 introduction humans have been changing landscapes for millennia by converting natural areas for agricultural production and settlement delcourt and delcourt 1988 as a result 40 50 of the world s land surface had been visibly transformed for these purposes by the 20th century western 2001 many of the different land uses are conflicting for instance there is agricultural and timber production on one side competing with space for urban settlements or protected areas on the other side all these anthropogenic usages impact the provision of ecosystem services ess and therefore directly affect for example soil quality as well as water quantities and quality fontana et al 2013 meanwhile natural areas provide habitats for wildlife and are especially important for the protection of endangered species behrman et al 2015 biodiversity loss has been directly linked to land use changes sala et al 2000 and population growth as well as increases of agricultural land use have been labelled the biggest threat to biodiversity and ess behrman et al 2015 one way to address biodiversity loss is to integrate ess into systematic conservation planning faith 2015 and re allocate land uses in order to support the multifunctionality of landscapes sustainable land use allocation therefore seeks to take into account the current and future provision of ess and biodiversity in order to determine so called optimal land use allocations in general land use allocation also sometimes referred to as land use planning stewart et al 2004 is a type of resource allocation and can be defined as the process of allocating different activities or uses e g agriculture residential land recreational activities conservation to particular areal units within a region cao et al 2012 agricultural land use allocation specifically deals with the allocation of species and activities to areas in agricultural landscapes memmah et al 2015 decision support research within the field of natural resources management has relied heavily on multi criteria decision analysis mcda and its corresponding tools mendoza and martins 2006 in this paper we provide a detailed review of mcda and focus in particular on one branch of mcda optimization techniques since land use allocation problems have been widely formulated as mathematical optimization problems these problems typically consider multiple mostly conflicting objectives and aim to minimize the trade off between them liu et al 2013 porta et al 2013 these can include trade offs between various ess such as provisioning and regulating services but also between ess and biodiversity a trade off describes the amount that has to be given up of one ess in order to increase the provision of another rodríguez et al 2006 for example the intensification of agricultural production may reduce water quality due to a greater use of fertilizers and pesticides and the resulting nonpoint emissions of pollutants from the agricultural fields the main task is thus to find the right balance between the usage of different ess solving complex real world land use allocation problems remains a key research challenge fowler et al 2015 additionally recent applications underline the need for methods that allow for increased stakeholder involvement eikelboom et al 2015 stewart et al 2004 uhde et al 2015 this is particularly important since agricultural land use allocation involves many competing actors such as farmers farmers associations environmental agencies land planners and economists memmah et al 2015 participatory approaches thus help to find solutions that achieve biophysical objectives but also consider the different perspectives and preferences of various stakeholders groot and rossing 2011 land use allocation problems can greatly differ in their mathematical formulation and therefore require different optimization techniques see section 2 2 however the choice of a technique is often not guided by the characteristics of a problem but depends on the experience of the reseacher in charge or on historical usages memmah et al 2015 while there exist some reviews about mcda approaches and their applicability particularly in forest management mendoza and martins 2006 uhde et al 2015 current literature lacks guidelines for how to choose the best suitable optimization technique for a particular agricultural land use allocation problem therefore this paper aims to fill this gap by providing a review of current mcda optimization techniques and their applicability for land use allocation problems we specifically focus on agricultural landscapes and on studies that aimed to achieve objectives related to ess and biodiversity the following sections provide a review of optimization approaches that have been used in land use management for an overview we first classify multi objective optimization within the broader field of decision support techniques giving an introduction to mcda then we evaluate different multi criteria optimization methods in terms of their ability to integrate stakeholder opinions and identify trade offs between ess and biodiversity furthermore we mention how constraints can be handled the suitability of the optimization approaches for different types of land use allocation problems is discussed before we provide a short conclusion and give directions for further research 2 solving land use allocation problems with multi criteria decision analysis mcda 2 1 an overview of mcda mcda has been widely used to perform mathematical optimization in order to analyze multi objective decisions and incorporate the varying opinions of decision makers collins et al 2001 mcda addresses land allocation problems in a more realistic way than single objective approaches since in practice these problems consist of multiple conflicting objectives antoine et al 1997 especially when multiple ecosystem services are taken into account birkhofer et al 2015 furthermore mcda methods can combine ecological objectives with social and economic criteria and are able to consider non market values of ess therefore they are very popular and frequently used in ecological economics fontana et al 2013 uhde et al 2015 van huylenbroeck 1997 most of the literature classifies multi criteria optimization either within the broader field of decision support systems e g myllyviita et al 2011 or within mcda directly e g aerts et al 2003 therefore we first provide an overview of the linkage between the two fields and where multi criteria optimization is set amongst these see fig 1 mcda is one of many decision support techniques which can be divided into qualitative quantitative and hybrid methods qualitative methods e g interviews voting focus on structuring a problem they also help to define initial goals and to evaluate stakeholders opinions myllyviita et al 2011 uhde et al 2015 cost benefit analysis cba and mcda including mathematical optimization techniques belong to the group of quantitative methods that use numerical information in order to evaluate a number of decision alternatives finally hybrid methods are composed by the combination of different approaches see uhde et al 2015 for an overview of hybrid mcda methods in forest management mcda methods can be classified in different ways mendoza and martins 2006 belton and stewart 2002 suggest distinguishing three categories i value measurement models ii goal aspiration or reference level models and iii outranking models instead malczewski 1999 and zimmermann and gutsche 1991 distinguish between multi attribute decision making madm and multi objective decision making modm see fig 1 madm deals with the evaluation of a finite number of alternatives that are previously known to the decision maker therefore they require discrete mcda methods an example of a hybrid combination of madm techniques can be found in fontana et al 2013 the authors evaluated three land use alternatives i e larch meadow spruce forest and intensive meadow to determine their ability of providing certain ess first they derived the weights of six ess with a stakeholder questionnaire and the analytical hierarchy process ahp saaty 1988 later they applied an outranking method in order to evaluate the different alternatives more information about madm techniques like the outranking methods electre and promethee multiattribute utility theory maut and ahp can be found in belton and stewart 2002 and figueira et al 2005 since land use allocation problems usually include a range of competing objectives it might often be impossible to create a small set of scenarios that would cover all possible solutions bishop 2013 groot and rossing 2011 besides an optimal solution of sustainable land management might be located between two distinct scenarios seppelt et al 2013 in this case the application of design techniques can help to avoid this problem aerts and heuvelink 2002 multi criteria design problems are of a continuous nature and handled within modm here alternatives are either not known in advance or there are so many that the problem cannot be solved with evaluation methods anymore these problem types can be solved by applying mathematical optimization aerts and heuvelink 2002 uhde et al 2015 the focus of this paper then is to provide a detailed review of optimization techniques used in land use allocation which will be given in the following section multi criteria decision aid mcda is yet another perspective from which to solve multi criteria problems and further information can be found in bana e costa 1990 and zimmermann and gutsche 1991 according to them the main difference between classical madm and modm approaches is that mcda also incorporates vague incomplete inconsistent and subjective information also instead of a single optimal solution it provides a set of acceptable alternatives fuzzy programming and outranking methods such as electre and promethee figueira et al 2005 are but a few examples for mcda methods the classification in fig 1 must not be seen as strict there are some approaches that cannot be assigned fully to any of the categories for example some mcda methods may simultaneously contain portions from madm and modm zimmermann and gutsche 1991 a well structured overview of mcda applications in forestry and natural resources management can be found in mendoza and martins 2006 furthermore myllyviita et al 2011 provide a comparative review of studies that used mcda design techniques optimization cba and hybrid methods in sustainable forest management 2 2 optimization methods the process of land use allocation includes a series of individual steps which are highlighted in fig 2 belton and stewart 2002 groot and rossing 2011 talbi 2009 for the optimization the problem needs to be identified and clearly formulated before it can be modelled in this context we identified three important points that should be taken into account i mathematical problem formulation objectives constraints decision variables and problem type e g linear non linear discrete e g binary combinatorial continuous deb 2001 ii desired output required input problem scale e g local global seppelt et al 2013 amount and type of available data e g land use maps information about topography hydrology soil quality and if a trade off analysis is needed iii stakeholder involvement before a priori during interactive or after a posteriori the optimization process memmah et al 2015 all of these factors determine the size and complexity of the problem and have an influence on the choice of a suitable optimization method and on computation time at this point it should be considered that in the end the quality of the optimization result does not only depend on the performance of the selected algorithm but also on the conceptual design of the optimization problem particularly if there have been simplifications in the model formulation moilanen 2008 optimal solutions can then be used by decision makers to inform and support the implementation of new land use strategies the selection of optimization methods presented in this section is mainly based on the fact that spatial land use allocation problems are mostly multi objective combinatorial optimization problems porta et al 2013 that may often be non linear cao et al 2012 liu et al 2016 memmah et al 2015 these problems are usually complex and include a large number of alternative solutions requiring high computation times porta et al 2013 combinatorial problems as a type of discrete optimization problems are typically solved by applying local search algorithms such as simulated annealing tabu search genetic algorithms and ant colony optimization aarts and lenstra 2003 colorni et al 1996 these and other methods will be presented in the following in the rare case of a continuous problem formulation standard methods like the multi objective simplex algorithm figueira et al 2005 can be used for linear problems see sadeghi et al 2009 for an example but again if the problem is non linear then heuristic optimization methods are needed multi objective optimization is a useful tool for the evaluation of trade offs among conflicting objectives trade offs are represented by the pareto frontier in some studies this is also called the efficiency frontier or production possibility frontier polasky et al 2008 the pareto frontier is a set of optimal solutions to the respective multi criteria optimization problem assuming maximization a feasible solution to the optimization problem is said to be pareto optimal if there is no other feasible solution that would increase one of the objective function values without simultaneously decreasing another coello coello et al 2007 for trade off analyses it is sometimes necessary to obtain the whole pareto frontier particularly for visualization purposes therefore along with the presentation of different optimization algorithms we will also mention how trade off curves can be identified table a 1 in the appendix gives a selection of studies from the fields of general and agricultural land use allocation and a few from other research areas for each study it also includes information about the moment of stakeholder integration whether trade offs were determined and which ess were taken into account one approach for solving multi objective optimization problems is to define one objective function and add any additional objectives as constraints ε constraint method ehrgott 2005 then single objective algorithms can be applied for example van butsic and kuemmerle 2015 aimed to maximize agricultural production while minimizing species loss setting agricultural production as the objective function and target constraints for the species of concern they determined trade off curves between agricultural yield and the species population size by solving the problem multiple times with different targets nonetheless a high variety of multi objective algorithms is available that can simultaneously account for multiple conflicting objectives for an overview of multi objective optimization methods see chapter 17 of figueira et al 2005 generally multi objective optimization techniques follow two different approaches scalarization methods and pareto based methods cao et al 2012 madavan 2002 and these will be outlined below 2 2 1 scalarization methods scalarization methods combine multiple objective functions into a single objective scalar function miettinen and mäkelä 2002 the optimization problem is then solved with a single objective optimization algorithm which creates a single optimal solution to the optimization problem here again the selection of the algorithm depends on the problem type for spatial land use allocation problems heuristics like the greedy algorithm cormen 2007 simulated annealing genetic algorithms etc bozorg haddad et al 2017 are usually applicable there are two main methods to parameterize a problem either to maximize or minimize the weighted sum of all objectives by using weighting coefficients that specify the relative importance of each objective or by using a reference point based method wierzbicki 2000 weighted sum approaches provide pareto optimal solutions for convex solution sets the pareto frontier can be obtained by changing the coefficients of the scalar function and re solving the problem pareto optimal solutions of non convex parts however cannot be found using this method for an example see caramia and dell olmo 2008 kennedy et al 2016 used a weighted sum approach in combination with a greedy algorithm based on previous work by polasky et al 2008 they optimized three objectives agricultural production water quality and biodiversity for a watershed in an agricultural area in southeastern brazil by varying the weights of the individual objectives they obtained trade off curves between agricultural production and either water quality or biodiversity for different problem settings goal programming gp is a reference point method that guides an algorithm like an evolutionary deb and sundar 2006 or genetic algorithm deb 1999 for instance towards a solution that lies in the decision maker s preferred region of the solution space for this purpose the decision maker defines goals i e desired values for each objective then the distance between the goal vector and an attainable vector of the solution space is minimized and the optimum is a feasible solution that is closest to the goal vector defining and striving for a goal seems quite intuitive from a psychological perspective however from a mathematical viewpoint the minimization of a norm a distance measure cannot guarantee that the gp algorithm will find a pareto optimal solution romero 2014 therefore the more general reference point rp method has been developed instead of using a norm this approach minimizes a so called achievement function miettinen et al 2008 if norm minimization is avoided reference point approaches can obtain the pareto frontier even for non convex solution sets when solving the problem multiple times with different reference points wierzbicki 2000 stewart and janssen 2014 used a reference point method in combination with a genetic algorithm in order to solve a non linear combinatorial optimization problem the key objectives were the profitability of intensive agriculture maximization of the visual quality of the landscape including landscape perception cultural historic value and recreational value and maximization of the natural value of the area including meadow birds species rich grasslands and marsh birds as a result they generated land use maps that served as a basis for negotiating optimal land use strategies in a case study area in the netherlands this paper extends earlier work by stewart et al 2004 and janssen et al 2008 and forms the basis of work by eikelboom et al 2015 another scalarization approach is tabu search ts which is usually used in combination with a local search algorithm boussaïd et al 2013 ts was initially developed for single objective combinatorial optimization problems the algorithm works on an iterative basis by looking for an improved solution in the neighbourhood of the current solution in doing so ts uses a short term memory i e the tabu list where recently visited solutions or one or more of their attributes are recorded all potential new solutions that are on the tabu list cannot be visited again at this stage of the search this is to avoid endless cycling and prevents the algorithm from getting stuck at a local optimum boussaïd et al 2013 more information about ts can be found in glover and laguna 2013 ts can also be applied for multi objective optimization problems qi and altinakar 2011 used ts in order to optimize agricultural land use with integrated watershed management they considered three objective functions and combined them into a single objective function using weights to reflect the relative importance of each objective also behrman et al 2015 used weighting while solving their optimization problem with consnet ciarleglio et al 2009 which is based on a ts algorithm in their study the overall aim was to identify the trade offs between converting land to switchgrass for biofuel production agriculture and biodiversity in order to obtain optimal trade offs among the objectives they varied the weights that were applied to each of the three categories in addition ts approaches that are not based on scalarization but search directly for the pareto optimal set have been developed jaeggi et al 2005 however these methods have yet to be used in land use optimization simulated annealing sa kirkpatrick et al 1983 also applies weighting in order to combine multiple objective functions to a single objective function therefore as with ts methods trade offs can be determined by solving the problem multiple times with different weights the algorithm itself mimics the physical process of heating metal and cooling it again starting from an initial solution sa randomly chooses a new solution in a pre defined neighbourhood temperature is a parameter that is reduced over time when this temperature parameter is sufficiently high even solutions that decrease the objective function value can be accepted this prevents the algorithm from getting trapped in local optima at lower temperatures the algorithm accepts only improving new solutions and terminates once a stopping criterion is met aerts and heuvelink 2002 used sa to minimize development costs and compactness costs for the restoration of a mining area in spain the algorithm has also been applied for optimizing agricultural land use in santé riveira et al 2008 their aim is the optimal allocation of 13 land uses all different types of crops in a study area in spain all land uses were grouped into five use groups fodder cereals intensive agricultural crops productive forest and protective woodland they set the objective function to be a linear combination of three objectives maximize land suitability for the uses allocated to them maximize compactness of the total area assigned to a particular use and maximize compactness of the total area assigned to a particular group of uses the problem was solved for 11 different sets of weights 2 2 2 pareto based methods pareto based methods generate multiple pareto optimal solutions simultaneously and are able to provide the whole pareto frontier as a result of the optimization there is a wide range of evolutionary algorithms ea including genetic algorithms ga which have been used in land use optimization eas are inspired by biological evolution they begin with an initial population of solutions and use concepts such as selection crossover and mutation in order to create the next generation of solutions the fitness of a solution evaluates how good it fulfills the problem criteria and determines whether or not it will be selected as a parent for the next generation the algorithm terminates once a predefined stopping criterion is met memmah et al 2015 lautenbach et al 2013 coupled a watershed model called the soil and water assessment tool swat arnold and fohrer 2005 with the non dominated sorting genetic algorithm ii nsga ii deb et al 2002 in order to analyze trade offs between bioenergy crop production food crop production water quantity and water quality in a case study area in central germany similarly fowler et al 2015 coupled a multi objective genetic algorithm from the dakota optimization suite adams et al 2014 with the modflow fmp2 software schmid and hanson 2009 which simulates the integrated supply and demand components of irrigated agriculture they optimize the selection of three different crops based on trade offs between agricultural revenue water usage and the deviation from actual yield and demand yield for each crop in an artificial study area in addition to maximizing agricultural profits groot et al 2007 also consider nature conservation and landscape quality by minimizing the loss of nutrients to the environment maximizing the nature value of fields and borders i e species abundance in grass swards and hedgerows and maximizing the variation of the landscape i e species presence and hedgerow allocation they explore the trade offs between these objectives with an evolutionary strategy algorithm of differential evolution storn and price 1997 and provide trade off curves of gross margin with either plant species number landscape value or nitrogen loss the underlying data is from a case study area in the netherlands other eas that gained much popularity in land use allocation during the last years are artificial immune systems huang et al 2012 and particularly swarm intelligence si algorithms see yang 2014 for an introduction such as ant colony optimization nguyen et al 2016 the artificial bee colony algorithm yang et al 2015 and particle swarm optimization pso liu et al 2016 ma et al 2011 the latter was developed to solve continuous optimization problems however liu et al 2016 present a method that uses pso for a binary multi objective optimization problem in general si algorithms mimic the collective behaviour of single agents in a decentralized and self organized system for example pso imitates the social behaviour of fish schooling and flocking of birds kumar and minz 2014 2 2 3 hybrid methods hybrid optimization approaches combine two or more optimization methods within a single framework to facilitate the search for the pareto frontier an example of a hybrid optimization approach applied to the calapooia river basin in oregon usa can be found in whittaker et al 2017 their approach couples the swat watershed model with data envelopment analysis dea cooper et al 2004 an economic linear optimization model within a bilevel optimization framework that uses nsga ii bilevel optimization problems consist of two nested optimization levels and each level contains its own set of objectives in this example the upper level is a government agency that seeks to maximize the total farm profit of the entire watershed and minimize the nitrogen loading at the watershed s outlet the lower level consists of farmers that seek to maximize their individual profits using dea the two levels are linked because the upper level sets tax rates for the use of nitrogen fertilizer by the lower level and the lower level s fertilizer use decisions ultimately impact the amount of nitrogen loading that occurs at the watershed s outlet nsga ii is used to find the optimal set of tax rates to optimize all objectives at both levels whittaker et al 2017 highlight the pareto frontier trade offs for the upper between total accumulated profit of the farmers and nitrogen loading at the watershed s outlet other studies like bostian et al 2015 and barnhart et al 2017 have utilized a similar methodology to target agri environmental policy to promote best management practices in order to achieve environmental benefits according to memmah et al 2015 the application of hybrid metaheuristics to land use allocation problems is still rare nevertheless some more examples e g a hybrid pso can be found in their study along with general information about the hybridization of algorithms apart from the methods presented above fuzzy programming from the field of multi criteria decision aid see fig 1 can also be applied to solve land use allocation problems for an example we refer to wang et al 2004 2 2 4 constraint handling as indicated at the beginning of this section constraints are an important part of the mathematical formulation of land use allocation problems they limit the space of feasible solutions by reflecting e g environmental social and political limits such as the total area that can be allocated to each land use stewart and janssen 2014 water quality and water demand supply constraints wang et al 2004 restrictions on nutrient input groot et al 2007 or biodiversity targets schröter et al 2014 to name only a few examples handling real world constraints is one of the most challenging tasks in the optimization process michalewicz and fogel 2004 especially since they can increase the computational complexity of a problem all of the methods presented in this paper support constraints though the way of handling them depends on the algorithm used the most popular constraint handling method for heuristics is using a penalty function that degrades the fitness value of an infeasible solution chehouri et al 2016 furthermore the use of feasibility operators which create feasible only child solutions deb 2001 and repairing infeasible solutions coello coello et al 2007 are common methods too linear and quadratic programming generally apply lagrange multipliers for constraint handling and linear continuous problems can be solved by the simplex algorithm which includes constraint handling cavazzuti 2013 more information about lagrange multiplier methods can be found in bertsekas and rheinboldt 2014 and for constraint handling methods for linear and non linear continuous single objective problems we refer to cottle and thapa 2017 furthermore table 1 includes information about constraint handling for the different methods presented in this paper 3 stakeholder integration in order to make land use planning more applicable to real world problems stakeholders are increasingly integrated into the decision making process memmah et al 2015 they can state their interests and ambitions but also provide expert knowledge on current and future developments this is especially valuable in agricultural areas since for example trends in cultivation techniques consumer demand but also policies need to be taken into account furthermore stakeholders might provide a deeper insight into the potential of certain areas which would help to facilitate the decision on where possible land use changes can be implemented in all cases it is important to find representative stakeholders first harrison and qureshi 2000 for example for agricultural land use allocation problems there should be a sound combination of people with different backgrounds and point of views like farmers conservationists or employees of state ministries hauck et al 2016 this should guarantee a dynamic discussion and prevents the optimization from being biased by too narrow perspectives however if the stakeholders cannot agree on a mutually consistent set of preferences malczewski 1999 multiple analyses of the problem might be necessary malczewski 1999 furthermore important technical terms like for example trade offs land sharing sparing etc and the optimization method applied must be communicated well to the stakeholders in order to create a mutual understanding otherwise there might be misunderstandings or the optimization might look like a black box which could create mistrust bishop 2013 the moment stakeholders are involved in the optimization process has an influence on the problem formulation and affects the choice of a suitable optimization method preferences can be included either before a priori during interactively or after a posteriori the optimization initially proposed by cohon and marks 1975 it is nowadays common practice to classify multi criteria optimization methods according to these three categories coello coello et al 2007 scalarization is an a priori method where the algorithm finds a solution that best meets the stakeholder s preferences these are represented by the objective function weights or optimization goals examples from the studies presented above are aerts et al 2003 behrman et al 2015 and santé riveira et al 2008 however if scalarization is used to calculate the pareto frontier by changing weights goals like in kennedy et al 2016 it is considered as an a posteriori approach additionally pareto based methods and with them the majority of evolutionary multi objective algorithms fall into this category deb and köksalan 2010 these algorithms provide a whole set of pareto optimal solutions and given these alternatives stakeholders can then select those that fit their preferences best see for example fowler et al 2015 groot et al 2007 and lautenbach et al 2013 for the case of a posteriori involvement methods from the field of madm fig 1 can be applied in the selection process however a priori and a posteriori approaches do not consider that it might be difficult for the stakeholders to express their preferences analytically and that values can change over time and with growing experience and learning coello coello et al 2007 therefore interactive approaches can be favourable they allow stakeholders to articulate preferences in a progressive way that is stakeholders are able to adjust them after each iteration of the optimization this step by step integration of preferences guides the optimization towards the relevant parts of the pareto frontier and may help to reduce computational time meignan et al 2015 such real time interaction however requires short computation times of intermediate solutions stewart et al 2004 this can also be challenging since the computational time to complete a single optimization should preferably be less than a minute bishop 2013 stewart et al 2004 reference point methods and tabu search are popular interactive optimization techniques see for example stewart and janssen 2014 and related studies and qi and altinakar 2011 but gas e g bennett et al 1999 can also be used wu et al 2016 present a generic framework for stakeholder integration in combination with multi objective eas and illustrate its applicability with a real world integrated urban water management problem for adelaide south australia 4 discussion recommendations in general most land use allocation problems are very complex and thus hard to solve with optimization techniques therefore scenario analysis might be a better option in some cases seppelt and voinov 2002 however working with optimization methods allows the decision maker to evaluate the potential of a landscape by analyzing trade offs between environmental social and economic objectives and to assess the efficiency of current land uses kennedy et al 2016 uhde et al 2015 recommend optimization techniques particularly for the consideration of provisioning and cultural ess for regulating and supporting ess they suggest a combination of madm with group decision making or spatial analysis since in practice their quantification can be difficult the moment of stakeholder integration and the decision on whether or not trade offs need to be identified have a major influence on the choice of a suitable optimization technique the analysis of biophysical trade offs requires a high level of objectivity certainly this is not entirely possible in modelling since most models contain some kind of human opinion or experience but stakeholders should at least be involved a posteriori however if the overall aim is only to allocate land according to the stakeholder s preferences by taking into account the ecological potential of the landscape then the optimization is rather subjective and it is reasonable to include stakeholders a priori or interactively and even involve them in the problem formulation also the amount of time that is available for computing solutions should be considered while interactive methods require solutions within seconds or a few minutes comparatively slower and thus perhaps more accurate algorithms can be used for a priori and a posteriori approaches for a more detailed discussion of the advantages and disadvantages of a priori interactive and a posteriori approaches we refer to coello coello et al 2007 before we provide guidelines for which of the above presented methods are suitable for different types of land use allocation problems we need to discuss some of the method s particular strengths and weaknesses especially in terms of trade off identification and stakeholder integration we will begin by comparing scalarization with pareto based methods in general then we will provide detailed discussion of the methods that fall into these categories the advantages and disadvantages of scalarization versus pareto based methods first depend on the decision maker s expectation towards the number of optimal solutions that should be determined scalarization methods are comparatively easy to implement and efficient if it is sufficient to find only one or a limited number of pareto optimal solutions in the preferred regions of the solution space however to complete a trade off analysis the pareto frontier needs to be determined by solving the optimization problem multiple times and this can be computationally expensive and time consuming janssen et al 2008 additionally scalarization approaches can serve to rapidly find tentative solutions for first discussions stewart and janssen 2014 another important factor is the number of objectives within the problem for more than two objectives it may be difficult for the stakeholders to interpret visualizations of the pareto frontier deb and köksalan 2010 for more than four objectives even the visualization itself becomes difficult therefore single solution approaches seem to be more practical however representing the whole pareto frontier can also be advantageous the whole range of equally optimal solutions comprises much more information than single solutions this can be for example trade offs and alternative land use options that could have been missed if preferences had been stated in advance furthermore the points of view of multiple stakeholders are reflected better by multiple solutions memmah et al 2015 4 1 using scalarization methods of the presented methods above weighted sum is one of the most common and easy to use approaches bishop 2013 nevertheless it has some major drawbacks generally preferences follow a non linear relationship but the weighted sum is only a linear approximation of this function therefore it favours unbalanced i e extreme solutions although decision makers typically prefer balanced ones marler and arora 2010 if a weighted sum is used in order to obtain the pareto frontier it also has to be considered that most of the land use allocation problems are non convex problems this means that the method may not find all pareto optimal solutions additionally the weights have to be changed with every optimization run but here one has to consider that a uniformly distributed set of weights does not necessarily lead to a uniformly distributed set of pareto optimal solutions deb 2001 furthermore it should be taken into account that highly correlated objective functions may distort the weighted objective function value and might even hamper convergence salmasnia et al 2013 steuer 1989 marler and arora 2010 discuss some more aspects that must be taken into account when using the weighted sum approach for multi objective optimization problems e g weight setting they conclude that alternative methods should be applied if the aim is to depict the pareto frontier with low computational effort particularly for non convex problems and if complex preferences must be accurately articulated if the initial solution is already sufficiently good simulated annealing can find optimal solutions with comparatively low computational expenses even for non linear objective functions santé riveira et al 2008 sa is therefore a fast and simple method that is more recommendable at an early stage of the land use allocation process aerts and heuvelink 2002 according to memmah et al 2015 sa as well as tabu search perform well for problems with many constraints thus different views of all stakeholders can be taken into account qi and altinakar 2011 but like sa ts requires a good initial solution otherwise it may take more iterations and thus more time to achieve an optimal solution ts and reference point methods are a priori techniques but as mentioned before they are also well known for interactive stakeholder integration meignan et al 2015 miettinen et al 2008 the benefit of rp methods is that the optimization is guided towards relevant solutions for stakeholders in contrast to weighted sum methods they even promote balanced solutions if achievement functions are applied the whole approach may seem too subjective for trade off analyses but the pareto frontier can be obtained by varying the aspiration levels though this can lead to long computation times therefore a pareto based method can be more practical for trade off comparisons 4 2 using pareto based methods methaheuristics e g evolutionary or genetic algorithms are widely applied in land use allocation they form the most popular group of algorithms that are able to solve hard combinatorial and non linear problems memmah et al 2015 nevertheless memmah et al 2015 also state that the more sophisticated algorithms have not propagated to the land use optimization community since the newer methods are not as straightforward as their basic versions population based algorithms have three main advantages i they are gradient free which means that they can deal with complex non linear and discontinuous problems ii they are highly explorative which increases the probability of finding global optima though there is no guarantee that they actually will be found and iii they can be implemented in a parallel way which decreases computation time yang 2014 the latter is helpful for problems that consider a high number of generations of a large population which usually leads to long convergence times one drawback of population based algorithms is that parameter tuning is often done by trial and error however in the context of land use allocation the main flaw is probably that pareto dominance is inappropriate for many objective problems i e problems with more than four objectives lautenbach et al 2013 the reason for this is that with an increasing number of objectives the amount of non dominated solutions increases exponentially making it more difficult for the algorithm to converge towards the pareto frontier memmah et al 2015 for pareto based methods stakeholder integration usually occurs a posteriori however there are some exceptions for example porta et al 2013 maximize a weighted fitness function with a ga which requires prior weight setting and liu et al 2016 use pso with knowledge informed rules that are partly based on a priori information about stakeholder s preferences also an interactive ga has been applied by bennett et al 1999 a summary of the different optimization methods their particular timing of stakeholder integration and how trade offs can be identified are all given in table 1 in summary all optimization methods have their individual advantages and disadvantages regarding technical aspects e g complexity of the algorithm and computation time trade off identification and stakeholder integration in some cases a combination of different approaches e g an ea with local search or an optimization method with madm can compensate for the drawbacks of one method and add useful features of another thus hybrid approaches can lead to a more effective and efficient search uhde et al 2015 for example the bilevel optimization approach used by whittaker et al 2017 was able to find better solutions compared to single level or sequential optimization approaches however the authors argue that the optimization method is challenging in terms of mathematical requirements e g convexity continuity linearity and computationally expensive though it is relatively simple to set up for parallel execution 4 3 recommendations for the method selection in light of the methods presented and discussed above a few questions turned out to be essential when selecting an appropriate approach for a land use allocation problem see fig 3 i what type of optimization problem is it that is is it multi objective non linear or combinatorial to answer this question the problem formulation must be clear in general one can say that the more objective functions a problem has and the more details are considered the more complex it will be to solve this will result in longer run times and found solutions might be not as close to optimal as expected this is because non convex complex optimization problems can only be solved by meta heuristics which cannot guarantee the finding of the optimal solution s given a multi objective optimization problem it should first be considered whether a trade off analysis is required ii that is is it necessary to depict the whole pareto frontier if yes then iii how many objective functions does the problem have if it is many then iv can their number be reduced for example by combining them or converting some of them to constraints because some population based algorithms cannot handle many objective 4 problems lautenbach et al 2013 if the number of objectives is manageable then pareto based algorithms e g eas gas si with a posteriori stakeholder integration are recommendable if it is not possible to reduce the number of objectives or no trade off analysis is required anyway then the land use planner should choose from the group of a priori and interactive approaches that include all of the scalarization methods at this point reference point methods like goal programming but also tabu search are recommendable for interactive stakeholder integration scalarization methods also allow the identification of the pareto frontier though it may be computationally more expensive than a pareto based approach of course fig 3 should only serve as a first orientation to the available methods for land use planners there are methods like hybrid algorithms or the application of knowledge informed rules that would not fit into this scheme after all the suitability of an algorithm is highly dependent on the structure of the optimization problem itself and since every problem is unique it is impossible to give a general recommendation that would hold for any kind of optimization problem although decision support techniques including optimization are promising tools for allocating land use their application is still limited mcintosh et al 2011 there are several reasons for this amongst them a lack of skills and knowledge on the usage of these methods in practice volk et al 2010 therefore a strong collaboration between experts e g scientists and stakeholders decision makers is needed and even promoted hauck et al 2016 for this one of the key issues but also challenges is the proper communication of the land use optimization approach and the results to all parties involved in the decision making process parker et al 2002 5 conclusion we presented a review of available optimization techniques that can be useful for agricultural land use allocation we first classified them within the broader field of decision support techniques before we presented the methods themselves we distinguished between scalarization e g weighted sum rp ts and sa and pareto based optimization methods e g eas gas si algorithms and illustrated each of them with examples from existing studies we also highlighted how trade offs can be identified either by changing weights goals and solving the problem iteratively scalarization approaches or by determining and analysing the pareto frontier directly pareto based approaches furthermore we mentioned how constraints can be handled for the different optimization methods and addressed the topic of stakeholder integration a priori interactive a posteriori in cases where no trade off analysis is required and the main focus lies on finding optimal land use patterns that best fulfil the stakeholders preferences we recommend a priori and particularly interactive approaches if the identification of trade offs is of high priority stakeholders should be involved a posteriori also for problems where it is impossible or not necessary to include stakeholders a posteriori i e pareto based methods are most suitable furthermore land use allocation usually occurs at a regional or local scale where ess should be addressed uhde et al 2015 nevertheless optimal solutions might look different if linkages to other regions were considered e g trade or animal migration van butsic and kuemmerle 2015 also optimal solutions will depend on scale for example the allocation of certain crops could look completely different at a local scale compared to a national or even global level however large scale studies are difficult to conduct and cannot capture the details of small scale studies ultimately implementations of global solutions are likely to fail since there is no world institution for land management to date most of the agricultural land use allocation studies have focused on economic trade offs between ess use e g crop production and ess provision e g water provisioning water quality soil erosion fowler et al 2015 groot et al 2012 sadeghi et al 2009 only few studies explicitly included biodiversity trade offs like groot et al 2007 groot et al 2018 or kennedy et al 2016 see also table a 1 in the appendix on the other hand systematic conservation planning creates areas that protect species and habitats but it does not consider biodiversity protection on working lands polasky et al 2008 this substantiates the impression that there is optimal land use allocation as a distinct objective and biodiversity protection as another also macfadyen et al 2012 argue that from focusing only on either biodiversity conservation or ess management it does not follow that this will provide reciprocal benefits of the kind we should be seeking in land use decision making therefore a stronger collaboration between both research areas is needed to determine economically efficient land use patterns that are ecologically sustainable and protect biodiversity at the same time a first step has been done by including the ess concept in conservation planning though this is still a fairly new practice schröter and remme 2015 in this context mathematical optimization offers powerful and flexible methods that allow for the integration of biophysical and biodiversity models here a clear mathematical formulation and specification of the optimization problem is necessary and forms a mutual basis for decision makers and stakeholders for discussing land use solutions at the same time it makes problem understanding and repeatability for other case studies much easier this is particularly important since some methods from related fields like spatial conservation prioritization or spatial forest planning could also be used in agricultural land use allocation and vice versa after all conservation prioritization as well as forest planning handle similar optimization problems kurttila 2001 2001 mendoza and martins 2006 moilanen and wilson 2009 however in the end it should be clear that mathematical land use optimization as well as any other mcda technique is only a tool to support decision making stewart et al 2004 and none of them provides a completely objective analysis that always leads to the right answer belton and stewart 2002 for future research the development of hybrid methods that combine different optimization algorithms or integrate other mcda techniques along with the use of parallelization techniques and participatory approaches are seen as most relevant memmah et al 2015 uhde et al 2015 particularly for a priori stakeholder integration the application of knowledge informed rules can improve the finding of efficient and effective land use solutions liu et al 2016 future research should aim at the integration of changing climatic conditions klein et al 2013 and uncertainties malczewski and rinner 2015 into optimization frameworks acknowledgement this research was funded through the 2013 2014 biodiversa facce jpi joint call with the national funder bmbf german federal ministry of education and research project tale towards multifunctional agricultural landscapes in europe assessing and governing synergies between food production biodiversity and ecosystem services grant 01 lc 1404 a we particularly thank bradley barnhart whose comments greatly improved an earlier version of this manuscript appendix table a 1a selection of studies from the research fields of general and agricultural land use allocation and examples from other related areas all studies used mathematical optimization the table indicates the optimization method used when stakeholders were included whether trade offs were determined and which ecosystem services including biodiversity were considered for studies where es were not mentioned explicitly we translated considered land use types into the respective es e g forest to forest related es reference stakeholder optimization method trade offs ecosystem services a priori interactive a posteriori reference point evolutionary genetic algorithm particle swarm optimization ant colony optimization simulated annealing tabu search other agricultural production water soil erosion forest related es wildlife nature carbon sequestration storage recreation cultural value other biodiversity land use in general cao et al 2012 eikelboom et al 2015 huang et al 2012 janssen et al 2008 liu et al 2013 liu et al 2016 ma et al 2011 porta et al 2013 stewart et al 2004 stewart and janssen 2014 wang et al 2004 yang et al 2015 agriculture antoine et al 1997 behrman et al 2015 bekele and nicklow 2005 bennett et al 1999 bostian et al 2015 chikumbo et al 2012 fowler et al 2015 groot et al 2007 groot et al 2012 groot et al 2018 kennedy et al 2016 klein et al 2013 lautenbach et al 2013 lu and van ittersum 2004 mishra et al 2014 nguyen et al 2016 polasky et al 2008 qi and altinakar 2011 sadeghi et al 2009 santé riveira et al 2008 seppelt and voinov 2002 shaygan et al 2014 van butsic and kuemmerle 2015 whittaker et al 2017 other e g conservation planning forestry restoration watershed management aerts and heuvelink 2002 arabi et al 2006 keller et al 2015 rabotyagov et al 2010 randhir and shriver 2009 schröter et al 2014 
26380,optimal land use allocation with the intention of ecosystem services provision and biodiversity conservation is one of the key challenges in agricultural management optimization techniques have been especially prevalent for solving land use problems however there is no guideline supporting the selection of an appropriate method to enhance the applicability of optimization techniques for real world case studies this study provides an overview of optimization methods used for targeting land use decisions in agricultural areas we explore their relative abilities for the integration of stakeholders and the identification of ecosystem service trade offs since these are especially pertinent to land use planners finally we provide recommendations for the use of the different optimization methods for example scalarization methods e g reference point methods tabu search are particularly useful for a priori or interactive stakeholder integration whereas pareto based approaches e g evolutionary algorithms are appropriate for trade off analyses and a posteriori stakeholder involvement keywords agricultural land use allocation multi criteria decision analysis mcda multi criteria optimization stakeholder integration trade off analysis constraint handling 1 introduction humans have been changing landscapes for millennia by converting natural areas for agricultural production and settlement delcourt and delcourt 1988 as a result 40 50 of the world s land surface had been visibly transformed for these purposes by the 20th century western 2001 many of the different land uses are conflicting for instance there is agricultural and timber production on one side competing with space for urban settlements or protected areas on the other side all these anthropogenic usages impact the provision of ecosystem services ess and therefore directly affect for example soil quality as well as water quantities and quality fontana et al 2013 meanwhile natural areas provide habitats for wildlife and are especially important for the protection of endangered species behrman et al 2015 biodiversity loss has been directly linked to land use changes sala et al 2000 and population growth as well as increases of agricultural land use have been labelled the biggest threat to biodiversity and ess behrman et al 2015 one way to address biodiversity loss is to integrate ess into systematic conservation planning faith 2015 and re allocate land uses in order to support the multifunctionality of landscapes sustainable land use allocation therefore seeks to take into account the current and future provision of ess and biodiversity in order to determine so called optimal land use allocations in general land use allocation also sometimes referred to as land use planning stewart et al 2004 is a type of resource allocation and can be defined as the process of allocating different activities or uses e g agriculture residential land recreational activities conservation to particular areal units within a region cao et al 2012 agricultural land use allocation specifically deals with the allocation of species and activities to areas in agricultural landscapes memmah et al 2015 decision support research within the field of natural resources management has relied heavily on multi criteria decision analysis mcda and its corresponding tools mendoza and martins 2006 in this paper we provide a detailed review of mcda and focus in particular on one branch of mcda optimization techniques since land use allocation problems have been widely formulated as mathematical optimization problems these problems typically consider multiple mostly conflicting objectives and aim to minimize the trade off between them liu et al 2013 porta et al 2013 these can include trade offs between various ess such as provisioning and regulating services but also between ess and biodiversity a trade off describes the amount that has to be given up of one ess in order to increase the provision of another rodríguez et al 2006 for example the intensification of agricultural production may reduce water quality due to a greater use of fertilizers and pesticides and the resulting nonpoint emissions of pollutants from the agricultural fields the main task is thus to find the right balance between the usage of different ess solving complex real world land use allocation problems remains a key research challenge fowler et al 2015 additionally recent applications underline the need for methods that allow for increased stakeholder involvement eikelboom et al 2015 stewart et al 2004 uhde et al 2015 this is particularly important since agricultural land use allocation involves many competing actors such as farmers farmers associations environmental agencies land planners and economists memmah et al 2015 participatory approaches thus help to find solutions that achieve biophysical objectives but also consider the different perspectives and preferences of various stakeholders groot and rossing 2011 land use allocation problems can greatly differ in their mathematical formulation and therefore require different optimization techniques see section 2 2 however the choice of a technique is often not guided by the characteristics of a problem but depends on the experience of the reseacher in charge or on historical usages memmah et al 2015 while there exist some reviews about mcda approaches and their applicability particularly in forest management mendoza and martins 2006 uhde et al 2015 current literature lacks guidelines for how to choose the best suitable optimization technique for a particular agricultural land use allocation problem therefore this paper aims to fill this gap by providing a review of current mcda optimization techniques and their applicability for land use allocation problems we specifically focus on agricultural landscapes and on studies that aimed to achieve objectives related to ess and biodiversity the following sections provide a review of optimization approaches that have been used in land use management for an overview we first classify multi objective optimization within the broader field of decision support techniques giving an introduction to mcda then we evaluate different multi criteria optimization methods in terms of their ability to integrate stakeholder opinions and identify trade offs between ess and biodiversity furthermore we mention how constraints can be handled the suitability of the optimization approaches for different types of land use allocation problems is discussed before we provide a short conclusion and give directions for further research 2 solving land use allocation problems with multi criteria decision analysis mcda 2 1 an overview of mcda mcda has been widely used to perform mathematical optimization in order to analyze multi objective decisions and incorporate the varying opinions of decision makers collins et al 2001 mcda addresses land allocation problems in a more realistic way than single objective approaches since in practice these problems consist of multiple conflicting objectives antoine et al 1997 especially when multiple ecosystem services are taken into account birkhofer et al 2015 furthermore mcda methods can combine ecological objectives with social and economic criteria and are able to consider non market values of ess therefore they are very popular and frequently used in ecological economics fontana et al 2013 uhde et al 2015 van huylenbroeck 1997 most of the literature classifies multi criteria optimization either within the broader field of decision support systems e g myllyviita et al 2011 or within mcda directly e g aerts et al 2003 therefore we first provide an overview of the linkage between the two fields and where multi criteria optimization is set amongst these see fig 1 mcda is one of many decision support techniques which can be divided into qualitative quantitative and hybrid methods qualitative methods e g interviews voting focus on structuring a problem they also help to define initial goals and to evaluate stakeholders opinions myllyviita et al 2011 uhde et al 2015 cost benefit analysis cba and mcda including mathematical optimization techniques belong to the group of quantitative methods that use numerical information in order to evaluate a number of decision alternatives finally hybrid methods are composed by the combination of different approaches see uhde et al 2015 for an overview of hybrid mcda methods in forest management mcda methods can be classified in different ways mendoza and martins 2006 belton and stewart 2002 suggest distinguishing three categories i value measurement models ii goal aspiration or reference level models and iii outranking models instead malczewski 1999 and zimmermann and gutsche 1991 distinguish between multi attribute decision making madm and multi objective decision making modm see fig 1 madm deals with the evaluation of a finite number of alternatives that are previously known to the decision maker therefore they require discrete mcda methods an example of a hybrid combination of madm techniques can be found in fontana et al 2013 the authors evaluated three land use alternatives i e larch meadow spruce forest and intensive meadow to determine their ability of providing certain ess first they derived the weights of six ess with a stakeholder questionnaire and the analytical hierarchy process ahp saaty 1988 later they applied an outranking method in order to evaluate the different alternatives more information about madm techniques like the outranking methods electre and promethee multiattribute utility theory maut and ahp can be found in belton and stewart 2002 and figueira et al 2005 since land use allocation problems usually include a range of competing objectives it might often be impossible to create a small set of scenarios that would cover all possible solutions bishop 2013 groot and rossing 2011 besides an optimal solution of sustainable land management might be located between two distinct scenarios seppelt et al 2013 in this case the application of design techniques can help to avoid this problem aerts and heuvelink 2002 multi criteria design problems are of a continuous nature and handled within modm here alternatives are either not known in advance or there are so many that the problem cannot be solved with evaluation methods anymore these problem types can be solved by applying mathematical optimization aerts and heuvelink 2002 uhde et al 2015 the focus of this paper then is to provide a detailed review of optimization techniques used in land use allocation which will be given in the following section multi criteria decision aid mcda is yet another perspective from which to solve multi criteria problems and further information can be found in bana e costa 1990 and zimmermann and gutsche 1991 according to them the main difference between classical madm and modm approaches is that mcda also incorporates vague incomplete inconsistent and subjective information also instead of a single optimal solution it provides a set of acceptable alternatives fuzzy programming and outranking methods such as electre and promethee figueira et al 2005 are but a few examples for mcda methods the classification in fig 1 must not be seen as strict there are some approaches that cannot be assigned fully to any of the categories for example some mcda methods may simultaneously contain portions from madm and modm zimmermann and gutsche 1991 a well structured overview of mcda applications in forestry and natural resources management can be found in mendoza and martins 2006 furthermore myllyviita et al 2011 provide a comparative review of studies that used mcda design techniques optimization cba and hybrid methods in sustainable forest management 2 2 optimization methods the process of land use allocation includes a series of individual steps which are highlighted in fig 2 belton and stewart 2002 groot and rossing 2011 talbi 2009 for the optimization the problem needs to be identified and clearly formulated before it can be modelled in this context we identified three important points that should be taken into account i mathematical problem formulation objectives constraints decision variables and problem type e g linear non linear discrete e g binary combinatorial continuous deb 2001 ii desired output required input problem scale e g local global seppelt et al 2013 amount and type of available data e g land use maps information about topography hydrology soil quality and if a trade off analysis is needed iii stakeholder involvement before a priori during interactive or after a posteriori the optimization process memmah et al 2015 all of these factors determine the size and complexity of the problem and have an influence on the choice of a suitable optimization method and on computation time at this point it should be considered that in the end the quality of the optimization result does not only depend on the performance of the selected algorithm but also on the conceptual design of the optimization problem particularly if there have been simplifications in the model formulation moilanen 2008 optimal solutions can then be used by decision makers to inform and support the implementation of new land use strategies the selection of optimization methods presented in this section is mainly based on the fact that spatial land use allocation problems are mostly multi objective combinatorial optimization problems porta et al 2013 that may often be non linear cao et al 2012 liu et al 2016 memmah et al 2015 these problems are usually complex and include a large number of alternative solutions requiring high computation times porta et al 2013 combinatorial problems as a type of discrete optimization problems are typically solved by applying local search algorithms such as simulated annealing tabu search genetic algorithms and ant colony optimization aarts and lenstra 2003 colorni et al 1996 these and other methods will be presented in the following in the rare case of a continuous problem formulation standard methods like the multi objective simplex algorithm figueira et al 2005 can be used for linear problems see sadeghi et al 2009 for an example but again if the problem is non linear then heuristic optimization methods are needed multi objective optimization is a useful tool for the evaluation of trade offs among conflicting objectives trade offs are represented by the pareto frontier in some studies this is also called the efficiency frontier or production possibility frontier polasky et al 2008 the pareto frontier is a set of optimal solutions to the respective multi criteria optimization problem assuming maximization a feasible solution to the optimization problem is said to be pareto optimal if there is no other feasible solution that would increase one of the objective function values without simultaneously decreasing another coello coello et al 2007 for trade off analyses it is sometimes necessary to obtain the whole pareto frontier particularly for visualization purposes therefore along with the presentation of different optimization algorithms we will also mention how trade off curves can be identified table a 1 in the appendix gives a selection of studies from the fields of general and agricultural land use allocation and a few from other research areas for each study it also includes information about the moment of stakeholder integration whether trade offs were determined and which ess were taken into account one approach for solving multi objective optimization problems is to define one objective function and add any additional objectives as constraints ε constraint method ehrgott 2005 then single objective algorithms can be applied for example van butsic and kuemmerle 2015 aimed to maximize agricultural production while minimizing species loss setting agricultural production as the objective function and target constraints for the species of concern they determined trade off curves between agricultural yield and the species population size by solving the problem multiple times with different targets nonetheless a high variety of multi objective algorithms is available that can simultaneously account for multiple conflicting objectives for an overview of multi objective optimization methods see chapter 17 of figueira et al 2005 generally multi objective optimization techniques follow two different approaches scalarization methods and pareto based methods cao et al 2012 madavan 2002 and these will be outlined below 2 2 1 scalarization methods scalarization methods combine multiple objective functions into a single objective scalar function miettinen and mäkelä 2002 the optimization problem is then solved with a single objective optimization algorithm which creates a single optimal solution to the optimization problem here again the selection of the algorithm depends on the problem type for spatial land use allocation problems heuristics like the greedy algorithm cormen 2007 simulated annealing genetic algorithms etc bozorg haddad et al 2017 are usually applicable there are two main methods to parameterize a problem either to maximize or minimize the weighted sum of all objectives by using weighting coefficients that specify the relative importance of each objective or by using a reference point based method wierzbicki 2000 weighted sum approaches provide pareto optimal solutions for convex solution sets the pareto frontier can be obtained by changing the coefficients of the scalar function and re solving the problem pareto optimal solutions of non convex parts however cannot be found using this method for an example see caramia and dell olmo 2008 kennedy et al 2016 used a weighted sum approach in combination with a greedy algorithm based on previous work by polasky et al 2008 they optimized three objectives agricultural production water quality and biodiversity for a watershed in an agricultural area in southeastern brazil by varying the weights of the individual objectives they obtained trade off curves between agricultural production and either water quality or biodiversity for different problem settings goal programming gp is a reference point method that guides an algorithm like an evolutionary deb and sundar 2006 or genetic algorithm deb 1999 for instance towards a solution that lies in the decision maker s preferred region of the solution space for this purpose the decision maker defines goals i e desired values for each objective then the distance between the goal vector and an attainable vector of the solution space is minimized and the optimum is a feasible solution that is closest to the goal vector defining and striving for a goal seems quite intuitive from a psychological perspective however from a mathematical viewpoint the minimization of a norm a distance measure cannot guarantee that the gp algorithm will find a pareto optimal solution romero 2014 therefore the more general reference point rp method has been developed instead of using a norm this approach minimizes a so called achievement function miettinen et al 2008 if norm minimization is avoided reference point approaches can obtain the pareto frontier even for non convex solution sets when solving the problem multiple times with different reference points wierzbicki 2000 stewart and janssen 2014 used a reference point method in combination with a genetic algorithm in order to solve a non linear combinatorial optimization problem the key objectives were the profitability of intensive agriculture maximization of the visual quality of the landscape including landscape perception cultural historic value and recreational value and maximization of the natural value of the area including meadow birds species rich grasslands and marsh birds as a result they generated land use maps that served as a basis for negotiating optimal land use strategies in a case study area in the netherlands this paper extends earlier work by stewart et al 2004 and janssen et al 2008 and forms the basis of work by eikelboom et al 2015 another scalarization approach is tabu search ts which is usually used in combination with a local search algorithm boussaïd et al 2013 ts was initially developed for single objective combinatorial optimization problems the algorithm works on an iterative basis by looking for an improved solution in the neighbourhood of the current solution in doing so ts uses a short term memory i e the tabu list where recently visited solutions or one or more of their attributes are recorded all potential new solutions that are on the tabu list cannot be visited again at this stage of the search this is to avoid endless cycling and prevents the algorithm from getting stuck at a local optimum boussaïd et al 2013 more information about ts can be found in glover and laguna 2013 ts can also be applied for multi objective optimization problems qi and altinakar 2011 used ts in order to optimize agricultural land use with integrated watershed management they considered three objective functions and combined them into a single objective function using weights to reflect the relative importance of each objective also behrman et al 2015 used weighting while solving their optimization problem with consnet ciarleglio et al 2009 which is based on a ts algorithm in their study the overall aim was to identify the trade offs between converting land to switchgrass for biofuel production agriculture and biodiversity in order to obtain optimal trade offs among the objectives they varied the weights that were applied to each of the three categories in addition ts approaches that are not based on scalarization but search directly for the pareto optimal set have been developed jaeggi et al 2005 however these methods have yet to be used in land use optimization simulated annealing sa kirkpatrick et al 1983 also applies weighting in order to combine multiple objective functions to a single objective function therefore as with ts methods trade offs can be determined by solving the problem multiple times with different weights the algorithm itself mimics the physical process of heating metal and cooling it again starting from an initial solution sa randomly chooses a new solution in a pre defined neighbourhood temperature is a parameter that is reduced over time when this temperature parameter is sufficiently high even solutions that decrease the objective function value can be accepted this prevents the algorithm from getting trapped in local optima at lower temperatures the algorithm accepts only improving new solutions and terminates once a stopping criterion is met aerts and heuvelink 2002 used sa to minimize development costs and compactness costs for the restoration of a mining area in spain the algorithm has also been applied for optimizing agricultural land use in santé riveira et al 2008 their aim is the optimal allocation of 13 land uses all different types of crops in a study area in spain all land uses were grouped into five use groups fodder cereals intensive agricultural crops productive forest and protective woodland they set the objective function to be a linear combination of three objectives maximize land suitability for the uses allocated to them maximize compactness of the total area assigned to a particular use and maximize compactness of the total area assigned to a particular group of uses the problem was solved for 11 different sets of weights 2 2 2 pareto based methods pareto based methods generate multiple pareto optimal solutions simultaneously and are able to provide the whole pareto frontier as a result of the optimization there is a wide range of evolutionary algorithms ea including genetic algorithms ga which have been used in land use optimization eas are inspired by biological evolution they begin with an initial population of solutions and use concepts such as selection crossover and mutation in order to create the next generation of solutions the fitness of a solution evaluates how good it fulfills the problem criteria and determines whether or not it will be selected as a parent for the next generation the algorithm terminates once a predefined stopping criterion is met memmah et al 2015 lautenbach et al 2013 coupled a watershed model called the soil and water assessment tool swat arnold and fohrer 2005 with the non dominated sorting genetic algorithm ii nsga ii deb et al 2002 in order to analyze trade offs between bioenergy crop production food crop production water quantity and water quality in a case study area in central germany similarly fowler et al 2015 coupled a multi objective genetic algorithm from the dakota optimization suite adams et al 2014 with the modflow fmp2 software schmid and hanson 2009 which simulates the integrated supply and demand components of irrigated agriculture they optimize the selection of three different crops based on trade offs between agricultural revenue water usage and the deviation from actual yield and demand yield for each crop in an artificial study area in addition to maximizing agricultural profits groot et al 2007 also consider nature conservation and landscape quality by minimizing the loss of nutrients to the environment maximizing the nature value of fields and borders i e species abundance in grass swards and hedgerows and maximizing the variation of the landscape i e species presence and hedgerow allocation they explore the trade offs between these objectives with an evolutionary strategy algorithm of differential evolution storn and price 1997 and provide trade off curves of gross margin with either plant species number landscape value or nitrogen loss the underlying data is from a case study area in the netherlands other eas that gained much popularity in land use allocation during the last years are artificial immune systems huang et al 2012 and particularly swarm intelligence si algorithms see yang 2014 for an introduction such as ant colony optimization nguyen et al 2016 the artificial bee colony algorithm yang et al 2015 and particle swarm optimization pso liu et al 2016 ma et al 2011 the latter was developed to solve continuous optimization problems however liu et al 2016 present a method that uses pso for a binary multi objective optimization problem in general si algorithms mimic the collective behaviour of single agents in a decentralized and self organized system for example pso imitates the social behaviour of fish schooling and flocking of birds kumar and minz 2014 2 2 3 hybrid methods hybrid optimization approaches combine two or more optimization methods within a single framework to facilitate the search for the pareto frontier an example of a hybrid optimization approach applied to the calapooia river basin in oregon usa can be found in whittaker et al 2017 their approach couples the swat watershed model with data envelopment analysis dea cooper et al 2004 an economic linear optimization model within a bilevel optimization framework that uses nsga ii bilevel optimization problems consist of two nested optimization levels and each level contains its own set of objectives in this example the upper level is a government agency that seeks to maximize the total farm profit of the entire watershed and minimize the nitrogen loading at the watershed s outlet the lower level consists of farmers that seek to maximize their individual profits using dea the two levels are linked because the upper level sets tax rates for the use of nitrogen fertilizer by the lower level and the lower level s fertilizer use decisions ultimately impact the amount of nitrogen loading that occurs at the watershed s outlet nsga ii is used to find the optimal set of tax rates to optimize all objectives at both levels whittaker et al 2017 highlight the pareto frontier trade offs for the upper between total accumulated profit of the farmers and nitrogen loading at the watershed s outlet other studies like bostian et al 2015 and barnhart et al 2017 have utilized a similar methodology to target agri environmental policy to promote best management practices in order to achieve environmental benefits according to memmah et al 2015 the application of hybrid metaheuristics to land use allocation problems is still rare nevertheless some more examples e g a hybrid pso can be found in their study along with general information about the hybridization of algorithms apart from the methods presented above fuzzy programming from the field of multi criteria decision aid see fig 1 can also be applied to solve land use allocation problems for an example we refer to wang et al 2004 2 2 4 constraint handling as indicated at the beginning of this section constraints are an important part of the mathematical formulation of land use allocation problems they limit the space of feasible solutions by reflecting e g environmental social and political limits such as the total area that can be allocated to each land use stewart and janssen 2014 water quality and water demand supply constraints wang et al 2004 restrictions on nutrient input groot et al 2007 or biodiversity targets schröter et al 2014 to name only a few examples handling real world constraints is one of the most challenging tasks in the optimization process michalewicz and fogel 2004 especially since they can increase the computational complexity of a problem all of the methods presented in this paper support constraints though the way of handling them depends on the algorithm used the most popular constraint handling method for heuristics is using a penalty function that degrades the fitness value of an infeasible solution chehouri et al 2016 furthermore the use of feasibility operators which create feasible only child solutions deb 2001 and repairing infeasible solutions coello coello et al 2007 are common methods too linear and quadratic programming generally apply lagrange multipliers for constraint handling and linear continuous problems can be solved by the simplex algorithm which includes constraint handling cavazzuti 2013 more information about lagrange multiplier methods can be found in bertsekas and rheinboldt 2014 and for constraint handling methods for linear and non linear continuous single objective problems we refer to cottle and thapa 2017 furthermore table 1 includes information about constraint handling for the different methods presented in this paper 3 stakeholder integration in order to make land use planning more applicable to real world problems stakeholders are increasingly integrated into the decision making process memmah et al 2015 they can state their interests and ambitions but also provide expert knowledge on current and future developments this is especially valuable in agricultural areas since for example trends in cultivation techniques consumer demand but also policies need to be taken into account furthermore stakeholders might provide a deeper insight into the potential of certain areas which would help to facilitate the decision on where possible land use changes can be implemented in all cases it is important to find representative stakeholders first harrison and qureshi 2000 for example for agricultural land use allocation problems there should be a sound combination of people with different backgrounds and point of views like farmers conservationists or employees of state ministries hauck et al 2016 this should guarantee a dynamic discussion and prevents the optimization from being biased by too narrow perspectives however if the stakeholders cannot agree on a mutually consistent set of preferences malczewski 1999 multiple analyses of the problem might be necessary malczewski 1999 furthermore important technical terms like for example trade offs land sharing sparing etc and the optimization method applied must be communicated well to the stakeholders in order to create a mutual understanding otherwise there might be misunderstandings or the optimization might look like a black box which could create mistrust bishop 2013 the moment stakeholders are involved in the optimization process has an influence on the problem formulation and affects the choice of a suitable optimization method preferences can be included either before a priori during interactively or after a posteriori the optimization initially proposed by cohon and marks 1975 it is nowadays common practice to classify multi criteria optimization methods according to these three categories coello coello et al 2007 scalarization is an a priori method where the algorithm finds a solution that best meets the stakeholder s preferences these are represented by the objective function weights or optimization goals examples from the studies presented above are aerts et al 2003 behrman et al 2015 and santé riveira et al 2008 however if scalarization is used to calculate the pareto frontier by changing weights goals like in kennedy et al 2016 it is considered as an a posteriori approach additionally pareto based methods and with them the majority of evolutionary multi objective algorithms fall into this category deb and köksalan 2010 these algorithms provide a whole set of pareto optimal solutions and given these alternatives stakeholders can then select those that fit their preferences best see for example fowler et al 2015 groot et al 2007 and lautenbach et al 2013 for the case of a posteriori involvement methods from the field of madm fig 1 can be applied in the selection process however a priori and a posteriori approaches do not consider that it might be difficult for the stakeholders to express their preferences analytically and that values can change over time and with growing experience and learning coello coello et al 2007 therefore interactive approaches can be favourable they allow stakeholders to articulate preferences in a progressive way that is stakeholders are able to adjust them after each iteration of the optimization this step by step integration of preferences guides the optimization towards the relevant parts of the pareto frontier and may help to reduce computational time meignan et al 2015 such real time interaction however requires short computation times of intermediate solutions stewart et al 2004 this can also be challenging since the computational time to complete a single optimization should preferably be less than a minute bishop 2013 stewart et al 2004 reference point methods and tabu search are popular interactive optimization techniques see for example stewart and janssen 2014 and related studies and qi and altinakar 2011 but gas e g bennett et al 1999 can also be used wu et al 2016 present a generic framework for stakeholder integration in combination with multi objective eas and illustrate its applicability with a real world integrated urban water management problem for adelaide south australia 4 discussion recommendations in general most land use allocation problems are very complex and thus hard to solve with optimization techniques therefore scenario analysis might be a better option in some cases seppelt and voinov 2002 however working with optimization methods allows the decision maker to evaluate the potential of a landscape by analyzing trade offs between environmental social and economic objectives and to assess the efficiency of current land uses kennedy et al 2016 uhde et al 2015 recommend optimization techniques particularly for the consideration of provisioning and cultural ess for regulating and supporting ess they suggest a combination of madm with group decision making or spatial analysis since in practice their quantification can be difficult the moment of stakeholder integration and the decision on whether or not trade offs need to be identified have a major influence on the choice of a suitable optimization technique the analysis of biophysical trade offs requires a high level of objectivity certainly this is not entirely possible in modelling since most models contain some kind of human opinion or experience but stakeholders should at least be involved a posteriori however if the overall aim is only to allocate land according to the stakeholder s preferences by taking into account the ecological potential of the landscape then the optimization is rather subjective and it is reasonable to include stakeholders a priori or interactively and even involve them in the problem formulation also the amount of time that is available for computing solutions should be considered while interactive methods require solutions within seconds or a few minutes comparatively slower and thus perhaps more accurate algorithms can be used for a priori and a posteriori approaches for a more detailed discussion of the advantages and disadvantages of a priori interactive and a posteriori approaches we refer to coello coello et al 2007 before we provide guidelines for which of the above presented methods are suitable for different types of land use allocation problems we need to discuss some of the method s particular strengths and weaknesses especially in terms of trade off identification and stakeholder integration we will begin by comparing scalarization with pareto based methods in general then we will provide detailed discussion of the methods that fall into these categories the advantages and disadvantages of scalarization versus pareto based methods first depend on the decision maker s expectation towards the number of optimal solutions that should be determined scalarization methods are comparatively easy to implement and efficient if it is sufficient to find only one or a limited number of pareto optimal solutions in the preferred regions of the solution space however to complete a trade off analysis the pareto frontier needs to be determined by solving the optimization problem multiple times and this can be computationally expensive and time consuming janssen et al 2008 additionally scalarization approaches can serve to rapidly find tentative solutions for first discussions stewart and janssen 2014 another important factor is the number of objectives within the problem for more than two objectives it may be difficult for the stakeholders to interpret visualizations of the pareto frontier deb and köksalan 2010 for more than four objectives even the visualization itself becomes difficult therefore single solution approaches seem to be more practical however representing the whole pareto frontier can also be advantageous the whole range of equally optimal solutions comprises much more information than single solutions this can be for example trade offs and alternative land use options that could have been missed if preferences had been stated in advance furthermore the points of view of multiple stakeholders are reflected better by multiple solutions memmah et al 2015 4 1 using scalarization methods of the presented methods above weighted sum is one of the most common and easy to use approaches bishop 2013 nevertheless it has some major drawbacks generally preferences follow a non linear relationship but the weighted sum is only a linear approximation of this function therefore it favours unbalanced i e extreme solutions although decision makers typically prefer balanced ones marler and arora 2010 if a weighted sum is used in order to obtain the pareto frontier it also has to be considered that most of the land use allocation problems are non convex problems this means that the method may not find all pareto optimal solutions additionally the weights have to be changed with every optimization run but here one has to consider that a uniformly distributed set of weights does not necessarily lead to a uniformly distributed set of pareto optimal solutions deb 2001 furthermore it should be taken into account that highly correlated objective functions may distort the weighted objective function value and might even hamper convergence salmasnia et al 2013 steuer 1989 marler and arora 2010 discuss some more aspects that must be taken into account when using the weighted sum approach for multi objective optimization problems e g weight setting they conclude that alternative methods should be applied if the aim is to depict the pareto frontier with low computational effort particularly for non convex problems and if complex preferences must be accurately articulated if the initial solution is already sufficiently good simulated annealing can find optimal solutions with comparatively low computational expenses even for non linear objective functions santé riveira et al 2008 sa is therefore a fast and simple method that is more recommendable at an early stage of the land use allocation process aerts and heuvelink 2002 according to memmah et al 2015 sa as well as tabu search perform well for problems with many constraints thus different views of all stakeholders can be taken into account qi and altinakar 2011 but like sa ts requires a good initial solution otherwise it may take more iterations and thus more time to achieve an optimal solution ts and reference point methods are a priori techniques but as mentioned before they are also well known for interactive stakeholder integration meignan et al 2015 miettinen et al 2008 the benefit of rp methods is that the optimization is guided towards relevant solutions for stakeholders in contrast to weighted sum methods they even promote balanced solutions if achievement functions are applied the whole approach may seem too subjective for trade off analyses but the pareto frontier can be obtained by varying the aspiration levels though this can lead to long computation times therefore a pareto based method can be more practical for trade off comparisons 4 2 using pareto based methods methaheuristics e g evolutionary or genetic algorithms are widely applied in land use allocation they form the most popular group of algorithms that are able to solve hard combinatorial and non linear problems memmah et al 2015 nevertheless memmah et al 2015 also state that the more sophisticated algorithms have not propagated to the land use optimization community since the newer methods are not as straightforward as their basic versions population based algorithms have three main advantages i they are gradient free which means that they can deal with complex non linear and discontinuous problems ii they are highly explorative which increases the probability of finding global optima though there is no guarantee that they actually will be found and iii they can be implemented in a parallel way which decreases computation time yang 2014 the latter is helpful for problems that consider a high number of generations of a large population which usually leads to long convergence times one drawback of population based algorithms is that parameter tuning is often done by trial and error however in the context of land use allocation the main flaw is probably that pareto dominance is inappropriate for many objective problems i e problems with more than four objectives lautenbach et al 2013 the reason for this is that with an increasing number of objectives the amount of non dominated solutions increases exponentially making it more difficult for the algorithm to converge towards the pareto frontier memmah et al 2015 for pareto based methods stakeholder integration usually occurs a posteriori however there are some exceptions for example porta et al 2013 maximize a weighted fitness function with a ga which requires prior weight setting and liu et al 2016 use pso with knowledge informed rules that are partly based on a priori information about stakeholder s preferences also an interactive ga has been applied by bennett et al 1999 a summary of the different optimization methods their particular timing of stakeholder integration and how trade offs can be identified are all given in table 1 in summary all optimization methods have their individual advantages and disadvantages regarding technical aspects e g complexity of the algorithm and computation time trade off identification and stakeholder integration in some cases a combination of different approaches e g an ea with local search or an optimization method with madm can compensate for the drawbacks of one method and add useful features of another thus hybrid approaches can lead to a more effective and efficient search uhde et al 2015 for example the bilevel optimization approach used by whittaker et al 2017 was able to find better solutions compared to single level or sequential optimization approaches however the authors argue that the optimization method is challenging in terms of mathematical requirements e g convexity continuity linearity and computationally expensive though it is relatively simple to set up for parallel execution 4 3 recommendations for the method selection in light of the methods presented and discussed above a few questions turned out to be essential when selecting an appropriate approach for a land use allocation problem see fig 3 i what type of optimization problem is it that is is it multi objective non linear or combinatorial to answer this question the problem formulation must be clear in general one can say that the more objective functions a problem has and the more details are considered the more complex it will be to solve this will result in longer run times and found solutions might be not as close to optimal as expected this is because non convex complex optimization problems can only be solved by meta heuristics which cannot guarantee the finding of the optimal solution s given a multi objective optimization problem it should first be considered whether a trade off analysis is required ii that is is it necessary to depict the whole pareto frontier if yes then iii how many objective functions does the problem have if it is many then iv can their number be reduced for example by combining them or converting some of them to constraints because some population based algorithms cannot handle many objective 4 problems lautenbach et al 2013 if the number of objectives is manageable then pareto based algorithms e g eas gas si with a posteriori stakeholder integration are recommendable if it is not possible to reduce the number of objectives or no trade off analysis is required anyway then the land use planner should choose from the group of a priori and interactive approaches that include all of the scalarization methods at this point reference point methods like goal programming but also tabu search are recommendable for interactive stakeholder integration scalarization methods also allow the identification of the pareto frontier though it may be computationally more expensive than a pareto based approach of course fig 3 should only serve as a first orientation to the available methods for land use planners there are methods like hybrid algorithms or the application of knowledge informed rules that would not fit into this scheme after all the suitability of an algorithm is highly dependent on the structure of the optimization problem itself and since every problem is unique it is impossible to give a general recommendation that would hold for any kind of optimization problem although decision support techniques including optimization are promising tools for allocating land use their application is still limited mcintosh et al 2011 there are several reasons for this amongst them a lack of skills and knowledge on the usage of these methods in practice volk et al 2010 therefore a strong collaboration between experts e g scientists and stakeholders decision makers is needed and even promoted hauck et al 2016 for this one of the key issues but also challenges is the proper communication of the land use optimization approach and the results to all parties involved in the decision making process parker et al 2002 5 conclusion we presented a review of available optimization techniques that can be useful for agricultural land use allocation we first classified them within the broader field of decision support techniques before we presented the methods themselves we distinguished between scalarization e g weighted sum rp ts and sa and pareto based optimization methods e g eas gas si algorithms and illustrated each of them with examples from existing studies we also highlighted how trade offs can be identified either by changing weights goals and solving the problem iteratively scalarization approaches or by determining and analysing the pareto frontier directly pareto based approaches furthermore we mentioned how constraints can be handled for the different optimization methods and addressed the topic of stakeholder integration a priori interactive a posteriori in cases where no trade off analysis is required and the main focus lies on finding optimal land use patterns that best fulfil the stakeholders preferences we recommend a priori and particularly interactive approaches if the identification of trade offs is of high priority stakeholders should be involved a posteriori also for problems where it is impossible or not necessary to include stakeholders a posteriori i e pareto based methods are most suitable furthermore land use allocation usually occurs at a regional or local scale where ess should be addressed uhde et al 2015 nevertheless optimal solutions might look different if linkages to other regions were considered e g trade or animal migration van butsic and kuemmerle 2015 also optimal solutions will depend on scale for example the allocation of certain crops could look completely different at a local scale compared to a national or even global level however large scale studies are difficult to conduct and cannot capture the details of small scale studies ultimately implementations of global solutions are likely to fail since there is no world institution for land management to date most of the agricultural land use allocation studies have focused on economic trade offs between ess use e g crop production and ess provision e g water provisioning water quality soil erosion fowler et al 2015 groot et al 2012 sadeghi et al 2009 only few studies explicitly included biodiversity trade offs like groot et al 2007 groot et al 2018 or kennedy et al 2016 see also table a 1 in the appendix on the other hand systematic conservation planning creates areas that protect species and habitats but it does not consider biodiversity protection on working lands polasky et al 2008 this substantiates the impression that there is optimal land use allocation as a distinct objective and biodiversity protection as another also macfadyen et al 2012 argue that from focusing only on either biodiversity conservation or ess management it does not follow that this will provide reciprocal benefits of the kind we should be seeking in land use decision making therefore a stronger collaboration between both research areas is needed to determine economically efficient land use patterns that are ecologically sustainable and protect biodiversity at the same time a first step has been done by including the ess concept in conservation planning though this is still a fairly new practice schröter and remme 2015 in this context mathematical optimization offers powerful and flexible methods that allow for the integration of biophysical and biodiversity models here a clear mathematical formulation and specification of the optimization problem is necessary and forms a mutual basis for decision makers and stakeholders for discussing land use solutions at the same time it makes problem understanding and repeatability for other case studies much easier this is particularly important since some methods from related fields like spatial conservation prioritization or spatial forest planning could also be used in agricultural land use allocation and vice versa after all conservation prioritization as well as forest planning handle similar optimization problems kurttila 2001 2001 mendoza and martins 2006 moilanen and wilson 2009 however in the end it should be clear that mathematical land use optimization as well as any other mcda technique is only a tool to support decision making stewart et al 2004 and none of them provides a completely objective analysis that always leads to the right answer belton and stewart 2002 for future research the development of hybrid methods that combine different optimization algorithms or integrate other mcda techniques along with the use of parallelization techniques and participatory approaches are seen as most relevant memmah et al 2015 uhde et al 2015 particularly for a priori stakeholder integration the application of knowledge informed rules can improve the finding of efficient and effective land use solutions liu et al 2016 future research should aim at the integration of changing climatic conditions klein et al 2013 and uncertainties malczewski and rinner 2015 into optimization frameworks acknowledgement this research was funded through the 2013 2014 biodiversa facce jpi joint call with the national funder bmbf german federal ministry of education and research project tale towards multifunctional agricultural landscapes in europe assessing and governing synergies between food production biodiversity and ecosystem services grant 01 lc 1404 a we particularly thank bradley barnhart whose comments greatly improved an earlier version of this manuscript appendix table a 1a selection of studies from the research fields of general and agricultural land use allocation and examples from other related areas all studies used mathematical optimization the table indicates the optimization method used when stakeholders were included whether trade offs were determined and which ecosystem services including biodiversity were considered for studies where es were not mentioned explicitly we translated considered land use types into the respective es e g forest to forest related es reference stakeholder optimization method trade offs ecosystem services a priori interactive a posteriori reference point evolutionary genetic algorithm particle swarm optimization ant colony optimization simulated annealing tabu search other agricultural production water soil erosion forest related es wildlife nature carbon sequestration storage recreation cultural value other biodiversity land use in general cao et al 2012 eikelboom et al 2015 huang et al 2012 janssen et al 2008 liu et al 2013 liu et al 2016 ma et al 2011 porta et al 2013 stewart et al 2004 stewart and janssen 2014 wang et al 2004 yang et al 2015 agriculture antoine et al 1997 behrman et al 2015 bekele and nicklow 2005 bennett et al 1999 bostian et al 2015 chikumbo et al 2012 fowler et al 2015 groot et al 2007 groot et al 2012 groot et al 2018 kennedy et al 2016 klein et al 2013 lautenbach et al 2013 lu and van ittersum 2004 mishra et al 2014 nguyen et al 2016 polasky et al 2008 qi and altinakar 2011 sadeghi et al 2009 santé riveira et al 2008 seppelt and voinov 2002 shaygan et al 2014 van butsic and kuemmerle 2015 whittaker et al 2017 other e g conservation planning forestry restoration watershed management aerts and heuvelink 2002 arabi et al 2006 keller et al 2015 rabotyagov et al 2010 randhir and shriver 2009 schröter et al 2014 
26381,environmental fate and transport processes are influenced by many factors simulation models that mimic these processes often have complex implementations which can lead to over parameterization sensitivity analyses are subsequently used to identify critical parameters whose uncertainties can be further reduced or better described and prediction variability minimized in this study a variance decomposition based global sensitivity analysis technique sobol method is conducted based on estimated concentrations in vertical soil compartments using the pesticide root zone model przm daily simulations are performed that explore the input parameter space estimated concentrations are compared to data collected over the course of a growing season from an experimental site in georgia our results suggest that model sensitivity is conditional and should be examined at appropriate spatial and temporal resolution to avoid omitting important parameters this approach can yield a better understanding about the interplay between sensitivity uncertainty and model dynamics in non monotonic non linear systems keywords sobol sensitivity analysis global sensitivity analysis vertical transport fate and transport pesticides 1 introduction 1 1 overview of sensitivity analysis sensitivity analysis investigates how uncertainty in model outputs can be apportioned to different input sources saltelli et al 2004 2008 which usually focuses on the following aspects 1 identifying parameters which are the most influential contributing most to output variability for the calibration process 2 highlighting parameters which require additional research for strengthening the knowledge base and 3 determining parameters which are insignificant and can be eliminated from the final model to avoid overparameterization hamby 1994 iman and helton 1988 based on the type of analysis it can be considered as qualitative or quantitative whose major methods are listed in fig 1 as a representative of the qualitative sensitivity analysis method scatter plot inspects the influence of individual inputs on an output visually cook and weisberg 2009 frey and patil 2002 although scatter plot does not provide quantitative rankings of inputs it could depict the possible non linear or non monotonic dependence between an input and output cook and weisberg 2009 thus scatter plot is usually selected as the first step in the sensitivity analysis which allows for the identification of potentially complex dependencies and guides for the selection of quantitative sensitively analysis methods a drawback of this approach is that interpretation of a scatter plot is subjective frey and patil 2002 quantitative sensitivity analysis methods are categorized into local or global approaches given the number of co perturbed parameters local methods refer to analyses which characterize model inputs local gradients at a given point at a time while other parameters usually are set to their nominal or mean values although with a low computational cost the oat method could contain biased results for non linear models since it assumes linearity between inputs and outputs nossent and bauwens 2012a saltelli et al 2005 global sensitivity analysis method searches the whole parameter space in a random or systematic approach in addition it allows all inputs to be varied simultaneously but has a high computational cost however with increasing computing capacity global sensitivity analysis has become more prevalent since this makes exploring multi dimensional parameter spaces are more feasible massmann and holzmann 2012 reusser et al 2011 saltelli et al 2005 thus local sensitivity analysis measures sensitivity information between a specific input value and the corresponding output space while global analyses consider the whole input and output spaces which allows all inputs to be varied simultaneously saltelli et al 2010 1 2 types of global sensitivity analysis global sensitivity analysis is a model independent technique which means that the sensitivity analysis method does not require a specific type of relationship between inputs and model outputs baroni and tarantola 2014 when full coverage of parameter spaces are properly sampled it can handle non linearity non monotonicity and non additivity models nossent and bauwens 2012a there are three types of global sensitivity analysis methods fig 1 screening methods regression based methods and variance based methods confalonieri et al 2010 the most common screening method morris method captures not only the overall importance of a parameter but also its interactions with other parameters the morris method is effective in identifying important parameters for a monotonic model at a reasonable computational cost for non monotonic models campolongo et al 2007 improved morris algorithms by considering absolute effective effects which reduces the probability of not identifying important parameters type ii error regression methods build linear equations between model inputs and outputs by sampling with monte carlo or latin hypercube techniques when inputs are independent of each other their standard regression coefficients src representing the effect of changing an input from its baseline value by a fixed fraction of the standard deviation are used to rank inputs based on their impacts on model outputs confalonieri et al 2010 however parameters ranked by src are based on the linear regression model which is used to describe the target model not directly on the target model saltelli et al 1999 the performance of this method depends on coefficient of determination r2 which captures the percentage of variance that can be explained by the proposed regression model confalonieri et al 2010 saltelli et al 2005 the regression method is thus better suited to linear models saltelli et al 1999 2008 although computationally intensive benefits associated to variance decomposition type of sensitivity analysis are significant which are reliable for both linear and non linear models as well as monotonic and non monotonic models global sensitivity analysis methods not only account for impacts from individual inputs but also consider influences from interactions among inputs fourier amplitude sensitivity test fast developed by cukier and his colleagues is considered to be the earliest variance based method fast method adopts fourier transformation function to sample input spaces in estimating their first order sensitivity indices which is also known as main effect contribution of each input to the variance of the output cukier et al 1975 saltelli et al 1999 later saltelli et al 1999 extended the original fast method with the ability to compute total sensitivity indices for identified parameters efast however the two main disadvantages for fast and efast methods are that they fail to sample inputs directly from their distributions and results are not reliable when inputs are not continuously distributed confalonieri et al 2010 frey and patil 2002 sobol method estimates inputs first and higher order sensitivity indices by evaluating a multidimensional integral through a monte carlo simulation patelli et al 2010 sobol method has become the most powerful sensitivity analysis techniques since it directly samples parameter spaces which is the main difference between fast and sobol method saltelli et al 1999 and is capable of handling the case of dependent variables glen and isaacs 2012 kucherenko et al 2012 li et al 2013 though sobol method is straightforward to apply it is computationally expensive as a result many researchers have developed optimized algorithms jansen 1999 saltelli 2002 saltelli et al 2010 sobol 2001 a comparison of commonly applied sensitivity methods are listed in table 1 and an in depth review are available from matott et al 2009 and pianosi et al 2016 1 3 study objective the objective of this study is to apply sobol sensitivity analysis on pesticide root zone model przm a compartment model that predicts the fate and transport of pesticides in unsaturated soil systems at plant root zone depths to explore parameters impacts to the estimated pesticide soil concentrations przm has been used by the usepa as part of their risk assessment process for pesticide registration we propose to evaluate those impacts spatially and temporally as suggested by recent studies guse et al 2014 herman et al 2013a 2013b massmann et al 2014 sieber and uhlenbrook 2005 the remaining parts are as follows the second part of the paper introduces background of przm model and algorithms used to compute sobol index the third part presents results based on a field pesticide leaching study conducted in south georgia jones and russell 2001 with this approach we conditionally identify sensitive parameters over time and vertical depth we use first order sobol sensitivity indices to locate the high priority model inputs based on their relative ranking that directly influence estimated pesticide soil concentrations at different time periods and depths in addition we use low ranking total order indices the sum of interaction terms with other varied parameters plus first order sensitivity index to focus on excluding non sensitive parameters based on their low first order and parameter interaction contributions 2 method 2 1 przm model przm is a heavily used one dimensional dynamic compartment model capable of predicting the fate and transport of pesticides in unsaturated soil systems at plant root zone depths przm is a finite difference model that uses a method of characteristics algorithm to solve partial difference equations that account for relevant climatic chemical and agronomic phenomena including soil temperature volatilization irrigation and cropping practices solubility sorption and microbial transformation processes two major components in a przm model are hydrology and chemical transport the hydrologic component estimates runoff erosion evapotranspiration and water movement the transport component distributes organic and inorganic chemicals in the soil przm has been applied in many environmental applications including pesticide leaching on agricultural lands banton and villeneuve 1989 chang et al 2008 du et al 2008 jackson and estes 2007 construction of municipal landfills aivalioti and karatzas 2006 and assessing industrial emissions shin et al 2011 when coupled with a geographic information system gis przm can assess transport from multiple spatial locations akbar and lin 2010 for example akbar et al 2011 developed a health risk map by spatially modeling bentazon leaching in woodruff county ak jackson et al 2007 created plus a przm and gis based tool which ranked the vulnerability caused by agricultural practices on over 8000 soil types and weather combinations similarly luo and zhang luo and zhang 2009 2010 2011 employed a geo referenced modeling system and simulated the spatiotemporal variations of pesticide transport at watershed scales another linkage used the exposure analysis modeling system exams to simulate impacts based on przm outputs within this framework chiovarou and siewicki 2008 compared risk to resident biota in estuarine headwaters at two locations with varying pesticide application scenarios and storm intensities davis et al 2007 estimated ecological risk to non target aquatic organisms from applying six common mosquitocides used to control west nile virus sabbagh et al 2010 coupled the przm exams approach with a vegetated filter strip vfs model to evaluate performance on pesticide reductions there have also been many verification studies testing whether predictions from przm are reliable in comparison to other models and or field observations przm has been compared to the root zone water quality model rzwqm fox et al 2006 the leaching estimation and chemistry model leachp the macro model mcqueen et al 2007 and has also been validated against field data mamy et al 2008 as with other highly dimensioned environmental models przm s inputs and outputs could contain is likely over parameterized despite the fact that its inputs and outputs have significant uncertainty the federal insecticide fungicide and rodenticide act fifra environmental model validation task force femvtf performed a model evaluation of przm by comparing model results against field measurements collected at 18 locations carbone 2002 carbone et al 2002 russell and jones 2002 singh and jones 2002 as part of the validation process femvtf conducted sensitivity analysis using plackett and burman s approach to identify the most influential parameters carbone 2002 plackett and burman 1946 warren hicks et al 2002 wolt et al 2002 although plackett and burman method is computational efficient it only evaluates parameters at two fixed levels and finds 2 way interactions beres and hawkins 2001 gan et al 2014 2 2 sobol method let equation 1 represent a model whose independent parameters are k elements in vector x and its output is a scalar 1 y f x f x 1 x k the essential concept behind sobol method is that function f x can be decomposed into a series of factors and their interactions with increasing dimensionality kucherenko et al 2009 eq 2 2 f x 1 x k f 0 i 1 k f i x i i j k f i j x i x j f 1 k x i x k where f0 can be treated as the expectation of eq 2 and subscripts i j k represents model parameters in addition if parameters of f x are independent and the individual terms are square integrable and have zero mean over their domain this decomposition is unique as a result the overall variance is expressed in eq 3 and it can be decomposed into eq 4 chan et al 1997 patelli et al 2010 3 v a r y f 2 x d x f 0 2 4 v a r y i 1 k v a r i y i 1 k v a r i j y v a r 1 k y where 5 v a r 1 k y f 1 k 2 x i x k d x 1 d x k the sobol sensitivity index sobol 2001 is the ratio between partial variance and total variance the first order index eq 6 measures the proportion of the total model output variance explained by variations in parameter i higher order index eq 7 captures this ratio between parameter interaction terms and total model output variance while total order index eq 8 accounts for the proportion of total variance explained by parameter xi as well as by interactions of xi and all other parameters when the sum of all first order indices si is close to 1 a model s inputs are more likely to be orthogonal meaning less impact due to interactions among parameters values calculated by eq 7 are quit low 6 first order index s i v a r i y v a r y 7 higher order index s i k v a r i k y v a r y 8 total order index s t i 1 v a r i y v a r y where va r i y represent variance from parameter i va r i k y is the variance from interaction terms va r i y represents total order variance which is total variance explained by all other parameters except xi on its own and also by all parameter combinations not including xi homma and saltelli 1996 researchers including sobol have developed several monte carlo based estimators to compute eqs 7 and 8 in this study we adopt saltelli et al s 2010 approach to quantify the first order variance and compute the total order variance as developed by jansen 1999 eq 10 these variance computations and their derivations are well developed in the literature jansen 1999 saltelli 2002 saltelli et al 2010 sobol 2001 sobol et al 2007 9 va r i y 1 n 1 j 1 n f b j f a b i j f a j 10 va r i y v a r y 1 2 n 1 j 1 n f a j f a b i j 2 where a and b are named sample and resample matrices which are two independent matrices based on inputs distributions with a dimension of n by k n iterations k variables and j is a loop index from iteration 1 to n matrix ab i s columns are identical to matrix a sample matrix except that the i th column comes from matrix b resample matrix thus if a model has k parameters there should be k different ab matrices ab 1 ab k composition of matrices a b and ab i are provide in fig 2 2 3 identifying sensitive parameters the objective of our study is to identify the most sensitive przm parameters using global sensitivity techniques presented in section 2 2 essential steps included in this assessment are described as a flowchart in fig 3 the first step is to generate matrices a and b which are sometimes referred to as sample and re sample matrices saltelli et al 2010 to simplify it is assumed that the total number of przm parameters to be evaluated is k corresponding to k columns in fig 2 and these variables will be sampled n times from either already known or assumed statistical distributions thus the dimension for matrix a or b is n by k to construct matrices ab i adopting a k iteration loop is suggested in which matrix a s i th column is replaced by the corresponding column from the b matrices finally matrices a b and ab i are vertically stacked into one combined matrix p which has n k 2 rows and k columns the second step is to execute a monte carlo simulation with a total of n k 2 iterations and collect model outputs which are time series of pesticide concentrations averaged over certain depth here outputs are the predicted average pesticide soil concentrations across six depths 0 15 cm 15 30 cm 30 45 cm 45 60 cm 60 75 cm 75 90 cm for data collected over 140 consecutive days between aug 13 1992 julian day 226 and dec 31 1992 julian day 366 thus the dimension for the return matrix is 840 by n k 2 which contains f a f b and f ab i the third and fourth steps apply eqs 6 and 8 10 to quantify the first and total order of sensitivity indices where higher first order indices are used to highlight parameters with conditionally identifiable influences and remove those with persistently low total order indices over the entire simulation period as relatively unimportant przm inputs as part of the post processing a small proportion of daily variable sensitivity index estimates for insensitive parameters that were slightly below zero due to numerical errors were set to zero further sampling did not diminish these negative values which was verified by increasing the dimension of a sample matrix a to 15 000 rows which took three days and 15 gb memory to evaluate the 195 000 simulations our convergence studies showed that additional further sampling and setting negative values to zero did not alter the sensitivity index probability distributions and the relative ranking of parameters therefore post processing of the estimates included setting all negative indices to zero the procedures described above were implemented in r r core team 2013 where inputs were generated passed to a compiled przm executable file and model output was post processed based on a previous field pesticide leaching study in south georgia jones and russell 2001 wolt et al 2002 11 candidate przm parameters inputs were selected and sampled independently their influences on predicted pesticide soil concentrations across different depths were evaluated the dimension of the sample resample matrix is 15 000 by 11 making the total evaluated przm functions 195 000 15 000 11 2 the justification for including these candidate parameters is based on a previous sensitivity analysis conducted by femvtf wolt et al 2002 in addition a rain intensity scalar parameter based on authors judgment is included to account for rainfall errors in measurement and extrapolation from the gauge location and also to allow for the assessment of the influence of the rainfall time series a known sensitive parameter for vertical transport a summary of ranges of inputs are presented in table 2 2 4 normalized nash sutcliffe efficiency normalized nash sutcliffe efficiency nnse eq 11 is applied to evaluate the predictive power of przm simulations against field mean observations nash and sutcliffe 1970 nossent and bauwens 2012b the nnse value ranges from 0 to 1 which represents very poor to perfect model performance a nnse of 0 5 indicates the performance of the dynamic model predictions is no better than a linear model fit through the mean of the observations in addition unlike the traditional nash sutcliffe efficiency the normalized coefficient can retain more accuracy in the estimated variance related terms nossent and bauwens 2012b 11 n n s e 1 1 t y o t y i t 2 t y o t y o 2 where yo t is the observation on day t y o is the average across all the observations and yi t is the simulated value on day t from the ith monte carlo realization we use nnse to provide context for the performance of the simulations it is not a required step in the process of applying the sobol sensitivity analysis 2 5 conditional pearson partial correlation coefficients conditional daily pearson partial correlation coefficients lee rodgers and nicewander 1988 between the sample matrix of 11 przm parameters and the model output pesticide soil concentrations were also run as a rough linear estimate of sensitivity the partial correlation is a global not local method but does not handle the non linearity of the pesticide concentration output or potential non linearity of inputs as sobol can however despite the linearity limitations the sign of the correlation coefficient indicating positive or negative correlation between the input and the output is information that is not provided by a sobol analysis 3 results 3 1 simulation of the pesticide soil concentration fig 4 illustrates the mean black solid the 5th green solid and 95th red solid percentiles of the simulated pesticide soil concentrations ppb against the mean black dashed minimum green dashed and maximum red dashed field observations at depths of 0 15 cm and 15 30 cm simulated data presented in fig 4 are used for the following sensitivity analysis and field observations are from a previous site specific leaching study conducted by femvtf as part of the przm model calibration projects jones and russell 2001 pesticide applications purple bars and rain events orange bars are also presented with height representing magnitude cm day of the event as expected the first soil compartment 0 15 cm depth shows increases in pesticide concentrations that are positively associated with pesticide applications reductions in pesticide concentrations due to transfer of pesticides to deeper soil depths are associated with precipitation events the magnitude of vertical pesticide concentration movement is positively linked to precipitation level for instance a significant precipitation event on julian dates 277 and 278 7 22 cm and 3 39 cm of rainfall caused the greatest pesticide concentration movement during the simulation period a source of pesticide concentration reduction is pesticide degradation which was captured between julian dates 294 and 306 an extended period with no new applications or rainfall events comparing simulated results and observed values shows similar trends over the observation period in general however simulations overestimated pesticide soil concentrations for the second soil compartment 15 30 cm depth magnitudes of simulated concentrations also decrease as a result of decay processes and loss to deeper compartments concentrations were relatively insensitive to timing of pesticide applications since time and rainfall are required for vertical transfer to greater depths pesticide concentrations in the second layer were positively correlated to rain events while first soil compartment concentrations were negatively correlated the simulated concentrations in the second compartment are consistently lower than simulated concentrations in the first compartment yet consistently above observed concentrations figure s1 of the supporting document includes the mean black solid the 5th green solid and 95th red solid percentiles of the simulated pesticide soil concentrations ppb as well as the timing of precipitation and pesticide application events for soil depths between 35 cm and 90 cm at increments of 15 cm no field observation data are included in figure s1 since few chemicals were detected in the soil below 35 cm patterns of pesticide concentrations among the four soil compartments are similar since uniform soil conditions are assumed for those layers including same soil parameters e g bulk density curve number etc and depth attenuation effects results of nnses show that for the first soil compartment 0 15 cm depth the median performance of the model predictions is similar to the mean of field observations 45 of the 195 000 nnses are greater than 0 5 however for the second soil compartment less than 1 of the 195 000 nnses are greater than 0 5 these results echo those of femvtf carbone 2002 fox et al 2006 mamy et al 2008 mcqueen et al 2007 sabbagh et al 2010 who found that adding a depth dimension to the decay parameters in przm and or adding a supplemental model was necessary to coerce reasonable fits to pesticide concentrations at deeper depths although the proportions of nnses could be increased by a model calibration process use of additional model for deeper depths and or by increasing the dimension of input parameters we are interested in examining the sensitivity of the przm model across the full distributions of the default przm input parameters the general trend of better model fits to observed data in the top soil compartment can visually be confirmed with fig 4 where field observations black dashed associated with the first soil compartment are closer to the simulations solid lines than those in deeper compartments 3 2 sensitivity indices the first si and total order sti sensitivity indices are presented as box plots for the 11 candidate parameters and an additional box representing the summation of first order indices fig 5 another possible visualization is through a circos plot kelleher et al 2013 pianosi et al 2016 box plots are chosen to present the entire distribution of sensitivity indices for all relevant days the first order sensitivity index measures the influence of candidate parameters on predicted pesticide soil concentrations across all six soil compartments during the simulation period julian dates 226 to 365 for year 1992 based on this index parameters with a moderate influence only includes pesticide decay rate in water and soil decay control after comparing their median index value to the threshold set to 0 2 shin et al 2013 van werkhoven et al 2009 if a parameter s median index value is less than the threshold it is considered as a non important parameter vanrolleghem et al 2015 including partitioning coefficient organic component kd oc and application rate tapp the rain intensity scalar rain bulk density bd and pesticide decay rate on foliage pldkrt the summation of first order sensitivity indices for all parameters can be employed to evaluate main effects of the parameters on outputs while a total sensitivity index considers impacts from parameter interactions saltelli 2002 saltelli et al 2010 the sum of the first order sensitivity indices suggests that 70 of the output variability can be explained by these first order parameter contributions while the remaining 30 is due to parameter interactions boxplots are generated based on sensitivity indices over the 140 day simulation period aug 13 1992 to dec 31 1992 in terms of the boxplot the band inside the box is always the second quartiles the median the bottom top of the box are always the first third quartiles the lower upper whiskers represent values from the bottom top to the ones that are within 1 5 iqr therefore the difference between the first order and the total sensitivity indices quantifies the joint effects of parameters on predicted pesticide concentrations a parameter with a low total sensitivity index typically implies that it is unimportant since the total sensitivity index captures direct impacts of the variable on model outputs and also accounts for its interaction with other candidate parameters from the perspective of the total sensitivity index parameters including pan factor pfac maximum rooting depth amxdr cropping runoff curve number cn c and management factors uslecs do not have a discernible impact on determining pesticide concentrations 3 3 conditional temporal analyses spatial and temporal sensitivity analyses are restricted to the six inputs whose total sensitivity indices are significantly different from zero they can be roughly ranked as follows decay control kd oc tapp rain bd and pldkrt changes of parameters first and total order of sensitivity indices over time julian days 226 366 for the top six influential parameters identified previously are illustrated by fig 6 to capture the impact of parameters across depth figures are generated for both the shallowest 0 15 cm left panel and the deepest 75 90 cm right panel soil compartments sensitivity indices for a given parameter can change significantly over the course of the simulation so we highlight changes in response to application and precipitation events fig 6 the parameter for the pesticide decay rate in water and soil decay control are related to precipitation and pesticide application events this is because the first order sensitivity index measures the proportion of total variance on model output dissolved pesticide soil concentrations that can be explained by a certain parameter thus any events e g precipitation pesticide application related to pesticide concentrations can have an influence on the calculated indices for pesticide decay process parameters in the first soil compartment precipitation events orange bars often trigger a sensitivity index decline during the event and is then followed by a significant rise in sensitivity in the days after the event this is because precipitations have a more direct impact on changing the predicted pesticide concentrations than the decay effects this phenomenon also can be observed from the lower half of table 3 where the 5th percent of the first order sensitivity index for decay control estimated during the precipitation periods 0 15 cm 3 72e 2 75 90 cm 2 39e 3 are smaller than those from dry periods 0 15 cm 7 84e 2 75 90 cm 1 90e 2 similarly first order sensitivity index for decay control estimated during the pesticide application periods are smaller than the one calculated during pesticide free period table 3 and fig 7 b this is because although pesticide applications provide mass for decay processes to operate on the applied pesticide has a more direct impact on the model predicted pesticide soil concentration for the rain intensity scalar rain increase in sensitivity indices is not surprisingly associated with precipitation events higher percentiles of first order sensitivity indices in table 3 however the magnitude of increase is more significant within the pesticide application period julian days 226 299 which could due to the existence of higher pesticide concentrations interesting during the same period the relative importance of the rain intensity scalar has been diluted by the pesticide application events which is because the proportion of model variability explained by other pesticide application related parameters has been increase also the first and total sensitivity index lines diverge in the post pesticide application period julian days 300 365 with total sensitivity increasing this implies that residual chemical concentrations are driven by interaction effects with other parameters at a deeper level the rain sensitivity index is less sensitive to precipitation events since rainfall interacts with the top soil compartment more directly and attenuation over the vertical profile can reduce direct correlation the divergence with depth between the first and total sensitivity indices also implies that deeper level soil concentrations are determined by multiple interacting factors the temporal change in the first order index for the pesticide decay rate on foliage pldkrt in the first soil compartment is also positively associated with precipitation events after pesticide applications higher percentiles under the column no in the first half of table 3 differences between the first and total sensitivity indices increase towards the end of the modeling period indicating that residual concentrations are jointly determined by several parameters as expected both sensitivity indices in the deepest modeled soil compartment are essentially zero suggesting that pldkrt has little impact on deeper soil concentrations sensitivity indices for the partitioning coefficient organic component kd oc are impacted by pesticide application and precipitation events pesticide applications reduce kd oc s sensitivity index since more pesticide mass is introduced into the soil system at the surface having a larger impact on immediate first compartment concentrations than system partitioning processes precipitation events do not show a consistent upward or downward effect on kd oc sensitivity indices precipitation could increase kd oc s sensitivity indices which is because first rainfall caused vertical water flow lowers pesticide soil concentrations by transferring them to deeper depth when this happened kd oc determined internal partitioning process becomes more important however in the deepest simulated soil compartment sensitivity indices of kd oc are not dependent on time or on events except the sensitivity curve shows a sharp peak at the beginning when the pesticide comes first into contact with the soil because it is used for estimating the equilibrium concentrations of pesticide in the soil sensitivity indices for bulk density bd are positively associated with pesticide applications and negatively related to precipitation in the upper compartments higher percentiles under the column yes in table 3 however these correlations change with depth in the last soil compartment indices are positively correlated only to precipitation and independent of pesticide applications although reasons behind these phenomena are not very clear it is true that high bd is an indicator of low soil porosity and soil compaction which impacts water infiltration usda 2015 the sensitivity curves for application rate tapp show temporal dependence on events in shallow soil compartments indices increase when pesticides have been applied higher percentiles under the column yes in first half of table 3 and decrease during high precipitation periods in the deepest soil layer 75 90 cm indices show no temporal dependence 3 4 conditional depth analyses to further illustrate the differences in the first red and total green order of sensitivity indices at different depths conditional on pesticide applications two sets of boxplots are conditionally generated based on whether the days are during the primary pesticide application period sensitivity indices on the left panels of fig 7a and b are based on periods with pesticide applications 25 application events between julian day 226 and 299 while the right panels represent days without pesticide applications julian day 300 365 within each figure boxplots of sensitivity indices associated to six different depths are listed on the x axis in ascending order for the rain intensity scalar the first order sensitivity index is for the most part depth independent although indices are somewhat higher during the pesticide application period the total sensitivity index of the rain intensity scalar is positively correlated to the soil depth with the exception of the first soil compartment during the no application period this positive correlation suggests that rain intensity scalar and other parameters jointly determine pesticide soil concentrations in deeper compartments the high total sensitivity index within the first 15 cm during the pesticide free period also indicates that the concentration in this area is jointly determined by several parameters the pesticide decay rate on foliage pldkrt parameter is not a sensitivity input during the pesticide application period which is because other parameters during the applied pesticide period could have more direct impacts on determining pesticide soil concentration while pldkrt can be identified as a sensitive input only in the first soil compartment during the pesticide free period which is because przm assumes that rainfall caused pesticide loss only transferring applied pesticide from foliage to the first 4 cm of soil suárez 2005 these findings demonstrate variability in the sensitivity of the parameters as a function of depth and pesticide application timing during the pesticide application period the first and total sensitivity indices of the partitioning coefficient organic kd oc increase with depth this implies that the partitioning coefficient has more influence on determining the pesticide soil concentrations at deeper depths where pesticides are more likely not evenly distributed however when no pesticides are applied the sensitivity indices of kd oc are depth independent likely because sufficient time has elapsed since the previous pesticide application to allow the soil concentration profile to be more uniformly distributed in fig 7b when pesticide are not applied soil bulk density bd is not an important model parameter based on its total sensitivity index during the pesticide application period sensitivity indices for the soil bulk density are positively correlated for the first two soil compartments but negatively correlated to the deeper components this situation is likely due to pesticide concentrations decreasing dramatically in deeper zones which reduces the influence of bulk density another good example to support the depth dependence of parameter sensitivity is the application rate parameter tapp this parameter is highly influential in the first soil zone however this impact diminishes quickly as soil depth increases attenuation in soil prevents the applied pesticide from quickly being transported to deeper components meanwhile other parameters and processes are needed to yield these concentrations at depth and therefore have higher sensitivity as expected during time periods when pesticides are not applied pesticide soil concentrations are insensitive to the application rate the importance of the pesticide decay rate in water and soil decay control parameter increases in the first two soil compartments then has little influence in the rest of the soil zones demonstrating a pattern similar to soil bulk density when pesticides are not applied its first order index has a similar trend to the left panel of fig 7b but with higher values therefore during time periods of no pesticide inputs decay rates play a relatively more important role at deeper depths this can be viewed in the total sensitivity index plots since there is an increase in sensitivity from the first to the rest of the depth zones which indicates pesticide decay rate has more power in determining pesticide soil concentrations in deeper areas due to interactions with other parameters 3 5 conditional pearson correlation pearson correlation coefficients daily conditional on pesticide applications and precipitations between the sample matrix of 11 przm parameters and the model output pesticide soil concentrations results of correlation coefficients across modeling period as well as over iterations were aggregated into boxplots in fig 8 to highlight figure s conditional feature five types of modeling periods were included days with pesticide application red days without pesticide applications light green days with precipitation green days without precipitation blue and days including all conditions purple based on the position of median correlation coefficients the horizontal bar inside box the parameters with moderate correlations to the outputs pesticide application timing partitioning coefficients decay control bulk density and rainfall intensity are similar to those identified by the sobol analysis however the magnitudes of correlation coefficients vary based on external events which highlights the importance of such conditional correlation analysis in absence of a global sensitivity analysis for instance pesticide decay rate in water and soil decay has less correlations to the predicted pesticide soil concentrations during either pesticide application period red or precipitations green which is because the existences of those external events could have a direct impact on the pesticide soil concentration comparing to sobol method the sign of the correlation coefficient indicating positive or negative correlation between the input and the output is information in this case significant negative correlations between output concentrations and both decay control and bulk density show that higher values for these parameters cause a decline in output concentrations within the estimated time series such conditional correlation analysis can also be applied to soil compartments at different depths however pearson correlation coefficients were estimated based on the linear assumption between inputs and outputs when a model s response curve is not linear it is possible that an input s pearson s coefficient is close to zero but its first order sensitivity index is close to 1 i e inputs and outputs has a u shape relationship under this situation it is necessary to reply on more sophisticated sensitivity analysis method although it is more efficient to calculate correlation coefficients 4 discussion and conclusion in this study we apply a global sensitivity analysis technique the sobol method on a widely adopted u s epa exposure model pesticide root zone model przm to conditionally identify sensitive parameters over time and vertical depth we use high ranking first order sensitivity indices to locate the high priority model inputs based on their relative ranking that directly influence estimated pesticide soil concentrations at different time periods and depths in addition we use low ranking total order indices the sum of interaction terms with other varied parameters plus first order sensitivity index to focus on excluding non sensitive parameters based on their low first order and parameter interaction contributions one of the most significant findings is that for highly parameterized complex models e g environmental fate and transport models determining the most sensitive parameters depends on temporal spatial and event driven conditions since not all input parameters will have the same relative power in determining model output over the different model dimensions sensitivities can and do change when they are evaluated temporally with depth as well as taking other related conditions into account comparing to estimating time varying averaged averaging across a time window or whole modeling period averaged sensitivity indices this study computed daily sensitivity indices the maximum time resolution based on inputs which could better identify parameters whose impacts only last a short time period or under certain conditions herman et al 2013b massmann et al 2014 in contrast parameters identified using conditional global sensitivity methods can better capture influential parameters at key spatial locations and time points in the model resulting in more accurate system behavior if parameterized and dimensioned correctly when used to estimate environmental conditions moreover this information can guide the process of model calibration for instance to better predict pesticide residual concentration at shallow depths 0 15 cm one should focus on pesticide application rates tapp pesticide decay rate on foliage pldkrt and rain intensity rain have more impact on predicted pesticide concentrations while in deeper soil compartments 15 cm more attention should be paid to the partitioning coefficient organic component kd oc the decay rate in the soil and water decay control and the accuracy of the rainfall time series when przm is applied to low precipitation areas rates of pesticide applications and chemical decay rates in water and soil are drivers in determining model outputs while these parameters become rain intensity and chemical decay rates on foliage at high precipitation areas these analyses present a more nuanced and robust assessment of the relative importance of model parameters over the course of the simulations in dynamic spatial and temporal simulations failing to capture variability provided by conditionally important input parameters during key time periods and or locations can lead to suboptimal modeling in terms of overall goodness of fit to observations this can happen early in the calibration process when sensitivity analyses are used to reduce the number of uncertain parameters despite the fact that final parameter ranges have not yet been identified in addition when unconditional sensitivity analyses are used to jettison variables that are important under certain conditions the overall dynamic features of the models may change therefore examination of conditional sensitivity analyses is less likely to lead to incorrect or misleading behavior of the models that are used for environmental applications and decision making one straightforward approach to leveraging conditional sensitivity analyses would be to use a higher percentile in addition use of the sobol method can provide insight into significant parameter interactions most sensitivity analyses conducted are based on the assumption that input variables are independent including the analysis conducted here however significant total order sensitivity index values sum of the interaction terms plus first order sensitivity index from the sobol analyses conducted with independence assumptions can be used as one indication regarding whether parameter covariance is important capturing interactions between parameters can have significant effects on system dynamics that influence the estimation of model outputs and subsequent goodness of fit these interactions can be captured in a monte carlo simulation e g adding parameters for an input variance covariance matrix during a calibration uncertainty analysis process although covariance in the monte carlo sampling can cause subsequent problems for both local and global sensitivity computation approaches including sobol these difficulties are not insurmountable and the sobol method is able to identify the significance of each possible interaction and can guide covariance decisions if such variance covariance matrix is not available one can also partition inputs into distinct sets the sobol method is still valid as long as the groups are independent inputs within each group can be correlated glen and isaacs 2012 in this application the 30 higher order interaction index sum indicates that estimation and calibration of such a matrix may capture some of the interaction variance with corresponding improvements in model fit candidate parameters in przm that may merit covariance parameters include pesticide decay rates for foliage and in water and soil pldkrt and decay control management factors uslec1 and uslec2 and soil properties bd and kdoc incorporation of additional covariance terms may address calibration difficulties in deeper depths where przm has traditionally labored for many applications software availability source code for the przm 3 model used for this manuscript is available from https www epa gov exposure assessment models przm version index r code input data results and binaries used for the analyses are available from a github repository located at https github com puruckertom hongpurucker2016 acknowledgments the authors appreciate valuable discussion and inputs from bertrand iooss a r package sensitivity author thanks to mike cyterski for peer review and fran rauschenberg for manuscript review and edits this research was supported in part by an appointment to the postdoctoral research program at the usepa ecosystems research division athens ga administered by the oak ridge institute for science and education through interagency agreement no dw8992298301 between the u s department of energy and the u s environmental protection agency the views expressed in this article are those of the authors and do not necessarily represent the views or policies of the u s environmental protection agency appendix a supplementary data the following are the supplementary data related to this article data profile data profile supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 018 
26381,environmental fate and transport processes are influenced by many factors simulation models that mimic these processes often have complex implementations which can lead to over parameterization sensitivity analyses are subsequently used to identify critical parameters whose uncertainties can be further reduced or better described and prediction variability minimized in this study a variance decomposition based global sensitivity analysis technique sobol method is conducted based on estimated concentrations in vertical soil compartments using the pesticide root zone model przm daily simulations are performed that explore the input parameter space estimated concentrations are compared to data collected over the course of a growing season from an experimental site in georgia our results suggest that model sensitivity is conditional and should be examined at appropriate spatial and temporal resolution to avoid omitting important parameters this approach can yield a better understanding about the interplay between sensitivity uncertainty and model dynamics in non monotonic non linear systems keywords sobol sensitivity analysis global sensitivity analysis vertical transport fate and transport pesticides 1 introduction 1 1 overview of sensitivity analysis sensitivity analysis investigates how uncertainty in model outputs can be apportioned to different input sources saltelli et al 2004 2008 which usually focuses on the following aspects 1 identifying parameters which are the most influential contributing most to output variability for the calibration process 2 highlighting parameters which require additional research for strengthening the knowledge base and 3 determining parameters which are insignificant and can be eliminated from the final model to avoid overparameterization hamby 1994 iman and helton 1988 based on the type of analysis it can be considered as qualitative or quantitative whose major methods are listed in fig 1 as a representative of the qualitative sensitivity analysis method scatter plot inspects the influence of individual inputs on an output visually cook and weisberg 2009 frey and patil 2002 although scatter plot does not provide quantitative rankings of inputs it could depict the possible non linear or non monotonic dependence between an input and output cook and weisberg 2009 thus scatter plot is usually selected as the first step in the sensitivity analysis which allows for the identification of potentially complex dependencies and guides for the selection of quantitative sensitively analysis methods a drawback of this approach is that interpretation of a scatter plot is subjective frey and patil 2002 quantitative sensitivity analysis methods are categorized into local or global approaches given the number of co perturbed parameters local methods refer to analyses which characterize model inputs local gradients at a given point at a time while other parameters usually are set to their nominal or mean values although with a low computational cost the oat method could contain biased results for non linear models since it assumes linearity between inputs and outputs nossent and bauwens 2012a saltelli et al 2005 global sensitivity analysis method searches the whole parameter space in a random or systematic approach in addition it allows all inputs to be varied simultaneously but has a high computational cost however with increasing computing capacity global sensitivity analysis has become more prevalent since this makes exploring multi dimensional parameter spaces are more feasible massmann and holzmann 2012 reusser et al 2011 saltelli et al 2005 thus local sensitivity analysis measures sensitivity information between a specific input value and the corresponding output space while global analyses consider the whole input and output spaces which allows all inputs to be varied simultaneously saltelli et al 2010 1 2 types of global sensitivity analysis global sensitivity analysis is a model independent technique which means that the sensitivity analysis method does not require a specific type of relationship between inputs and model outputs baroni and tarantola 2014 when full coverage of parameter spaces are properly sampled it can handle non linearity non monotonicity and non additivity models nossent and bauwens 2012a there are three types of global sensitivity analysis methods fig 1 screening methods regression based methods and variance based methods confalonieri et al 2010 the most common screening method morris method captures not only the overall importance of a parameter but also its interactions with other parameters the morris method is effective in identifying important parameters for a monotonic model at a reasonable computational cost for non monotonic models campolongo et al 2007 improved morris algorithms by considering absolute effective effects which reduces the probability of not identifying important parameters type ii error regression methods build linear equations between model inputs and outputs by sampling with monte carlo or latin hypercube techniques when inputs are independent of each other their standard regression coefficients src representing the effect of changing an input from its baseline value by a fixed fraction of the standard deviation are used to rank inputs based on their impacts on model outputs confalonieri et al 2010 however parameters ranked by src are based on the linear regression model which is used to describe the target model not directly on the target model saltelli et al 1999 the performance of this method depends on coefficient of determination r2 which captures the percentage of variance that can be explained by the proposed regression model confalonieri et al 2010 saltelli et al 2005 the regression method is thus better suited to linear models saltelli et al 1999 2008 although computationally intensive benefits associated to variance decomposition type of sensitivity analysis are significant which are reliable for both linear and non linear models as well as monotonic and non monotonic models global sensitivity analysis methods not only account for impacts from individual inputs but also consider influences from interactions among inputs fourier amplitude sensitivity test fast developed by cukier and his colleagues is considered to be the earliest variance based method fast method adopts fourier transformation function to sample input spaces in estimating their first order sensitivity indices which is also known as main effect contribution of each input to the variance of the output cukier et al 1975 saltelli et al 1999 later saltelli et al 1999 extended the original fast method with the ability to compute total sensitivity indices for identified parameters efast however the two main disadvantages for fast and efast methods are that they fail to sample inputs directly from their distributions and results are not reliable when inputs are not continuously distributed confalonieri et al 2010 frey and patil 2002 sobol method estimates inputs first and higher order sensitivity indices by evaluating a multidimensional integral through a monte carlo simulation patelli et al 2010 sobol method has become the most powerful sensitivity analysis techniques since it directly samples parameter spaces which is the main difference between fast and sobol method saltelli et al 1999 and is capable of handling the case of dependent variables glen and isaacs 2012 kucherenko et al 2012 li et al 2013 though sobol method is straightforward to apply it is computationally expensive as a result many researchers have developed optimized algorithms jansen 1999 saltelli 2002 saltelli et al 2010 sobol 2001 a comparison of commonly applied sensitivity methods are listed in table 1 and an in depth review are available from matott et al 2009 and pianosi et al 2016 1 3 study objective the objective of this study is to apply sobol sensitivity analysis on pesticide root zone model przm a compartment model that predicts the fate and transport of pesticides in unsaturated soil systems at plant root zone depths to explore parameters impacts to the estimated pesticide soil concentrations przm has been used by the usepa as part of their risk assessment process for pesticide registration we propose to evaluate those impacts spatially and temporally as suggested by recent studies guse et al 2014 herman et al 2013a 2013b massmann et al 2014 sieber and uhlenbrook 2005 the remaining parts are as follows the second part of the paper introduces background of przm model and algorithms used to compute sobol index the third part presents results based on a field pesticide leaching study conducted in south georgia jones and russell 2001 with this approach we conditionally identify sensitive parameters over time and vertical depth we use first order sobol sensitivity indices to locate the high priority model inputs based on their relative ranking that directly influence estimated pesticide soil concentrations at different time periods and depths in addition we use low ranking total order indices the sum of interaction terms with other varied parameters plus first order sensitivity index to focus on excluding non sensitive parameters based on their low first order and parameter interaction contributions 2 method 2 1 przm model przm is a heavily used one dimensional dynamic compartment model capable of predicting the fate and transport of pesticides in unsaturated soil systems at plant root zone depths przm is a finite difference model that uses a method of characteristics algorithm to solve partial difference equations that account for relevant climatic chemical and agronomic phenomena including soil temperature volatilization irrigation and cropping practices solubility sorption and microbial transformation processes two major components in a przm model are hydrology and chemical transport the hydrologic component estimates runoff erosion evapotranspiration and water movement the transport component distributes organic and inorganic chemicals in the soil przm has been applied in many environmental applications including pesticide leaching on agricultural lands banton and villeneuve 1989 chang et al 2008 du et al 2008 jackson and estes 2007 construction of municipal landfills aivalioti and karatzas 2006 and assessing industrial emissions shin et al 2011 when coupled with a geographic information system gis przm can assess transport from multiple spatial locations akbar and lin 2010 for example akbar et al 2011 developed a health risk map by spatially modeling bentazon leaching in woodruff county ak jackson et al 2007 created plus a przm and gis based tool which ranked the vulnerability caused by agricultural practices on over 8000 soil types and weather combinations similarly luo and zhang luo and zhang 2009 2010 2011 employed a geo referenced modeling system and simulated the spatiotemporal variations of pesticide transport at watershed scales another linkage used the exposure analysis modeling system exams to simulate impacts based on przm outputs within this framework chiovarou and siewicki 2008 compared risk to resident biota in estuarine headwaters at two locations with varying pesticide application scenarios and storm intensities davis et al 2007 estimated ecological risk to non target aquatic organisms from applying six common mosquitocides used to control west nile virus sabbagh et al 2010 coupled the przm exams approach with a vegetated filter strip vfs model to evaluate performance on pesticide reductions there have also been many verification studies testing whether predictions from przm are reliable in comparison to other models and or field observations przm has been compared to the root zone water quality model rzwqm fox et al 2006 the leaching estimation and chemistry model leachp the macro model mcqueen et al 2007 and has also been validated against field data mamy et al 2008 as with other highly dimensioned environmental models przm s inputs and outputs could contain is likely over parameterized despite the fact that its inputs and outputs have significant uncertainty the federal insecticide fungicide and rodenticide act fifra environmental model validation task force femvtf performed a model evaluation of przm by comparing model results against field measurements collected at 18 locations carbone 2002 carbone et al 2002 russell and jones 2002 singh and jones 2002 as part of the validation process femvtf conducted sensitivity analysis using plackett and burman s approach to identify the most influential parameters carbone 2002 plackett and burman 1946 warren hicks et al 2002 wolt et al 2002 although plackett and burman method is computational efficient it only evaluates parameters at two fixed levels and finds 2 way interactions beres and hawkins 2001 gan et al 2014 2 2 sobol method let equation 1 represent a model whose independent parameters are k elements in vector x and its output is a scalar 1 y f x f x 1 x k the essential concept behind sobol method is that function f x can be decomposed into a series of factors and their interactions with increasing dimensionality kucherenko et al 2009 eq 2 2 f x 1 x k f 0 i 1 k f i x i i j k f i j x i x j f 1 k x i x k where f0 can be treated as the expectation of eq 2 and subscripts i j k represents model parameters in addition if parameters of f x are independent and the individual terms are square integrable and have zero mean over their domain this decomposition is unique as a result the overall variance is expressed in eq 3 and it can be decomposed into eq 4 chan et al 1997 patelli et al 2010 3 v a r y f 2 x d x f 0 2 4 v a r y i 1 k v a r i y i 1 k v a r i j y v a r 1 k y where 5 v a r 1 k y f 1 k 2 x i x k d x 1 d x k the sobol sensitivity index sobol 2001 is the ratio between partial variance and total variance the first order index eq 6 measures the proportion of the total model output variance explained by variations in parameter i higher order index eq 7 captures this ratio between parameter interaction terms and total model output variance while total order index eq 8 accounts for the proportion of total variance explained by parameter xi as well as by interactions of xi and all other parameters when the sum of all first order indices si is close to 1 a model s inputs are more likely to be orthogonal meaning less impact due to interactions among parameters values calculated by eq 7 are quit low 6 first order index s i v a r i y v a r y 7 higher order index s i k v a r i k y v a r y 8 total order index s t i 1 v a r i y v a r y where va r i y represent variance from parameter i va r i k y is the variance from interaction terms va r i y represents total order variance which is total variance explained by all other parameters except xi on its own and also by all parameter combinations not including xi homma and saltelli 1996 researchers including sobol have developed several monte carlo based estimators to compute eqs 7 and 8 in this study we adopt saltelli et al s 2010 approach to quantify the first order variance and compute the total order variance as developed by jansen 1999 eq 10 these variance computations and their derivations are well developed in the literature jansen 1999 saltelli 2002 saltelli et al 2010 sobol 2001 sobol et al 2007 9 va r i y 1 n 1 j 1 n f b j f a b i j f a j 10 va r i y v a r y 1 2 n 1 j 1 n f a j f a b i j 2 where a and b are named sample and resample matrices which are two independent matrices based on inputs distributions with a dimension of n by k n iterations k variables and j is a loop index from iteration 1 to n matrix ab i s columns are identical to matrix a sample matrix except that the i th column comes from matrix b resample matrix thus if a model has k parameters there should be k different ab matrices ab 1 ab k composition of matrices a b and ab i are provide in fig 2 2 3 identifying sensitive parameters the objective of our study is to identify the most sensitive przm parameters using global sensitivity techniques presented in section 2 2 essential steps included in this assessment are described as a flowchart in fig 3 the first step is to generate matrices a and b which are sometimes referred to as sample and re sample matrices saltelli et al 2010 to simplify it is assumed that the total number of przm parameters to be evaluated is k corresponding to k columns in fig 2 and these variables will be sampled n times from either already known or assumed statistical distributions thus the dimension for matrix a or b is n by k to construct matrices ab i adopting a k iteration loop is suggested in which matrix a s i th column is replaced by the corresponding column from the b matrices finally matrices a b and ab i are vertically stacked into one combined matrix p which has n k 2 rows and k columns the second step is to execute a monte carlo simulation with a total of n k 2 iterations and collect model outputs which are time series of pesticide concentrations averaged over certain depth here outputs are the predicted average pesticide soil concentrations across six depths 0 15 cm 15 30 cm 30 45 cm 45 60 cm 60 75 cm 75 90 cm for data collected over 140 consecutive days between aug 13 1992 julian day 226 and dec 31 1992 julian day 366 thus the dimension for the return matrix is 840 by n k 2 which contains f a f b and f ab i the third and fourth steps apply eqs 6 and 8 10 to quantify the first and total order of sensitivity indices where higher first order indices are used to highlight parameters with conditionally identifiable influences and remove those with persistently low total order indices over the entire simulation period as relatively unimportant przm inputs as part of the post processing a small proportion of daily variable sensitivity index estimates for insensitive parameters that were slightly below zero due to numerical errors were set to zero further sampling did not diminish these negative values which was verified by increasing the dimension of a sample matrix a to 15 000 rows which took three days and 15 gb memory to evaluate the 195 000 simulations our convergence studies showed that additional further sampling and setting negative values to zero did not alter the sensitivity index probability distributions and the relative ranking of parameters therefore post processing of the estimates included setting all negative indices to zero the procedures described above were implemented in r r core team 2013 where inputs were generated passed to a compiled przm executable file and model output was post processed based on a previous field pesticide leaching study in south georgia jones and russell 2001 wolt et al 2002 11 candidate przm parameters inputs were selected and sampled independently their influences on predicted pesticide soil concentrations across different depths were evaluated the dimension of the sample resample matrix is 15 000 by 11 making the total evaluated przm functions 195 000 15 000 11 2 the justification for including these candidate parameters is based on a previous sensitivity analysis conducted by femvtf wolt et al 2002 in addition a rain intensity scalar parameter based on authors judgment is included to account for rainfall errors in measurement and extrapolation from the gauge location and also to allow for the assessment of the influence of the rainfall time series a known sensitive parameter for vertical transport a summary of ranges of inputs are presented in table 2 2 4 normalized nash sutcliffe efficiency normalized nash sutcliffe efficiency nnse eq 11 is applied to evaluate the predictive power of przm simulations against field mean observations nash and sutcliffe 1970 nossent and bauwens 2012b the nnse value ranges from 0 to 1 which represents very poor to perfect model performance a nnse of 0 5 indicates the performance of the dynamic model predictions is no better than a linear model fit through the mean of the observations in addition unlike the traditional nash sutcliffe efficiency the normalized coefficient can retain more accuracy in the estimated variance related terms nossent and bauwens 2012b 11 n n s e 1 1 t y o t y i t 2 t y o t y o 2 where yo t is the observation on day t y o is the average across all the observations and yi t is the simulated value on day t from the ith monte carlo realization we use nnse to provide context for the performance of the simulations it is not a required step in the process of applying the sobol sensitivity analysis 2 5 conditional pearson partial correlation coefficients conditional daily pearson partial correlation coefficients lee rodgers and nicewander 1988 between the sample matrix of 11 przm parameters and the model output pesticide soil concentrations were also run as a rough linear estimate of sensitivity the partial correlation is a global not local method but does not handle the non linearity of the pesticide concentration output or potential non linearity of inputs as sobol can however despite the linearity limitations the sign of the correlation coefficient indicating positive or negative correlation between the input and the output is information that is not provided by a sobol analysis 3 results 3 1 simulation of the pesticide soil concentration fig 4 illustrates the mean black solid the 5th green solid and 95th red solid percentiles of the simulated pesticide soil concentrations ppb against the mean black dashed minimum green dashed and maximum red dashed field observations at depths of 0 15 cm and 15 30 cm simulated data presented in fig 4 are used for the following sensitivity analysis and field observations are from a previous site specific leaching study conducted by femvtf as part of the przm model calibration projects jones and russell 2001 pesticide applications purple bars and rain events orange bars are also presented with height representing magnitude cm day of the event as expected the first soil compartment 0 15 cm depth shows increases in pesticide concentrations that are positively associated with pesticide applications reductions in pesticide concentrations due to transfer of pesticides to deeper soil depths are associated with precipitation events the magnitude of vertical pesticide concentration movement is positively linked to precipitation level for instance a significant precipitation event on julian dates 277 and 278 7 22 cm and 3 39 cm of rainfall caused the greatest pesticide concentration movement during the simulation period a source of pesticide concentration reduction is pesticide degradation which was captured between julian dates 294 and 306 an extended period with no new applications or rainfall events comparing simulated results and observed values shows similar trends over the observation period in general however simulations overestimated pesticide soil concentrations for the second soil compartment 15 30 cm depth magnitudes of simulated concentrations also decrease as a result of decay processes and loss to deeper compartments concentrations were relatively insensitive to timing of pesticide applications since time and rainfall are required for vertical transfer to greater depths pesticide concentrations in the second layer were positively correlated to rain events while first soil compartment concentrations were negatively correlated the simulated concentrations in the second compartment are consistently lower than simulated concentrations in the first compartment yet consistently above observed concentrations figure s1 of the supporting document includes the mean black solid the 5th green solid and 95th red solid percentiles of the simulated pesticide soil concentrations ppb as well as the timing of precipitation and pesticide application events for soil depths between 35 cm and 90 cm at increments of 15 cm no field observation data are included in figure s1 since few chemicals were detected in the soil below 35 cm patterns of pesticide concentrations among the four soil compartments are similar since uniform soil conditions are assumed for those layers including same soil parameters e g bulk density curve number etc and depth attenuation effects results of nnses show that for the first soil compartment 0 15 cm depth the median performance of the model predictions is similar to the mean of field observations 45 of the 195 000 nnses are greater than 0 5 however for the second soil compartment less than 1 of the 195 000 nnses are greater than 0 5 these results echo those of femvtf carbone 2002 fox et al 2006 mamy et al 2008 mcqueen et al 2007 sabbagh et al 2010 who found that adding a depth dimension to the decay parameters in przm and or adding a supplemental model was necessary to coerce reasonable fits to pesticide concentrations at deeper depths although the proportions of nnses could be increased by a model calibration process use of additional model for deeper depths and or by increasing the dimension of input parameters we are interested in examining the sensitivity of the przm model across the full distributions of the default przm input parameters the general trend of better model fits to observed data in the top soil compartment can visually be confirmed with fig 4 where field observations black dashed associated with the first soil compartment are closer to the simulations solid lines than those in deeper compartments 3 2 sensitivity indices the first si and total order sti sensitivity indices are presented as box plots for the 11 candidate parameters and an additional box representing the summation of first order indices fig 5 another possible visualization is through a circos plot kelleher et al 2013 pianosi et al 2016 box plots are chosen to present the entire distribution of sensitivity indices for all relevant days the first order sensitivity index measures the influence of candidate parameters on predicted pesticide soil concentrations across all six soil compartments during the simulation period julian dates 226 to 365 for year 1992 based on this index parameters with a moderate influence only includes pesticide decay rate in water and soil decay control after comparing their median index value to the threshold set to 0 2 shin et al 2013 van werkhoven et al 2009 if a parameter s median index value is less than the threshold it is considered as a non important parameter vanrolleghem et al 2015 including partitioning coefficient organic component kd oc and application rate tapp the rain intensity scalar rain bulk density bd and pesticide decay rate on foliage pldkrt the summation of first order sensitivity indices for all parameters can be employed to evaluate main effects of the parameters on outputs while a total sensitivity index considers impacts from parameter interactions saltelli 2002 saltelli et al 2010 the sum of the first order sensitivity indices suggests that 70 of the output variability can be explained by these first order parameter contributions while the remaining 30 is due to parameter interactions boxplots are generated based on sensitivity indices over the 140 day simulation period aug 13 1992 to dec 31 1992 in terms of the boxplot the band inside the box is always the second quartiles the median the bottom top of the box are always the first third quartiles the lower upper whiskers represent values from the bottom top to the ones that are within 1 5 iqr therefore the difference between the first order and the total sensitivity indices quantifies the joint effects of parameters on predicted pesticide concentrations a parameter with a low total sensitivity index typically implies that it is unimportant since the total sensitivity index captures direct impacts of the variable on model outputs and also accounts for its interaction with other candidate parameters from the perspective of the total sensitivity index parameters including pan factor pfac maximum rooting depth amxdr cropping runoff curve number cn c and management factors uslecs do not have a discernible impact on determining pesticide concentrations 3 3 conditional temporal analyses spatial and temporal sensitivity analyses are restricted to the six inputs whose total sensitivity indices are significantly different from zero they can be roughly ranked as follows decay control kd oc tapp rain bd and pldkrt changes of parameters first and total order of sensitivity indices over time julian days 226 366 for the top six influential parameters identified previously are illustrated by fig 6 to capture the impact of parameters across depth figures are generated for both the shallowest 0 15 cm left panel and the deepest 75 90 cm right panel soil compartments sensitivity indices for a given parameter can change significantly over the course of the simulation so we highlight changes in response to application and precipitation events fig 6 the parameter for the pesticide decay rate in water and soil decay control are related to precipitation and pesticide application events this is because the first order sensitivity index measures the proportion of total variance on model output dissolved pesticide soil concentrations that can be explained by a certain parameter thus any events e g precipitation pesticide application related to pesticide concentrations can have an influence on the calculated indices for pesticide decay process parameters in the first soil compartment precipitation events orange bars often trigger a sensitivity index decline during the event and is then followed by a significant rise in sensitivity in the days after the event this is because precipitations have a more direct impact on changing the predicted pesticide concentrations than the decay effects this phenomenon also can be observed from the lower half of table 3 where the 5th percent of the first order sensitivity index for decay control estimated during the precipitation periods 0 15 cm 3 72e 2 75 90 cm 2 39e 3 are smaller than those from dry periods 0 15 cm 7 84e 2 75 90 cm 1 90e 2 similarly first order sensitivity index for decay control estimated during the pesticide application periods are smaller than the one calculated during pesticide free period table 3 and fig 7 b this is because although pesticide applications provide mass for decay processes to operate on the applied pesticide has a more direct impact on the model predicted pesticide soil concentration for the rain intensity scalar rain increase in sensitivity indices is not surprisingly associated with precipitation events higher percentiles of first order sensitivity indices in table 3 however the magnitude of increase is more significant within the pesticide application period julian days 226 299 which could due to the existence of higher pesticide concentrations interesting during the same period the relative importance of the rain intensity scalar has been diluted by the pesticide application events which is because the proportion of model variability explained by other pesticide application related parameters has been increase also the first and total sensitivity index lines diverge in the post pesticide application period julian days 300 365 with total sensitivity increasing this implies that residual chemical concentrations are driven by interaction effects with other parameters at a deeper level the rain sensitivity index is less sensitive to precipitation events since rainfall interacts with the top soil compartment more directly and attenuation over the vertical profile can reduce direct correlation the divergence with depth between the first and total sensitivity indices also implies that deeper level soil concentrations are determined by multiple interacting factors the temporal change in the first order index for the pesticide decay rate on foliage pldkrt in the first soil compartment is also positively associated with precipitation events after pesticide applications higher percentiles under the column no in the first half of table 3 differences between the first and total sensitivity indices increase towards the end of the modeling period indicating that residual concentrations are jointly determined by several parameters as expected both sensitivity indices in the deepest modeled soil compartment are essentially zero suggesting that pldkrt has little impact on deeper soil concentrations sensitivity indices for the partitioning coefficient organic component kd oc are impacted by pesticide application and precipitation events pesticide applications reduce kd oc s sensitivity index since more pesticide mass is introduced into the soil system at the surface having a larger impact on immediate first compartment concentrations than system partitioning processes precipitation events do not show a consistent upward or downward effect on kd oc sensitivity indices precipitation could increase kd oc s sensitivity indices which is because first rainfall caused vertical water flow lowers pesticide soil concentrations by transferring them to deeper depth when this happened kd oc determined internal partitioning process becomes more important however in the deepest simulated soil compartment sensitivity indices of kd oc are not dependent on time or on events except the sensitivity curve shows a sharp peak at the beginning when the pesticide comes first into contact with the soil because it is used for estimating the equilibrium concentrations of pesticide in the soil sensitivity indices for bulk density bd are positively associated with pesticide applications and negatively related to precipitation in the upper compartments higher percentiles under the column yes in table 3 however these correlations change with depth in the last soil compartment indices are positively correlated only to precipitation and independent of pesticide applications although reasons behind these phenomena are not very clear it is true that high bd is an indicator of low soil porosity and soil compaction which impacts water infiltration usda 2015 the sensitivity curves for application rate tapp show temporal dependence on events in shallow soil compartments indices increase when pesticides have been applied higher percentiles under the column yes in first half of table 3 and decrease during high precipitation periods in the deepest soil layer 75 90 cm indices show no temporal dependence 3 4 conditional depth analyses to further illustrate the differences in the first red and total green order of sensitivity indices at different depths conditional on pesticide applications two sets of boxplots are conditionally generated based on whether the days are during the primary pesticide application period sensitivity indices on the left panels of fig 7a and b are based on periods with pesticide applications 25 application events between julian day 226 and 299 while the right panels represent days without pesticide applications julian day 300 365 within each figure boxplots of sensitivity indices associated to six different depths are listed on the x axis in ascending order for the rain intensity scalar the first order sensitivity index is for the most part depth independent although indices are somewhat higher during the pesticide application period the total sensitivity index of the rain intensity scalar is positively correlated to the soil depth with the exception of the first soil compartment during the no application period this positive correlation suggests that rain intensity scalar and other parameters jointly determine pesticide soil concentrations in deeper compartments the high total sensitivity index within the first 15 cm during the pesticide free period also indicates that the concentration in this area is jointly determined by several parameters the pesticide decay rate on foliage pldkrt parameter is not a sensitivity input during the pesticide application period which is because other parameters during the applied pesticide period could have more direct impacts on determining pesticide soil concentration while pldkrt can be identified as a sensitive input only in the first soil compartment during the pesticide free period which is because przm assumes that rainfall caused pesticide loss only transferring applied pesticide from foliage to the first 4 cm of soil suárez 2005 these findings demonstrate variability in the sensitivity of the parameters as a function of depth and pesticide application timing during the pesticide application period the first and total sensitivity indices of the partitioning coefficient organic kd oc increase with depth this implies that the partitioning coefficient has more influence on determining the pesticide soil concentrations at deeper depths where pesticides are more likely not evenly distributed however when no pesticides are applied the sensitivity indices of kd oc are depth independent likely because sufficient time has elapsed since the previous pesticide application to allow the soil concentration profile to be more uniformly distributed in fig 7b when pesticide are not applied soil bulk density bd is not an important model parameter based on its total sensitivity index during the pesticide application period sensitivity indices for the soil bulk density are positively correlated for the first two soil compartments but negatively correlated to the deeper components this situation is likely due to pesticide concentrations decreasing dramatically in deeper zones which reduces the influence of bulk density another good example to support the depth dependence of parameter sensitivity is the application rate parameter tapp this parameter is highly influential in the first soil zone however this impact diminishes quickly as soil depth increases attenuation in soil prevents the applied pesticide from quickly being transported to deeper components meanwhile other parameters and processes are needed to yield these concentrations at depth and therefore have higher sensitivity as expected during time periods when pesticides are not applied pesticide soil concentrations are insensitive to the application rate the importance of the pesticide decay rate in water and soil decay control parameter increases in the first two soil compartments then has little influence in the rest of the soil zones demonstrating a pattern similar to soil bulk density when pesticides are not applied its first order index has a similar trend to the left panel of fig 7b but with higher values therefore during time periods of no pesticide inputs decay rates play a relatively more important role at deeper depths this can be viewed in the total sensitivity index plots since there is an increase in sensitivity from the first to the rest of the depth zones which indicates pesticide decay rate has more power in determining pesticide soil concentrations in deeper areas due to interactions with other parameters 3 5 conditional pearson correlation pearson correlation coefficients daily conditional on pesticide applications and precipitations between the sample matrix of 11 przm parameters and the model output pesticide soil concentrations results of correlation coefficients across modeling period as well as over iterations were aggregated into boxplots in fig 8 to highlight figure s conditional feature five types of modeling periods were included days with pesticide application red days without pesticide applications light green days with precipitation green days without precipitation blue and days including all conditions purple based on the position of median correlation coefficients the horizontal bar inside box the parameters with moderate correlations to the outputs pesticide application timing partitioning coefficients decay control bulk density and rainfall intensity are similar to those identified by the sobol analysis however the magnitudes of correlation coefficients vary based on external events which highlights the importance of such conditional correlation analysis in absence of a global sensitivity analysis for instance pesticide decay rate in water and soil decay has less correlations to the predicted pesticide soil concentrations during either pesticide application period red or precipitations green which is because the existences of those external events could have a direct impact on the pesticide soil concentration comparing to sobol method the sign of the correlation coefficient indicating positive or negative correlation between the input and the output is information in this case significant negative correlations between output concentrations and both decay control and bulk density show that higher values for these parameters cause a decline in output concentrations within the estimated time series such conditional correlation analysis can also be applied to soil compartments at different depths however pearson correlation coefficients were estimated based on the linear assumption between inputs and outputs when a model s response curve is not linear it is possible that an input s pearson s coefficient is close to zero but its first order sensitivity index is close to 1 i e inputs and outputs has a u shape relationship under this situation it is necessary to reply on more sophisticated sensitivity analysis method although it is more efficient to calculate correlation coefficients 4 discussion and conclusion in this study we apply a global sensitivity analysis technique the sobol method on a widely adopted u s epa exposure model pesticide root zone model przm to conditionally identify sensitive parameters over time and vertical depth we use high ranking first order sensitivity indices to locate the high priority model inputs based on their relative ranking that directly influence estimated pesticide soil concentrations at different time periods and depths in addition we use low ranking total order indices the sum of interaction terms with other varied parameters plus first order sensitivity index to focus on excluding non sensitive parameters based on their low first order and parameter interaction contributions one of the most significant findings is that for highly parameterized complex models e g environmental fate and transport models determining the most sensitive parameters depends on temporal spatial and event driven conditions since not all input parameters will have the same relative power in determining model output over the different model dimensions sensitivities can and do change when they are evaluated temporally with depth as well as taking other related conditions into account comparing to estimating time varying averaged averaging across a time window or whole modeling period averaged sensitivity indices this study computed daily sensitivity indices the maximum time resolution based on inputs which could better identify parameters whose impacts only last a short time period or under certain conditions herman et al 2013b massmann et al 2014 in contrast parameters identified using conditional global sensitivity methods can better capture influential parameters at key spatial locations and time points in the model resulting in more accurate system behavior if parameterized and dimensioned correctly when used to estimate environmental conditions moreover this information can guide the process of model calibration for instance to better predict pesticide residual concentration at shallow depths 0 15 cm one should focus on pesticide application rates tapp pesticide decay rate on foliage pldkrt and rain intensity rain have more impact on predicted pesticide concentrations while in deeper soil compartments 15 cm more attention should be paid to the partitioning coefficient organic component kd oc the decay rate in the soil and water decay control and the accuracy of the rainfall time series when przm is applied to low precipitation areas rates of pesticide applications and chemical decay rates in water and soil are drivers in determining model outputs while these parameters become rain intensity and chemical decay rates on foliage at high precipitation areas these analyses present a more nuanced and robust assessment of the relative importance of model parameters over the course of the simulations in dynamic spatial and temporal simulations failing to capture variability provided by conditionally important input parameters during key time periods and or locations can lead to suboptimal modeling in terms of overall goodness of fit to observations this can happen early in the calibration process when sensitivity analyses are used to reduce the number of uncertain parameters despite the fact that final parameter ranges have not yet been identified in addition when unconditional sensitivity analyses are used to jettison variables that are important under certain conditions the overall dynamic features of the models may change therefore examination of conditional sensitivity analyses is less likely to lead to incorrect or misleading behavior of the models that are used for environmental applications and decision making one straightforward approach to leveraging conditional sensitivity analyses would be to use a higher percentile in addition use of the sobol method can provide insight into significant parameter interactions most sensitivity analyses conducted are based on the assumption that input variables are independent including the analysis conducted here however significant total order sensitivity index values sum of the interaction terms plus first order sensitivity index from the sobol analyses conducted with independence assumptions can be used as one indication regarding whether parameter covariance is important capturing interactions between parameters can have significant effects on system dynamics that influence the estimation of model outputs and subsequent goodness of fit these interactions can be captured in a monte carlo simulation e g adding parameters for an input variance covariance matrix during a calibration uncertainty analysis process although covariance in the monte carlo sampling can cause subsequent problems for both local and global sensitivity computation approaches including sobol these difficulties are not insurmountable and the sobol method is able to identify the significance of each possible interaction and can guide covariance decisions if such variance covariance matrix is not available one can also partition inputs into distinct sets the sobol method is still valid as long as the groups are independent inputs within each group can be correlated glen and isaacs 2012 in this application the 30 higher order interaction index sum indicates that estimation and calibration of such a matrix may capture some of the interaction variance with corresponding improvements in model fit candidate parameters in przm that may merit covariance parameters include pesticide decay rates for foliage and in water and soil pldkrt and decay control management factors uslec1 and uslec2 and soil properties bd and kdoc incorporation of additional covariance terms may address calibration difficulties in deeper depths where przm has traditionally labored for many applications software availability source code for the przm 3 model used for this manuscript is available from https www epa gov exposure assessment models przm version index r code input data results and binaries used for the analyses are available from a github repository located at https github com puruckertom hongpurucker2016 acknowledgments the authors appreciate valuable discussion and inputs from bertrand iooss a r package sensitivity author thanks to mike cyterski for peer review and fran rauschenberg for manuscript review and edits this research was supported in part by an appointment to the postdoctoral research program at the usepa ecosystems research division athens ga administered by the oak ridge institute for science and education through interagency agreement no dw8992298301 between the u s department of energy and the u s environmental protection agency the views expressed in this article are those of the authors and do not necessarily represent the views or policies of the u s environmental protection agency appendix a supplementary data the following are the supplementary data related to this article data profile data profile supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 018 
26382,risk assessment and decision making in ecology hydrology and biology often employ dynamic models with multiple calibrations the global sensitivity analysis of models is usually completed at each time step of a single output however due to the enormous volume of data and model complexity a single index cannot give a full scale analysis of such models the purposes of this paper are 1 to apply t pooling for analysing multiple outputs at a lower computational cost 2 to consider the influence of the correlations among the outputs and the output dimensions on sensitivity analysis and 3 to propose a procedure that combines the sobol index for a single output and the generalised sensitivity method and t pooling index for multiple outputs to analyse dynamic models comprehensively the proposed procedure and index are applied to a hydrologiska byråns vattenbalansavdelning hbv model with three calibrations to provide an uncertainty analysis across time periods ranging from a single time step to the entire time period keywords sensitivity analysis multivariate outputs dynamic model t pooling hydrologiska byråns vattenbalansavdelning hbv model probability integral transformation pit 1 introduction computer simulation models are essential components in the research design development and decision making for science and engineering applications with continuous advances in the physical understanding of the processes to be modelled and also computing capabilities such models continue to evolve with increasing complexity and more user defined factors e g increased model parameters and boundary conditions to obtain a better understanding of the role and importance of different model factors on the output model responses the procedure known as sensitivity analysis sa can be very helpful for developing evaluating and improving complex modelling studies ratto et al 2012 fonseca et al 2014 tang et al 2015 2018 pianosi et al 2016 xia and tang 2017 traditional methods for sa including the variance based method homma and saltelli 1996 sobol 2001 elementary effect method campolongo et al 2007 2011 derivative based method sobol and kucherenko 2009 sobol and kucherenko 2010 and safety systems based method tang et al 2016 2017 were designed for a scalar output these methods can be applied to dynamic models which give information on how the global sensitivity changes over time they are effective in identifying which inputs affect the uncertainty of a given output at a given time step in the model however as indicated by lamboni et al 2011 these methods contain considerable redundancy when identifying strong correlations among multiple outputs from different time steps in a given model a simple and useful approach the output decomposition method was proposed by campbell et al 2006 for sa of models with multiple outputs lamboni et al 2011 applied it to mathematical models of crop growth with dynamic outputs the output decomposition method consists of first performing an orthogonal decomposition of the multivariate outputs and then applying individual sa to only the most informative components this method devotes more attention to a few components rather than the entire dynamic process a new set of sensitivity indices which is based on the decomposition of the covariance of the model outputs gamboa et al 2013 is both equivalent to the sobol indices in the scalar case and also more computationally efficient since it does not require spectral decomposition in contrast to the output decomposition method lamboni et al 2011 details on the comparison and equivalence between the output and covariance decomposition approaches can be found in garcia cabrejo and valocchi 2014 marrel et al 2009 have mapped the sobol indices over the grid associated to the model outputs and rosolem et al 2012 have presented the rank based multiple criteria implementation of the sobol variance based sa approach despite their advantages these multivariate sa methods are based on the variances of the multivariate outputs these methods can be used for analysing a single response model or a model with multiple uncorrelated outputs but they are no longer appropriate for analysing multiple correlated responses developing a novel methodology to address this latter case is thus the motivation of the research presented here there are three key situations where multiple correlated outputs are of interest 1 a computational model that generates multiple measurements calibrations or outputs that share similar underlying physics hills 2006 van werkhoven et al 2009 kollat et al 2012 2 the collection of model responses from the same experiment that is a function of spatial and or temporal variables oberkampf and barone 2006 dowding et al 2008 young and ratto 2011 and 3 the dynamic model combines disparate responses or calibrations at multiple time steps pianosi and soncini sessa 2009 pianosi and raso 2012 in each of these cases there is a strong correlation between any pair of outputs from the same experiment therefore a sensitivity index that refers to the entire distribution of the multivariate outputs should be used if one wants to assess which input influences the decision maker state of knowledge cui et al 2010 extended the moment independent sa method for scalar outputs borgonovo 2007 to the multivariate case and defined a sensitivity index based on the joint probability density function pdf of the multivariate outputs this method takes both the entire uncertainty and correlation of the multivariate output into account however it suffers severely from the curse of dimensionality due to the computational costs required for the high dimensional integration in the sensitivity index as well as the difficulty in estimating the joint pdf of the high dimensional variables the advantages of using the distance between cumulative distribution functions cdfs as a measure of the input importance in the case of a scalar output have been manifested in many papers baucells and borgonovo 2013 borgonovo et al 2013 2015 borgonovo et al 2013 pianosi and wagener 2015 but it has not been extended to the case of multivariate outputs as pointed output in liu and homma 2009 the cdf based method is easier to implement than the pdf based method and the computational efficiency of the cdf based method can be improved as compared with the pdf based method the multivariate probability integral transformation pit is recognised for containing valuable information about the correlation structure of the joint cdf of the outputs with numerous applications in various fields including a few examples mentioned here a paper by imlahi and chakak 2007 examined the application of pit to obtain the maximum likelihood estimation of dependence parameters genest et al 2006 applied pit to test the goodness of fit of copula function while ishida 2005 evaluated the application of pit in estimating the conditional density forecast in the econometric mainstream the pit has also been employed to represent correlated random variables from multivariate outputs by luyi et al 2016 moreover the present methods for multivariate outputs are more suitable to the dynamic models with a single calibration rather than the dynamic models with multiple calibrations it is often difficult to interpret and simultaneously aggregate various data from different time steps and different calibrations because they are influenced by the dimensions of the model outputs a sensitivity index that refers to the entire distribution of the multivariate outputs in a dynamic process should thus be used to assess which input influences the decision maker state of knowledge the u pooling metric is used to compare the marginal distributions of simulations and the physical measurements for model validation ferson et al 2008 physical observations collected at multiple validation sites can be incorporated into a single metric to assess the global predictive capability of a model by applying the u pooling metric you and mahadevan 2013 li et al 2014 extended the u pooling metric and proposed the t pooling metric for observations at validation settings of interest the main advantage of the t pooling metric is the ability to integrate the evidence from all the relevant data of multi response quantities over an intended validation domain into a single measure to estimate the overall disagreement however this has not been extended to sa for the case of multiple outputs over the time domain li et al 2014 indicated that the cdfs of the model outputs and time points can be transformed twice first as a multivariate pit and then as a univariate pit to yield a comprehensive and comparable distribution furthermore each of these sa methods has their own merits and drawbacks such that it is so inappropriate to analyse the dynamic model with multiple calibrations and a single method we cannot consider some features such as correlation dimension or interaction effect at the same time in the dynamic process therefore a sensitivity procedure that contains different methods and corresponds to the different kinds of requirements should be performed to analyse such models over a range of time windows based on this literature review we believe that there is a lack of guidance to support global sensitivity analysis gsa users attempting to address a problem using a dynamic model with correlated outputs while there is an opportunity for supplementing current approaches with reduced computational costs thus the objectives of the present study are 1 a new importance measure is defined that is based on the t pooling metric which allows the pooling of information from multiple outputs at different time steps the importance of the input over the entire time domain and outputs can be measured by determining the area difference between the joint unconditional cdfs and the joint conditional cdfs of the twice transformed pit distributions 2 we implement our proposed method to address the challenges of considering uncertainty correlations and dimensions by the twice transformed pit distributions due to the univariate nature of the multivariate pit the proposed methods are evaluated through univariate integrations regardless of the number of simulations which reduces the computational cost compared to other methods 3 a new sensitivity procedure is introduced for a time dependent model with multivariate outputs which analyses the global sensitivity from the microcosm a single time step and a single output to the macrocosm the entire time domain and multiple outputs the remainder of this paper is presented as follows a brief introduction of the hydrologiska byråns vattenbalansavdelning hbv model model data and the multi objective calibrations is provided in section 2 in section 3 a brief introduction of the covariance decomposition approach the probability integral transformation theorem and sa based on pit are provided and the new sensitivity index based on t pooling is defined the proposed procedure for a time dependent model with multiple outputs is presented at the end of section 3 three numerical examples are used to test the effectiveness efficiency and robustness of the new method in section 4 the experiment results of the microscopic partial and macroscopic analyses for hbv are shown and discussed in section 5 the conclusions of our study are summarised in section 6 2 model the hbv model bergström 1976 is a widely used lumped rainfall runoff model it has been applied in a variety of studies for example to study the effect of water resources management scenarios akhtar et al 2008 to calculate the hydrological forecasts demirel et al 2013 and to predict the potential climate changes or floods bergström 1992 2 1 model description the hbv model is a conceptual model for rainfall runoff simulations where precipitation temperature and potential evaporation are the inputs the model also contains a soil moisture accounting model and a runoff response model kollat et al 2012 a schematic diagram of the hbv model is shown in fig 1 and the model parameters are listed in table 1 a storage capacity distribution function is used for the storage elements of the catchment in the soil moisture accounting model the capacity of the largest soil moisture store is defined by the maximum soil moisture content fc the shape parameter beta represents the degree of spatial variability of the stores and lp represents the soil moisture limit at which potential evaporation occurs for soil moisture storage between zero and lp the actual evaporation is a linear fraction of the potential evaporation for soil moisture storage at or above lp the actual evaporation is equal to the potential evaporation the excess rainfall that remains after evaporation and the filling of the soil moisture stores is routed into an upper response reservoir in the runoff response model the runoff of the upper response reservoir can be divided into near surface flow interflow and percolation to base flow these three kinds of flow are defined by the near surface flow recession coefficient k0 the interflow recession coefficient k1 and the percolation rate perc when the height of the runoff in the upper response reservoir is at or above a threshold uzl near surface flow occurs the runoff percolating to the lower response reservoir is related to the base flow recession coefficient k2 finally the runoff from the upper and lower response reservoirs is the transformed by a triangular distribution with a base length maxbas the nine above mentioned parameters should be calibrated for the hbv model these parameters and their corresponding ranges are shown in table 1 the ranges of the parameters are based on prior studies kollat et al 2012 2 2 model data the nezicot river in turner center maine usa which has been investigated extensively is selected for this study two consecutive years january 1 1948 to january 1 1950 of data daily precipitation stream flow and potential evapotranspiration estimates are available for this catchment that represent a wide range of hydrological conditions the precipitation data are processed in the national weather service hydrology laboratory the potential evaporation is based on the national oceanic and atmospheric association evaporation atlas the stream flow is obtained from the u s geological survey national water information system available at http water usgs gov nwis see duan et al 2006 for more details the computational experiments were performed with the safe toolbox pianosi and wagener 2015 available at http www bris ac uk cabot resources safe toolbox safe implements many gsa methods and applications of hydrologic models 2 3 multi objective calibration outputs the multi objective formulation employed here which was first proposed by van werkhoven et al 2009 focuses on peak flows low flows water balance and flashiness seibert 1997 suggested that a combination of different functions was suitable to judge different parameter sets that may perform in more or less the same fashion according to only one function the first objective emphasises the peak flow errors using the nash sutcliffe efficiency nse gupta et al 2009 as shown in eq 1 1 n s e 1 t 1 n q s t q o t 2 t 1 n q o t q o 2 where q s t is the simulated runoff at time t q o t is the observed runoff at time t and q o is the mean of the observed flow over the calibration period n is the maximum time step in the calibration period such that the summation extends the entire calibration period from t 1 to t n nse is most often used as a hydrologic model calibration objective with a value ranging from 1 to the second objective emphasises the low flow errors by minimising the root mean square error rmse abebe et al 2010 shown in eq 2 2 r m s e 1 n t 1 n q s t q o t 2 the third output addresses the water balance by minimising the average annual runoff coefficient percent error roce as shown in eq 3 3 roce 1 y y 1 y q s q o 1 100 where q s is the mean of the annual simulated runoff and q o is the mean of the annual observed runoff the summation occurs over y years of the calibration period from which an average annual value is then calculated kollat et al 2012 3 methodology we first need to review the methods of sa for multivariate outputs such as the covariance decomposition approach gamboa et al 2013 and the pit index luyi et al 2016 we then extend pit based sensitivity index and propose a new measure for sa of models with multivariate outputs in the dynamic process finally we introduce the sa procedure for time dependent models with multivariate outputs which combines sobol indices the covariance decomposition method and the t pooling index the first method is used to analyse the uncertainty of a single output in the static process whereas the covariance decomposition for estimating the interaction effects and the t pooling method for accessing both the dimensions and correlations among responses can be applied to dynamic processes 3 1 covariance decomposition approach let y y 1 y m denote the m dimensional model output vector where y r g r x r 1 m and x x 1 x 2 x n t is the vector of n dimensional independent input variables in the case of a scalar output y 1 the sobol decomposition of the function y 1 g 1 x 1 x 2 x n is given by 4 y 1 g 0 1 i 1 n g i 1 i i 1 n i 2 1 i 1 n g i 1 i 2 1 x i 1 x i 2 g 1 2 n 1 x 1 x n where g 0 1 e y 1 g i 1 e y 1 x i g 0 1 and g i 1 i 2 1 e y 1 x i 1 x i 2 g i 1 1 g i 2 1 g 0 1 e is the mean value in the case of multivariate outputs garcia cabrejo and valocchi 2014 took the covariance matrices for both sides of y to obtain 5 c y 1 y m i 1 n c i y 1 y m 1 i j n c i j y 1 y m 1 i j k n c i j k y 1 y m c 1 2 n y 1 y m this expression implies that the covariance matrix c of the multivariate outputs can be partitioned into the sum of covariance matrices that comes from changes in single c i pairs c i j triples c i j k and so forth of the input variables when m 1 eq 5 degenerates to the decomposition of the variance for the scalar outputs by taking the trace about both sides of eq 5 eq 6 can then be obtained 6 t r c y 1 y m i 1 n t r c i y 1 y m 1 i j n t r c i j y 1 y m 1 i j k n t r c i j k y 1 y m t r c 1 2 n y 1 y m according to eq 6 the multivariate single effect index s 1 i m of the input variable x i is given by 7 s 1 i m y 1 y m t r c i t r c and the multivariate total effect index s t i m can be defined as 8 s t i m y 1 y m t r c i j 1 i j t r c i j j k 1 j k i t r c i j k t r c 1 2 n t r c the trace t r c is the sum of the variances of all outputs y r r 1 m the s 1 i m or s t i m can thus be interpreted as the sum of the variance contributions associated with the input variables x i and x i of all the outputs y garcia cabrejo and valocchi 2014 pointed out that the output decomposition method and the covariance decomposition method are equivalent if the first k eigenvectors in the principle component decomposition preserve the original variance of the outputs the output and covariance decomposition methods mainly focus on the sum of the variances of the multivariate outputs however the comprehensive effect of the input variable on the multiple outputs may not be equal to the sum of each input contribution to the scalar output if the correlation is in the output the traditional sensitivity measure for the multivariate outputs is difficult to interpret as the indices do not include the non opposite angle elements of the covariance matrix of outputs we are unable to determine how the correlation influences the result furthermore these methods ignore the influence of the output variable dimension if some outputs have a higher order of magnitude than others they will be incorrectly identified as making larger contributions among all the outputs xu et al 2016 3 2 the sensitivity index based on the probability integral transformation 3 2 1 probability integral transformation the pit for the univariate case is well established in the literature given a random variable y with a continuous cdf f y y the pit of y is a standard uniform random variable v transformed by the relation v f y y i e v u 0 1 the general proof of this theorem can be found in casella and berger 2002 nevertheless the pit for the multivariate case is far less understood genest and rivest 2001 let f y y 1 y m be the joint cdf of the multivariate output y y 1 y m the m dimensional pit of y can then be similarly obtained by v f y y 1 y m the cdf of v denoted by k v v is referred to as the pit distribution of y this differs from the pit distribution in the univariate case in that k v v is not a standard uniform distribution in multivariate case because it relies on the correlation structure underlying the joint cdf of y specifically in the multivariate case k v v is distributed between 0 1 and written as k v v p v f y y 1 y m genest and rivest 2001 where p is the probability of the event when the correlation coefficient between each pairs of random variables is either 1 or 1 the pit distribution for this random vector will follow a standard uniform distribution genest and rivest 2001 it has been recognised that k v v contains valuable information about the correlation structure underlying the joint cdf of y genest and rivest 2001 fig 2 illustrates the pit distribution of a bivariate cdf 3 2 2 sa based on pit index given a series of time steps t t 1 t 2 t k the experimental data y s 1 t j y s d t j j 1 k are obtained by measuring the physical experiment outputs g s 1 x t g s d x t at these time steps where s d is the number of outputs and x x 1 x 2 x n t if we assume that the joint cdf of the multivariate output vector y t i y s 1 t i y s d t i at t i time is f y t i y s 1 t i y s d t i then the corresponding pit distribution is k v v p v f y t i y s 1 t i y s d t i if we further assume that one of the inputs x i is fixed at one of its realisations f y x i m is then the conditional joint cdf of y t i with the corresponding pit distribution k v x i v p v f y t i x i y s 1 t i y s d t i the effect of the input variable x i on the multivariate outputs can be measured as the difference between k v v and k v x i v the difference can then be obtained by 9 s x i 0 1 k v v k v x i v d v since x i is a random variable with pdf f x i x i the expectation of s x i can be used to describe the average effect of x i on the multivariate output as 10 e x i s x i f x i x i 0 1 k v u k v x i u d u d x i and the sensitivity index can then be defined as 11 γ i 1 2 e x i s x i 3 3 the new sensitivity index based on the t pooling metric the pit is a standard uniform distribution for any univariate cdf for multiple outputs however the pit distribution of the joint cdfs from multivariate outputs which contains the information of all outputs of both the marginal distributions and the correlations is different from the standard uniform distribution the t pooling metric the transformation based area metric li et al 2014 is proposed for integrating the evidence from all relevant data of the multiple output quantities over the time domain into a single measure to access the overall difference here the t pooling method is used to transfer data from various calibrations which have different dimensions and physical significances into the pit distributions that could be compared the flowchart of the importance measure based on the t pooling method is shown in fig 3 the cdfs of the unconditional and conditional outputs at different time steps are transformed twice first as a multivariate pit and then as a univariate pit to yield a comprehensive and comparable distribution step 1 on both sides of the flowchart the data from the unconditional output y t j are collected from the model at a dependent time step t i i 1 k if we assume that one of the inputs x i is fixed at one of its realisations then y x i t j is the conditional output of y t j step 2 the computer model is also simulated at these time steps t t 1 t 2 t k to construct the relevant joint unconditional cdfs of the model outputs and the joint conditional cdfs of the model outputs i e f y t j y s 1 t j y s d t j j 1 k and f y x i t j y i s 1 t j y i s d t j j 1 k step 3 for the first transformation the joint unconditional cdfs f y t 1 f y t j f y t k of multiple outputs and the joint conditional cdfs f y x i t 1 f y x i t j f y x i t k of multiple outputs from different time steps are transformed into the unconditional multivariate pit distributions k v v t 1 1 k v v t j j k v v t k k and the corresponding conditional multivariate pit distributions k v x i v x i t 1 1 k v x i v x i t j j k v x i v x i t k k simultaneously the outputs at each time step are transformed by the joint unconditional cdfs and the joint conditional cdfs of multiple outputs into a set of unconditional v values v t j f y t j y s 1 t j y s d t j and conditional v values v x i t j f y x i t j y s 1 t j y s d t j this step generally has a similar importance measure based on the pit method however the v values obtained from different time steps correspond to different pit distributions therefore the t pooling method needs a second transformation to integrate the evidence from all relevant time steps which is conducted for the v values and the pit distributions step 4 for the second transformation the joint unconditional cdf f k k v v t j 1 k v v t j j k v v t j k of the unconditional multivariate pit distributions and the joint conditional cdf f k x i k v x i v x i t 1 1 k v x i v x i t j j k v x i v x i t k k of the conditional multivariate pit distributions are transformed into the unconditional multivariate pit distribution k u u p u f k k v v t j 1 k v v t j j k v v t j k and the conditional corresponding multivariate pit distribution k u x i u p u f k x i k v x i v x i t 1 1 k v x i v x i t j j k v x i v x i t k k k u u and k u x i u have the same calculation way as k v v and k v x i v but using different notations to distinguish the second transformation from the first one the unconditional v values v t j f y t j m y s 1 t j y s d t j and the conditional v values v x i t j f y x i t j m y s 1 t j y s d t j are also transformed into a sequence of unconditional u values u f k m k y t 1 m v 1 k y t j m v t j k y t k m v t k and conditional u values u x i f k x i k v x i v x i t 1 1 k v x i v x i t j j k v x i v x i t k k by the corresponding pit distributions this process makes the comparison between the unconditional distribution of u values u and the conditional distribution of u values u x i step 5 finally the effect of the fixed value of x i on the multivariate output of the time dependent model can be measured as the difference between k u u and k u x i u which is given by eq 12 12 s x i 0 1 k u u k u x i u d u similar to the pit index the expectation of s x i can be obtained by 13 e x i s x i f x i x i 0 1 k u u k u x i u d u d x i and the new index is defined by 14 η i 1 2 e x i s x i for illustration purposes we assume that there are k time steps at each output if k 1 the proposed index based on t pooling can still be used degrading to the indices based on pit which highlights that the pit index is a special case of the new importance measure we thus use the t pooling method instead of pit in the discussions below the method proposed in this section similar to the t pooling metric inherits many of the good features of the area metric and the u pooling metric for a single output such as high efficiency and the capability of providing global accuracy measures of the model the correlation and uncertainty have all been considered by the transformed pit distribution and the transformed data sequence anghileri et al 2014 respectively and these advantages have also been incorporated into the t pooling index the new importance measure has the same mathematical properties as the pit based index since the pit distribution and the distribution of the second transformation are both distributed over 0 1 the range of the proposed index is normalised therefore the importance measure based on t pooling is more applicable regardless of the dimension of the outputs 3 4 procedure time dependent models with either multiple calibrations or a single calibration have temporal variables and the latter one is a special case of the former because the outputs from different calibrations have different physical meanings and dimensions they cannot be measured with existing sa for multivariate outputs furthermore for the time dependent hbv model with multiple outputs there are many features that should be considered during the entire time domain and outputs such as the uncertainties the correlations between outputs from different time steps and the dimensions of the different calibrations using a single index is too hard to account for all the important features of the time dependent outputs thus requiring a complete process that consists of different analyses for the different requirements the procedure proposed here is composed of three steps microscopic analysis partial analysis and macroscopic analysis which is illustrated in fig 4 in the first step the microscopic analysis employs the sobol index or other indices to obtain the gsa at each time step here we can observe the daily changing trends of the main and total effects of each input variable for different calibrations in the second step the covariance decomposition approach or the new index based on t pooling is applied for partial analysis the variance based method captures the interaction effects of the input factors on the defined time domain whereas the t pooling based method can consider both the entire uncertainty and the correlation of a given time period at reduced computational costs combining the results of these methods thus enables the user to obtain insights on the inputs that have the greatest impact on the output uncertainties in the defined time domain as well as on the parameters achieving the greatest variance reduction the defined time domain can be set to any required time period during the second step to address the needs of the user however this second step only considers a single calibration the t pooling index is then used to evaluate the effect of the inputs on the entire distribution for multiple calibrations in the given time domain and thus obtain a more comprehensive and macroscopic analysis of the system 4 numerical examples the new sensitivity index is applied to three numerical examples to analyse the influence of the input variables on multivariate outputs the results of the t pooling method will be compared with those of the covariance decomposition method to ensure the convergence of the computational results during the monte carlo simulation mcs we define a sample size of n 1000000 for each sensitivity index the convergence and robustness results for the three numerical examples are shown in appendix a fig 10 the sensitivity results are obtained for the main index s i and also the total index s t i the covariance decomposition method s 1 i m and also total index s t i m and the proposed index η i 4 1 example 4 1 consider a model that yields a low correlation between its independent inputs and multiple outputs as follows 15 y 1 100 x 1 x 2 x 1 x 3 y 2 x 1 x 2 x 2 2 x 3 y 3 x 1 x 2 x 3 3 x 1 n 0 1 x 2 n 0 1 x 3 n 0 1 the correlation coefficients between the outputs are shown in table 2 y 1 and y 2 are uncorrelated and the other relationships between the outputs have low correlations we thus magnify the second function y 2 by a factor of 100 to simulate the influence of the dimension on the above equations the values in parentheses for x 1 x 3 are the rankings of the three input variables for each sensitivity index we only provide the main effects of the original sensitivity indices in table 3 this example shows how the t pooling method conveys information about both the dimension and the correlation of the outputs from table 3 it follows that firstly due to the numerical error the order of importance of the input variables obtained according to s i for y 1 s 1 i m for y 1 y 2 y 3 and s 1 i m for y 1 y 2 y 3 is x 2 x 3 x 1 these indices are influenced by the high order of magnitude of the dimension of the first function y 1 therefore s 1 i m cannot measure the sensitivity of multiple outputs comprehensively under different output dimensions furthermore because y 1 and y 2 are uncorrelated η i for y 1 y 2 only shows that the t pooling method could remove the influence of the dimension of the outputs especially y 1 secondly we note that the importance values of the input variables obtained according to η i for y 2 y 3 are different from the values obtained by s 1 i m for y 2 y 3 because of the correlation between y 2 and y 3 the sensitivities of x 1 and x 2 obtained by the t pooling method are higher than those obtained by sobol index thirdly the η i result for y 1 y 2 y 3 which based on the entire distribution includes information on the dimensions and correlations between all the outputs at the same time this indicates that the importance measures based on the t pooling method are more comprehensive than the generalised sobol indices 4 2 example 4 2 consider the following highly nonlinear model with correlated outputs 16 y 1 0 03 1 905 x 1 x 2 2 x 3 x 7 0 565 x 1 x 2 2 x 4 x 8 y 2 x 5 x 3 1 185 x 1 x 2 y 3 x 6 x 4 0 75 x 1 x 2 the input variables are independent and follow a normal distribution their distribution parameters are shown in table 4 the correlation coefficients between the outputs in example 4 2 are also shown in table 4 the sensitivity results are listed in table 5 since this example has a number of interaction terms both the main and total effect indices which are based on the covariance decomposition and the t pooling method respectively are presented in table 5 it follows from table 6 that the sensitivity values and the rankings of the input variables obtained according to η i are not the same as the rankings obtained according to s 1 i m s t i m this is easy to explain by the fact that s 1 i m s t i m is variance based whereas η i is distribution based this indicates that in the case of multivariate outputs an input that influences the variance the most is not necessarily the input that influences the entire distribution of the outputs the most 4 3 example 4 3 consider the following time variant model with high correlation 17 g 1 x t 20 x 1 2 x 2 5 x 1 t 3 x 2 x 3 1 t 2 g 2 x t 10 4 x 1 t x 1 7 2 x 2 t 2 4 x 2 x 3 1 15 where 18 x 1 n 10 5 2 x 2 n 8 4 2 t 0 3 the input variables are independent and follow a normal distribution the correlation coefficients between functions in example 4 3 are shown in fig 4 the sensitivity results of multiple outputs at each time step are given in fig 5 the results of a single output and multiple outputs over the time period are listed in table 7 from fig 5a it is obvious that the relationships between the g 1 time steps have the high positive and negative correlations while these relationships those in g 2 only have high positive correlations in fig 5b the correlations between g 1 and g 2 change from being high positive to high negative a comparison of fig 5b and c s 1 i m g 2 and s 1 i m g 1 g 2 respectively highlights that they have the same rankings and values of sa which reflect the fact that the covariance decomposition method is influenced by dimension the generalised sobol indices also cannot be used for this case fig 5a and b and 6 a and d show that the sensitivity values change sharply over the correlations between outputs especially over time interval 1 2 5 indicating that the sensitivity is influenced by the correlation therefore the t pooling index η i provides an efficient alternative for the sa of this type of dynamic model while also considering its uncertainty dimension and correlation furthermore the computation of η i is much more tractable than that of s 1 i m s t i m when considering that the estimation of η i does not require the high dimensional integration and that the pit distribution of the multivariate outputs can be easily approximated based on the empirical joint cdf 5 results and discussion here we analyse the hbv from different aspects we defined two time domains the winter of 1948 11 07 1948 02 07 1949 and the entire two year time domain 01 01 1948 01 01 1950 for the three analyses the microscopic analysis consisted of applying the sobol index for three objective calibrations nse rmse and roce to each day in the model the results of the partial analysis were obtained by the covariance decomposition and t pooling index approaches to ensure good coverage of the input space the sample size of the generalised sensitivity index for both the microscopic and partial analyses was set to n 100000 sarrazin et al 2016 the new index was employed for the macroscopic analysis to estimate the effect of the input parameter uncertainties on all of the outputs during the two year time period we present the results of a convergence study to evaluate the importance measure based on the t pooling index in appendix a fig 10 5 1 the microscopic analysis results sobol indices on a single output the microscopic analysis results from the nse rmse and roce calibrations at each time step day are shown in figs 7 9 respectively the blue and red lines represent the main and total effects respectively the red lines often exceed the blue lines indicating that some values in the global sensitivity indices are too small to obtain the true accuracy of the calculations from these figures we can find that each input parameter influences the main and total effects in different days after careful observation of all the data we see that both beta and fc are important variables on nse rmse and roce with k1 and maxbas also possessing significant effects on nse and rmse however it is difficult to interpret their importance and the rankings of the importance measures during the full two year time period it is obvious in fig 7 that lp k0 k1 uzl and maxbas have comparatively large differences between the main and total effects whereas perc and k2 have only minor differences these differences change over time the main and total effects of beta lp fc and k2 rapidly decrease at about day 250 whereas perc k0 k1 uzl and maxbas rapidly increase at the same time and peak at about day 300 the total effects of lp and fc then rebound and reach peak values within days of the rapid decreases furthermore the interaction effects of lp fc k0 k1 uzl and maxbas play a more important role after about day 250 on nse in fig 8 all model parameters except lp have little difference between the main and total effects and these differences vary drastically with time the main and total effects of beta exhibit a cyclic trend that becomes quite evident after about day 250 the main and total effects of fc exhibit a reverse trend to beta with peaks in fc corresponding to the troughs in beta and vice versa a key observation among all the variables is that they exhibit huge fluctuations with time thus making it difficult to measure the exact values of the effects over the time domain on rmse compared to fig 7 the roce fluctuations and variation trends are the similar to the nse trends with the exception of beta lp and fc which exhibit a gentle waviness over time due to roce fig 9 however due to the small scale of the y axis we see that the main and total effects of perc k0 k1 k2 uzl and maxbas are considerably less important on roce 5 2 the partial analysis results the covariance decomposition approach and t pooling index for a single calibration over the time domain according to the microscopic analysis results we find that almost all the importance measures for beta and fc have a huge fluctuation between about day 250 and day 550 with the largest peak or valley occurring at about day 400 we thus assume that the defined time domain is the winter of 1948 which is a period that contains day 400 and has different environment conditions the partial analysis results of nse rmse and roce for the winter of 1948 and the two year time period are obtained by the covariance decomposition approach and the t pooling index as shown in table 8 except for three the least important parameters k0 perc and k2 the main effects s 1 i m the total effects s t i m the interaction effects s t i m s 1 i m and the t pooling index η i cost 9 2 105 9 2 105 9 2 105 and 9 1 103 simulations respectively compared to the sobol index on a single output at each time step it is straightforward to determine the importance of the main interaction and total effects exactly are for the winter of 1948 and the entire two year time period on each calibration table 8 gives the rankings and values of the sa variance based and t pooling based distributions for a single calibration over the time period fig 10 contains all the correlations between the different calibrations for every time step in the hbv model high positive correlations occur among the same calibrations whereas high negative correlations occur among the different calibrations table 8 shows that although both η i and s 1 i m η i or s t i m s 1 i m identify beta and fc as the most important parameters in most cases their rankings are not necessarily the same for rmse η i indicates that fc is more important than beta for the entire distribution of multiple outputs while s 1 i m and s t i m show that beta is the most important parameter for the variability of the multivariate output followed by fc the other variables also have different rankings among the different methods this suggests that in multivariate outputs case the importance ranking of the inputs for the variance is not necessarily the same as that for the entire distribution of outputs according to fig 10 there are high positive correlations among the same calibrations and the high negative correlations among different calibrations and the distribution takes into account both the entire uncertainty and correlation of the multivariate outputs the user needs to choose the key parameters that will best address the key questions in their research or decision making model moreover it is difficult to measure the exact values of the interaction effects over the entire time domain on rmse and roce in figs 8 and 9 however they are readily measured in the partial analysis for example the rankings of the interaction effects for rmse in the winter of 1948 are fc maxbas beta k1 uzl lp which means that the soil moisture content and the base length for transformation affect the low flow error interactively in winter furthermore η i takes less time and a smaller sample size than the variance based method such that the proposed measure η i provides an efficient alternative for the sa of the model with multivariate outputs while also considering both its uncertainty and correlation although we can obtain the rankings and values of the importance measures for any defined time period of any single calibration the correlation and the dimension among the calibrations has not been considered 5 3 the macroscopic analysis results the t pooling index for multiple calibrations over the time domain the results for the t pooling index and the covariance decomposition method of the hbv model with multiple calibrations including nse rmse and roce for the entire time domain are shown in table 9 to study the convergence the estimated new index as a function of time step is shown in appendix a fig 11 a comparison of the results in tables 8 and 9 highlights that the s 1 i m results for nse rmse roce and the s 1 i m results for nse are quite similar because the generalised sobol indices are primarily influenced by the dimension of nse the same holds true for s t i m and s t i m s 1 i m therefore the covariance decomposition method cannot be used for the case of multiple calibrations from table 9 the rankings obtained by the t pooling index are fc k1 beta maxbas uzl lp for the winter of 1948 time period and beta fc k1 maxbas lp uzl for the two year time period these factor rankings provide a convenient metric of how important each model parameter is which will then allow the model user to make more comprehensive decisions regarding the model outputs compared to the former analysis the correlation and dimension among the different types of outputs are addressed by two the pit transformations the new index spends nearly one thirtieth of computational time required by the variance based method requiring only 3 9 1 103 30000 model evaluations to produce the final model outputs thus highlighting its superior computational efficiency the t pooling index simultaneously synthesises the information that is spread between both the multiple calibrations and the temporal outputs 6 conclusions the t pooling index is proposed to evaluate the comprehensive effect of the inputs on the entire distribution of multiple outputs over the time domain the procedure is established in this paper is designed to discuss and analyse the complex dynamic model with multiple calibrations across the entire time domain covering a range of time intervals three numerical examples and an time dependent model with three objective calibrations are employed to illustrate the effectiveness of the proposed method compared to both the sobol index for a scalar output at each time step and the covariance decomposition approach for different calibrations on the entire time domain the new index can directly obtain the factor ranking of the effect of each input on the entire uncertainty of the time domain and the multiple outputs furthermore the computational time costs of the proposed index are much less than those of the variance based method which is primarily due to the univariate nature of the pit the results obtained by sobol index at each time step are used to observe the dynamic change regularity of gsa on each output forming the microscopic analysis of this study the partial analysis employed the covariance decomposition approach and the t pooling index to measure the uncertainty of the defined time steps on each calibration finally the t pooling index was used in the macroscopic analysis to provide a more holistic view of the system by exploring the correlations and dimensions across the entire time domain and all the outputs the new index which is a modified combination of the sobol index and the covariance decomposition approach would enable the user to obtain insights on the parameters that affect the uncertainty of a random time section and multiple outputs simultaneously as well as the variables that achieve the greatest variance reduction on a single output at each time step or any time domain the user would thus need to choose the set of indicators that characterises the model that will address their end goals for our model the new index obtains a convergent result through a series of tractable computations 10000 for 9 parameters while taking into account both the uncertainty and correlation of the time steps and the multivariate outputs the dimensionality is reduced by the procedure involving the two pit transformations after which some information are lost future work will be devoted to further implement and improve the proposed methodology software availability name of software safe v1 1 sensitivity analysis for everybody matlab octave toolbox developer francesca pianosi thorsten wagener and fanny sarrazin owned by the university of bristol programming language matlab contact francesca pianosi bristol ac uk thorsten wagener bristol ac uk and fanny sarrazin bristol ac uk program availability and cost the software can be obtained for free at the safe website http www bris ac uk cabot resources safe toolbox it implements several established global sensitive analysis methods and allows for easy integration with other methods acknowledgements this work was supported by the national natural science foundation of china grant no nsfc 51505382 and the natural science foundation of shaan xi province grant no 2016jq1034 appendix a fig 11 the convergence results of the t pooling index a example 4 1 b example 4 2 c example 4 3 and d the macroscopic analysis of the nine parameters in the hbv model over the two year time period simple size is the number of model the number of model evaluations fig 11 appendix b supplementary data the following is the supplementary data related to this article online data online data appendix b supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 026 
26382,risk assessment and decision making in ecology hydrology and biology often employ dynamic models with multiple calibrations the global sensitivity analysis of models is usually completed at each time step of a single output however due to the enormous volume of data and model complexity a single index cannot give a full scale analysis of such models the purposes of this paper are 1 to apply t pooling for analysing multiple outputs at a lower computational cost 2 to consider the influence of the correlations among the outputs and the output dimensions on sensitivity analysis and 3 to propose a procedure that combines the sobol index for a single output and the generalised sensitivity method and t pooling index for multiple outputs to analyse dynamic models comprehensively the proposed procedure and index are applied to a hydrologiska byråns vattenbalansavdelning hbv model with three calibrations to provide an uncertainty analysis across time periods ranging from a single time step to the entire time period keywords sensitivity analysis multivariate outputs dynamic model t pooling hydrologiska byråns vattenbalansavdelning hbv model probability integral transformation pit 1 introduction computer simulation models are essential components in the research design development and decision making for science and engineering applications with continuous advances in the physical understanding of the processes to be modelled and also computing capabilities such models continue to evolve with increasing complexity and more user defined factors e g increased model parameters and boundary conditions to obtain a better understanding of the role and importance of different model factors on the output model responses the procedure known as sensitivity analysis sa can be very helpful for developing evaluating and improving complex modelling studies ratto et al 2012 fonseca et al 2014 tang et al 2015 2018 pianosi et al 2016 xia and tang 2017 traditional methods for sa including the variance based method homma and saltelli 1996 sobol 2001 elementary effect method campolongo et al 2007 2011 derivative based method sobol and kucherenko 2009 sobol and kucherenko 2010 and safety systems based method tang et al 2016 2017 were designed for a scalar output these methods can be applied to dynamic models which give information on how the global sensitivity changes over time they are effective in identifying which inputs affect the uncertainty of a given output at a given time step in the model however as indicated by lamboni et al 2011 these methods contain considerable redundancy when identifying strong correlations among multiple outputs from different time steps in a given model a simple and useful approach the output decomposition method was proposed by campbell et al 2006 for sa of models with multiple outputs lamboni et al 2011 applied it to mathematical models of crop growth with dynamic outputs the output decomposition method consists of first performing an orthogonal decomposition of the multivariate outputs and then applying individual sa to only the most informative components this method devotes more attention to a few components rather than the entire dynamic process a new set of sensitivity indices which is based on the decomposition of the covariance of the model outputs gamboa et al 2013 is both equivalent to the sobol indices in the scalar case and also more computationally efficient since it does not require spectral decomposition in contrast to the output decomposition method lamboni et al 2011 details on the comparison and equivalence between the output and covariance decomposition approaches can be found in garcia cabrejo and valocchi 2014 marrel et al 2009 have mapped the sobol indices over the grid associated to the model outputs and rosolem et al 2012 have presented the rank based multiple criteria implementation of the sobol variance based sa approach despite their advantages these multivariate sa methods are based on the variances of the multivariate outputs these methods can be used for analysing a single response model or a model with multiple uncorrelated outputs but they are no longer appropriate for analysing multiple correlated responses developing a novel methodology to address this latter case is thus the motivation of the research presented here there are three key situations where multiple correlated outputs are of interest 1 a computational model that generates multiple measurements calibrations or outputs that share similar underlying physics hills 2006 van werkhoven et al 2009 kollat et al 2012 2 the collection of model responses from the same experiment that is a function of spatial and or temporal variables oberkampf and barone 2006 dowding et al 2008 young and ratto 2011 and 3 the dynamic model combines disparate responses or calibrations at multiple time steps pianosi and soncini sessa 2009 pianosi and raso 2012 in each of these cases there is a strong correlation between any pair of outputs from the same experiment therefore a sensitivity index that refers to the entire distribution of the multivariate outputs should be used if one wants to assess which input influences the decision maker state of knowledge cui et al 2010 extended the moment independent sa method for scalar outputs borgonovo 2007 to the multivariate case and defined a sensitivity index based on the joint probability density function pdf of the multivariate outputs this method takes both the entire uncertainty and correlation of the multivariate output into account however it suffers severely from the curse of dimensionality due to the computational costs required for the high dimensional integration in the sensitivity index as well as the difficulty in estimating the joint pdf of the high dimensional variables the advantages of using the distance between cumulative distribution functions cdfs as a measure of the input importance in the case of a scalar output have been manifested in many papers baucells and borgonovo 2013 borgonovo et al 2013 2015 borgonovo et al 2013 pianosi and wagener 2015 but it has not been extended to the case of multivariate outputs as pointed output in liu and homma 2009 the cdf based method is easier to implement than the pdf based method and the computational efficiency of the cdf based method can be improved as compared with the pdf based method the multivariate probability integral transformation pit is recognised for containing valuable information about the correlation structure of the joint cdf of the outputs with numerous applications in various fields including a few examples mentioned here a paper by imlahi and chakak 2007 examined the application of pit to obtain the maximum likelihood estimation of dependence parameters genest et al 2006 applied pit to test the goodness of fit of copula function while ishida 2005 evaluated the application of pit in estimating the conditional density forecast in the econometric mainstream the pit has also been employed to represent correlated random variables from multivariate outputs by luyi et al 2016 moreover the present methods for multivariate outputs are more suitable to the dynamic models with a single calibration rather than the dynamic models with multiple calibrations it is often difficult to interpret and simultaneously aggregate various data from different time steps and different calibrations because they are influenced by the dimensions of the model outputs a sensitivity index that refers to the entire distribution of the multivariate outputs in a dynamic process should thus be used to assess which input influences the decision maker state of knowledge the u pooling metric is used to compare the marginal distributions of simulations and the physical measurements for model validation ferson et al 2008 physical observations collected at multiple validation sites can be incorporated into a single metric to assess the global predictive capability of a model by applying the u pooling metric you and mahadevan 2013 li et al 2014 extended the u pooling metric and proposed the t pooling metric for observations at validation settings of interest the main advantage of the t pooling metric is the ability to integrate the evidence from all the relevant data of multi response quantities over an intended validation domain into a single measure to estimate the overall disagreement however this has not been extended to sa for the case of multiple outputs over the time domain li et al 2014 indicated that the cdfs of the model outputs and time points can be transformed twice first as a multivariate pit and then as a univariate pit to yield a comprehensive and comparable distribution furthermore each of these sa methods has their own merits and drawbacks such that it is so inappropriate to analyse the dynamic model with multiple calibrations and a single method we cannot consider some features such as correlation dimension or interaction effect at the same time in the dynamic process therefore a sensitivity procedure that contains different methods and corresponds to the different kinds of requirements should be performed to analyse such models over a range of time windows based on this literature review we believe that there is a lack of guidance to support global sensitivity analysis gsa users attempting to address a problem using a dynamic model with correlated outputs while there is an opportunity for supplementing current approaches with reduced computational costs thus the objectives of the present study are 1 a new importance measure is defined that is based on the t pooling metric which allows the pooling of information from multiple outputs at different time steps the importance of the input over the entire time domain and outputs can be measured by determining the area difference between the joint unconditional cdfs and the joint conditional cdfs of the twice transformed pit distributions 2 we implement our proposed method to address the challenges of considering uncertainty correlations and dimensions by the twice transformed pit distributions due to the univariate nature of the multivariate pit the proposed methods are evaluated through univariate integrations regardless of the number of simulations which reduces the computational cost compared to other methods 3 a new sensitivity procedure is introduced for a time dependent model with multivariate outputs which analyses the global sensitivity from the microcosm a single time step and a single output to the macrocosm the entire time domain and multiple outputs the remainder of this paper is presented as follows a brief introduction of the hydrologiska byråns vattenbalansavdelning hbv model model data and the multi objective calibrations is provided in section 2 in section 3 a brief introduction of the covariance decomposition approach the probability integral transformation theorem and sa based on pit are provided and the new sensitivity index based on t pooling is defined the proposed procedure for a time dependent model with multiple outputs is presented at the end of section 3 three numerical examples are used to test the effectiveness efficiency and robustness of the new method in section 4 the experiment results of the microscopic partial and macroscopic analyses for hbv are shown and discussed in section 5 the conclusions of our study are summarised in section 6 2 model the hbv model bergström 1976 is a widely used lumped rainfall runoff model it has been applied in a variety of studies for example to study the effect of water resources management scenarios akhtar et al 2008 to calculate the hydrological forecasts demirel et al 2013 and to predict the potential climate changes or floods bergström 1992 2 1 model description the hbv model is a conceptual model for rainfall runoff simulations where precipitation temperature and potential evaporation are the inputs the model also contains a soil moisture accounting model and a runoff response model kollat et al 2012 a schematic diagram of the hbv model is shown in fig 1 and the model parameters are listed in table 1 a storage capacity distribution function is used for the storage elements of the catchment in the soil moisture accounting model the capacity of the largest soil moisture store is defined by the maximum soil moisture content fc the shape parameter beta represents the degree of spatial variability of the stores and lp represents the soil moisture limit at which potential evaporation occurs for soil moisture storage between zero and lp the actual evaporation is a linear fraction of the potential evaporation for soil moisture storage at or above lp the actual evaporation is equal to the potential evaporation the excess rainfall that remains after evaporation and the filling of the soil moisture stores is routed into an upper response reservoir in the runoff response model the runoff of the upper response reservoir can be divided into near surface flow interflow and percolation to base flow these three kinds of flow are defined by the near surface flow recession coefficient k0 the interflow recession coefficient k1 and the percolation rate perc when the height of the runoff in the upper response reservoir is at or above a threshold uzl near surface flow occurs the runoff percolating to the lower response reservoir is related to the base flow recession coefficient k2 finally the runoff from the upper and lower response reservoirs is the transformed by a triangular distribution with a base length maxbas the nine above mentioned parameters should be calibrated for the hbv model these parameters and their corresponding ranges are shown in table 1 the ranges of the parameters are based on prior studies kollat et al 2012 2 2 model data the nezicot river in turner center maine usa which has been investigated extensively is selected for this study two consecutive years january 1 1948 to january 1 1950 of data daily precipitation stream flow and potential evapotranspiration estimates are available for this catchment that represent a wide range of hydrological conditions the precipitation data are processed in the national weather service hydrology laboratory the potential evaporation is based on the national oceanic and atmospheric association evaporation atlas the stream flow is obtained from the u s geological survey national water information system available at http water usgs gov nwis see duan et al 2006 for more details the computational experiments were performed with the safe toolbox pianosi and wagener 2015 available at http www bris ac uk cabot resources safe toolbox safe implements many gsa methods and applications of hydrologic models 2 3 multi objective calibration outputs the multi objective formulation employed here which was first proposed by van werkhoven et al 2009 focuses on peak flows low flows water balance and flashiness seibert 1997 suggested that a combination of different functions was suitable to judge different parameter sets that may perform in more or less the same fashion according to only one function the first objective emphasises the peak flow errors using the nash sutcliffe efficiency nse gupta et al 2009 as shown in eq 1 1 n s e 1 t 1 n q s t q o t 2 t 1 n q o t q o 2 where q s t is the simulated runoff at time t q o t is the observed runoff at time t and q o is the mean of the observed flow over the calibration period n is the maximum time step in the calibration period such that the summation extends the entire calibration period from t 1 to t n nse is most often used as a hydrologic model calibration objective with a value ranging from 1 to the second objective emphasises the low flow errors by minimising the root mean square error rmse abebe et al 2010 shown in eq 2 2 r m s e 1 n t 1 n q s t q o t 2 the third output addresses the water balance by minimising the average annual runoff coefficient percent error roce as shown in eq 3 3 roce 1 y y 1 y q s q o 1 100 where q s is the mean of the annual simulated runoff and q o is the mean of the annual observed runoff the summation occurs over y years of the calibration period from which an average annual value is then calculated kollat et al 2012 3 methodology we first need to review the methods of sa for multivariate outputs such as the covariance decomposition approach gamboa et al 2013 and the pit index luyi et al 2016 we then extend pit based sensitivity index and propose a new measure for sa of models with multivariate outputs in the dynamic process finally we introduce the sa procedure for time dependent models with multivariate outputs which combines sobol indices the covariance decomposition method and the t pooling index the first method is used to analyse the uncertainty of a single output in the static process whereas the covariance decomposition for estimating the interaction effects and the t pooling method for accessing both the dimensions and correlations among responses can be applied to dynamic processes 3 1 covariance decomposition approach let y y 1 y m denote the m dimensional model output vector where y r g r x r 1 m and x x 1 x 2 x n t is the vector of n dimensional independent input variables in the case of a scalar output y 1 the sobol decomposition of the function y 1 g 1 x 1 x 2 x n is given by 4 y 1 g 0 1 i 1 n g i 1 i i 1 n i 2 1 i 1 n g i 1 i 2 1 x i 1 x i 2 g 1 2 n 1 x 1 x n where g 0 1 e y 1 g i 1 e y 1 x i g 0 1 and g i 1 i 2 1 e y 1 x i 1 x i 2 g i 1 1 g i 2 1 g 0 1 e is the mean value in the case of multivariate outputs garcia cabrejo and valocchi 2014 took the covariance matrices for both sides of y to obtain 5 c y 1 y m i 1 n c i y 1 y m 1 i j n c i j y 1 y m 1 i j k n c i j k y 1 y m c 1 2 n y 1 y m this expression implies that the covariance matrix c of the multivariate outputs can be partitioned into the sum of covariance matrices that comes from changes in single c i pairs c i j triples c i j k and so forth of the input variables when m 1 eq 5 degenerates to the decomposition of the variance for the scalar outputs by taking the trace about both sides of eq 5 eq 6 can then be obtained 6 t r c y 1 y m i 1 n t r c i y 1 y m 1 i j n t r c i j y 1 y m 1 i j k n t r c i j k y 1 y m t r c 1 2 n y 1 y m according to eq 6 the multivariate single effect index s 1 i m of the input variable x i is given by 7 s 1 i m y 1 y m t r c i t r c and the multivariate total effect index s t i m can be defined as 8 s t i m y 1 y m t r c i j 1 i j t r c i j j k 1 j k i t r c i j k t r c 1 2 n t r c the trace t r c is the sum of the variances of all outputs y r r 1 m the s 1 i m or s t i m can thus be interpreted as the sum of the variance contributions associated with the input variables x i and x i of all the outputs y garcia cabrejo and valocchi 2014 pointed out that the output decomposition method and the covariance decomposition method are equivalent if the first k eigenvectors in the principle component decomposition preserve the original variance of the outputs the output and covariance decomposition methods mainly focus on the sum of the variances of the multivariate outputs however the comprehensive effect of the input variable on the multiple outputs may not be equal to the sum of each input contribution to the scalar output if the correlation is in the output the traditional sensitivity measure for the multivariate outputs is difficult to interpret as the indices do not include the non opposite angle elements of the covariance matrix of outputs we are unable to determine how the correlation influences the result furthermore these methods ignore the influence of the output variable dimension if some outputs have a higher order of magnitude than others they will be incorrectly identified as making larger contributions among all the outputs xu et al 2016 3 2 the sensitivity index based on the probability integral transformation 3 2 1 probability integral transformation the pit for the univariate case is well established in the literature given a random variable y with a continuous cdf f y y the pit of y is a standard uniform random variable v transformed by the relation v f y y i e v u 0 1 the general proof of this theorem can be found in casella and berger 2002 nevertheless the pit for the multivariate case is far less understood genest and rivest 2001 let f y y 1 y m be the joint cdf of the multivariate output y y 1 y m the m dimensional pit of y can then be similarly obtained by v f y y 1 y m the cdf of v denoted by k v v is referred to as the pit distribution of y this differs from the pit distribution in the univariate case in that k v v is not a standard uniform distribution in multivariate case because it relies on the correlation structure underlying the joint cdf of y specifically in the multivariate case k v v is distributed between 0 1 and written as k v v p v f y y 1 y m genest and rivest 2001 where p is the probability of the event when the correlation coefficient between each pairs of random variables is either 1 or 1 the pit distribution for this random vector will follow a standard uniform distribution genest and rivest 2001 it has been recognised that k v v contains valuable information about the correlation structure underlying the joint cdf of y genest and rivest 2001 fig 2 illustrates the pit distribution of a bivariate cdf 3 2 2 sa based on pit index given a series of time steps t t 1 t 2 t k the experimental data y s 1 t j y s d t j j 1 k are obtained by measuring the physical experiment outputs g s 1 x t g s d x t at these time steps where s d is the number of outputs and x x 1 x 2 x n t if we assume that the joint cdf of the multivariate output vector y t i y s 1 t i y s d t i at t i time is f y t i y s 1 t i y s d t i then the corresponding pit distribution is k v v p v f y t i y s 1 t i y s d t i if we further assume that one of the inputs x i is fixed at one of its realisations f y x i m is then the conditional joint cdf of y t i with the corresponding pit distribution k v x i v p v f y t i x i y s 1 t i y s d t i the effect of the input variable x i on the multivariate outputs can be measured as the difference between k v v and k v x i v the difference can then be obtained by 9 s x i 0 1 k v v k v x i v d v since x i is a random variable with pdf f x i x i the expectation of s x i can be used to describe the average effect of x i on the multivariate output as 10 e x i s x i f x i x i 0 1 k v u k v x i u d u d x i and the sensitivity index can then be defined as 11 γ i 1 2 e x i s x i 3 3 the new sensitivity index based on the t pooling metric the pit is a standard uniform distribution for any univariate cdf for multiple outputs however the pit distribution of the joint cdfs from multivariate outputs which contains the information of all outputs of both the marginal distributions and the correlations is different from the standard uniform distribution the t pooling metric the transformation based area metric li et al 2014 is proposed for integrating the evidence from all relevant data of the multiple output quantities over the time domain into a single measure to access the overall difference here the t pooling method is used to transfer data from various calibrations which have different dimensions and physical significances into the pit distributions that could be compared the flowchart of the importance measure based on the t pooling method is shown in fig 3 the cdfs of the unconditional and conditional outputs at different time steps are transformed twice first as a multivariate pit and then as a univariate pit to yield a comprehensive and comparable distribution step 1 on both sides of the flowchart the data from the unconditional output y t j are collected from the model at a dependent time step t i i 1 k if we assume that one of the inputs x i is fixed at one of its realisations then y x i t j is the conditional output of y t j step 2 the computer model is also simulated at these time steps t t 1 t 2 t k to construct the relevant joint unconditional cdfs of the model outputs and the joint conditional cdfs of the model outputs i e f y t j y s 1 t j y s d t j j 1 k and f y x i t j y i s 1 t j y i s d t j j 1 k step 3 for the first transformation the joint unconditional cdfs f y t 1 f y t j f y t k of multiple outputs and the joint conditional cdfs f y x i t 1 f y x i t j f y x i t k of multiple outputs from different time steps are transformed into the unconditional multivariate pit distributions k v v t 1 1 k v v t j j k v v t k k and the corresponding conditional multivariate pit distributions k v x i v x i t 1 1 k v x i v x i t j j k v x i v x i t k k simultaneously the outputs at each time step are transformed by the joint unconditional cdfs and the joint conditional cdfs of multiple outputs into a set of unconditional v values v t j f y t j y s 1 t j y s d t j and conditional v values v x i t j f y x i t j y s 1 t j y s d t j this step generally has a similar importance measure based on the pit method however the v values obtained from different time steps correspond to different pit distributions therefore the t pooling method needs a second transformation to integrate the evidence from all relevant time steps which is conducted for the v values and the pit distributions step 4 for the second transformation the joint unconditional cdf f k k v v t j 1 k v v t j j k v v t j k of the unconditional multivariate pit distributions and the joint conditional cdf f k x i k v x i v x i t 1 1 k v x i v x i t j j k v x i v x i t k k of the conditional multivariate pit distributions are transformed into the unconditional multivariate pit distribution k u u p u f k k v v t j 1 k v v t j j k v v t j k and the conditional corresponding multivariate pit distribution k u x i u p u f k x i k v x i v x i t 1 1 k v x i v x i t j j k v x i v x i t k k k u u and k u x i u have the same calculation way as k v v and k v x i v but using different notations to distinguish the second transformation from the first one the unconditional v values v t j f y t j m y s 1 t j y s d t j and the conditional v values v x i t j f y x i t j m y s 1 t j y s d t j are also transformed into a sequence of unconditional u values u f k m k y t 1 m v 1 k y t j m v t j k y t k m v t k and conditional u values u x i f k x i k v x i v x i t 1 1 k v x i v x i t j j k v x i v x i t k k by the corresponding pit distributions this process makes the comparison between the unconditional distribution of u values u and the conditional distribution of u values u x i step 5 finally the effect of the fixed value of x i on the multivariate output of the time dependent model can be measured as the difference between k u u and k u x i u which is given by eq 12 12 s x i 0 1 k u u k u x i u d u similar to the pit index the expectation of s x i can be obtained by 13 e x i s x i f x i x i 0 1 k u u k u x i u d u d x i and the new index is defined by 14 η i 1 2 e x i s x i for illustration purposes we assume that there are k time steps at each output if k 1 the proposed index based on t pooling can still be used degrading to the indices based on pit which highlights that the pit index is a special case of the new importance measure we thus use the t pooling method instead of pit in the discussions below the method proposed in this section similar to the t pooling metric inherits many of the good features of the area metric and the u pooling metric for a single output such as high efficiency and the capability of providing global accuracy measures of the model the correlation and uncertainty have all been considered by the transformed pit distribution and the transformed data sequence anghileri et al 2014 respectively and these advantages have also been incorporated into the t pooling index the new importance measure has the same mathematical properties as the pit based index since the pit distribution and the distribution of the second transformation are both distributed over 0 1 the range of the proposed index is normalised therefore the importance measure based on t pooling is more applicable regardless of the dimension of the outputs 3 4 procedure time dependent models with either multiple calibrations or a single calibration have temporal variables and the latter one is a special case of the former because the outputs from different calibrations have different physical meanings and dimensions they cannot be measured with existing sa for multivariate outputs furthermore for the time dependent hbv model with multiple outputs there are many features that should be considered during the entire time domain and outputs such as the uncertainties the correlations between outputs from different time steps and the dimensions of the different calibrations using a single index is too hard to account for all the important features of the time dependent outputs thus requiring a complete process that consists of different analyses for the different requirements the procedure proposed here is composed of three steps microscopic analysis partial analysis and macroscopic analysis which is illustrated in fig 4 in the first step the microscopic analysis employs the sobol index or other indices to obtain the gsa at each time step here we can observe the daily changing trends of the main and total effects of each input variable for different calibrations in the second step the covariance decomposition approach or the new index based on t pooling is applied for partial analysis the variance based method captures the interaction effects of the input factors on the defined time domain whereas the t pooling based method can consider both the entire uncertainty and the correlation of a given time period at reduced computational costs combining the results of these methods thus enables the user to obtain insights on the inputs that have the greatest impact on the output uncertainties in the defined time domain as well as on the parameters achieving the greatest variance reduction the defined time domain can be set to any required time period during the second step to address the needs of the user however this second step only considers a single calibration the t pooling index is then used to evaluate the effect of the inputs on the entire distribution for multiple calibrations in the given time domain and thus obtain a more comprehensive and macroscopic analysis of the system 4 numerical examples the new sensitivity index is applied to three numerical examples to analyse the influence of the input variables on multivariate outputs the results of the t pooling method will be compared with those of the covariance decomposition method to ensure the convergence of the computational results during the monte carlo simulation mcs we define a sample size of n 1000000 for each sensitivity index the convergence and robustness results for the three numerical examples are shown in appendix a fig 10 the sensitivity results are obtained for the main index s i and also the total index s t i the covariance decomposition method s 1 i m and also total index s t i m and the proposed index η i 4 1 example 4 1 consider a model that yields a low correlation between its independent inputs and multiple outputs as follows 15 y 1 100 x 1 x 2 x 1 x 3 y 2 x 1 x 2 x 2 2 x 3 y 3 x 1 x 2 x 3 3 x 1 n 0 1 x 2 n 0 1 x 3 n 0 1 the correlation coefficients between the outputs are shown in table 2 y 1 and y 2 are uncorrelated and the other relationships between the outputs have low correlations we thus magnify the second function y 2 by a factor of 100 to simulate the influence of the dimension on the above equations the values in parentheses for x 1 x 3 are the rankings of the three input variables for each sensitivity index we only provide the main effects of the original sensitivity indices in table 3 this example shows how the t pooling method conveys information about both the dimension and the correlation of the outputs from table 3 it follows that firstly due to the numerical error the order of importance of the input variables obtained according to s i for y 1 s 1 i m for y 1 y 2 y 3 and s 1 i m for y 1 y 2 y 3 is x 2 x 3 x 1 these indices are influenced by the high order of magnitude of the dimension of the first function y 1 therefore s 1 i m cannot measure the sensitivity of multiple outputs comprehensively under different output dimensions furthermore because y 1 and y 2 are uncorrelated η i for y 1 y 2 only shows that the t pooling method could remove the influence of the dimension of the outputs especially y 1 secondly we note that the importance values of the input variables obtained according to η i for y 2 y 3 are different from the values obtained by s 1 i m for y 2 y 3 because of the correlation between y 2 and y 3 the sensitivities of x 1 and x 2 obtained by the t pooling method are higher than those obtained by sobol index thirdly the η i result for y 1 y 2 y 3 which based on the entire distribution includes information on the dimensions and correlations between all the outputs at the same time this indicates that the importance measures based on the t pooling method are more comprehensive than the generalised sobol indices 4 2 example 4 2 consider the following highly nonlinear model with correlated outputs 16 y 1 0 03 1 905 x 1 x 2 2 x 3 x 7 0 565 x 1 x 2 2 x 4 x 8 y 2 x 5 x 3 1 185 x 1 x 2 y 3 x 6 x 4 0 75 x 1 x 2 the input variables are independent and follow a normal distribution their distribution parameters are shown in table 4 the correlation coefficients between the outputs in example 4 2 are also shown in table 4 the sensitivity results are listed in table 5 since this example has a number of interaction terms both the main and total effect indices which are based on the covariance decomposition and the t pooling method respectively are presented in table 5 it follows from table 6 that the sensitivity values and the rankings of the input variables obtained according to η i are not the same as the rankings obtained according to s 1 i m s t i m this is easy to explain by the fact that s 1 i m s t i m is variance based whereas η i is distribution based this indicates that in the case of multivariate outputs an input that influences the variance the most is not necessarily the input that influences the entire distribution of the outputs the most 4 3 example 4 3 consider the following time variant model with high correlation 17 g 1 x t 20 x 1 2 x 2 5 x 1 t 3 x 2 x 3 1 t 2 g 2 x t 10 4 x 1 t x 1 7 2 x 2 t 2 4 x 2 x 3 1 15 where 18 x 1 n 10 5 2 x 2 n 8 4 2 t 0 3 the input variables are independent and follow a normal distribution the correlation coefficients between functions in example 4 3 are shown in fig 4 the sensitivity results of multiple outputs at each time step are given in fig 5 the results of a single output and multiple outputs over the time period are listed in table 7 from fig 5a it is obvious that the relationships between the g 1 time steps have the high positive and negative correlations while these relationships those in g 2 only have high positive correlations in fig 5b the correlations between g 1 and g 2 change from being high positive to high negative a comparison of fig 5b and c s 1 i m g 2 and s 1 i m g 1 g 2 respectively highlights that they have the same rankings and values of sa which reflect the fact that the covariance decomposition method is influenced by dimension the generalised sobol indices also cannot be used for this case fig 5a and b and 6 a and d show that the sensitivity values change sharply over the correlations between outputs especially over time interval 1 2 5 indicating that the sensitivity is influenced by the correlation therefore the t pooling index η i provides an efficient alternative for the sa of this type of dynamic model while also considering its uncertainty dimension and correlation furthermore the computation of η i is much more tractable than that of s 1 i m s t i m when considering that the estimation of η i does not require the high dimensional integration and that the pit distribution of the multivariate outputs can be easily approximated based on the empirical joint cdf 5 results and discussion here we analyse the hbv from different aspects we defined two time domains the winter of 1948 11 07 1948 02 07 1949 and the entire two year time domain 01 01 1948 01 01 1950 for the three analyses the microscopic analysis consisted of applying the sobol index for three objective calibrations nse rmse and roce to each day in the model the results of the partial analysis were obtained by the covariance decomposition and t pooling index approaches to ensure good coverage of the input space the sample size of the generalised sensitivity index for both the microscopic and partial analyses was set to n 100000 sarrazin et al 2016 the new index was employed for the macroscopic analysis to estimate the effect of the input parameter uncertainties on all of the outputs during the two year time period we present the results of a convergence study to evaluate the importance measure based on the t pooling index in appendix a fig 10 5 1 the microscopic analysis results sobol indices on a single output the microscopic analysis results from the nse rmse and roce calibrations at each time step day are shown in figs 7 9 respectively the blue and red lines represent the main and total effects respectively the red lines often exceed the blue lines indicating that some values in the global sensitivity indices are too small to obtain the true accuracy of the calculations from these figures we can find that each input parameter influences the main and total effects in different days after careful observation of all the data we see that both beta and fc are important variables on nse rmse and roce with k1 and maxbas also possessing significant effects on nse and rmse however it is difficult to interpret their importance and the rankings of the importance measures during the full two year time period it is obvious in fig 7 that lp k0 k1 uzl and maxbas have comparatively large differences between the main and total effects whereas perc and k2 have only minor differences these differences change over time the main and total effects of beta lp fc and k2 rapidly decrease at about day 250 whereas perc k0 k1 uzl and maxbas rapidly increase at the same time and peak at about day 300 the total effects of lp and fc then rebound and reach peak values within days of the rapid decreases furthermore the interaction effects of lp fc k0 k1 uzl and maxbas play a more important role after about day 250 on nse in fig 8 all model parameters except lp have little difference between the main and total effects and these differences vary drastically with time the main and total effects of beta exhibit a cyclic trend that becomes quite evident after about day 250 the main and total effects of fc exhibit a reverse trend to beta with peaks in fc corresponding to the troughs in beta and vice versa a key observation among all the variables is that they exhibit huge fluctuations with time thus making it difficult to measure the exact values of the effects over the time domain on rmse compared to fig 7 the roce fluctuations and variation trends are the similar to the nse trends with the exception of beta lp and fc which exhibit a gentle waviness over time due to roce fig 9 however due to the small scale of the y axis we see that the main and total effects of perc k0 k1 k2 uzl and maxbas are considerably less important on roce 5 2 the partial analysis results the covariance decomposition approach and t pooling index for a single calibration over the time domain according to the microscopic analysis results we find that almost all the importance measures for beta and fc have a huge fluctuation between about day 250 and day 550 with the largest peak or valley occurring at about day 400 we thus assume that the defined time domain is the winter of 1948 which is a period that contains day 400 and has different environment conditions the partial analysis results of nse rmse and roce for the winter of 1948 and the two year time period are obtained by the covariance decomposition approach and the t pooling index as shown in table 8 except for three the least important parameters k0 perc and k2 the main effects s 1 i m the total effects s t i m the interaction effects s t i m s 1 i m and the t pooling index η i cost 9 2 105 9 2 105 9 2 105 and 9 1 103 simulations respectively compared to the sobol index on a single output at each time step it is straightforward to determine the importance of the main interaction and total effects exactly are for the winter of 1948 and the entire two year time period on each calibration table 8 gives the rankings and values of the sa variance based and t pooling based distributions for a single calibration over the time period fig 10 contains all the correlations between the different calibrations for every time step in the hbv model high positive correlations occur among the same calibrations whereas high negative correlations occur among the different calibrations table 8 shows that although both η i and s 1 i m η i or s t i m s 1 i m identify beta and fc as the most important parameters in most cases their rankings are not necessarily the same for rmse η i indicates that fc is more important than beta for the entire distribution of multiple outputs while s 1 i m and s t i m show that beta is the most important parameter for the variability of the multivariate output followed by fc the other variables also have different rankings among the different methods this suggests that in multivariate outputs case the importance ranking of the inputs for the variance is not necessarily the same as that for the entire distribution of outputs according to fig 10 there are high positive correlations among the same calibrations and the high negative correlations among different calibrations and the distribution takes into account both the entire uncertainty and correlation of the multivariate outputs the user needs to choose the key parameters that will best address the key questions in their research or decision making model moreover it is difficult to measure the exact values of the interaction effects over the entire time domain on rmse and roce in figs 8 and 9 however they are readily measured in the partial analysis for example the rankings of the interaction effects for rmse in the winter of 1948 are fc maxbas beta k1 uzl lp which means that the soil moisture content and the base length for transformation affect the low flow error interactively in winter furthermore η i takes less time and a smaller sample size than the variance based method such that the proposed measure η i provides an efficient alternative for the sa of the model with multivariate outputs while also considering both its uncertainty and correlation although we can obtain the rankings and values of the importance measures for any defined time period of any single calibration the correlation and the dimension among the calibrations has not been considered 5 3 the macroscopic analysis results the t pooling index for multiple calibrations over the time domain the results for the t pooling index and the covariance decomposition method of the hbv model with multiple calibrations including nse rmse and roce for the entire time domain are shown in table 9 to study the convergence the estimated new index as a function of time step is shown in appendix a fig 11 a comparison of the results in tables 8 and 9 highlights that the s 1 i m results for nse rmse roce and the s 1 i m results for nse are quite similar because the generalised sobol indices are primarily influenced by the dimension of nse the same holds true for s t i m and s t i m s 1 i m therefore the covariance decomposition method cannot be used for the case of multiple calibrations from table 9 the rankings obtained by the t pooling index are fc k1 beta maxbas uzl lp for the winter of 1948 time period and beta fc k1 maxbas lp uzl for the two year time period these factor rankings provide a convenient metric of how important each model parameter is which will then allow the model user to make more comprehensive decisions regarding the model outputs compared to the former analysis the correlation and dimension among the different types of outputs are addressed by two the pit transformations the new index spends nearly one thirtieth of computational time required by the variance based method requiring only 3 9 1 103 30000 model evaluations to produce the final model outputs thus highlighting its superior computational efficiency the t pooling index simultaneously synthesises the information that is spread between both the multiple calibrations and the temporal outputs 6 conclusions the t pooling index is proposed to evaluate the comprehensive effect of the inputs on the entire distribution of multiple outputs over the time domain the procedure is established in this paper is designed to discuss and analyse the complex dynamic model with multiple calibrations across the entire time domain covering a range of time intervals three numerical examples and an time dependent model with three objective calibrations are employed to illustrate the effectiveness of the proposed method compared to both the sobol index for a scalar output at each time step and the covariance decomposition approach for different calibrations on the entire time domain the new index can directly obtain the factor ranking of the effect of each input on the entire uncertainty of the time domain and the multiple outputs furthermore the computational time costs of the proposed index are much less than those of the variance based method which is primarily due to the univariate nature of the pit the results obtained by sobol index at each time step are used to observe the dynamic change regularity of gsa on each output forming the microscopic analysis of this study the partial analysis employed the covariance decomposition approach and the t pooling index to measure the uncertainty of the defined time steps on each calibration finally the t pooling index was used in the macroscopic analysis to provide a more holistic view of the system by exploring the correlations and dimensions across the entire time domain and all the outputs the new index which is a modified combination of the sobol index and the covariance decomposition approach would enable the user to obtain insights on the parameters that affect the uncertainty of a random time section and multiple outputs simultaneously as well as the variables that achieve the greatest variance reduction on a single output at each time step or any time domain the user would thus need to choose the set of indicators that characterises the model that will address their end goals for our model the new index obtains a convergent result through a series of tractable computations 10000 for 9 parameters while taking into account both the uncertainty and correlation of the time steps and the multivariate outputs the dimensionality is reduced by the procedure involving the two pit transformations after which some information are lost future work will be devoted to further implement and improve the proposed methodology software availability name of software safe v1 1 sensitivity analysis for everybody matlab octave toolbox developer francesca pianosi thorsten wagener and fanny sarrazin owned by the university of bristol programming language matlab contact francesca pianosi bristol ac uk thorsten wagener bristol ac uk and fanny sarrazin bristol ac uk program availability and cost the software can be obtained for free at the safe website http www bris ac uk cabot resources safe toolbox it implements several established global sensitive analysis methods and allows for easy integration with other methods acknowledgements this work was supported by the national natural science foundation of china grant no nsfc 51505382 and the natural science foundation of shaan xi province grant no 2016jq1034 appendix a fig 11 the convergence results of the t pooling index a example 4 1 b example 4 2 c example 4 3 and d the macroscopic analysis of the nine parameters in the hbv model over the two year time period simple size is the number of model the number of model evaluations fig 11 appendix b supplementary data the following is the supplementary data related to this article online data online data appendix b supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 026 
26383,the european directives for ambient air quality require to assess areas where air pollutant concentrations exceed a regulatory threshold as the spatial distribution of the pollutant is not exactly known deterministic atmospheric dispersion models are commonly used to supplement the observational network to reduce the computational time the simulations are made on irregular grids especially in urban areas where the grid is refined close to the roads an interpolation method is then necessary to map the dispersion model at any location we propose a new geostatistical approach based on kriging with external drift to distinguish the information along and across the roads an exponential function is introduced to describe the decrease of the concentrations across the roads its series expansion is used to build a set of polynomial auxiliary predictors with unknown coefficients this framework leads to a drift that is more generic and flexible in the kriging system keywords geostatistics kriging air pollution urban scale series expansion drift 1 introduction the european legislation directive 2008 50 ec 2008 on ambient air quality defines some environmental objectives expressed as annual daily hourly averaged concentrations or maximum numbers of days hours in exceedance depending on the pollutant and the time resolution when one or several thresholds are exceeded the member states have to delineate the spatial extent and the population exposed to these exceedances a lot of studies estimate the exposure to exceedances by crossing a map of concentrations with a static map of the population i e counted at their place of residence which is also the norm in the european regulation for this reason this work falls within this framework and is focused on the improvement of the concentration maps without worrying about the issue of the cross combination with the population however a growing umber of studies have recently estimated the same exposure but taking into account the dynamic aspect of the population that moves and sometimes works away from the residential areas these works are mostly built on activity based exposure models beckx et al 2009 hatzopoulou and miller 2010 jantunen et al 1999 who 2005 or simulations of the daily movement of the population using for instance mobile phone tracking liu et al 2013 gariazzo et al 2016 when occurring in urban areas and it is mostly the case for no2 and pm10 the exceedances are usually assessed by using urban scale dispersion models let us note ℳ this type of model to limit the computational costs a widespread practice is to calculate the concentrations in a two step procedure first the concentrations z x α are simulated on the irregular grid x α α 1 p with a coarse regular resolution in background areas and a higher adaptative resolution close to the roads see e g leelőssy et al 2014 next they are interpolated on a regular grid with high resolution over the whole domain of simulation d regarding the observational data t x α fixed monitoring sites passive sampling measurements available at locations x α α 1 d d p they are seldom introduced in a data assimilation framework tilloy et al 2013 to reduce the errors made by the dispersion model ℳ a kriging based combination of simulated data and passive sampling measurements can also be performed see e g aspa 2014 technical report in that case the frequency and the extent of the sampling campaigns play an important part in the quality of the results as a consequence the final mapping of z x over d mostly depends on three criteria the number p of x α in the simulation also denoted as the receptor points their locations and the precision of the interpolation technique in air quality the simulated concentrations are usually interpolated by linear methods implemented in post processing tools such as climate data operators cdo netcdf operators nco see zender 2016 in the urban configuration there is an additional difficulty because the concentrations come from both traffic related and background sources of pollution thus according to the spatial distribution of the receptor points the usual interpolation methods may lead to some artifacts in this paper an original adaptation of kriging based interpolation is proposed for the simulations made by an urban scale dispersion model first a review of the standard existing approaches is presented then an external drift modelling is introduced to assess the behaviour of the concentrations across the roads the concentration z x simulated by the model is seen as a stochastic process explicitly decomposed into a deterministic part m z x and a zero mean second order stationary random field w x the series expansion of the exponential function is used to linearize the expression of the deterministic part and thus stick to the usual kriging with external drift framework chiles and delfiner 2012 the three next sections are dedicated to an application of the methodology on the french city of orléans in 2010 source lig air the no2 annual mean simulated by the adms urban air quality model is considered in section 3 the dataset for both z x α and t x α are presented as well as the traffic emissions used to build the predictors of m x section 4 compares the results of a selection of standard interpolators and the exponential drift framework in terms of mapping and cross validation scores a detailed study concerning the advantages of the series expansion is also provided with additional guidelines regarding the truncation of the series and the minimal number of receptor points to use last a discussion is given to comment some key points of the methodology and in particular the consequences of the polynomial approximation on the coefficients of the drift estimated by kriging section 5 is focused on a kriging where the interpolation z k x of the simulation dataset is used as an external drift to improve the estimation t k x of the data collected from a passive sampling campaign finally section 6 is dedicated to software availability just before the conclusion of the paper 2 materials 2 1 overview of commonly used approaches the linear interpolation is widely used to provide air quality model outputs anywhere in space whatever the scale the model deals with it is considered as valid when the variations between the x α s are low or when the distances between the x α s are small a few meters in traffic related configuration but up to a few kilometers for low background concentrations more recently fortin et al 2012 developed a method based on the delaunay triangulation see also lixin et al 2011 for further applications of the delaunay triangulation to air quality interpolation assuming that the pollutant concentration along the triangle edges varies linearly polygons linking all the positions where the concentration is equal to a given value are defined last the ordinary kriging chiles and delfiner 2012 is a linear combination of the data with optimal weights satisfying unbiasedness constraint e z k x z x 0 and optimality for the variance of the error var z k x z x the statistical geostatistical and gis software make it possible to implement linear or delaunay interpolation and ordinary kriging with limited effort in addition an evaluation study beauchamp et al 2016 using 5 different french urban datasets bourges nantes niort orléans reims and tours has shown the relative efficiency of the linear and delaunay interpolation compared to ordinary kriging this can be explained by the non stationarity related to the multi source origin of the concentrations either influenced by the traffic or only reflecting the background pollution ordinary kriging is not able to account for it when dealing with few data jeannee and lemarchand 2012 handled this issue by considering locally varying anisotropies but other kriging options are available in particular universal kriging based approaches see section 2 2 in which the non stationarities are taken into account through the modelling of the deterministic part m x also called the drift of the random process other interpolators inverse distance nearest neighbour akima s interpolator will not be considered in this paper because they are too simplistic and only relevant for smooth concentrations fields more sophisticated estimation methods are not addressed in this paper they could be useful if hourly daily simulation outputs are considered instead of the annual averaged values among them the bayesian maximum entropy methodology bme that is a superset of classical geostatistics interpolators has already been used in air pollution and atmospheric studies christakos and li 1998 christakos et al 2004 yu et al 2016 2011 the growing popular estimation approach through the spde stochastic partial differential equation framework lindgren et al 2011 must also be mentioned an interesting application to space time pm10 pollution data is made in cameletti et al 2012 using the inla integrated nested laplace approximation computational approach a basic introduction to these methods is given in appendix a 2 2 traffic related external drift modelling in urban areas the pollution at a location x can be considered as the sum of a background component and a traffic related contribution due to the emissions of the roads in the close neighbourhood of x to simplify these two components will be supposed spatially independent see for instance font et al 2014 let denote z x the concentration simulated by the model ℳ at x not to be mistaken with the true value of the concentration denoted as t x and introduced later in section 2 5 y x is the random function related to the background feature and s x is the random function that deals with the pollution increment related to traffic emissions 1 z x y x s x the independence between y x and s x can be checked out by averaging the values s x α at locations x α s considered as traffic influenced per urban background concentration classes y 1 y p y x α is not known but may be obtained either by roughly removing the traffic related sources in the air quality simulation or by kriging the background data i e the receptor points located far enough from the road network in the literature more sophisticated models can also be used to estimate this quantity see e g pournazeri et al 2014 the appropriate definition of far enough depends on the corresponding environments so that the model in eq 1 is correct according to the literature see for example baldwin et al 2015 zou et al 2006 gilbert et al 2003 roorde knape et al 1999 and repeated results from regular campaigns in france the direct impact of road traffic can be considered insignificant 400 m away from the road thus it possibly holds as true for similar environments elsewhere as a consequence when x is located more than 400 m away from the road s x is neglected regarding the traffic related part of the process the chemical transformations are not taken into account as a simplification doing so the traffic related pollution s x is seen as the sum of the traffic related terms s k x k δ x belonging to each road r k located in the close vicinity δ x of the location x in the following the neighbourhood δ x k 1 n x x r k d max is defined according to a maximal distance d max 400 m around x in the following n x the number of roads in the neighbourhood of x is simply denoted as n 2 z x y x k δ x s k x the pollution term s k x brought by the road r k at location x is seen as a sum of a second order stationary and isotropic random field with zero mean w s k x and a deterministic part the drift m s k x e s k x this drift is seen as a continuous sum of the function g whose two parameters are s k r the contributions brought by each position r along r k and the distance x r between the position r at the centre of the road and the location x used to describe the decrease of these contributions s k r when moving away from the road 3 s k x m s k x w s k x r k g s k r x r d r w s k x fig 1 presents the variables and processes directly related to the concentration close to the traffic network fig 1a is the simplified configuration in which only the distance to the road is considered to build the drift m s k x in eq 3 fig 1b details the main other parameters that actually may impact the value of the concentration the screen effect of the buildings between r and x is left aside because it is difficult to describe from the available informations height surface volume and shape of the building however in the real world it might imply accumulation of pollution but also a change in the diffusion of the pollutant over the area the meteorology and the chemistry also play a part in the decrease across the road especially for hourly or daily averaged simulations see e g pineda rojas 2014 but it is not considered here as a consequence m s k x is a strong approximation of the reality whose errors go into the residual w s k x regarding the background part of z x if the domain d of simulation is small enough a few km2 so should be the variability of the background concentrations and it can thus be assumed as constant because this variability is negligible with respect to the range of the traffic related pollution s x if not y x can be seen as a random process 4 y x m y x w y x with w y x once again a zero mean second order stationary random field and the deterministic part m y x is a linear function of the background nox emissions denoted as f y x and used as a predictor the background emissions can be defined as the emissions mainly due to residential commercial agriculture and forestry sectors directly responsible for the background concentration i e the concentration that would be measured if local sources traffic industrial were not present it would also be possible to use a large scale ctm output as f y x to explain the actual background concentration or use these two sources of information as predictors for m y x doing so the full statistical model becomes 5 z x m y x m s x w x m z x w x where m s x is the sum of the n x local drifts m s k x and w x is the global residual i e the sum of w y x and all the w s k x whose spatial correlation will be studied and used to improve the estimation of z x 2 3 a polynomial series expansion for the decreasing function a statistical model is proposed to explain both s k r and the decreasing behaviour according to the function g s k r the no2 traffic related pollution at location r of road r k is seen as a linear function c φ r c ℝ of the traffic related nox emissions φ r at the same location because it is obviously not possible to quantify the values of the sources at any location r φ r is calculated as the ratio between φ k the annual emission of nox of the road r k as specified in a geographic information system gis in kg year 1 and r k its length according to previous studies taylor et al 2015 baldwin et al 2015 faus kessler et al 2008 cape et al 2004 pleijel et al 2004 it is relevant to consider that the traffic related pollution decreases exponentially with the distance to the road it makes sense for the nox concentration but also for the no2 despite the local chemistry which converts no to no2 because of the linear relationship existing between no2 and nox emissions even for daily averaged data thus the expected value for the random function s k x is 6 e s k x r k c φ k r k e κ x r x r d r κ x r should depend on location x and r but it is now considered as a constant parameter it is a key point in the method because the kriging based approach that is proposed further in eq 15 enables to estimate local values of the drift parameters thus relaxing the simplifications made in eq 6 the integral term along the road is in practice not known and has to be approximated by decomposing each road into p small segments φ r φ k r k φ k r l k r k where r l k is the l th section of the road r k and r is now located at the center of this section seguret et al 2003 proposed another modelling in a work intended to map roadside no2 concentrations around the main road of the thur valley in france the drift is seen as a combination of exponential and polynomial functions whose decrease across the road is adjusted according to observational data which is consistent with other pollution studies see e g monn et al 1997 in the orléans dataset presented in section 3 about 2000 roads are introduced the introduction of such a model would make harder both the estimation for the variogram of w x and the computation of the drift instead of fixing the value of κ in the exponential function the decreasing function is approximated using a series expansion as a polynomial approximation that is a sum of power functions with alternate signs in which the coefficients are unknown these coefficients will be locally estimated by solving the linear kriging system 15 with an appropriate moving neighbourhood i e by using only the points x α within a 100 m moving window around the location point x since the power functions x u are increasing functions when x 0 the only way to capture a decreasing shape is to alternate signs which is prone to lack of robustness hence the artifacts produced by this approximation as well as the number of power functions needed to reach a satisfactory performance score are discussed in section 4 3 from eq 6 the expected value of s x is 7 e s x k 1 n e s k x k 1 n r k c φ k r k e κ x r d r the parameters c and κ are not known the idea is to use the power series of the exponential function so that the decrease in e s x when moving away from the roads will be approximated by a polynomial combination of the distances x r e κ x r u 0 1 u κ u x r u u thus the deterministic part m s x of s x can be seen as a linear combination of predictors 8 e s x u 0 c κ u s u x with the predictors s u x defined as 9 s u x k 1 n r k φ k r k 1 u x r u u d r the kriging system for the estimation of z k x 0 α λ α z x α is built upon 1 the unbiasedness condition e z k x 0 z x 0 0 that leads to α λ α u 0 c κ u s u x α u 0 c κ u s u x 0 and then u 0 κ u α λ α s u x α s u x 0 0 it finally consists in finding the coefficients of the polynomial a 0 a 1 κ a n κ n since κ is unknown the unbiasedness constraint becomes 10 α λ α s u x α s u x 0 0 u this unbiasedness constraint does not require any prior information for the parameters c and κ κ is actually not known but neither explicitly linked between all the auxiliary predictors as it should be in the true expansion series of the exponential as a consequence the model for the drift becomes 11 e s x u 0 a u s u x where the a u are the coefficients to be estimated by the kriging a discussion about the advantages and the drawbacks of condition 10 is given in section 4 3 let us note that if the background part of z x is itself considered as an independent random process with a deterministic part potentially explained by an auxiliary predictor f y x see eq 4 at the end of section 2 2 then this predictor would be added to the set of auxiliary variables s u x in the kriging system with the additional unbiasedness condition 12 α λ α f y x α f y x 0 and the intercept term b of this background drift also adds another unbiasedness condition 13 λ α 1 this usual unbiasedness constraint in kriging makes valid the substitution of the covariance by the corresponding variogram in the kriging system in the case where y x b is a constant this condition also appears in the kriging system 2 the optimality of the variance of the error under conditions 10 12 and 13 the weights λ α are given by minimizing 14 λ α arg min λ α var z k x 0 z x 0 2 ν 0 α λ α 1 2 ν 1 α λ α f y x α f y x 0 2 u 0 μ u α λ α s u x α s u x 0 when m y x a f y x b and where the μ u s are the lagrangian parameters related to order u of the series ν 0 and ν 1 are the lagrangian parameters related to conditions 12 and 13 finally the kriging system can be written 15 α 1 n λ α γ x α x β ν 0 ν 1 f y x β u 0 u max μ u s u x β γ x β x 0 β α 1 n λ α 1 α 1 n λ α f y x α f y x 0 α 1 n λ α s u x α s u x 0 u 0 u max where γ is the variogram of the residuals w x s x e s x the size of the system is proportional to the order of truncation u max of the series the value of u max is pragmatically defined according to the convergence study of the performance scores provided in section 4 2 the algorithm to carry out the interpolation using the polynomial approximation of eq 8 as external drift is presented in appendix b to sum up even if the effect induced by all the roads mixes up all the distances to the roads in the neighbourhood of x are taken into account by s u x and the coefficients a u k x are estimated locally by kriging and thus supposed to describe the local behaviour of the decreasing function based on the spatial correlation of the stochastic residual w x although as in the usual external drift modelling the parameters a u do no longer appear in the kriging system however the variogram of the residuals w x is still supposed to be known because the drift is described as a linear combination of u max predictors in the kriging system the variogram is here directly estimated using an iterative fitting procedure of the drift where the residuals come from a k fold cross validation it can also be estimated by intrinsic random function of order k irf k indirect fitting chiles and delfiner 2012 or bayesian kriging techniques see e g omre 1987 breaking down the exponential function in its power series clearly makes appear a local polynomial drift usually involved in universal kriging in which the local coordinate system of the drift is based on the distance from the road seguret et al 2003 mention this analogy in their study of the thur valley in france as a consequence instead of introducing the exponential function processing the data through the irf k theory matheron 1971 is an other option it may fit most part of the phenomenon by using all possible linear combinations of polynomials of degree k what is done here by truncation of the exponential series but the spatial structure of the residual w x would be accounted for by a generalized covariance thus solving the bias that might be introduced when estimating the variogram through a preliminary cross estimation of the residuals last in section 3 the study case deals with annual data even if annual averaging in air pollutant concentrations can generally reduce concentration peaks and troughs that are present in daily time series the proposed methodology should also apply for day to day collected data the increased concentration variability compared to the annually aggregated values is not expected to produce any additional issues in the application thanks to the use of eq 11 that allows a local adaptation of the drift if the values of s u x remain the same the coefficients a u k x estimated by kriging and supposed to describe the decrease of the concentrations across the roads will not be the same if z x are hourly daily or annual data 2 4 a monotonic drift because the polynomial approximation of eq 11 has no clear physical meaning and thus may appear as a rather artificial way to produce a decreasing function another competitor method is considered it is a universal kriging with very simple regressors that takes into account the distance from the road network according to some clever metrics for instance in the kriging system 15 the uth predictor in location x could be 16 s u x k r k g u r x d r k r k φ k r k 1 x r u d r it ensures the drift to be strictly monotonic because g r x u g u r x 0 when x r which seems to be more consistent to describe the expected value of the decrease in concentration across the roads 2 5 coupling observations and model outputs deterministic urban models have an inherent modelling error due to the simplification of physical and chemical processes in addition the simulation outputs may also have some errors related to lacks and uncertainties of the input data for these reasons using observational data as another source of information is recommended to estimate the concentration for large scale mapping applications a usual procedure consists in combining a chemistry transport model ctm as an external drift with observations from a national or continental measurement network malherbe et al 2011 at smaller scale there are still few similar works it can be explained by the higher variability of concentrations and the difficulty to take all the sources of pollution into account to build a statistical estimator nevertheless the recent work done in the field of data assimilation tilloy et al 2013 with the blue estimator has to be mentioned it also introduces the notion of distance to the roads in the covariance matrix a multivariate approach by cokriging has also been experimented between passive sampling campaign and outputs of urban models see e g aspa s technical report 2014 in section 5 the large scale methodology is applied to urban datasets because the number of fixed monitoring stations in urban areas is often limited passive sampling diffusion measurements t x α carried out along the year make a convenient method for estimating no2 annual mean from a larger number of observational points x α even if it is not the standard use in geostatistics to repeat successive kriging algorithms a two step procedure is set up the step a consists in building an auxiliary predictor f t x z k x that is the interpolation of model ℳ outputs z x α from eq 15 based on the cross validation scores from section 4 2 the error made when estimating the explanatory variable f t x is thus strongly reduced in comparison to what it would be with a linear interpolation denoted as z l x the step b is a kriging t k x 0 α λ α t x α of the passive sampling data t x α in which the auxiliary variable f t x is estimated from step a and used as a universality constraint 17 α 1 n λ α γ x α x β μ 0 μ 1 f t x β γ x β x 0 β α 1 n λ α 1 α 1 n λ α f t x α f t x 0 where γ denotes here the variogram of the residuals t x m t x and the λ α are the weights of the observations t x α this makes sense because in kriging with external drift a first step is sometimes involved to interpolate the auxiliary variable f t x at the sample locations x α and thus be able to estimate the coefficients of the drift m t x a f t x b before computing the sample variogram last even if the diffusion tube uncertainty is known to be possibly large in high polluted areas it is not considered here because this information was not available if it was it would still possible to account for it by adding random white noise ε x α with no spatial correlation to the observations leading to add the corresponding variance term var ε x α on the diagonal of the kriging matrix γ x α x α 3 data 3 1 observations no2 concentration data were provided by lig air the french local agency in charge of air quality monitoring in the region centre val de loire they were collected during a passive sampling campaign carried out in 2010 in the urban area of orléans fig 2 shows the spatial distribution of the sampling data t x α the sampling sites x α were located at background locations 15 sites along the roads 29 sites and at small distances across the roads 6 sites to assess how the concentrations decrease when moving away from the traffic network the annual mean in each sampling site is estimated by averaging the data of the twelve monthly periods of measurements along the year 3 2 adms urban the dataset z x α from the air quality modelling software adms urban is also made available by lig air and covers the same period adms urban carruthers et al 1998 is an air quality dispersion model that includes the wide range of emission sources such as road traffic industrial and domestic emissions the solution of navier stokes is analytically solved using a three dimensional quasi gaussian formulation the input data required by the model are the meteorology the background concentrations and the emission inventories the output simulation mesh is usually qualified as smart it is a coarse regular grid for background configurations and an adaptative meshing across the roads with the main emission sources the required boundary parameters are computed by a meteorological pre processor from specified input data wind speed cloud cover for more details see tilloy et al 2013 and references therein for a good summary of the physical processes involved in the model this model is assumed to reproduce the distribution of the traffic related concentrations since it takes into account the topography of the road network more especially when buildings are higher than 0 5 m a street canyon module is activated to solve the concentration field it is based on a danish street pollution model hertel and berkowicz 1989 in which the no2 concentration is determined from the nox concentration as described in derwent and middleton 1996 the modelling grid includes 3136 receptor points x α distributed all over the domain see fig 3 the simulated background concentrations i e the concentrations simulated at the points located more than 400 m away from any roads are moderate ranging from 20 to 25 μ gm 3 when approaching the road traffic network the concentrations significantly increase and the regulatory limit value defined for the no2annual mean 40 μ gm 3 is often exceeded on average the model can be seen as a smoother of the observations shown in fig 2 i e it underestimates no2 in high polluted areas and overestimates it in the background 3 3 emissions the emissions inventory only includes traffic related sources fig 4 because the size of the domain is very small over orléans 2 5 km 1 5 km and the background concentration is thus considered as locally constant over the area as discussed in section 2 2 it is however possible to include industrial and domestic emissions in the methodology if m y x see eq 4 is introduced the urban topography is estimated from lig air database the road emissions are computed using copert iv the computer program to calculate emission from road transport coordinated by the european environment agency eea in the context of the activities of the european topic centre for air pollution and climate change mitigation it relies on a database of unitary emission factors the methodology adopted is to attribute for each pollutant and for each vehicle class a unitary emission factor that depends on the fuel mode the engine size the vehicle registration date and the vehicle speed as defined by road signs it also depends on the traffic conditions known from the previous observations of traffic counters in the agglomeration these conditions integrate monthly and daily profiles warm cold and slope induced emissions are taken into account for the heavy transport and some corrections are applied for old vehicles and for fuel improvements the road emissions in grams are then computed as the product of the number of vehicles per km and the unitary emission factor in g km vehicles 1 4 results in the application case because y x is assumed to be constant its value has to be roughly defined and is set to 24 μ g m 3 according to the simulation at background receptor points in order to estimate the experimental variogram γ ˆ x α x β though when the model ℳ is close to the truth the value y x b k x estimated by kriging can be slightly different see section 4 3 4 1 mapping as a first step the linear delaunay ordinary kriging and the newly kriging based with external drift interpolation of the adms urban model outputs are used to map z x onto the domain d via a 10 m 10 m regular grid ordinary kriging fig 5 c is computed with an isotropic variogram nugget effect 9 7 μ g 2 m 6 spherical model with range 813 m and sill 31 μ g 2 m 6 and a moving neighbourhood with the 10 nearest points the smoothing appears non realistic which is confirmed by cross validation scores section 4 2 the linear and delaunay interpolations fig 5a and b look quite similar the patterns along and across the roads are well described but in the areas with few or no data the configuration of the road network fig 4 is not known and the interpolation behaves like a background concentration even though some roads should logically induce higher concentrations the traffic related drift modelling fig 5d and e with an isotropic variogram of the residuals nugget effect 7 μ g 2 m 6 spherical model with range 700 m and sill 8 μ g 2 m 6 seems to solve the problems induced by the other methods the mapping is more realistic without any oversmoothing close to the roads the polynomial and monotonic drift lead to very similar estimations the value of u max needed to reach the optimal performance of the kriging is however much lower for the monotonic drift 3 against 10 this point is addressed in section 4 2c the road traffic nox emission and the distance to the roads are well taken into account the monotonic decreasing trend seems to generate smoother patterns across the roads which is visually nice and consistent with the decreasing forcing but not necessarily realistic see section 4 3 a spot effect is emphasized at the intersection of the roads and may be explained by the way the nox emission gis file is built a gridcell located at or close to this intersection can count twice as an additional source of pollution thus the concentrations might be falsely estimated to be twice as high in absence of validation measurements it is difficult to say if this kind of contribution has to be removed the best solution might be to provide a high resolution simulation run of the model ℳ not available in this study and compare this exhaustive simulation with the interpolation if some sources are indeed counted twice in the computation the statistical model should be expanded to solve this issue last the standard deviation of the kriging error when using the polynomial approximation of the drift is shown in figure 5f when umax is large a large number of predictors s u x are involved in the kriging that could artificially reduce its standard deviation in fact as u increases the uth drift and the related uth lagrange parameter become negligible and do not have a significant impact on the kriging variance 4 2 scores 4 2 1 cross validation the quality of the interpolation is assessed by leave one out cross validation for each value simulated by model ℳ an estimation of z x α is provided by interpolating z at location x α from the training set z x β α i e all the simulated data without z x α a geometric relationship connects the three statistics commonly used to evaluate the quality of a model here of the interpolation the correlation coefficient r the centered root mean squared error c rmse and the ability of the model here the interpolation to reproduce the variability as a standard deviation σ m of the original dataset σ d here the dispersion model taylor integrated this relationship into a convenient figure the taylor diagram taylor 2001 using a system of polar coordinates the degree of similarity between the original data and the different interpolations linear delaunay ordinary kriging kriging with external drift can thus be easily assessed the position of a specific interpolator on the diagram is defined so that its distance from the origin is equal to σ m its azimuthal position gives the correlation coefficient r a perfect estimate would therefore be located on the x axis of the diagram on the value σ m σ d and r 1 the empty circle on fig 6 because a lot of data are available the scores of all the interpolators are good with a correlation greater than 0 90 and the taylor diagram may give the impression they are all quite similar in performance going into the details see table 1 the kriging with a polynomial approximation of the drift gives the best results r 0 96 rmse 1 90 the scores obtained with the strictly monotonic and decreasing function are similar but still slightly worse linear and delaunay interpolations have lower correlations and higher rmse r 0 92 rmse 2 43 these two interpolators are almost identical which confirms the visual impression seen on the maps however they are better than the ordinary kriging for which the correlation coefficient drops to 0 91 and rmse increases to 2 6 for the krigings some additional tests were made with different estimation neighbourhoods but it has no significant impact in terms of correlation it can sometimes slightly modify the value of the standard deviation it is well known that the variance of a kriging interpolation is generally lower than the standard deviation of the original dataset to avoid an excessive smoothing the simulation from model ℳ has to be prevented from being too dispersed around a given interpolated value table 2 presents the total variance of the kriging error and the two terms in which it can be divided the conditional bias and the conditional dispersion see rivoirard 1984 to compute these scores the concentrations simulated by adms urban are averaged by classes of interpolated values once again the use of the polynomial approximation as a drift in kriging behaves best in terms of conditional bias 4 2 2 influence of the number of data the computational time for a simulation run of model ℳ is related to the number of receptor points involved in the simulation a major issue is thus to find out the minimal number of receptor points so that the estimation on a regular grid after interpolation is reliable to address that point six data subsets are thus randomly built from the original simulation dataset z x α in which the number p of data is respectively 200 500 1000 1500 2000 and 2500 a cross validation is then done with a moving neighbourhood where the number of neighbours is set to 500 as a compromise based on the localisation of the receptor points as a consequence the neighbourhood is unique for p 200 and p 500 fig 7 shows the increase respectively decrease of the correlation respectively rmse when the number of receptor points increases the linear and delaunay interpolation cannot be distinguished because the scores obtained with these two methods are very similar from p 500 the linear delaunay interpolators and ordinary kriging reveal a slight improvement of their performance when the number of receptor points increases the correlation goes from 0 89 to 0 91 when p increases from 500 to 3136 the original size of the simulation dataset the rmse lies between 3 and 2 6 regarding the polynomial approximation the scores remain almost stable around 0 95 correlation and 2 rmse the monotonic drift behaves similarly but not as good note that under too low a number of points 500 all the interpolators show a critical decrease of their performance because some important features are removed from the data for instance from p 500 to p 200 the correlation of the linear interpolation drops from 0 89 to 0 82 and its rmse increases from 3 to 3 3 it is even worse for the polynomial approximation of the drift because the statistical relationship is badly fitted when only few receptor points are available and thus sparsely sampled the correlation drops from 0 95 to 0 85 and the rmse increases from 2 to 3 however it still performs better than the other interpolators it might be interesting to use an empirical methodology so that an approximate threshold for the minimal number of points to use in different case studies can be found once again a solution could be to run the dispersion model on a high resolution grid and compare the results with the interpolations using different numbers of data and different grid configurations once the comparison is made and the minimal number of receptor points is found the grid used for running the simulations can be defined once and for all its future use in order to optimize the computational cost in any case the computational time of the proposed kriging based interpolator is a great advantage with respect to the computational time induced by a simulation on a high resolution grid the 2920 full three hourly simulations needed to get access to the annual average on a 10m 10 m grid 37500 receptor points would take approximately 3 days depending on the configuration of the machine the sources of pollution accounted for in the model the chemical scheme etc meanwhile the kriging based estimation of the no2 annual average from a limited number of receptor points is only taking a few minutes without any parallel programming exactly 9 min on an intel broadwell 16 cpu cluster 2 1 ghz with 60 gb ram in addition to the 7 5 h required for the simulations on p 3136 receptor points or 2 5 h if we reduce p down to 1000 with a similar performing skill even when dealing with daily average data the 24 full hourly simulations needed to get access to the daily average would still take 4 times longer than the interpolation 4 2 3 influence of the series expansion order of truncation the performance of the interpolation is clearly improved by the traffic related drift modelling however the validity of the statistical modelling cannot only be assessed from cross validation scores indeed the simple fact of introducing additional information about nox emissions is sometimes enough to improve the scores thus both mappings and scores obtained when u max the order of truncation of the series lies between 2 and 20 are looked at for the strictly monotonic decreasing function u max is assessed from 1 to 10 the behaviour of the correlation and the rmse fig 8 reveals a strong improvement when u max increases from 2 to 10 the slope is then lower but the scores keep improving till reaching a sill when u max 15 for the monotonic drift based on simple regressors of the form 1 x r u the speed of convergence is higher the sill of both correlation and rmse is reached for u max 3 but its performance is not as good regarding the mapping fig 9 and more specifically the polynomial approximation of the drift it is clear that when u max is too low the drift does not catch at all the influence of the roads on the concentration and the map looks like a linear interpolation fig 5a but when u max increases the quality of the drift improves and the patterns related to the traffic network also progressively appear up to stabilization these results are a first validation of the idea to use a universal kriging in which the drift is a series whose order of truncation depends on the nature of the decreasing function in addition the fact that the polynomial approximation converges with a reasonable number of terms indicates that the choice to not link the coefficients of the regressors in the kriging system is a key point in the methodology if the decreasing function was really an exponential the number of terms of the series required to ensure a reasonable approximation for the values of κ 1e 2 and the range of distances considered 0 400 m would be a hundred and not 20 thus letting the coefficients a u k x of the drift be freely adjusted enables to converge faster without constraining the form of the drift based on the complete analysis of the scores kriging the data with a polynomial approximation of the drift appears to be the best estimator among the set of interpolators tested a complete discussion on this polynomial approximation is thus provided in the next section 4 3 about the drift coefficients the kriging with external drift allows to estimate the drift itself m k x 0 see e g chiles and delfiner 2012 in particular when y x is constant over the domain m z k x 0 u 0 u max a u k x 0 s u x 0 b k x 0 α λ α m x 0 z x α where a u k x 0 and b k x 0 are the coefficients of the drift estimated by kriging the λ α m s are the weights for the kriging of the drift and m z k x 0 is the non stationary drift estimated by kriging i e a local mean the kriging system is exactly the same see eq 15 but on its right hand side the term γ x β x 0 is replaced by 0 and the predictor s u x 0 by δ u l 1 if u l 0 else because the kriging weights of the nearest neighbours quickly decrease fig 10 but also to limit the cpu time and avoid a strong neighbourhood effect on the maps the number of nearest neighbours used for the kriging estimation at location x 0 is set to 500 over a large part of the domain the constant term b k x estimated by kriging can be interpreted as a background concentration its median is about 26 μgm 3 fig 11 b which is consistent with the range of background concentrations presented in section 3 according to the map of traffic related nox emissions fig 4 the modelling of the drift m s x is not completely valid in some areas with a dense road network fig 11a and this term b k x does not only describe the background feature but can be interpreted as a correction term due to the simplified model of eq 11 guaranteeing the kriging error from not having an overall positive or negative bias as already said a kriging with external drift is basically a sum of a deterministic local mean m x and a zero mean second order stationary random field w x when the exponential function is decomposed as a series the drift model becomes a linear combination of auxiliary predictors see eq 11 the coefficients a u are unknown now lets us consider a 0 c κ 0 and a 1 c κ 1 a 0 κ as it should have been in eq 8 in that case the other coefficients a u should be related to a 0 and a 1 by the following relation 18 to guarantee the series to be a true approximation of the exponential function 18 a u a 0 a 1 a 0 u u 2 however this constraint does not exist in the kriging system 15 doing so the power series gives more freedom in the computation of the drift but this polynomial approximation can significantly deviate from the original exponential series fig 12 shows the distribution of the coefficients a u k u 2 3 4 computed on the estimation grid at a resolution of 10 m and how they deviate from the theoretical coefficients in eq 18 because the coefficients a u are few they are larger than those of the true power series expansion which is infinite even if these coefficients end to be quickly negligible the cross validation in section 4 2 shows that they still have a significant influence for values of u lower than 15 however the polynomial approximation in its current state presents some fluctuations potentially negative they could result in concentrations lower than the underlying background concentration in the vicinity of the road but real observations and even simulations might not always have a strictly decreasing behavior according to the distance across the roads because of some parameters related to the street network configuration that can induce an accumulation of pollution in areas located reasonably far from the roads see fig 1b these parameters are not taken into account by the model 11 thus it is a good thing that the flexibility of the external drift framework allows the estimation of a non monotonic drift m s k x possibly more realistic and pragmatically performing better than any other decreasing functions clearly the fact that some negative values of the drift not especially of the final estimation may appear is a problem that has to be discussed but this can occur in any other kriging estimations in the concrete application this issue only concerns very specific traffic related situations moreover increasing the number of coefficients umax reduces the fluctuations of the polynomial approximation around zero given the available computing resources nowadays the use of 15 predictors s u x for the polynomial approximation or 3 for the other decreasing function should not significantly affect the kriging computational time 5 coupling model and passive sampling data in sections 4 1 and 4 2 the concentrations were estimated from the adms urban simulation outputs z x α without use of any observational data t x α in that case it was shown that the interpolation z k x 0 of the simulation dataset based on the kriging system 15 is the best among all the interpolators tested when available it may be interesting to use such information to get closer to the level of concentration t x 0 given at locations x α by the observations and supposed to be closer to the truth than the model outputs neglecting their uncertainty of measurement a cokriging with external drift between the passive sampling data t x α and the adms simulations z x α using eq 8 as the model for the local mean would be the most convenient however it was shown in section 4 2b that too low a number of data does not lead to robust interpolations because the drift is poorly estimated thus because the passive sampling observations are too few or too far from the main roads this method would not be reasonable a way to get around this issue is to pragmatically use the two step procedure described in section 2 5 a comparison is now carried out when using either the kriging based interpolator with polynomial drift z k x 0 described in eq 15 or the linear interpolation z l x 0 for the step a of the procedure the results are unexpected while z k x α is the most efficient to interpolate the simulation dataset z x α in step a see section 4 2 it entails a decrease of the correlation between the simulations and the passive sampling data 0 69 fig 13 a compared to the same correlation computed with z l x α 0 732 fig 13b another statistical tool to quantify the intensity of a non necessarily linear relationship between two variables is the correlation ratio saporta 2011 19 η 2 k 1 p n k x k x 2 i 1 n x i x 2 where n k is the number of points in the class k x k the average of the estimation in class k the full red circles on fig 13a and b surrounded by the standard deviation of the class and x the global mean in our case 8 classes x k k 1 8 are built so that each class has the same amount of data n k the correlation ratio between t x α and z l x α also drops when using z k x α 0 63 against 0 58 based on the results of section 4 2 z k x 0 performs in average better than z l x 0 to estimate z x 0 and is a good interpolation of what an exhaustive adms simulation would be but the correlation with the observations t k x α is lower when using z k x α meaning that adms shows less good agreement with the observations than the linear interpolation let suppose it is probably due to the oversmoothing of the linear methods that leads to better scores or it may reflect measurement uncertainty the direct consequence is a drop of correlation between observational data and their kriging estimates in step b fig 14 a and b away from the sampling measurement sites the weight of the drift in the estimation is more important in these areas the estimation t k x 0 built from eq 17 is thus closer to the simulation z x 0 but not necessarily to the true observed concentration t x 0 because improving the interpolation of the drift predictor f t x 0 might take the estimation away from the observations see again fig 13a and b a possible explanation can be found in castelier 1993 a drift function with a higher variability than the residuals is not a good idea to improve the estimation in kriging with external drift 6 software availability the cpu time and difficulty in application of each interpolator used in this study grows with the complexity of the method the maps 5a and 5b are built with the help of the r packages akima and rgeos it only takes a few seconds to compute an estimation z x 0 for each gridcell of a 10 m 10 m grid the map 5c is computed with the geostatistical r library rgeostats renard et al 2010 it also takes less than 1 min to run this ordinary kriging depending on the neighbouring options an original c program has been developed to build the maps 5d and 5e according to the algorithm presented in appendix b and the methodology proposed in section 2 2 and 2 3 the armadillo c library conrad 2010 is involved for the numerical analysis related to kriging and the gdal gdal development team 2016 geospatial library enables to handle the gis component last an interface between r and c is used inside the c program towards the r library rcpparmadillo eddelbuettel and sanderson 2014 so that all the descriptive geostatistical part e g the variogram computation and fitting are done by rgeostats through this interface the pre processing step that consists in computing the terms s u x α implies an increase of the computational time for the kriging step but with an efficient programming it takes less than 10 min the full code is available via mendeley data https doi org 10 17632 ysmzhz5555 1 it is ready to use after installation of the appropriate libraries and specifications of the input data the appropriate shape for the inputs that are very basic are given as comments line along the program 7 conclusion air quality dispersion modelling is increasingly applied to produce concentration maps on the urban scale and evaluate population exposure to air pollution the proposed methodology uses an external drift modelling in which the traffic related pollution is a linear function of emissions that decreases exponentially with the distance from the road the exponential function is then broken down into its power series so that the multiplicative factor of the emissions and the decreasing parameter of the exponential in the kriging system are not constant which would have led to a residual kriging but can vary spatially because this polynomial approximation does not guarantee the decreasing function across the roads to be monotonic the same kriging system with simple predictors that ensures the drift to be monotonic is also introduced the performance of these kriging based approaches are assessed by cross validation and it is shown that they perform better than any other methods currently in use linear interpolation delaunay triangulation based interpolation ordinary kriging in addition after linearization of the exponential function the number of predictors which is necessary to make the kriging converge till its optimal performance is reasonable among the two models of decreasing functions across the roads the kriging system using simpler regressors and thus a monotonic drift converges faster till its optimal but it performs less well finally the polynomial approximation of the exponential function allows the estimation of a drift more flexible and consistent with the data since the use of atmospheric dispersion models keeps growing the computation time to provide an accurate information on grids with high resolution is becoming more and more important despite of the improvement of the computational resources the polynomial approximation of the drift makes it possible to reduce the number of locations where the urban model has to provide a simulation of the concentration before the kriging based mapping finally the interpolation map is very similar to a map simulated by the model at the same resolution in a similar fashion to what is done for large scale kriging in air quality a combination between the simulation outputs and actual observational data is made in the case of orléans the data comes from a passive sampling campaign carried out over the domain of the simulation when a drift is involved in kriging it has to be known everywhere and thus a prior interpolation step is used because the new kriging based approach is better to interpolate the simulation outputs than other interpolators based on linearity assumptions it should also be a better predictor for accounting for the non stationarity of the observations in kriging but that is not true in our case study the correlation between the sampling data and the simulation outputs interpolated at the sampling locations is better when using the linear interpolation instead of the kriging based approach it means that improving the interpolation of the simulation does not necessarily enable to better explain the concentrations observed at the monitoring sites in other words if the measurement uncertainty is ignored the urban dispersion model is further from the true level of concentration than anticipated by using a simple interpolation method because the post processing correction of the dispersion model with observational data is not satisfactory a good solution would be to encourage a more systematic use of observational data assimilation da techniques in areas benefiting from a dense monitoring network da will help to produce more accurate simulations based upon these improved simulations the kriging based approach can still be used to map the dispersion model outputs on a regular grid it provides a level of precision almost as high as the true simulation made with the dispersion model on a high resolution regular grid acknowledgements this study was conducted by the french central laboratory for air quality monitoring lcsqa it was funded by the french ministry in charge of the environment the authors are thankful to lig air for providing the different datasets appendices a review of commonly used approaches a 1 simplistic approaches linear interpolation let x α be the modelling dataset from which the interpolation is built the triangle t is made of three vertices x 1 x 11 x 21 x 2 x 12 x 22 and x 3 x 13 x 23 in which the point of interest x 0 x 10 x 20 lies the concentration z x 0 at x 0 is assumed to depend on its coordinates z x 0 z x 10 x 20 α x 10 β x 20 γ the solution is obtained from a linear combination of the observations at the vertices of the triangle by solving the system α x 11 β x 21 γ z x 1 α x 12 β x 22 γ z x 2 α x 13 β x 23 γ z x 3 fig a1 linear interpolation fig a1 solving this system provides the coefficients for the calculation at any point included in the triangle the solution is z x 0 i 1 3 t i z x i t where t is the area of the triangle with vertices x 1 x 2 and x 3 t 1 respectively t 2 and t 3 is the area of the triangle with vertices x 0 x 2 and x 3 respectively x 0 x 1 x 3 and x 0 x 1 x 2 each vertex get a weight proportional to the surface of the opposite triangle last in this kind of interpolations based on a triangulation of the domain it is not possible to interpolate outside of the convex envelope of the x α s delaunay interpolation the delaunay interpolation is based on a delaunay triangulation that defines a mesh of triangles as compact as possible the circle in which each triangle is circumscribed only contains the vertices of this single triangle a first variant of this technique consists in generating a regular grid where the value interpolated at each point is calculated from the 3 values of the triangle in which it is located inversely weighted by the distance more recently researchers from the french laboratory irstv in partnership with air pays de la loire a french local agency in charge of monitoring air quality developed another variant more suited to pollution in urban areas fortin et al 2012 it is available in the software orbisgis http orbisgis org in this variant a delaunay triangulation fig a2 for instance is built from the irregularly distributed modelling dataset fig a2 delaunay triangulation on adms data 2 fig a2 fig a3 interpolation across the edges fig a3 the delaunay interpolation aims at decomposing the initial delaunay triangulation in a set of polygons ƥ according to a predefined scale of concentration i s o 0 i s o 1 i s o l where i s o i i 1 l are isocontours i e concentration values related to a given variable no2 annual mean here to achieve this decomposition for each triangle t x 1 x 2 x 3 of d the variation of concentrations along the edges is assumed to be linear thus between two vertices x i and x j it is possible to locate the exact position along the edge x i x j where the concentration reaches a given isocontour i s o k then let us consider the positions of the two consecutive isocontours i s o k and i s o k 1 along each edge of t then the convex envelope of these positions is a polygon p triangle or quadrangle inside which the value of concentration z p lies between i s o k and i s o k 1 see fig a3 in this approach the more accurate is the predefined scale of concentration and the more precise will be the mapping in respect to the linear hypothesis akima s polynomial interpolation a two dimensional bivariate interpolation method has been developed by akima 1978 for irregular datasets based on the fitting of a surface whose equation is a local polynomial of degree 5 he used a triangulation method lawson 1972 that maximizes the minimal internal angle of the triangles z x 0 j 0 5 k 0 5 j α j k x 10 j x 20 k therefore 21 conditions are defined to find the 21 coefficients α j k the three first conditions are obtained from the values at vertices six additional conditions are obtained from the first partial derivative at vertices nine additional conditions are obtained from the second partial derivatives at vertices the three last conditions are deducted from continuity and differentiability at the limits of each triangle the partial derivative of the interpolation function in the normal direction to each edge x i x j is a polynomial of degree 3 at most bilinear interpolation the bilinear interpolation can be used when the original data set is available on a regular grid it is widely used in air quality mapping for large scale outputs of chemistry transport model ctm but not that much at urban scale where the model outputs are generally not spatially distributed on a regular grid fig a4 bilinear interpolation fig a4 it allows to calculate the value at any point x 0 x 10 x 20 with an interpolator that is the product of two linear functions z x 0 a x 10 b x 20 c x 10 x 20 d a b c and d are constants defined from its four nearest neighbours x 1 x 11 x 21 x 2 x 12 x 21 x 3 x 11 x 23 and x 4 x 12 x 23 1 x 11 x 21 x 11 x 21 1 x 11 x 23 x 11 x 23 1 x 12 x 21 x 12 x 21 1 x 12 x 23 x 12 x 23 a b c d z x 1 z x 2 z x 3 z x 4 inverse distance weighting the weights w α associated to the concentrations z x α decrease according to the specific function 1 d x x α p d is the metric used to compute the distance and p ℝ z x 0 k 0 n w α z x α k 0 n w α nearest neighbour the nearest neighbour algorithm is the simplest way to interpolate non regularly distributed dataset it selects the value of the nearest point x α as an estimation of z x 0 at location x 0 yielding a piecewise constant interpolator the related voronoi diagram is a partition of the domain into cells made by assigning to all the points inside a cell the value of the nearest given point that is always inside the cell z x 0 arg min z x α d x 0 x α fig a5 voronoi diagram on adms data fig a5 a 2 spatial statistics approach ordinary kriging kriging is a geostatistical interpolator in its univariate version the generic formulation of z k x 0 the estimation of the concentration z x 0 at x 0 is the weighted average of the sampling data over the domain z k x 0 α λ α z x α where the kriging weights λ α s are deducted from the unbiasedness condition e z k x z x 0 and optimality for the variance of the error var z k x z x see e g chiles and delfiner 2012 the estimation is fully controlled by the choice of the covariance model or more generally of the variogram within the intrinsec hypothesis a stationary assumption is introduced it means that the degree of correlation between two location only depends on the distance between them rather than their location classically a sample variogram γ ˆ h is computed by averaging the square deviations between two observations located in the same class of distance γ ˆ h δ 1 2 n h δ α β n h δ z x α z x β 2 where the distance h is taken plus or minus some bin width tolerance range δ it is then fitted with a linear combination of valid models spherical exponential gaussian power etc two different krigings can be performed block kriging consists in estimating the mean value in each gridcell which requires the definition of an intra block covariance while in punctual kriging the value is estimated in the center of the gridcell since the resolution of the final interpolation map is usually very high in urban areas in the range of a meter punctual kriging is sufficient and block kriging is not useful this usual way of calculating the variogram and performing kriging is however not well adapted to urban pollution data indeed the concentrations are influenced by traffic related emission sources that clearly make the stationary assumption of the random process not valid however this type of non stationarity can fairly be explained by explanatory variables thus the external drift framework chiles and delfiner 2012 should be used to estimate the traffic related trend but it has to be adapted to the specific configuration of the road network bayesian maximum entropy bme approach in the listing of existing alternative the bayesian maximum entropy methodology christakos 2000 that is a superset of classical geostatistics interpolators should be mentioned it is based on a space time random field so it was not selected to deal with our data that are annual averaged values of observed and simulated air quality data nevertheless it could be applied to provide estimations for hourly daily data in urban areas the use of additional information sources render it a significant contender in reducing artifacts regardless of the spatial distribution of the modelling points see e g bme based air pollution and atmospheric studies in christakos and li 1998 christakos et al 2004 yu et al 2016 2011 the bme framework is based on a space time random field z p where p x t x denotes the spatial location and t the time the framework is divided into two knowledge base kb first the general one g kb m z p β c z p p θ where m z p β is the expectation of z parametrized by β and c z p p θ c w x x t t is the covariance function parametrized by θ that describes the space time dependencies of the residuals w between z and m z assumed as stationary second the site specific one s kb z α f s z m where z α are the so called hard data i e the observations made at the monitoring stations p α are considered as an exact value for the vector of random variables z α p z α z α 1 z m are the soft data predictions made by the model at locations p m in terms of a site specific pdf f s z m i e no assumptions are made on the relationship between z m and z m p z α z m z m f s z m the classic set of bme equations is d z g z e g e μ t g z 0 d ξ s z e μ t g z a f k z 0 0 where z is a vector of concentrations at locations p p α p 0 union of the data points p α and estimation sites p 0 g is a vector of functions chosen such that e g is known from g kb ξ s is an operator representing s kb a is a normalization constant and f s z 0 is the bme posterior pdf of z 0 at p 0 finally the subscript k in f k stands for the blending of g kb and s kb in their work dealing with ozone mapping de nazelle et al 2010 show that in this context the bme set of equations simply is f k z 0 a 1 d z m f s z m f g z d z m f s z m f g z where z z 0 z α z m is a realization of z 0 z α z m whose pdf f g z is a multivariate gaussian pdf a f s z m f g z is a normalization constant when β θ are unknown f g z f z β ˆ θ ˆ z α with least square estimates β ˆ θ ˆ f s z m i 1 n m f z i z i p i measures the quality of the predictions made by the model n m is the number of receptor points in the model z i is the true value of concentration at receptor point p i and z i its prediction by the model in de nazelle et al 2010 an explicit formulation of f z i z i p i is made assuming z i is normally distributed with a truncation below zero its expected value λ 1 z i and variance λ 2 z i are only functions of the model predictions without any changes across space and or time xu et al 2016 extended this work and make λ 1 z i p i and λ 2 z i p i be functions of both z i space and time coordinates for applications to larger domain stochastic partial differentiate equations spde a growing popular approach is the estimation through the spde approach lindgren et al 2011 let the matérn covariance function between two locations x α and x β ℝ d d 2 here be defined as r x α x β σ 2 γ ν 2 ν 1 κ x β x α ν k ν κ x β x α where denotes the euclidean distance in ℝ d k ν is the modified bessel function of second kind and order ν 0 κ 0 is a scaling parameter and σ 2 is the marginal variance matérn fields i e gaussian field gf with matérn covariance z x z x x d on ℝ d are stationary solutions to the linear fractional stochastic partial differential equation spde κ 2 δ α 2 z x w x x ℝ d α ν d 2 κ 0 ν 0 where δ i 1 d 2 x i 2 is the laplacian and κ 2 δ α 2 is a pseudo differential operator defines through its spectral properties whittle 1954 1963 w is spatial gaussian white noise with unit variance the spde approach aims at finding the best n dimensional gaussian markov random field gmrf representation z z 1 z n n μ q 1 of the gf with a local neighbourhood and a precision matrix q that is sparse from the markovian property of the process and thus enables to avoid the so called big n problem that occurs when working with the dense covariance matrix of a gf this gmrf representation of the matérn fields is found for a given triangulation by building a finite element representation of the solution to the spde as z x l 1 n ψ k x ω k where n is the total number of vertices ψ k x are some chosen basic functions and ω k are gaussian distributed weights finally the precision matrix q of the gmrf ω ω 1 ω n is defined according to equation 10 of lindgren et al 2011 as a function of κ 2 according to the drift modelling defined for the non stationary process z x an estimation using bayesian inference to get approximated posterior marginal for both latent variables and hyperparameters of the trend are possible an interesting application to space time pollution data is made in cameletti et al 2012 using the inla computational approach in lindgren et al 2011 are also shown study cases for non stationary fields non isotropic models with spatial deformations and non separable space time models even if this approach is appealing the computational cost and the performance of our modelling are satisfactory and does not require to involve such a framework algorithm for interpolation of urban model outputs image 6 
26383,the european directives for ambient air quality require to assess areas where air pollutant concentrations exceed a regulatory threshold as the spatial distribution of the pollutant is not exactly known deterministic atmospheric dispersion models are commonly used to supplement the observational network to reduce the computational time the simulations are made on irregular grids especially in urban areas where the grid is refined close to the roads an interpolation method is then necessary to map the dispersion model at any location we propose a new geostatistical approach based on kriging with external drift to distinguish the information along and across the roads an exponential function is introduced to describe the decrease of the concentrations across the roads its series expansion is used to build a set of polynomial auxiliary predictors with unknown coefficients this framework leads to a drift that is more generic and flexible in the kriging system keywords geostatistics kriging air pollution urban scale series expansion drift 1 introduction the european legislation directive 2008 50 ec 2008 on ambient air quality defines some environmental objectives expressed as annual daily hourly averaged concentrations or maximum numbers of days hours in exceedance depending on the pollutant and the time resolution when one or several thresholds are exceeded the member states have to delineate the spatial extent and the population exposed to these exceedances a lot of studies estimate the exposure to exceedances by crossing a map of concentrations with a static map of the population i e counted at their place of residence which is also the norm in the european regulation for this reason this work falls within this framework and is focused on the improvement of the concentration maps without worrying about the issue of the cross combination with the population however a growing umber of studies have recently estimated the same exposure but taking into account the dynamic aspect of the population that moves and sometimes works away from the residential areas these works are mostly built on activity based exposure models beckx et al 2009 hatzopoulou and miller 2010 jantunen et al 1999 who 2005 or simulations of the daily movement of the population using for instance mobile phone tracking liu et al 2013 gariazzo et al 2016 when occurring in urban areas and it is mostly the case for no2 and pm10 the exceedances are usually assessed by using urban scale dispersion models let us note ℳ this type of model to limit the computational costs a widespread practice is to calculate the concentrations in a two step procedure first the concentrations z x α are simulated on the irregular grid x α α 1 p with a coarse regular resolution in background areas and a higher adaptative resolution close to the roads see e g leelőssy et al 2014 next they are interpolated on a regular grid with high resolution over the whole domain of simulation d regarding the observational data t x α fixed monitoring sites passive sampling measurements available at locations x α α 1 d d p they are seldom introduced in a data assimilation framework tilloy et al 2013 to reduce the errors made by the dispersion model ℳ a kriging based combination of simulated data and passive sampling measurements can also be performed see e g aspa 2014 technical report in that case the frequency and the extent of the sampling campaigns play an important part in the quality of the results as a consequence the final mapping of z x over d mostly depends on three criteria the number p of x α in the simulation also denoted as the receptor points their locations and the precision of the interpolation technique in air quality the simulated concentrations are usually interpolated by linear methods implemented in post processing tools such as climate data operators cdo netcdf operators nco see zender 2016 in the urban configuration there is an additional difficulty because the concentrations come from both traffic related and background sources of pollution thus according to the spatial distribution of the receptor points the usual interpolation methods may lead to some artifacts in this paper an original adaptation of kriging based interpolation is proposed for the simulations made by an urban scale dispersion model first a review of the standard existing approaches is presented then an external drift modelling is introduced to assess the behaviour of the concentrations across the roads the concentration z x simulated by the model is seen as a stochastic process explicitly decomposed into a deterministic part m z x and a zero mean second order stationary random field w x the series expansion of the exponential function is used to linearize the expression of the deterministic part and thus stick to the usual kriging with external drift framework chiles and delfiner 2012 the three next sections are dedicated to an application of the methodology on the french city of orléans in 2010 source lig air the no2 annual mean simulated by the adms urban air quality model is considered in section 3 the dataset for both z x α and t x α are presented as well as the traffic emissions used to build the predictors of m x section 4 compares the results of a selection of standard interpolators and the exponential drift framework in terms of mapping and cross validation scores a detailed study concerning the advantages of the series expansion is also provided with additional guidelines regarding the truncation of the series and the minimal number of receptor points to use last a discussion is given to comment some key points of the methodology and in particular the consequences of the polynomial approximation on the coefficients of the drift estimated by kriging section 5 is focused on a kriging where the interpolation z k x of the simulation dataset is used as an external drift to improve the estimation t k x of the data collected from a passive sampling campaign finally section 6 is dedicated to software availability just before the conclusion of the paper 2 materials 2 1 overview of commonly used approaches the linear interpolation is widely used to provide air quality model outputs anywhere in space whatever the scale the model deals with it is considered as valid when the variations between the x α s are low or when the distances between the x α s are small a few meters in traffic related configuration but up to a few kilometers for low background concentrations more recently fortin et al 2012 developed a method based on the delaunay triangulation see also lixin et al 2011 for further applications of the delaunay triangulation to air quality interpolation assuming that the pollutant concentration along the triangle edges varies linearly polygons linking all the positions where the concentration is equal to a given value are defined last the ordinary kriging chiles and delfiner 2012 is a linear combination of the data with optimal weights satisfying unbiasedness constraint e z k x z x 0 and optimality for the variance of the error var z k x z x the statistical geostatistical and gis software make it possible to implement linear or delaunay interpolation and ordinary kriging with limited effort in addition an evaluation study beauchamp et al 2016 using 5 different french urban datasets bourges nantes niort orléans reims and tours has shown the relative efficiency of the linear and delaunay interpolation compared to ordinary kriging this can be explained by the non stationarity related to the multi source origin of the concentrations either influenced by the traffic or only reflecting the background pollution ordinary kriging is not able to account for it when dealing with few data jeannee and lemarchand 2012 handled this issue by considering locally varying anisotropies but other kriging options are available in particular universal kriging based approaches see section 2 2 in which the non stationarities are taken into account through the modelling of the deterministic part m x also called the drift of the random process other interpolators inverse distance nearest neighbour akima s interpolator will not be considered in this paper because they are too simplistic and only relevant for smooth concentrations fields more sophisticated estimation methods are not addressed in this paper they could be useful if hourly daily simulation outputs are considered instead of the annual averaged values among them the bayesian maximum entropy methodology bme that is a superset of classical geostatistics interpolators has already been used in air pollution and atmospheric studies christakos and li 1998 christakos et al 2004 yu et al 2016 2011 the growing popular estimation approach through the spde stochastic partial differential equation framework lindgren et al 2011 must also be mentioned an interesting application to space time pm10 pollution data is made in cameletti et al 2012 using the inla integrated nested laplace approximation computational approach a basic introduction to these methods is given in appendix a 2 2 traffic related external drift modelling in urban areas the pollution at a location x can be considered as the sum of a background component and a traffic related contribution due to the emissions of the roads in the close neighbourhood of x to simplify these two components will be supposed spatially independent see for instance font et al 2014 let denote z x the concentration simulated by the model ℳ at x not to be mistaken with the true value of the concentration denoted as t x and introduced later in section 2 5 y x is the random function related to the background feature and s x is the random function that deals with the pollution increment related to traffic emissions 1 z x y x s x the independence between y x and s x can be checked out by averaging the values s x α at locations x α s considered as traffic influenced per urban background concentration classes y 1 y p y x α is not known but may be obtained either by roughly removing the traffic related sources in the air quality simulation or by kriging the background data i e the receptor points located far enough from the road network in the literature more sophisticated models can also be used to estimate this quantity see e g pournazeri et al 2014 the appropriate definition of far enough depends on the corresponding environments so that the model in eq 1 is correct according to the literature see for example baldwin et al 2015 zou et al 2006 gilbert et al 2003 roorde knape et al 1999 and repeated results from regular campaigns in france the direct impact of road traffic can be considered insignificant 400 m away from the road thus it possibly holds as true for similar environments elsewhere as a consequence when x is located more than 400 m away from the road s x is neglected regarding the traffic related part of the process the chemical transformations are not taken into account as a simplification doing so the traffic related pollution s x is seen as the sum of the traffic related terms s k x k δ x belonging to each road r k located in the close vicinity δ x of the location x in the following the neighbourhood δ x k 1 n x x r k d max is defined according to a maximal distance d max 400 m around x in the following n x the number of roads in the neighbourhood of x is simply denoted as n 2 z x y x k δ x s k x the pollution term s k x brought by the road r k at location x is seen as a sum of a second order stationary and isotropic random field with zero mean w s k x and a deterministic part the drift m s k x e s k x this drift is seen as a continuous sum of the function g whose two parameters are s k r the contributions brought by each position r along r k and the distance x r between the position r at the centre of the road and the location x used to describe the decrease of these contributions s k r when moving away from the road 3 s k x m s k x w s k x r k g s k r x r d r w s k x fig 1 presents the variables and processes directly related to the concentration close to the traffic network fig 1a is the simplified configuration in which only the distance to the road is considered to build the drift m s k x in eq 3 fig 1b details the main other parameters that actually may impact the value of the concentration the screen effect of the buildings between r and x is left aside because it is difficult to describe from the available informations height surface volume and shape of the building however in the real world it might imply accumulation of pollution but also a change in the diffusion of the pollutant over the area the meteorology and the chemistry also play a part in the decrease across the road especially for hourly or daily averaged simulations see e g pineda rojas 2014 but it is not considered here as a consequence m s k x is a strong approximation of the reality whose errors go into the residual w s k x regarding the background part of z x if the domain d of simulation is small enough a few km2 so should be the variability of the background concentrations and it can thus be assumed as constant because this variability is negligible with respect to the range of the traffic related pollution s x if not y x can be seen as a random process 4 y x m y x w y x with w y x once again a zero mean second order stationary random field and the deterministic part m y x is a linear function of the background nox emissions denoted as f y x and used as a predictor the background emissions can be defined as the emissions mainly due to residential commercial agriculture and forestry sectors directly responsible for the background concentration i e the concentration that would be measured if local sources traffic industrial were not present it would also be possible to use a large scale ctm output as f y x to explain the actual background concentration or use these two sources of information as predictors for m y x doing so the full statistical model becomes 5 z x m y x m s x w x m z x w x where m s x is the sum of the n x local drifts m s k x and w x is the global residual i e the sum of w y x and all the w s k x whose spatial correlation will be studied and used to improve the estimation of z x 2 3 a polynomial series expansion for the decreasing function a statistical model is proposed to explain both s k r and the decreasing behaviour according to the function g s k r the no2 traffic related pollution at location r of road r k is seen as a linear function c φ r c ℝ of the traffic related nox emissions φ r at the same location because it is obviously not possible to quantify the values of the sources at any location r φ r is calculated as the ratio between φ k the annual emission of nox of the road r k as specified in a geographic information system gis in kg year 1 and r k its length according to previous studies taylor et al 2015 baldwin et al 2015 faus kessler et al 2008 cape et al 2004 pleijel et al 2004 it is relevant to consider that the traffic related pollution decreases exponentially with the distance to the road it makes sense for the nox concentration but also for the no2 despite the local chemistry which converts no to no2 because of the linear relationship existing between no2 and nox emissions even for daily averaged data thus the expected value for the random function s k x is 6 e s k x r k c φ k r k e κ x r x r d r κ x r should depend on location x and r but it is now considered as a constant parameter it is a key point in the method because the kriging based approach that is proposed further in eq 15 enables to estimate local values of the drift parameters thus relaxing the simplifications made in eq 6 the integral term along the road is in practice not known and has to be approximated by decomposing each road into p small segments φ r φ k r k φ k r l k r k where r l k is the l th section of the road r k and r is now located at the center of this section seguret et al 2003 proposed another modelling in a work intended to map roadside no2 concentrations around the main road of the thur valley in france the drift is seen as a combination of exponential and polynomial functions whose decrease across the road is adjusted according to observational data which is consistent with other pollution studies see e g monn et al 1997 in the orléans dataset presented in section 3 about 2000 roads are introduced the introduction of such a model would make harder both the estimation for the variogram of w x and the computation of the drift instead of fixing the value of κ in the exponential function the decreasing function is approximated using a series expansion as a polynomial approximation that is a sum of power functions with alternate signs in which the coefficients are unknown these coefficients will be locally estimated by solving the linear kriging system 15 with an appropriate moving neighbourhood i e by using only the points x α within a 100 m moving window around the location point x since the power functions x u are increasing functions when x 0 the only way to capture a decreasing shape is to alternate signs which is prone to lack of robustness hence the artifacts produced by this approximation as well as the number of power functions needed to reach a satisfactory performance score are discussed in section 4 3 from eq 6 the expected value of s x is 7 e s x k 1 n e s k x k 1 n r k c φ k r k e κ x r d r the parameters c and κ are not known the idea is to use the power series of the exponential function so that the decrease in e s x when moving away from the roads will be approximated by a polynomial combination of the distances x r e κ x r u 0 1 u κ u x r u u thus the deterministic part m s x of s x can be seen as a linear combination of predictors 8 e s x u 0 c κ u s u x with the predictors s u x defined as 9 s u x k 1 n r k φ k r k 1 u x r u u d r the kriging system for the estimation of z k x 0 α λ α z x α is built upon 1 the unbiasedness condition e z k x 0 z x 0 0 that leads to α λ α u 0 c κ u s u x α u 0 c κ u s u x 0 and then u 0 κ u α λ α s u x α s u x 0 0 it finally consists in finding the coefficients of the polynomial a 0 a 1 κ a n κ n since κ is unknown the unbiasedness constraint becomes 10 α λ α s u x α s u x 0 0 u this unbiasedness constraint does not require any prior information for the parameters c and κ κ is actually not known but neither explicitly linked between all the auxiliary predictors as it should be in the true expansion series of the exponential as a consequence the model for the drift becomes 11 e s x u 0 a u s u x where the a u are the coefficients to be estimated by the kriging a discussion about the advantages and the drawbacks of condition 10 is given in section 4 3 let us note that if the background part of z x is itself considered as an independent random process with a deterministic part potentially explained by an auxiliary predictor f y x see eq 4 at the end of section 2 2 then this predictor would be added to the set of auxiliary variables s u x in the kriging system with the additional unbiasedness condition 12 α λ α f y x α f y x 0 and the intercept term b of this background drift also adds another unbiasedness condition 13 λ α 1 this usual unbiasedness constraint in kriging makes valid the substitution of the covariance by the corresponding variogram in the kriging system in the case where y x b is a constant this condition also appears in the kriging system 2 the optimality of the variance of the error under conditions 10 12 and 13 the weights λ α are given by minimizing 14 λ α arg min λ α var z k x 0 z x 0 2 ν 0 α λ α 1 2 ν 1 α λ α f y x α f y x 0 2 u 0 μ u α λ α s u x α s u x 0 when m y x a f y x b and where the μ u s are the lagrangian parameters related to order u of the series ν 0 and ν 1 are the lagrangian parameters related to conditions 12 and 13 finally the kriging system can be written 15 α 1 n λ α γ x α x β ν 0 ν 1 f y x β u 0 u max μ u s u x β γ x β x 0 β α 1 n λ α 1 α 1 n λ α f y x α f y x 0 α 1 n λ α s u x α s u x 0 u 0 u max where γ is the variogram of the residuals w x s x e s x the size of the system is proportional to the order of truncation u max of the series the value of u max is pragmatically defined according to the convergence study of the performance scores provided in section 4 2 the algorithm to carry out the interpolation using the polynomial approximation of eq 8 as external drift is presented in appendix b to sum up even if the effect induced by all the roads mixes up all the distances to the roads in the neighbourhood of x are taken into account by s u x and the coefficients a u k x are estimated locally by kriging and thus supposed to describe the local behaviour of the decreasing function based on the spatial correlation of the stochastic residual w x although as in the usual external drift modelling the parameters a u do no longer appear in the kriging system however the variogram of the residuals w x is still supposed to be known because the drift is described as a linear combination of u max predictors in the kriging system the variogram is here directly estimated using an iterative fitting procedure of the drift where the residuals come from a k fold cross validation it can also be estimated by intrinsic random function of order k irf k indirect fitting chiles and delfiner 2012 or bayesian kriging techniques see e g omre 1987 breaking down the exponential function in its power series clearly makes appear a local polynomial drift usually involved in universal kriging in which the local coordinate system of the drift is based on the distance from the road seguret et al 2003 mention this analogy in their study of the thur valley in france as a consequence instead of introducing the exponential function processing the data through the irf k theory matheron 1971 is an other option it may fit most part of the phenomenon by using all possible linear combinations of polynomials of degree k what is done here by truncation of the exponential series but the spatial structure of the residual w x would be accounted for by a generalized covariance thus solving the bias that might be introduced when estimating the variogram through a preliminary cross estimation of the residuals last in section 3 the study case deals with annual data even if annual averaging in air pollutant concentrations can generally reduce concentration peaks and troughs that are present in daily time series the proposed methodology should also apply for day to day collected data the increased concentration variability compared to the annually aggregated values is not expected to produce any additional issues in the application thanks to the use of eq 11 that allows a local adaptation of the drift if the values of s u x remain the same the coefficients a u k x estimated by kriging and supposed to describe the decrease of the concentrations across the roads will not be the same if z x are hourly daily or annual data 2 4 a monotonic drift because the polynomial approximation of eq 11 has no clear physical meaning and thus may appear as a rather artificial way to produce a decreasing function another competitor method is considered it is a universal kriging with very simple regressors that takes into account the distance from the road network according to some clever metrics for instance in the kriging system 15 the uth predictor in location x could be 16 s u x k r k g u r x d r k r k φ k r k 1 x r u d r it ensures the drift to be strictly monotonic because g r x u g u r x 0 when x r which seems to be more consistent to describe the expected value of the decrease in concentration across the roads 2 5 coupling observations and model outputs deterministic urban models have an inherent modelling error due to the simplification of physical and chemical processes in addition the simulation outputs may also have some errors related to lacks and uncertainties of the input data for these reasons using observational data as another source of information is recommended to estimate the concentration for large scale mapping applications a usual procedure consists in combining a chemistry transport model ctm as an external drift with observations from a national or continental measurement network malherbe et al 2011 at smaller scale there are still few similar works it can be explained by the higher variability of concentrations and the difficulty to take all the sources of pollution into account to build a statistical estimator nevertheless the recent work done in the field of data assimilation tilloy et al 2013 with the blue estimator has to be mentioned it also introduces the notion of distance to the roads in the covariance matrix a multivariate approach by cokriging has also been experimented between passive sampling campaign and outputs of urban models see e g aspa s technical report 2014 in section 5 the large scale methodology is applied to urban datasets because the number of fixed monitoring stations in urban areas is often limited passive sampling diffusion measurements t x α carried out along the year make a convenient method for estimating no2 annual mean from a larger number of observational points x α even if it is not the standard use in geostatistics to repeat successive kriging algorithms a two step procedure is set up the step a consists in building an auxiliary predictor f t x z k x that is the interpolation of model ℳ outputs z x α from eq 15 based on the cross validation scores from section 4 2 the error made when estimating the explanatory variable f t x is thus strongly reduced in comparison to what it would be with a linear interpolation denoted as z l x the step b is a kriging t k x 0 α λ α t x α of the passive sampling data t x α in which the auxiliary variable f t x is estimated from step a and used as a universality constraint 17 α 1 n λ α γ x α x β μ 0 μ 1 f t x β γ x β x 0 β α 1 n λ α 1 α 1 n λ α f t x α f t x 0 where γ denotes here the variogram of the residuals t x m t x and the λ α are the weights of the observations t x α this makes sense because in kriging with external drift a first step is sometimes involved to interpolate the auxiliary variable f t x at the sample locations x α and thus be able to estimate the coefficients of the drift m t x a f t x b before computing the sample variogram last even if the diffusion tube uncertainty is known to be possibly large in high polluted areas it is not considered here because this information was not available if it was it would still possible to account for it by adding random white noise ε x α with no spatial correlation to the observations leading to add the corresponding variance term var ε x α on the diagonal of the kriging matrix γ x α x α 3 data 3 1 observations no2 concentration data were provided by lig air the french local agency in charge of air quality monitoring in the region centre val de loire they were collected during a passive sampling campaign carried out in 2010 in the urban area of orléans fig 2 shows the spatial distribution of the sampling data t x α the sampling sites x α were located at background locations 15 sites along the roads 29 sites and at small distances across the roads 6 sites to assess how the concentrations decrease when moving away from the traffic network the annual mean in each sampling site is estimated by averaging the data of the twelve monthly periods of measurements along the year 3 2 adms urban the dataset z x α from the air quality modelling software adms urban is also made available by lig air and covers the same period adms urban carruthers et al 1998 is an air quality dispersion model that includes the wide range of emission sources such as road traffic industrial and domestic emissions the solution of navier stokes is analytically solved using a three dimensional quasi gaussian formulation the input data required by the model are the meteorology the background concentrations and the emission inventories the output simulation mesh is usually qualified as smart it is a coarse regular grid for background configurations and an adaptative meshing across the roads with the main emission sources the required boundary parameters are computed by a meteorological pre processor from specified input data wind speed cloud cover for more details see tilloy et al 2013 and references therein for a good summary of the physical processes involved in the model this model is assumed to reproduce the distribution of the traffic related concentrations since it takes into account the topography of the road network more especially when buildings are higher than 0 5 m a street canyon module is activated to solve the concentration field it is based on a danish street pollution model hertel and berkowicz 1989 in which the no2 concentration is determined from the nox concentration as described in derwent and middleton 1996 the modelling grid includes 3136 receptor points x α distributed all over the domain see fig 3 the simulated background concentrations i e the concentrations simulated at the points located more than 400 m away from any roads are moderate ranging from 20 to 25 μ gm 3 when approaching the road traffic network the concentrations significantly increase and the regulatory limit value defined for the no2annual mean 40 μ gm 3 is often exceeded on average the model can be seen as a smoother of the observations shown in fig 2 i e it underestimates no2 in high polluted areas and overestimates it in the background 3 3 emissions the emissions inventory only includes traffic related sources fig 4 because the size of the domain is very small over orléans 2 5 km 1 5 km and the background concentration is thus considered as locally constant over the area as discussed in section 2 2 it is however possible to include industrial and domestic emissions in the methodology if m y x see eq 4 is introduced the urban topography is estimated from lig air database the road emissions are computed using copert iv the computer program to calculate emission from road transport coordinated by the european environment agency eea in the context of the activities of the european topic centre for air pollution and climate change mitigation it relies on a database of unitary emission factors the methodology adopted is to attribute for each pollutant and for each vehicle class a unitary emission factor that depends on the fuel mode the engine size the vehicle registration date and the vehicle speed as defined by road signs it also depends on the traffic conditions known from the previous observations of traffic counters in the agglomeration these conditions integrate monthly and daily profiles warm cold and slope induced emissions are taken into account for the heavy transport and some corrections are applied for old vehicles and for fuel improvements the road emissions in grams are then computed as the product of the number of vehicles per km and the unitary emission factor in g km vehicles 1 4 results in the application case because y x is assumed to be constant its value has to be roughly defined and is set to 24 μ g m 3 according to the simulation at background receptor points in order to estimate the experimental variogram γ ˆ x α x β though when the model ℳ is close to the truth the value y x b k x estimated by kriging can be slightly different see section 4 3 4 1 mapping as a first step the linear delaunay ordinary kriging and the newly kriging based with external drift interpolation of the adms urban model outputs are used to map z x onto the domain d via a 10 m 10 m regular grid ordinary kriging fig 5 c is computed with an isotropic variogram nugget effect 9 7 μ g 2 m 6 spherical model with range 813 m and sill 31 μ g 2 m 6 and a moving neighbourhood with the 10 nearest points the smoothing appears non realistic which is confirmed by cross validation scores section 4 2 the linear and delaunay interpolations fig 5a and b look quite similar the patterns along and across the roads are well described but in the areas with few or no data the configuration of the road network fig 4 is not known and the interpolation behaves like a background concentration even though some roads should logically induce higher concentrations the traffic related drift modelling fig 5d and e with an isotropic variogram of the residuals nugget effect 7 μ g 2 m 6 spherical model with range 700 m and sill 8 μ g 2 m 6 seems to solve the problems induced by the other methods the mapping is more realistic without any oversmoothing close to the roads the polynomial and monotonic drift lead to very similar estimations the value of u max needed to reach the optimal performance of the kriging is however much lower for the monotonic drift 3 against 10 this point is addressed in section 4 2c the road traffic nox emission and the distance to the roads are well taken into account the monotonic decreasing trend seems to generate smoother patterns across the roads which is visually nice and consistent with the decreasing forcing but not necessarily realistic see section 4 3 a spot effect is emphasized at the intersection of the roads and may be explained by the way the nox emission gis file is built a gridcell located at or close to this intersection can count twice as an additional source of pollution thus the concentrations might be falsely estimated to be twice as high in absence of validation measurements it is difficult to say if this kind of contribution has to be removed the best solution might be to provide a high resolution simulation run of the model ℳ not available in this study and compare this exhaustive simulation with the interpolation if some sources are indeed counted twice in the computation the statistical model should be expanded to solve this issue last the standard deviation of the kriging error when using the polynomial approximation of the drift is shown in figure 5f when umax is large a large number of predictors s u x are involved in the kriging that could artificially reduce its standard deviation in fact as u increases the uth drift and the related uth lagrange parameter become negligible and do not have a significant impact on the kriging variance 4 2 scores 4 2 1 cross validation the quality of the interpolation is assessed by leave one out cross validation for each value simulated by model ℳ an estimation of z x α is provided by interpolating z at location x α from the training set z x β α i e all the simulated data without z x α a geometric relationship connects the three statistics commonly used to evaluate the quality of a model here of the interpolation the correlation coefficient r the centered root mean squared error c rmse and the ability of the model here the interpolation to reproduce the variability as a standard deviation σ m of the original dataset σ d here the dispersion model taylor integrated this relationship into a convenient figure the taylor diagram taylor 2001 using a system of polar coordinates the degree of similarity between the original data and the different interpolations linear delaunay ordinary kriging kriging with external drift can thus be easily assessed the position of a specific interpolator on the diagram is defined so that its distance from the origin is equal to σ m its azimuthal position gives the correlation coefficient r a perfect estimate would therefore be located on the x axis of the diagram on the value σ m σ d and r 1 the empty circle on fig 6 because a lot of data are available the scores of all the interpolators are good with a correlation greater than 0 90 and the taylor diagram may give the impression they are all quite similar in performance going into the details see table 1 the kriging with a polynomial approximation of the drift gives the best results r 0 96 rmse 1 90 the scores obtained with the strictly monotonic and decreasing function are similar but still slightly worse linear and delaunay interpolations have lower correlations and higher rmse r 0 92 rmse 2 43 these two interpolators are almost identical which confirms the visual impression seen on the maps however they are better than the ordinary kriging for which the correlation coefficient drops to 0 91 and rmse increases to 2 6 for the krigings some additional tests were made with different estimation neighbourhoods but it has no significant impact in terms of correlation it can sometimes slightly modify the value of the standard deviation it is well known that the variance of a kriging interpolation is generally lower than the standard deviation of the original dataset to avoid an excessive smoothing the simulation from model ℳ has to be prevented from being too dispersed around a given interpolated value table 2 presents the total variance of the kriging error and the two terms in which it can be divided the conditional bias and the conditional dispersion see rivoirard 1984 to compute these scores the concentrations simulated by adms urban are averaged by classes of interpolated values once again the use of the polynomial approximation as a drift in kriging behaves best in terms of conditional bias 4 2 2 influence of the number of data the computational time for a simulation run of model ℳ is related to the number of receptor points involved in the simulation a major issue is thus to find out the minimal number of receptor points so that the estimation on a regular grid after interpolation is reliable to address that point six data subsets are thus randomly built from the original simulation dataset z x α in which the number p of data is respectively 200 500 1000 1500 2000 and 2500 a cross validation is then done with a moving neighbourhood where the number of neighbours is set to 500 as a compromise based on the localisation of the receptor points as a consequence the neighbourhood is unique for p 200 and p 500 fig 7 shows the increase respectively decrease of the correlation respectively rmse when the number of receptor points increases the linear and delaunay interpolation cannot be distinguished because the scores obtained with these two methods are very similar from p 500 the linear delaunay interpolators and ordinary kriging reveal a slight improvement of their performance when the number of receptor points increases the correlation goes from 0 89 to 0 91 when p increases from 500 to 3136 the original size of the simulation dataset the rmse lies between 3 and 2 6 regarding the polynomial approximation the scores remain almost stable around 0 95 correlation and 2 rmse the monotonic drift behaves similarly but not as good note that under too low a number of points 500 all the interpolators show a critical decrease of their performance because some important features are removed from the data for instance from p 500 to p 200 the correlation of the linear interpolation drops from 0 89 to 0 82 and its rmse increases from 3 to 3 3 it is even worse for the polynomial approximation of the drift because the statistical relationship is badly fitted when only few receptor points are available and thus sparsely sampled the correlation drops from 0 95 to 0 85 and the rmse increases from 2 to 3 however it still performs better than the other interpolators it might be interesting to use an empirical methodology so that an approximate threshold for the minimal number of points to use in different case studies can be found once again a solution could be to run the dispersion model on a high resolution grid and compare the results with the interpolations using different numbers of data and different grid configurations once the comparison is made and the minimal number of receptor points is found the grid used for running the simulations can be defined once and for all its future use in order to optimize the computational cost in any case the computational time of the proposed kriging based interpolator is a great advantage with respect to the computational time induced by a simulation on a high resolution grid the 2920 full three hourly simulations needed to get access to the annual average on a 10m 10 m grid 37500 receptor points would take approximately 3 days depending on the configuration of the machine the sources of pollution accounted for in the model the chemical scheme etc meanwhile the kriging based estimation of the no2 annual average from a limited number of receptor points is only taking a few minutes without any parallel programming exactly 9 min on an intel broadwell 16 cpu cluster 2 1 ghz with 60 gb ram in addition to the 7 5 h required for the simulations on p 3136 receptor points or 2 5 h if we reduce p down to 1000 with a similar performing skill even when dealing with daily average data the 24 full hourly simulations needed to get access to the daily average would still take 4 times longer than the interpolation 4 2 3 influence of the series expansion order of truncation the performance of the interpolation is clearly improved by the traffic related drift modelling however the validity of the statistical modelling cannot only be assessed from cross validation scores indeed the simple fact of introducing additional information about nox emissions is sometimes enough to improve the scores thus both mappings and scores obtained when u max the order of truncation of the series lies between 2 and 20 are looked at for the strictly monotonic decreasing function u max is assessed from 1 to 10 the behaviour of the correlation and the rmse fig 8 reveals a strong improvement when u max increases from 2 to 10 the slope is then lower but the scores keep improving till reaching a sill when u max 15 for the monotonic drift based on simple regressors of the form 1 x r u the speed of convergence is higher the sill of both correlation and rmse is reached for u max 3 but its performance is not as good regarding the mapping fig 9 and more specifically the polynomial approximation of the drift it is clear that when u max is too low the drift does not catch at all the influence of the roads on the concentration and the map looks like a linear interpolation fig 5a but when u max increases the quality of the drift improves and the patterns related to the traffic network also progressively appear up to stabilization these results are a first validation of the idea to use a universal kriging in which the drift is a series whose order of truncation depends on the nature of the decreasing function in addition the fact that the polynomial approximation converges with a reasonable number of terms indicates that the choice to not link the coefficients of the regressors in the kriging system is a key point in the methodology if the decreasing function was really an exponential the number of terms of the series required to ensure a reasonable approximation for the values of κ 1e 2 and the range of distances considered 0 400 m would be a hundred and not 20 thus letting the coefficients a u k x of the drift be freely adjusted enables to converge faster without constraining the form of the drift based on the complete analysis of the scores kriging the data with a polynomial approximation of the drift appears to be the best estimator among the set of interpolators tested a complete discussion on this polynomial approximation is thus provided in the next section 4 3 about the drift coefficients the kriging with external drift allows to estimate the drift itself m k x 0 see e g chiles and delfiner 2012 in particular when y x is constant over the domain m z k x 0 u 0 u max a u k x 0 s u x 0 b k x 0 α λ α m x 0 z x α where a u k x 0 and b k x 0 are the coefficients of the drift estimated by kriging the λ α m s are the weights for the kriging of the drift and m z k x 0 is the non stationary drift estimated by kriging i e a local mean the kriging system is exactly the same see eq 15 but on its right hand side the term γ x β x 0 is replaced by 0 and the predictor s u x 0 by δ u l 1 if u l 0 else because the kriging weights of the nearest neighbours quickly decrease fig 10 but also to limit the cpu time and avoid a strong neighbourhood effect on the maps the number of nearest neighbours used for the kriging estimation at location x 0 is set to 500 over a large part of the domain the constant term b k x estimated by kriging can be interpreted as a background concentration its median is about 26 μgm 3 fig 11 b which is consistent with the range of background concentrations presented in section 3 according to the map of traffic related nox emissions fig 4 the modelling of the drift m s x is not completely valid in some areas with a dense road network fig 11a and this term b k x does not only describe the background feature but can be interpreted as a correction term due to the simplified model of eq 11 guaranteeing the kriging error from not having an overall positive or negative bias as already said a kriging with external drift is basically a sum of a deterministic local mean m x and a zero mean second order stationary random field w x when the exponential function is decomposed as a series the drift model becomes a linear combination of auxiliary predictors see eq 11 the coefficients a u are unknown now lets us consider a 0 c κ 0 and a 1 c κ 1 a 0 κ as it should have been in eq 8 in that case the other coefficients a u should be related to a 0 and a 1 by the following relation 18 to guarantee the series to be a true approximation of the exponential function 18 a u a 0 a 1 a 0 u u 2 however this constraint does not exist in the kriging system 15 doing so the power series gives more freedom in the computation of the drift but this polynomial approximation can significantly deviate from the original exponential series fig 12 shows the distribution of the coefficients a u k u 2 3 4 computed on the estimation grid at a resolution of 10 m and how they deviate from the theoretical coefficients in eq 18 because the coefficients a u are few they are larger than those of the true power series expansion which is infinite even if these coefficients end to be quickly negligible the cross validation in section 4 2 shows that they still have a significant influence for values of u lower than 15 however the polynomial approximation in its current state presents some fluctuations potentially negative they could result in concentrations lower than the underlying background concentration in the vicinity of the road but real observations and even simulations might not always have a strictly decreasing behavior according to the distance across the roads because of some parameters related to the street network configuration that can induce an accumulation of pollution in areas located reasonably far from the roads see fig 1b these parameters are not taken into account by the model 11 thus it is a good thing that the flexibility of the external drift framework allows the estimation of a non monotonic drift m s k x possibly more realistic and pragmatically performing better than any other decreasing functions clearly the fact that some negative values of the drift not especially of the final estimation may appear is a problem that has to be discussed but this can occur in any other kriging estimations in the concrete application this issue only concerns very specific traffic related situations moreover increasing the number of coefficients umax reduces the fluctuations of the polynomial approximation around zero given the available computing resources nowadays the use of 15 predictors s u x for the polynomial approximation or 3 for the other decreasing function should not significantly affect the kriging computational time 5 coupling model and passive sampling data in sections 4 1 and 4 2 the concentrations were estimated from the adms urban simulation outputs z x α without use of any observational data t x α in that case it was shown that the interpolation z k x 0 of the simulation dataset based on the kriging system 15 is the best among all the interpolators tested when available it may be interesting to use such information to get closer to the level of concentration t x 0 given at locations x α by the observations and supposed to be closer to the truth than the model outputs neglecting their uncertainty of measurement a cokriging with external drift between the passive sampling data t x α and the adms simulations z x α using eq 8 as the model for the local mean would be the most convenient however it was shown in section 4 2b that too low a number of data does not lead to robust interpolations because the drift is poorly estimated thus because the passive sampling observations are too few or too far from the main roads this method would not be reasonable a way to get around this issue is to pragmatically use the two step procedure described in section 2 5 a comparison is now carried out when using either the kriging based interpolator with polynomial drift z k x 0 described in eq 15 or the linear interpolation z l x 0 for the step a of the procedure the results are unexpected while z k x α is the most efficient to interpolate the simulation dataset z x α in step a see section 4 2 it entails a decrease of the correlation between the simulations and the passive sampling data 0 69 fig 13 a compared to the same correlation computed with z l x α 0 732 fig 13b another statistical tool to quantify the intensity of a non necessarily linear relationship between two variables is the correlation ratio saporta 2011 19 η 2 k 1 p n k x k x 2 i 1 n x i x 2 where n k is the number of points in the class k x k the average of the estimation in class k the full red circles on fig 13a and b surrounded by the standard deviation of the class and x the global mean in our case 8 classes x k k 1 8 are built so that each class has the same amount of data n k the correlation ratio between t x α and z l x α also drops when using z k x α 0 63 against 0 58 based on the results of section 4 2 z k x 0 performs in average better than z l x 0 to estimate z x 0 and is a good interpolation of what an exhaustive adms simulation would be but the correlation with the observations t k x α is lower when using z k x α meaning that adms shows less good agreement with the observations than the linear interpolation let suppose it is probably due to the oversmoothing of the linear methods that leads to better scores or it may reflect measurement uncertainty the direct consequence is a drop of correlation between observational data and their kriging estimates in step b fig 14 a and b away from the sampling measurement sites the weight of the drift in the estimation is more important in these areas the estimation t k x 0 built from eq 17 is thus closer to the simulation z x 0 but not necessarily to the true observed concentration t x 0 because improving the interpolation of the drift predictor f t x 0 might take the estimation away from the observations see again fig 13a and b a possible explanation can be found in castelier 1993 a drift function with a higher variability than the residuals is not a good idea to improve the estimation in kriging with external drift 6 software availability the cpu time and difficulty in application of each interpolator used in this study grows with the complexity of the method the maps 5a and 5b are built with the help of the r packages akima and rgeos it only takes a few seconds to compute an estimation z x 0 for each gridcell of a 10 m 10 m grid the map 5c is computed with the geostatistical r library rgeostats renard et al 2010 it also takes less than 1 min to run this ordinary kriging depending on the neighbouring options an original c program has been developed to build the maps 5d and 5e according to the algorithm presented in appendix b and the methodology proposed in section 2 2 and 2 3 the armadillo c library conrad 2010 is involved for the numerical analysis related to kriging and the gdal gdal development team 2016 geospatial library enables to handle the gis component last an interface between r and c is used inside the c program towards the r library rcpparmadillo eddelbuettel and sanderson 2014 so that all the descriptive geostatistical part e g the variogram computation and fitting are done by rgeostats through this interface the pre processing step that consists in computing the terms s u x α implies an increase of the computational time for the kriging step but with an efficient programming it takes less than 10 min the full code is available via mendeley data https doi org 10 17632 ysmzhz5555 1 it is ready to use after installation of the appropriate libraries and specifications of the input data the appropriate shape for the inputs that are very basic are given as comments line along the program 7 conclusion air quality dispersion modelling is increasingly applied to produce concentration maps on the urban scale and evaluate population exposure to air pollution the proposed methodology uses an external drift modelling in which the traffic related pollution is a linear function of emissions that decreases exponentially with the distance from the road the exponential function is then broken down into its power series so that the multiplicative factor of the emissions and the decreasing parameter of the exponential in the kriging system are not constant which would have led to a residual kriging but can vary spatially because this polynomial approximation does not guarantee the decreasing function across the roads to be monotonic the same kriging system with simple predictors that ensures the drift to be monotonic is also introduced the performance of these kriging based approaches are assessed by cross validation and it is shown that they perform better than any other methods currently in use linear interpolation delaunay triangulation based interpolation ordinary kriging in addition after linearization of the exponential function the number of predictors which is necessary to make the kriging converge till its optimal performance is reasonable among the two models of decreasing functions across the roads the kriging system using simpler regressors and thus a monotonic drift converges faster till its optimal but it performs less well finally the polynomial approximation of the exponential function allows the estimation of a drift more flexible and consistent with the data since the use of atmospheric dispersion models keeps growing the computation time to provide an accurate information on grids with high resolution is becoming more and more important despite of the improvement of the computational resources the polynomial approximation of the drift makes it possible to reduce the number of locations where the urban model has to provide a simulation of the concentration before the kriging based mapping finally the interpolation map is very similar to a map simulated by the model at the same resolution in a similar fashion to what is done for large scale kriging in air quality a combination between the simulation outputs and actual observational data is made in the case of orléans the data comes from a passive sampling campaign carried out over the domain of the simulation when a drift is involved in kriging it has to be known everywhere and thus a prior interpolation step is used because the new kriging based approach is better to interpolate the simulation outputs than other interpolators based on linearity assumptions it should also be a better predictor for accounting for the non stationarity of the observations in kriging but that is not true in our case study the correlation between the sampling data and the simulation outputs interpolated at the sampling locations is better when using the linear interpolation instead of the kriging based approach it means that improving the interpolation of the simulation does not necessarily enable to better explain the concentrations observed at the monitoring sites in other words if the measurement uncertainty is ignored the urban dispersion model is further from the true level of concentration than anticipated by using a simple interpolation method because the post processing correction of the dispersion model with observational data is not satisfactory a good solution would be to encourage a more systematic use of observational data assimilation da techniques in areas benefiting from a dense monitoring network da will help to produce more accurate simulations based upon these improved simulations the kriging based approach can still be used to map the dispersion model outputs on a regular grid it provides a level of precision almost as high as the true simulation made with the dispersion model on a high resolution regular grid acknowledgements this study was conducted by the french central laboratory for air quality monitoring lcsqa it was funded by the french ministry in charge of the environment the authors are thankful to lig air for providing the different datasets appendices a review of commonly used approaches a 1 simplistic approaches linear interpolation let x α be the modelling dataset from which the interpolation is built the triangle t is made of three vertices x 1 x 11 x 21 x 2 x 12 x 22 and x 3 x 13 x 23 in which the point of interest x 0 x 10 x 20 lies the concentration z x 0 at x 0 is assumed to depend on its coordinates z x 0 z x 10 x 20 α x 10 β x 20 γ the solution is obtained from a linear combination of the observations at the vertices of the triangle by solving the system α x 11 β x 21 γ z x 1 α x 12 β x 22 γ z x 2 α x 13 β x 23 γ z x 3 fig a1 linear interpolation fig a1 solving this system provides the coefficients for the calculation at any point included in the triangle the solution is z x 0 i 1 3 t i z x i t where t is the area of the triangle with vertices x 1 x 2 and x 3 t 1 respectively t 2 and t 3 is the area of the triangle with vertices x 0 x 2 and x 3 respectively x 0 x 1 x 3 and x 0 x 1 x 2 each vertex get a weight proportional to the surface of the opposite triangle last in this kind of interpolations based on a triangulation of the domain it is not possible to interpolate outside of the convex envelope of the x α s delaunay interpolation the delaunay interpolation is based on a delaunay triangulation that defines a mesh of triangles as compact as possible the circle in which each triangle is circumscribed only contains the vertices of this single triangle a first variant of this technique consists in generating a regular grid where the value interpolated at each point is calculated from the 3 values of the triangle in which it is located inversely weighted by the distance more recently researchers from the french laboratory irstv in partnership with air pays de la loire a french local agency in charge of monitoring air quality developed another variant more suited to pollution in urban areas fortin et al 2012 it is available in the software orbisgis http orbisgis org in this variant a delaunay triangulation fig a2 for instance is built from the irregularly distributed modelling dataset fig a2 delaunay triangulation on adms data 2 fig a2 fig a3 interpolation across the edges fig a3 the delaunay interpolation aims at decomposing the initial delaunay triangulation in a set of polygons ƥ according to a predefined scale of concentration i s o 0 i s o 1 i s o l where i s o i i 1 l are isocontours i e concentration values related to a given variable no2 annual mean here to achieve this decomposition for each triangle t x 1 x 2 x 3 of d the variation of concentrations along the edges is assumed to be linear thus between two vertices x i and x j it is possible to locate the exact position along the edge x i x j where the concentration reaches a given isocontour i s o k then let us consider the positions of the two consecutive isocontours i s o k and i s o k 1 along each edge of t then the convex envelope of these positions is a polygon p triangle or quadrangle inside which the value of concentration z p lies between i s o k and i s o k 1 see fig a3 in this approach the more accurate is the predefined scale of concentration and the more precise will be the mapping in respect to the linear hypothesis akima s polynomial interpolation a two dimensional bivariate interpolation method has been developed by akima 1978 for irregular datasets based on the fitting of a surface whose equation is a local polynomial of degree 5 he used a triangulation method lawson 1972 that maximizes the minimal internal angle of the triangles z x 0 j 0 5 k 0 5 j α j k x 10 j x 20 k therefore 21 conditions are defined to find the 21 coefficients α j k the three first conditions are obtained from the values at vertices six additional conditions are obtained from the first partial derivative at vertices nine additional conditions are obtained from the second partial derivatives at vertices the three last conditions are deducted from continuity and differentiability at the limits of each triangle the partial derivative of the interpolation function in the normal direction to each edge x i x j is a polynomial of degree 3 at most bilinear interpolation the bilinear interpolation can be used when the original data set is available on a regular grid it is widely used in air quality mapping for large scale outputs of chemistry transport model ctm but not that much at urban scale where the model outputs are generally not spatially distributed on a regular grid fig a4 bilinear interpolation fig a4 it allows to calculate the value at any point x 0 x 10 x 20 with an interpolator that is the product of two linear functions z x 0 a x 10 b x 20 c x 10 x 20 d a b c and d are constants defined from its four nearest neighbours x 1 x 11 x 21 x 2 x 12 x 21 x 3 x 11 x 23 and x 4 x 12 x 23 1 x 11 x 21 x 11 x 21 1 x 11 x 23 x 11 x 23 1 x 12 x 21 x 12 x 21 1 x 12 x 23 x 12 x 23 a b c d z x 1 z x 2 z x 3 z x 4 inverse distance weighting the weights w α associated to the concentrations z x α decrease according to the specific function 1 d x x α p d is the metric used to compute the distance and p ℝ z x 0 k 0 n w α z x α k 0 n w α nearest neighbour the nearest neighbour algorithm is the simplest way to interpolate non regularly distributed dataset it selects the value of the nearest point x α as an estimation of z x 0 at location x 0 yielding a piecewise constant interpolator the related voronoi diagram is a partition of the domain into cells made by assigning to all the points inside a cell the value of the nearest given point that is always inside the cell z x 0 arg min z x α d x 0 x α fig a5 voronoi diagram on adms data fig a5 a 2 spatial statistics approach ordinary kriging kriging is a geostatistical interpolator in its univariate version the generic formulation of z k x 0 the estimation of the concentration z x 0 at x 0 is the weighted average of the sampling data over the domain z k x 0 α λ α z x α where the kriging weights λ α s are deducted from the unbiasedness condition e z k x z x 0 and optimality for the variance of the error var z k x z x see e g chiles and delfiner 2012 the estimation is fully controlled by the choice of the covariance model or more generally of the variogram within the intrinsec hypothesis a stationary assumption is introduced it means that the degree of correlation between two location only depends on the distance between them rather than their location classically a sample variogram γ ˆ h is computed by averaging the square deviations between two observations located in the same class of distance γ ˆ h δ 1 2 n h δ α β n h δ z x α z x β 2 where the distance h is taken plus or minus some bin width tolerance range δ it is then fitted with a linear combination of valid models spherical exponential gaussian power etc two different krigings can be performed block kriging consists in estimating the mean value in each gridcell which requires the definition of an intra block covariance while in punctual kriging the value is estimated in the center of the gridcell since the resolution of the final interpolation map is usually very high in urban areas in the range of a meter punctual kriging is sufficient and block kriging is not useful this usual way of calculating the variogram and performing kriging is however not well adapted to urban pollution data indeed the concentrations are influenced by traffic related emission sources that clearly make the stationary assumption of the random process not valid however this type of non stationarity can fairly be explained by explanatory variables thus the external drift framework chiles and delfiner 2012 should be used to estimate the traffic related trend but it has to be adapted to the specific configuration of the road network bayesian maximum entropy bme approach in the listing of existing alternative the bayesian maximum entropy methodology christakos 2000 that is a superset of classical geostatistics interpolators should be mentioned it is based on a space time random field so it was not selected to deal with our data that are annual averaged values of observed and simulated air quality data nevertheless it could be applied to provide estimations for hourly daily data in urban areas the use of additional information sources render it a significant contender in reducing artifacts regardless of the spatial distribution of the modelling points see e g bme based air pollution and atmospheric studies in christakos and li 1998 christakos et al 2004 yu et al 2016 2011 the bme framework is based on a space time random field z p where p x t x denotes the spatial location and t the time the framework is divided into two knowledge base kb first the general one g kb m z p β c z p p θ where m z p β is the expectation of z parametrized by β and c z p p θ c w x x t t is the covariance function parametrized by θ that describes the space time dependencies of the residuals w between z and m z assumed as stationary second the site specific one s kb z α f s z m where z α are the so called hard data i e the observations made at the monitoring stations p α are considered as an exact value for the vector of random variables z α p z α z α 1 z m are the soft data predictions made by the model at locations p m in terms of a site specific pdf f s z m i e no assumptions are made on the relationship between z m and z m p z α z m z m f s z m the classic set of bme equations is d z g z e g e μ t g z 0 d ξ s z e μ t g z a f k z 0 0 where z is a vector of concentrations at locations p p α p 0 union of the data points p α and estimation sites p 0 g is a vector of functions chosen such that e g is known from g kb ξ s is an operator representing s kb a is a normalization constant and f s z 0 is the bme posterior pdf of z 0 at p 0 finally the subscript k in f k stands for the blending of g kb and s kb in their work dealing with ozone mapping de nazelle et al 2010 show that in this context the bme set of equations simply is f k z 0 a 1 d z m f s z m f g z d z m f s z m f g z where z z 0 z α z m is a realization of z 0 z α z m whose pdf f g z is a multivariate gaussian pdf a f s z m f g z is a normalization constant when β θ are unknown f g z f z β ˆ θ ˆ z α with least square estimates β ˆ θ ˆ f s z m i 1 n m f z i z i p i measures the quality of the predictions made by the model n m is the number of receptor points in the model z i is the true value of concentration at receptor point p i and z i its prediction by the model in de nazelle et al 2010 an explicit formulation of f z i z i p i is made assuming z i is normally distributed with a truncation below zero its expected value λ 1 z i and variance λ 2 z i are only functions of the model predictions without any changes across space and or time xu et al 2016 extended this work and make λ 1 z i p i and λ 2 z i p i be functions of both z i space and time coordinates for applications to larger domain stochastic partial differentiate equations spde a growing popular approach is the estimation through the spde approach lindgren et al 2011 let the matérn covariance function between two locations x α and x β ℝ d d 2 here be defined as r x α x β σ 2 γ ν 2 ν 1 κ x β x α ν k ν κ x β x α where denotes the euclidean distance in ℝ d k ν is the modified bessel function of second kind and order ν 0 κ 0 is a scaling parameter and σ 2 is the marginal variance matérn fields i e gaussian field gf with matérn covariance z x z x x d on ℝ d are stationary solutions to the linear fractional stochastic partial differential equation spde κ 2 δ α 2 z x w x x ℝ d α ν d 2 κ 0 ν 0 where δ i 1 d 2 x i 2 is the laplacian and κ 2 δ α 2 is a pseudo differential operator defines through its spectral properties whittle 1954 1963 w is spatial gaussian white noise with unit variance the spde approach aims at finding the best n dimensional gaussian markov random field gmrf representation z z 1 z n n μ q 1 of the gf with a local neighbourhood and a precision matrix q that is sparse from the markovian property of the process and thus enables to avoid the so called big n problem that occurs when working with the dense covariance matrix of a gf this gmrf representation of the matérn fields is found for a given triangulation by building a finite element representation of the solution to the spde as z x l 1 n ψ k x ω k where n is the total number of vertices ψ k x are some chosen basic functions and ω k are gaussian distributed weights finally the precision matrix q of the gmrf ω ω 1 ω n is defined according to equation 10 of lindgren et al 2011 as a function of κ 2 according to the drift modelling defined for the non stationary process z x an estimation using bayesian inference to get approximated posterior marginal for both latent variables and hyperparameters of the trend are possible an interesting application to space time pollution data is made in cameletti et al 2012 using the inla computational approach in lindgren et al 2011 are also shown study cases for non stationary fields non isotropic models with spatial deformations and non separable space time models even if this approach is appealing the computational cost and the performance of our modelling are satisfactory and does not require to involve such a framework algorithm for interpolation of urban model outputs image 6 
26384,win win solutions might be short lived government permission for smallholder farmers to extract and sell resin from a pine savanna biosphere reserve in mexico has settled a long dispute among different stakeholders in the short term however forest production and conservation beyond 20 years are compromised due to low pine recruitment caused by competition with exotic grasses grass control practiced by farmers through grazing and fire has previously been discouraged by conservation authorities which inadvertently limits long term pine conservation and use we describe the participatory design rationale and simulation attributes of an educational interactive agent based model that explores suites of management options and their economic and ecological outputs we present and analyze the outcomes of four simulation workshops where farmers and external actors better grasped the complex ecological interactions involved in conserving and using pines in grazed pine savanna with exotic grasses and discussed these findings with a long term vision and tradeoff analysis approach keywords agent based model cattle in forests decision making forest management tradeoffs participatory modelling smallholder farmers software availability 1 introduction the man and the biosphere reserve mabr program is a worldwide program formally implemented in the 1970s as a space for accomplishing both paradigmatic rural development and protection of nature unesco 1996 in mexico this program incorporated many territories occupied by smallholder farmers within a new model of conservation by decree to improve the social and economic well being of populations it was supposed to promote economic social and environmental policies to allow families long established in these territories to sustain decent livelihoods by creating or supporting agroforestry or silvopastoral landscapes in buffer zones of mabr serving as high quality matrices for conservation bouamrane et al 2016 cruz morales 2014 martín lópez et al 2011 achieving this goal has been at best problematic from the very beginning di castri 1976 reasons range from stark conflict among actors related to possession or control over land to poor understanding and agreement over the effects of land management strategies on ecosystems and smallholder farmers livelihoods adams 2004 cruz morales 2014 garcía barrios and gonzález espinosa 2017 ma et al 2009 wittmer et al 2006 in this context win win land management solutions in mabrs are desirable but unusual when pathways are reasonably accepted by most or all parties they may represent progress in some dimensions plummer et al 2017 but will likely generate new issues and tradeoffs elsewhere something to be expected in any complex social ecological system agrawal and ostrom 2001 defries et al 2007 martín lópez et al 2011 more importantly some emerging issues might work directly against the previously agreed upon solution yet this might not be easily perceived or detected because their consequences are mid or long term allen and gunderson 2011 these emerging issues commonly become invisible get ignored or postponed for better times swept under the carpet by resource stricken actors accustomed to jointly muddle through so called wicked problems allen et al 2011 sierra huelsz et al 2017 this sometimes unavoidable mishap may have dire consequences when actors are dealing with changes in land cover land use and or livelihoods close to tipping points as the situation becomes extremely sensitive to miscalculations unconsidered indirect interactions and short term pragmatism carpenter and gunderson 2001 huber sannwald et al 2012 ribeiro palacios et al 2013 in la sepultura mabr chiapas mexico created in 1995 anthropogenic pine savannas surround highly valued montane forest core zones as part of the buffer zone conanp 2013 prior to that year modest pine lumber extraction and extensive cattle grazing were part of people s livelihood and intentional or accidental burning of the savanna understory was common guevara hernández et al 2013 navarro et al 2017 then as a conservation strategy the national commission of natural protected areas conanp according to its spanish acronym prohibited fire use tree extraction and livestock production in the pine savannas however these top down decisions affected smallholder farmers livelihoods and ignited a decade long conflict between communities and conanp cruz morales 2014 farmers saw no reason to protect pine trees on their land other than to avoid monetary sanctions or jail because of illegal extraction guevara hernández et al 2013 in 2012 all parties interests finally converged in a joint project to extract turpentine resin from adult pine trees to be sold to the alen del norte corporation under these new perspectives the imposed land management strategies now made more sense to smallholder farmers at least for a 20 year time span ahead during which current adult pine trees would yield a marketable oily product yet a hidden contradiction remained in most pine stands small native grasses and herbs have long been outcompeted by tall exotic grasses in the absence of fire and grazing they can be a significant obstacle for recruiting future generations of productive adult pine trees braasch et al 2017 by targeting this attractive short term win win solution i e protecting the pines and extracting resin actors in this partnership are paying little attention to the long term effects or do not reach consensus over strategies to deal with them insights from smallholder farmers and plant ecology suggest livestock grazing could create opportunities for pine recruitment but may also cause trampling damage or mortality of saplings archer et al 2017 braasch et al 2017 van langevelde et al 2003 werner 2005 interactive agent based simulation models abm and socioecological board games have emerged within different social learning frameworks e g the companion modelling approach methodology commod etienne 2014 has useful participatory education tools such as role playing games rpg that facilitate communication and reflection among those involved in resource management and promote a common knowledge ground from where to build effective management and governance le page et al 2010 etienne et al 2011 garcia barrios et al 2017 2015 2011 some of these frameworks and tools have allowed smallholder farmers and other actors to simulate and jointly explore land use and management options in rural small holder territories barnaud et al 2013 berthet et al 2016 etienne 2014 villamor and van noordwijk 2011 and more specifically in those contiguous or within mabr bouamrane et al 2016 perrotton et al 2017 abms are currently used in many fields of scientific research education and policy making as extremely powerful tools to better grasp complex processes an ever growing model library is currently available rollins et al 2014 many social educational and technical challenges associated with abm remain spanning from their proper development to their use as multi actor social learning tools within rural settings barnaud et al 2013 becu et al 2008 garcia barrios et al 2017 le page and perrotton 2017 fruitfully discuss the different objectives and tradeoffs involved in abstract stylized and realistic agent based models and stress that the requirements and purposes of social learning should guide the choice construction and use of these simulation models in multi actor land stewardship processes since 2007 the second author lgb has been leading participatory multidisciplinary research in la sepultura mabr with special emphasis on the social and ecological consequences of land use innovations meant to reconcile livelihoods and conservation garcía barrios and gonzález espinosa 2017 valencia et al 2015 2014 zabala et al 2017 in this process a number of agent based models speelman et al 2014a b speelman and garcía barrios 2010 and socio ecological board games garcia barrios et al 2009 2015 2011 have been developed for this study starting in 2014 we were welcomed by actors to follow and support the resin production project and we engaged in participatory research comprising field transects forest inventories ecological experiments farmer surveys agent based modelling and scenario simulation workshops to address the following questions do actors consider the pine savanna and resin turpentine extraction a short or long term livelihood and conservation option is pine recruitment actually lower where exotic grass is dominant is low pine recruitment a critical issue for the resin project and for which actors what management options for controlling exotic grass are preferred by different types of actors what are potential short and long term tradeoffs of these management options we described in detail the anthropogenic origin of the pine savanna braasch et al 2017 we showed that current pine population structure and low recruitment due to dense exotic grass cover cannot support long term resin production furthermore we presented experimental evidence that cattle grazing in the savanna may have both positive and negative effects on recruitment here we describe and discuss the development and use of an interactive agent based model with farmers and other actors to help address the above questions the objectives of this paper are to a describe the interactive stylized agent based model true grasp tree recruitment under exotic grass in the pine savanna and its background rationale and main attributes b present and discuss the outcomes of four true grasp simulation workshops held separately and jointly with smallholder farmers and external actors to address in a stylized qualitative way the questions listed above and to support social learning of all parties involved including ourselves 2 methods 2 1 study area the study was carried out in the pine savanna under ejido tenure a mixture of private and communal land belonging to the rural towns of california and tres picos located in the buffer zone of la sepultura biosphere reserve sbr in chiapas mexico 16 16 40 16 12 40 n and 93 37 10 93 32 55 w fig 1 the topography of the area is highly irregular with steep slopes dominant soils are regosols and cambisols over granitic rock the tropical climate is seasonally dry annual mean temperature ranges between 25 and 28 c average annual precipitation reaches 2003 484 mm 30 year average conagua 2015 the pine savanna is located between 900 and 1100 m above sea level both communities were established during the 1970s by landless people cruz morales 2014 since the settlement cattle raising together with maize and bean production for self supply and regional markets have formed part of the smallholders livelihoods cruz morales 2014 but livestock became even more important in the late 90s when maize prices plummeted in 1995 as a result of nafta garcía barrios et al 2009 garcía barrios and gonzález espinosa 2017 in the same year the federal government designated a buffer zone in the sbr currently the people grow mainly maize and beans for self supply for monetary income they raise livestock grow organic coffee and more recently extract resin from pinus oocarpa schiede ex schltdl of their total land area pine savanna is particularly important for production as cattle raising plus resin represents a significant share of their income in both communities pine savanna extends close to the forest frontier in one of the core protected areas of the sbr which consists of a highly biodiverse montane cloud forest ecosystem conanp 2013 in the pine savanna the most abundant exotic grass species are melinis minutiflora p beauv gordura grass and hyparrhenia rufa d a reid jaragua grass which were introduced to mexico in the late 19th century for livestock production parsons 1972 pinus oocarpa dominates the pine savanna tree stratum braasch et al 2017 2 2 extensive surveys with resin producers main topics and analyses the most relevant actors related to pine savanna management are the local families roughly 74 of them are involved the conservation authority conanp and the national corporation that buys the turpentine alen del norte secondary actors are the national forestry department conafor an environmental conservation and development ngo pronatura and public research institutes ecosur and universidad autónoma de chapingo the authors of this paper have sustained frequent interactions with all actors during the past three years both in the field and in organized meetings from these interactions it became apparent that within and among these groups of actors there are different and sometimes conflicting views on the level and importance of pine recruitment in the savanna and on the existence of ecological and economic tradeoffs associated with exotic grass management options with the aid of questionnaires maps photos drawings and field visits in 2016 we interviewed 52 local people involved in turpentine extraction men and women of different age groups from both communities to further clarify their activities land use interests expectations about resin extraction as a short or long term livelihood option knowledge on ecological factors affecting pine recruitment tradeoffs involved in each exotic grass control measure controlled fire weeding grazing and preferred control option surveys were analyzed with descriptive statistics and results were used to build true grasp and design farmer and multi actor workshops farmers were involved in the modelling process most resin producers participated in defining its general purpose fifty two discussed with researchers the relevant processes involved and provided empirical data which were later stylized by researchers as model parameters ten farmers tested the user interface and validated the model s qualitative behavior and outcomes in a pre workshop meeting all along they showed interest in research oriented towards exploring management options for recruitment the day they attended the single actor workshop farmers were invited to actively work all day and thus received six usd covering a local daily salary 2 3 the true grasp agent based model 2 3 1 virtual world and components in this section we provide a summary of how true grasp was designed and describe its components for more detail see the overview design concepts and details odd protocol in supplementary data appendix a where we followed the updated odd protocol by grimm et al 2010 and müller et al 2013 true grasp is an agent based model created with netlogo v 5 2 1 wilensky 1999 it allows users farmers and other actors to set management parameters run the simulation observe the trajectory of relevant variables and repeatedly reset the simulation and its parameters to stir specific trajectories towards desired ecological and productive outcomes with acceptable tradeoffs the entire landscape of the virtual world has an extension of 81 x 129 patches 10 449 cells which represents 4 ha of land divided into two equal parts pine savanna and open pasture representing typical landscapes in the sbr fig 2 the total size of the netlogo world was selected to contain a realistic initial population of 50 adult pine trees per ha of savanna while allowing space for all other user interface features buttons sliders switches plots and monitors each patch covered 3 8 square meters an arbitrary but convenient size to reconcile the different spatial scales of the modeled agents and their movements grass fire farmer cattle pine dispersion it is worth noting that these agents movements are highly stylized we deliberately avoided dubbing them with complex dispersion and search behaviors we chose the rabbit grass weed algorithm wilensky 2001 to produce a semi random walk for cattle the mushroom hunter model algorithm railsback and grimm 2012 for farmers searching pines and moore neighborhood colonization for grass and fire the general assumption in this stylized approach is that for the current purpose of the model the intensity and consequences of interactions among all these agents reasonably and sufficiently depend on their local and global densities albeit in nonlinear ways the pine savanna is a forest stand with an open canopy and initially contains 100 mature pine trees in two ha land the forest understory cells patches in netlogo language can be covered by the following agents representing different vegetation cover types 1 pine seedling 2 pine sapling 3 exotic tall grass cell that impedes seed germination and seedling establishment 4 short grass cell short statured native grass or recently burned forest floor or exotic grass kept short by cattle grazing with all these cell occupations allowing pine seedling establishment 5 pine needle litter or shade near adult pine trees not allowing seed germination or seedling establishment see supplementary data a fig 2 other agents are resin producer cow and fire the user embodies and assists the virtual resin producer in defining cattle load and the frequency of cattle rotation use of fire and manual weeding the most relevant outputs reported annually and cumulatively are the number of pine seedling saplings mature productive resin trees resin barrels and calves each iteration time step in the model represents one day a thirty year simulation with an intel core i5 processor takes between 5 and 10 min several decades 50 100 years of simulation can transform the pine savanna into an open pasture land if no recruitment takes place or into a closed pine forest without grass in the understory if tree recovery and growth is high intermediate states are also possible although half of the virtual world is open pasture land potential pine recruitment can also proceed there albeit at a low pace 2 3 1 1 pine lifecycle in the savanna each adult pine produces seeds every year they are randomly distributed within a radius of 15 cells a seed only germinates in short grass cells the daily seedling growth rate is reduced as a function of surrounding exotic tall grass cells within its moore neighborhood which slows down the process by which the sapling becomes a young tree and is free from this competitive effect seedlings and saplings adjacent to trees older than 10 years die by a self thinning process young trees can resist cattle trampling and fire once they reach the age of 3 and 9 years respectively thus a successfully established tree needs to have found space for germination and survive all risks fire trampling competition of exotic grasses and self thinning by other pines during its lifecycle with intermediate exotic grass competition an established new pine can be tapped for resin by the age of 25 years an average individual pine s resin production lasts between 10 and 20 years depending on tapping intensity in the real world a tree face is tapped for five years by moving the resin tapping face upwards to a maximum height of 2 5 m each year before a new tapping face is initiated at the other side of the tree tree diameter in the study area permits between 2 and 4 faces on a single tree see supplementary data a fig 7 by the age of 45 years an adult tree will have exhausted its resin production and thus can be felled for lumber otherwise it dies naturally at the age of around 140 years fig 3 we do not incorporate the probability of death due to bark beetles because they are not an appreciable factor in the area 2 3 1 2 short grass browsed or burnt cells they refer to all cells on the forest floor not occupied by adult pine trees pine needle litter or tall exotic grass which can be colonized by a pine seedling under appropriate environmental seasonal conditions 2 3 1 3 exotic tall grass cells preclude pine recruitment and provide fuel for fire and fodder for cows if in following iterations the exotic tall grass cell is covered by pine needle litter or shade grazed by cattle or burnt it becomes a short grass cell which again can become an exotic tall grass cell if tall grass later recovers 2 3 1 4 pine needle litter shade cells exotic tall grass and short grass cells within a five cell radius around an adult pine tree transit gradually into litter shade cells and do not allow seedling establishment only fire manual cleaning or litter decomposition transforms the cell back to a short grass cell full pine leaf litter decomposition occurs two years after an old tree has died naturally or has been felled allowing grass growth 2 3 1 5 cow it moves in a semi random walk within the assigned space pasture land savanna or both with or without rotation if a cow crosses over a susceptible seedling aged 3 years it tramples and kills the seedling the cow starts with an initial energy weight of 1000 units energy is lost each time step day due to movement and energy increases only with consumption of exotic tall grass cells if the cow encounters insufficient exotic tall grass cells its energy eventually falls to zero and the cow dies if availability of exotic tall grass cells is high and the cow reaches more than 1650 units reproductive weight it conserves 1000 units for itself and devotes the rest to produce a calf each cow is calibrated to produce no more than one calf per year see also section 2 3 3 calves do not consume grass as they are sold and thereby removed from the virtual world 2 3 1 6 resin producer five family members farmers move in the savanna searching for pines if a farmer encounters a resin producing tree he taps it and harvests the resin the farmer is initially endowed with 100 energy units which he spends walking harvested resin first compensates for this energy loss kg of resin converts into money which covers his labor costs and any resin surplus accumulates in the farmer s resin container if the energy level reaches zero the farmer quits being a resin producer if there is a surplus of resin in the farmer s resin container 400 units 40 kg the harvested resin is stored in 200 kg barrels for sale and thus leaves the system in this simulation the resin producer is always in the savanna and moves forward one cell per time step day in search of resinous trees although obviously unrealistic this stylized tree search dynamic is parameterized so that the average weekly harvest of this family in the 2 ha virtual pine stand resembles average yield per week in the study area if a farmer moves over an exotic tall grass cell his movement is delayed compared to a short grass cell free of obstacles and also spends more energy that decreases his net resin accumulation slightly 2 3 2 management practices to control exotic grasses and pine needle litter exotic grasses and pine needle litter both influence pine seedling establishment the model considers the following management practices to overcome this constraint the user can select among 2 3 2 1 manual weeding and cleaning the farmer in the model converts by cutting manually exotic tall grass cells and cleaning pine needle litter within a 4 cell radius of a resinous tree into short grass cells this accelerates the farmer s forward movement and opens space for seedling establishment but also reduces the farmer s energy level due to the invested labor which ultimately affects his net resin harvest 2 3 2 2 fire it is simulated following the simple fire percolation model from netlogo 5 2 1 model library wilensky 1997 fire always starts in the center cell of the virtual world and spreads with each time step from a burning cell to any of the eight surrounding cells moore neighborhood that contain fuel exotic tall grass cells or pine needle litter fire converts these cells to burnt short grass cells if fire reaches a cell covered by fuel and containing a susceptible pine tree the latter dies fire can occur spontaneously each year with low probability 4 it can also be used as a management practice by the user at any moment or with a fixed periodicity in the current version of true grasp fire is not required for pine seed germination 2 3 2 3 cattle stocking and rotation frequency the model user chooses a certain number of cows between zero and eight and decides whether they occupy only the pasture land the savanna or both in the second case pasture land and savanna cows can perambulate freely or rotate between fenced paddocks with a user defined frequency 2 3 3 model parameterization and calibration in appendix b we present a non exhaustive but comprehensive multivariate sensitivity analysis of true grasp responses to a relevant set of parameters here we highlight some important aspects of the model s rationale true grasp stylizes ecological and economic processes and produces outputs that do not mimic the exact quantities to be expected in real world situations and which are still largely unknown and highly variable it is not predictive in that sense but its time series reproduce fairly well the short and mid term qualitative system behavior described by local and external actors in response to their proposed management practices in the pine savanna moreover it produces reasonably well long term scenarios that seem plausible to users a central purpose of the model is to allow users to explore individual and combined effects and tradeoffs of different options fire weeding and cattle grazing of controlling exotic grasses in the pine savanna cattle grazing is currently the most contentious option and the best studied in the area so it is better specified in the model than the other options actual grazing strategies in these and surrounding communities cattle stocks and rotation rates are context dependent and therefore highly variable in the sbr smallholder farmers cattle herds are composed of 5 20 animals and are rotated adaptively in their land between open pasture savanna or both their combination farmer s rule of thumb for an annual average stock which allows the production of 0 8 1 0 calf per cow per year and that does not produce a steady long term decline of grass cover in open pasture is one cow per hectare of open pasture however detailed surveys and analysis prior to this study rosabal ayan 2015 valdivieso pérez et al 2012 and in garcía barrios et al unpublished database showed that for rangelands combining open pastures and savannas an appropriate stocking density is 0 5 cow per ha therefore we selected and coupled parameters for cow reproduction and exotic grass recovery rate so that a a cow would produce 1 0 calf per year in 2 ha of open pasture and 0 8 calf in the combination of 1 ha of savanna and 1 ha of open pasture and b a cow browsing 2 ha of open pasture would keep grass cover at an equilibrium value near 80 thus more than 1 cow per 2 ha increases the percentage of short grass cells for pine recruitment at the expense of calf production while a lower cattle load could significantly reduce space for pine recruitment with little or no gain in calf production fig 4 a c shows these variables trajectories and sensitivity analyses for different cattle stocks appendix b provides further details and sensitivity analyses for a broader set of outputs fire was modeled rather crudely using a simple percolation model wilensky 1997 which was parameterized such that pine recruits are sensitive to burning during the first nine years of establishment thereby allowing reasonable recruitment under a low fire frequency regime weeding was parameterized to reflect the fact that it ceases to be cost effective when used too frequently or as the sole grass management practice 2 4 single and multi actor workshops 2 4 1 preliminary surveys before starting each of three single actor workshops with smallholder farmers of california tres picos and external actors participants were asked to a determine knowledge on factors affecting pine recruitment and potential tradeoffs associated with preferred management practice to control exotic tall grass cover and b explain the type and consequences of emerging interactions direct or inverse with the help of a resin production system diagram to make aware of tradeoffs when including e g cows or fire and identify the preferred effects of these components 2 4 2 single actor workshops three 4 h workshops were held separately with 10 participants from california 7 participants from tres picos and 5 external actors respectively on march 27 to 29 2017 two participants and one trained facilitator from the researcher students team sat at each computer workshops included the following sessions 1 welcome ice breaking dynamics and sharing workshop purpose 2 presentation of an illustrated talk leading participants from real images of the savanna landscape with its vegetation livestock and human components to their in silico representation in a netlogo world this was accompanied by brief simulations of the behavior of each netlogo agent and patch 3 demonstration of a 15 and 30 years simulation carried out by the first author showed how a pine and native grass savanna without cows allow pine recruitment and natural development towards a closed pine forest with its accumulated saplings productive trees and resin production 4 based on the same initial spatial state a 15 and 30 years simulation led by the first author and executed by participants with the help of facilitators showed the consequences on pine recruitment when substituting native grass with exotic grass considering the previous simulation setting see 3 each participant was asked to write down his predictions on how outputs would change qualitatively increase remain decrease and then compare them with the actual simulation outcomes 5 eight 15 years simulation scenarios led by the first author and executed by participants and facilitators showed both the consequences on recruitment and other outcomes of a burning the savanna b weeding tall grass and c allowing cattle grazing in the whole grassland savanna area scenario 1 starts with a full ground cover of native grasses leading to a closed pine stand with abundant recruits and no exotic grass see 3 scenario 2 substitutes native for exotic grass and it is headed in the long run 50 yr to an open grassland see 4 scenarios 3 4 and 5 add to scenario 2 a fire event every year in years 1 4 8 12 and in years 1 and 12 respectively only the last of these fire regimes creates a window for saplings to escape size related vulnerability to fire and therefore increases recruitment scenarios 6 and 7 consider scenario 2 but include manual weeding around pine trees every year and in years 1 and 15 respectively scenario 8 also starts with scenario 2 but with 6 cows grazing freely the open pasture and pine savanna the whole netlogo world 6 in both farmer and external actors workshops during a 30 min period pairs of participants were asked to select a set of parameters available on the user interface to achieve five non trivial output goals simultaneously pre established by researchers by year ten these ecological economical goals were 40 recruits 20 resin barrels 18 calves less than 25 trampled saplings and more than 50 tall grass cover this goal oriented approach was chosen to increase participant s familiarity with the interface and for them to experience the many potential interconnected tradeoffs involved 7 the two brothers exercise seven pairs of smallholder farmers were teamed up such that in real life one team member has pines and cattle while the other one has only pines they were told suppose you are two young brothers a and b whose father wishes to inherit a small herd of cattle and 4 ha of land two as pine savanna and two as open pasture your father says i am inheriting the whole property to both of you a will own any present and future pine tree on the whole property and b any present and future cattle and grass it is up to you how you will manage the whole property together to make a livelihood by year ten a must meet 18 resin barrels 45 recruits trampling less than 30 and a non negative weeding subsidy b must meet 20 calves and grass cover not less than 50 reaching both sets of goals was possible but non trivial and the process could drift towards one participant s interests at the expense of the other s participants had 30 min per team to select and explore parameter sets and run simulations to reach together their respective goals 8 a 30 min collective reflection on the workshop experiences concluded the single actor workshop 2 4 3 multi actor joint workshop participants were six smallholder farmers from the ejidos california and tres picos 3 representatives from each village three conanp officers two representatives from the ngo pronatura and the regional officer of alen del norte all 12 participants had been acquainted previously as neighbors and or partners or observers of the resin business after a brief reminder presentation of the model operation each farmer was paired with an external actor to explore and reach the following three goals in a single attempt with one fifteen year simulation optimize sapling number resin and calve production the five teams were free to define their own sets of management parameters for this one shot experiment except for the number of cattle which was fixed to five by researchers an excessive and suboptimal stock would make participants confront stronger tradeoffs among outputs the exercise was presented as a contest to see which team would achieve the best result immediately after the simulation contest and reflection over the outcomes we conducted a collective and public exercise for all actors to explore hypothetical pine savanna management choices along a decision tree where questions were revealed to them step by step binomial decisions were is resin production a long term project 50 years should saplings be recruited naturally or nursed and planted should the main exotic grass control strategy be based on cattle stock management or grass weeding and scorching are such interventions collective or private decisions and endeavors should they be subsidized 3 results 3 1 farmer interviews to explore local ecological knowledge the majority of interviewees were males older than 40 active in resin extraction and with four to five primary activities resin maize beans coffee and livestock fifty three percent of resin producers in california and tres picos owned livestock at that time most considered that the livelihood importance of resin and coffee would grow while maize and livestock would remain stationary older farmers were well aware of how the current landscape came about through selective logging and land clearing for crop and cattle production for more detail see braasch et al 2017 fifty percent envisioned local landscapes in the next ten to twenty years to consist of a semi closed pine oak forest combining resin cattle and firewood production 23 chose a closed pine forest dominated by pinus oocarpa to increase resin production and 19 decided for a mixture of several land use types pasture land open pine oak forest and closed pine oak forest but separated in space a few also included oak forest 6 for firewood production regarding strategies by smallholder farmers to maintain pine stands all interviewees said do not cut pine trees two thirds do not burn only two fifths recruit saplings and only one in twenty reforestation with nursery pine trees eighty percent considered that natural pine recruitment was appropriate at the whole ejido level while 58 considered it was reasonably high in their own pine stands only half of the interviewees were aware that p oocarpa trees need to reach 25 40 years of age before they produce resin in this area considering pine recruitment thirty five percent of the interviewees considered it requires bare soil while 19 mentioned grazing and very few included post fire conditions such as fertile soil and a seed shedding pine tree nearby more than one third did not have an answer to the question on regeneration niche when asked about obstacles to recruitment fig 5 a two thirds considered ungrazed exotic grass and pine leaf litter accumulation while only one fifth included also trampling of saplings by cows regarding cattle effects on forest floor fig 5b almost all interviewees mentioned grazing lowers fire risk and grass competition for saplings three fourths cattle browse and trample one third trampling prepares the soil for pine germination 8 cattle causes soil erosion and 13 no effect overall responses showed that many farmers have a broad and precise knowledge of landscape history resin production requirements pine life cycle and regeneration niche factors affecting pine recruitment and tradeoffs of cattle grazing in pine savannas for sapling establishment and growth when considering all interviewees knowledge and opinions were diverse incomplete and in some cases contradictory e g some farmers mentioned the positive effect of fire fire is needed to stimulate tree recruitment but at the same time they said that they do not use it because it is bad as it causes wildfires contrasting opinions occur most likely due to differences in age activities and livelihood related preferences and opportunities the rich yet incomplete ecological knowledge of smallholder farmers and the diversity of their interests regarding savanna management were useful both to guide the design and parameterization of the abm and to further value the pertinence of facilitating farmer workshops on this matter 3 2 agent based model capabilities true grasp proved capable to qualitatively reproduce three different long term scenarios of interest to the smallholder farmers for resin extraction and calf production these were table 1 a a baseline scenario previous to exotic grass invasion represented by closed pine stands with native grass pine needle litter understory and abundant pine recruitment b open exotic grassland as a consequence of lack of pine recruitment and c exotic grass cover with moderate cattle load two cows in four ha with both high recruitment and high calf production a win win situation resin producers and other actors considered each of these scenarios relevant and graphically fig 6 and conceptually credible each scenario proved to be robust under a range of ecological and management conditions and sensitive to threshold values of a single or several parameters potentially causing regime shifts between some of these scenarios 3 3 abm supported scenario exploration workshops seventeen persons in total participated in two farmer workshops in 2017 with the same proportion of farmers owning cattle 53 as in the 52 interviews held in 2016 five persons participated in an external actor workshop and 12 in a multi actor workshop 3 3 1 pre agent based model surveys and tests pre abm interviews and exercises revealed that a all actors got high scores when identifying the sign of direct interactions among pine savanna silvopastoral components in fig 7 yet positive effects of cattle and fire on pine recruitment were more frequently missed the former more by external agents and the latter more by farmers in consequence the same trend was observed when actors identified tradeoffs associated with cattle presence and with fire use b after the fire and cattle tradeoffs were identified by actors or pointed out by facilitators 60 of farmers and only 20 of externals saw more benefit than damage in cattle regarding controlled fire tradeoffs the opposite occurred all externals and only one elder farmer an ejido founder saw more benefits and would apply fire to control exotic grasses 3 3 2 smallholder farmers and externals understanding and validation of the agent based model fig 8 a shows the abm outcomes for recruits under eight mid term scenarios 15 years presented to participants for them to explore on the computer see also supplementary data appendix b figs 3 and 4 the figure shows that recruitment is very high under scenario 1 and collapses in 2 compared to scenario 2 recruitment increases slightly with a 12 year fire regime but increases more with yearly weeding instead of fire albeit with very high labor costs when using cattle instead of weeding recruitment is only slightly lower but produces additionally around 30 calves participants were presented these eight scenarios sequentially and were asked to predict prior to each simulation if recruitment would be higher or lower the simulation of the native grass cover was compared with the initial condition exotic grass cover simulation was compared with the output of the native grass scenario while the predictions for fire weeding and cattle management were compared to the exotic grass cover scenario fig 8b shows almost all farmers predicted qualitative outcomes correctly in each case and only a few underestimated the damage caused by yearly or four year interval fires in the course of these simulations participants started to become aware of other outputs as well resin and calf production grass cover and the nonlinearities and tradeoffs associated with the modeled situations through these predictive exercises farmers a learned the user interface and got used to interact with it b developed confidence in the tool to later explore qualitatively ranges and combinations of management options and their tradeoffs and c learned to understand the importance of discussing mid and long term effects of management rather than short sighted snapshots of immediate effects between the simulation of scenario 2 exotic grass dominates and the simulations with fire weeding and grazing scenarios participants were asked to list their options to deal with the low recruitment associated with unmanaged exotic grass around 50 of the farmers mentioned weeding and planting saplings 24 excluding cattle from pine stands and 25 using cattle grazing to control grass very few mentioned controlled fire fig 9 in contrast all external actors mentioned fire as an option and cattle grazing only as the second choice weeding and planting saplings were no real options for externals because of high labor costs 3 3 3 smallholder farmers and externals management parameter explorations in search for pre established ecological and economic goals in the first goal oriented exercise 33 of farmer teams met three goals 44 met four and only 22 met all external actors did better 25 met four and the rest met five goals both groups success frequency was very similar for recruits barrels and calves the difference lied in farmer s lower success due to trampling and very low grass cover because on average they stocked more cattle per land unit examples of selected management strategies and their multivariable output are presented for farmers fig 10 a and externals fig 10b in both cases the most successful model outputs combined moderate weeding with the rotation of medium 2 4 animals ha cattle loads all available management practices affect exotic grass cover directly and this eventually affects indirectly all simulation outputs where cattle were included the teams had to figure out how to deal with the direct and indirect effects of grazing in order to strike a balance between the positive and negative effects of cows on recruitment and to reach the pre defined calf and recruit scores fig 11 shows examples of different search strategies more generally fig 12 a d shows as gray clouds the relations and nonlinear tradeoffs between some of the model s output sets produced by 3240 parameter combinations available to participants for fire frequency 0 1 2 3 5 and 10 burnings weeding frequency 0 2 3 5 and 10 weedings cattle loads 0 8 cows and rotation frequency 0 1 2 3 4 6 and 12 per year in these ten years scenarios reduction in the exotic grass is associated to broad and nonlinear sets of responses for both recruits and calves fig 12a and b produced by a myriad of ways how management options can be combined this in turn defines a relation between recruit and calf sets that on average turns from synergistic to antagonistic fig 12c superimposed on these clouds are black dots representing the actual x y ordered pairs of outputs achieved by participants in all their attempts to reach the established goals the latter outputs show that most participants did not explore parameters in a veil of ignorance and at random but found their way in this multivariate and nonlinear search space towards the model s win win scenarios for cattle production pine stand persistence and long term resin production both regressions in each graph are showing the same trends for the regression between resin barrels and exotic grass cover fig 12d r 2 value polynomial regression is low because in ten years there is yet no correlation between this set and the effect of recruitment on barrels is yet to come see also supplementary data appendix b figs 5 and 6 3 3 4 the two brothers exercise in the two brothers exercise two out of seven teams met both brothers goals calf vs resin recruit productions they arrived at proper cattle loads combined with rotation and moderate fire or manual weeding but not both which rendered productive and cost effective levels of exotic grass the other five teams also arrived at 3 or 4 cows but did not meet all goals three penalized their income from resins due to high weeding costs two penalized their calf production by excessively reducing grass availability by weeding and burning in the one actor exercise team members had only common goals and therefore clear reasons to collaborate and deal with tradeoffs together in this brothers exercise they had individual goals and there was room for transforming tradeoff management into conflict and dominance of one brother s goals and interest over the other yet search spaces of the two brother exercise did not differ with the first exercise nor compared to the regression trend of the sensitivity analysis fig 13 we observed collaboration to try and meet both participant s goals although again few teams actually met them 3 3 5 reflection meetings both groups of farmers considered the simulation exercises increased their awareness of the long term effects of any current management strategy on resin production and forest cover in the pine savanna and on the consequences of not paying sufficient attention to recruitment they also said they became more clearly aware of the cattle tradeoff and the need to handle cattle load properly one farmer summarized it as i am used to putting any number of cows in my pine stand without much thought and take them out when the grass becomes too short but with no consideration for pine saplings now i know managing stocking size can make a difference farmers did not express concern for the fact that most teams did not meet all goals set by researchers in either of the free exercises rather they highlighted the many tradeoffs involved in such challenging multi goal searches interestingly some farmers valued specifically true grasp as a tool with which they could experience the connectedness of many inputs and outputs through their joint responses some also mentioned that they could see very clearly what were the preferences of their team mates and other participants when faced with management choices and output tradeoffs many farmers stated that the abm sufficiently captures what goes on in the pine savanna and that trying to meet goals really made them get involved that they had fun and paid attention to the behavior of the many factors involved and therefore learned much more than by sleeping over a long tedious power point external actors valued tradeoff analysis but some were concerned with the model not being sufficiently realistic e g not having slope effects soil erosion nor being quantitatively predictive e g exactly how many saplings would be produced in real life and management cost effectiveness 3 3 6 multi actor joint abm exploration during the last workshop all multi actor teams used a suite of different options with different frequency rather than focusing on a single one fig 14 interestingly when trying to define a winning team all concluded that it was not possible nor reasonable as some were ahead in some variables while behind in others an experience which made the concept of tradeoff even clearer to participants recruitment wise a winning team did stand out the farmer said about their scores these resin tanks are our present these recruits are our future and these few calves are the tradeoff for taking the future into account 3 3 7 multi actor meeting to discuss decision tree immediately after the simulation contest we conducted an exercise for actors to make hypothetical management choices along a decision tree revealed to them step by step fig 15 all chose resin production as a long term project 50 years the most consolidated production partners ejido california and alen co preferred tree stand regeneration with pine nurseries and sapling planting something they sustained since the pre simulation interviews fig 9 while all others preferred assisted natural recruitment both ejidos preferred exotic grass control around sapling by cattle and suppress fire while the ngo and alen co preferred weeding and controlled ground burning some but not all participants changed their points of view along the process and particularly after the collective exercise conanp had strongly advocated concentrating cattle in intensive land use areas and keeping them out of the savanna it now accepts it is sound and less costly to control exotic grass with a combination of cattle grazing and controlled fire and occasionally other methods the alen officer initially stated that cattle were the cause of exotic grass invasion during the simulations he acknowledged the capacity of cattle to control these grasses and favor recruitment during the decision making exercise he again dismissed cattle presence in the pine savanna alen co considered that proper management of pine stands and associated monetary costs to promote new trees should be each farmer s endeavor the ejidos preferred to request government subsidies for such practices through a resin producer organization while conanp and the ngo did not favor subsidy requests 4 discussion and conclusion peasant populations established at tropical and subtropical forest frontiers have secularly developed silvo pastoral practices livelihoods and landscapes in their territories garcía barrios and gonzález espinosa 2017 koning 2014 sloan 2007 van vliet et al 2012 walker et al 2002 many of their multiple use forested areas have recently been claimed by other actors and declared mabr the few new opportunities and the many constraints to silvopastoral use and management dictated by external actors have frequently led to all types and levels of conflict and lack of success for all parties bernard et al 2014 bouamrane et al 2016 cortina villar et al 2012 ma et al 2009 martín lópez et al 2011 in consequence actors in some cases have slowly acknowledged the need to engage in collective learning and deliberation to better understand and negotiate their interests socio ecological researchers have shown interest in these processes and are active in helping to develop and deploy strategies methods and tools to support learning negotiation and decision making berthet et al 2016 etienne et al 2011 kok 2009 mathevet et al 2011 tenza et al 2017 villamor et al 2014 voinov and bousquet 2010 wittmer et al 2006 these multi actor efforts are riddled with theoretical and practical challenges related to the different values knowledge frameworks interests and power relations of those involved galafassi et al 2017 huntington 2000 tenza et al 2017 a number of participatory decision making frameworks have been developed to deal with these challenges for a review see lynam et al 2007 additionally land under silvopastoral use exhibits complex non linear social and ecological interactions that confer both obvious and subtle tradeoffs which not only have short term consequences but can lead to long term undesired shifts in vegetation regime and to local production and livelihood collapse allen and gunderson 2011 carpenter and gunderson 2001 filatova et al 2016 filatova and polhill 2012 abms and rpgs are instruments well suited to capture and explore in stylized and dynamical form these complex silvopastoral behaviors an 2012 becu et al 2008 bousquet et al 2002 etienne 2014 filatova et al 2016 2013 parker et al 2003 villamor and van noordwijk 2011 voinov and bousquet 2010 a small but growing suite of rangeland and silvopastoral tools have been developed sylvopast etienne 2003 sierra springs garcía barrios et al 2011 2015 abm rpg grazing tool for herders and foresters dumrongrojwatthana et al 2011 rangeland rummy farrie et al 2014 sequia basalto bommel et al 2014 grazing game villamor and badmos 2015 forage rummy martin 2015 and kulayijana perrotton et al 2017 they differ in their specific purposes complexity precision realism and actors involvement in the various stages of development creating an abm supported rpg that represents the dynamics of silvopastoral land subject to a suite of management options and actors interests is in itself a very elaborate process that involves dealing with many design tradeoffs and difficult choices regarding realism precision and generality we and others have previously found that smallholder farmers in some but not all senses are initially in disadvantage relative to other actors when learning using and interpreting these complex tools but that the gap can be closed garcia barrios et al 2017 thus we consider that design decisions should be led by making sure that smallholder farmers can engage trust the qualitative outcomes enjoy the virtual immersion in complex behaviors and contribute to the collective learning experience galafassi et al 2017 garcia barrios et al 2017 2015 2011 le page and perrotton 2017 perrotton et al 2017 the tool described in this paper allows exploring management options and assessing their consequences in the short 10 15 years and mid 16 50 years term each combination of options is investigated independently the tool is therefore currently suitable to explore individual management options at the farm level or to represent landscape effects assuming that a collective agreement for centralized management exists yet the abm would need to be further developed in order to capture more complex social behaviors where defection and free riding may exist and where individual agents are making concurrent decisions accounting for what the others may decide at the same time the consequences on the landscape could differ significantly from centralized management such a model which could be built in netlogo s hubnet platform would offer a group of 5 6 participants the possibility to re adjust their individual management options at each step during a simulation run this would allow going further than the two brothers experiment into co managing renewable resources in short there is ample space for further developing the model we will now briefly discuss the most relevant findings derived from pre modelling interviews model building pre workshop quizzes interactive simulation workshops and group deliberations 4 1 pre modelling interviews most smallholder farmers stated that a resin as an income option will grow in the future and that they are willing to conserve the pine oak savanna b sapling establishment requires bare soil and is reduced by dense exotic grass cover pine leaf litter accumulation and sapling trampling by cows c grazing is one of several possible means for pine recruitment very few mentioned fire management to burn the grass this recent fire taboo or silence stems from the previous conflict with conanp over traditional fire use in the area braasch et al 2017 guevara hernández et al 2013 navarro et al 2017 smallholder farmers strongly depend on their ecological knowledge but also incorporate information from external actors soto pinto et al 2007 valencia et al 2015 there is no unique truth processes change continuously and unpredictably and therefore knowledge is never complete participatory approaches can help navigate such complexity by combining farmer and academic knowledge and developing together new insights allen and gunderson 2011 dawoe et al 2012 garcia barrios et al 2017 vandermeer and perfecto 2013 4 2 model development and validity structuring and parameterizing true grasp based on quantitative and qualitative knowledge provided by farmers field research and literature was successful this simulation framework exhibits sufficient internal validity because as shown by a thorough sensitivity analysis a it reproduces qualitatively the different long term scenarios of interest for pine recruitment closed pine forest open grassland and pine savanna b it produces qualitative scenarios that are robust under various combinations of management practices cattle rotation weeding fire but may shift when ecological thresholds are crossed c it exhibits interesting and credible nonlinear responses to each practice s range of possible values as well as credible nonlinear tradeoffs among practices and among desired outputs d it clearly captures the fact that similar output syndromes sets of relevant output values can be achieved with different management combinations as reported by an 2012 abm are increasingly used to simulate the complexity of ses because they can deal with heterogeneity featuring feedbacks nonlinearities and adaptation they provide several advantages over other models when dealing with land cover change regime shifts and tradeoffs filatova et al 2016 miyasaka et al 2017 parker et al 2003 4 3 pre simulation quizzes true grasp directly exposes users to nonlinear non trivial tradeoffs when using different management practices such tradeoffs became apparent to all actors only after having solved the quizzes smallholder farmers were more interested in managing cattle tradeoffs while external actors in managing fire tradeoffs again manifesting their respective biases and preference taboos it is ironic considering that a few years ago smallholder farmers used fire liberally and externals prohibited it as described in galafassi et al 2017 these outputs can be explained in three ways a tradeoffs are often invisible because of a lack of systemic understanding b tradeoffs are perceived differently by different actors different people see gains and losses differently and c tradeoffs are often hidden or ignored when taboos are involved daw et al 2015 schoemaker and tetlock 2012 4 4 learning trusting and using true grasp all actors found the long term qualitative forest cover scenarios relevant and credible appreciated this long term approach to management choices and increasingly trusted the tool for qualitatively exploring ranges and combinations of management options and their tradeoffs most users correctly predicted the qualitative forest cover scenarios discovered and dealt with tradeoffs and found their way towards the goals for cattle production pine stand persistence and resin production pre established by researchers as training exercises as mentioned earlier true grasp behaviors and outputs are only qualitatively predictive sun et al 2016 called these kinds of qualitative models simple abms and le page and perrotton 2017 call them stylized referring to the model structure as compared to quantitative data hungry prediction models generic stylized qualitative models are highly recommended for participative approaches to foster deliberation and decision making edmonds and moss 2004 le page and perrotton 2017 sun et al 2016 tenza et al 2017 voinov and bousquet 2010 especially when dealing with tradeoffs hard choices between ecological and social benefits individual and community benefits and among actors who bear different costs and benefits lazos chavero et al 2016 interestingly the reserve management team expected an abm with more realism and prediction of operational quantities while farmers expressed they were content with experiencing and becoming more aware of interactions tradeoffs and indirect effects thus the tension between favoring generality realism and precision levins 1966 and between building theoretical stylized or realistic abms le page and perrotton 2017 is probably unavoidable in multi actor settings during these exercises team members with common goals had clear reasons to collaborate in the two brothers exercise performed by farmers there was room for transforming tradeoff management into conflict and dominance of one brother s goals and interests over the other yet this did not occur this has at least two explanations or their combination users would actually collaborate in real life in an effort to balance the tradeoffs in a fair way as found by garcía barrios et al 2015 where users enjoyed exploring the possibility of such collaboration in a safe environment with no significant cost in their real life relations as found by berthet et al 2016 4 5 user s evaluation of true grasp most actors said that the exercises had increased their awareness of a the long term resin production and forest cover effects b the consequences of low pine recruitment c the usefulness of cattle and or fire to increase pine recruitment and d the many tradeoffs involved they said that true grasp really made them get involved in dealing with the relevant issues have fun and pay attention to the behavior of the many factors involved and their interactions the capacity of the true grasp workshops to produce collective socio ecological learning is in line with other abm and rpg rural workshops berthet et al 2016 garcia barrios et al 2017 2015 2011 patel et al 2007 perrotton et al 2017 speelman et al 2014a 2014b villamor et al 2014 villamor and van noordwijk 2011 4 6 multi actor exercises and deliberation exploration and negotiation among actors ran smoothly and collaboratively although subtle dominance by external actors was frequently expressed by hoarding the computer mouse and output sheet these actors have been interacting and negotiating different issues for the past 20 years and the exercise reflected in a playful way both collaboration and unspoken conflict of their past relationships one principle of rpg described by the commod group but also by other authors etienne 2014 lynam et al 2007 villamor and van noordwijk 2011 is the capacity to cross boundaries among actors belonging to different worlds while being interested in the same resource and to promote a dialog in a fair and balanced multi actor space fostering the dialog in mabr is essential because actors would be able to present discuss and better understand one another s perspectives and needs bouamrane et al 2016 however it is also important to provide a space for social and collective learning among actors to solve problems conflicts and to negotiate agreements patel et al 2007 while in the true grasp exercise participants were more open to combine contrasting management practices during the decision tree exercise they privileged their previously expressed real life preferences while stating they remained open to further discussion this is not surprising as abm rpg workshops should not be expected to produce effects totally aligned with the model s stylized propositions nor to do so immediately and in a one shot experience yet we are certain that the simulation exercise leveled the ground for a more honest discussion and for understanding other actors statements and choices during this last exercise as mentioned by bodin 2017 participatory approaches are sometimes unable to deliver immediate and expected concrete results or create the illusion of results in the form of symbolic outcomes such as aggregated wish lists where conflicts of interest are left untouched the creation of a socially and ecologically sustainable management plan implies a fair and balanced designed arena for productive discussions and negotiations between the smallholder farmers and external actors etienne 2014 perrotton et al 2017 however this calls for the commitment of researchers and all actors to social learning that truly involves smallholder farmers and provides tools that do not overwhelm them galafassi et al 2017 garcia barrios et al 2017 software availability the agent based model true grasp is available online at comses net openabm following this link https www comses net codebases 929f4083 af57 45bd a984 88292ac71be6 releases 1 0 0 true grasp was developed in netlogo language program version 5 2 1 developers of true grasp are marco braasch marcobraasch gmail com and luis garcia barrios luis garciabarrios gmail com acknowledgements we thank the ejido members of california and tres picos in the la sepultura biosphere reserve and all external actors conanp pronatura alen de norte for their participation in the workshop and the biosphere staff in california for supporting research logistics this work was supported by ecosur s m t family agriculture project 11066 and the rufford foundation 17207 1 mb thanks conacyt for a phd scholarship 375409 250287 we gratefully acknowledge yanus dechnik vásquez alan heinze yothers and alejandra hernández guzmán who participated as facilitators during the workshop and helped in logistics we thank the three anonymous reviewers for their comments which greatly improved the clarity of the text appendix a supplementary data the following are the supplementary data related to this article appendix a appendix a appendix b appendix b appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 022 
26384,win win solutions might be short lived government permission for smallholder farmers to extract and sell resin from a pine savanna biosphere reserve in mexico has settled a long dispute among different stakeholders in the short term however forest production and conservation beyond 20 years are compromised due to low pine recruitment caused by competition with exotic grasses grass control practiced by farmers through grazing and fire has previously been discouraged by conservation authorities which inadvertently limits long term pine conservation and use we describe the participatory design rationale and simulation attributes of an educational interactive agent based model that explores suites of management options and their economic and ecological outputs we present and analyze the outcomes of four simulation workshops where farmers and external actors better grasped the complex ecological interactions involved in conserving and using pines in grazed pine savanna with exotic grasses and discussed these findings with a long term vision and tradeoff analysis approach keywords agent based model cattle in forests decision making forest management tradeoffs participatory modelling smallholder farmers software availability 1 introduction the man and the biosphere reserve mabr program is a worldwide program formally implemented in the 1970s as a space for accomplishing both paradigmatic rural development and protection of nature unesco 1996 in mexico this program incorporated many territories occupied by smallholder farmers within a new model of conservation by decree to improve the social and economic well being of populations it was supposed to promote economic social and environmental policies to allow families long established in these territories to sustain decent livelihoods by creating or supporting agroforestry or silvopastoral landscapes in buffer zones of mabr serving as high quality matrices for conservation bouamrane et al 2016 cruz morales 2014 martín lópez et al 2011 achieving this goal has been at best problematic from the very beginning di castri 1976 reasons range from stark conflict among actors related to possession or control over land to poor understanding and agreement over the effects of land management strategies on ecosystems and smallholder farmers livelihoods adams 2004 cruz morales 2014 garcía barrios and gonzález espinosa 2017 ma et al 2009 wittmer et al 2006 in this context win win land management solutions in mabrs are desirable but unusual when pathways are reasonably accepted by most or all parties they may represent progress in some dimensions plummer et al 2017 but will likely generate new issues and tradeoffs elsewhere something to be expected in any complex social ecological system agrawal and ostrom 2001 defries et al 2007 martín lópez et al 2011 more importantly some emerging issues might work directly against the previously agreed upon solution yet this might not be easily perceived or detected because their consequences are mid or long term allen and gunderson 2011 these emerging issues commonly become invisible get ignored or postponed for better times swept under the carpet by resource stricken actors accustomed to jointly muddle through so called wicked problems allen et al 2011 sierra huelsz et al 2017 this sometimes unavoidable mishap may have dire consequences when actors are dealing with changes in land cover land use and or livelihoods close to tipping points as the situation becomes extremely sensitive to miscalculations unconsidered indirect interactions and short term pragmatism carpenter and gunderson 2001 huber sannwald et al 2012 ribeiro palacios et al 2013 in la sepultura mabr chiapas mexico created in 1995 anthropogenic pine savannas surround highly valued montane forest core zones as part of the buffer zone conanp 2013 prior to that year modest pine lumber extraction and extensive cattle grazing were part of people s livelihood and intentional or accidental burning of the savanna understory was common guevara hernández et al 2013 navarro et al 2017 then as a conservation strategy the national commission of natural protected areas conanp according to its spanish acronym prohibited fire use tree extraction and livestock production in the pine savannas however these top down decisions affected smallholder farmers livelihoods and ignited a decade long conflict between communities and conanp cruz morales 2014 farmers saw no reason to protect pine trees on their land other than to avoid monetary sanctions or jail because of illegal extraction guevara hernández et al 2013 in 2012 all parties interests finally converged in a joint project to extract turpentine resin from adult pine trees to be sold to the alen del norte corporation under these new perspectives the imposed land management strategies now made more sense to smallholder farmers at least for a 20 year time span ahead during which current adult pine trees would yield a marketable oily product yet a hidden contradiction remained in most pine stands small native grasses and herbs have long been outcompeted by tall exotic grasses in the absence of fire and grazing they can be a significant obstacle for recruiting future generations of productive adult pine trees braasch et al 2017 by targeting this attractive short term win win solution i e protecting the pines and extracting resin actors in this partnership are paying little attention to the long term effects or do not reach consensus over strategies to deal with them insights from smallholder farmers and plant ecology suggest livestock grazing could create opportunities for pine recruitment but may also cause trampling damage or mortality of saplings archer et al 2017 braasch et al 2017 van langevelde et al 2003 werner 2005 interactive agent based simulation models abm and socioecological board games have emerged within different social learning frameworks e g the companion modelling approach methodology commod etienne 2014 has useful participatory education tools such as role playing games rpg that facilitate communication and reflection among those involved in resource management and promote a common knowledge ground from where to build effective management and governance le page et al 2010 etienne et al 2011 garcia barrios et al 2017 2015 2011 some of these frameworks and tools have allowed smallholder farmers and other actors to simulate and jointly explore land use and management options in rural small holder territories barnaud et al 2013 berthet et al 2016 etienne 2014 villamor and van noordwijk 2011 and more specifically in those contiguous or within mabr bouamrane et al 2016 perrotton et al 2017 abms are currently used in many fields of scientific research education and policy making as extremely powerful tools to better grasp complex processes an ever growing model library is currently available rollins et al 2014 many social educational and technical challenges associated with abm remain spanning from their proper development to their use as multi actor social learning tools within rural settings barnaud et al 2013 becu et al 2008 garcia barrios et al 2017 le page and perrotton 2017 fruitfully discuss the different objectives and tradeoffs involved in abstract stylized and realistic agent based models and stress that the requirements and purposes of social learning should guide the choice construction and use of these simulation models in multi actor land stewardship processes since 2007 the second author lgb has been leading participatory multidisciplinary research in la sepultura mabr with special emphasis on the social and ecological consequences of land use innovations meant to reconcile livelihoods and conservation garcía barrios and gonzález espinosa 2017 valencia et al 2015 2014 zabala et al 2017 in this process a number of agent based models speelman et al 2014a b speelman and garcía barrios 2010 and socio ecological board games garcia barrios et al 2009 2015 2011 have been developed for this study starting in 2014 we were welcomed by actors to follow and support the resin production project and we engaged in participatory research comprising field transects forest inventories ecological experiments farmer surveys agent based modelling and scenario simulation workshops to address the following questions do actors consider the pine savanna and resin turpentine extraction a short or long term livelihood and conservation option is pine recruitment actually lower where exotic grass is dominant is low pine recruitment a critical issue for the resin project and for which actors what management options for controlling exotic grass are preferred by different types of actors what are potential short and long term tradeoffs of these management options we described in detail the anthropogenic origin of the pine savanna braasch et al 2017 we showed that current pine population structure and low recruitment due to dense exotic grass cover cannot support long term resin production furthermore we presented experimental evidence that cattle grazing in the savanna may have both positive and negative effects on recruitment here we describe and discuss the development and use of an interactive agent based model with farmers and other actors to help address the above questions the objectives of this paper are to a describe the interactive stylized agent based model true grasp tree recruitment under exotic grass in the pine savanna and its background rationale and main attributes b present and discuss the outcomes of four true grasp simulation workshops held separately and jointly with smallholder farmers and external actors to address in a stylized qualitative way the questions listed above and to support social learning of all parties involved including ourselves 2 methods 2 1 study area the study was carried out in the pine savanna under ejido tenure a mixture of private and communal land belonging to the rural towns of california and tres picos located in the buffer zone of la sepultura biosphere reserve sbr in chiapas mexico 16 16 40 16 12 40 n and 93 37 10 93 32 55 w fig 1 the topography of the area is highly irregular with steep slopes dominant soils are regosols and cambisols over granitic rock the tropical climate is seasonally dry annual mean temperature ranges between 25 and 28 c average annual precipitation reaches 2003 484 mm 30 year average conagua 2015 the pine savanna is located between 900 and 1100 m above sea level both communities were established during the 1970s by landless people cruz morales 2014 since the settlement cattle raising together with maize and bean production for self supply and regional markets have formed part of the smallholders livelihoods cruz morales 2014 but livestock became even more important in the late 90s when maize prices plummeted in 1995 as a result of nafta garcía barrios et al 2009 garcía barrios and gonzález espinosa 2017 in the same year the federal government designated a buffer zone in the sbr currently the people grow mainly maize and beans for self supply for monetary income they raise livestock grow organic coffee and more recently extract resin from pinus oocarpa schiede ex schltdl of their total land area pine savanna is particularly important for production as cattle raising plus resin represents a significant share of their income in both communities pine savanna extends close to the forest frontier in one of the core protected areas of the sbr which consists of a highly biodiverse montane cloud forest ecosystem conanp 2013 in the pine savanna the most abundant exotic grass species are melinis minutiflora p beauv gordura grass and hyparrhenia rufa d a reid jaragua grass which were introduced to mexico in the late 19th century for livestock production parsons 1972 pinus oocarpa dominates the pine savanna tree stratum braasch et al 2017 2 2 extensive surveys with resin producers main topics and analyses the most relevant actors related to pine savanna management are the local families roughly 74 of them are involved the conservation authority conanp and the national corporation that buys the turpentine alen del norte secondary actors are the national forestry department conafor an environmental conservation and development ngo pronatura and public research institutes ecosur and universidad autónoma de chapingo the authors of this paper have sustained frequent interactions with all actors during the past three years both in the field and in organized meetings from these interactions it became apparent that within and among these groups of actors there are different and sometimes conflicting views on the level and importance of pine recruitment in the savanna and on the existence of ecological and economic tradeoffs associated with exotic grass management options with the aid of questionnaires maps photos drawings and field visits in 2016 we interviewed 52 local people involved in turpentine extraction men and women of different age groups from both communities to further clarify their activities land use interests expectations about resin extraction as a short or long term livelihood option knowledge on ecological factors affecting pine recruitment tradeoffs involved in each exotic grass control measure controlled fire weeding grazing and preferred control option surveys were analyzed with descriptive statistics and results were used to build true grasp and design farmer and multi actor workshops farmers were involved in the modelling process most resin producers participated in defining its general purpose fifty two discussed with researchers the relevant processes involved and provided empirical data which were later stylized by researchers as model parameters ten farmers tested the user interface and validated the model s qualitative behavior and outcomes in a pre workshop meeting all along they showed interest in research oriented towards exploring management options for recruitment the day they attended the single actor workshop farmers were invited to actively work all day and thus received six usd covering a local daily salary 2 3 the true grasp agent based model 2 3 1 virtual world and components in this section we provide a summary of how true grasp was designed and describe its components for more detail see the overview design concepts and details odd protocol in supplementary data appendix a where we followed the updated odd protocol by grimm et al 2010 and müller et al 2013 true grasp is an agent based model created with netlogo v 5 2 1 wilensky 1999 it allows users farmers and other actors to set management parameters run the simulation observe the trajectory of relevant variables and repeatedly reset the simulation and its parameters to stir specific trajectories towards desired ecological and productive outcomes with acceptable tradeoffs the entire landscape of the virtual world has an extension of 81 x 129 patches 10 449 cells which represents 4 ha of land divided into two equal parts pine savanna and open pasture representing typical landscapes in the sbr fig 2 the total size of the netlogo world was selected to contain a realistic initial population of 50 adult pine trees per ha of savanna while allowing space for all other user interface features buttons sliders switches plots and monitors each patch covered 3 8 square meters an arbitrary but convenient size to reconcile the different spatial scales of the modeled agents and their movements grass fire farmer cattle pine dispersion it is worth noting that these agents movements are highly stylized we deliberately avoided dubbing them with complex dispersion and search behaviors we chose the rabbit grass weed algorithm wilensky 2001 to produce a semi random walk for cattle the mushroom hunter model algorithm railsback and grimm 2012 for farmers searching pines and moore neighborhood colonization for grass and fire the general assumption in this stylized approach is that for the current purpose of the model the intensity and consequences of interactions among all these agents reasonably and sufficiently depend on their local and global densities albeit in nonlinear ways the pine savanna is a forest stand with an open canopy and initially contains 100 mature pine trees in two ha land the forest understory cells patches in netlogo language can be covered by the following agents representing different vegetation cover types 1 pine seedling 2 pine sapling 3 exotic tall grass cell that impedes seed germination and seedling establishment 4 short grass cell short statured native grass or recently burned forest floor or exotic grass kept short by cattle grazing with all these cell occupations allowing pine seedling establishment 5 pine needle litter or shade near adult pine trees not allowing seed germination or seedling establishment see supplementary data a fig 2 other agents are resin producer cow and fire the user embodies and assists the virtual resin producer in defining cattle load and the frequency of cattle rotation use of fire and manual weeding the most relevant outputs reported annually and cumulatively are the number of pine seedling saplings mature productive resin trees resin barrels and calves each iteration time step in the model represents one day a thirty year simulation with an intel core i5 processor takes between 5 and 10 min several decades 50 100 years of simulation can transform the pine savanna into an open pasture land if no recruitment takes place or into a closed pine forest without grass in the understory if tree recovery and growth is high intermediate states are also possible although half of the virtual world is open pasture land potential pine recruitment can also proceed there albeit at a low pace 2 3 1 1 pine lifecycle in the savanna each adult pine produces seeds every year they are randomly distributed within a radius of 15 cells a seed only germinates in short grass cells the daily seedling growth rate is reduced as a function of surrounding exotic tall grass cells within its moore neighborhood which slows down the process by which the sapling becomes a young tree and is free from this competitive effect seedlings and saplings adjacent to trees older than 10 years die by a self thinning process young trees can resist cattle trampling and fire once they reach the age of 3 and 9 years respectively thus a successfully established tree needs to have found space for germination and survive all risks fire trampling competition of exotic grasses and self thinning by other pines during its lifecycle with intermediate exotic grass competition an established new pine can be tapped for resin by the age of 25 years an average individual pine s resin production lasts between 10 and 20 years depending on tapping intensity in the real world a tree face is tapped for five years by moving the resin tapping face upwards to a maximum height of 2 5 m each year before a new tapping face is initiated at the other side of the tree tree diameter in the study area permits between 2 and 4 faces on a single tree see supplementary data a fig 7 by the age of 45 years an adult tree will have exhausted its resin production and thus can be felled for lumber otherwise it dies naturally at the age of around 140 years fig 3 we do not incorporate the probability of death due to bark beetles because they are not an appreciable factor in the area 2 3 1 2 short grass browsed or burnt cells they refer to all cells on the forest floor not occupied by adult pine trees pine needle litter or tall exotic grass which can be colonized by a pine seedling under appropriate environmental seasonal conditions 2 3 1 3 exotic tall grass cells preclude pine recruitment and provide fuel for fire and fodder for cows if in following iterations the exotic tall grass cell is covered by pine needle litter or shade grazed by cattle or burnt it becomes a short grass cell which again can become an exotic tall grass cell if tall grass later recovers 2 3 1 4 pine needle litter shade cells exotic tall grass and short grass cells within a five cell radius around an adult pine tree transit gradually into litter shade cells and do not allow seedling establishment only fire manual cleaning or litter decomposition transforms the cell back to a short grass cell full pine leaf litter decomposition occurs two years after an old tree has died naturally or has been felled allowing grass growth 2 3 1 5 cow it moves in a semi random walk within the assigned space pasture land savanna or both with or without rotation if a cow crosses over a susceptible seedling aged 3 years it tramples and kills the seedling the cow starts with an initial energy weight of 1000 units energy is lost each time step day due to movement and energy increases only with consumption of exotic tall grass cells if the cow encounters insufficient exotic tall grass cells its energy eventually falls to zero and the cow dies if availability of exotic tall grass cells is high and the cow reaches more than 1650 units reproductive weight it conserves 1000 units for itself and devotes the rest to produce a calf each cow is calibrated to produce no more than one calf per year see also section 2 3 3 calves do not consume grass as they are sold and thereby removed from the virtual world 2 3 1 6 resin producer five family members farmers move in the savanna searching for pines if a farmer encounters a resin producing tree he taps it and harvests the resin the farmer is initially endowed with 100 energy units which he spends walking harvested resin first compensates for this energy loss kg of resin converts into money which covers his labor costs and any resin surplus accumulates in the farmer s resin container if the energy level reaches zero the farmer quits being a resin producer if there is a surplus of resin in the farmer s resin container 400 units 40 kg the harvested resin is stored in 200 kg barrels for sale and thus leaves the system in this simulation the resin producer is always in the savanna and moves forward one cell per time step day in search of resinous trees although obviously unrealistic this stylized tree search dynamic is parameterized so that the average weekly harvest of this family in the 2 ha virtual pine stand resembles average yield per week in the study area if a farmer moves over an exotic tall grass cell his movement is delayed compared to a short grass cell free of obstacles and also spends more energy that decreases his net resin accumulation slightly 2 3 2 management practices to control exotic grasses and pine needle litter exotic grasses and pine needle litter both influence pine seedling establishment the model considers the following management practices to overcome this constraint the user can select among 2 3 2 1 manual weeding and cleaning the farmer in the model converts by cutting manually exotic tall grass cells and cleaning pine needle litter within a 4 cell radius of a resinous tree into short grass cells this accelerates the farmer s forward movement and opens space for seedling establishment but also reduces the farmer s energy level due to the invested labor which ultimately affects his net resin harvest 2 3 2 2 fire it is simulated following the simple fire percolation model from netlogo 5 2 1 model library wilensky 1997 fire always starts in the center cell of the virtual world and spreads with each time step from a burning cell to any of the eight surrounding cells moore neighborhood that contain fuel exotic tall grass cells or pine needle litter fire converts these cells to burnt short grass cells if fire reaches a cell covered by fuel and containing a susceptible pine tree the latter dies fire can occur spontaneously each year with low probability 4 it can also be used as a management practice by the user at any moment or with a fixed periodicity in the current version of true grasp fire is not required for pine seed germination 2 3 2 3 cattle stocking and rotation frequency the model user chooses a certain number of cows between zero and eight and decides whether they occupy only the pasture land the savanna or both in the second case pasture land and savanna cows can perambulate freely or rotate between fenced paddocks with a user defined frequency 2 3 3 model parameterization and calibration in appendix b we present a non exhaustive but comprehensive multivariate sensitivity analysis of true grasp responses to a relevant set of parameters here we highlight some important aspects of the model s rationale true grasp stylizes ecological and economic processes and produces outputs that do not mimic the exact quantities to be expected in real world situations and which are still largely unknown and highly variable it is not predictive in that sense but its time series reproduce fairly well the short and mid term qualitative system behavior described by local and external actors in response to their proposed management practices in the pine savanna moreover it produces reasonably well long term scenarios that seem plausible to users a central purpose of the model is to allow users to explore individual and combined effects and tradeoffs of different options fire weeding and cattle grazing of controlling exotic grasses in the pine savanna cattle grazing is currently the most contentious option and the best studied in the area so it is better specified in the model than the other options actual grazing strategies in these and surrounding communities cattle stocks and rotation rates are context dependent and therefore highly variable in the sbr smallholder farmers cattle herds are composed of 5 20 animals and are rotated adaptively in their land between open pasture savanna or both their combination farmer s rule of thumb for an annual average stock which allows the production of 0 8 1 0 calf per cow per year and that does not produce a steady long term decline of grass cover in open pasture is one cow per hectare of open pasture however detailed surveys and analysis prior to this study rosabal ayan 2015 valdivieso pérez et al 2012 and in garcía barrios et al unpublished database showed that for rangelands combining open pastures and savannas an appropriate stocking density is 0 5 cow per ha therefore we selected and coupled parameters for cow reproduction and exotic grass recovery rate so that a a cow would produce 1 0 calf per year in 2 ha of open pasture and 0 8 calf in the combination of 1 ha of savanna and 1 ha of open pasture and b a cow browsing 2 ha of open pasture would keep grass cover at an equilibrium value near 80 thus more than 1 cow per 2 ha increases the percentage of short grass cells for pine recruitment at the expense of calf production while a lower cattle load could significantly reduce space for pine recruitment with little or no gain in calf production fig 4 a c shows these variables trajectories and sensitivity analyses for different cattle stocks appendix b provides further details and sensitivity analyses for a broader set of outputs fire was modeled rather crudely using a simple percolation model wilensky 1997 which was parameterized such that pine recruits are sensitive to burning during the first nine years of establishment thereby allowing reasonable recruitment under a low fire frequency regime weeding was parameterized to reflect the fact that it ceases to be cost effective when used too frequently or as the sole grass management practice 2 4 single and multi actor workshops 2 4 1 preliminary surveys before starting each of three single actor workshops with smallholder farmers of california tres picos and external actors participants were asked to a determine knowledge on factors affecting pine recruitment and potential tradeoffs associated with preferred management practice to control exotic tall grass cover and b explain the type and consequences of emerging interactions direct or inverse with the help of a resin production system diagram to make aware of tradeoffs when including e g cows or fire and identify the preferred effects of these components 2 4 2 single actor workshops three 4 h workshops were held separately with 10 participants from california 7 participants from tres picos and 5 external actors respectively on march 27 to 29 2017 two participants and one trained facilitator from the researcher students team sat at each computer workshops included the following sessions 1 welcome ice breaking dynamics and sharing workshop purpose 2 presentation of an illustrated talk leading participants from real images of the savanna landscape with its vegetation livestock and human components to their in silico representation in a netlogo world this was accompanied by brief simulations of the behavior of each netlogo agent and patch 3 demonstration of a 15 and 30 years simulation carried out by the first author showed how a pine and native grass savanna without cows allow pine recruitment and natural development towards a closed pine forest with its accumulated saplings productive trees and resin production 4 based on the same initial spatial state a 15 and 30 years simulation led by the first author and executed by participants with the help of facilitators showed the consequences on pine recruitment when substituting native grass with exotic grass considering the previous simulation setting see 3 each participant was asked to write down his predictions on how outputs would change qualitatively increase remain decrease and then compare them with the actual simulation outcomes 5 eight 15 years simulation scenarios led by the first author and executed by participants and facilitators showed both the consequences on recruitment and other outcomes of a burning the savanna b weeding tall grass and c allowing cattle grazing in the whole grassland savanna area scenario 1 starts with a full ground cover of native grasses leading to a closed pine stand with abundant recruits and no exotic grass see 3 scenario 2 substitutes native for exotic grass and it is headed in the long run 50 yr to an open grassland see 4 scenarios 3 4 and 5 add to scenario 2 a fire event every year in years 1 4 8 12 and in years 1 and 12 respectively only the last of these fire regimes creates a window for saplings to escape size related vulnerability to fire and therefore increases recruitment scenarios 6 and 7 consider scenario 2 but include manual weeding around pine trees every year and in years 1 and 15 respectively scenario 8 also starts with scenario 2 but with 6 cows grazing freely the open pasture and pine savanna the whole netlogo world 6 in both farmer and external actors workshops during a 30 min period pairs of participants were asked to select a set of parameters available on the user interface to achieve five non trivial output goals simultaneously pre established by researchers by year ten these ecological economical goals were 40 recruits 20 resin barrels 18 calves less than 25 trampled saplings and more than 50 tall grass cover this goal oriented approach was chosen to increase participant s familiarity with the interface and for them to experience the many potential interconnected tradeoffs involved 7 the two brothers exercise seven pairs of smallholder farmers were teamed up such that in real life one team member has pines and cattle while the other one has only pines they were told suppose you are two young brothers a and b whose father wishes to inherit a small herd of cattle and 4 ha of land two as pine savanna and two as open pasture your father says i am inheriting the whole property to both of you a will own any present and future pine tree on the whole property and b any present and future cattle and grass it is up to you how you will manage the whole property together to make a livelihood by year ten a must meet 18 resin barrels 45 recruits trampling less than 30 and a non negative weeding subsidy b must meet 20 calves and grass cover not less than 50 reaching both sets of goals was possible but non trivial and the process could drift towards one participant s interests at the expense of the other s participants had 30 min per team to select and explore parameter sets and run simulations to reach together their respective goals 8 a 30 min collective reflection on the workshop experiences concluded the single actor workshop 2 4 3 multi actor joint workshop participants were six smallholder farmers from the ejidos california and tres picos 3 representatives from each village three conanp officers two representatives from the ngo pronatura and the regional officer of alen del norte all 12 participants had been acquainted previously as neighbors and or partners or observers of the resin business after a brief reminder presentation of the model operation each farmer was paired with an external actor to explore and reach the following three goals in a single attempt with one fifteen year simulation optimize sapling number resin and calve production the five teams were free to define their own sets of management parameters for this one shot experiment except for the number of cattle which was fixed to five by researchers an excessive and suboptimal stock would make participants confront stronger tradeoffs among outputs the exercise was presented as a contest to see which team would achieve the best result immediately after the simulation contest and reflection over the outcomes we conducted a collective and public exercise for all actors to explore hypothetical pine savanna management choices along a decision tree where questions were revealed to them step by step binomial decisions were is resin production a long term project 50 years should saplings be recruited naturally or nursed and planted should the main exotic grass control strategy be based on cattle stock management or grass weeding and scorching are such interventions collective or private decisions and endeavors should they be subsidized 3 results 3 1 farmer interviews to explore local ecological knowledge the majority of interviewees were males older than 40 active in resin extraction and with four to five primary activities resin maize beans coffee and livestock fifty three percent of resin producers in california and tres picos owned livestock at that time most considered that the livelihood importance of resin and coffee would grow while maize and livestock would remain stationary older farmers were well aware of how the current landscape came about through selective logging and land clearing for crop and cattle production for more detail see braasch et al 2017 fifty percent envisioned local landscapes in the next ten to twenty years to consist of a semi closed pine oak forest combining resin cattle and firewood production 23 chose a closed pine forest dominated by pinus oocarpa to increase resin production and 19 decided for a mixture of several land use types pasture land open pine oak forest and closed pine oak forest but separated in space a few also included oak forest 6 for firewood production regarding strategies by smallholder farmers to maintain pine stands all interviewees said do not cut pine trees two thirds do not burn only two fifths recruit saplings and only one in twenty reforestation with nursery pine trees eighty percent considered that natural pine recruitment was appropriate at the whole ejido level while 58 considered it was reasonably high in their own pine stands only half of the interviewees were aware that p oocarpa trees need to reach 25 40 years of age before they produce resin in this area considering pine recruitment thirty five percent of the interviewees considered it requires bare soil while 19 mentioned grazing and very few included post fire conditions such as fertile soil and a seed shedding pine tree nearby more than one third did not have an answer to the question on regeneration niche when asked about obstacles to recruitment fig 5 a two thirds considered ungrazed exotic grass and pine leaf litter accumulation while only one fifth included also trampling of saplings by cows regarding cattle effects on forest floor fig 5b almost all interviewees mentioned grazing lowers fire risk and grass competition for saplings three fourths cattle browse and trample one third trampling prepares the soil for pine germination 8 cattle causes soil erosion and 13 no effect overall responses showed that many farmers have a broad and precise knowledge of landscape history resin production requirements pine life cycle and regeneration niche factors affecting pine recruitment and tradeoffs of cattle grazing in pine savannas for sapling establishment and growth when considering all interviewees knowledge and opinions were diverse incomplete and in some cases contradictory e g some farmers mentioned the positive effect of fire fire is needed to stimulate tree recruitment but at the same time they said that they do not use it because it is bad as it causes wildfires contrasting opinions occur most likely due to differences in age activities and livelihood related preferences and opportunities the rich yet incomplete ecological knowledge of smallholder farmers and the diversity of their interests regarding savanna management were useful both to guide the design and parameterization of the abm and to further value the pertinence of facilitating farmer workshops on this matter 3 2 agent based model capabilities true grasp proved capable to qualitatively reproduce three different long term scenarios of interest to the smallholder farmers for resin extraction and calf production these were table 1 a a baseline scenario previous to exotic grass invasion represented by closed pine stands with native grass pine needle litter understory and abundant pine recruitment b open exotic grassland as a consequence of lack of pine recruitment and c exotic grass cover with moderate cattle load two cows in four ha with both high recruitment and high calf production a win win situation resin producers and other actors considered each of these scenarios relevant and graphically fig 6 and conceptually credible each scenario proved to be robust under a range of ecological and management conditions and sensitive to threshold values of a single or several parameters potentially causing regime shifts between some of these scenarios 3 3 abm supported scenario exploration workshops seventeen persons in total participated in two farmer workshops in 2017 with the same proportion of farmers owning cattle 53 as in the 52 interviews held in 2016 five persons participated in an external actor workshop and 12 in a multi actor workshop 3 3 1 pre agent based model surveys and tests pre abm interviews and exercises revealed that a all actors got high scores when identifying the sign of direct interactions among pine savanna silvopastoral components in fig 7 yet positive effects of cattle and fire on pine recruitment were more frequently missed the former more by external agents and the latter more by farmers in consequence the same trend was observed when actors identified tradeoffs associated with cattle presence and with fire use b after the fire and cattle tradeoffs were identified by actors or pointed out by facilitators 60 of farmers and only 20 of externals saw more benefit than damage in cattle regarding controlled fire tradeoffs the opposite occurred all externals and only one elder farmer an ejido founder saw more benefits and would apply fire to control exotic grasses 3 3 2 smallholder farmers and externals understanding and validation of the agent based model fig 8 a shows the abm outcomes for recruits under eight mid term scenarios 15 years presented to participants for them to explore on the computer see also supplementary data appendix b figs 3 and 4 the figure shows that recruitment is very high under scenario 1 and collapses in 2 compared to scenario 2 recruitment increases slightly with a 12 year fire regime but increases more with yearly weeding instead of fire albeit with very high labor costs when using cattle instead of weeding recruitment is only slightly lower but produces additionally around 30 calves participants were presented these eight scenarios sequentially and were asked to predict prior to each simulation if recruitment would be higher or lower the simulation of the native grass cover was compared with the initial condition exotic grass cover simulation was compared with the output of the native grass scenario while the predictions for fire weeding and cattle management were compared to the exotic grass cover scenario fig 8b shows almost all farmers predicted qualitative outcomes correctly in each case and only a few underestimated the damage caused by yearly or four year interval fires in the course of these simulations participants started to become aware of other outputs as well resin and calf production grass cover and the nonlinearities and tradeoffs associated with the modeled situations through these predictive exercises farmers a learned the user interface and got used to interact with it b developed confidence in the tool to later explore qualitatively ranges and combinations of management options and their tradeoffs and c learned to understand the importance of discussing mid and long term effects of management rather than short sighted snapshots of immediate effects between the simulation of scenario 2 exotic grass dominates and the simulations with fire weeding and grazing scenarios participants were asked to list their options to deal with the low recruitment associated with unmanaged exotic grass around 50 of the farmers mentioned weeding and planting saplings 24 excluding cattle from pine stands and 25 using cattle grazing to control grass very few mentioned controlled fire fig 9 in contrast all external actors mentioned fire as an option and cattle grazing only as the second choice weeding and planting saplings were no real options for externals because of high labor costs 3 3 3 smallholder farmers and externals management parameter explorations in search for pre established ecological and economic goals in the first goal oriented exercise 33 of farmer teams met three goals 44 met four and only 22 met all external actors did better 25 met four and the rest met five goals both groups success frequency was very similar for recruits barrels and calves the difference lied in farmer s lower success due to trampling and very low grass cover because on average they stocked more cattle per land unit examples of selected management strategies and their multivariable output are presented for farmers fig 10 a and externals fig 10b in both cases the most successful model outputs combined moderate weeding with the rotation of medium 2 4 animals ha cattle loads all available management practices affect exotic grass cover directly and this eventually affects indirectly all simulation outputs where cattle were included the teams had to figure out how to deal with the direct and indirect effects of grazing in order to strike a balance between the positive and negative effects of cows on recruitment and to reach the pre defined calf and recruit scores fig 11 shows examples of different search strategies more generally fig 12 a d shows as gray clouds the relations and nonlinear tradeoffs between some of the model s output sets produced by 3240 parameter combinations available to participants for fire frequency 0 1 2 3 5 and 10 burnings weeding frequency 0 2 3 5 and 10 weedings cattle loads 0 8 cows and rotation frequency 0 1 2 3 4 6 and 12 per year in these ten years scenarios reduction in the exotic grass is associated to broad and nonlinear sets of responses for both recruits and calves fig 12a and b produced by a myriad of ways how management options can be combined this in turn defines a relation between recruit and calf sets that on average turns from synergistic to antagonistic fig 12c superimposed on these clouds are black dots representing the actual x y ordered pairs of outputs achieved by participants in all their attempts to reach the established goals the latter outputs show that most participants did not explore parameters in a veil of ignorance and at random but found their way in this multivariate and nonlinear search space towards the model s win win scenarios for cattle production pine stand persistence and long term resin production both regressions in each graph are showing the same trends for the regression between resin barrels and exotic grass cover fig 12d r 2 value polynomial regression is low because in ten years there is yet no correlation between this set and the effect of recruitment on barrels is yet to come see also supplementary data appendix b figs 5 and 6 3 3 4 the two brothers exercise in the two brothers exercise two out of seven teams met both brothers goals calf vs resin recruit productions they arrived at proper cattle loads combined with rotation and moderate fire or manual weeding but not both which rendered productive and cost effective levels of exotic grass the other five teams also arrived at 3 or 4 cows but did not meet all goals three penalized their income from resins due to high weeding costs two penalized their calf production by excessively reducing grass availability by weeding and burning in the one actor exercise team members had only common goals and therefore clear reasons to collaborate and deal with tradeoffs together in this brothers exercise they had individual goals and there was room for transforming tradeoff management into conflict and dominance of one brother s goals and interest over the other yet search spaces of the two brother exercise did not differ with the first exercise nor compared to the regression trend of the sensitivity analysis fig 13 we observed collaboration to try and meet both participant s goals although again few teams actually met them 3 3 5 reflection meetings both groups of farmers considered the simulation exercises increased their awareness of the long term effects of any current management strategy on resin production and forest cover in the pine savanna and on the consequences of not paying sufficient attention to recruitment they also said they became more clearly aware of the cattle tradeoff and the need to handle cattle load properly one farmer summarized it as i am used to putting any number of cows in my pine stand without much thought and take them out when the grass becomes too short but with no consideration for pine saplings now i know managing stocking size can make a difference farmers did not express concern for the fact that most teams did not meet all goals set by researchers in either of the free exercises rather they highlighted the many tradeoffs involved in such challenging multi goal searches interestingly some farmers valued specifically true grasp as a tool with which they could experience the connectedness of many inputs and outputs through their joint responses some also mentioned that they could see very clearly what were the preferences of their team mates and other participants when faced with management choices and output tradeoffs many farmers stated that the abm sufficiently captures what goes on in the pine savanna and that trying to meet goals really made them get involved that they had fun and paid attention to the behavior of the many factors involved and therefore learned much more than by sleeping over a long tedious power point external actors valued tradeoff analysis but some were concerned with the model not being sufficiently realistic e g not having slope effects soil erosion nor being quantitatively predictive e g exactly how many saplings would be produced in real life and management cost effectiveness 3 3 6 multi actor joint abm exploration during the last workshop all multi actor teams used a suite of different options with different frequency rather than focusing on a single one fig 14 interestingly when trying to define a winning team all concluded that it was not possible nor reasonable as some were ahead in some variables while behind in others an experience which made the concept of tradeoff even clearer to participants recruitment wise a winning team did stand out the farmer said about their scores these resin tanks are our present these recruits are our future and these few calves are the tradeoff for taking the future into account 3 3 7 multi actor meeting to discuss decision tree immediately after the simulation contest we conducted an exercise for actors to make hypothetical management choices along a decision tree revealed to them step by step fig 15 all chose resin production as a long term project 50 years the most consolidated production partners ejido california and alen co preferred tree stand regeneration with pine nurseries and sapling planting something they sustained since the pre simulation interviews fig 9 while all others preferred assisted natural recruitment both ejidos preferred exotic grass control around sapling by cattle and suppress fire while the ngo and alen co preferred weeding and controlled ground burning some but not all participants changed their points of view along the process and particularly after the collective exercise conanp had strongly advocated concentrating cattle in intensive land use areas and keeping them out of the savanna it now accepts it is sound and less costly to control exotic grass with a combination of cattle grazing and controlled fire and occasionally other methods the alen officer initially stated that cattle were the cause of exotic grass invasion during the simulations he acknowledged the capacity of cattle to control these grasses and favor recruitment during the decision making exercise he again dismissed cattle presence in the pine savanna alen co considered that proper management of pine stands and associated monetary costs to promote new trees should be each farmer s endeavor the ejidos preferred to request government subsidies for such practices through a resin producer organization while conanp and the ngo did not favor subsidy requests 4 discussion and conclusion peasant populations established at tropical and subtropical forest frontiers have secularly developed silvo pastoral practices livelihoods and landscapes in their territories garcía barrios and gonzález espinosa 2017 koning 2014 sloan 2007 van vliet et al 2012 walker et al 2002 many of their multiple use forested areas have recently been claimed by other actors and declared mabr the few new opportunities and the many constraints to silvopastoral use and management dictated by external actors have frequently led to all types and levels of conflict and lack of success for all parties bernard et al 2014 bouamrane et al 2016 cortina villar et al 2012 ma et al 2009 martín lópez et al 2011 in consequence actors in some cases have slowly acknowledged the need to engage in collective learning and deliberation to better understand and negotiate their interests socio ecological researchers have shown interest in these processes and are active in helping to develop and deploy strategies methods and tools to support learning negotiation and decision making berthet et al 2016 etienne et al 2011 kok 2009 mathevet et al 2011 tenza et al 2017 villamor et al 2014 voinov and bousquet 2010 wittmer et al 2006 these multi actor efforts are riddled with theoretical and practical challenges related to the different values knowledge frameworks interests and power relations of those involved galafassi et al 2017 huntington 2000 tenza et al 2017 a number of participatory decision making frameworks have been developed to deal with these challenges for a review see lynam et al 2007 additionally land under silvopastoral use exhibits complex non linear social and ecological interactions that confer both obvious and subtle tradeoffs which not only have short term consequences but can lead to long term undesired shifts in vegetation regime and to local production and livelihood collapse allen and gunderson 2011 carpenter and gunderson 2001 filatova et al 2016 filatova and polhill 2012 abms and rpgs are instruments well suited to capture and explore in stylized and dynamical form these complex silvopastoral behaviors an 2012 becu et al 2008 bousquet et al 2002 etienne 2014 filatova et al 2016 2013 parker et al 2003 villamor and van noordwijk 2011 voinov and bousquet 2010 a small but growing suite of rangeland and silvopastoral tools have been developed sylvopast etienne 2003 sierra springs garcía barrios et al 2011 2015 abm rpg grazing tool for herders and foresters dumrongrojwatthana et al 2011 rangeland rummy farrie et al 2014 sequia basalto bommel et al 2014 grazing game villamor and badmos 2015 forage rummy martin 2015 and kulayijana perrotton et al 2017 they differ in their specific purposes complexity precision realism and actors involvement in the various stages of development creating an abm supported rpg that represents the dynamics of silvopastoral land subject to a suite of management options and actors interests is in itself a very elaborate process that involves dealing with many design tradeoffs and difficult choices regarding realism precision and generality we and others have previously found that smallholder farmers in some but not all senses are initially in disadvantage relative to other actors when learning using and interpreting these complex tools but that the gap can be closed garcia barrios et al 2017 thus we consider that design decisions should be led by making sure that smallholder farmers can engage trust the qualitative outcomes enjoy the virtual immersion in complex behaviors and contribute to the collective learning experience galafassi et al 2017 garcia barrios et al 2017 2015 2011 le page and perrotton 2017 perrotton et al 2017 the tool described in this paper allows exploring management options and assessing their consequences in the short 10 15 years and mid 16 50 years term each combination of options is investigated independently the tool is therefore currently suitable to explore individual management options at the farm level or to represent landscape effects assuming that a collective agreement for centralized management exists yet the abm would need to be further developed in order to capture more complex social behaviors where defection and free riding may exist and where individual agents are making concurrent decisions accounting for what the others may decide at the same time the consequences on the landscape could differ significantly from centralized management such a model which could be built in netlogo s hubnet platform would offer a group of 5 6 participants the possibility to re adjust their individual management options at each step during a simulation run this would allow going further than the two brothers experiment into co managing renewable resources in short there is ample space for further developing the model we will now briefly discuss the most relevant findings derived from pre modelling interviews model building pre workshop quizzes interactive simulation workshops and group deliberations 4 1 pre modelling interviews most smallholder farmers stated that a resin as an income option will grow in the future and that they are willing to conserve the pine oak savanna b sapling establishment requires bare soil and is reduced by dense exotic grass cover pine leaf litter accumulation and sapling trampling by cows c grazing is one of several possible means for pine recruitment very few mentioned fire management to burn the grass this recent fire taboo or silence stems from the previous conflict with conanp over traditional fire use in the area braasch et al 2017 guevara hernández et al 2013 navarro et al 2017 smallholder farmers strongly depend on their ecological knowledge but also incorporate information from external actors soto pinto et al 2007 valencia et al 2015 there is no unique truth processes change continuously and unpredictably and therefore knowledge is never complete participatory approaches can help navigate such complexity by combining farmer and academic knowledge and developing together new insights allen and gunderson 2011 dawoe et al 2012 garcia barrios et al 2017 vandermeer and perfecto 2013 4 2 model development and validity structuring and parameterizing true grasp based on quantitative and qualitative knowledge provided by farmers field research and literature was successful this simulation framework exhibits sufficient internal validity because as shown by a thorough sensitivity analysis a it reproduces qualitatively the different long term scenarios of interest for pine recruitment closed pine forest open grassland and pine savanna b it produces qualitative scenarios that are robust under various combinations of management practices cattle rotation weeding fire but may shift when ecological thresholds are crossed c it exhibits interesting and credible nonlinear responses to each practice s range of possible values as well as credible nonlinear tradeoffs among practices and among desired outputs d it clearly captures the fact that similar output syndromes sets of relevant output values can be achieved with different management combinations as reported by an 2012 abm are increasingly used to simulate the complexity of ses because they can deal with heterogeneity featuring feedbacks nonlinearities and adaptation they provide several advantages over other models when dealing with land cover change regime shifts and tradeoffs filatova et al 2016 miyasaka et al 2017 parker et al 2003 4 3 pre simulation quizzes true grasp directly exposes users to nonlinear non trivial tradeoffs when using different management practices such tradeoffs became apparent to all actors only after having solved the quizzes smallholder farmers were more interested in managing cattle tradeoffs while external actors in managing fire tradeoffs again manifesting their respective biases and preference taboos it is ironic considering that a few years ago smallholder farmers used fire liberally and externals prohibited it as described in galafassi et al 2017 these outputs can be explained in three ways a tradeoffs are often invisible because of a lack of systemic understanding b tradeoffs are perceived differently by different actors different people see gains and losses differently and c tradeoffs are often hidden or ignored when taboos are involved daw et al 2015 schoemaker and tetlock 2012 4 4 learning trusting and using true grasp all actors found the long term qualitative forest cover scenarios relevant and credible appreciated this long term approach to management choices and increasingly trusted the tool for qualitatively exploring ranges and combinations of management options and their tradeoffs most users correctly predicted the qualitative forest cover scenarios discovered and dealt with tradeoffs and found their way towards the goals for cattle production pine stand persistence and resin production pre established by researchers as training exercises as mentioned earlier true grasp behaviors and outputs are only qualitatively predictive sun et al 2016 called these kinds of qualitative models simple abms and le page and perrotton 2017 call them stylized referring to the model structure as compared to quantitative data hungry prediction models generic stylized qualitative models are highly recommended for participative approaches to foster deliberation and decision making edmonds and moss 2004 le page and perrotton 2017 sun et al 2016 tenza et al 2017 voinov and bousquet 2010 especially when dealing with tradeoffs hard choices between ecological and social benefits individual and community benefits and among actors who bear different costs and benefits lazos chavero et al 2016 interestingly the reserve management team expected an abm with more realism and prediction of operational quantities while farmers expressed they were content with experiencing and becoming more aware of interactions tradeoffs and indirect effects thus the tension between favoring generality realism and precision levins 1966 and between building theoretical stylized or realistic abms le page and perrotton 2017 is probably unavoidable in multi actor settings during these exercises team members with common goals had clear reasons to collaborate in the two brothers exercise performed by farmers there was room for transforming tradeoff management into conflict and dominance of one brother s goals and interests over the other yet this did not occur this has at least two explanations or their combination users would actually collaborate in real life in an effort to balance the tradeoffs in a fair way as found by garcía barrios et al 2015 where users enjoyed exploring the possibility of such collaboration in a safe environment with no significant cost in their real life relations as found by berthet et al 2016 4 5 user s evaluation of true grasp most actors said that the exercises had increased their awareness of a the long term resin production and forest cover effects b the consequences of low pine recruitment c the usefulness of cattle and or fire to increase pine recruitment and d the many tradeoffs involved they said that true grasp really made them get involved in dealing with the relevant issues have fun and pay attention to the behavior of the many factors involved and their interactions the capacity of the true grasp workshops to produce collective socio ecological learning is in line with other abm and rpg rural workshops berthet et al 2016 garcia barrios et al 2017 2015 2011 patel et al 2007 perrotton et al 2017 speelman et al 2014a 2014b villamor et al 2014 villamor and van noordwijk 2011 4 6 multi actor exercises and deliberation exploration and negotiation among actors ran smoothly and collaboratively although subtle dominance by external actors was frequently expressed by hoarding the computer mouse and output sheet these actors have been interacting and negotiating different issues for the past 20 years and the exercise reflected in a playful way both collaboration and unspoken conflict of their past relationships one principle of rpg described by the commod group but also by other authors etienne 2014 lynam et al 2007 villamor and van noordwijk 2011 is the capacity to cross boundaries among actors belonging to different worlds while being interested in the same resource and to promote a dialog in a fair and balanced multi actor space fostering the dialog in mabr is essential because actors would be able to present discuss and better understand one another s perspectives and needs bouamrane et al 2016 however it is also important to provide a space for social and collective learning among actors to solve problems conflicts and to negotiate agreements patel et al 2007 while in the true grasp exercise participants were more open to combine contrasting management practices during the decision tree exercise they privileged their previously expressed real life preferences while stating they remained open to further discussion this is not surprising as abm rpg workshops should not be expected to produce effects totally aligned with the model s stylized propositions nor to do so immediately and in a one shot experience yet we are certain that the simulation exercise leveled the ground for a more honest discussion and for understanding other actors statements and choices during this last exercise as mentioned by bodin 2017 participatory approaches are sometimes unable to deliver immediate and expected concrete results or create the illusion of results in the form of symbolic outcomes such as aggregated wish lists where conflicts of interest are left untouched the creation of a socially and ecologically sustainable management plan implies a fair and balanced designed arena for productive discussions and negotiations between the smallholder farmers and external actors etienne 2014 perrotton et al 2017 however this calls for the commitment of researchers and all actors to social learning that truly involves smallholder farmers and provides tools that do not overwhelm them galafassi et al 2017 garcia barrios et al 2017 software availability the agent based model true grasp is available online at comses net openabm following this link https www comses net codebases 929f4083 af57 45bd a984 88292ac71be6 releases 1 0 0 true grasp was developed in netlogo language program version 5 2 1 developers of true grasp are marco braasch marcobraasch gmail com and luis garcia barrios luis garciabarrios gmail com acknowledgements we thank the ejido members of california and tres picos in the la sepultura biosphere reserve and all external actors conanp pronatura alen de norte for their participation in the workshop and the biosphere staff in california for supporting research logistics this work was supported by ecosur s m t family agriculture project 11066 and the rufford foundation 17207 1 mb thanks conacyt for a phd scholarship 375409 250287 we gratefully acknowledge yanus dechnik vásquez alan heinze yothers and alejandra hernández guzmán who participated as facilitators during the workshop and helped in logistics we thank the three anonymous reviewers for their comments which greatly improved the clarity of the text appendix a supplementary data the following are the supplementary data related to this article appendix a appendix a appendix b appendix b appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 022 
