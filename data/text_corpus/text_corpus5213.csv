index,text
26065,prediction in data scarce regions is one of the challenging issues in environmental problems in hydrology this issue is commonly addressed by utilizing regression or similarity based regionalization techniques the core of similarity based regionalization techniques is a physical climatic similarity metric that is typically predetermined from the knowledge about the physics of the problem and study area the purpose of this paper is to 1 reduce the subjectivity that exists in the selection of the physical climatic similarity metric by establishing a systematic approach and 2 propose a generic similarity based regionalization framework that estimates the parameters of environmental models in data scarce regions the efficacy of the proposed framework is evaluated for the regionalization of a statistical model that creates probabilistic floodplain maps in data scarce regions results show a trained support vector machine svm with ten basin descriptors and accuracy of 86 is an appropriate physical climatic similarity metric that creates reliable floodplain maps in the arkansas white red region keywords regionalization environmental models data scarce regions classification floodplain mapping machine learning 1 introduction simulation models are simplified representations of real world systems and are widely used to understand the behavior of a complex system devia et al 2015 silvert 2001 a key step in reliable simulation is model calibration that involves the proper estimation of model parameters such that the model generates outputs that match the reference data observations closely the existence of reference data at some points within the model domain is crucial for a successful calibration in environmental and water resources problems different types of models e g climate hydrologic hydraulic geomorphic and statistical models are developed and calibrated based on users needs and the problem at hand additionally these models are implemented at different spatial scales ranging from a few square meters to several thousand square kilometers as the spatial scale increases the heterogeneity in the physical characteristics of the area as well as the actual physical processes increases thus model calibration becomes challenging for large scale environmental models this is especially so complex in data scarce regions due to the absence of reference data for model calibration in hydrology this issue termed prediction in ungauged basins pub has gained a great deal of attention for the past several years hrachowitz et al 2013 sivapalan 2003 blöschl et al 2013 ungauged basins generally refer to basins with no available observed data specifically streamflow thus hydrologic models cannot be calibrated in ungauged basins and flow prediction will involve considerable uncertainty without a reliable calibration to make predictions in ungauged basins a large number of methods namely regionalization techniques have been proposed in the literature regionalization refers to all the methods used to transfer information from gauged basins to the ungauged ones by relating hydrologic phenomena to basin descriptors blöschl and sivapalan 1995 oudin et al 2010 young 2006 in hydrology two different types of regionalization exist one type of studies focus on the regionalization of streamflow statistics e g flood quantiles in regional flood frequency analysis rffa while the other type regionalizes the hydrologic model parameters for continuous streamflow prediction in ungauged basins although a few relevant studies in rffa are discussed in this paper the focus is on the second type as this study aims to propose a framework for the regionalization of model parameters please refer to he et al 2011 and razavi and coulibaly 2012 for a review of model parameter regionalization techniques among the techniques used for the regionalization of model parameters regression and similarity based methods have frequently been applied in regression based methods a hydrologic model is calibrated on a large number of gauged basins and the parameters of the hydrologic model are related to some of the basin descriptors by establishing multiple regression relationships these regression relationships can then be used to estimate the model parameters in an ungauged basin sefton and howarth 1998 tung et al 1997 despite their popularity in the last two decades regression based methods suffer from certain limitations fernandez et al 2000 gibbs et al 2012 hundecha and bárdossy 2004 kim and kaluarachchi 2008 lee et al 2006 merz and blöschl 2004 seibert 1999 specifically regression based methods assume that the model parameters are independent and that the error residuals are normally distributed but these assumptions do not hold in many cases mcintyre et al 2005 furthermore there might be considerable uncertainty in the model parameters due to the calibration method model structure and forcing uncertainty hamby 1994 jakeman et al 2006 matott et al 2009 spear et al 1994 therefore applying the optimum parameter set as a dependent variable in regression equations is not the best approach due to equifinality which is referred to as when multiple sets of parameters lead to similar model performance anderson et al 2001 beven and freer 2001 similarity based regionalization methods on the other hand search for one or a set of basins referred to as donor basins whose importance can be weighted according to a similarity measure from an ungauged basin referred to as the target basin then transfer the entire model parameter set of donor basins to the target basin the transfer of all model parameters as a set in similarity based methods is useful compared to traditional regression based methods which neglect the interdependencies of parameter sets mcintyre et al 2005 parajka et al 2005 kokkonen et al 2003 concluded that when there is a reason to believe that in the sense of hydrological behavior a gauged catchment resembles the ungauged catchment to a sufficient extent it is worthwhile to adapt the entire calibrated parameters from the gauged basin typically a similarity based method consists of two steps including 1 selecting the similar basins and 2 transferring the information from donor basins to the target basin notwithstanding the primary role of the donor basin selection in similarity based regionalization problems most studies have focused on the second step for advanced transferring methods that consider the model parameter uncertainties holmes et al 2002 kay et al 2007 masih et al 2010 mcintyre et al 2005 for example mcintyre et al 2005 introduced an extension of the generalized likelihood uncertainty estimation glue framework beven and binley 1992 for regionalization problems by proposing the use of a weighted average of donor basin simulation results the weights were defined by the product of a prior likelihood of a model and the relative likelihood of that model being applicable to the target basin kay et al 2007 proposed another weighted averaging method where the uncertainty of model calibration was taken into account during the parameter transposition despite these promising advances in transferring information from donor basins to target basins the methods proposed in these studies fall short in selecting the donor basins a proper selection of donor basins in the first step of similarity based regionalization problems increases the chance of reliable and robust prediction for target basins regardless of the complexity of the procedure used in the second step to properly determine the donor basins in a similarity based regionalization method two key questions need to be answered 1 which physical climatic descriptors of the basins should be selected to reflect the similarity 2 which similarity metric should be used to find the most appropriate donor basins typically a set of physical climatic characteristics of basins assuming that they are representative of hydrologic similarity are used within a predetermined similarity metric e g euclidean distance cosine distance or normalized absolute distance viviroli et al 2009 samuel et al 2011 parajka et al 2005 the subjectivity in the selection of these descriptors and the similarity metric is a critical research question one solution to reduce this subjectivity is to utilize a systematic two step approach so that first a reference dis similarity measure that is more reliable than a physical climatic similarity metric prinzio et al 2011 is used for the gauged basins and then the optimum physical climatic similarity metric is explored by mapping the reference dis similarity measure to the basin descriptor space samaniego et al 2010 brunner et al 2018 in this approach the reference dis similarity measure applied on the gauged basins plays a vital role in the selection of optimum physical climatic similarity metric and the overall regionalization performance using an appropriate reference dis similarity measure in the first step of this approach is still very challenging and subjective because it is calculated from one or a set of stream signatures which may not represent the hydrological behavior of gauged basins completely although the selection of appropriate streamflow signatures for representing the hydrological similarity in gauged basins has been widely discussed for regionalization studies in hydrology and for detecting the hydrologically homogenous areas in catchment classification problems yadav et al 2007 sawicz et al 2011 ali et al 2012 ssegane et al 2012 boscarello et al 2016 there is no consensus about the appropriate signatures to be used to reflect the hydrological similarity moreover since this study aims at extending the regionalization framework to other applications selecting the appropriate response signatures that represent the similarity of data rich basins can be more challenging in fields other than hydrology due to their limited literature on regionalization considering these issues this study proposes a generic framework with a behavioral based hierarchical clustering algorithm that does not need any response signature for clustering data rich basins in the first step the similarity measure embedded in this algorithm accounts for the behavioral similarity instead of attribute e g response signatures or physical characteristics similarities the behavioral similarity concept also referred to as hydrological similarity was introduced by oudin et al 2010 on the basis of model parameter transferability they used this concept to compare hydrologically similar basins based on physical climatic characteristics in this paper we use behavioral similarity as a generalization of the term hydrologic similarity and define it as similarity of all physical processes that transfer the environmental model inputs to the target outputs in a basin as mentioned earlier a hierarchical clustering algorithm that uses this behavioral concept within its structure is coupled with a classification algorithm to provide a systematic approach to the regionalization of environmental model parameters this framework overcomes the subjectivity issue at both levels of the response signature and physical climatic characteristic selections the proposed framework aims to extend the concept of regionalization beyond hydrology and provides a generic solution to the regionalization of various environmental model parameters it should be noted that the term environmental model is used without specifying its type to make the proposed framework applicable for different purposes and modeling types e g hydrologic hydraulic geomorphic water quality groundwater and statistical models the hydrologic behavior of a basin should be distinguished from the hydraulic geomorphic or any other behavior because the type and distribution of processes involved vary for example considering the scarcity of water quality variables as these data are relatively expensive and difficult to measure or the limited access to the suspended sediment monitoring stations worldwide the proposed framework can be an effective procedure for the regionalization of water quality models and rating curves of suspended sediments in data scarce regions slaughter et al 2017 tramblay et al 2010 while the water basins are the common computational unit of lumped hydrologic models the proposed framework can be used for other computational units as well for example a grid cell a river and an aquifer can be used as a computational unit for distributed hydrologic modeling sediment modeling and groundwater modeling respectively in groundwater modeling the proposed framework can be applied for regionalizing groundwater levels using a groundwater model and then the parameters of the model can be estimated for aquifers in data scarce regions where field data is limited or not available for model calibration here the application and effectiveness of the method are demonstrated by focusing on a probabilistic floodplain mapping problem where a statistical model already calibrated for several basins is transferred to data scarce basins 2 similarity from a general perspective a model denoted by f in equation 1 is a function of inputs and parameters to produce output s related to a specific domain of application 1 y f x θ where x and y denote the model inputs and outputs respectively and θ represents a vector of model parameters as mentioned earlier model f is a general term that can be used for a broad range of problems for example for streamflow prediction f refers to a hydrologic model where y is the streamflow time series and x is the precipitation data for a hydraulic model y denotes the water depth in a river while the geometry of river and streamflow information may be used as input x in data rich basins reference data is used to calibrate model f and determine its parameter set θ considering a problem where reference data is available for n basins model f can be calibrated for all n basins which results in n known vectors of θ in order to perform a successful regionalization the concept of similarity should be well understood similarity between two given basins can vary depending on the problem at hand for example two basins that show a similar hydrologic response to a rainfall event can present a different behavior in converting the streamflow to flood inundation areas assume that the similarity of two basins i and j with models y i f x i θ i and y j f x j θ j is desired here we introduce three types of similarity 1 physical climatic similarity 2 response signature similarity and 3 behavioral similarity the physical climatic similarity is calculated based on the similarity in some physical climatic characteristics of two basins as explained in the introduction the major concern about this similarity is the high subjectivity for the selection of appropriate physical climatic characteristics that reflect the actual similarity in basins however this similarity is the only available option if one or both basins are located in data scarce regions response signature similarity can be calculated by comparing some signatures calculated from measured response values in a basin this similarity metric is appropriate if the selected signatures reflect the entire process that occurs in the basin for example in order to define hydrological similarity among different basins a large number of streamflow signatures have been proposed in different studies yadav et al 2007 sawicz et al 2011 ali et al 2012 ssegane et al 2012 boscarello et al 2016 still subjectivity in the selection of appropriate response signatures is a concerning issue which would be exacerbated in other fields when there is not sufficient information about the selection of these signatures for example in the regionalization of the floodplain mapping model conducted in this study the target outputs are flood extent maps selecting the appropriate signatures that reflect the similarity in floodplains of two basins is not straightforward because each basin has a different stream network with distinct topographic features and floodplains are formed around streams in addition signatures should be able to represent the shape of flood extent areas the behavioral similarity explains that basins i and j are similar if the entire process y i f x i θ i is similar to y j f x j θ j in order to assess the similarity of the whole process a new similarity measure is defined so that basin i can be considered similar to basin j if the additional errors generated by running the calibrated model of basin i on the basin j and running the calibrated model of basin j on the basin i are negligible the behavioral similarity also outweighs the response signature similarity concept as it automatically considers both the inputs and responses simultaneously by running the model with new inputs and comparing the errors generated from cross simulations in addition the main advantage of behavioral similarity compared to the response signature similarity is that it does not need to deal with the selection of appropriate signatures but directly finds the similarities 3 the bhc ca framework the framework proposed in this study integrates a behavioral based hierarchical clustering algorithm bhc with classification c and aggregation phases a to systematically regionalize the environmental model parameters first the bhc establishes the regions of similar basins then the classification phase relates the physical climatic attributes to those similar regions and the aggregation phase assigns a regional model to each region while failure at each of these three steps can result in a poor regionalization it is worth to emphasize the pivotal role of clustering for exploring the regions of similar basins as can be seen in fig 1 the classification and aggregation phases are located at the same level underneath the clustering algorithm so that their performances are completely dependent on the results of clustering therefore it is fundamental to determine an appropriate approach to detecting similar basins within the clustering algorithm the bhc algorithm clusters similar data rich basins using a similarity measure based on the behavioral similarity concept section 3 1 the class labels of data rich basins determined by bhc are then used to perform the classification phase using a supervised scenario section 3 2 to select the most relevant physical climatic attributes and the appropriate classifier in the classification phase it is recommended to apply the feature extraction and transformation methods using a cross validation technique the best classifier trained on the selected physical climatic attributes maps the attributes to the class labels produced by bhc in the previous step the classification pattern detected by the classifier trained classifier will be the final physical climatic similarity metric which can be used for a data scarce basin to find its donor basins in addition to the trained classifier developed in the training step the aggregation phase is performed to create the regional models corresponding to each class of basins section 3 3 in the test step fig 1 data scarce basins are used as inputs and two estimated outputs generated in the training step namely the trained classifier and the regional models are used as processors to determine the regional models for data scarce basins 3 1 behavioral based hierarchical clustering bhc algorithm basin clustering aims to convert an n dimensional problem into m dimensions where n is the total number of data rich basins n basins with n calibrated models m refers to the number of classes m group of basins with m calibrated models and m n agglomerative hierarchical clustering rokach and maimon 2005 a well known algorithm for grouping data points can be applied to cluster all similar basins into one class in this algorithm pairwise comparisons are applied among all data rich basins and a multi level hierarchy named a dendrogram is created the readers are referred to kaufman and rousseeuw 2009 and ward 1963 for more information about hierarchical clustering the essential component of this clustering algorithm is the pairwise comparison step where the similarity of two basins is evaluated and two basins of the pair with the highest similarity are joined gibert et al 2010 gross et al 2010 wilcke and bärring 2016 various similarity measures have been introduced to decide the priority for joining the most similar data points in the hierarchy these similarity measures can be calculated based on either physical climatic or response signature attributes in hierarchical clustering algorithms a matrix of dis similarity values is required to create the dendrogram and cluster the data points the similarity matrix is typically calculated by inputting a matrix of attributes to a similarity metric e g euclidean metric the main advantage of the bhc is that it directly calculates the similarity matrix without the need for the attributes or the similarity metrics according to this method a similarity measure is proposed to reflect the behavioral similarity between basins i and j using equations 2 3 and 4 below 2 δ i j e i j e i i 3 δ j i e j i e j j 4 d i j d j i δ i j δ j i 2 where e i i or e j j refers to the existing error in basin i j when the model is calibrated on the same basin i j this is the error e g rmse value reported after the calibration of a hydrologic model that exists in all models when the calibration does not result in a perfect match between model results and reference data e i j or e j i referred to as cross modeling error is the error when the model calibrated from basin j i is applied to basin i j δ i j or δ j i referred to as the net error is the difference between the cross modeling and existing error in basin i j d i j or d j i is the similarity measure between basins i and j this measure reflects the dissimilarity between two basins as higher values show fewer similarities the behavioral similarity measure clusters the basins on the basis of model parameter transferability oudin et al 2010 this measure is model dependent and it is assumed that the model used to calculate the behavioral measure can simulate the existing processes that occur in the basin accepting this assumption and considering the fact that this measure is calculated by cross modeling it is implicitly inferred that the measure reflects the similarity in processes denoted as the behavioral similarity one of the critical research questions in clustering is to decide the optimum number of clusters this number is commonly determined by optimizing the performance of the clustering algorithm using a so called cluster validation index cvi sevilla villanueva et al 2016 in this study the silhouette index is used due to its successful performance in validating the clustering algorithms rousseeuw 1987 arbelaitz et al 2013 before optimizing the silhouette index for finding the suitable number of clusters a visual presentation of hierarchical clustering by dendrogram is a useful approach for interpreting distances at different stages of the hierarchy and creating a list of the potential optimum clustering numbers the main limitation of the proposed similarity measure is that it can only estimate the similarity between two data rich basins however to recognize the donor basins for a target basin the similarity between the target basin and data rich basins should be determined to overcome this issue a classification phase is linked to the hierarchical clustering algorithm where the class labels of data rich basins generated by the hierarchical clustering algorithm are fed into a classifier the trained classifier can later be used as an appropriate similarity metric to estimate the similarity of data rich basins with the target basin 3 2 classification phase the classification phase consists of preprocessing e g feature extraction and transformations and supervised learning first the potential attributes including a list of all physical climatic basin descriptors that may be effective on the classification are calculated second the attributes are normalized standardized the attributes are either rescaled into a range of 0 1 normalization or their values are rescaled so that they will have the properties of gaussian distribution with a mean of zero and unit standard deviation standardization then feature extraction techniques are applied to select the significant attributes for classification in supervised learning techniques a classifier is trained on data with available class labels referred to as the training step and then it is used for predicting the class labels of unknown data referred to as the test step a schematic diagram of the supervised learning algorithms used in the bhc ca framework is illustrated in fig 2 let x and y represent the attribute and class label matrices for n data rich basins where x i j and y i represent the jth attribute for the ith basin and the class label of ith basin respectively i 1 2 n and j 1 2 k in the training step a classifier finds the best fit function for relating attributes x to class labels y the fit function or trained classifier in the proposed framework is a physical climatic similarity metric used for estimating the class label of basins in data scarce environments the performance of a classifier is highly dependent on the reliability of training class labels y in the bhc ca framework these class labels are produced by the hierarchical clustering algorithm this shows the critical role of the hierarchical clustering algorithm and its similarity measure on the overall performance of the classifier see fig 1 3 3 aggregation phase the hierarchical clustering algorithm reduces an n dimensional problem into m dimensions by partitioning n basins into m groups to complete the process of dimension reduction the n calibrated models corresponding to the n basins f 1 f 2 f n should also be reduced to m regional models f ˆ t t 1 2 m the aggregation phase creates a regional model for each cluster by aggregating the calibrated models of all basins that belong to that cluster a theoretically reasonable approach is to perform a global calibration for each cluster trying to minimize the total error corresponding to the difference of the simulated outputs and observed targets for all basins within a cluster however this method is rarely applied in practice due to the limitation of calibration methods for optimizing complex decision spaces that include a cluster of different datasets a practical solution for creating the regional models is to directly combine all models that belong to the same cluster using either the model parameter averaging or output averaging oudin et al 2008 it is worth mentioning the output averaging is only applied if the outputs are at the same scales otherwise the model parameter averaging should be used in the cases where the models are statistical distributions e g this case study it is also reasonable to sample data from all models that belong to the same cluster and fit a new distribution function on the pooled data ilorme and griffis 2013 despite the type of method used for the aggregation validating the aggregation phase is warranted 3 4 validation of bhc ca framework to validate the performance of the bhc ca first the efficacy of the aggregation phase is investigated by applying both local and regional models on each data rich basin then the performance of the classification phase is evaluated using an extensive cross validation technique it should be noted that both of these validation steps examine the performance of clustering and its relationship with the classification and aggregation phases implicitly because the inputs to the classification and aggregation phases are derived from the results of the clustering step in the bhc ca framework please refer to the position of the clustering step in fig 1 which is above the classification and aggregation phases in order to validate the aggregation phase m outputs y 1 y 2 y m are generated for each basin using f ˆ t t 1 2 m considering the available reference outputs for these basins the error of predictions corresponding to f ˆ functions are calculated the net regionalization error is calculated for each basin using equation 5 5 δ r i t e r i t e i i where e r i t denotes the regional error in basin i using the regional function f ˆ t e i i is the local error which is calculated by using locally calibrated f on the same basin i deducting the local error from the regional error gives δ r i t which is the net regional error on basin i using the regional function f ˆ t assuming a given basin i that belongs to cluster c the aggregation phase is successful if the net regional error corresponding to the regional function f ˆ c is the minimum of all net regional errors equation 6 and the difference between the distribution of the local and regional errors is acceptable defining acceptability is relatively subjective and it depends on the scale the purpose of modeling and the requirement and limitations of the problem it is typically decided based on the expertise and experience of the modeler for example assume a hydrologic model used for flood prediction there should be some limitations on the maximum expected error of predictions as larger errors can highly affect the life and properties of people in addition the knowledge of the modeler about the structure of that specific model as well as his her experience about the potential range of errors in the literature of flood modeling provides some hint to select an appropriate margin for the regionalization error 6 m i n δ r i t t 1 2 m δ r i c the validation of the classification phase is performed by using the k fold cross validation technique where n basins are divided into k groups and each time the model is trained on k 1 groups and tested on the other group repeating this process for k times the model is tested on all n basins and the cross validation score is reported in addition to examining the performance of the classifier the k fold cross validation is used to select both the best features and classifiers among several feature extraction techniques used in machine learning the recursive feature elimination rfe is a well known technique linked to the classifier and initiates with all potential features the rfe eliminates the worst feature based on the importance of the features weights determined by the classifier and repeats this process to reach the optimum number of the features guyon et al 2002 for a given classifier the integration of the rfe with k fold cross validation provides a set of validation scores for different numbers of features repeating the entire process for several classifiers leads to the selection of the best classifier with its optimum features 4 framework application for probabilistic floodplain mapping in data scarce environments jafarzadegan and merwade 2019 proposed a statistical function named ϕ h a n d that can be used for a given basin to generate a probabilistic floodplain map the independent variable hand in the ϕ h a n d is a hydrogeomorphic feature defined as height above nearest drainage nobre et al 2011 rennó et al 2008 to create h a n d a digital elevation model dem and the stream network of the basin are needed jafarzadegan and merwade 2017 the ϕ function is derived from the gamma cumulative density function c d f g a m m a using equation 7 and can be directly calculated by equation 8 7 ϕ 1 c d f g a m m a 8 ϕ h a n d 1 1 τ k γ k h a n d θ where k and θ are the shape and scale parameters of ϕ function which should be estimated for each basin the τ a and γ a b are the complete and the lower incomplete gamma functions respectively equations 9 and 10 calculated as 9 τ a 0 x a 1 e x d x 10 γ a b 0 b x a 1 e x d x the optimum parameters of the ϕ function are determined by minimizing the error of a predicted flood extent compared to a reference floodplain map readers are referred to jafarzadegan and merwade 2019 for more details related to estimating the error in this study the flood insurance rate maps firms provided by the u s federal emergency management agency fema are used as reference maps fema firms were created using detailed field measurements and modeling for many areas in the u s and thus form a good basis for comparing results from other floodplain mapping efforts the ϕ h a n d function provides a simple and computationally efficient probabilistic floodplain mapping approach over large areas the development of ϕ h a n d through the calibration of its parameters depends on the availability of reference data thus estimating its parameters for areas with limited or no reference data poses the classical challenge of prediction in ungauged basins in other words the research question is how to transfer the calibrated ϕ functions to data scarce environments this question is addressed by applying the bhc ca framework to the arkansas white red region in the u s with highly variable topography and climate specifically the western part of this area is mountainous while the eastern and the southern parts are relatively flat to utilize the bhc ca framework 35 basins that have fema firms are selected the basins are selected to ensure the variability in geographic topographic and climatic conditions using the fema firms as reference floodplain maps the ϕ function is calibrated for all basins 4 1 clustering the data rich basins with bhc algorithm to cluster the data rich basins the proposed behavioral similarity measure is calculated for all possible pairs of basins and the similarity matrix is generated using this matrix and the average linkage method the agglomerative hierarchical clustering algorithm creates a dendrogram fig 4 the three horizontal dash lines provided on this figure show the potentially reasonable cutoffs that should be used to generate two three or four clusters respectively testing the smaller cutoffs closer to the leaves of the dendrogram is not necessary due to the high density of the basins at the lower levels of the dendrogram in addition generating fewer number of clusters as input to the classifier increases the chance of a successful classification therefore there should be an overall tendency toward selecting a lower number of clusters with the optimum clustering results to finalize the optimum number of clusters the silhouette index is calculated for all 35 basins and its distribution is plotted for three potential cases of clustering in fig 5 the higher mean of the silhouette index for two clusters red dash line as well as the preference for having the minimum number of clusters confirm that using two clusters is a reasonable decision for this problem the geographical locations of basins that belong to each class are displayed in fig 6 the map of clustered basins shows some geographical patterns that distinguish the western basins from the eastern ones these results strengthen the hypothesis that climatic and topographic characteristics are important factors that affect the floodplain mapping model in fig 7 the results of the hierarchical clustering using a common similarity measure namely euclidean on the model parameters are compared with the proposed behavioral similarity measure for two three and four clusters the parameter space in fig 7 shows that two basins namely basins 2 and 14 located in 0 24 12 93 and 2 91 0 65 are far away from others due to their high scale and shape parameters although these two basins have the highest distance in the parameter space the proposed behavioral similarity finds both basins similar and places them in one cluster fig 7a and b on the other hand using the euclidean distance these two basins are separated from the other basins fig 7d and e these results clearly show a considerable difference between the model parameter similarity and the behavioral similarity in fact it confirms that how relying on the model parameter similarity can adversely impact on the clustering results due to equifinality the proposed similarity measure considers this factor by running the ϕ function on other basins overall the completely different clustering results generated by each of these two measures explain the importance of selecting an appropriate similarity measure for a clustering problem 4 2 classification phase the next step of the bhc ca framework is to perform the classification phase based on the class labels provided by clustering before training the classifier the attributes should be calculated and preprocessed a collection of 25 basin descriptors related to the shape and location topography climate land use and hydrography of the basins are calculated for all 35 basins table 1 a python script is developed to calculate all these basin descriptors simultaneously using a 30 m horizontal resolution dem from the national elevation dataset ned a set of climate rasters average temperature average precipitation wettest month precipitation from worldclim global data the 2011 national land cover dataset the national hydrography dataset nhd flowlines and the nhd basin boundaries jafarzadegan et al 2018 a power transformation operator named yeo johnson transform is applied to transform the original data into normally distributed numbers the yeo johnson transformation is an extension of the box cox power family used for both positive and negative values weisberg 2001 in this study four commonly used classifiers namely decision tree dt random forest rf logistic regression lr and support vector machine svm are used for the classification phase the dt algorithms safavian and landgrebe 1991 are popular machine learning tools for classifying non linear problems where their tree like structure is used to split the complex samples into simple sub samples the rf algorithms breiman 2001 are the ensemble learning techniques that use a multitude of dts to correct the overfitting issue in dt algorithms the lr algorithms jr et al 2013 are well known for binary classification because they use a sigmoid function ranging from 0 to 1 to predict the probability of binary variables the svm algorithms cristianini and shawe taylor 2000 search for the optimum hyperplane which divides the decision space into separate parts similar to lr the svm algorithms were originally developed for binary classification although both of them can be used for multi classification as well in order to find the best classifier with the optimum features 7 fold cross validation is linked to the rfa algorithm at each fold a given classifier with a fixed number of features is trained on the labels of 30 basins provided by the hierarchical clustering and the labels are predicted for the remaining 5 test basins by averaging the validation score of the 7 folds the cross validation score is reported for all combinations of the classifier and features fig 8 presents the result of the cross validation and rfa method to explore an appropriate classifier and check its overall accuracy in fig 8a the svm with 10 features is shown as the best classifier with an accuracy of 86 the overall accuracy of the other three classifiers namely the dt lr and random forest rf with their optimum features are also reported as 82 80 and 78 respectively this shows that all four algorithms perform almost similar and the choice of the classifier is not that critical in this study in fig 8 b the cross validation score of each classifier with its optimum features is illustrated at each fold the dt has predicted all 5 test basins at four different folds however the overall accuracy of the svm is still higher than dt the high accuracy of svm 86 estimated with the cross validation of all 35 basins demonstrates the success of the classification phase in the bhc ca framework this also implies the acceptable performance of the hierarchical clustering and its new similarity measure because the high accuracy of the classification is dependent on the class labels generated by the clustering algorithm 4 3 aggregation phase after performing the clustering and classification of the 35 basins the locally calibrated ϕ functions are aggregated to form 2 new regional models denoted by ϕ 1 and ϕ 2 the approach of applying a new global calibration on the data aggregated from the same cluster is computationally expensive and almost impossible for this case study to provide a better estimation the local calibration is performed by a particle swarm optimization pso algorithm using millions of cells for each basin jafarzadegan and merwade 2019 because ϕ function can be easily derived from the cdf the aggregation process of ϕ functions can be considered as the aggregation of cdfs therefore a more straightforward approach is to sample data from all cdfs belonging to the same cluster pool the sampled data fit a regional cdf to the entire data and convert the regional cdf to a regional ϕ model fig 9 illustrates ϕ 1 and ϕ 2 the two regional models aggregated from 35 locally calibrated ϕ functions the figure shows that for example the probability of flooding for a given cell inside a basin with h a n d 2 m is estimated to be around 35 if the basin belongs to class 1 and the probability of flooding for this cell rises to a value around 60 if the basin belongs to class 2 ϕ 1 and ϕ 2 are the results of converting a 35 dimensional problem into 2 dimensions therefore for any basin inside the study area one of these two models can be used to generate the probabilistic floodplain map to validate the aggregation phase the local model along with the regional models ϕ 1 and ϕ 2 are used for all 35 basins fig 10 a presents the net regional errors for all basins it is expected that the minimum regional model for each basin matches with its cluster number in fig 6 for example since basin 1 belongs to class 1 in fig 6 the regional model ϕ 1 should provide the minimum error in fig 9 which is correct a one by one comparison for all basins indicates that all 35 basins have been aggregated correctly fig 10 b compares the distribution of the local errors with regional errors the small differences between the mean of these two distributions and the similar shape of their kernel densities also confirm the success of the aggregation phase 4 4 probabilistic floodplain mapping using bhc ca the trained svm along with the two regional models ϕ 1 and ϕ 2 can be used to generate probabilistic floodplain maps for any basin within the domain of the study area in fig 11 the application of the bhc ca framework for probabilistic floodplain mapping in data scarce environments is visually investigated a portion of the flood maps generated for a western and an eastern basin basins 19 and 33 respectively are visually compared with the reference fema maps as can be seen the bhc ca framework is able to generate accurate probabilistic floodplain maps for two completely different basins located in the mountainous and flat areas of the arkansas white red region the practical advantage of the bhc ca can be explained by the fact that more than 60 of the rivers in this region lack fema firms in addition the available fema firms are deterministic maps with a high degree of uncertainty the bhc ca framework is able to generate probabilistic floodplain maps for all basins located in this region the high efficiency of using bhc ca for floodplain mapping is another practical advantage especially because it does not sacrifice the accuracy to improve efficiency on average a computer desktop with core i7 3 6 ghz processor and 16 gb memory ram needs 8 12 h to calibrate the ϕ model and accordingly creates the probabilistic floodplain map for a basin on the other hand using two regional models ϕ 1 and ϕ 2 along with the trained svm the bhc ca framework creates probabilistic floodplain maps in less than 10 min with relatively similar accuracy 5 discussion the fundamental role of the donor basin selection in the success of a similarity based regionalization method is the main motivation for proposing the bhc ca framework in this study applying a systematic approach for finding an appropriate physical climatic similarity metric is of paramount importance in the regionalization problems a typical approach to the regionalization of hydrologic model parameters is to predetermine some physical climatic attributes and estimate the similarity of basins using the euclidean distance between those attributes two questions arise 1 whether those physical climatic attributes are the best descriptors of the hydrologic similarity 2 whether using equal weights for those attributes within a euclidean distance metric is appropriate the bhc ca framework targets these two questions by systematically creating an appropriate metric referred to as a trained classifier similar to the work conducted by brunner et al 2018 the bhc ca framework integrates the clustering classification and aggregation to reduce the subjectivity of the physical climatic similarity metric and accordingly the selection of donor basins this integration however creates a new subjective and challenging step where a response signature dis similarity measure should be determined for clustering the data rich basins in the first step in some fields such as hydrology and ecology the abundance and variety of response signatures proposed in different studies increases the subjectivity of the selection of appropriate indices olden and poff 2003 on the other hand in another set of problems such as floodplain mapping conducted in this manuscript the challenge of response signature selection is due to limited literature on regionalization because the aggregation of clustering with classification and aggregation should be applicable for a wide range of environmental models this study proposes bhc a behavioral hierarchical clustering algorithm that finds similar basins without needing to select the basin response signatures the bhc algorithm uses a similarity measure on the basis of model parameter transferability firstly introduced by oudin et al 2010 for behavioral similarity in hydrology in their study the overlap between the physically similar basins and hydrologically similar basins were investigated assuming that the behavioral similarity reflects the hydrological similarity while oudin et al 2010 only used the behavioral similarity for the purpose of validation in hydrology here we propose that the behavioral similarity concept and accordingly its similarity measure should be generalized for a broad range of environmental models and applied as an effective tool for the regionalization task in addition to reducing the subjectivity of donor basin selection using the behavioral similarity in the bhc ca is advantageous as its model transferability concept matches well with the overall idea of aggregation in the aggregation phase the local models of two basins are replaced with a regional model that can be used in both basins the behavioral similarity concept also defines two basins as similar if their models can be interchangeably used in both basins with negligible additional errors considering the fact that models are aggregated based on the results of bhc the compatibility of the criterion used by bhc with the concept of an aggregation phase is a valuable feature of bhc ca that leads to the reliable aggregation and regionalization of environmental model parameters to demonstrate the efficacy of the bhc ca framework as a generic approach to the regionalization of environmental model parameters we use this framework for a floodplain mapping problem where a statistical gamma distribution model is used to create the probabilistic maps the validation results show that svm can classify 86 of basins correctly it should be noted that the 86 accuracy can be interpreted as a validation score presenting the performance of the clustering classification and their connection the high accuracy of classification should be still viewed with caution because binary classifiers can randomly pose a 50 accuracy on average without any training it is also worth mentioning the small sample size of 35 basins in this regionalization study can affect the selection of the optimum number of clusters given the fact that cluster validity measures are usually biased towards suggesting two clusters rao and srinivas 2006 the possible sources of errors in the bhc ca arise from uncertainty in the clustering classification and aggregation algorithms and uncertainty in the basin descriptor calculations the linkage method used to merge the groups of data points in the hierarchical clustering algorithm can be one of the possible sources of clustering uncertainty while this study uses the average of similarity measure values using other linkage methods may provide some changes in the clustering results in future studies the sensitivity of the bhc ca framework to the linkage method in the hierarchical clustering should be evaluated the inherent uncertainty that exists in the classification algorithm and the averaging method in the aggregation phase are other potential sources of uncertainty uncertainty in the basin descriptor calculations originates from the uncertainty of input data and averaging the dem climate and land use rasters used to find the basin descriptors are all derived from remote sensing imagery which have their own level of uncertainties also we take the spatial average of all grids inside a basin to calculate the basin descriptor using one single value which is representative of a large heterogeneous basin may not properly reflect the basin characteristics and can cause additional errors in the final results 6 conclusion the bhc ca framework provides a systematic approach to the selection of appropriate donor basins in a similarity based regionalization typically a physical climatic similarity metric is used to identify the donor basins the main assumption in regionalization problems is that the basins determined as physically similar by this metric are functionally similar as well which is not always true by focusing on this assumption the proposed framework explores the optimum physical climatic similarity metric that represents the behavioral similarity of the basins the bhc ca framework uses a large number of data rich basins as input and provides a trained classifier as output referred to as a physical climatic similarity metric the trained classifier can identify the class label of a data scarce basin the data rich basins which have a class label similar to a data scarce basin are recognized as donor basins the physical climatic similarity metric explored by this framework is the result of a supervised scenario where the best mapping function for relating the attributes to the class labels are determined therefore if the class labels used for training the classifier are selected properly the final physical climatic similarity metric would be an appropriate metric as well to assure this we propose a new version of the hierarchical clustering algorithm on the basis of the behavioral similarity concept that generates the class labels of data rich basins as input to the classifier the behavioral similarity concept is formulated by defining a similarity measure that focuses on the model errors and parameter transferability by running locally calibrated models on other basins the efficacy of the proposed framework is tested by regionalizing a statistical gamma based function for probabilistic floodplain mapping results show that a trained svm with 10 physical climatic attributes and an accuracy of 86 is an appropriate physical climatic similarity metric for this problem the similar distribution of the model errors produced by the locally calibrated models compared to the errors produced by the regional models also indicates the acceptable performance of the aggregation phase in future studies the bhc ca framework can be applied for other regionalization purposes specifically this framework can be applied to hydrologic similarity based regionalization problems to determine a proper physical climatic similarity metric the capability of this framework for exploring an appropriate physical climatic similarity metric based on the behavioral similarity concept and its generic structure makes this approach an attractive solution to a range of regionalization problems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the editor and three anonymous reviewers of this manuscript for providing constructive comments to revise the manuscript we would like to acknowledge the federal emergency management agency fema for developing and providing access to the flood insurance rate maps the authors declare no competing financial interest appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104613 
26065,prediction in data scarce regions is one of the challenging issues in environmental problems in hydrology this issue is commonly addressed by utilizing regression or similarity based regionalization techniques the core of similarity based regionalization techniques is a physical climatic similarity metric that is typically predetermined from the knowledge about the physics of the problem and study area the purpose of this paper is to 1 reduce the subjectivity that exists in the selection of the physical climatic similarity metric by establishing a systematic approach and 2 propose a generic similarity based regionalization framework that estimates the parameters of environmental models in data scarce regions the efficacy of the proposed framework is evaluated for the regionalization of a statistical model that creates probabilistic floodplain maps in data scarce regions results show a trained support vector machine svm with ten basin descriptors and accuracy of 86 is an appropriate physical climatic similarity metric that creates reliable floodplain maps in the arkansas white red region keywords regionalization environmental models data scarce regions classification floodplain mapping machine learning 1 introduction simulation models are simplified representations of real world systems and are widely used to understand the behavior of a complex system devia et al 2015 silvert 2001 a key step in reliable simulation is model calibration that involves the proper estimation of model parameters such that the model generates outputs that match the reference data observations closely the existence of reference data at some points within the model domain is crucial for a successful calibration in environmental and water resources problems different types of models e g climate hydrologic hydraulic geomorphic and statistical models are developed and calibrated based on users needs and the problem at hand additionally these models are implemented at different spatial scales ranging from a few square meters to several thousand square kilometers as the spatial scale increases the heterogeneity in the physical characteristics of the area as well as the actual physical processes increases thus model calibration becomes challenging for large scale environmental models this is especially so complex in data scarce regions due to the absence of reference data for model calibration in hydrology this issue termed prediction in ungauged basins pub has gained a great deal of attention for the past several years hrachowitz et al 2013 sivapalan 2003 blöschl et al 2013 ungauged basins generally refer to basins with no available observed data specifically streamflow thus hydrologic models cannot be calibrated in ungauged basins and flow prediction will involve considerable uncertainty without a reliable calibration to make predictions in ungauged basins a large number of methods namely regionalization techniques have been proposed in the literature regionalization refers to all the methods used to transfer information from gauged basins to the ungauged ones by relating hydrologic phenomena to basin descriptors blöschl and sivapalan 1995 oudin et al 2010 young 2006 in hydrology two different types of regionalization exist one type of studies focus on the regionalization of streamflow statistics e g flood quantiles in regional flood frequency analysis rffa while the other type regionalizes the hydrologic model parameters for continuous streamflow prediction in ungauged basins although a few relevant studies in rffa are discussed in this paper the focus is on the second type as this study aims to propose a framework for the regionalization of model parameters please refer to he et al 2011 and razavi and coulibaly 2012 for a review of model parameter regionalization techniques among the techniques used for the regionalization of model parameters regression and similarity based methods have frequently been applied in regression based methods a hydrologic model is calibrated on a large number of gauged basins and the parameters of the hydrologic model are related to some of the basin descriptors by establishing multiple regression relationships these regression relationships can then be used to estimate the model parameters in an ungauged basin sefton and howarth 1998 tung et al 1997 despite their popularity in the last two decades regression based methods suffer from certain limitations fernandez et al 2000 gibbs et al 2012 hundecha and bárdossy 2004 kim and kaluarachchi 2008 lee et al 2006 merz and blöschl 2004 seibert 1999 specifically regression based methods assume that the model parameters are independent and that the error residuals are normally distributed but these assumptions do not hold in many cases mcintyre et al 2005 furthermore there might be considerable uncertainty in the model parameters due to the calibration method model structure and forcing uncertainty hamby 1994 jakeman et al 2006 matott et al 2009 spear et al 1994 therefore applying the optimum parameter set as a dependent variable in regression equations is not the best approach due to equifinality which is referred to as when multiple sets of parameters lead to similar model performance anderson et al 2001 beven and freer 2001 similarity based regionalization methods on the other hand search for one or a set of basins referred to as donor basins whose importance can be weighted according to a similarity measure from an ungauged basin referred to as the target basin then transfer the entire model parameter set of donor basins to the target basin the transfer of all model parameters as a set in similarity based methods is useful compared to traditional regression based methods which neglect the interdependencies of parameter sets mcintyre et al 2005 parajka et al 2005 kokkonen et al 2003 concluded that when there is a reason to believe that in the sense of hydrological behavior a gauged catchment resembles the ungauged catchment to a sufficient extent it is worthwhile to adapt the entire calibrated parameters from the gauged basin typically a similarity based method consists of two steps including 1 selecting the similar basins and 2 transferring the information from donor basins to the target basin notwithstanding the primary role of the donor basin selection in similarity based regionalization problems most studies have focused on the second step for advanced transferring methods that consider the model parameter uncertainties holmes et al 2002 kay et al 2007 masih et al 2010 mcintyre et al 2005 for example mcintyre et al 2005 introduced an extension of the generalized likelihood uncertainty estimation glue framework beven and binley 1992 for regionalization problems by proposing the use of a weighted average of donor basin simulation results the weights were defined by the product of a prior likelihood of a model and the relative likelihood of that model being applicable to the target basin kay et al 2007 proposed another weighted averaging method where the uncertainty of model calibration was taken into account during the parameter transposition despite these promising advances in transferring information from donor basins to target basins the methods proposed in these studies fall short in selecting the donor basins a proper selection of donor basins in the first step of similarity based regionalization problems increases the chance of reliable and robust prediction for target basins regardless of the complexity of the procedure used in the second step to properly determine the donor basins in a similarity based regionalization method two key questions need to be answered 1 which physical climatic descriptors of the basins should be selected to reflect the similarity 2 which similarity metric should be used to find the most appropriate donor basins typically a set of physical climatic characteristics of basins assuming that they are representative of hydrologic similarity are used within a predetermined similarity metric e g euclidean distance cosine distance or normalized absolute distance viviroli et al 2009 samuel et al 2011 parajka et al 2005 the subjectivity in the selection of these descriptors and the similarity metric is a critical research question one solution to reduce this subjectivity is to utilize a systematic two step approach so that first a reference dis similarity measure that is more reliable than a physical climatic similarity metric prinzio et al 2011 is used for the gauged basins and then the optimum physical climatic similarity metric is explored by mapping the reference dis similarity measure to the basin descriptor space samaniego et al 2010 brunner et al 2018 in this approach the reference dis similarity measure applied on the gauged basins plays a vital role in the selection of optimum physical climatic similarity metric and the overall regionalization performance using an appropriate reference dis similarity measure in the first step of this approach is still very challenging and subjective because it is calculated from one or a set of stream signatures which may not represent the hydrological behavior of gauged basins completely although the selection of appropriate streamflow signatures for representing the hydrological similarity in gauged basins has been widely discussed for regionalization studies in hydrology and for detecting the hydrologically homogenous areas in catchment classification problems yadav et al 2007 sawicz et al 2011 ali et al 2012 ssegane et al 2012 boscarello et al 2016 there is no consensus about the appropriate signatures to be used to reflect the hydrological similarity moreover since this study aims at extending the regionalization framework to other applications selecting the appropriate response signatures that represent the similarity of data rich basins can be more challenging in fields other than hydrology due to their limited literature on regionalization considering these issues this study proposes a generic framework with a behavioral based hierarchical clustering algorithm that does not need any response signature for clustering data rich basins in the first step the similarity measure embedded in this algorithm accounts for the behavioral similarity instead of attribute e g response signatures or physical characteristics similarities the behavioral similarity concept also referred to as hydrological similarity was introduced by oudin et al 2010 on the basis of model parameter transferability they used this concept to compare hydrologically similar basins based on physical climatic characteristics in this paper we use behavioral similarity as a generalization of the term hydrologic similarity and define it as similarity of all physical processes that transfer the environmental model inputs to the target outputs in a basin as mentioned earlier a hierarchical clustering algorithm that uses this behavioral concept within its structure is coupled with a classification algorithm to provide a systematic approach to the regionalization of environmental model parameters this framework overcomes the subjectivity issue at both levels of the response signature and physical climatic characteristic selections the proposed framework aims to extend the concept of regionalization beyond hydrology and provides a generic solution to the regionalization of various environmental model parameters it should be noted that the term environmental model is used without specifying its type to make the proposed framework applicable for different purposes and modeling types e g hydrologic hydraulic geomorphic water quality groundwater and statistical models the hydrologic behavior of a basin should be distinguished from the hydraulic geomorphic or any other behavior because the type and distribution of processes involved vary for example considering the scarcity of water quality variables as these data are relatively expensive and difficult to measure or the limited access to the suspended sediment monitoring stations worldwide the proposed framework can be an effective procedure for the regionalization of water quality models and rating curves of suspended sediments in data scarce regions slaughter et al 2017 tramblay et al 2010 while the water basins are the common computational unit of lumped hydrologic models the proposed framework can be used for other computational units as well for example a grid cell a river and an aquifer can be used as a computational unit for distributed hydrologic modeling sediment modeling and groundwater modeling respectively in groundwater modeling the proposed framework can be applied for regionalizing groundwater levels using a groundwater model and then the parameters of the model can be estimated for aquifers in data scarce regions where field data is limited or not available for model calibration here the application and effectiveness of the method are demonstrated by focusing on a probabilistic floodplain mapping problem where a statistical model already calibrated for several basins is transferred to data scarce basins 2 similarity from a general perspective a model denoted by f in equation 1 is a function of inputs and parameters to produce output s related to a specific domain of application 1 y f x θ where x and y denote the model inputs and outputs respectively and θ represents a vector of model parameters as mentioned earlier model f is a general term that can be used for a broad range of problems for example for streamflow prediction f refers to a hydrologic model where y is the streamflow time series and x is the precipitation data for a hydraulic model y denotes the water depth in a river while the geometry of river and streamflow information may be used as input x in data rich basins reference data is used to calibrate model f and determine its parameter set θ considering a problem where reference data is available for n basins model f can be calibrated for all n basins which results in n known vectors of θ in order to perform a successful regionalization the concept of similarity should be well understood similarity between two given basins can vary depending on the problem at hand for example two basins that show a similar hydrologic response to a rainfall event can present a different behavior in converting the streamflow to flood inundation areas assume that the similarity of two basins i and j with models y i f x i θ i and y j f x j θ j is desired here we introduce three types of similarity 1 physical climatic similarity 2 response signature similarity and 3 behavioral similarity the physical climatic similarity is calculated based on the similarity in some physical climatic characteristics of two basins as explained in the introduction the major concern about this similarity is the high subjectivity for the selection of appropriate physical climatic characteristics that reflect the actual similarity in basins however this similarity is the only available option if one or both basins are located in data scarce regions response signature similarity can be calculated by comparing some signatures calculated from measured response values in a basin this similarity metric is appropriate if the selected signatures reflect the entire process that occurs in the basin for example in order to define hydrological similarity among different basins a large number of streamflow signatures have been proposed in different studies yadav et al 2007 sawicz et al 2011 ali et al 2012 ssegane et al 2012 boscarello et al 2016 still subjectivity in the selection of appropriate response signatures is a concerning issue which would be exacerbated in other fields when there is not sufficient information about the selection of these signatures for example in the regionalization of the floodplain mapping model conducted in this study the target outputs are flood extent maps selecting the appropriate signatures that reflect the similarity in floodplains of two basins is not straightforward because each basin has a different stream network with distinct topographic features and floodplains are formed around streams in addition signatures should be able to represent the shape of flood extent areas the behavioral similarity explains that basins i and j are similar if the entire process y i f x i θ i is similar to y j f x j θ j in order to assess the similarity of the whole process a new similarity measure is defined so that basin i can be considered similar to basin j if the additional errors generated by running the calibrated model of basin i on the basin j and running the calibrated model of basin j on the basin i are negligible the behavioral similarity also outweighs the response signature similarity concept as it automatically considers both the inputs and responses simultaneously by running the model with new inputs and comparing the errors generated from cross simulations in addition the main advantage of behavioral similarity compared to the response signature similarity is that it does not need to deal with the selection of appropriate signatures but directly finds the similarities 3 the bhc ca framework the framework proposed in this study integrates a behavioral based hierarchical clustering algorithm bhc with classification c and aggregation phases a to systematically regionalize the environmental model parameters first the bhc establishes the regions of similar basins then the classification phase relates the physical climatic attributes to those similar regions and the aggregation phase assigns a regional model to each region while failure at each of these three steps can result in a poor regionalization it is worth to emphasize the pivotal role of clustering for exploring the regions of similar basins as can be seen in fig 1 the classification and aggregation phases are located at the same level underneath the clustering algorithm so that their performances are completely dependent on the results of clustering therefore it is fundamental to determine an appropriate approach to detecting similar basins within the clustering algorithm the bhc algorithm clusters similar data rich basins using a similarity measure based on the behavioral similarity concept section 3 1 the class labels of data rich basins determined by bhc are then used to perform the classification phase using a supervised scenario section 3 2 to select the most relevant physical climatic attributes and the appropriate classifier in the classification phase it is recommended to apply the feature extraction and transformation methods using a cross validation technique the best classifier trained on the selected physical climatic attributes maps the attributes to the class labels produced by bhc in the previous step the classification pattern detected by the classifier trained classifier will be the final physical climatic similarity metric which can be used for a data scarce basin to find its donor basins in addition to the trained classifier developed in the training step the aggregation phase is performed to create the regional models corresponding to each class of basins section 3 3 in the test step fig 1 data scarce basins are used as inputs and two estimated outputs generated in the training step namely the trained classifier and the regional models are used as processors to determine the regional models for data scarce basins 3 1 behavioral based hierarchical clustering bhc algorithm basin clustering aims to convert an n dimensional problem into m dimensions where n is the total number of data rich basins n basins with n calibrated models m refers to the number of classes m group of basins with m calibrated models and m n agglomerative hierarchical clustering rokach and maimon 2005 a well known algorithm for grouping data points can be applied to cluster all similar basins into one class in this algorithm pairwise comparisons are applied among all data rich basins and a multi level hierarchy named a dendrogram is created the readers are referred to kaufman and rousseeuw 2009 and ward 1963 for more information about hierarchical clustering the essential component of this clustering algorithm is the pairwise comparison step where the similarity of two basins is evaluated and two basins of the pair with the highest similarity are joined gibert et al 2010 gross et al 2010 wilcke and bärring 2016 various similarity measures have been introduced to decide the priority for joining the most similar data points in the hierarchy these similarity measures can be calculated based on either physical climatic or response signature attributes in hierarchical clustering algorithms a matrix of dis similarity values is required to create the dendrogram and cluster the data points the similarity matrix is typically calculated by inputting a matrix of attributes to a similarity metric e g euclidean metric the main advantage of the bhc is that it directly calculates the similarity matrix without the need for the attributes or the similarity metrics according to this method a similarity measure is proposed to reflect the behavioral similarity between basins i and j using equations 2 3 and 4 below 2 δ i j e i j e i i 3 δ j i e j i e j j 4 d i j d j i δ i j δ j i 2 where e i i or e j j refers to the existing error in basin i j when the model is calibrated on the same basin i j this is the error e g rmse value reported after the calibration of a hydrologic model that exists in all models when the calibration does not result in a perfect match between model results and reference data e i j or e j i referred to as cross modeling error is the error when the model calibrated from basin j i is applied to basin i j δ i j or δ j i referred to as the net error is the difference between the cross modeling and existing error in basin i j d i j or d j i is the similarity measure between basins i and j this measure reflects the dissimilarity between two basins as higher values show fewer similarities the behavioral similarity measure clusters the basins on the basis of model parameter transferability oudin et al 2010 this measure is model dependent and it is assumed that the model used to calculate the behavioral measure can simulate the existing processes that occur in the basin accepting this assumption and considering the fact that this measure is calculated by cross modeling it is implicitly inferred that the measure reflects the similarity in processes denoted as the behavioral similarity one of the critical research questions in clustering is to decide the optimum number of clusters this number is commonly determined by optimizing the performance of the clustering algorithm using a so called cluster validation index cvi sevilla villanueva et al 2016 in this study the silhouette index is used due to its successful performance in validating the clustering algorithms rousseeuw 1987 arbelaitz et al 2013 before optimizing the silhouette index for finding the suitable number of clusters a visual presentation of hierarchical clustering by dendrogram is a useful approach for interpreting distances at different stages of the hierarchy and creating a list of the potential optimum clustering numbers the main limitation of the proposed similarity measure is that it can only estimate the similarity between two data rich basins however to recognize the donor basins for a target basin the similarity between the target basin and data rich basins should be determined to overcome this issue a classification phase is linked to the hierarchical clustering algorithm where the class labels of data rich basins generated by the hierarchical clustering algorithm are fed into a classifier the trained classifier can later be used as an appropriate similarity metric to estimate the similarity of data rich basins with the target basin 3 2 classification phase the classification phase consists of preprocessing e g feature extraction and transformations and supervised learning first the potential attributes including a list of all physical climatic basin descriptors that may be effective on the classification are calculated second the attributes are normalized standardized the attributes are either rescaled into a range of 0 1 normalization or their values are rescaled so that they will have the properties of gaussian distribution with a mean of zero and unit standard deviation standardization then feature extraction techniques are applied to select the significant attributes for classification in supervised learning techniques a classifier is trained on data with available class labels referred to as the training step and then it is used for predicting the class labels of unknown data referred to as the test step a schematic diagram of the supervised learning algorithms used in the bhc ca framework is illustrated in fig 2 let x and y represent the attribute and class label matrices for n data rich basins where x i j and y i represent the jth attribute for the ith basin and the class label of ith basin respectively i 1 2 n and j 1 2 k in the training step a classifier finds the best fit function for relating attributes x to class labels y the fit function or trained classifier in the proposed framework is a physical climatic similarity metric used for estimating the class label of basins in data scarce environments the performance of a classifier is highly dependent on the reliability of training class labels y in the bhc ca framework these class labels are produced by the hierarchical clustering algorithm this shows the critical role of the hierarchical clustering algorithm and its similarity measure on the overall performance of the classifier see fig 1 3 3 aggregation phase the hierarchical clustering algorithm reduces an n dimensional problem into m dimensions by partitioning n basins into m groups to complete the process of dimension reduction the n calibrated models corresponding to the n basins f 1 f 2 f n should also be reduced to m regional models f ˆ t t 1 2 m the aggregation phase creates a regional model for each cluster by aggregating the calibrated models of all basins that belong to that cluster a theoretically reasonable approach is to perform a global calibration for each cluster trying to minimize the total error corresponding to the difference of the simulated outputs and observed targets for all basins within a cluster however this method is rarely applied in practice due to the limitation of calibration methods for optimizing complex decision spaces that include a cluster of different datasets a practical solution for creating the regional models is to directly combine all models that belong to the same cluster using either the model parameter averaging or output averaging oudin et al 2008 it is worth mentioning the output averaging is only applied if the outputs are at the same scales otherwise the model parameter averaging should be used in the cases where the models are statistical distributions e g this case study it is also reasonable to sample data from all models that belong to the same cluster and fit a new distribution function on the pooled data ilorme and griffis 2013 despite the type of method used for the aggregation validating the aggregation phase is warranted 3 4 validation of bhc ca framework to validate the performance of the bhc ca first the efficacy of the aggregation phase is investigated by applying both local and regional models on each data rich basin then the performance of the classification phase is evaluated using an extensive cross validation technique it should be noted that both of these validation steps examine the performance of clustering and its relationship with the classification and aggregation phases implicitly because the inputs to the classification and aggregation phases are derived from the results of the clustering step in the bhc ca framework please refer to the position of the clustering step in fig 1 which is above the classification and aggregation phases in order to validate the aggregation phase m outputs y 1 y 2 y m are generated for each basin using f ˆ t t 1 2 m considering the available reference outputs for these basins the error of predictions corresponding to f ˆ functions are calculated the net regionalization error is calculated for each basin using equation 5 5 δ r i t e r i t e i i where e r i t denotes the regional error in basin i using the regional function f ˆ t e i i is the local error which is calculated by using locally calibrated f on the same basin i deducting the local error from the regional error gives δ r i t which is the net regional error on basin i using the regional function f ˆ t assuming a given basin i that belongs to cluster c the aggregation phase is successful if the net regional error corresponding to the regional function f ˆ c is the minimum of all net regional errors equation 6 and the difference between the distribution of the local and regional errors is acceptable defining acceptability is relatively subjective and it depends on the scale the purpose of modeling and the requirement and limitations of the problem it is typically decided based on the expertise and experience of the modeler for example assume a hydrologic model used for flood prediction there should be some limitations on the maximum expected error of predictions as larger errors can highly affect the life and properties of people in addition the knowledge of the modeler about the structure of that specific model as well as his her experience about the potential range of errors in the literature of flood modeling provides some hint to select an appropriate margin for the regionalization error 6 m i n δ r i t t 1 2 m δ r i c the validation of the classification phase is performed by using the k fold cross validation technique where n basins are divided into k groups and each time the model is trained on k 1 groups and tested on the other group repeating this process for k times the model is tested on all n basins and the cross validation score is reported in addition to examining the performance of the classifier the k fold cross validation is used to select both the best features and classifiers among several feature extraction techniques used in machine learning the recursive feature elimination rfe is a well known technique linked to the classifier and initiates with all potential features the rfe eliminates the worst feature based on the importance of the features weights determined by the classifier and repeats this process to reach the optimum number of the features guyon et al 2002 for a given classifier the integration of the rfe with k fold cross validation provides a set of validation scores for different numbers of features repeating the entire process for several classifiers leads to the selection of the best classifier with its optimum features 4 framework application for probabilistic floodplain mapping in data scarce environments jafarzadegan and merwade 2019 proposed a statistical function named ϕ h a n d that can be used for a given basin to generate a probabilistic floodplain map the independent variable hand in the ϕ h a n d is a hydrogeomorphic feature defined as height above nearest drainage nobre et al 2011 rennó et al 2008 to create h a n d a digital elevation model dem and the stream network of the basin are needed jafarzadegan and merwade 2017 the ϕ function is derived from the gamma cumulative density function c d f g a m m a using equation 7 and can be directly calculated by equation 8 7 ϕ 1 c d f g a m m a 8 ϕ h a n d 1 1 τ k γ k h a n d θ where k and θ are the shape and scale parameters of ϕ function which should be estimated for each basin the τ a and γ a b are the complete and the lower incomplete gamma functions respectively equations 9 and 10 calculated as 9 τ a 0 x a 1 e x d x 10 γ a b 0 b x a 1 e x d x the optimum parameters of the ϕ function are determined by minimizing the error of a predicted flood extent compared to a reference floodplain map readers are referred to jafarzadegan and merwade 2019 for more details related to estimating the error in this study the flood insurance rate maps firms provided by the u s federal emergency management agency fema are used as reference maps fema firms were created using detailed field measurements and modeling for many areas in the u s and thus form a good basis for comparing results from other floodplain mapping efforts the ϕ h a n d function provides a simple and computationally efficient probabilistic floodplain mapping approach over large areas the development of ϕ h a n d through the calibration of its parameters depends on the availability of reference data thus estimating its parameters for areas with limited or no reference data poses the classical challenge of prediction in ungauged basins in other words the research question is how to transfer the calibrated ϕ functions to data scarce environments this question is addressed by applying the bhc ca framework to the arkansas white red region in the u s with highly variable topography and climate specifically the western part of this area is mountainous while the eastern and the southern parts are relatively flat to utilize the bhc ca framework 35 basins that have fema firms are selected the basins are selected to ensure the variability in geographic topographic and climatic conditions using the fema firms as reference floodplain maps the ϕ function is calibrated for all basins 4 1 clustering the data rich basins with bhc algorithm to cluster the data rich basins the proposed behavioral similarity measure is calculated for all possible pairs of basins and the similarity matrix is generated using this matrix and the average linkage method the agglomerative hierarchical clustering algorithm creates a dendrogram fig 4 the three horizontal dash lines provided on this figure show the potentially reasonable cutoffs that should be used to generate two three or four clusters respectively testing the smaller cutoffs closer to the leaves of the dendrogram is not necessary due to the high density of the basins at the lower levels of the dendrogram in addition generating fewer number of clusters as input to the classifier increases the chance of a successful classification therefore there should be an overall tendency toward selecting a lower number of clusters with the optimum clustering results to finalize the optimum number of clusters the silhouette index is calculated for all 35 basins and its distribution is plotted for three potential cases of clustering in fig 5 the higher mean of the silhouette index for two clusters red dash line as well as the preference for having the minimum number of clusters confirm that using two clusters is a reasonable decision for this problem the geographical locations of basins that belong to each class are displayed in fig 6 the map of clustered basins shows some geographical patterns that distinguish the western basins from the eastern ones these results strengthen the hypothesis that climatic and topographic characteristics are important factors that affect the floodplain mapping model in fig 7 the results of the hierarchical clustering using a common similarity measure namely euclidean on the model parameters are compared with the proposed behavioral similarity measure for two three and four clusters the parameter space in fig 7 shows that two basins namely basins 2 and 14 located in 0 24 12 93 and 2 91 0 65 are far away from others due to their high scale and shape parameters although these two basins have the highest distance in the parameter space the proposed behavioral similarity finds both basins similar and places them in one cluster fig 7a and b on the other hand using the euclidean distance these two basins are separated from the other basins fig 7d and e these results clearly show a considerable difference between the model parameter similarity and the behavioral similarity in fact it confirms that how relying on the model parameter similarity can adversely impact on the clustering results due to equifinality the proposed similarity measure considers this factor by running the ϕ function on other basins overall the completely different clustering results generated by each of these two measures explain the importance of selecting an appropriate similarity measure for a clustering problem 4 2 classification phase the next step of the bhc ca framework is to perform the classification phase based on the class labels provided by clustering before training the classifier the attributes should be calculated and preprocessed a collection of 25 basin descriptors related to the shape and location topography climate land use and hydrography of the basins are calculated for all 35 basins table 1 a python script is developed to calculate all these basin descriptors simultaneously using a 30 m horizontal resolution dem from the national elevation dataset ned a set of climate rasters average temperature average precipitation wettest month precipitation from worldclim global data the 2011 national land cover dataset the national hydrography dataset nhd flowlines and the nhd basin boundaries jafarzadegan et al 2018 a power transformation operator named yeo johnson transform is applied to transform the original data into normally distributed numbers the yeo johnson transformation is an extension of the box cox power family used for both positive and negative values weisberg 2001 in this study four commonly used classifiers namely decision tree dt random forest rf logistic regression lr and support vector machine svm are used for the classification phase the dt algorithms safavian and landgrebe 1991 are popular machine learning tools for classifying non linear problems where their tree like structure is used to split the complex samples into simple sub samples the rf algorithms breiman 2001 are the ensemble learning techniques that use a multitude of dts to correct the overfitting issue in dt algorithms the lr algorithms jr et al 2013 are well known for binary classification because they use a sigmoid function ranging from 0 to 1 to predict the probability of binary variables the svm algorithms cristianini and shawe taylor 2000 search for the optimum hyperplane which divides the decision space into separate parts similar to lr the svm algorithms were originally developed for binary classification although both of them can be used for multi classification as well in order to find the best classifier with the optimum features 7 fold cross validation is linked to the rfa algorithm at each fold a given classifier with a fixed number of features is trained on the labels of 30 basins provided by the hierarchical clustering and the labels are predicted for the remaining 5 test basins by averaging the validation score of the 7 folds the cross validation score is reported for all combinations of the classifier and features fig 8 presents the result of the cross validation and rfa method to explore an appropriate classifier and check its overall accuracy in fig 8a the svm with 10 features is shown as the best classifier with an accuracy of 86 the overall accuracy of the other three classifiers namely the dt lr and random forest rf with their optimum features are also reported as 82 80 and 78 respectively this shows that all four algorithms perform almost similar and the choice of the classifier is not that critical in this study in fig 8 b the cross validation score of each classifier with its optimum features is illustrated at each fold the dt has predicted all 5 test basins at four different folds however the overall accuracy of the svm is still higher than dt the high accuracy of svm 86 estimated with the cross validation of all 35 basins demonstrates the success of the classification phase in the bhc ca framework this also implies the acceptable performance of the hierarchical clustering and its new similarity measure because the high accuracy of the classification is dependent on the class labels generated by the clustering algorithm 4 3 aggregation phase after performing the clustering and classification of the 35 basins the locally calibrated ϕ functions are aggregated to form 2 new regional models denoted by ϕ 1 and ϕ 2 the approach of applying a new global calibration on the data aggregated from the same cluster is computationally expensive and almost impossible for this case study to provide a better estimation the local calibration is performed by a particle swarm optimization pso algorithm using millions of cells for each basin jafarzadegan and merwade 2019 because ϕ function can be easily derived from the cdf the aggregation process of ϕ functions can be considered as the aggregation of cdfs therefore a more straightforward approach is to sample data from all cdfs belonging to the same cluster pool the sampled data fit a regional cdf to the entire data and convert the regional cdf to a regional ϕ model fig 9 illustrates ϕ 1 and ϕ 2 the two regional models aggregated from 35 locally calibrated ϕ functions the figure shows that for example the probability of flooding for a given cell inside a basin with h a n d 2 m is estimated to be around 35 if the basin belongs to class 1 and the probability of flooding for this cell rises to a value around 60 if the basin belongs to class 2 ϕ 1 and ϕ 2 are the results of converting a 35 dimensional problem into 2 dimensions therefore for any basin inside the study area one of these two models can be used to generate the probabilistic floodplain map to validate the aggregation phase the local model along with the regional models ϕ 1 and ϕ 2 are used for all 35 basins fig 10 a presents the net regional errors for all basins it is expected that the minimum regional model for each basin matches with its cluster number in fig 6 for example since basin 1 belongs to class 1 in fig 6 the regional model ϕ 1 should provide the minimum error in fig 9 which is correct a one by one comparison for all basins indicates that all 35 basins have been aggregated correctly fig 10 b compares the distribution of the local errors with regional errors the small differences between the mean of these two distributions and the similar shape of their kernel densities also confirm the success of the aggregation phase 4 4 probabilistic floodplain mapping using bhc ca the trained svm along with the two regional models ϕ 1 and ϕ 2 can be used to generate probabilistic floodplain maps for any basin within the domain of the study area in fig 11 the application of the bhc ca framework for probabilistic floodplain mapping in data scarce environments is visually investigated a portion of the flood maps generated for a western and an eastern basin basins 19 and 33 respectively are visually compared with the reference fema maps as can be seen the bhc ca framework is able to generate accurate probabilistic floodplain maps for two completely different basins located in the mountainous and flat areas of the arkansas white red region the practical advantage of the bhc ca can be explained by the fact that more than 60 of the rivers in this region lack fema firms in addition the available fema firms are deterministic maps with a high degree of uncertainty the bhc ca framework is able to generate probabilistic floodplain maps for all basins located in this region the high efficiency of using bhc ca for floodplain mapping is another practical advantage especially because it does not sacrifice the accuracy to improve efficiency on average a computer desktop with core i7 3 6 ghz processor and 16 gb memory ram needs 8 12 h to calibrate the ϕ model and accordingly creates the probabilistic floodplain map for a basin on the other hand using two regional models ϕ 1 and ϕ 2 along with the trained svm the bhc ca framework creates probabilistic floodplain maps in less than 10 min with relatively similar accuracy 5 discussion the fundamental role of the donor basin selection in the success of a similarity based regionalization method is the main motivation for proposing the bhc ca framework in this study applying a systematic approach for finding an appropriate physical climatic similarity metric is of paramount importance in the regionalization problems a typical approach to the regionalization of hydrologic model parameters is to predetermine some physical climatic attributes and estimate the similarity of basins using the euclidean distance between those attributes two questions arise 1 whether those physical climatic attributes are the best descriptors of the hydrologic similarity 2 whether using equal weights for those attributes within a euclidean distance metric is appropriate the bhc ca framework targets these two questions by systematically creating an appropriate metric referred to as a trained classifier similar to the work conducted by brunner et al 2018 the bhc ca framework integrates the clustering classification and aggregation to reduce the subjectivity of the physical climatic similarity metric and accordingly the selection of donor basins this integration however creates a new subjective and challenging step where a response signature dis similarity measure should be determined for clustering the data rich basins in the first step in some fields such as hydrology and ecology the abundance and variety of response signatures proposed in different studies increases the subjectivity of the selection of appropriate indices olden and poff 2003 on the other hand in another set of problems such as floodplain mapping conducted in this manuscript the challenge of response signature selection is due to limited literature on regionalization because the aggregation of clustering with classification and aggregation should be applicable for a wide range of environmental models this study proposes bhc a behavioral hierarchical clustering algorithm that finds similar basins without needing to select the basin response signatures the bhc algorithm uses a similarity measure on the basis of model parameter transferability firstly introduced by oudin et al 2010 for behavioral similarity in hydrology in their study the overlap between the physically similar basins and hydrologically similar basins were investigated assuming that the behavioral similarity reflects the hydrological similarity while oudin et al 2010 only used the behavioral similarity for the purpose of validation in hydrology here we propose that the behavioral similarity concept and accordingly its similarity measure should be generalized for a broad range of environmental models and applied as an effective tool for the regionalization task in addition to reducing the subjectivity of donor basin selection using the behavioral similarity in the bhc ca is advantageous as its model transferability concept matches well with the overall idea of aggregation in the aggregation phase the local models of two basins are replaced with a regional model that can be used in both basins the behavioral similarity concept also defines two basins as similar if their models can be interchangeably used in both basins with negligible additional errors considering the fact that models are aggregated based on the results of bhc the compatibility of the criterion used by bhc with the concept of an aggregation phase is a valuable feature of bhc ca that leads to the reliable aggregation and regionalization of environmental model parameters to demonstrate the efficacy of the bhc ca framework as a generic approach to the regionalization of environmental model parameters we use this framework for a floodplain mapping problem where a statistical gamma distribution model is used to create the probabilistic maps the validation results show that svm can classify 86 of basins correctly it should be noted that the 86 accuracy can be interpreted as a validation score presenting the performance of the clustering classification and their connection the high accuracy of classification should be still viewed with caution because binary classifiers can randomly pose a 50 accuracy on average without any training it is also worth mentioning the small sample size of 35 basins in this regionalization study can affect the selection of the optimum number of clusters given the fact that cluster validity measures are usually biased towards suggesting two clusters rao and srinivas 2006 the possible sources of errors in the bhc ca arise from uncertainty in the clustering classification and aggregation algorithms and uncertainty in the basin descriptor calculations the linkage method used to merge the groups of data points in the hierarchical clustering algorithm can be one of the possible sources of clustering uncertainty while this study uses the average of similarity measure values using other linkage methods may provide some changes in the clustering results in future studies the sensitivity of the bhc ca framework to the linkage method in the hierarchical clustering should be evaluated the inherent uncertainty that exists in the classification algorithm and the averaging method in the aggregation phase are other potential sources of uncertainty uncertainty in the basin descriptor calculations originates from the uncertainty of input data and averaging the dem climate and land use rasters used to find the basin descriptors are all derived from remote sensing imagery which have their own level of uncertainties also we take the spatial average of all grids inside a basin to calculate the basin descriptor using one single value which is representative of a large heterogeneous basin may not properly reflect the basin characteristics and can cause additional errors in the final results 6 conclusion the bhc ca framework provides a systematic approach to the selection of appropriate donor basins in a similarity based regionalization typically a physical climatic similarity metric is used to identify the donor basins the main assumption in regionalization problems is that the basins determined as physically similar by this metric are functionally similar as well which is not always true by focusing on this assumption the proposed framework explores the optimum physical climatic similarity metric that represents the behavioral similarity of the basins the bhc ca framework uses a large number of data rich basins as input and provides a trained classifier as output referred to as a physical climatic similarity metric the trained classifier can identify the class label of a data scarce basin the data rich basins which have a class label similar to a data scarce basin are recognized as donor basins the physical climatic similarity metric explored by this framework is the result of a supervised scenario where the best mapping function for relating the attributes to the class labels are determined therefore if the class labels used for training the classifier are selected properly the final physical climatic similarity metric would be an appropriate metric as well to assure this we propose a new version of the hierarchical clustering algorithm on the basis of the behavioral similarity concept that generates the class labels of data rich basins as input to the classifier the behavioral similarity concept is formulated by defining a similarity measure that focuses on the model errors and parameter transferability by running locally calibrated models on other basins the efficacy of the proposed framework is tested by regionalizing a statistical gamma based function for probabilistic floodplain mapping results show that a trained svm with 10 physical climatic attributes and an accuracy of 86 is an appropriate physical climatic similarity metric for this problem the similar distribution of the model errors produced by the locally calibrated models compared to the errors produced by the regional models also indicates the acceptable performance of the aggregation phase in future studies the bhc ca framework can be applied for other regionalization purposes specifically this framework can be applied to hydrologic similarity based regionalization problems to determine a proper physical climatic similarity metric the capability of this framework for exploring an appropriate physical climatic similarity metric based on the behavioral similarity concept and its generic structure makes this approach an attractive solution to a range of regionalization problems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the editor and three anonymous reviewers of this manuscript for providing constructive comments to revise the manuscript we would like to acknowledge the federal emergency management agency fema for developing and providing access to the flood insurance rate maps the authors declare no competing financial interest appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104613 
26066,regression models e g land use regression are currently the most popular way to estimate retrospective exposures to air pollution however these models lack important features of atmospheric dispersion we developed a new non linear air quality regression model which is based on the physical grounds of the well established and commonly applied gaussian dispersion model this was achieved through parametrization of the basic gaussian model including its standard deviations and optimizing the parameters to provide a least squares fit with ambient measurements at each individual time point the new model gaussodm outperformed both a simpler regression model and a benchmark interpolation model in predicting spatial ambient nitrogen oxides nox concentrations the gaussodm enables a deeper understanding of the relationship between air pollution and adverse health effects this is partly because it is better adapted at incorporating meteorological data and the effects of elevated emissions compared with previously available air pollution regression models keywords air quality modelling data assimilation exposure assessment gaussian dispersion nitrogen oxides regression models software and data availability the gaussodm was applied as a matlab program using matlab r2017a available since 2017 the program together with the input data we used to produce the results presented in this publication are available for download at this link figshare com s 4c27dc3758f680910130 250 mb download at least 70 gb of ram are required to run the model a cpu with at least 12 cores is recommended we ran the model on a lenovo thinkstation p910 with a dual intel xeon e5 2603 v4 processor 15mb cache 1 7 ghz and 512gb of 2400mhz ecc rdimm ram on this system simulating a full year at a half hourly temporal resolution took 1 week 1 introduction exposure to the complex mixture of emissions from the combustion of carbon based fuels has been linked to a variety of adverse health effects claxton 2015 guarnieri and balmes 2014 manzetti and andersen 2016 of the extensively monitored criteria pollutants nitrogen oxides no x have the best correlation with various combustion by products including ultrafine particles and carbonaceous particulate matter hamra et al 2015 minguillón et al 2012 thus no x concentrations are often used as an indicator of exposure to harmful combustion by products in epidemiological studies nyberg et al 2000 hamra et al 2015 specifically exposure to no x has been linked with cardiorespiratory mortality asthma and various cancers jacquemin et al 2015 newell et al 2018 cohen et al 2019 however better individualized exposure assessment is required to reduce the uncertainties in the effect size of established links and to determine if additional links exist hamra et al 2015 wolf et al 2017 weisskopf and webster 2017 exposure to ambient no x can be assessed through either direct personal measurements or modelling hannam et al 2013 steinle et al 2013 the modelling approach is appealing due to its scalability to large cohorts and the ability to produce retrospective exposure estimates zou et al 2009 hannam et al 2013 the two main types of air quality models are mechanistic models and regression models holloway et al 2005 shahraiyni and sodoudi 2016 mechanist models such as adms and chimere incorporate many of the important features of atmospheric dispersion and can provide high resolution air quality assessment riviere et al 2019 however they require very detailed highly accurate and often unavailable three dimensional meteorology and emissions input data in order to provide reasonable results brauer 2012 tartakovsky et al 2013 de hoogh et al 2014 therefore regression models are more often applied for personalized exposure assessment hoek et al 2008 xie et al 2017 the most common type of regression is linear land use regression lur in this approach a set often 40 100 of geographical variables which are deemed to be related to air quality are identified and undergo a selection process as explanatory variables in the regression model hoek et al 2008 once the model parameters or coefficients are selected and fitted to the data at the monitoring locations they can be used to predict concentration at any other location in which the geographical variables are known and at a high spatial resolution down to a few tens of metres this approach was previously shown to perform well in cross validation procedures hoek et al 2008 however lur models are usually only applicable on a coarse temporal resolution most commonly for annual exposure this is partly due to the low temporal coverage of the dedicated lur monitoring campaigns usually only a few representative weeks from a year are chosen however it is also limited by fundamental properties of commonly applied lur models such as the use of static explanatory variables and the lack of atmospheric dispersion principles within the lur scheme hoek et al 2008 wilton 2011 tunno et al 2016 retrospective exposure assessment at a high hourly temporal resolution is necessary for some epidemiological applications such as those incorporating time activity patterns or examining short term effects of air pollution hoek et al 2008 batterman et al 2014 shafran nathan et al 2017 rasche et al 2018 some regression models achieved a high temporal resolution by incorporating the predictions of a mechanistic model as an additional covariate within the lur scheme michanowicz et al 2016 cordioli et al 2017 de hoogh et al 2018 however this approach still requires the costly implementation of a mechanistic model other studies incorporated temporal modulations of the annual lur predictions based on continuous regulatory monitoring or satellite data novotny et al 2011 johnson et al 2013 dons et al 2014 anand and monks 2017 cordioli et al 2017 however this approach is limited in its ability to capture the temporal changes in the spatial pollutant patterns a different approach is to incorporate some factors responsible for the temporal variability of ambient no x directly into the regression model the two main relevant factors are emissions and meteorology dobson 1997 liu et al 2012 ducret stich et al 2013 ai et al 2018 the incorporation of temporally resolved emission data has been achieved recently both for traffic chen et al 2016 matikolaei et al 2017 son et al 2018 and industrial no x emissions chen et al 2018 some meteorological parameters such as wind speed and solar radiation can be incorporated in linear lur models as was done in some studies kim and guldmann 2015 liu et al 2015 cheewinsiriwat 2016 however in order to incorporate the wind direction the model form must be adjusted lur models typically assign the values of the explanatory variables based on isotropic circular buffers around the receptor points hoek et al 2008 when the wind direction is incorporated into these linear models it is usually by replacing the circular buffers with wind directed wedges or by weighing the explanatory variables based on a wind rose arain et al 2007 su et al 2008 kim and guldmann 2011 li et al 2015 shi et al 2017 naughton et al 2018 however it is well established that pollution levels change non linearly with distance from the source naser et al 2009 bell and ashenden 1997 yuval et al 2013 presented a novel approach in which a non linear regression model was formulated incorporating key dispersion properties in a heuristic framework this optimized dispersion model odm out performed inverse distance weighing idw interpolation lur and chimere simulations yuval et al 2017 the formulation of the odm was extended to incorporate industrial emissions in addition to traffic by adding a term which was based on vertical gaussian dispersion chen et al 2018 exposure estimates made using the odm were especially useful for pregnancy outcomes epidemiology since particular stages of foetal development are of interest raz et al 2018 kioumourtzoglou et al 2019 these previous works demonstrated the potential of more complex regression models which incorporate physicochemical processes that are the basis of atmospheric dispersion in this work we extend this idea further by formulating a complete gaussian form of the odm which can be used for retrospective exposure assessment 2 methods 2 1 regression model the gaussian plume is an approximate analytical solution of the challenging system of differential equations that describe the pollutant concentration due to dispersion processes 1 c ˆ x y z q 2 π σ y σ z u exp y 2 2 σ y 2 exp z h 2 2 σ z 2 exp z h 2 2 σ z 2 where c ˆ is the modelled concentration at a given location q is the rate of pollutant emission from a point source that is located at the origin of the coordinate system x y and z are the downwind crosswind and vertical distances from the source respectively u is the time averaged wind speed at the effective height of release h along the x axis with the coordinate system rotated accordingly and σ y and σ z represent the standard deviations of the crosswind and vertical gaussian distribution of the pollutant concentration respectively the effective height of release is given by 2 h h s δ h were h s is the elevation of the point of emission and δ h is the plume rise the gaussian plume model is based on many assumptions which include a homogeneously turbulent wind field steady state air flow a steady state point pollution source and complete reflection of the pollutant from the ground leelőssy et al 2014 it is the basis of several popular mechanistic air pollution models such as adms and aermod heist et al 2013 we parametrized the gaussian plume such that it can be used as a non linear regression model in our study area section 2 3 the emission rates are not known with absolute certainty and only conservative high end estimates are available from regulatory bodies assuming that the relative errors in the emission rates are similar for all sources the actual emission rates are modelled to be α q where 0 α 1 such that the ground level z 0 concentration can be written as 3 c ˆ α q π σ y σ z u exp y 2 2 σ y 2 exp h 2 2 σ z 2 many schemes exist for the parametrization of the dispersion standard deviations due to its common usage seinfeld and pandis 2016 mathematical simplicity and relatively low number of parameters needed power law relationships with the downwind distance from the source were chosen 4 σ y x p 7 2 p 6 σ z x p 9 2 p 8 where p 6 9 are optimization parameters substituting these terms into eq 3 results in the following model 5 c ˆ 2 α q p 6 p 8 π u x p 7 p 9 e x p p 6 y 2 x 2 p 7 e x p p 8 h 2 x 2 p 9 since α is not known at any given time point and in order to reduce computational difficulties in calculating the model s derivatives the term 2 α p 6 p 8 π was replaced with a new optimization parameter p 3 as follows 6 c ˆ p 3 q u x p 7 p 9 exp p 6 y 2 x 2 p 7 exp p 8 h 2 x 2 p 9 traffic emissions are represented as area sources in this model as described in turner 1994 an offset was added to the downwind distance x from such sources to account for the initial width of the plume of the area source this offset is denoted p 4 in the gaussian odm formulation in addition it is assumed that traffic attributes are homogeneously in space but not in time converted to emission rates using some unknown factor p 2 since traffic sources are already at ground level h 0 the vertical dispersion exponent is always equal to 1 for these sources the complete gaussian odm accounting for all the point and area sources in the study area estimates the concentration at the i th grid cell as 7 c i ˆ p 1 p 2 j 1 m t j g t x i j y i j u p 3 j 1 k q j g q x i j y i j h j u where p 1 is a spatially uniform baseline concentration t j is the traffic volume in the j th grid cell m is the number of grid cells in the study area k is the number of point sources in the study area q j is the emission rate from the j th industrial stack and g q and g t are defined as 8 g q x y h u 1 u x p 7 p 9 exp p 6 y 2 x 2 p 7 p 8 h 2 x 2 p 9 p 5 x 9 g t x y u 1 u x p 4 p 7 p 9 exp p 6 y 2 x p 4 2 p 7 p 5 x where p 5 is the modelled first order removal coefficient of the pollutant and p 7 9 are used to model the dispersion standard deviations as was shown in eq 4 the chemical interactions in the atmosphere can in reality be more complex than first order removal as modelled by p 5 however accurate representation of these interactions requires knowledge that is often unavailable such as the concentration of hydroxyl radicals instead we attempt to infer this complexity through the aggregated removal rate within the optimization procedure this new model eq 7 is termed gaussodm since no data are available regarding the suitability of different plume rise models to our study area we implemented in the model the moses carson formula carson and moses 1969 any other plume rise model can be used without loss of generality the gaussodm is a re formulation of the mechanistic yet highly simplistic gaussian plume as a regression model like other regression models its parameters in this case p 1 9 are fitted to observations however since unlike lur the model is non linear finding the values of the parameters which provide the best fit with ambient measurements is non trivial and requires an iterative optimization process 2 2 optimization process the implemented objective function was ordinary least squares ols 10 argmin p 1 9 i 1 n c i c i ˆ p 1 9 2 where n is the number of monitoring stations for which monitoring data were available at the modelled time point c i is the measured concentration at the i th monitoring station and c i ˆ is the modelled concentration at the same location all parameters p 1 9 were bounded to obtain positive values to address the physical constraints of the modelled processes the optimization was performed in matlab r2017a using the f m i n c o n function and the i n t e r i o r p o i n t algorithm coleman et al 1999 waltz et al 2006 the optimization was performed separately for each half hourly time point such that at each time point a unique set of values for p 1 9 was obtained the application of the model was initially attempted using only the bound constraints this resulted in over fitting of the model to the ambient concentrations to alleviate this issue three constraints were added the first constraint sets an upper bound of 1 for α see section 2 1 11 p 3 2 α p 6 p 8 π the other two constraints ensure that at each time point the ratio of the vertical to horizontal standard deviations will not exceed that which was found in previous studies seinfeld and pandis 2016 12 σ z x 100 m σ y x 100 m 1 13 σ z x 10 km σ y x 10 km 3 the jacobian and hessian of the objective function were calculated analytically to enhance the convergence towards local minima the derivatives were validated at several time points using adaptive robust numerical differentiation d errico 2006 since our objective function is non convex finding the true global minima is not guaranteed to increase the chance of reaching the true global minima the optimization was started from 12 distinct initial guesses for the values of the parameters and progressed using the interior point algorithm independently once all instances reached 40 iterations or otherwise reported convergence the best lowest objective function value of the 12 solutions was chosen for that time point the initial guesses were a mix of optimal results from previous time points and published parametrizations of the 6 classical stability classes as described in seinfeld and pandis 2016 three sets of power law parametrizations are listed in seinfeld and pandis 2016 pasquil gifford asme and klug the pasquil gifford formulation gives the same value to p 7 eq 4 regardless of the stability class which was deemed to be not flexible enough for use in the odm the asme parametrization is missing values for the c and e stability classes therefore the klug values were used as initial guesses 1 6 the remaining 6 initial guesses were the optimally chosen sets of parameters from the previous 6 time points if there were no previous 6 time points available i e at the start of the simulation then the values of asme for stability classes a b d and f and the values of pasquil gifford for stability classes c and e were used temporarily random initial values were not used in order to maintain reproducibility of the results although the current work is focused on no x predictions the developed model is suitable for any primary air pollutant 2 3 study area the study area covers the coastal plain of israel see fig 1 an area of 2788km2 the area s population is approximately 4 3 million about 52 of the total israeli population including the tel aviv metropolis israeli central bureau of statistics 2014 the relatively flat coastal terrain supports the use of a spatially homogeneous yet temporally varying wind field the meteorology in the area is largely dominated by a breeze cycle with mostly north westerly winds from the mediterranean sea inland during the day and land breeze dominating at night more detail about the study area is available in chen et al 2018 using the israel transverse mercator itm coordinate system a grid of 11 200cells with 500 metres spatial separation was generated each 500 500 m 2 grid cell was used both as a traffic emission source and as a receptor in which the modelled no x concentrations were calculated at a half hourly temporal resolution throughout 2014 data availability section 2 4 was the determining factor in setting the spatio temporal resolution of the model 2 4 data 2 4 1 emissions as a proxy for vehicular emissions t j in eq 7 we used traffic volume estimates from 2012 that were calculated over a 250 250 m 2 grid by decell ltd tel aviv israel each grid cell contained a distinct mean volume average number of vehicles passing through the cell per hour for buses trucks and private vehicles with trucks defined as vehicles over 5 tonnes excluding buses and minibuses counted as private vehicles no specific information was available on fuel type generally private vehicles in the study area are powered by gasoline while the heavier portions of the fleet are powered by diesel for each of the three vehicle types mean volumes were provided for 11 daily time windows 00 00 03 00 03 00 06 00 06 00 07 00 07 00 08 00 08 00 09 00 09 00 12 00 12 00 15 00 15 00 18 00 18 00 20 00 20 00 22 00 and 22 00 24 00 these time windows provide more granular separation in hours with higher temporal variability i e rush hours and coarser separation for times with reduced traffic activity moreover separate data were obtained for weekdays sunday thursday in israel fridays and saturdays the weekend in israel additional data granulation has been made for routine and vacation jewish holidays and summer periods for a total of 198 data per grid cell this granulation was chosen in order to optimize the trade off between the temporal resolution of the dataset and the sample size of the signals used to calculate each datum this traffic dataset has been evaluated in depth previously chen et al 2016 to use these data in a 500 500 m 2 resolution we assumed a uniform distribution of vehicles within each 250 250 m 2 decell grid cell and summed the traffic within each of our 500 500 m 2 grid cells trucks were considered to have a relative emission factor of 3 6 relatively to private vehicles and the bus volumes were not used due to inaccuracies found by chen et al 2016 point source emissions were obtained from the israeli ministry of environmental protection imoep national air pollution emissions inventory the database was compiled in january 2015 and provides estimates of upper bounds of emissions from each stack in the study area it includes emission profiles for each source based on time of day day of the week and month of the year it also includes the geometry height and diameter of the stacks the temperature and the vertical speed of the emissions a total of 847 point sources of no x emission were identified in the study area emitting a combined total of 14 6th 1 of no x yearly mean this dataset was described in greater detail by chen et al 2018 only the four largest industrial emitters which are responsible for 94 of the industrial no x emissions are shown in fig 1 although all 847 sources were included in the model 2 4 2 ambient measurements air pollution monitoring data in the study area are observed by a network of air quality monitoring stations and are available from the imoep website we excluded data from road side monitoring stations as they were deemed unrepresentative of the 500 500 m 2 grid cells for which we ran our model hence we used the 2014 no x data from the 41 stations which were situated to meet the requirement of the eu council directive eu 1999 for protection of human health and that reported no x values during 2014 see fig 1 at these monitoring stations the no x concentration is obtained by the chemiluminescence method en 14211 due to calibrations power outages and other technical issues the stations did not report concentrations for all 17 520half hourly timepoints however data from over 35 stations was available most of the time fig 2 the wind direction input was the half hourly representative wind calculated from the wind observations in the monitoring stations using an algorithm described by yuval and broday 2009 the representative wind data have full temporal coverage and were the only meteorological data used directly by the model additional meteorological variables were collected in order to analyse the model results but were not incorporated in the model itself the standard deviation of the wind direction stwd is considered to be proportional to atmospheric turbulence a major determinant of dispersion of air pollutants weber 1997 a 10 minutes series of stwd was observed in a permanent meteorological station situated in beit dagan close to the centre of the study area fig 1 the data were time averaged to half hourly resolution using the method of bailey 2000 another important determinant of air pollution concentrations is the vertical temperature lapse rate γ data from twice daily around 13 00 and 01 00 local time radiosonde launches close to the beit dagan meteorological station were used to calculate it considering the temperature difference between the ground and the 12th radiosonde reporting level mean of 108m the ambient temperature t and solar radiation sr were also collected from the beit dagan meteorological station a series of quality control steps were carried out prior to using the data to ensure their fidelity these included omission of sporadic negative values a manual process of identifying drift in reported values and a careful comparison of trends in adjacent monitoring stations for detection of abnormalities unless the type of error in the reported values was trivial to correct such as misreported units of measurement the detected abnormal values were removed from the dataset 2 5 evaluation of the model results the spatio temporal assessment of the model results was performed by running the model for an entire year 17 513half hourly time points for which at least 20 air quality monitors reported no x concentrations in 2014 and visually examining the spatio temporal patterns of the estimated no x concentrations within the study area specifically a comparison was made between the spatial patterns obtained in time points with similar temporal properties time of day day of the week and month of the year for each of the 12 months a coefficient of variation cv of the modelled no x concentrations was calculated for each combination of time of day day of the week and grid cell 11 200grid cells 48times of day 7days of the week 3 763 200cvs per month the distribution of these cvs in the different months of the year was compared this was performed also for the monitoring data in order to compare the temporal trends in addition the average model predictions in the four seasons as defined by alpert et al 2004 were compared both in their spatial patterns and in the relative contribution of each emission sector i e traffic and industry the model performance was assessed using 10 distinct model fit measures which were calculated in a leave one out cross validation loocv routine due to the heavy computational demands of the procedure it was performed on a randomly selected sample of 42 7384 of the half hourly time points in 2014 at each of these time points 41 different optimization problems were solved each time a different station was left out such that the cross validation was complete within those randomly selected time points the selected time points were roughly uniformly distributed across the different months of the year and times of day fig 3 the measures used were the coefficient of efficiency e k see legates and mccabe 2013 and eq 14 mean absolute error mae mean spatial pearson correlation mspc mean spatial spearman correlation mssc fraction of modelled values within a factor of 2 of ambient measurements fac2 mean bias mb the square of the pearson correlation coefficient r p 2 the fraction of modelled values within 10 of ambient measurements 10 and the index of agreement ioa carslaw and ropkins 2012 to calculate the mspc and mssc the correlation coefficient was calculated separately for each half hourly time point and averaged over the entire period of interest when the pearson correlation was calculated once rather than averaged it was noted as r p all other performance measures were pooled over all pairs of modelled and observed values in the cross validated time points of 2014 a total of 264 761unique pairs e k was calculated using the following equation 14 e k 1 i 1 n c i ˆ c i k i 1 n c i c i k where n is the number of observations 264 761in this study c i is the measured i th observation c i is the average of the n observations and c i ˆ is the i th prediction by the model both 1 and 2 were chosen as values for k the model performance was compared to both idw interpolation and a previously published odm version ti odm see chen et al 2018 in addition the spearman correlation between the model performance and the following parameters was calculated the ambient temperature t the wind speed ws the standard deviation of the wind direction stwd the solar radiation sr the number of no x observations num and the mean measured no x level m n l the spatial autocorrelation of the model fit measures was assessed by i m as described in yuval et al 2017 the statistical significance of i m was tested using a two tailed permutation test against the null hypothesis of i m 0 for each test we used 10 000permutations and defined sets of values that exhibited a p value of less than 0 05 as statistically significant we suspected that the model predictions might be substantially influenced by the inclusion or exclusion of a monitoring station for which industrial emissions contributed a high percentage of the observed no x concentration therefore we calculated the spearman correlation between both the cross validated model errors and the cross validated absolute model errors against the contribution of the industrial sector to the modelled no x level at the same locations in the full model 3 results and discussion the spatial patterns of the yearly mean modelled no x concentration were very similar between the gaussodm and the ti odm fig 4 the main differences are higher concentrations in the gaussodm around the major industrial emitters and lower concentrations in the rest of the study area this is likely due to the difference in the constraints between the two models in both models a constraint was employed to decrease over fitting of the results by means of an abnormally high contribution of industrial stacks in the ti odm the constraint was based on the ratio between the impact of industry and traffic however in the gaussodm the two sectors are decoupled and the impact of industry was constrained according to the analytically derived gaussian dispersion model α 1 overall industry was found to be responsible for 4 03 of the ambient no x levels over the whole study area according to the gaussodm vs only 1 45 in the ti odm this is likely due to the gaussodm allowing for a substantial contribution of industrial sources to ambient levels at times when traffic data and ambient no x are not well correlated 3 1 cross validated model performance in the leave one out cross validation procedure the gaussodm displayed an improved mspc compared with both the ti odm and idw interpolation fig 5 this suggests that the gaussodm better accounts for the spatial patterns of no x than the other tested models the improved mspc of the gaussodm over the ti odm is especially apparent during midday during this time westerly winds which carry no x emitted from the coastal power plants inland are very common this is possibly another implication of the decoupling of the traffic and industry terms in the gaussian variation of the odm since high vertical turbulence during midday disperses traffic emissions relatively quickly other than the performance measures that are specifically spatially oriented the mspc and mssc there is little difference between the cross validated performance of the three models table 1 the implication of this finding is that the improvement in the spatial fit of the model has no apparent deleterious trade off in terms of other model performance criteria both the mae and r p of the gaussodm had a modest negative correlation with the ambient temperature which was stronger than the correlation with any other observed meteorological parameter fig 6 however the temperature was negatively correlated with the average measured no x concentrations which in turn had a stronger positive correlation than the temperature with the mae and r p therefore we believe that the model performance is not particularly sensitive to the meteorological conditions but has more to do with the magnitude of no x concentrations in each time point the likely reason for the increase in r p with increasing no x levels is that high no x concentrations are usually caused by a strong impact of local emissions which are included in the model however when the no x level are low then a larger percentage of the pollution likely originates from processes such as long range transport and re circulation which could be better represented by a dynamic model the positive correlation between the mae and the mean no x concentration likely stems from the combination of a having a lower bound of zero both for the measured and modelled concentrations and b the predicted values being centred around the observed values in regression models such as the gaussodm this combination allows for a greater absolute range of random deviations of the predictions from the observations when the observations are higher neither model fit measure was correlated with the number of observations which implies that the model performance was not very sensitive to it within the range of available observations see fig 2 for this range a spatial autocorrelation was not detected in the cross validated mean bias fig 7a however the temporal correlations were similar in adjacent monitoring stations p value 0 05 in particular the temporal correlations were higher in the central region of the study area the tel aviv metropolis this region is characterized by a high traffic density fig 1 and relatively high no x concentrations which are conducive for the gaussodm performance generally the temporal correlations shown in fig 7b are higher than the spatial correlations shown in fig 5 this likely stems from the various stations having similar temporal trends of no x concentrations e g higher concentrations in the winter due to this similarity even in a cross validation procedure the temporal trends can be captured well by practically any regression model neither the cross validated model residuals nor the cross validated absolute model residuals were correlated with the industrial contribution to the modelled concentration in the full model spearman s r was 0 02 and 0 01 respectively therefore the gaussodm performance seems not to be affected to a large extent by missing single locations at which the industrial contribution to ambient no x is high 3 2 the relationship between model results and meteorology higher concentrations were estimated by the gaussodm when the stwd was lower fig 8 this increase in no x could not be attributed to greater emissions since the stwd was positively correlated spearman s r 0 49 with the traffic volumes averaged over the entire study area at each time point in addition to the difference in the magnitude of no x levels there was also a substantial difference in the spatial patterns of the pollutant when the stwd was higher no x concentrations were more homogeneously distributed across the study area whereas when the stwd was lower than the median a clear hotspot of no x concentrations in the coastal busy city of tel aviv can be seen around coordinates 180 660 km in fig 8 radiosonde measurements of γ were only available twice daily and both for the nightly and for the midday measurements the modelled concentrations were higher when γ was lower fig 9 generally the atmosphere was much more stable during the nightly measurements than at daytime the aforementioned tel aviv hotspot is apparent in both nightly images of fig 9 but with lower intensity in panel b when γ values were relatively higher at midday a larger value of γ corresponded to two different properties of the ground level no x field 1 a lower impact of traffic and 2 a greater impact of the large industrial stacks both of these differences can be explained by greater vertical convection of no x dispersed from the ground level sources upwards and transported from the high industrial plumes downward it is noteworthy that these physically sensible results were obtained despite not incorporating γ stwd or any other parameter of atmospheric turbulence directly into the model instead calibrating the model parameters to fit the observed pollution concentration data resulted in automatic incorporation of the effect of atmospheric turbulence and stability on the pollution levels which agrees with the common understanding of their role in pollution dispersion and with the negative correlations of the observed concentrations and the atmospheric stability parameters in out study area fig 6 of all the model parameters p 9 had the strongest correlations with meteorological variables related to atmospheric turbulence table 2 and in particular with γ since the value of p 9 determines the rate at which σ z grows as the downwind distance increases this result is reasonable and further manifests the potential to infer the state of the atmosphere from air pollution levels similarly the inverse correlations of p 8 with the meteorological variables is also reasonable see eqs 3 and 4 however we could not explain the inverse correlations shown for p 7 which has the same role as p 9 but for σ y future studies should further investigate the relationship between the gaussodm parameters and meteorology if these relationships can be modelled successfully the gaussodm may become useful for forecasting rather than just for retrospective exposure assessment in the hot summer season the gaussodm generally predicted lower concentrations than in the rest of the year which is expected since the input ambient no x measurements were also low fig 10a in addition the substantial dispersion predicted at this time period results in a high relative contribution of industrial emissions to ground level modelled concentrations see fig 1 for the locations of the major point sources this result fits well with previous research uzan and alpert 2012 in the autumn predicted values were higher and more focused around the central coastal region which is characterized by dense traffic fig 10b these phenomena are further exemplified in the coldest season fig 10c in both cases the industrial point sources are overshadowed by traffic emissions in the spring fig 10d an intermediate image is seen where some industrial plumes can be discerned and the central traffic pollution hotspot is manifested to a lesser degree than in the cold seasons but substantially more than in the summer the impact of the two emitting sectors on ground level modelled concentrations is shown in table 3 the absolute impact of traffic on modelled ambient no x levels was greater during the cold seasons while the exact opposite was true for the tall industrial stacks overall the predictions of the gaussodm fit well with what is expected from the meteorology on the macro seasonal temporal scale 3 3 spatio temporal variability of the model results on the micro half hourly temporal scale the model results were not as predictable as an example fig 11 shows the results from four time points in the same month time of day and day of the week in all four time points both the traffic and industrial emissions inputs were identical however the differences in the monitoring data born of different atmospheric conditions drove the model to estimate very distinct no x spatial patterns and mean concentration levels the crosswind dispersion in panel b is noticeably the highest among the four while in panel c it is the lowest in time point c the modelled atmosphere is stable resulting in no x concentrations approaching 100 ppb near the most congested roads indeed the calculated values of the traffic σ y in each of the 4 time points show this difference in modelled turbulence table 4 the difference between the traffic σ y and industrial σ y stems from the offset for the traffic area sources p 4 the variation in σ y does not correspond with a trend in any of the concurrently measured meteorological variables for example the two most disperse time points b and d have the lowest b and highest d wind speeds of the four time point b is characterized by the highest γ of the four which corresponds well with the highly dispersed spatial pattern and low modelled concentrations however γ is almost identical between a and c although they differ greatly especially in their peak concentrations in addition the substantial modelled turbulence in time point d is not expected from the relatively low concurrently observed value of γ purely considering the available meteorological data for these time points without incorporating the air pollution monitoring data could not have led us to the varied dispersion patterns shown in fig 11 the granularity manifested by the gaussodm is expected to impact exposure assessment when considering time activity patterns of subjects or exposures during specific time windows the variability of the spatial patterns was generally greater during the cold season than in the summer months fig 12 despite the gaussodm not having direct input data related to atmospheric stability these results fit well with previous studies on the meteorological patterns in our study area dayan and rodnizki 1999 alpert et al 2004 the benefit of applying a model with such a high and meaningful temporal resolution is further manifested by comparing the distribution of no x concentrations at two locations with the same yearly mean concentration jaffa port jp coordinates 176 663 km and or yehuda oy coordinates 186 658 km see fig 1 for their locations while the median modelled pollution level in oy is 50 higher than in jp there are more extreme pollution events at jp table 5 as such the gaussodm enables studying the health effects of a pollutant considering a richer spectrum of statistical properties of its distribution in addition to its mean concentration value 4 conclusions this work demonstrated estimation of no x concentrations by the gaussodm however we expect the model to be applicable to any primary pollutant provided that sufficient data is available regarding the emission sources and enough monitoring stations observe the pollutant in addition since the model formulation is not specific to our study area we expect it to be applicable in other study areas in which a homogeneous wind field can be assumed at any time point but which can change considerably along the day a potentially fruitful avenue for future research is a spatially heterogeneous background level p 1 which would incorporate knowledge of sources outside of the study area in our case these may be shipping the gaza strip and the west bank the current form of the gaussodm is only useful for retrospective assessment forecasting and impact analysis applications would only be possible if robust predictive models of p 1 9 are obtained in future studies although the gaussodm is a substantial advancement in the incorporation of dispersion principles into regression models it is still an over simplification of real world dispersion the potential automatic calibration of modern mechanistic models has been discussed in the literature sportisse 2007 in practice this type of procedure was only successfully applied to account for the modulations of emission rates and background concentrations rather than the dispersion parameters bocquet et al 2015 future studies may explore the implementation of mechanistic models even more complex than the gaussian plume as non linear regression models in order to enhance ambient data assimilation and improve the estimation accuracy even further these future models may better represent pollutant transport and dispersion in a heterogeneous wind field and the plethora of chemical reactions not accounted for in the gaussodm formulation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research was supported in part by the environment and health fund israel ehf by a scholarship from the buncher foundation doctoral fellowship fund in memory of james s balter and a generous contribution of the leona h and harry b helmsley charitable trust to the technion shantou university collaboration in environmental health grant 2015pg isl006 the authors declare no competing financial interest 
26066,regression models e g land use regression are currently the most popular way to estimate retrospective exposures to air pollution however these models lack important features of atmospheric dispersion we developed a new non linear air quality regression model which is based on the physical grounds of the well established and commonly applied gaussian dispersion model this was achieved through parametrization of the basic gaussian model including its standard deviations and optimizing the parameters to provide a least squares fit with ambient measurements at each individual time point the new model gaussodm outperformed both a simpler regression model and a benchmark interpolation model in predicting spatial ambient nitrogen oxides nox concentrations the gaussodm enables a deeper understanding of the relationship between air pollution and adverse health effects this is partly because it is better adapted at incorporating meteorological data and the effects of elevated emissions compared with previously available air pollution regression models keywords air quality modelling data assimilation exposure assessment gaussian dispersion nitrogen oxides regression models software and data availability the gaussodm was applied as a matlab program using matlab r2017a available since 2017 the program together with the input data we used to produce the results presented in this publication are available for download at this link figshare com s 4c27dc3758f680910130 250 mb download at least 70 gb of ram are required to run the model a cpu with at least 12 cores is recommended we ran the model on a lenovo thinkstation p910 with a dual intel xeon e5 2603 v4 processor 15mb cache 1 7 ghz and 512gb of 2400mhz ecc rdimm ram on this system simulating a full year at a half hourly temporal resolution took 1 week 1 introduction exposure to the complex mixture of emissions from the combustion of carbon based fuels has been linked to a variety of adverse health effects claxton 2015 guarnieri and balmes 2014 manzetti and andersen 2016 of the extensively monitored criteria pollutants nitrogen oxides no x have the best correlation with various combustion by products including ultrafine particles and carbonaceous particulate matter hamra et al 2015 minguillón et al 2012 thus no x concentrations are often used as an indicator of exposure to harmful combustion by products in epidemiological studies nyberg et al 2000 hamra et al 2015 specifically exposure to no x has been linked with cardiorespiratory mortality asthma and various cancers jacquemin et al 2015 newell et al 2018 cohen et al 2019 however better individualized exposure assessment is required to reduce the uncertainties in the effect size of established links and to determine if additional links exist hamra et al 2015 wolf et al 2017 weisskopf and webster 2017 exposure to ambient no x can be assessed through either direct personal measurements or modelling hannam et al 2013 steinle et al 2013 the modelling approach is appealing due to its scalability to large cohorts and the ability to produce retrospective exposure estimates zou et al 2009 hannam et al 2013 the two main types of air quality models are mechanistic models and regression models holloway et al 2005 shahraiyni and sodoudi 2016 mechanist models such as adms and chimere incorporate many of the important features of atmospheric dispersion and can provide high resolution air quality assessment riviere et al 2019 however they require very detailed highly accurate and often unavailable three dimensional meteorology and emissions input data in order to provide reasonable results brauer 2012 tartakovsky et al 2013 de hoogh et al 2014 therefore regression models are more often applied for personalized exposure assessment hoek et al 2008 xie et al 2017 the most common type of regression is linear land use regression lur in this approach a set often 40 100 of geographical variables which are deemed to be related to air quality are identified and undergo a selection process as explanatory variables in the regression model hoek et al 2008 once the model parameters or coefficients are selected and fitted to the data at the monitoring locations they can be used to predict concentration at any other location in which the geographical variables are known and at a high spatial resolution down to a few tens of metres this approach was previously shown to perform well in cross validation procedures hoek et al 2008 however lur models are usually only applicable on a coarse temporal resolution most commonly for annual exposure this is partly due to the low temporal coverage of the dedicated lur monitoring campaigns usually only a few representative weeks from a year are chosen however it is also limited by fundamental properties of commonly applied lur models such as the use of static explanatory variables and the lack of atmospheric dispersion principles within the lur scheme hoek et al 2008 wilton 2011 tunno et al 2016 retrospective exposure assessment at a high hourly temporal resolution is necessary for some epidemiological applications such as those incorporating time activity patterns or examining short term effects of air pollution hoek et al 2008 batterman et al 2014 shafran nathan et al 2017 rasche et al 2018 some regression models achieved a high temporal resolution by incorporating the predictions of a mechanistic model as an additional covariate within the lur scheme michanowicz et al 2016 cordioli et al 2017 de hoogh et al 2018 however this approach still requires the costly implementation of a mechanistic model other studies incorporated temporal modulations of the annual lur predictions based on continuous regulatory monitoring or satellite data novotny et al 2011 johnson et al 2013 dons et al 2014 anand and monks 2017 cordioli et al 2017 however this approach is limited in its ability to capture the temporal changes in the spatial pollutant patterns a different approach is to incorporate some factors responsible for the temporal variability of ambient no x directly into the regression model the two main relevant factors are emissions and meteorology dobson 1997 liu et al 2012 ducret stich et al 2013 ai et al 2018 the incorporation of temporally resolved emission data has been achieved recently both for traffic chen et al 2016 matikolaei et al 2017 son et al 2018 and industrial no x emissions chen et al 2018 some meteorological parameters such as wind speed and solar radiation can be incorporated in linear lur models as was done in some studies kim and guldmann 2015 liu et al 2015 cheewinsiriwat 2016 however in order to incorporate the wind direction the model form must be adjusted lur models typically assign the values of the explanatory variables based on isotropic circular buffers around the receptor points hoek et al 2008 when the wind direction is incorporated into these linear models it is usually by replacing the circular buffers with wind directed wedges or by weighing the explanatory variables based on a wind rose arain et al 2007 su et al 2008 kim and guldmann 2011 li et al 2015 shi et al 2017 naughton et al 2018 however it is well established that pollution levels change non linearly with distance from the source naser et al 2009 bell and ashenden 1997 yuval et al 2013 presented a novel approach in which a non linear regression model was formulated incorporating key dispersion properties in a heuristic framework this optimized dispersion model odm out performed inverse distance weighing idw interpolation lur and chimere simulations yuval et al 2017 the formulation of the odm was extended to incorporate industrial emissions in addition to traffic by adding a term which was based on vertical gaussian dispersion chen et al 2018 exposure estimates made using the odm were especially useful for pregnancy outcomes epidemiology since particular stages of foetal development are of interest raz et al 2018 kioumourtzoglou et al 2019 these previous works demonstrated the potential of more complex regression models which incorporate physicochemical processes that are the basis of atmospheric dispersion in this work we extend this idea further by formulating a complete gaussian form of the odm which can be used for retrospective exposure assessment 2 methods 2 1 regression model the gaussian plume is an approximate analytical solution of the challenging system of differential equations that describe the pollutant concentration due to dispersion processes 1 c ˆ x y z q 2 π σ y σ z u exp y 2 2 σ y 2 exp z h 2 2 σ z 2 exp z h 2 2 σ z 2 where c ˆ is the modelled concentration at a given location q is the rate of pollutant emission from a point source that is located at the origin of the coordinate system x y and z are the downwind crosswind and vertical distances from the source respectively u is the time averaged wind speed at the effective height of release h along the x axis with the coordinate system rotated accordingly and σ y and σ z represent the standard deviations of the crosswind and vertical gaussian distribution of the pollutant concentration respectively the effective height of release is given by 2 h h s δ h were h s is the elevation of the point of emission and δ h is the plume rise the gaussian plume model is based on many assumptions which include a homogeneously turbulent wind field steady state air flow a steady state point pollution source and complete reflection of the pollutant from the ground leelőssy et al 2014 it is the basis of several popular mechanistic air pollution models such as adms and aermod heist et al 2013 we parametrized the gaussian plume such that it can be used as a non linear regression model in our study area section 2 3 the emission rates are not known with absolute certainty and only conservative high end estimates are available from regulatory bodies assuming that the relative errors in the emission rates are similar for all sources the actual emission rates are modelled to be α q where 0 α 1 such that the ground level z 0 concentration can be written as 3 c ˆ α q π σ y σ z u exp y 2 2 σ y 2 exp h 2 2 σ z 2 many schemes exist for the parametrization of the dispersion standard deviations due to its common usage seinfeld and pandis 2016 mathematical simplicity and relatively low number of parameters needed power law relationships with the downwind distance from the source were chosen 4 σ y x p 7 2 p 6 σ z x p 9 2 p 8 where p 6 9 are optimization parameters substituting these terms into eq 3 results in the following model 5 c ˆ 2 α q p 6 p 8 π u x p 7 p 9 e x p p 6 y 2 x 2 p 7 e x p p 8 h 2 x 2 p 9 since α is not known at any given time point and in order to reduce computational difficulties in calculating the model s derivatives the term 2 α p 6 p 8 π was replaced with a new optimization parameter p 3 as follows 6 c ˆ p 3 q u x p 7 p 9 exp p 6 y 2 x 2 p 7 exp p 8 h 2 x 2 p 9 traffic emissions are represented as area sources in this model as described in turner 1994 an offset was added to the downwind distance x from such sources to account for the initial width of the plume of the area source this offset is denoted p 4 in the gaussian odm formulation in addition it is assumed that traffic attributes are homogeneously in space but not in time converted to emission rates using some unknown factor p 2 since traffic sources are already at ground level h 0 the vertical dispersion exponent is always equal to 1 for these sources the complete gaussian odm accounting for all the point and area sources in the study area estimates the concentration at the i th grid cell as 7 c i ˆ p 1 p 2 j 1 m t j g t x i j y i j u p 3 j 1 k q j g q x i j y i j h j u where p 1 is a spatially uniform baseline concentration t j is the traffic volume in the j th grid cell m is the number of grid cells in the study area k is the number of point sources in the study area q j is the emission rate from the j th industrial stack and g q and g t are defined as 8 g q x y h u 1 u x p 7 p 9 exp p 6 y 2 x 2 p 7 p 8 h 2 x 2 p 9 p 5 x 9 g t x y u 1 u x p 4 p 7 p 9 exp p 6 y 2 x p 4 2 p 7 p 5 x where p 5 is the modelled first order removal coefficient of the pollutant and p 7 9 are used to model the dispersion standard deviations as was shown in eq 4 the chemical interactions in the atmosphere can in reality be more complex than first order removal as modelled by p 5 however accurate representation of these interactions requires knowledge that is often unavailable such as the concentration of hydroxyl radicals instead we attempt to infer this complexity through the aggregated removal rate within the optimization procedure this new model eq 7 is termed gaussodm since no data are available regarding the suitability of different plume rise models to our study area we implemented in the model the moses carson formula carson and moses 1969 any other plume rise model can be used without loss of generality the gaussodm is a re formulation of the mechanistic yet highly simplistic gaussian plume as a regression model like other regression models its parameters in this case p 1 9 are fitted to observations however since unlike lur the model is non linear finding the values of the parameters which provide the best fit with ambient measurements is non trivial and requires an iterative optimization process 2 2 optimization process the implemented objective function was ordinary least squares ols 10 argmin p 1 9 i 1 n c i c i ˆ p 1 9 2 where n is the number of monitoring stations for which monitoring data were available at the modelled time point c i is the measured concentration at the i th monitoring station and c i ˆ is the modelled concentration at the same location all parameters p 1 9 were bounded to obtain positive values to address the physical constraints of the modelled processes the optimization was performed in matlab r2017a using the f m i n c o n function and the i n t e r i o r p o i n t algorithm coleman et al 1999 waltz et al 2006 the optimization was performed separately for each half hourly time point such that at each time point a unique set of values for p 1 9 was obtained the application of the model was initially attempted using only the bound constraints this resulted in over fitting of the model to the ambient concentrations to alleviate this issue three constraints were added the first constraint sets an upper bound of 1 for α see section 2 1 11 p 3 2 α p 6 p 8 π the other two constraints ensure that at each time point the ratio of the vertical to horizontal standard deviations will not exceed that which was found in previous studies seinfeld and pandis 2016 12 σ z x 100 m σ y x 100 m 1 13 σ z x 10 km σ y x 10 km 3 the jacobian and hessian of the objective function were calculated analytically to enhance the convergence towards local minima the derivatives were validated at several time points using adaptive robust numerical differentiation d errico 2006 since our objective function is non convex finding the true global minima is not guaranteed to increase the chance of reaching the true global minima the optimization was started from 12 distinct initial guesses for the values of the parameters and progressed using the interior point algorithm independently once all instances reached 40 iterations or otherwise reported convergence the best lowest objective function value of the 12 solutions was chosen for that time point the initial guesses were a mix of optimal results from previous time points and published parametrizations of the 6 classical stability classes as described in seinfeld and pandis 2016 three sets of power law parametrizations are listed in seinfeld and pandis 2016 pasquil gifford asme and klug the pasquil gifford formulation gives the same value to p 7 eq 4 regardless of the stability class which was deemed to be not flexible enough for use in the odm the asme parametrization is missing values for the c and e stability classes therefore the klug values were used as initial guesses 1 6 the remaining 6 initial guesses were the optimally chosen sets of parameters from the previous 6 time points if there were no previous 6 time points available i e at the start of the simulation then the values of asme for stability classes a b d and f and the values of pasquil gifford for stability classes c and e were used temporarily random initial values were not used in order to maintain reproducibility of the results although the current work is focused on no x predictions the developed model is suitable for any primary air pollutant 2 3 study area the study area covers the coastal plain of israel see fig 1 an area of 2788km2 the area s population is approximately 4 3 million about 52 of the total israeli population including the tel aviv metropolis israeli central bureau of statistics 2014 the relatively flat coastal terrain supports the use of a spatially homogeneous yet temporally varying wind field the meteorology in the area is largely dominated by a breeze cycle with mostly north westerly winds from the mediterranean sea inland during the day and land breeze dominating at night more detail about the study area is available in chen et al 2018 using the israel transverse mercator itm coordinate system a grid of 11 200cells with 500 metres spatial separation was generated each 500 500 m 2 grid cell was used both as a traffic emission source and as a receptor in which the modelled no x concentrations were calculated at a half hourly temporal resolution throughout 2014 data availability section 2 4 was the determining factor in setting the spatio temporal resolution of the model 2 4 data 2 4 1 emissions as a proxy for vehicular emissions t j in eq 7 we used traffic volume estimates from 2012 that were calculated over a 250 250 m 2 grid by decell ltd tel aviv israel each grid cell contained a distinct mean volume average number of vehicles passing through the cell per hour for buses trucks and private vehicles with trucks defined as vehicles over 5 tonnes excluding buses and minibuses counted as private vehicles no specific information was available on fuel type generally private vehicles in the study area are powered by gasoline while the heavier portions of the fleet are powered by diesel for each of the three vehicle types mean volumes were provided for 11 daily time windows 00 00 03 00 03 00 06 00 06 00 07 00 07 00 08 00 08 00 09 00 09 00 12 00 12 00 15 00 15 00 18 00 18 00 20 00 20 00 22 00 and 22 00 24 00 these time windows provide more granular separation in hours with higher temporal variability i e rush hours and coarser separation for times with reduced traffic activity moreover separate data were obtained for weekdays sunday thursday in israel fridays and saturdays the weekend in israel additional data granulation has been made for routine and vacation jewish holidays and summer periods for a total of 198 data per grid cell this granulation was chosen in order to optimize the trade off between the temporal resolution of the dataset and the sample size of the signals used to calculate each datum this traffic dataset has been evaluated in depth previously chen et al 2016 to use these data in a 500 500 m 2 resolution we assumed a uniform distribution of vehicles within each 250 250 m 2 decell grid cell and summed the traffic within each of our 500 500 m 2 grid cells trucks were considered to have a relative emission factor of 3 6 relatively to private vehicles and the bus volumes were not used due to inaccuracies found by chen et al 2016 point source emissions were obtained from the israeli ministry of environmental protection imoep national air pollution emissions inventory the database was compiled in january 2015 and provides estimates of upper bounds of emissions from each stack in the study area it includes emission profiles for each source based on time of day day of the week and month of the year it also includes the geometry height and diameter of the stacks the temperature and the vertical speed of the emissions a total of 847 point sources of no x emission were identified in the study area emitting a combined total of 14 6th 1 of no x yearly mean this dataset was described in greater detail by chen et al 2018 only the four largest industrial emitters which are responsible for 94 of the industrial no x emissions are shown in fig 1 although all 847 sources were included in the model 2 4 2 ambient measurements air pollution monitoring data in the study area are observed by a network of air quality monitoring stations and are available from the imoep website we excluded data from road side monitoring stations as they were deemed unrepresentative of the 500 500 m 2 grid cells for which we ran our model hence we used the 2014 no x data from the 41 stations which were situated to meet the requirement of the eu council directive eu 1999 for protection of human health and that reported no x values during 2014 see fig 1 at these monitoring stations the no x concentration is obtained by the chemiluminescence method en 14211 due to calibrations power outages and other technical issues the stations did not report concentrations for all 17 520half hourly timepoints however data from over 35 stations was available most of the time fig 2 the wind direction input was the half hourly representative wind calculated from the wind observations in the monitoring stations using an algorithm described by yuval and broday 2009 the representative wind data have full temporal coverage and were the only meteorological data used directly by the model additional meteorological variables were collected in order to analyse the model results but were not incorporated in the model itself the standard deviation of the wind direction stwd is considered to be proportional to atmospheric turbulence a major determinant of dispersion of air pollutants weber 1997 a 10 minutes series of stwd was observed in a permanent meteorological station situated in beit dagan close to the centre of the study area fig 1 the data were time averaged to half hourly resolution using the method of bailey 2000 another important determinant of air pollution concentrations is the vertical temperature lapse rate γ data from twice daily around 13 00 and 01 00 local time radiosonde launches close to the beit dagan meteorological station were used to calculate it considering the temperature difference between the ground and the 12th radiosonde reporting level mean of 108m the ambient temperature t and solar radiation sr were also collected from the beit dagan meteorological station a series of quality control steps were carried out prior to using the data to ensure their fidelity these included omission of sporadic negative values a manual process of identifying drift in reported values and a careful comparison of trends in adjacent monitoring stations for detection of abnormalities unless the type of error in the reported values was trivial to correct such as misreported units of measurement the detected abnormal values were removed from the dataset 2 5 evaluation of the model results the spatio temporal assessment of the model results was performed by running the model for an entire year 17 513half hourly time points for which at least 20 air quality monitors reported no x concentrations in 2014 and visually examining the spatio temporal patterns of the estimated no x concentrations within the study area specifically a comparison was made between the spatial patterns obtained in time points with similar temporal properties time of day day of the week and month of the year for each of the 12 months a coefficient of variation cv of the modelled no x concentrations was calculated for each combination of time of day day of the week and grid cell 11 200grid cells 48times of day 7days of the week 3 763 200cvs per month the distribution of these cvs in the different months of the year was compared this was performed also for the monitoring data in order to compare the temporal trends in addition the average model predictions in the four seasons as defined by alpert et al 2004 were compared both in their spatial patterns and in the relative contribution of each emission sector i e traffic and industry the model performance was assessed using 10 distinct model fit measures which were calculated in a leave one out cross validation loocv routine due to the heavy computational demands of the procedure it was performed on a randomly selected sample of 42 7384 of the half hourly time points in 2014 at each of these time points 41 different optimization problems were solved each time a different station was left out such that the cross validation was complete within those randomly selected time points the selected time points were roughly uniformly distributed across the different months of the year and times of day fig 3 the measures used were the coefficient of efficiency e k see legates and mccabe 2013 and eq 14 mean absolute error mae mean spatial pearson correlation mspc mean spatial spearman correlation mssc fraction of modelled values within a factor of 2 of ambient measurements fac2 mean bias mb the square of the pearson correlation coefficient r p 2 the fraction of modelled values within 10 of ambient measurements 10 and the index of agreement ioa carslaw and ropkins 2012 to calculate the mspc and mssc the correlation coefficient was calculated separately for each half hourly time point and averaged over the entire period of interest when the pearson correlation was calculated once rather than averaged it was noted as r p all other performance measures were pooled over all pairs of modelled and observed values in the cross validated time points of 2014 a total of 264 761unique pairs e k was calculated using the following equation 14 e k 1 i 1 n c i ˆ c i k i 1 n c i c i k where n is the number of observations 264 761in this study c i is the measured i th observation c i is the average of the n observations and c i ˆ is the i th prediction by the model both 1 and 2 were chosen as values for k the model performance was compared to both idw interpolation and a previously published odm version ti odm see chen et al 2018 in addition the spearman correlation between the model performance and the following parameters was calculated the ambient temperature t the wind speed ws the standard deviation of the wind direction stwd the solar radiation sr the number of no x observations num and the mean measured no x level m n l the spatial autocorrelation of the model fit measures was assessed by i m as described in yuval et al 2017 the statistical significance of i m was tested using a two tailed permutation test against the null hypothesis of i m 0 for each test we used 10 000permutations and defined sets of values that exhibited a p value of less than 0 05 as statistically significant we suspected that the model predictions might be substantially influenced by the inclusion or exclusion of a monitoring station for which industrial emissions contributed a high percentage of the observed no x concentration therefore we calculated the spearman correlation between both the cross validated model errors and the cross validated absolute model errors against the contribution of the industrial sector to the modelled no x level at the same locations in the full model 3 results and discussion the spatial patterns of the yearly mean modelled no x concentration were very similar between the gaussodm and the ti odm fig 4 the main differences are higher concentrations in the gaussodm around the major industrial emitters and lower concentrations in the rest of the study area this is likely due to the difference in the constraints between the two models in both models a constraint was employed to decrease over fitting of the results by means of an abnormally high contribution of industrial stacks in the ti odm the constraint was based on the ratio between the impact of industry and traffic however in the gaussodm the two sectors are decoupled and the impact of industry was constrained according to the analytically derived gaussian dispersion model α 1 overall industry was found to be responsible for 4 03 of the ambient no x levels over the whole study area according to the gaussodm vs only 1 45 in the ti odm this is likely due to the gaussodm allowing for a substantial contribution of industrial sources to ambient levels at times when traffic data and ambient no x are not well correlated 3 1 cross validated model performance in the leave one out cross validation procedure the gaussodm displayed an improved mspc compared with both the ti odm and idw interpolation fig 5 this suggests that the gaussodm better accounts for the spatial patterns of no x than the other tested models the improved mspc of the gaussodm over the ti odm is especially apparent during midday during this time westerly winds which carry no x emitted from the coastal power plants inland are very common this is possibly another implication of the decoupling of the traffic and industry terms in the gaussian variation of the odm since high vertical turbulence during midday disperses traffic emissions relatively quickly other than the performance measures that are specifically spatially oriented the mspc and mssc there is little difference between the cross validated performance of the three models table 1 the implication of this finding is that the improvement in the spatial fit of the model has no apparent deleterious trade off in terms of other model performance criteria both the mae and r p of the gaussodm had a modest negative correlation with the ambient temperature which was stronger than the correlation with any other observed meteorological parameter fig 6 however the temperature was negatively correlated with the average measured no x concentrations which in turn had a stronger positive correlation than the temperature with the mae and r p therefore we believe that the model performance is not particularly sensitive to the meteorological conditions but has more to do with the magnitude of no x concentrations in each time point the likely reason for the increase in r p with increasing no x levels is that high no x concentrations are usually caused by a strong impact of local emissions which are included in the model however when the no x level are low then a larger percentage of the pollution likely originates from processes such as long range transport and re circulation which could be better represented by a dynamic model the positive correlation between the mae and the mean no x concentration likely stems from the combination of a having a lower bound of zero both for the measured and modelled concentrations and b the predicted values being centred around the observed values in regression models such as the gaussodm this combination allows for a greater absolute range of random deviations of the predictions from the observations when the observations are higher neither model fit measure was correlated with the number of observations which implies that the model performance was not very sensitive to it within the range of available observations see fig 2 for this range a spatial autocorrelation was not detected in the cross validated mean bias fig 7a however the temporal correlations were similar in adjacent monitoring stations p value 0 05 in particular the temporal correlations were higher in the central region of the study area the tel aviv metropolis this region is characterized by a high traffic density fig 1 and relatively high no x concentrations which are conducive for the gaussodm performance generally the temporal correlations shown in fig 7b are higher than the spatial correlations shown in fig 5 this likely stems from the various stations having similar temporal trends of no x concentrations e g higher concentrations in the winter due to this similarity even in a cross validation procedure the temporal trends can be captured well by practically any regression model neither the cross validated model residuals nor the cross validated absolute model residuals were correlated with the industrial contribution to the modelled concentration in the full model spearman s r was 0 02 and 0 01 respectively therefore the gaussodm performance seems not to be affected to a large extent by missing single locations at which the industrial contribution to ambient no x is high 3 2 the relationship between model results and meteorology higher concentrations were estimated by the gaussodm when the stwd was lower fig 8 this increase in no x could not be attributed to greater emissions since the stwd was positively correlated spearman s r 0 49 with the traffic volumes averaged over the entire study area at each time point in addition to the difference in the magnitude of no x levels there was also a substantial difference in the spatial patterns of the pollutant when the stwd was higher no x concentrations were more homogeneously distributed across the study area whereas when the stwd was lower than the median a clear hotspot of no x concentrations in the coastal busy city of tel aviv can be seen around coordinates 180 660 km in fig 8 radiosonde measurements of γ were only available twice daily and both for the nightly and for the midday measurements the modelled concentrations were higher when γ was lower fig 9 generally the atmosphere was much more stable during the nightly measurements than at daytime the aforementioned tel aviv hotspot is apparent in both nightly images of fig 9 but with lower intensity in panel b when γ values were relatively higher at midday a larger value of γ corresponded to two different properties of the ground level no x field 1 a lower impact of traffic and 2 a greater impact of the large industrial stacks both of these differences can be explained by greater vertical convection of no x dispersed from the ground level sources upwards and transported from the high industrial plumes downward it is noteworthy that these physically sensible results were obtained despite not incorporating γ stwd or any other parameter of atmospheric turbulence directly into the model instead calibrating the model parameters to fit the observed pollution concentration data resulted in automatic incorporation of the effect of atmospheric turbulence and stability on the pollution levels which agrees with the common understanding of their role in pollution dispersion and with the negative correlations of the observed concentrations and the atmospheric stability parameters in out study area fig 6 of all the model parameters p 9 had the strongest correlations with meteorological variables related to atmospheric turbulence table 2 and in particular with γ since the value of p 9 determines the rate at which σ z grows as the downwind distance increases this result is reasonable and further manifests the potential to infer the state of the atmosphere from air pollution levels similarly the inverse correlations of p 8 with the meteorological variables is also reasonable see eqs 3 and 4 however we could not explain the inverse correlations shown for p 7 which has the same role as p 9 but for σ y future studies should further investigate the relationship between the gaussodm parameters and meteorology if these relationships can be modelled successfully the gaussodm may become useful for forecasting rather than just for retrospective exposure assessment in the hot summer season the gaussodm generally predicted lower concentrations than in the rest of the year which is expected since the input ambient no x measurements were also low fig 10a in addition the substantial dispersion predicted at this time period results in a high relative contribution of industrial emissions to ground level modelled concentrations see fig 1 for the locations of the major point sources this result fits well with previous research uzan and alpert 2012 in the autumn predicted values were higher and more focused around the central coastal region which is characterized by dense traffic fig 10b these phenomena are further exemplified in the coldest season fig 10c in both cases the industrial point sources are overshadowed by traffic emissions in the spring fig 10d an intermediate image is seen where some industrial plumes can be discerned and the central traffic pollution hotspot is manifested to a lesser degree than in the cold seasons but substantially more than in the summer the impact of the two emitting sectors on ground level modelled concentrations is shown in table 3 the absolute impact of traffic on modelled ambient no x levels was greater during the cold seasons while the exact opposite was true for the tall industrial stacks overall the predictions of the gaussodm fit well with what is expected from the meteorology on the macro seasonal temporal scale 3 3 spatio temporal variability of the model results on the micro half hourly temporal scale the model results were not as predictable as an example fig 11 shows the results from four time points in the same month time of day and day of the week in all four time points both the traffic and industrial emissions inputs were identical however the differences in the monitoring data born of different atmospheric conditions drove the model to estimate very distinct no x spatial patterns and mean concentration levels the crosswind dispersion in panel b is noticeably the highest among the four while in panel c it is the lowest in time point c the modelled atmosphere is stable resulting in no x concentrations approaching 100 ppb near the most congested roads indeed the calculated values of the traffic σ y in each of the 4 time points show this difference in modelled turbulence table 4 the difference between the traffic σ y and industrial σ y stems from the offset for the traffic area sources p 4 the variation in σ y does not correspond with a trend in any of the concurrently measured meteorological variables for example the two most disperse time points b and d have the lowest b and highest d wind speeds of the four time point b is characterized by the highest γ of the four which corresponds well with the highly dispersed spatial pattern and low modelled concentrations however γ is almost identical between a and c although they differ greatly especially in their peak concentrations in addition the substantial modelled turbulence in time point d is not expected from the relatively low concurrently observed value of γ purely considering the available meteorological data for these time points without incorporating the air pollution monitoring data could not have led us to the varied dispersion patterns shown in fig 11 the granularity manifested by the gaussodm is expected to impact exposure assessment when considering time activity patterns of subjects or exposures during specific time windows the variability of the spatial patterns was generally greater during the cold season than in the summer months fig 12 despite the gaussodm not having direct input data related to atmospheric stability these results fit well with previous studies on the meteorological patterns in our study area dayan and rodnizki 1999 alpert et al 2004 the benefit of applying a model with such a high and meaningful temporal resolution is further manifested by comparing the distribution of no x concentrations at two locations with the same yearly mean concentration jaffa port jp coordinates 176 663 km and or yehuda oy coordinates 186 658 km see fig 1 for their locations while the median modelled pollution level in oy is 50 higher than in jp there are more extreme pollution events at jp table 5 as such the gaussodm enables studying the health effects of a pollutant considering a richer spectrum of statistical properties of its distribution in addition to its mean concentration value 4 conclusions this work demonstrated estimation of no x concentrations by the gaussodm however we expect the model to be applicable to any primary pollutant provided that sufficient data is available regarding the emission sources and enough monitoring stations observe the pollutant in addition since the model formulation is not specific to our study area we expect it to be applicable in other study areas in which a homogeneous wind field can be assumed at any time point but which can change considerably along the day a potentially fruitful avenue for future research is a spatially heterogeneous background level p 1 which would incorporate knowledge of sources outside of the study area in our case these may be shipping the gaza strip and the west bank the current form of the gaussodm is only useful for retrospective assessment forecasting and impact analysis applications would only be possible if robust predictive models of p 1 9 are obtained in future studies although the gaussodm is a substantial advancement in the incorporation of dispersion principles into regression models it is still an over simplification of real world dispersion the potential automatic calibration of modern mechanistic models has been discussed in the literature sportisse 2007 in practice this type of procedure was only successfully applied to account for the modulations of emission rates and background concentrations rather than the dispersion parameters bocquet et al 2015 future studies may explore the implementation of mechanistic models even more complex than the gaussian plume as non linear regression models in order to enhance ambient data assimilation and improve the estimation accuracy even further these future models may better represent pollutant transport and dispersion in a heterogeneous wind field and the plethora of chemical reactions not accounted for in the gaussodm formulation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research was supported in part by the environment and health fund israel ehf by a scholarship from the buncher foundation doctoral fellowship fund in memory of james s balter and a generous contribution of the leona h and harry b helmsley charitable trust to the technion shantou university collaboration in environmental health grant 2015pg isl006 the authors declare no competing financial interest 
26067,the utah energy balance ueb model supports gridded simulation of snow processes over a watershed to enhance computational efficiency we developed two parallel versions of the model one using the message passing interface mpi and the other using nvidia s cuda code on graphics processing unit gpu evaluation of the speed up and efficiency of the mpi version shows that the effect of input output io operations on the parallel model performance increases as the number of processor cores increases as a result although the computation kernel scales well with the number of cores the efficiency of the parallel code as a whole degrades the performance improves when the number of io operations is reduced by reading writing larger data arrays the cuda gpu implementation was done without major refactoring of the original ueb code and tests demonstrated that satisfactory performance could be obtained without a major re work of the existing ueb code keywords utah energy balance snowmelt model ueb message passing interface mpi compute unified device architecture cuda graphics processing unit gpu parallel io software availability program name ueb parallel description parallel version of the utah energy balance snowmelt model ueb platform platform independent tested on microsoft windows linux centos 6 x source language c cuda cost license free open source gnu general public license developers tarboton research group utah state university availability http github com dtarb ueb 1 introduction hydrological models are used to predict environmental flow of water under diverse drivers of change that are complex and heterogeneous one of the prime motivators of current hydrological research is the need to understand and quantify the possible impacts on water resources of changes in climate land cover land use population and urbanization fowler et al 2007 such studies may require modeling of the hydrologic processes at various scales ranging from headwater watersheds to river basins scales as the terrestrial water cycle is affected by its interactions with atmospheric and oceanic processes hydrological models at river basin or global scale may also need to consider the various pathways of water in the global cycle and magnitudes of feed backs between different layers components of the cycle levine and salvucci 1999 maxwell et al 2014 paniconi and putti 2015 a few years ago wood et al 2011 made a call for hyperresolution global land surface modeling to sufficiently resolve local processes in a model of global or continental scale this task of modeling the hydrologic cycle at large scale with sufficient resolution of individual processes poses multiple challenges one of these challenges is the desire to use high performance computing hpc resources to reduce computational time or increase the level of detail and hence complexity at which these problems are investigated on the other hand the availability of hpc resources is increasing this coupled with the recognition of the scientific needs for undertaking large scale hydrologic simulation has led to development of simulation models that implement parallel processing technologies for example kollet et al 2010 present results of a study where an integrated multi dimensional modeling problem with a number of unknowns in the order of 109 was solved within a feasible simulation time the challenge for hydrological modelers is thus shifting from the lack of computing resources to reconfiguring their modeling software to be able to take advantage of these new resources it should be noted here that parallel programming in hydrological and environmental modeling is not a new opportunity or issue e g paglieri et al 1997 rao 2004 however in the past two decades a strong argument has been made that the basic approach to software development should incorporate concurrency multi processes programming because of the power wall i e the upper limit imposed on the clock speed of single core due to overheating of high frequency cores and other efficiency optimization considerations brodtkorb et al 2013 sutter 2005 sutter and larus 2005 concurrency programing has also been spurred by the definition of standard programing interfaces that abstract away most of the low level operations and a number of library implementations of these interfaces thereby freeing a research programmer to focus on domain specific modeling issues e g mpi http www mpi forum org openmp http openmp org wp given the desire to apply more physically based distributed high resolution hydrologic models and given the opportunities offered by the parallel programing standards and libraries the question has then become what method to choose for a given model and what factors affect efficient scaling of the modeling code in this study we evaluated parallel processing implementations of the utah energy balance ueb snow accumulation and melt model we evaluated two implementations one using the mpich2 library of the message passing interface mpi specification gropp et al 2005 and the other using nvidia s compute unified device architecture cuda code on graphics processing units gpus nickolls et al 2008 the mpi is a distributed memory programming approach that promises good efficiency for the distributed ueb model that requires independent data for different model grid cells on the other hand the cuda code with its compatibility to c enhances the accessibility of general purpose gpus that have ability to handle compute intensive tasks the computational performance of the parallel codes was evaluated using simulations of the logan river watershed utah for a period of five years for the implementation with mpi we evaluated the speed up and efficiency of the code with increasing number of processor cores and compared the speed up with the ideal speed up computed based on amdahl s law amdahl 1967 2007 with regard to the application of gpus neal et al 2010 had found earlier that even though their gpu code was faster and more efficient than their mpi implementation the development time it required was prohibitive in contrast tristram et al 2014 reported that not only were gpus more cost efficient for their application but also achieving satisfactory speed up with gpus did not require major refactoring of their existing code for the cuda implementation of the ueb code in this study we also evaluated if satisfactory performance could be achieved by the gpu code without major refactoring of the code this paper is organized as follows in the next section a brief discussion of factors to consider in parallel programing based on review of literature is given focusing on hydrologic models in section 3 methods we describe the ueb model the algorithms for the parallel implementations the modeling case study to test the performance of the parallel codes and the performance metrics results from the performance tests and discussion are given in section 4 followed by conclusions in section 5 2 factors to consider in parallel programing the choice of a particular parallel programing approach may depend on a number of factors including familiarity with the programing interface ease of adaptation of existing serial code to a parallel version and the data and memory configuration of the problem being modeled neal et al 2010 investigated the application for 2d flood inundation modeling of three of the commonly used programming methods shared memory open multi processing openmp distributed memory message passing interface mpi and graphics processing unit gpu they tested the three approaches with respect to applicability to a given problem solution parallel code efficiency achieved and required implementation effort development time they concluded that the mpi approach was the most suitable compromise between the efficiency achieved and programming complexity involved they found that even though the gpu code was the fastest and most efficient the development time it required was prohibitive another important factor in parallel programming of simulation models is domain data decomposition among processes data partitioning schemes often try to address the issue of load balancing between multiple processes a good example is a 2d flood inundation model where some of the grid cells in the flood plain remain dry for part of the model run time resulting in some idle process time data partitioning schemes should strive to minimize such idle times brodtkorb et al 2012 sanders et al 2010 with respect to hydrological models domain decomposition is related to the flow dependencies upstream to downstream between computational sub domains where the computational sub domain can be a representative elementary watershed rew hydrologic response unit hru a structured or unstructured grid cell or a river reach in addition load imbalance may arise from the spatial variability of processes considered such as areas covered with snow versus those with no snow upstream hillslope versus riparian area or river channel presence and type of vegetation etc some of the approaches used in recent research are described below the first one is the use of multiple layers in which simulation units grids are divided based on their degree of dependency on upstream units accordingly units that do not depend on other units are placed at highest priority layer and units that depend only on a single unit are placed in the second highest priority layer and so on liu et al 2014 another approach involves dynamic run time allocation of a data partition to an available idle process once the partition no longer depends on upstream processes li et al 2011 dividing a 2d model domain into strips where the only inter process communications are at the boundaries between two adjacent sub domains is another approach tarboton et al 2009 tesfa et al 2011 a different approach particularly useful for a model with a tightly coupled set of processes is collecting all the governing equations into a global system of equations which are solved by a matrix solver qu and duffy 2007 such a matrix solver may divide the global matrix into sub matrices that are mapped onto multiple processes the examples above do not form an exhaustive list however they indicate that generally the choice of a given domain decomposition would be dictated by the specific modeling problem small et al 2013 presently a researcher or a modeler has to consider their own problem domain and decide whether to use a method similar to those listed above or develop their own the ability to automatically choose from certain domain decomposition methods given a problem domain type is a potential area of future study the extent to which a parallel program s performance increases with increased availability of computing resources e g number of processor cores depends primarily on the fraction of code that must be executed in serial this is the essence of amdahl s law amdahl 1967 2007 based on which the maximum possible speed up for a given problem is computed as a function of the number of cores and the serial fraction of the code the effect of the serial fraction of the code becomes more important with increasing parallelization yavits et al 2014 review a number of research works that dealt with the effect of data transfer between the serial and parallel portions of the code and the inter process communications on the maximum speed up determined from amdahl s law they provide models that revise amdahl s law incorporating terms that represent arithmetic intensity the ratio of total compute operations to data transfer computations data transfer synchronization between the serial and parallel portions of the program and inter process communication and synchronization accordingly the amount of inter process communication and or serial to parallel data synchronization in a program might inhibit its suitability for extensive parallelization for problems with relatively high inter process communication and data synchronization they suggest using fewer large cores rather than many small cores finally the cost efficiency of the parallel method will have to be considered as indicated earlier neal et al 2010 concluded that the developer time for programming using graphics cards was prohibitive in their modeling case however total cost should also include the price of the computing hardware units and operating costs tristram et al 2014 reported results from a parallel hydrologic uncertainty model with multiple ensembles using gpus they compared the cpu and gpu performances with respect to speed up the cost of processors and the cost of power usage and found gpus to be more cost efficient for their application regarding programmer s time they showed that to achieve satisfactory speed up with gpus major refactoring of their existing code was not necessary in addition the performance was further enhanced with an optimization involving memory access configuration the general purpose programming toolkits such as cuda https developer nvidia com cuda toolkit and opencl https www khronos org opencl coupled with the cheap graphics cards on commodity computers make gpu programming more accessible to scientific research programmers garland et al 2008 however realizing the full benefits still requires learning efficient program organization and optimizations such as latency hiding by overlapping computation with input output io operations wise management of register and caches and memory layout configuration de la asunción et al 2012 tristram et al 2014 which requires more effort and time brodtkorb et al 2013 3 methods 3 1 utah energy balance ueb snowmelt model the utah energy balance ueb snowmelt model tracks the accumulation and ablation of a single snow layer at the ground surface by energy and mass balance computations tarboton and luce 1996 in forested watersheds the model accounts for canopy snow interception partitioning of incoming solar and atmospheric radiation through the canopy layer and turbulent fluxes of latent and sensible heat within and below the canopy layer mahat and tarboton 2012 2014 mahat et al 2013 the snow surface temperature is computed using the modified force restore method that characterizes the conduction of heat into the snowpack as a function of the temperature gradient between the snow surface and the average temperature of the snowpack and by taking into account the temperature profile of the snowpack in the past 24 h luce and tarboton 2010 you et al 2014 in addition glacier melt processes were modeled with ueb sen gupta and tarboton 2013 the model equations and further descriptions can be found in previous publications mahat and tarboton 2012 mahat et al 2013 tarboton and luce 1996 ueb is a point model in that the equations describing the state flux relationships are applicable for a model element with uniform or representative values of terrain characteristics slope aspect etc canopy variables and meteorological forcing for spatially distributed modeling earlier research explored the use of depletion curves to deal with sub watershed variability luce et al 1999 work by sen gupta and tarboton 2013 configured the model to be run as fully distributed using cartesian grids in the gridded model the model computations are carried out separately on individual grid cells with the only interaction between grid cells occurring when aggregating outputs from watersheds or sub watersheds this configuration makes ueb amenable to parallel computation with domain decomposition that is constrained only by the aggregation operations the ueb model is driven by air temperature precipitation radiation humidity and wind speed the network common data form netcdf a data format that enables storage and manipulation of multi dimensional arrays http www unidata ucar edu is used as the input output format for ueb netcdf includes a set of libraries and tools that enable array oriented data access with advantages that include concurrent access multiple readers platform independence efficient sub setting and data appending i e additional data are added to a file without rewriting it or copying the entire contents a netcdf file is self contained as the metadata to describe the contents of the data and other ancillary information are stored within the file rew et al 2014 a benefit of using netcdf is that many array oriented datasets come in some form of gridded binary format compatible with netcdf the choice to use netcdf as input output format for ueb was driven by the vision to couple the model with data sources and other models that follow the same standard 3 2 ueb parallel the flow chart for the parallel version of the ueb model with mpi implementation developed in this work is shown in fig 1 many of the tasks including the weather forcing io operations are contained in a code block named run ueb in the grid cell for all time steps this step loops through all grid cells and runs the model for all time steps of the simulation period for a given grid cell the operations at each grid cell are carried out independently from the other grid cells this block of code takes more than 99 the total simulation time this code block was therefore parallelized with mpi in which the active grid cells excluding the no compute cells were divided into the total number of processes when the total number of grid cells is not evenly divisible by the number of processes some processes may be allocated an extra grid leaving the others with an idle time of one grid cell computation for large sized problems this idle time is expected to be small at the end of the simulation the processes collectively write results to output netcdf files one output file for each output variable this netcdf write requires synchronization among the processes as they access the netcdf file simultaneously one factor we evaluated in this study was the extent to which the io operations can be parallelized and the degree to which the synchronized access to data in netcdf files by multiple processes can affect the overall performance of the parallel codes for this implementation of the ueb model with mpi we compared an io reading writing in which multiple arrays of data were handled at once to the looping through the grids approach mentioned above that accesses a single array at a time multiple arrays versus one array at a time the flow chart for the model implementation with gpu is shown in fig 2 in this case a ueb class was defined first as a class that encapsulates watershed terrain and canopy variables as data members and the ueb simulation functions as methods member functions this results in an array of active grid cells consisting of an array of ueb class instances objects this configuration was chosen because it was easier to copy arrays of objects structures between the host cpu and device gpu nodes than copying individual variable arrays the cpu host allocates gpu device memory and copies the data to the device all the snow accumulation and ablation computations are carried out by the gpu functions i e kernels and outputs are copied back to the cpu node that writes them into netcdf output files in this case in contrast to the mpi implementation only a few cpu nodes carry out the io operations we implemented the cuda code in such a way that the changes to the ueb mpi code were kept minimal as can be seen from comparison of figs 1 and 2 the total number of code lines in the mpi version was 4930 of these 572 lines 11 were modified in the gpu version which generally involved edits to ensure compatibility with cuda devices while 303 lines 6 were removed and 570 new lines 11 were added the objective here was to evaluate if the observation by tristram et al 2014 that implementing gpu code with satisfactory performance may not necessarily require major re work on an existing programming code also applies to ueb an important difference between the gpu implementation and the one with mpi is that in the gpu case the time loop is outside of the grid cell loop i e simulations at all grid cells are carried out for a few time steps typically a few days before advancing to the next step this way the highly parallel nature of individual grid cell computations is taken advantage of without having to copy all the weather forcing data to the device at once copying weather forcing data for all time steps at once would require large memory at the device that could be difficult to allocate the flow chart in fig 2 was drawn assuming that the total number of gpu threads that can be scheduled equals or exceeds the total number of model grid cells which was the case here this assumption is unlikely to be an issue for most modeling cases like the one tested in this paper because the upper limit on the number of threads per device is quite large where the number of grid cells exceeds the number of cuda threads available a strategy similar to the assignment of grid cells to threads used in the mpi approach of fig 1 would be needed in the mpi implementation the active grid cells were evenly distributed among mpi processes when that was possible in both versions of the ueb model there is spatial variability in grid cell properties such as vegetation covered versus no vegetation and snow covered versus no snow that affects the number of iterations to converge to a particular solution in a given cell and that introduces some variability in the total compute time among processes threads this is considered to be part of simulation overhead and will diminish the efficiency in both parallel implementations compared to the ideal case of uniform grid cells two other sources of overhead are reading configuration files at the beginning of the program and an outputs aggregation step where watershed average or total quantities are computed the aggregation operations involve inter process communication where all processes transfer the values of the aggregated variables to the root process note that input forcing data reading and output writing are handled by the netcdf io and in this study are not considered part of the inter process communication 3 3 test case study the logan river watershed we used simulations of snow accumulation and melt for five years october 1 2007 september 30 2012 in the logan river watershed utah to evaluate the performance of the parallel codes the logan river watershed is a 554 km2 watershed located in the bear river range of utah in the western u s the watershed elevation ranges from 1497 to 3025 m with mean elevation of 2271 m most of the upland watershed is covered by shrubs grass and forest and is primarily used for grazing while the lower reaches of the river support irrigation the average precipitation varies between 450 and 1500 mm per year with most of it in the form of snow the river peaks late in the spring from snowmelt fig 3 shows the location map of the logan river watershed and its digital elevation map the input data were setup as follows the watershed domain was delineated from the 30 m usgs national elevation dataset digital elevation model ned dem using the terrain analysis software taudem tarboton et al 2009 tarboton 2015 tesfa et al 2011 terrain variables slope and aspect were calculated from the dem in esri s arcgis software www esri com the canopy leaf area index lai were obtained from the moderate resolution imaging spectroradiometer modis product mod15a2 the other vegetation variables including canopy height and fraction of the grid cell covered by vegetation were determined using the national land cover database 2011 nlcd 2011 homer et al 2015 weather forcing data were obtained from snow telemetry snotel stations https www wcc nrcs usda gov snow in and nearby the watershed these data were gridded with the bilinear interpolation method to grid size of 120 m the model resolution and downscaled adjusted for elevation using a methodology descripted by sen gupta and tarboton 2016 the focus of this study was evaluation of the performance of the parallel codes thus no calibration or validation of model parameters was performed apart from verification to make sure the parallel models outputs are consistent with those of the original serial version the ueb model input data for the logan river watershed used for testing the parallel models in this paper are provided as a hydroshare resource gichamo and tarboton 2019 3 4 description of computing resources both versions of the ueb parallel model were written as a platform independent code with the c programming language the gpu version uses cuda for the device codes performance tests were made in a linux clusters with up to 128 cores for the mpi version while the gpu code was tested on a linux machine with a cpu node linked to gpu node both these implementations were compared to a simulation with a single mpi process on a desktop pc the specifications of the computing resources are as follows linux cluster for mpi intel r xeon r cpu e5 4620 0 2 20 ghz maximum turbo boost frequency 2 60 ghz with 8 cores per cpu and 4 cpus per node for a total of 32 cores per node 64 threads per node with hyper threading enabled and up to 1 tb memory per node linux nodes with gpu intel r e5 2670 sandy bridge cpu nvidia k20x gpu with 2688 processor cores and processor core clock of 732 mhz desktop pc with intel r core i7 3770 4 cores with 3 40 ghz maximum turbo boost frequency 3 90 ghz and hyper threading 32 gb ram 3 5 performance metrics total simulation time speed up and efficiency were used to test the performance of the parallel codes speed up is computed as the ratio of the total simulation time by a single core to that by multiple cores p number of cores speed up varies with the total number of cores efficiency is the speed up divided by the number of cores eager et al 1989 for a given unit of work the efficiency may change with the number of cores due to an increased overhead inter process communication and or increased synchronization according to amdahl s law amdahl 1967 2007 the maximum possible speed up ideal speed up is constrained by the fraction of the code that has to be executed serially and hence is executed by all cores for the ueb model this was assumed to be the code outside of the loop through active grid cells portion of the code described above in reality however some of the codes inside the loop could also contribute to it these are generally considered overhead and this is partly why the ideal speed up is called ideal because in practice the overheads further reduce the speed up note that in an mpi implementation multiple processes can be executed on a single core in this study the tests were carried out to evaluate the performance speed up and efficiency as a function of the number of processor cores hence the number of mpi processes equals the number of cores assigned equations 1 3 below give speed up efficiency and representation of amdahl s law given a number of cores p equations 1 and 3 represent the ratio of two similar units often computed as the simulation time per one core divided by simulation time per multiple cores both equations 1 and 3 apply to processor cores that are of uniform type of similar core size and speed the units of the numerator and the denominator in equation 3 can thus be considered as that of time per unit core 1 s p t 1 t p 2 e p s p p 3 s p m a x 1 f s 1 f s p where sp speed up by p number of cores t1 execution time for a single core tp execution time for p number of cores ep efficiency for p number of cores p number of cores spmax maximum speed up based on amdahl s law fs fraction of code that can only be executed serially in the case of ueb with mpi with much of the code in the parallelizable loop through active grid cells block as described earlier speed up approaching the number of cores p is to be expected given little model setup overhead inter process communications that occurs only at the output aggregation step and few blocking operations that force processes to wait for each other some degradation from maximum efficiency is expected due to variability in the processing time for each cell an increase in the workload by increasing the size of the watershed and or the duration of simulation would primarily increase the tasks inside the loop and is expected to increase the speed up per total number of cores i e efficiency when computing the ideal speed up and efficiency using amdahl s law above we considered the inter process communications time to be part of the fraction of the code executed in serial and assumed that io operations were parallelizable an alternative analysis is to consider the inter process communications and io operations separately in such a situation we have ueb code sections for model setup run in serial inter process communication unlikely to be parallelizable io operations possibly parallelizable and model computational kernel expected to be highly parallelizable to demonstrate the effect of io operations on the parallel performance of ueb we use equation 4 which is a slightly modified equation from yavits et al 2014 p 7 eqn 13 for symmetric compute cores of uniform compute ability the modification in equation 4 here from equation 13 of yavits et al 2014 is that we assume the inter process communication to be negligible in ueb hence the term for inter process communication was dropped 4 s p m a x r 1 1 f p f p p t s t 1 where fp fraction of code that can be executed in parallel p number of cores t1 execution time for a single core ts io synchronization time sequential to parallel data synchronization time in yavits et al 2014 spmaxr maximum speed up based on amdahl s law revised to account for io the term t s t 1 in equation 4 is referred to as synchronization intensity in yavits et al 2014 here it accounts for the time spent reading input forcing data and writing ueb outputs from or to netcdf files the operations of file access and partitioning of data to the respective processes synchronizing are considered separate from the inter process communication in this study the effect of synchronization intensity is to decrease the ideal speed up and its importance increases for larger number of parallel cores 4 results and discussion fig 4 shows plots of the total simulation time speed up efficiency and ratio of io time to total simulation time as a function of the number of mpi processes same as number of cores these values were computed after running the model twice for each case number of processes and taking the average of the two simulation times it can be seen that the slope of the speed up curve decreases with increase in the number of processes fig 4b also compares the speed up against the maximum speed up based on amdahl s law excluding the code inside the loop through the grids which also includes io the remaining part of the code takes less than 0 01 of the total simulation time this initially suggested a highly parallelizable code which led to expectation of speed up close to the ideal speed up the actual speed up curve however is much lower than the ideal speed up curve and its slope decreases with increasing number of processes fig 4b and c also include speed up and efficiency plots for the model computational kernel i e excluding io operations as can be seen from the figures the speed up of the computational kernel is closer to the ideal speed up the reason for the poor performance of the total code compared to the computational kernel is that the io is not as readily parallelizable as the rest of the code for the serial version of the program the io takes about 1 7 of the total execution time of the code because this fraction of code ended up not being parallelized contrary to the assumption in equation 3 it affects the performance of the whole model with increasing importance as the number of processes is increased as shown in fig 4d the deviation of the computational kernel speed up from the ideal line can be caused by any of or a combination of the factors that were considered overhead during the design in section 2 in addition after the total active grid cells were evenly divided between the processes a few processes would be allocated one additional grid cell to simulate this means some processes may have to stay idle for a duration of one grid cell computation the time it takes for a full computation of one grid cell on average is about 2 5 s bridging the gap between the good scaling of the computational kernel and the poor scaling of the total model run caused by poor io scaling is important as the io starts to become dominant with increasing parallelism so much so that increasing the number of processor cores beyond 64 may not be justifiably useful while the parallel netcdf4 which is based on hdf5 s mpi io feature would enable synchronized file access by multiple processes efficient io scaling requires coupling it with some file access strategies that take advantage of the synchronization chilan et al 2006 fig 5 shows the results for a modified ueb io reading writing in which multiple arrays of data were handled at once instead of the looping through the grids approach used in fig 4 as can be seen in fig 5 the performance improves appreciably particularly for the higher number of cores for 64 cores the speed up increases from 24 to 42 while the efficiency increases from about 0 38 to 0 67 similarly for 128 cores the speed up and efficiency increase from 31 to 63 and 0 25 to 0 49 respectively this approach would reduce the total file access operations however it may require larger memory per core to be effective fig 6 is a re plot of fig 5a with the ideal speed up revised according to equation 4 to account for the effect of io operations this figure indicates that any further performance improvement would only come from better io parallelization the model run times for the gpu version with cuda code are shown in table 1 together with run times on desktop pc with one process the run time on desktop pc with one process is equivalent to that of a serial code on desktop pc table 1 reports the speed up with the cuda gpu relative to the serial code on desktop pc we do not have multiple simulations in gpu so there are no plots for gpu comparable to those for mpi in the gpu case the io operations were carried out by the cpu host while the numerical simulations were performed by the gpu device in addition to io operations data synchronization between the host and device is required table 1 also includes run times from the mpi implementations in linux cluster with one and 64 processes and their respective speed ups over the serial code on desktop pc the gpu implementation presents a slightly better speed up when compared to the mpi code executed on the cpu cluster of 64 processes table 1 our result leads us to a similar observation by tristram et al 2014 that implementing a gpu code with a performance comparable to other parallel methods may not necessarily require a major re work of an existing programming code in our case the computational kernel and much of the data partitioning code remained the same as it was written in c which is compatible to the cuda specification nvidia 2015 given the fact that gpus provide superior performance per total resource cost price of hardware and power usage see e g http www fmslib com mkt gpus html and considering the comparatively short developer time for some existing codes like ueb gpus appear to be a worthwhile alternative to the mpi implementation the speed up shown in the last column of table 1 is computed in a different way from the speed ups and efficiencies reported in figs 4 6 while the numbers in figs 4 6 help evaluate the performance of the parallel code with increasing cores the speed up values in table 1 are measures of the performance enhancement achieved by implementing concurrency programming using clusters of cpu and or gpu resources compared to a serial code on a desktop pc brodtkorb et al 2013 table 1 shows that our parallel implementations achieve speed ups of 8 10 times over a serial code on a desktop pc this represents the actual enhancement to hydrologic research due to improved access to high performance computation resources these results report the actual reduction in time spent simulating a ueb model instance using the linux cluster or the gpu resources rather than the desktop pc such reduction in modeling time facilitates the evaluation and adoption of physically based models like ueb in operational settings such as streamflow forecasting where computational time can be critical or for studies of effects of climate change which require large scale simulations the first column of table 1 for the model setting which involves reading the model domain terrain and parameter files by all processes serially has a very large value for mpi with 64 processes this large overhead was unexpected and contradicted our assumption earlier that the overhead would be negligible with an increase in the number of processes this overhead was likely caused by competitive file reading by multiple processes having a single root process read the model setting files and then broadcast the values to all processes reduces the model setting time to 1 s however the reduction in model setting time does not have a significant effect on the total simulation time and it does not change the conclusion about the overall speed up an anomaly in table 1 results is the large computation and hence large total time for a single mpi process on the linux cluster we would have anticipated this to be about the same as the desktop pc but it is about five times slower this difference cannot be explained by the difference in processor core speed between the two resources alone we do not know what the impacts of different amounts of memory are and whether there is an overhead involved in addressing the memory available on the nodes of the linux cluster or how comparable the memory access times are across these systems this difference remains a result that we do not fully understand it is partly because of this anomaly that we chose to report speed up relative to the desktop pc which although it has different hardware provides a more realistic and honest reporting of the speed up we have demonstrated that a researcher will experience further investigation is required to find out the causes for this difference such as the possible introduction of poorly optimized code sections during modification of the serial code to the parallel version and the choice of compiling optimization parameters for different platforms it is possible that the mpi implementation in this paper just like the gpu one can be further optimized to gain even better performance finally sample outputs of snow water equivalent swe simulated by the two parallel versions of ueb mpi and cuda gpu are shown in fig 7 this figure demonstrates that the outputs from the two implementations are for practical purposes equivalent this is expected since the data types and the computation logic remain unchanged in both implementations as stated earlier no parameter calibration and or validation by comparison to observations was done given that the objective of this study was assessing computational efficiency of the parallel implementations 5 summary and conclusions the implementation and evaluation of parallel processing methods in the utah energy balance ueb distributed snow accumulation and melt model is presented in this paper two parallel implementations of ueb were evaluated one using the message passing interface mpi and the other using cuda gpu continuous simulation of the logan river watershed for five years was used to test the performance of the parallel codes and the speed up and efficiency as function of number of processor cores and plots of speed up compared to the ideal speed up based on amdahl s law were used as performance metrics of the parallel codes for the mpi implementation results show the importance of input output io operations in the degradation of efficiency with increase in the number of processes in the serial code io accounts for about 1 7 of the code run time however as this is not reduced by parallelization its impact becomes more pronounced with increased number of processes this was verified using the revised form of amdahl s law yavits et al 2014 that separately accounts for io operations and inter process communications the plot of this revised form of amdahl s law indicates that further performance increase of the parallel code could only be possible with improved performance of the io operations the performance of the mpi implementation improves when utilizing an io strategy that reduces the number of file accesses by reading and writing multiple arrays of data in one step the cuda gpu implementation achieves slightly better performance with one gpu node when compared with the mpi implementation executed on 64 cores the gpu implementation was done without major refactoring of the original code as the computational kernel and much of the data partitioning code remain the same this shows that for some models such as ueb obtaining a cuda gpu performance comparable to other parallel methods does not necessarily require a major re work of an existing programming code given the fact that gpus provide superior performance per total resource cost price of hardware and power usage this makes it a worthwhile alternative to the mpi implementation overall our parallel implementations help achieve speed ups of 8 10 times over a serial code on a desktop pc this represents the actual speed up available to hydrologic researchers from use of high performance computing resources instead of a desktop pc most distributed physically based hydrological models are data intensive this work demonstrates the importance of including io operations within the parallelizable code section and using efficient io handling together with distributed computing resources efficient io scaling requires adopting file read write strategies that take advantage of parallel file access or separate files for different processes the approach we used in this paper is a simple one which involves reading and writing multiple arrays of data in one step and it could be limited by the availability of memory per core there is a need for more advanced io strategy that also accounts for the available memory finally the modeling work presented here is only for the case of snow accumulation and melt outputs from this model are used as input to runoff and river routing models we can qualitatively predict that coupling ueb to a runoff model would increase the arithmetic intensity because more computations would be done without significantly increasing the io volume additional inter process communications would be introduced but would likely be smaller than the added arithmetic operations therefore it would be interesting to extend this study to examine if a better efficiency may be achieved with the coupled model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national science foundation under collaborative grants eps 1135482 and 1135483 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation compute storage support and other resources from the division of research computing in the office of research and graduate studies at utah state university and advanced research computing center at the university of wyoming are gratefully acknowledged we appreciate the anonymous ems reviewer whose suggestions have improved this manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104614 
26067,the utah energy balance ueb model supports gridded simulation of snow processes over a watershed to enhance computational efficiency we developed two parallel versions of the model one using the message passing interface mpi and the other using nvidia s cuda code on graphics processing unit gpu evaluation of the speed up and efficiency of the mpi version shows that the effect of input output io operations on the parallel model performance increases as the number of processor cores increases as a result although the computation kernel scales well with the number of cores the efficiency of the parallel code as a whole degrades the performance improves when the number of io operations is reduced by reading writing larger data arrays the cuda gpu implementation was done without major refactoring of the original ueb code and tests demonstrated that satisfactory performance could be obtained without a major re work of the existing ueb code keywords utah energy balance snowmelt model ueb message passing interface mpi compute unified device architecture cuda graphics processing unit gpu parallel io software availability program name ueb parallel description parallel version of the utah energy balance snowmelt model ueb platform platform independent tested on microsoft windows linux centos 6 x source language c cuda cost license free open source gnu general public license developers tarboton research group utah state university availability http github com dtarb ueb 1 introduction hydrological models are used to predict environmental flow of water under diverse drivers of change that are complex and heterogeneous one of the prime motivators of current hydrological research is the need to understand and quantify the possible impacts on water resources of changes in climate land cover land use population and urbanization fowler et al 2007 such studies may require modeling of the hydrologic processes at various scales ranging from headwater watersheds to river basins scales as the terrestrial water cycle is affected by its interactions with atmospheric and oceanic processes hydrological models at river basin or global scale may also need to consider the various pathways of water in the global cycle and magnitudes of feed backs between different layers components of the cycle levine and salvucci 1999 maxwell et al 2014 paniconi and putti 2015 a few years ago wood et al 2011 made a call for hyperresolution global land surface modeling to sufficiently resolve local processes in a model of global or continental scale this task of modeling the hydrologic cycle at large scale with sufficient resolution of individual processes poses multiple challenges one of these challenges is the desire to use high performance computing hpc resources to reduce computational time or increase the level of detail and hence complexity at which these problems are investigated on the other hand the availability of hpc resources is increasing this coupled with the recognition of the scientific needs for undertaking large scale hydrologic simulation has led to development of simulation models that implement parallel processing technologies for example kollet et al 2010 present results of a study where an integrated multi dimensional modeling problem with a number of unknowns in the order of 109 was solved within a feasible simulation time the challenge for hydrological modelers is thus shifting from the lack of computing resources to reconfiguring their modeling software to be able to take advantage of these new resources it should be noted here that parallel programming in hydrological and environmental modeling is not a new opportunity or issue e g paglieri et al 1997 rao 2004 however in the past two decades a strong argument has been made that the basic approach to software development should incorporate concurrency multi processes programming because of the power wall i e the upper limit imposed on the clock speed of single core due to overheating of high frequency cores and other efficiency optimization considerations brodtkorb et al 2013 sutter 2005 sutter and larus 2005 concurrency programing has also been spurred by the definition of standard programing interfaces that abstract away most of the low level operations and a number of library implementations of these interfaces thereby freeing a research programmer to focus on domain specific modeling issues e g mpi http www mpi forum org openmp http openmp org wp given the desire to apply more physically based distributed high resolution hydrologic models and given the opportunities offered by the parallel programing standards and libraries the question has then become what method to choose for a given model and what factors affect efficient scaling of the modeling code in this study we evaluated parallel processing implementations of the utah energy balance ueb snow accumulation and melt model we evaluated two implementations one using the mpich2 library of the message passing interface mpi specification gropp et al 2005 and the other using nvidia s compute unified device architecture cuda code on graphics processing units gpus nickolls et al 2008 the mpi is a distributed memory programming approach that promises good efficiency for the distributed ueb model that requires independent data for different model grid cells on the other hand the cuda code with its compatibility to c enhances the accessibility of general purpose gpus that have ability to handle compute intensive tasks the computational performance of the parallel codes was evaluated using simulations of the logan river watershed utah for a period of five years for the implementation with mpi we evaluated the speed up and efficiency of the code with increasing number of processor cores and compared the speed up with the ideal speed up computed based on amdahl s law amdahl 1967 2007 with regard to the application of gpus neal et al 2010 had found earlier that even though their gpu code was faster and more efficient than their mpi implementation the development time it required was prohibitive in contrast tristram et al 2014 reported that not only were gpus more cost efficient for their application but also achieving satisfactory speed up with gpus did not require major refactoring of their existing code for the cuda implementation of the ueb code in this study we also evaluated if satisfactory performance could be achieved by the gpu code without major refactoring of the code this paper is organized as follows in the next section a brief discussion of factors to consider in parallel programing based on review of literature is given focusing on hydrologic models in section 3 methods we describe the ueb model the algorithms for the parallel implementations the modeling case study to test the performance of the parallel codes and the performance metrics results from the performance tests and discussion are given in section 4 followed by conclusions in section 5 2 factors to consider in parallel programing the choice of a particular parallel programing approach may depend on a number of factors including familiarity with the programing interface ease of adaptation of existing serial code to a parallel version and the data and memory configuration of the problem being modeled neal et al 2010 investigated the application for 2d flood inundation modeling of three of the commonly used programming methods shared memory open multi processing openmp distributed memory message passing interface mpi and graphics processing unit gpu they tested the three approaches with respect to applicability to a given problem solution parallel code efficiency achieved and required implementation effort development time they concluded that the mpi approach was the most suitable compromise between the efficiency achieved and programming complexity involved they found that even though the gpu code was the fastest and most efficient the development time it required was prohibitive another important factor in parallel programming of simulation models is domain data decomposition among processes data partitioning schemes often try to address the issue of load balancing between multiple processes a good example is a 2d flood inundation model where some of the grid cells in the flood plain remain dry for part of the model run time resulting in some idle process time data partitioning schemes should strive to minimize such idle times brodtkorb et al 2012 sanders et al 2010 with respect to hydrological models domain decomposition is related to the flow dependencies upstream to downstream between computational sub domains where the computational sub domain can be a representative elementary watershed rew hydrologic response unit hru a structured or unstructured grid cell or a river reach in addition load imbalance may arise from the spatial variability of processes considered such as areas covered with snow versus those with no snow upstream hillslope versus riparian area or river channel presence and type of vegetation etc some of the approaches used in recent research are described below the first one is the use of multiple layers in which simulation units grids are divided based on their degree of dependency on upstream units accordingly units that do not depend on other units are placed at highest priority layer and units that depend only on a single unit are placed in the second highest priority layer and so on liu et al 2014 another approach involves dynamic run time allocation of a data partition to an available idle process once the partition no longer depends on upstream processes li et al 2011 dividing a 2d model domain into strips where the only inter process communications are at the boundaries between two adjacent sub domains is another approach tarboton et al 2009 tesfa et al 2011 a different approach particularly useful for a model with a tightly coupled set of processes is collecting all the governing equations into a global system of equations which are solved by a matrix solver qu and duffy 2007 such a matrix solver may divide the global matrix into sub matrices that are mapped onto multiple processes the examples above do not form an exhaustive list however they indicate that generally the choice of a given domain decomposition would be dictated by the specific modeling problem small et al 2013 presently a researcher or a modeler has to consider their own problem domain and decide whether to use a method similar to those listed above or develop their own the ability to automatically choose from certain domain decomposition methods given a problem domain type is a potential area of future study the extent to which a parallel program s performance increases with increased availability of computing resources e g number of processor cores depends primarily on the fraction of code that must be executed in serial this is the essence of amdahl s law amdahl 1967 2007 based on which the maximum possible speed up for a given problem is computed as a function of the number of cores and the serial fraction of the code the effect of the serial fraction of the code becomes more important with increasing parallelization yavits et al 2014 review a number of research works that dealt with the effect of data transfer between the serial and parallel portions of the code and the inter process communications on the maximum speed up determined from amdahl s law they provide models that revise amdahl s law incorporating terms that represent arithmetic intensity the ratio of total compute operations to data transfer computations data transfer synchronization between the serial and parallel portions of the program and inter process communication and synchronization accordingly the amount of inter process communication and or serial to parallel data synchronization in a program might inhibit its suitability for extensive parallelization for problems with relatively high inter process communication and data synchronization they suggest using fewer large cores rather than many small cores finally the cost efficiency of the parallel method will have to be considered as indicated earlier neal et al 2010 concluded that the developer time for programming using graphics cards was prohibitive in their modeling case however total cost should also include the price of the computing hardware units and operating costs tristram et al 2014 reported results from a parallel hydrologic uncertainty model with multiple ensembles using gpus they compared the cpu and gpu performances with respect to speed up the cost of processors and the cost of power usage and found gpus to be more cost efficient for their application regarding programmer s time they showed that to achieve satisfactory speed up with gpus major refactoring of their existing code was not necessary in addition the performance was further enhanced with an optimization involving memory access configuration the general purpose programming toolkits such as cuda https developer nvidia com cuda toolkit and opencl https www khronos org opencl coupled with the cheap graphics cards on commodity computers make gpu programming more accessible to scientific research programmers garland et al 2008 however realizing the full benefits still requires learning efficient program organization and optimizations such as latency hiding by overlapping computation with input output io operations wise management of register and caches and memory layout configuration de la asunción et al 2012 tristram et al 2014 which requires more effort and time brodtkorb et al 2013 3 methods 3 1 utah energy balance ueb snowmelt model the utah energy balance ueb snowmelt model tracks the accumulation and ablation of a single snow layer at the ground surface by energy and mass balance computations tarboton and luce 1996 in forested watersheds the model accounts for canopy snow interception partitioning of incoming solar and atmospheric radiation through the canopy layer and turbulent fluxes of latent and sensible heat within and below the canopy layer mahat and tarboton 2012 2014 mahat et al 2013 the snow surface temperature is computed using the modified force restore method that characterizes the conduction of heat into the snowpack as a function of the temperature gradient between the snow surface and the average temperature of the snowpack and by taking into account the temperature profile of the snowpack in the past 24 h luce and tarboton 2010 you et al 2014 in addition glacier melt processes were modeled with ueb sen gupta and tarboton 2013 the model equations and further descriptions can be found in previous publications mahat and tarboton 2012 mahat et al 2013 tarboton and luce 1996 ueb is a point model in that the equations describing the state flux relationships are applicable for a model element with uniform or representative values of terrain characteristics slope aspect etc canopy variables and meteorological forcing for spatially distributed modeling earlier research explored the use of depletion curves to deal with sub watershed variability luce et al 1999 work by sen gupta and tarboton 2013 configured the model to be run as fully distributed using cartesian grids in the gridded model the model computations are carried out separately on individual grid cells with the only interaction between grid cells occurring when aggregating outputs from watersheds or sub watersheds this configuration makes ueb amenable to parallel computation with domain decomposition that is constrained only by the aggregation operations the ueb model is driven by air temperature precipitation radiation humidity and wind speed the network common data form netcdf a data format that enables storage and manipulation of multi dimensional arrays http www unidata ucar edu is used as the input output format for ueb netcdf includes a set of libraries and tools that enable array oriented data access with advantages that include concurrent access multiple readers platform independence efficient sub setting and data appending i e additional data are added to a file without rewriting it or copying the entire contents a netcdf file is self contained as the metadata to describe the contents of the data and other ancillary information are stored within the file rew et al 2014 a benefit of using netcdf is that many array oriented datasets come in some form of gridded binary format compatible with netcdf the choice to use netcdf as input output format for ueb was driven by the vision to couple the model with data sources and other models that follow the same standard 3 2 ueb parallel the flow chart for the parallel version of the ueb model with mpi implementation developed in this work is shown in fig 1 many of the tasks including the weather forcing io operations are contained in a code block named run ueb in the grid cell for all time steps this step loops through all grid cells and runs the model for all time steps of the simulation period for a given grid cell the operations at each grid cell are carried out independently from the other grid cells this block of code takes more than 99 the total simulation time this code block was therefore parallelized with mpi in which the active grid cells excluding the no compute cells were divided into the total number of processes when the total number of grid cells is not evenly divisible by the number of processes some processes may be allocated an extra grid leaving the others with an idle time of one grid cell computation for large sized problems this idle time is expected to be small at the end of the simulation the processes collectively write results to output netcdf files one output file for each output variable this netcdf write requires synchronization among the processes as they access the netcdf file simultaneously one factor we evaluated in this study was the extent to which the io operations can be parallelized and the degree to which the synchronized access to data in netcdf files by multiple processes can affect the overall performance of the parallel codes for this implementation of the ueb model with mpi we compared an io reading writing in which multiple arrays of data were handled at once to the looping through the grids approach mentioned above that accesses a single array at a time multiple arrays versus one array at a time the flow chart for the model implementation with gpu is shown in fig 2 in this case a ueb class was defined first as a class that encapsulates watershed terrain and canopy variables as data members and the ueb simulation functions as methods member functions this results in an array of active grid cells consisting of an array of ueb class instances objects this configuration was chosen because it was easier to copy arrays of objects structures between the host cpu and device gpu nodes than copying individual variable arrays the cpu host allocates gpu device memory and copies the data to the device all the snow accumulation and ablation computations are carried out by the gpu functions i e kernels and outputs are copied back to the cpu node that writes them into netcdf output files in this case in contrast to the mpi implementation only a few cpu nodes carry out the io operations we implemented the cuda code in such a way that the changes to the ueb mpi code were kept minimal as can be seen from comparison of figs 1 and 2 the total number of code lines in the mpi version was 4930 of these 572 lines 11 were modified in the gpu version which generally involved edits to ensure compatibility with cuda devices while 303 lines 6 were removed and 570 new lines 11 were added the objective here was to evaluate if the observation by tristram et al 2014 that implementing gpu code with satisfactory performance may not necessarily require major re work on an existing programming code also applies to ueb an important difference between the gpu implementation and the one with mpi is that in the gpu case the time loop is outside of the grid cell loop i e simulations at all grid cells are carried out for a few time steps typically a few days before advancing to the next step this way the highly parallel nature of individual grid cell computations is taken advantage of without having to copy all the weather forcing data to the device at once copying weather forcing data for all time steps at once would require large memory at the device that could be difficult to allocate the flow chart in fig 2 was drawn assuming that the total number of gpu threads that can be scheduled equals or exceeds the total number of model grid cells which was the case here this assumption is unlikely to be an issue for most modeling cases like the one tested in this paper because the upper limit on the number of threads per device is quite large where the number of grid cells exceeds the number of cuda threads available a strategy similar to the assignment of grid cells to threads used in the mpi approach of fig 1 would be needed in the mpi implementation the active grid cells were evenly distributed among mpi processes when that was possible in both versions of the ueb model there is spatial variability in grid cell properties such as vegetation covered versus no vegetation and snow covered versus no snow that affects the number of iterations to converge to a particular solution in a given cell and that introduces some variability in the total compute time among processes threads this is considered to be part of simulation overhead and will diminish the efficiency in both parallel implementations compared to the ideal case of uniform grid cells two other sources of overhead are reading configuration files at the beginning of the program and an outputs aggregation step where watershed average or total quantities are computed the aggregation operations involve inter process communication where all processes transfer the values of the aggregated variables to the root process note that input forcing data reading and output writing are handled by the netcdf io and in this study are not considered part of the inter process communication 3 3 test case study the logan river watershed we used simulations of snow accumulation and melt for five years october 1 2007 september 30 2012 in the logan river watershed utah to evaluate the performance of the parallel codes the logan river watershed is a 554 km2 watershed located in the bear river range of utah in the western u s the watershed elevation ranges from 1497 to 3025 m with mean elevation of 2271 m most of the upland watershed is covered by shrubs grass and forest and is primarily used for grazing while the lower reaches of the river support irrigation the average precipitation varies between 450 and 1500 mm per year with most of it in the form of snow the river peaks late in the spring from snowmelt fig 3 shows the location map of the logan river watershed and its digital elevation map the input data were setup as follows the watershed domain was delineated from the 30 m usgs national elevation dataset digital elevation model ned dem using the terrain analysis software taudem tarboton et al 2009 tarboton 2015 tesfa et al 2011 terrain variables slope and aspect were calculated from the dem in esri s arcgis software www esri com the canopy leaf area index lai were obtained from the moderate resolution imaging spectroradiometer modis product mod15a2 the other vegetation variables including canopy height and fraction of the grid cell covered by vegetation were determined using the national land cover database 2011 nlcd 2011 homer et al 2015 weather forcing data were obtained from snow telemetry snotel stations https www wcc nrcs usda gov snow in and nearby the watershed these data were gridded with the bilinear interpolation method to grid size of 120 m the model resolution and downscaled adjusted for elevation using a methodology descripted by sen gupta and tarboton 2016 the focus of this study was evaluation of the performance of the parallel codes thus no calibration or validation of model parameters was performed apart from verification to make sure the parallel models outputs are consistent with those of the original serial version the ueb model input data for the logan river watershed used for testing the parallel models in this paper are provided as a hydroshare resource gichamo and tarboton 2019 3 4 description of computing resources both versions of the ueb parallel model were written as a platform independent code with the c programming language the gpu version uses cuda for the device codes performance tests were made in a linux clusters with up to 128 cores for the mpi version while the gpu code was tested on a linux machine with a cpu node linked to gpu node both these implementations were compared to a simulation with a single mpi process on a desktop pc the specifications of the computing resources are as follows linux cluster for mpi intel r xeon r cpu e5 4620 0 2 20 ghz maximum turbo boost frequency 2 60 ghz with 8 cores per cpu and 4 cpus per node for a total of 32 cores per node 64 threads per node with hyper threading enabled and up to 1 tb memory per node linux nodes with gpu intel r e5 2670 sandy bridge cpu nvidia k20x gpu with 2688 processor cores and processor core clock of 732 mhz desktop pc with intel r core i7 3770 4 cores with 3 40 ghz maximum turbo boost frequency 3 90 ghz and hyper threading 32 gb ram 3 5 performance metrics total simulation time speed up and efficiency were used to test the performance of the parallel codes speed up is computed as the ratio of the total simulation time by a single core to that by multiple cores p number of cores speed up varies with the total number of cores efficiency is the speed up divided by the number of cores eager et al 1989 for a given unit of work the efficiency may change with the number of cores due to an increased overhead inter process communication and or increased synchronization according to amdahl s law amdahl 1967 2007 the maximum possible speed up ideal speed up is constrained by the fraction of the code that has to be executed serially and hence is executed by all cores for the ueb model this was assumed to be the code outside of the loop through active grid cells portion of the code described above in reality however some of the codes inside the loop could also contribute to it these are generally considered overhead and this is partly why the ideal speed up is called ideal because in practice the overheads further reduce the speed up note that in an mpi implementation multiple processes can be executed on a single core in this study the tests were carried out to evaluate the performance speed up and efficiency as a function of the number of processor cores hence the number of mpi processes equals the number of cores assigned equations 1 3 below give speed up efficiency and representation of amdahl s law given a number of cores p equations 1 and 3 represent the ratio of two similar units often computed as the simulation time per one core divided by simulation time per multiple cores both equations 1 and 3 apply to processor cores that are of uniform type of similar core size and speed the units of the numerator and the denominator in equation 3 can thus be considered as that of time per unit core 1 s p t 1 t p 2 e p s p p 3 s p m a x 1 f s 1 f s p where sp speed up by p number of cores t1 execution time for a single core tp execution time for p number of cores ep efficiency for p number of cores p number of cores spmax maximum speed up based on amdahl s law fs fraction of code that can only be executed serially in the case of ueb with mpi with much of the code in the parallelizable loop through active grid cells block as described earlier speed up approaching the number of cores p is to be expected given little model setup overhead inter process communications that occurs only at the output aggregation step and few blocking operations that force processes to wait for each other some degradation from maximum efficiency is expected due to variability in the processing time for each cell an increase in the workload by increasing the size of the watershed and or the duration of simulation would primarily increase the tasks inside the loop and is expected to increase the speed up per total number of cores i e efficiency when computing the ideal speed up and efficiency using amdahl s law above we considered the inter process communications time to be part of the fraction of the code executed in serial and assumed that io operations were parallelizable an alternative analysis is to consider the inter process communications and io operations separately in such a situation we have ueb code sections for model setup run in serial inter process communication unlikely to be parallelizable io operations possibly parallelizable and model computational kernel expected to be highly parallelizable to demonstrate the effect of io operations on the parallel performance of ueb we use equation 4 which is a slightly modified equation from yavits et al 2014 p 7 eqn 13 for symmetric compute cores of uniform compute ability the modification in equation 4 here from equation 13 of yavits et al 2014 is that we assume the inter process communication to be negligible in ueb hence the term for inter process communication was dropped 4 s p m a x r 1 1 f p f p p t s t 1 where fp fraction of code that can be executed in parallel p number of cores t1 execution time for a single core ts io synchronization time sequential to parallel data synchronization time in yavits et al 2014 spmaxr maximum speed up based on amdahl s law revised to account for io the term t s t 1 in equation 4 is referred to as synchronization intensity in yavits et al 2014 here it accounts for the time spent reading input forcing data and writing ueb outputs from or to netcdf files the operations of file access and partitioning of data to the respective processes synchronizing are considered separate from the inter process communication in this study the effect of synchronization intensity is to decrease the ideal speed up and its importance increases for larger number of parallel cores 4 results and discussion fig 4 shows plots of the total simulation time speed up efficiency and ratio of io time to total simulation time as a function of the number of mpi processes same as number of cores these values were computed after running the model twice for each case number of processes and taking the average of the two simulation times it can be seen that the slope of the speed up curve decreases with increase in the number of processes fig 4b also compares the speed up against the maximum speed up based on amdahl s law excluding the code inside the loop through the grids which also includes io the remaining part of the code takes less than 0 01 of the total simulation time this initially suggested a highly parallelizable code which led to expectation of speed up close to the ideal speed up the actual speed up curve however is much lower than the ideal speed up curve and its slope decreases with increasing number of processes fig 4b and c also include speed up and efficiency plots for the model computational kernel i e excluding io operations as can be seen from the figures the speed up of the computational kernel is closer to the ideal speed up the reason for the poor performance of the total code compared to the computational kernel is that the io is not as readily parallelizable as the rest of the code for the serial version of the program the io takes about 1 7 of the total execution time of the code because this fraction of code ended up not being parallelized contrary to the assumption in equation 3 it affects the performance of the whole model with increasing importance as the number of processes is increased as shown in fig 4d the deviation of the computational kernel speed up from the ideal line can be caused by any of or a combination of the factors that were considered overhead during the design in section 2 in addition after the total active grid cells were evenly divided between the processes a few processes would be allocated one additional grid cell to simulate this means some processes may have to stay idle for a duration of one grid cell computation the time it takes for a full computation of one grid cell on average is about 2 5 s bridging the gap between the good scaling of the computational kernel and the poor scaling of the total model run caused by poor io scaling is important as the io starts to become dominant with increasing parallelism so much so that increasing the number of processor cores beyond 64 may not be justifiably useful while the parallel netcdf4 which is based on hdf5 s mpi io feature would enable synchronized file access by multiple processes efficient io scaling requires coupling it with some file access strategies that take advantage of the synchronization chilan et al 2006 fig 5 shows the results for a modified ueb io reading writing in which multiple arrays of data were handled at once instead of the looping through the grids approach used in fig 4 as can be seen in fig 5 the performance improves appreciably particularly for the higher number of cores for 64 cores the speed up increases from 24 to 42 while the efficiency increases from about 0 38 to 0 67 similarly for 128 cores the speed up and efficiency increase from 31 to 63 and 0 25 to 0 49 respectively this approach would reduce the total file access operations however it may require larger memory per core to be effective fig 6 is a re plot of fig 5a with the ideal speed up revised according to equation 4 to account for the effect of io operations this figure indicates that any further performance improvement would only come from better io parallelization the model run times for the gpu version with cuda code are shown in table 1 together with run times on desktop pc with one process the run time on desktop pc with one process is equivalent to that of a serial code on desktop pc table 1 reports the speed up with the cuda gpu relative to the serial code on desktop pc we do not have multiple simulations in gpu so there are no plots for gpu comparable to those for mpi in the gpu case the io operations were carried out by the cpu host while the numerical simulations were performed by the gpu device in addition to io operations data synchronization between the host and device is required table 1 also includes run times from the mpi implementations in linux cluster with one and 64 processes and their respective speed ups over the serial code on desktop pc the gpu implementation presents a slightly better speed up when compared to the mpi code executed on the cpu cluster of 64 processes table 1 our result leads us to a similar observation by tristram et al 2014 that implementing a gpu code with a performance comparable to other parallel methods may not necessarily require a major re work of an existing programming code in our case the computational kernel and much of the data partitioning code remained the same as it was written in c which is compatible to the cuda specification nvidia 2015 given the fact that gpus provide superior performance per total resource cost price of hardware and power usage see e g http www fmslib com mkt gpus html and considering the comparatively short developer time for some existing codes like ueb gpus appear to be a worthwhile alternative to the mpi implementation the speed up shown in the last column of table 1 is computed in a different way from the speed ups and efficiencies reported in figs 4 6 while the numbers in figs 4 6 help evaluate the performance of the parallel code with increasing cores the speed up values in table 1 are measures of the performance enhancement achieved by implementing concurrency programming using clusters of cpu and or gpu resources compared to a serial code on a desktop pc brodtkorb et al 2013 table 1 shows that our parallel implementations achieve speed ups of 8 10 times over a serial code on a desktop pc this represents the actual enhancement to hydrologic research due to improved access to high performance computation resources these results report the actual reduction in time spent simulating a ueb model instance using the linux cluster or the gpu resources rather than the desktop pc such reduction in modeling time facilitates the evaluation and adoption of physically based models like ueb in operational settings such as streamflow forecasting where computational time can be critical or for studies of effects of climate change which require large scale simulations the first column of table 1 for the model setting which involves reading the model domain terrain and parameter files by all processes serially has a very large value for mpi with 64 processes this large overhead was unexpected and contradicted our assumption earlier that the overhead would be negligible with an increase in the number of processes this overhead was likely caused by competitive file reading by multiple processes having a single root process read the model setting files and then broadcast the values to all processes reduces the model setting time to 1 s however the reduction in model setting time does not have a significant effect on the total simulation time and it does not change the conclusion about the overall speed up an anomaly in table 1 results is the large computation and hence large total time for a single mpi process on the linux cluster we would have anticipated this to be about the same as the desktop pc but it is about five times slower this difference cannot be explained by the difference in processor core speed between the two resources alone we do not know what the impacts of different amounts of memory are and whether there is an overhead involved in addressing the memory available on the nodes of the linux cluster or how comparable the memory access times are across these systems this difference remains a result that we do not fully understand it is partly because of this anomaly that we chose to report speed up relative to the desktop pc which although it has different hardware provides a more realistic and honest reporting of the speed up we have demonstrated that a researcher will experience further investigation is required to find out the causes for this difference such as the possible introduction of poorly optimized code sections during modification of the serial code to the parallel version and the choice of compiling optimization parameters for different platforms it is possible that the mpi implementation in this paper just like the gpu one can be further optimized to gain even better performance finally sample outputs of snow water equivalent swe simulated by the two parallel versions of ueb mpi and cuda gpu are shown in fig 7 this figure demonstrates that the outputs from the two implementations are for practical purposes equivalent this is expected since the data types and the computation logic remain unchanged in both implementations as stated earlier no parameter calibration and or validation by comparison to observations was done given that the objective of this study was assessing computational efficiency of the parallel implementations 5 summary and conclusions the implementation and evaluation of parallel processing methods in the utah energy balance ueb distributed snow accumulation and melt model is presented in this paper two parallel implementations of ueb were evaluated one using the message passing interface mpi and the other using cuda gpu continuous simulation of the logan river watershed for five years was used to test the performance of the parallel codes and the speed up and efficiency as function of number of processor cores and plots of speed up compared to the ideal speed up based on amdahl s law were used as performance metrics of the parallel codes for the mpi implementation results show the importance of input output io operations in the degradation of efficiency with increase in the number of processes in the serial code io accounts for about 1 7 of the code run time however as this is not reduced by parallelization its impact becomes more pronounced with increased number of processes this was verified using the revised form of amdahl s law yavits et al 2014 that separately accounts for io operations and inter process communications the plot of this revised form of amdahl s law indicates that further performance increase of the parallel code could only be possible with improved performance of the io operations the performance of the mpi implementation improves when utilizing an io strategy that reduces the number of file accesses by reading and writing multiple arrays of data in one step the cuda gpu implementation achieves slightly better performance with one gpu node when compared with the mpi implementation executed on 64 cores the gpu implementation was done without major refactoring of the original code as the computational kernel and much of the data partitioning code remain the same this shows that for some models such as ueb obtaining a cuda gpu performance comparable to other parallel methods does not necessarily require a major re work of an existing programming code given the fact that gpus provide superior performance per total resource cost price of hardware and power usage this makes it a worthwhile alternative to the mpi implementation overall our parallel implementations help achieve speed ups of 8 10 times over a serial code on a desktop pc this represents the actual speed up available to hydrologic researchers from use of high performance computing resources instead of a desktop pc most distributed physically based hydrological models are data intensive this work demonstrates the importance of including io operations within the parallelizable code section and using efficient io handling together with distributed computing resources efficient io scaling requires adopting file read write strategies that take advantage of parallel file access or separate files for different processes the approach we used in this paper is a simple one which involves reading and writing multiple arrays of data in one step and it could be limited by the availability of memory per core there is a need for more advanced io strategy that also accounts for the available memory finally the modeling work presented here is only for the case of snow accumulation and melt outputs from this model are used as input to runoff and river routing models we can qualitatively predict that coupling ueb to a runoff model would increase the arithmetic intensity because more computations would be done without significantly increasing the io volume additional inter process communications would be introduced but would likely be smaller than the added arithmetic operations therefore it would be interesting to extend this study to examine if a better efficiency may be achieved with the coupled model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national science foundation under collaborative grants eps 1135482 and 1135483 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation compute storage support and other resources from the division of research computing in the office of research and graduate studies at utah state university and advanced research computing center at the university of wyoming are gratefully acknowledged we appreciate the anonymous ems reviewer whose suggestions have improved this manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104614 
26068,this paper presents a matlab framework for forecasting optimal flow releases in a multi storage system for flood control this framework combines four widely used models intended for 1 performing hydrologic analysis for a watershed and for level pool routing in the storage systems 2 simulating river inundation 3 solving the optimization problem of determining hourly optimal flow releases in a multi storage system and 4 data management and plotting the integration of all software is performed in matlab which is a state of the art and an easy to use environment for integrating computation visualization and programming this paper focuses on 1 presenting the matlab scripts for interfacing the aforementioned four software 2 describing the rationale for the objective function of the optimization and 3 demonstrating the practical use of the matlab framework by applying it to the operation of a hypothetical eight pond system in the cypress creek watershed in houston texas the results of the aforementioned application show that this framework could help in mitigating flooding keywords flood control genetic algorithms hec dssvue hec hms hec ras matlab watershed 1 introduction according to the national oceanic and atmospheric administration noaa 2016 inland flooding produces more damage annually than any other weather event in the united states for the period between 1976 and 2006 the damage produced by inland flooding averaged 6 9 billion per year noaa 2016 according to the earth observatory of nasa nasa 2017 global warming will change the major climate patterns and in general as precipitation patterns change storms floods and droughts will be more severe furthermore the changes in land use associated with increasing trends in urban development will increase the peak discharge and frequency of floods usgs 2003 recently flood mitigation within the context of a watershed where the entire watershed becomes the objective of management is receiving increasing attention kusler 2004 flotemersch et al 2016 within a watershed often a network of storage systems e g detention ponds wetlands reservoirs is available for flood control however they are rarely managed in a coordinated manner the lack of coordination of storage systems control may limit their effectiveness for flood mitigation furthermore the remote operation of water release in storage systems is not common and it is often limited to reservoirs in an ideal flood mitigation system all storage systems of the watershed would be operated in a coordinated manner according to precipitation forecasts to achieve certain objectives such as minimizing inundation levels at certain sections of the river furthermore the storage systems would be partially or totally emptied ahead of e g a few hours or a couple of days before a heavy rainfall that would produce flooding the storage made available by the early release would provide extra water storage during the heavy rainfall thus mitigating floods detention ponds and reservoirs are often constructed for mitigating floods as a main objective and these systems could be operated aggressively during flooding conditions for mitigating inundation the operation of wetlands for flood control however could have ecological implications for instance many wetlands retain some water year round supporting long lived aquatic animals such as fish as well as wetland amphibians and invertebrates leon et al 2018 however there are also many wetlands that naturally have a short hydroperiod so their function is not necessarily decreased by partial draining leon et al 2018 as pointed out by leon et al 2018 draining from wetlands involves some risks that can be minimized by not draining the wetlands fully and by draining only when the certainty of storm events is very high in any case if wetlands would be used for flood control they would be wetlands that are naturally flooded for only short periods of the year a short hydroperiod and support primarily facultative wetland animals and plant species that benefit from or can tolerate occasional flooded conditions but do not require them most of the year snodgrass et al 2000 ehrenfeld 2004 tarr et al 2005 leon et al 2018 the target audience of this work is anyone interested in near real time flood control this could include engineers responsible for flood management and short term flood control this work is also intended for researchers and students interested in open source techniques for flood mitigation the focus of this manuscript is on flood control and thus the storage used for flood mitigation would exclude wetlands that support long lived aquatic animals to facilitate the coordinated operation of a multi storage system for flood mitigation the integration of hydrology and hydraulic models within an optimization framework would be required users often have unique applications that may require modifying the framework for their particular needs to achieve the latter it is desirable to integrate the aforementioned models within a state of the art and an easy to use environment that is suitable for computation visualization and programming one of such platforms is matlab which is a high performance language for technical computing that integrates computation visualization and programming in an easy to use environment mathworks 2015 this paper presents a matlab framework for forecasting optimal flow releases in a multi storage system for flood control this paper is organized as follows 1 the sub component models and the rationale for the objective function are briefly described 2 matlab scripts for interfacing the aforementioned sub component models are presented 3 a brief overview of a case study is presented to illustrate the application of the matlab framework finally the key results are summarized in the conclusion 2 considerations for the optimal and coordinated operation of storage systems for flood mitigation it is clear that each watershed prone to flooding may have specific challenges and different topographical conditions however in general most of these watersheds have inline storage systems e g reservoirs and off line storage systems e g detention ponds wetlands to make possible the optimal and coordinated operation of storage systems for flood control it would be necessary to add or retrofit gates siphons for water release of the storage systems so they can discharge most of the water stored in a pre determined time this task would include implementing the necessary hardware and software to make possible the remote operation of gates siphons in wetlands detention ponds and reservoirs remote operation of gates siphons are necessary because very often wetlands detention ponds and reservoirs are located in remote areas far away from urban areas the remote operation could be achieved using the classical supervisory control and data acquisition scada system or using latest and cheaper technologies such as those presented in leon and verma 2019 it is noted that adding conventional drainage pipes e g sloped pipes would require substantial earthwork as they would need to be installed in the earthen berm conversely installing siphons require minimal earthwork as only anchoring of the siphon pipes over the berms would be necessary it is noted that a siphon flow uphill flow followed by a downhill flow or a pure downhill flow are gravity driven flows which require a hydraulic gradient between the water surface level in the storage system and the discharge point e g natural or artificial drainage ditch in many instances even near flat land can be engineered to create a storage system and achieve a hydraulic gradient of at least 0 6 or 0 9 m which would still give significant flow velocities exceeding 1 m s a storage system could be engineered to be a single pond or an interconnected system of multiple ponds leon et al 2018 if the area for the pond construction is consistently flat the earthwork of a single pond would likely be the most economical option however in most cases the terrain is not horizontally leveled and the earthwork would be significantly reduced if a system of interconnected ponds with different bottom levels is used leon et al 2018 3 model description as mentioned earlier the coordinated operation of storage systems for flood control requires a computational framework that integrates hydrologic and hydraulic models within an optimization framework there is a vast array of hydrologic hydraulic and optimization models available in the literature because one of the most popular and freely available models for simulating river inundation is the u s army corps of engineers hydrologic engineering center s river analysis system hec ras hydrologic engineering center 2016a hydrologic engineering center 2016b this model was selected as the hydraulic model of the proposed framework the hydrologic model was selected based on three criteria 1 compatibility with the aforementioned hydraulic model 2 freely available and 3 widely used in practice based on these considerations the u s army corps of engineers hydrologic modeling system hec hms hydrologic engineering center 2017 was used as the hydrologic model for determining the optimal schedules of flow releases in a multi storage system it is needed an optimization solver intended for large scale constrained optimization problems a widely used solver in water resources engineering e g yang et al 2015 chen et al 2016 that meets the above criteria is the genetic algorithm ga because the proposed framework was implemented in matlab and due to the availability of the genetic algorithm toolbox within matlab this toolbox is used herein to minimize scripting for data exchange between hec hms and hec ras e g manipulation of hec dss files the u s army corps of engineers hec dssvue model which has several built in utilities for manipulating data in a hec dss database was used the hec hms hec ras matlab ga toolbox and hec dssvue model are briefly described below 3 1 hydrologic modeling system hec hms hec hms is intended to simulate the complete hydrologic processes of dendritic watershed systems and includes hydrologic analysis procedures such as event infiltration unit hydrographs and hydrologic routing hydrologic engineering center 2017 hec hms is often used for studies of flow forecasting future urbanization impact reservoir spillway design flood damage reduction floodplain regulation and systems operation hydrologic engineering center 2017 3 2 hydrologic engineering center s river analysis system hec ras a highly adopted software in inundation modeling is the u s army corps of engineers hydrologic engineering center s river analysis system hec ras hec ras can perform one and two dimensional hydraulic calculations for a full network of natural and constructed channels overbank floodplain areas levee protected areas etc hydrologic engineering center 2016a 2016b hec ras has four main modules 1 steady flow water surface profiles which is intended for calculating water surface profiles in steady gradually varied flow conditions 2 unsteady flow simulation which can simulate one dimensional two dimensional and combined one two dimensional unsteady flow through an open channel network 3 sediment transport computations which is intended for the simulation of one dimensional sediment transport movable boundary calculations resulting from scour and deposition over moderate to long time periods and 4 water quality analysis which is intended for performing riverine water quality analyses hydrologic engineering center 2016a 2016b standard applications of this model include flood wave routing and flood inundation studies in the present work inundation is simulated using the unsteady one dimensional 1d hec ras model which solves the full 1d saint venant equations using the four point implicit finite difference scheme hydrologic engineering center 2016a 3 3 the matlab genetic algorithm ga toolbox the optimization model uses the matlab genetic algorithm ga toolbox chipperfield and fleming 1995 the ga solves constrained and unconstrained optimization problems based on a natural selection process that mimics biological evolution chipperfield and fleming 1995 the ga repeatedly changes a population of individual solutions at each generation the ga randomly selects individuals from the current population and uses them as parents to produce children for the next generation after several generations the population is expected to evolve toward an optimal solution the ga is recommended to solve problems that are not well suited for standard optimization algorithms including problems in which the objective function is discontinuous nondifferentiable stochastic or highly non linear chipperfield and fleming 1995 for more details about the genetic algorithm and its application to water resources the reader is referred to wardlaw and sharif 1999 leon and kanashiro 2010 leon et al 2014 lerma et al 2015 yang et al 2015 and chen et al 2016 3 4 hec dssvue hec dssvue is a java based visual utilities software intended to plot tabulate and manipulate data in a hec dss database file hydrologic engineering center 2009 hec dssvue incorporates over sixty mathematical functions and provides several utility functions to enter data sets into a hec dss database rename data set names copy data sets to other hec dss database files and delete data sets hydrologic engineering center 2009 more importantly for the present work hec dssvue incorporates the jython standard scripting language which allows to execute a sequence of steps in a text format from a batch process hydrologic engineering center 2009 3 5 objective function a single objective function f is considered in the optimization which consists that during flood conditions the flow must be conveyed slightly below the specified maximum water level at the cross sections of interest e g maximum water levels without producing flooding in urban areas this objective is written as 1 f c u i 1 u j 1 v δ u e i j e i 2 c l i 1 u j 1 v δ l e i j e i 2 c w i 1 w δ w y w y e 2 where the first term on the right side of eq 1 indicates the penalty function when the water surface level at the cross section of interest exceeds the specified maximum water level constraint the second term on the right side of eq 1 indicates the penalty function when the water surface level at the cross section of interest is far below the specified maximum water level constraint it is noted that there is no contradiction between the first and second term on the right side of eq 1 to drain a watershed as fast as possible without producing river inundation the water level in the cross section of interest should be near the specified maximum water level constraint not exceeding this level and not far below the third term on the right side of eq 1 indicates the penalty function when the water depth at a managed storage pond y w is below the ecological water depth y e specified for the storage pond the ecological water depth can be defined as the minimum water depth in the storage pond to minimize the implications on the ecological functions of the storage pond in eq 1 u v and w are the number of cross sections of interest at which the water level constraint is checked the number of decision variables during flood conditions and the number of managed storage ponds respectively also e i j is water surface elevation at cross section of interest i for decision variable j e g flow release at hour j e i is the specified maximum water level constraint at cross section of interest i and e i is the water level located at some distance below e i the region between e and e can be seen as a buffer zone where the objective function is optimal and constant in the present work e was set to be 0 9m 3 ft below e also in eq 1 c u c l and c w are penalty constants imposed for violation of constraints when water level at the cross section of interest exceeds e water level at the cross section of interest is below e and water depth at a managed storage pond is below the ecological water depth specified for the storage pond respectively finally in eq 1 δ u δ l and δ w are given by δ u v w 1 if respective constraint u v or w is violated 0 if respective constraint u v or w is satisfied 4 the integrated model the flow chart of the integrated model for forecasting optimal flow releases in a multi storage system for flood control is presented in fig 1 in short first the schedule of outflows of the managed storage ponds which are generated by the optimization model are used by hec hms to update the water levels in these ponds then the outflows from hec hms which could be unmanaged flows sub basins without managed storage or managed flows sub basins with managed storage enter the streams in hec ras model as lateral flows then the hec ras model simulates inundation at the watershed scale for each simulation of inundation the objective function is calculated using the water levels in the cross sections of interest in hec ras and the water depths in the managed storage ponds see eq 1 the objective values are sent back to the optimization model and the process is repeated until the optimization stop criteria is satisfied the version of the models used herein are hec hms 4 2 1 hec ras 5 0 5 matlab r2018b and hec dssvue 2 0 1 the scripts for running the aforementioned models and the links between these models are briefly described below 4 1 executing hec hms from matlab listing 1 presents the main script for executing hec hms from matlab e x e c h m s in listing 1 specifies the location of the file that will be called by matlab r u n n i n g h m s t x t for executing hec hms the content of the file r u n n i n g h m s t x t is presented in listing 2 r u n h m s in listing 1 changes the entire line that starts with o p e n p r o j e c t in listing 2 to the specified name given in t e m p in listing 1 the latter is necessary when running multiple instances of hec hms such as when using ga optimization finally d o s e x e c h m s executes the hms model if the run is successful the status in listing 1 will be 0 otherwise the status will be 1 listing 1 script to execute hec hms from matlab image 1 4 2 executing hec ras from matlab to automate the hec ras calculations and to perform parallel computations simultaneous computations of hec ras in matlab the scripts in leon and goodell 2016 are used and are not repeated here due to space limitations the only difference is that the current work uses the last version of hec ras 5 0 5 instead of 5 0 used in leon and goodell 2016 when using the version 5 0 5 of hec ras the windows registry key should be changed to actxserver ras505 hecrascontroller 4 3 executing hec dssvue from matlab listing 3 presents the main script for executing hec dssvue from matlab the content of c h a n g e d s s f i l e s in listing 3 is presented in listing 4 c h a n g e d s s f i l e s is used to change the location and name of the storage outflows and the schedule of outflows for each storage in w r i t e d s s p y listing 5 t e x t t e m p in listing 3 indicates the new location and name of the storage outflow while as f l o w s t e m p extracts the outflow data of the ga optimization x r a s s i m u l i d p o s i t t e m p 3 p o s i t t e m p 4 for each population r a s s i m u l i d and each storage p o s i t t e m p 3 p o s i t t e m p 4 as observed in listing 4 to replace the data of t e x t t e m p it is necessary to find the key variable starting with m y d s s h e c d s s o p e n while as for f l o w s t e m p it is necessary to find the key variable starting with f l o w s the script in listing 5 was adapted from the w r i t e d s s p y jython script in hydrologic engineering center 2009 finally s y s t e m e x e t e m p in listing 3 executes hec dssvue with the information provided in w r i t e d s s p y if the run is successful the status in listing 3 will be 0 otherwise the status will be 1 listing 2 content of file running hms txt image 2 listing 3 writedss py script to execute hec dssvue from matlab image 3 4 4 executing genetic algorithm optimization in matlab listing 6 presents the script for calling the matlab ga optimization toolbox because the computations are done in parallel u s e v e c t o r i z e d t r u e it is necessary to allocate the number of processors available for the optimization n u m p r o c e s s o r s f o r p a r a l l e l c o m p u t i n g in listing 6 g a o p t i o n s indicates the options used for the ga optimization including population size p o p u l a t i o n s i z e maximum number of generations m a x g e n e r a t i o n s stall generations m a x s t a l l g e n e r a t i o n s and the function tolerance f u n c t i o n t o l e r a n c e m a x g e n e r a t i o n s m a x s t a l l g e n e r a t i o n s and f u n c t i o n t o l e r a n c e are all related to the stop criteria the ga will stop if the maximum number of iterations or generations is attained also the ga will stop if the average relative change in the best fitness function value over stall generations m a x s t a l l g e n e r a t i o n s is less than or equal to function tolerance f u n c t i o n t o l e r a n c e in listing 6 x is the best solution decision variables f v a l is the value of the fitness function e x i t f l a g indicates the reason of lb and ub indicates the set of lower and upper bounds on the decision variables respectively listing 4 changedss files m replaces the data of texttemp and flowstemp image 4 listing 5 writedss py writes a dss file adapted from writedss py jython script in hydrologic engineering center 2009 image 5 listing 6 script to execute ga optimization in matlab image 6 4 5 miscellaneous scripts besides the aforementioned scripts few more important scripts that are necessary for assembling the integrated framework are briefly described in this section 4 5 1 checking water level in a storage at a given time for estimating the third term on the right hand side of eq 1 it is necessary to know the time series of the water level in each storage in particular to verify if the storage is about to dry it is necessary to know the storage water level right before the hydrograph starts to deliver water to the storage e g lowest storage water level a storage with zero or negative depth would violate the physical water depth and would lead to the abortion of the hec hms program to find the water level in a storage at a given time hec dssvue is used for writing the time series of storage water levels to an excel file from which are read by matlab lines 1 to 3 in listing 7 write the excel file through the jython script w e t l a n d s d s s s a v e e x c e l p y which content is shown in listing 8 w e t d a t a line 9 in listing 7 reads the excel file while as w e t e l e v b e f o r e h y d r o g access the desired data in the excel file listing 7 script for finding storage water level at a given time image 7 listing 8 wetlandsdsssaveexcel py saves dss data to an excel file adapted from folsomsaveexcel py jython script in hydrologic engineering center 2009 image 8 4 5 2 plotting of optimal flows and other flow variables for each storage pond listing 9 presents the script for plotting the optimal schedule of outflows at each storage pond along other flow variables the plot is done through hec dssvue using the jython script p l o t o p t i m f l o w s t o r a g e w e t l a n d s p y shown in listing 10 in listing 9 w r i t e p y t h o n f o r p l o t o p t w e t l v a r i a b l e s is used for changing the variables for each storage such as storage name w e t l a n d n a m e the period for the plot w r i t i n g t i m i n g f r o m h m s and the location and name for saving the j p g figure an example of the plot generated is shown in fig 8 due to space limitations set the inflow and outflow colors and set the label text in listing 10 are included only for one of the flow variables water surface elevation for the other variables the user needs to make a copy and change the name of the variable listing 9 script for plotting optimal schedule of outflows at each storage along other flow variables image 9 listing 10 plot optim flow storage wetlands py script for plotting the optimal schedule of outflows at each storage along other flow variables adapted from various jython scripts in hydrologic engineering center 2009 image 10 image 10 5 brief overview of a case study application of framework to an eight pond system in the cypress creek watershed houston tx it is noted that the purpose of this paper is not to describe a case study but instead to present a series of matlab scripts for forecasting hourly optimal flow releases in a multi storage system for flood mitigation the optimization period considered in this case study is 11 days which means that 264 optimal flows need to be determined for each managed storage pond solely with the objective of illustrating the application of the matlab framework this section presents a brief overview of a case study using the operation of a hypothetical eight pond system in the cypress creek watershed which is located in houston texas for an in dept discussion of this case study the reader is referred to tang et al 2019a the total area of the cypress creek watershed is 8 33 108 m2 the cypress creek watershed is located in northern houston within the harris county flood control district fig 2 depicts the geographical location of this watershed cypress creek watershed has a drainage area of about 267 sq miles and it experiences about two to three flooding events per year on average hgac 2016 the cypress creek watershed experienced devastating floods during hurricane harvey in august 2017 the major stream in the watershed cypress creek originates from northwest of the watershed takes a north south course first and then changes course to a west east direction there are several tributaries along the way of which the largest tributary is named little cypress creek the upper half of the cypress creek watershed is mostly agricultural area and the downstream of the watershed is mainly urban area the upper half of the watershed was historically covered by wetlands and rice farms and as a result there are a multitude of existing levees which can be easily repaired to restore the function of wetlands the hydrologic model of the cypress creek watershed was created in hec hms the details of the model construction calibration and validation are discussed in tang et al 2019b the hec hms model of the cypress creek watershed was divided into 23 sub basins as shown in fig 3 as shown in this figure the watershed was divided into three portions upstream midstream and downstream with total areas of 2 55 108 2 88 108 and 2 90 108 m2 respectively to help in flood mitigation a hypothetical eight storage pond system w300 w310 w330 w380 w390 w400 w410 and w420 with a total area ranging from 0 7 to 6 9 of the total watershed area is placed in the midstream portion of the watershed which are displayed as yellow clouds in fig 3 the reason to implement storage ponds in midstream is that most of the natural wetlands and abandoned rice farms are located within this region for user defined water releases in hec hms the outflow structures method is selected and then for the release option select y e s then specify the name of the gage release time series data finally the filename and pathname of the dss file containing the gage release data is specified the optimization model will update the gage release data for each generation of the optimization until the stop criteria is satisfied the schedules of storage outflows of the last generation of the optimization are the optimal results for simulating the overflows over the pond berms the user should define the number of spillways in the reservoir e g storage pond module of hec hms one can simply choose one spillway and define the elevation length and discharge coefficient for the broad crested spillway which would represent a storage pond berm the flows overtopping the spillway are labeled as spill flow in the matlab framework the hec ras model of the major streams of cypress creek watershed was built using the hec georas tool within arcgis the plan and profile views of the constructed hec ras model are presented in figs 4 and 5 respectively the hec ras model is used to simulate inundation in the watershed the inflow data for the hec ras model is provided by the hec hms model the total outflows from each managed storage pond optimization derived outflows and spill flows along the unmanaged flows enter the main streams in hec ras as lateral flows thus in this case study only eight lateral flows change in hec ras at every generation during the optimization to speed up the computations all hec ras simulations are performed in parallel a screenshot of typical hec ras parallel computations is shown in fig 6 herein we have used 18 available processors in the 8th generation intel core i7 8700 18 parallel computations however for better display fig 6 shows only three parallel simulations as shown in fig 1 once the hec hms and hec ras models are constructed and validated the ga optimization generates the schedule of outflows for the managed storage ponds eight in this case then these flows are used by hec hms to update the water levels in the eight storage ponds next the outflows from hec hms unmanaged and managed enter the streams in hec ras as lateral flows to simulate inundation at the watershed scale next the optimization model calculates the objective function according to eq 1 to determine the new population of schedules of flow releases at the eight ponds this procedure is repeated until the optimization stopping criteria is satisfied tests using a population of 72 and a tolerance of 0 01 cfs required about 40 50 generations to converge the screenshot of a typical convergence process of the optimization is shown in fig 7 after the optimization stop criteria is satisfied the plots for the optimal schedule of outflows at all storage ponds are automatically generated each plot includes the time trace of the water surface elevation storage total inflow spill flow and total outflow a typical graph produced for one managed storage pond is shown in fig 8 as shown in fig 8 the optimization tends to release part of the water from the storage pond ahead of the inflow hydrograph to provide extra water storage during a heavy storm event in the present exercise the minimum ecological water depth was set to 0 5 ft and the model tried to keep the water level in the storage pond above this value in an actual application the model can be run every few hours to update the optimal schedule of outflows according to the new precipitation forecasts and updated water levels in the streams and storage ponds if available as mentioned earlier in this case study 18 processors in the 8th generation intel core i7 8700 was used which requires between 1 5 and 2 h to complete the optimization fig 9 shows downstream inundation area with and without dynamic storage management for eight storage ponds with a total combined area ranging from 0 7 to 6 9 of the total watershed area the results in fig 9 indicate that when the combined storage pond area is below around 1 4 of the total watershed area there is no visible impact from dynamic storage management the storage ponds fill up quickly at the beginning of the rainfall event and there is no much room for dynamic storage management when the percentage of combined storage pond area is above 1 4 of the total watershed area dynamic storage management can significantly decrease the downstream inundation area for example a percentage of combined pond area of 2 1 with dynamic storage management achieves almost the same inundation as a percentage of combined pond area of 3 5 without dynamic storage management the latter indicates a reduction in 40 of the total combined area of storage ponds by using dynamic storage management the results also show that inundation can be eliminated when the percentage of combined pond area is 4 8 with dynamic storage management and 6 2 without dynamic storage management fig 10 shows the time trace of inundation depth at the kenchester park in the cypress creek watershed with and without dynamic storage management for a percentage of combined pond area of 3 5 the results in fig 10 show that the maximum inundation depth and inundation period decreased about 35 with storage management in practice land available for flood control is often limited and the results above show that dynamic storage management could play an important role in flood mitigation 6 conclusions this paper presents a matlab framework for forecasting optimal flow releases in a multi storage system for flood control this framework combines four models namely hec hms hec ras the matlab genetic algorithm ga toolbox and hec dssvue this paper focuses on presenting a set of matlab scripts for interfacing the aforementioned four software the scripts are illustrated using the operation of a hypothetical eight pond system in the cypress creek in houston texas the results of the case study indicate that dynamic storage management can help to mitigate floods for instance in the present case study the total combined area of shallow storage ponds maximum pond height 0 9 m required for producing a given flood mitigation can be reduced in up to 40 by dynamic storage management it is clear that these results are case dependent and cannot be generalized acknowledgments the first author was partially supported by the national science foundation through nsf eng cbet under award no 1805417 and through nsf dbi bio under award no 1820778 
26068,this paper presents a matlab framework for forecasting optimal flow releases in a multi storage system for flood control this framework combines four widely used models intended for 1 performing hydrologic analysis for a watershed and for level pool routing in the storage systems 2 simulating river inundation 3 solving the optimization problem of determining hourly optimal flow releases in a multi storage system and 4 data management and plotting the integration of all software is performed in matlab which is a state of the art and an easy to use environment for integrating computation visualization and programming this paper focuses on 1 presenting the matlab scripts for interfacing the aforementioned four software 2 describing the rationale for the objective function of the optimization and 3 demonstrating the practical use of the matlab framework by applying it to the operation of a hypothetical eight pond system in the cypress creek watershed in houston texas the results of the aforementioned application show that this framework could help in mitigating flooding keywords flood control genetic algorithms hec dssvue hec hms hec ras matlab watershed 1 introduction according to the national oceanic and atmospheric administration noaa 2016 inland flooding produces more damage annually than any other weather event in the united states for the period between 1976 and 2006 the damage produced by inland flooding averaged 6 9 billion per year noaa 2016 according to the earth observatory of nasa nasa 2017 global warming will change the major climate patterns and in general as precipitation patterns change storms floods and droughts will be more severe furthermore the changes in land use associated with increasing trends in urban development will increase the peak discharge and frequency of floods usgs 2003 recently flood mitigation within the context of a watershed where the entire watershed becomes the objective of management is receiving increasing attention kusler 2004 flotemersch et al 2016 within a watershed often a network of storage systems e g detention ponds wetlands reservoirs is available for flood control however they are rarely managed in a coordinated manner the lack of coordination of storage systems control may limit their effectiveness for flood mitigation furthermore the remote operation of water release in storage systems is not common and it is often limited to reservoirs in an ideal flood mitigation system all storage systems of the watershed would be operated in a coordinated manner according to precipitation forecasts to achieve certain objectives such as minimizing inundation levels at certain sections of the river furthermore the storage systems would be partially or totally emptied ahead of e g a few hours or a couple of days before a heavy rainfall that would produce flooding the storage made available by the early release would provide extra water storage during the heavy rainfall thus mitigating floods detention ponds and reservoirs are often constructed for mitigating floods as a main objective and these systems could be operated aggressively during flooding conditions for mitigating inundation the operation of wetlands for flood control however could have ecological implications for instance many wetlands retain some water year round supporting long lived aquatic animals such as fish as well as wetland amphibians and invertebrates leon et al 2018 however there are also many wetlands that naturally have a short hydroperiod so their function is not necessarily decreased by partial draining leon et al 2018 as pointed out by leon et al 2018 draining from wetlands involves some risks that can be minimized by not draining the wetlands fully and by draining only when the certainty of storm events is very high in any case if wetlands would be used for flood control they would be wetlands that are naturally flooded for only short periods of the year a short hydroperiod and support primarily facultative wetland animals and plant species that benefit from or can tolerate occasional flooded conditions but do not require them most of the year snodgrass et al 2000 ehrenfeld 2004 tarr et al 2005 leon et al 2018 the target audience of this work is anyone interested in near real time flood control this could include engineers responsible for flood management and short term flood control this work is also intended for researchers and students interested in open source techniques for flood mitigation the focus of this manuscript is on flood control and thus the storage used for flood mitigation would exclude wetlands that support long lived aquatic animals to facilitate the coordinated operation of a multi storage system for flood mitigation the integration of hydrology and hydraulic models within an optimization framework would be required users often have unique applications that may require modifying the framework for their particular needs to achieve the latter it is desirable to integrate the aforementioned models within a state of the art and an easy to use environment that is suitable for computation visualization and programming one of such platforms is matlab which is a high performance language for technical computing that integrates computation visualization and programming in an easy to use environment mathworks 2015 this paper presents a matlab framework for forecasting optimal flow releases in a multi storage system for flood control this paper is organized as follows 1 the sub component models and the rationale for the objective function are briefly described 2 matlab scripts for interfacing the aforementioned sub component models are presented 3 a brief overview of a case study is presented to illustrate the application of the matlab framework finally the key results are summarized in the conclusion 2 considerations for the optimal and coordinated operation of storage systems for flood mitigation it is clear that each watershed prone to flooding may have specific challenges and different topographical conditions however in general most of these watersheds have inline storage systems e g reservoirs and off line storage systems e g detention ponds wetlands to make possible the optimal and coordinated operation of storage systems for flood control it would be necessary to add or retrofit gates siphons for water release of the storage systems so they can discharge most of the water stored in a pre determined time this task would include implementing the necessary hardware and software to make possible the remote operation of gates siphons in wetlands detention ponds and reservoirs remote operation of gates siphons are necessary because very often wetlands detention ponds and reservoirs are located in remote areas far away from urban areas the remote operation could be achieved using the classical supervisory control and data acquisition scada system or using latest and cheaper technologies such as those presented in leon and verma 2019 it is noted that adding conventional drainage pipes e g sloped pipes would require substantial earthwork as they would need to be installed in the earthen berm conversely installing siphons require minimal earthwork as only anchoring of the siphon pipes over the berms would be necessary it is noted that a siphon flow uphill flow followed by a downhill flow or a pure downhill flow are gravity driven flows which require a hydraulic gradient between the water surface level in the storage system and the discharge point e g natural or artificial drainage ditch in many instances even near flat land can be engineered to create a storage system and achieve a hydraulic gradient of at least 0 6 or 0 9 m which would still give significant flow velocities exceeding 1 m s a storage system could be engineered to be a single pond or an interconnected system of multiple ponds leon et al 2018 if the area for the pond construction is consistently flat the earthwork of a single pond would likely be the most economical option however in most cases the terrain is not horizontally leveled and the earthwork would be significantly reduced if a system of interconnected ponds with different bottom levels is used leon et al 2018 3 model description as mentioned earlier the coordinated operation of storage systems for flood control requires a computational framework that integrates hydrologic and hydraulic models within an optimization framework there is a vast array of hydrologic hydraulic and optimization models available in the literature because one of the most popular and freely available models for simulating river inundation is the u s army corps of engineers hydrologic engineering center s river analysis system hec ras hydrologic engineering center 2016a hydrologic engineering center 2016b this model was selected as the hydraulic model of the proposed framework the hydrologic model was selected based on three criteria 1 compatibility with the aforementioned hydraulic model 2 freely available and 3 widely used in practice based on these considerations the u s army corps of engineers hydrologic modeling system hec hms hydrologic engineering center 2017 was used as the hydrologic model for determining the optimal schedules of flow releases in a multi storage system it is needed an optimization solver intended for large scale constrained optimization problems a widely used solver in water resources engineering e g yang et al 2015 chen et al 2016 that meets the above criteria is the genetic algorithm ga because the proposed framework was implemented in matlab and due to the availability of the genetic algorithm toolbox within matlab this toolbox is used herein to minimize scripting for data exchange between hec hms and hec ras e g manipulation of hec dss files the u s army corps of engineers hec dssvue model which has several built in utilities for manipulating data in a hec dss database was used the hec hms hec ras matlab ga toolbox and hec dssvue model are briefly described below 3 1 hydrologic modeling system hec hms hec hms is intended to simulate the complete hydrologic processes of dendritic watershed systems and includes hydrologic analysis procedures such as event infiltration unit hydrographs and hydrologic routing hydrologic engineering center 2017 hec hms is often used for studies of flow forecasting future urbanization impact reservoir spillway design flood damage reduction floodplain regulation and systems operation hydrologic engineering center 2017 3 2 hydrologic engineering center s river analysis system hec ras a highly adopted software in inundation modeling is the u s army corps of engineers hydrologic engineering center s river analysis system hec ras hec ras can perform one and two dimensional hydraulic calculations for a full network of natural and constructed channels overbank floodplain areas levee protected areas etc hydrologic engineering center 2016a 2016b hec ras has four main modules 1 steady flow water surface profiles which is intended for calculating water surface profiles in steady gradually varied flow conditions 2 unsteady flow simulation which can simulate one dimensional two dimensional and combined one two dimensional unsteady flow through an open channel network 3 sediment transport computations which is intended for the simulation of one dimensional sediment transport movable boundary calculations resulting from scour and deposition over moderate to long time periods and 4 water quality analysis which is intended for performing riverine water quality analyses hydrologic engineering center 2016a 2016b standard applications of this model include flood wave routing and flood inundation studies in the present work inundation is simulated using the unsteady one dimensional 1d hec ras model which solves the full 1d saint venant equations using the four point implicit finite difference scheme hydrologic engineering center 2016a 3 3 the matlab genetic algorithm ga toolbox the optimization model uses the matlab genetic algorithm ga toolbox chipperfield and fleming 1995 the ga solves constrained and unconstrained optimization problems based on a natural selection process that mimics biological evolution chipperfield and fleming 1995 the ga repeatedly changes a population of individual solutions at each generation the ga randomly selects individuals from the current population and uses them as parents to produce children for the next generation after several generations the population is expected to evolve toward an optimal solution the ga is recommended to solve problems that are not well suited for standard optimization algorithms including problems in which the objective function is discontinuous nondifferentiable stochastic or highly non linear chipperfield and fleming 1995 for more details about the genetic algorithm and its application to water resources the reader is referred to wardlaw and sharif 1999 leon and kanashiro 2010 leon et al 2014 lerma et al 2015 yang et al 2015 and chen et al 2016 3 4 hec dssvue hec dssvue is a java based visual utilities software intended to plot tabulate and manipulate data in a hec dss database file hydrologic engineering center 2009 hec dssvue incorporates over sixty mathematical functions and provides several utility functions to enter data sets into a hec dss database rename data set names copy data sets to other hec dss database files and delete data sets hydrologic engineering center 2009 more importantly for the present work hec dssvue incorporates the jython standard scripting language which allows to execute a sequence of steps in a text format from a batch process hydrologic engineering center 2009 3 5 objective function a single objective function f is considered in the optimization which consists that during flood conditions the flow must be conveyed slightly below the specified maximum water level at the cross sections of interest e g maximum water levels without producing flooding in urban areas this objective is written as 1 f c u i 1 u j 1 v δ u e i j e i 2 c l i 1 u j 1 v δ l e i j e i 2 c w i 1 w δ w y w y e 2 where the first term on the right side of eq 1 indicates the penalty function when the water surface level at the cross section of interest exceeds the specified maximum water level constraint the second term on the right side of eq 1 indicates the penalty function when the water surface level at the cross section of interest is far below the specified maximum water level constraint it is noted that there is no contradiction between the first and second term on the right side of eq 1 to drain a watershed as fast as possible without producing river inundation the water level in the cross section of interest should be near the specified maximum water level constraint not exceeding this level and not far below the third term on the right side of eq 1 indicates the penalty function when the water depth at a managed storage pond y w is below the ecological water depth y e specified for the storage pond the ecological water depth can be defined as the minimum water depth in the storage pond to minimize the implications on the ecological functions of the storage pond in eq 1 u v and w are the number of cross sections of interest at which the water level constraint is checked the number of decision variables during flood conditions and the number of managed storage ponds respectively also e i j is water surface elevation at cross section of interest i for decision variable j e g flow release at hour j e i is the specified maximum water level constraint at cross section of interest i and e i is the water level located at some distance below e i the region between e and e can be seen as a buffer zone where the objective function is optimal and constant in the present work e was set to be 0 9m 3 ft below e also in eq 1 c u c l and c w are penalty constants imposed for violation of constraints when water level at the cross section of interest exceeds e water level at the cross section of interest is below e and water depth at a managed storage pond is below the ecological water depth specified for the storage pond respectively finally in eq 1 δ u δ l and δ w are given by δ u v w 1 if respective constraint u v or w is violated 0 if respective constraint u v or w is satisfied 4 the integrated model the flow chart of the integrated model for forecasting optimal flow releases in a multi storage system for flood control is presented in fig 1 in short first the schedule of outflows of the managed storage ponds which are generated by the optimization model are used by hec hms to update the water levels in these ponds then the outflows from hec hms which could be unmanaged flows sub basins without managed storage or managed flows sub basins with managed storage enter the streams in hec ras model as lateral flows then the hec ras model simulates inundation at the watershed scale for each simulation of inundation the objective function is calculated using the water levels in the cross sections of interest in hec ras and the water depths in the managed storage ponds see eq 1 the objective values are sent back to the optimization model and the process is repeated until the optimization stop criteria is satisfied the version of the models used herein are hec hms 4 2 1 hec ras 5 0 5 matlab r2018b and hec dssvue 2 0 1 the scripts for running the aforementioned models and the links between these models are briefly described below 4 1 executing hec hms from matlab listing 1 presents the main script for executing hec hms from matlab e x e c h m s in listing 1 specifies the location of the file that will be called by matlab r u n n i n g h m s t x t for executing hec hms the content of the file r u n n i n g h m s t x t is presented in listing 2 r u n h m s in listing 1 changes the entire line that starts with o p e n p r o j e c t in listing 2 to the specified name given in t e m p in listing 1 the latter is necessary when running multiple instances of hec hms such as when using ga optimization finally d o s e x e c h m s executes the hms model if the run is successful the status in listing 1 will be 0 otherwise the status will be 1 listing 1 script to execute hec hms from matlab image 1 4 2 executing hec ras from matlab to automate the hec ras calculations and to perform parallel computations simultaneous computations of hec ras in matlab the scripts in leon and goodell 2016 are used and are not repeated here due to space limitations the only difference is that the current work uses the last version of hec ras 5 0 5 instead of 5 0 used in leon and goodell 2016 when using the version 5 0 5 of hec ras the windows registry key should be changed to actxserver ras505 hecrascontroller 4 3 executing hec dssvue from matlab listing 3 presents the main script for executing hec dssvue from matlab the content of c h a n g e d s s f i l e s in listing 3 is presented in listing 4 c h a n g e d s s f i l e s is used to change the location and name of the storage outflows and the schedule of outflows for each storage in w r i t e d s s p y listing 5 t e x t t e m p in listing 3 indicates the new location and name of the storage outflow while as f l o w s t e m p extracts the outflow data of the ga optimization x r a s s i m u l i d p o s i t t e m p 3 p o s i t t e m p 4 for each population r a s s i m u l i d and each storage p o s i t t e m p 3 p o s i t t e m p 4 as observed in listing 4 to replace the data of t e x t t e m p it is necessary to find the key variable starting with m y d s s h e c d s s o p e n while as for f l o w s t e m p it is necessary to find the key variable starting with f l o w s the script in listing 5 was adapted from the w r i t e d s s p y jython script in hydrologic engineering center 2009 finally s y s t e m e x e t e m p in listing 3 executes hec dssvue with the information provided in w r i t e d s s p y if the run is successful the status in listing 3 will be 0 otherwise the status will be 1 listing 2 content of file running hms txt image 2 listing 3 writedss py script to execute hec dssvue from matlab image 3 4 4 executing genetic algorithm optimization in matlab listing 6 presents the script for calling the matlab ga optimization toolbox because the computations are done in parallel u s e v e c t o r i z e d t r u e it is necessary to allocate the number of processors available for the optimization n u m p r o c e s s o r s f o r p a r a l l e l c o m p u t i n g in listing 6 g a o p t i o n s indicates the options used for the ga optimization including population size p o p u l a t i o n s i z e maximum number of generations m a x g e n e r a t i o n s stall generations m a x s t a l l g e n e r a t i o n s and the function tolerance f u n c t i o n t o l e r a n c e m a x g e n e r a t i o n s m a x s t a l l g e n e r a t i o n s and f u n c t i o n t o l e r a n c e are all related to the stop criteria the ga will stop if the maximum number of iterations or generations is attained also the ga will stop if the average relative change in the best fitness function value over stall generations m a x s t a l l g e n e r a t i o n s is less than or equal to function tolerance f u n c t i o n t o l e r a n c e in listing 6 x is the best solution decision variables f v a l is the value of the fitness function e x i t f l a g indicates the reason of lb and ub indicates the set of lower and upper bounds on the decision variables respectively listing 4 changedss files m replaces the data of texttemp and flowstemp image 4 listing 5 writedss py writes a dss file adapted from writedss py jython script in hydrologic engineering center 2009 image 5 listing 6 script to execute ga optimization in matlab image 6 4 5 miscellaneous scripts besides the aforementioned scripts few more important scripts that are necessary for assembling the integrated framework are briefly described in this section 4 5 1 checking water level in a storage at a given time for estimating the third term on the right hand side of eq 1 it is necessary to know the time series of the water level in each storage in particular to verify if the storage is about to dry it is necessary to know the storage water level right before the hydrograph starts to deliver water to the storage e g lowest storage water level a storage with zero or negative depth would violate the physical water depth and would lead to the abortion of the hec hms program to find the water level in a storage at a given time hec dssvue is used for writing the time series of storage water levels to an excel file from which are read by matlab lines 1 to 3 in listing 7 write the excel file through the jython script w e t l a n d s d s s s a v e e x c e l p y which content is shown in listing 8 w e t d a t a line 9 in listing 7 reads the excel file while as w e t e l e v b e f o r e h y d r o g access the desired data in the excel file listing 7 script for finding storage water level at a given time image 7 listing 8 wetlandsdsssaveexcel py saves dss data to an excel file adapted from folsomsaveexcel py jython script in hydrologic engineering center 2009 image 8 4 5 2 plotting of optimal flows and other flow variables for each storage pond listing 9 presents the script for plotting the optimal schedule of outflows at each storage pond along other flow variables the plot is done through hec dssvue using the jython script p l o t o p t i m f l o w s t o r a g e w e t l a n d s p y shown in listing 10 in listing 9 w r i t e p y t h o n f o r p l o t o p t w e t l v a r i a b l e s is used for changing the variables for each storage such as storage name w e t l a n d n a m e the period for the plot w r i t i n g t i m i n g f r o m h m s and the location and name for saving the j p g figure an example of the plot generated is shown in fig 8 due to space limitations set the inflow and outflow colors and set the label text in listing 10 are included only for one of the flow variables water surface elevation for the other variables the user needs to make a copy and change the name of the variable listing 9 script for plotting optimal schedule of outflows at each storage along other flow variables image 9 listing 10 plot optim flow storage wetlands py script for plotting the optimal schedule of outflows at each storage along other flow variables adapted from various jython scripts in hydrologic engineering center 2009 image 10 image 10 5 brief overview of a case study application of framework to an eight pond system in the cypress creek watershed houston tx it is noted that the purpose of this paper is not to describe a case study but instead to present a series of matlab scripts for forecasting hourly optimal flow releases in a multi storage system for flood mitigation the optimization period considered in this case study is 11 days which means that 264 optimal flows need to be determined for each managed storage pond solely with the objective of illustrating the application of the matlab framework this section presents a brief overview of a case study using the operation of a hypothetical eight pond system in the cypress creek watershed which is located in houston texas for an in dept discussion of this case study the reader is referred to tang et al 2019a the total area of the cypress creek watershed is 8 33 108 m2 the cypress creek watershed is located in northern houston within the harris county flood control district fig 2 depicts the geographical location of this watershed cypress creek watershed has a drainage area of about 267 sq miles and it experiences about two to three flooding events per year on average hgac 2016 the cypress creek watershed experienced devastating floods during hurricane harvey in august 2017 the major stream in the watershed cypress creek originates from northwest of the watershed takes a north south course first and then changes course to a west east direction there are several tributaries along the way of which the largest tributary is named little cypress creek the upper half of the cypress creek watershed is mostly agricultural area and the downstream of the watershed is mainly urban area the upper half of the watershed was historically covered by wetlands and rice farms and as a result there are a multitude of existing levees which can be easily repaired to restore the function of wetlands the hydrologic model of the cypress creek watershed was created in hec hms the details of the model construction calibration and validation are discussed in tang et al 2019b the hec hms model of the cypress creek watershed was divided into 23 sub basins as shown in fig 3 as shown in this figure the watershed was divided into three portions upstream midstream and downstream with total areas of 2 55 108 2 88 108 and 2 90 108 m2 respectively to help in flood mitigation a hypothetical eight storage pond system w300 w310 w330 w380 w390 w400 w410 and w420 with a total area ranging from 0 7 to 6 9 of the total watershed area is placed in the midstream portion of the watershed which are displayed as yellow clouds in fig 3 the reason to implement storage ponds in midstream is that most of the natural wetlands and abandoned rice farms are located within this region for user defined water releases in hec hms the outflow structures method is selected and then for the release option select y e s then specify the name of the gage release time series data finally the filename and pathname of the dss file containing the gage release data is specified the optimization model will update the gage release data for each generation of the optimization until the stop criteria is satisfied the schedules of storage outflows of the last generation of the optimization are the optimal results for simulating the overflows over the pond berms the user should define the number of spillways in the reservoir e g storage pond module of hec hms one can simply choose one spillway and define the elevation length and discharge coefficient for the broad crested spillway which would represent a storage pond berm the flows overtopping the spillway are labeled as spill flow in the matlab framework the hec ras model of the major streams of cypress creek watershed was built using the hec georas tool within arcgis the plan and profile views of the constructed hec ras model are presented in figs 4 and 5 respectively the hec ras model is used to simulate inundation in the watershed the inflow data for the hec ras model is provided by the hec hms model the total outflows from each managed storage pond optimization derived outflows and spill flows along the unmanaged flows enter the main streams in hec ras as lateral flows thus in this case study only eight lateral flows change in hec ras at every generation during the optimization to speed up the computations all hec ras simulations are performed in parallel a screenshot of typical hec ras parallel computations is shown in fig 6 herein we have used 18 available processors in the 8th generation intel core i7 8700 18 parallel computations however for better display fig 6 shows only three parallel simulations as shown in fig 1 once the hec hms and hec ras models are constructed and validated the ga optimization generates the schedule of outflows for the managed storage ponds eight in this case then these flows are used by hec hms to update the water levels in the eight storage ponds next the outflows from hec hms unmanaged and managed enter the streams in hec ras as lateral flows to simulate inundation at the watershed scale next the optimization model calculates the objective function according to eq 1 to determine the new population of schedules of flow releases at the eight ponds this procedure is repeated until the optimization stopping criteria is satisfied tests using a population of 72 and a tolerance of 0 01 cfs required about 40 50 generations to converge the screenshot of a typical convergence process of the optimization is shown in fig 7 after the optimization stop criteria is satisfied the plots for the optimal schedule of outflows at all storage ponds are automatically generated each plot includes the time trace of the water surface elevation storage total inflow spill flow and total outflow a typical graph produced for one managed storage pond is shown in fig 8 as shown in fig 8 the optimization tends to release part of the water from the storage pond ahead of the inflow hydrograph to provide extra water storage during a heavy storm event in the present exercise the minimum ecological water depth was set to 0 5 ft and the model tried to keep the water level in the storage pond above this value in an actual application the model can be run every few hours to update the optimal schedule of outflows according to the new precipitation forecasts and updated water levels in the streams and storage ponds if available as mentioned earlier in this case study 18 processors in the 8th generation intel core i7 8700 was used which requires between 1 5 and 2 h to complete the optimization fig 9 shows downstream inundation area with and without dynamic storage management for eight storage ponds with a total combined area ranging from 0 7 to 6 9 of the total watershed area the results in fig 9 indicate that when the combined storage pond area is below around 1 4 of the total watershed area there is no visible impact from dynamic storage management the storage ponds fill up quickly at the beginning of the rainfall event and there is no much room for dynamic storage management when the percentage of combined storage pond area is above 1 4 of the total watershed area dynamic storage management can significantly decrease the downstream inundation area for example a percentage of combined pond area of 2 1 with dynamic storage management achieves almost the same inundation as a percentage of combined pond area of 3 5 without dynamic storage management the latter indicates a reduction in 40 of the total combined area of storage ponds by using dynamic storage management the results also show that inundation can be eliminated when the percentage of combined pond area is 4 8 with dynamic storage management and 6 2 without dynamic storage management fig 10 shows the time trace of inundation depth at the kenchester park in the cypress creek watershed with and without dynamic storage management for a percentage of combined pond area of 3 5 the results in fig 10 show that the maximum inundation depth and inundation period decreased about 35 with storage management in practice land available for flood control is often limited and the results above show that dynamic storage management could play an important role in flood mitigation 6 conclusions this paper presents a matlab framework for forecasting optimal flow releases in a multi storage system for flood control this framework combines four models namely hec hms hec ras the matlab genetic algorithm ga toolbox and hec dssvue this paper focuses on presenting a set of matlab scripts for interfacing the aforementioned four software the scripts are illustrated using the operation of a hypothetical eight pond system in the cypress creek in houston texas the results of the case study indicate that dynamic storage management can help to mitigate floods for instance in the present case study the total combined area of shallow storage ponds maximum pond height 0 9 m required for producing a given flood mitigation can be reduced in up to 40 by dynamic storage management it is clear that these results are case dependent and cannot be generalized acknowledgments the first author was partially supported by the national science foundation through nsf eng cbet under award no 1805417 and through nsf dbi bio under award no 1820778 
26069,coupled fire atmospheric modeling tools are increasingly used to understand the complex and dynamic behavior of wildland fires multiple research tools linking combustion to fluid flow use navier stokes numerical solutions coupled to a thermodynamic model to understand fire atmospheric feedbacks but these computational fluid dynamics approaches require high performance computing resources we present a new simulation tool called quic fire to rapidly solve these feedbacks by coupling the mature 3 d rapid wind solver quic urb to a physics based cellular automata fire spread model fire ca quic fire uses 3 d fuels inputs similar to the cfd model firetec allowing this tool to simulate effects of fuel structure on local winds and fire behavior results comparing fire behavior metrics to the computational fluid dynamic model firetec show strong agreement quic fire is the first tool intended to provide an opportunity for prescribed fire planners to compare evaluate and design burn plans including complex ignition patterns and coupled fire atmospheric feedbacks keywords fire behavior model fire atmospheric feedbacks cellular automata prescribed fire model 1 introduction wildland fire behavior and subsequent fire effects are largely driven by complex heterogenous and dynamic fire atmospheric feedbacks hilton et al 2015 linn et al 2013 understanding the complex interactions of ignition pattern heterogeneous vegetation and dynamic fire environmental conditions are critical to accurately predicting fire behavior hoffman et al 2018 and subsequent fire effects o brien et al 2018 many fire behavior models have been developed to predict fire spread energy release and fire effects using a variety of approaches sullivan 2009a 2009b but ultimately all wildland fire models must balance representing the complexity of fire fuel atmospheric feedbacks and the speed of predictions hilton et al 2018 in addition firefighter safety and rapid assessments of fire spread have dominated the objectives for much of the modeling to date leading to adoption of tools producing rapid outputs as a preference over models that capture complex wildland fire dynamics this bias for speed over accuracy is particularly problematic for prescribed fire practitioners who must account for complex ignition patterns and dynamic environmental conditions i e wind and moisture to plan and execute burning treatments intentional ignitions are used to manage 5 million hectares annually in the us melvin 2015 with many millions more burned globally bond and keeley 2005 guyette et al 2017 ichoku et al 2008 planning and development of legal prescriptions for prescribed fire practices to meet objectives depends on anticipating the fire s response to variation in fuels and weather conditions chiodi et al 2018 o brien et al 2018 wade et al 1989 one complication to the prediction of prescribed fire behavior compared to wildfire scenarios is the significant influence of the rates and patterns of ignition on fire behavior also the interaction between the fire environment and practitioner designed ignition practices depends on landscape scale mean fuel and weather conditions as well as their localized spatial and temporal variability canfield et al 2014 furman 2018 in managed fire regimes surface fire behavior is further complicated by three dimensional turbulent flows within the canopy parsons et al 2011 pimont et al 2011 and by surrounding edge effects such as those produced by roads or adjacent previously burned blocks linn et al 2012 in this case fuel structure influences wind flow through the canopy in the vicinity of the surface fires affecting air entrainment among firelines and draw between fires to cope with such complexity and ultimately the spread patterns and intensity potential of fires managers are forced to rely exclusively on experience or rules of thumb cruz and alexander 2019 as the simplified models of forward fire spread available to plan fire activities do not account for these kinds of critical processes mell et al 2018 there are a variety of models that have been developed to generate rapid wildfire behavior predictions such as the fire spread model of rothermel 1972 and its derivatives including behave andrews 1986 farsite finney 1998 and prometheus tymstra et al 2010 these models are either empirically derived from algebraic regressions of fire behavior observations or semi empirical solutions employing simplified physical bases for their algebraic functional forms with the addition of calibration constants calculated from fire behavior observations sullivan 2009a 2009b the algebraic functional forms of these models describe the spread of wildfires as functions of mean local ambient conditions such as wind speed fuel conditions and topographic slope they do not however resolve the individual processes that produced the fire behavior such approaches extrapolate the single fireline behavior to complex patterns of firelines ignoring the effect of the fire and atmosphere interactions thus misrepresenting the spreading behavior and the fire intensity in fact multiple ignitions commonly used in prescribed fires or wildfire suppression tactics violate several key assumptions of most simplified wildfire spread models such as steady state fire spread non interacting firelines yedinak et al 2018 and homogeneity of fuels hiers et al 2009 improving the effectiveness of prescribed fire management requires new modeling tools designed to capture the complex dynamics of multiple firelines heterogeneous fuels and variable environmental conditions hoffman et al 2018 o brien et al 2018 to represent the influences of the dynamic interaction between fire fuel and atmosphere fire behavior models have been linked to computational fluid dynamics cfd models that characterize the movement of the atmosphere linn et al 2002 mell et al 2007 current models that explicitly represent some aspect of fire behavior and associated atmospheric response to fires fall into two categories 1 empirical fire spread models coupled to an atmospheric fluid dynamics model e g wrf fire coen et al 2013 wrf sfire mandel et al 2011 cawfe coen 2013 and 2 process based fire models coupled to an atmospheric fluid dynamics model e g wfds mell et al 2007 firetec linn et al 2002 firestar morvan et al 2007 the challenge for the use of the former class of fire atmosphere models for prescribed fire is that they are designed to run at horizontal resolutions 20 m where the representations of the fire and atmospheric perturbations are smoothed to larger scales in order to be consistent with the underpinning empirical basis of the fire models these scales are then too coarse to simulate prescribed fire dynamics or the fuel heterogeneity that drives prescribed fire behavior which are often smaller than 10 m the challenge for the process based coupled fire atmosphere models is the computational demands of these models which typically exceed the time or computer resources for practical use in planning prescribed fires achtemeier 2013 used a cellular automata ca based approach to model the movement of fire based on a set of rules attached to observed or theoretical phenomenology because ca models attempt to represent some of the local phenomenology of fires which is dominated by local winds it is possible to couple them to flow models to capture some of the two way feedbacks between fires and the atmosphere achtemeier et al 2012 coupled their ca model to a simplified wind model and produced promising results using only a limited set of rules based on the expert opinion of the developer such 2d fire spread can then be coupled to potential flow to rapidly predict fire spread hilton et al 2018 while the model of achtemeier 2013 used a simple depth averaged wind flow to model fire spread through a simplified localized spot driven model its ruleset was not directly tied to physical processes which has led to limited adoption outside of its use within the daysmoke model achtemeier et al 2012 additionally fuel representation relied on 2d models of forest type with a rule based influence of fuelbed height while 2d representations are common and can simplify predictions e g wrf s fire hilton et al 2018 such simplifications limit potentially important fire atmospheric interactions driven by 3d fuel variation i e locally increased surface flows in stem spaces downwind of a canopy gap in surface fire regimes parsons et al 2011 pimont et al 2011 for realistic representation of fire behavior that drives fire effects in a prescribed fire setting 3d time resolved fire behavior must be coupled to more accurate representations of fuels o brien et al 2018 this is also critically important to understanding prescribed fire behavior which results from complex ignition patterns furman 2018 as the response of fires to these ignition patterns depends on the local vertical and horizontal vegetation structure the ability of a simplified model to rapidly and accurately represent fire atmospheric feedbacks enabling representation of the interaction between multiple ignitions and 3d fuel structure would represent a crucial advancement here we describe a recently developed wildland fire modeling tool quic fire that uses a ca coupled fire atmospheric modeling approach that builds on the approach by achtemeier 2013 but applies physical process based rules to achieve a more generalized fire spread model onto 3d fuel data in the following sections model formulation results of preliminary simulation results lessons learned concerning the impacts of fire atmosphere coupling and a discussion of a path forward for this model are described 2 methods 2 1 quic fire model description quic fire is a wildland fire simulation tool designed to capture the coupled fire atmosphere interactions that are essential for simulating prescribed fire without extreme computational demands its design allows for wider spread use and ensemble calculations to cover for instance a range of possible weather or fuel conditions since we developed quic fire as a prescribed fire planning tool it was purposefully designed to be able to represent the fire atmosphere feedbacks that determine the behavior of prescribed fires as well as the influences of heterogeneous vegetation this was accomplished by exploiting the capabilities of the quic urb wind field solver pardyjak and brown 2003 singh et al 2008 coupled with a new ca based fire spread model referred to here as fire ca quic urb is a fast running wind field solver that was originally designed for computing flow fields around buildings in urban settings pardyjak and brown 2003 it combines mass conservation constraints with observation based wind field phenomenology algorithms to quickly compute 3d flow fields that include the influence of both structure driven drag and now the dynamic buoyancy sources of wildland fires the coupling between the extended quic urb wind field solver and fire ca was executed by passing a 3d vegetation and fire influenced wind field associated turbulence intensity from quic urb to drive fire ca fire ca then feeds back the spatially resolved and evolving three dimensional vegetation and heat release distributions to quic urb 2 2 extension of quic urb for wildland fire application the presence of the fire alters the wind fields in two ways through the buoyancy driven flows and through its effects on the vegetation and resulting impacts on aerodynamic drag in order to extend quic urb for wildland fire scenarios a representation of the strong buoyancy induced flows was implemented part of the heat produced by the flames warms up the surrounding atmosphere generating a buoyantly driven vertical movement of heated air in turn the displacement of the heated air draws in adjacent air to fill the vacated volume producing a convergence zone ultimately the resulting flow pattern influences the fire spread achtemeier et al 2012 as the hot air travels upwards in an isolated plume it mixes with fresh air and the volume that it influences increases with height while its upward velocity decreases with sustained heat release the rising parcels of air form a plume that increases in diameter with height and can lean downwind depending on local wind speed to capture these phenomena the plume trajectory and lateral expansion are parameterized with the briggs theory davidson 1989 with the briggs theory the plume radius and centerline updraft are recalculated at each point on the trajectory for this approach we initially assumed that there is non zero local horizontal wind speed and thus as hot air rises it is also translating some horizontal distance x from the spot of plume origination the centerline updraft w c is a function of the heat released by the fire and the horizontal downwind distance from the point of origin davidson 1989 w c 1 β 2 3 f b w s z z v e r t here β is an empirical constant z v e r t is an offset for a virtual plume origin and f b is a buoyancy term in m4 s 3 z v e r t is related to cell size since it compensates for the fact that the plume is starting with an area that is the same as the horizontal area for the cell z v e r t a c e l l π β f b is defined by f b g t a e ρ a c p a where g is the acceleration due to gravity t a ρ a and c p a are the ambient air temperature density and specific heat of the undisturbed air and e is the heat released by the fire in any given computational cell into the atmosphere in watts computationally plumes are initiated from each quic urb cell where heat is generated plume radii at the plumes origin are computed from the horizontal area a c e l l of the computational cell the plumes develop over time under the influence of their own buoyancy and surrounding winds which are influenced by the presence of other plumes as well plumes also influence each other for instance for two perfectly vertical nearby plumes their trajectories will bend toward one another in particular the plume with the smaller buoyancy will bend more toward the one with higher buoyancy this phenomenon requires o n 2 calculations where n is the number of plumes hence tracking three dimensional plumes emanating from every heated cell is very computationally intensive however the plumes of hot gases rising from nearby locations eventually merge thus in order to streamline computations two plumes are merged when their trajectories are closer than the sum of their radii that is when the plumes start to overlap the updraft of the merged plume is calculated based on lai and lee 2012 and the merged plume radius recalculated to guarantee mass conservation a gaussian similarity profile is used to describe the radial updraft decay this updraft is superimposed onto the quic urb solution while the resulting horizontal flow is established by imposing mass consistency as fire consumes fuels drag is reduced and the horizontal wind speed increases quic urb incorporates a cionco type correlation to parameterize the vegetation drag cionco 1965 nelson et al 2009 here we applied a novel solution to the drag dynamics by linearly interpolating wind speed between the calculated cionco profile representing the unburnt canopy and the unimpeded flow representing a fully burnt canopy in particular the wind speed is a function of the reduction in vegetation due to fire consumption for example in the absence of fire induced flows the wind profile in the absence of vegetation is a log law profile as is typical of the atmosphere sutton 1953 but in the presence of the vegetation canopy the wind profile within the canopy is slower than would be predicted by the log law fig 1 as the canopy burns the wind speed value moves along the black line in fig 1 the drag impact on local wind in each cell that initially has fuel in it is adjusted as the fire progresses depending on the local fuel consumption at a specific time in the context of a spreading fire that is consuming vegetation this modification in quic urb successfully captured the higher wind speeds in areas where fuels were consumed the procedure also enables the wind field to reflect some of the influences of the spatial distribution of fuel treatments and fuel heterogeneity 2 3 fire ca fire ca is built on a conceptual model that leverages the work of achtemeier et al 2012 and achtemeier 2013 where the energy transfer from one location to another is accomplished through a cellular automata ca approach in fire ca energy packets eps are moved from one location to another based on local wind environments eps either evaporate moisture start new fires intensify existing fires or transfer their energy to the atmosphere based on the fuel conditions in their destination cells each ep represents a fixed amount of energy per unit time e e p and the number of eps produced by each cell n e p in a given time step is determined based on an estimate of the reaction rate within that cell 2 4 reaction rate mass loss rate and heat release rate in each burning cell the local reaction rate average reaction rate within the cell is described in terms of the change in bulk density of fine fuel particles ρ f with time and is captured by the following equation ρ f t c m ρ f ρ o ψ σ λ where c m is a dimensionless reaction rate constant ρ f and ρ o are the local bulk density kg m 3 of fine solid fuel fine scale biomass particles such as foliage or small twigs and oxygen respectively ψ is the fraction of the fuel in the computational cell that is actively burning σ is the turbulent mixing coefficient and λ is the dimensionless stoichiometry factor the oxygen density is modeled based on the local reaction rate through the following expression ρ o ρ g a s 0 21 γ o f l o o r e c o 2 ρ f t n o 2 v o l σ γ o f l o o r where ρ g a s is the combined density of the all gases in a computational cell γ o f l o o r is an assumed minimum bulk oxygen concentration that is possible within a burning cell and is set to 0 1 kg o2 kg air but could be adjusted to be a function of fire ca cell volume v o l the dimensionless constant c o 2 in the exponential is initially set to 2 based on cursory analysis of cfd calculations which will be refined through further explorations in subsequent studies n o 2 is a stoichiometry coefficient normalized by the mass of the total products for oxygen in the combustion of wood in a simple wood burning model n f w o o d n o 2 o x y g e n p r o d u c t s h e a t this chemical equation involving a solid fuel and oxygen is a significant simplification compared to the multistage chain of reactions involving pyrolysis and many gas phase reactions and it should be noted that this approach assumes that the rate of combustion is mixing limited the rate of pyrolysis is implicitly assumed to be limited by the availability of heat from the reaction which is limited by the mixing processes bringing oxygen to the pyrolyzing fuel the value of n o 2 5448 and n f 4552 based on drysdale s 2011 stoichiometry for net combustion of wood the stoichiometry factor λ is adjusted to prevent maximum burning with fuel or oxygen rich scenarios linn 1997 λ ρ f ρ o ρ f n f ρ o n o 2 2 the parameter ψ is calculated based on the number of eps n e p that have been absorbed by dry fuel in the previous time step using the following relationship ψ n e p e e p t b u r n o u t ρ f v o l h w o o d c p w o o d t c r i t t a m b i e n t where ψ is constrained to be between 0 and 1 and e e p is the energy per unit time per ep which can be specified by the user the value of e e p can be thought of as degree of energy resolution with large values leading to fewer eps for the same reaction rate and thus less computational cost but the representation of the distribution of energy transferred to surroundings is reduced smaller values provide better finer grain representation of energy transport but add computational cost e e p is set to 50 kw as a balance between computational cost and representation of the energy transport in the simulations for this manuscript t b u r n o u t is the assumed time for fine fuel particles to burn out which is currently estimated to be 30 s though the sensitivity of the modeled behavior to this parameter will be explored in future work t c r i t and t a m b i e n t are the temperature k where fuel is assumed to combust and the ambient air respectively h w o o d is the heat of combustion of wood which is taken to be 18620 kj kg 1 drysdale 2011 and c p w o o d is the specific heat of wood which is taken to be 1 7 kj kg 1 k 1 the turbulent mixing parameter σ is computed in a customary manner for turbulent diffusion coefficients σ 0 09 s ρ g a s k where ρ g a s is the density of the ambient air while the length scale s and turbulent kinetic energy k are based on grid cell size and a smagorinsky style formulation following germano et al 1991 the reaction rate r explicitly describes the reduction in fuel within a computational cell kg m 3 s 1 per computational time step which is currently taken to be 1 s in the context of this text the total energy release rate kj m 3 s 1 q t o t a l can be formulated in terms of the heat of combustion of wood h w o o d q t o t a l r h w o o d because it takes energy to heat fuel from ambient temperature to a nominal temperature of combustion t c r i t we compute the net energy release from the fuel as q n e t r h w o o d c p w o o d t c r i t t a m b i e n t a fraction of q n e t is assumed to be lost upward to the distant atmosphere c r a d l o s s but to avoid a complex radiation calculation to determine the precise amount of energy lost to the sky versus absorbed by surroundings we estimated c r a d l o s s to be 0 2 20 of the net energy this preliminary percentage estimate was based on consideration of radiation view factors and cursory examination of cfd calculations further analysis of detailed physics calculations and experiments could be used estimate as a function of fire or environmental factors in the future the number of eps that are available to be transported to other locations is computed as n e p 1 c r a d l o s s q n e t e e p 2 5 transporting energy energy transfer to unburned fuel and thus spreading fire is accomplished by moving eps from a location where they are produced to another location energy from wildfires is transported to surroundings through a variety of processes that occur over different scales for the purposes of this simplified representation of the energy transport the heat transfer processes are grouped into two different categories which are referred to as 1 wind dominated and 2 creeping in the context of this model and this simplified categorization the wind dominated category is intended to capture the combined heating effects of convective bursts that blow transport heat in the direction of the local wind and radiative heating from flames that are also influenced by the local wind this category of energy movement can be associated with fires in surface or elevated shrubs or trees fuels and is assumed to be the dominant mechanisms for head fire spread it is true that radiative heating is not constrained to the direction of the wind but the radiative heating that is contrary to the wind direction is often not effective at spreading the fire since it is quickly offset by convective cooling the creeping category includes the much slower influences of heat transfer processes occurring locally within the surface fuel beds including those associated with contact between adjacent fuel particles and fine scale mixing that happens within the fuel bed these surface fire mechanisms are assumed in the model to be key factors in flanking and backing fire spread portions of a fire moving perpendicular and contrary to the direction of fastest fire spread and evaluated against high resolution observations of experimental burns in 2017 2018 at tall timbers research station unpublished data using methods similar to loudermilk et al 2014 these two categories of heat transfer process wind dominated and creeping can be thought of as associated with fireline scales on the order of the width of the fireline in the direction spread or fuelbed scales on the order of the depth of the surface fuel bed these notional scale ranges are not mutually exclusive and they depend on the fire environment as the high intensity or wind driven fires tend to have much wider or deeper firelines than low intensity prescribed fires however fireline scales can be significantly larger than fuelbed scales and thus can contribute to much larger rates of fire spread the cellular automata aspect of this model represents the net influences of the movement of energy by wind dominated or creeping processes through the transport of energy packets per unit time or eps ep s are simply a conceptualized quantity of energy that can move from one location to another per unit time this could be the energy associated with hot gases or flames or radiant energy each ep represents a fixed amount of energy that is moved within a unit of time e e p and they move through space but are not associated with specific volumes the notion of dilution or diffusion of energy as it moves away from its source is accomplished by decreasing the ep density per unit volume as distance from source increases since a typical power for an ep is 50 kw there are often many of eps being emitted in a given time step depending on the fire intensity at a given moment and location currently the transport rate or velocity of the ep while being transported is not computed and thus the eps move from one location to another the movement of eps associated with wind dominated processes is achieved by deriving a direction and distance for the travel of each ep based on phenomenology observed in laboratory and field experiments as well as much higher fidelity coupled fire atmosphere simulations for each ep within a computational cell a location for production is chosen randomly from within the zone that is taken to be actively burning within a computational cell described in sub grid zones active fire fuel depletion moisture depletion subsection below a possible length scale for the ep travel is estimated based on the observed flame length relationships described in nelson et al 2012 in order to use their relationship a local fire intensity is estimated by i n e p e e p a a c t i v e f i r e z o n e where a a c t i v e f i r e z o n e is the area of the sub grid zone that is actively burning the flame height is then estimated by h f l a m e h f u e l 0 0155 i 2 3 where h f u e l is the height of the surface fuels and is taken to be zero for fuels in cells above the lowest vertical level nelson et al 2012 also provided an expression for the fire updraft speed within a flame w which is not the same as a cell average vertical velocity w 0 377 i the flame updraft speed w is concentrated in the location where the ep is emitted and is not necessarily the same as the plume velocity described above as the plume is associated with the collective buoyancy averaged over a cell we used the flame height and fire updraft speed to estimate a length scale for the distance of heat transfer l in calculating l we assume that the minimum length scale occurs at a horizontal wind speed of 0 but is stretched as horizontal wind components increases l h f l a m e m a x i m u m 1 3 u 2 v 2 w 2 1 4 where u and v are the local horizontal wind components this expression is effectively the scaled square root of a convective froude number using this expression when horizonal winds exceed three times the flame induced vertical wind speed the length scale starts to get stretched both the coefficient 3 and the square root of the froude number elements of this expression are preliminary and based on phenomenological expectations but will be verified or refined through comparison with observations in future work this length scale is used to describe the upper limit of the potential distance that an ep could move the length scale is then adjusted based on whether it is more aligned with the horizontal or vertical winds and the magnitude of those winds l l 2 α h w π 2 α h u 2 v 2 π u 2 v 2 w 2 where α h is the angle of the ep trajectory with respect to the horizontal and is described below in this equation w is a vertical wind component that is adjusted if only part of the cell is known to be on fire factoring in for the concentration of an updraft the distance that an ep moves is taken randomly from a triangular distribution ranging from 0 to l 0 being the location of maximum probability thus the travel distance for an ep d becomes d l 1 1 r n d 1 where rnd1 as well as rnd2 rnd3 rnd4 rnd5 and rnd6 below are a random number between 0 and 1 which are chosen for each ep rnd1 through rnd4 are used at convective scales and rnd 5 and rnd6 are used at creeping scales described below the ep trajectory angle relative to the x axis in the horizontal plane α x is determined by adding a perturbation to the u and v wind components as illustrated in fig 2 the strength of the perturbation is determined using a gaussian distribution with a standard deviation of k 2 where k is the local turbulent kinetic energy α x t a n 1 u k 2 erf 2 r n d 2 1 v k 2 erf 2 r n d 3 1 use of a gaussian function to represent turbulent fluctuations within a forest canopy is an oversimplification as velocity distributions within forest canopies are known to be non gaussian and skewed shaw and seginer 1987 mueller et al 2014 error induced by this oversimplification could be further amplified by the presence of a fire as heilman et al 2017 revealed that fires can further enhance the skewness of the velocity distributions the gaussian function was chosen for simplicity and evaluation of non gaussian velocity distributions will be investigated in future work the angle from the horizontal α h is determined by first computing a metric ϕ describing the local dominance of the fire induced updraft over the horizontal wind ϕ w w u 2 v 2 ϕ approaches one when the updraft dominates over the horizontal wind and goes to zero when the horizontal wind is strong and the updraft is minimal this metric is used to set the median angle of the ep trajectory above the horizontal plane where the median ep trajectory is vertical when the updrafts dominate see fig 3 α h π 2 ϕ based on wind tunnel and field observations as well as high fidelity simulations the movement of heat from the active fire is heavily influenced by a tower and trough structure finney et al 2015 the tower and trough structure tends to bifurcate the projection of the movement of hot air on a vertical plane aligned with α x into two general paths one with a strong horizontal component and one with a strong vertical component the result is that heat advection from the fire often does not follow the median fireline wind trajectory but instead follows either a path below the average trajectory or a more vertical path above the average trajectory based on this phenomenology as well as unsuccessful explorations where energy followed the median trajectory or local instantaneous mean wind direction quic fire bifurcates the movement of heat along either of two paths based on the value of α h in this initial version of quic fire we have made the approximation that ratio of α h to vertical π 2 determines the probability that an ep will follow a vertical path both this linear probability dependence function on α h and the strictly vertical and horizontal trajectories will be the subject of follow up research through detailed analysis of laboratory and field observations and cfd based calculations but for this preliminary version we use α h π 2 i f 2 π α h r n d 4 1 a n d α h 0 i f 2 π α h r n d 4 1 the transport of eps to represent the creeping category of heat transfer process is meant to be only within the surface fuels and thus unlike the wind dominated process model the travel direction for creeping movement of eps is currently constrained to be in a horizontal plane and only in the computational cells closest to the ground the creeping travel distance and direction are estimated based on observations of some of the mechanisms by which flanking and backing fires spread 1 short duration turbulent fluctuations 2 particle to particle spread and 3 small scale reverse flows caused by turbulence driven by surface fuel roughness the net result of these three mechanisms is a slow spread in directions that are not aligned with the mean wind these are parameterized with the following expressions for the direction relative to the x coordinate axis α x c r e e p creeping length scale l c r e e p and a triangular distribution of transport distances d c r e e p relative to the creeping length scale α x c r e e p 2 π r n d 5 l c r e e p l c r e e p 2 d t 1 u c o s α x c r e e p v s i n α x c r e e p u 2 x 2 d c r e e p l c r e e p 1 r n d 6 the creeping length scale depends on a length scale parameter l c r e e p which is meant to be a function of the fuel bed geometry height packing ratio etc and turbulence intensity but is currently estimated to be 2 m based loosely on the structure of grasses initially simulated and length time scales of local wind fluctuations observed in backing fires we intend to fully resolve this parameter in future versions of quic fire the ratio in the parentheses is effectively the projection of the unit vector of the horizontal wind speed on the creeping direction and thus l c r e e p is a maximum of 2 m when the creeping direction is in the opposite direction of the wind with this formulation d c r e e p is chosen from a triangular distribution of the length scales ranging from 0 to l c r e e p with the highest probability being 0 2 6 sub grid zones active fire fuel depletion moisture depletion for quic fire to rapidly simulate fires on a landscape scale kilometers to tens of kilometers with minimal computational resources we expect grid cell dimensions to be on the order of a meter or larger this is not the minimum model resolution but at finer scales the model would be unable to explicitly resolve the details of fire perimeters and fire widths of low intensity fires since our goal was to produce a model useful for prescribed fire it was important to be able to adequately capture the behavior of low intensity fires where the fire activity or fuel conditions are heterogeneous within computational cells for this reason quic fire models three subgrid spatial zones within each computational cell the active fire zone fuel depletion zone and the moisture depletion zone fig 4 for this initial version of the quic fire model we described the zones in two horizontal dimensions x and y this simplification was made for computational ease because in our current application the vertical dimensions were often about half the size of the horizontal grid dimensions i e 2 2 1 m and in cells closest to the ground the fire and depletion activity were limited to the height of the fuels it is important to note that by not tracking the vertical extent of the active fire zone we were not adding the vertical cell dimension to our vertical energy transport as eps were still emitted from random vertical positions from within the fuel bed we will explore the implications of a three dimensional zone description in future versions these zones active fire fuel depletion and moisture depletion shaded red brown and grey in fig 4 were described with a zone center a x a y f x f y d x d y and a standard deviation in the x and y direction indicated with σ in fig 4 for simplification purposes we assumed that within each of the three zones the associated properties of the fuels and fire activity were homogeneous in other words fuels and fuel moisture were homogenously distributed within their respective depletion zones we also assumed that no fuel or moisture was depleted outside of their respective zones fire activity was treated as uniform within the active fire zone and thus eps were assumed to be emitted from random locations within this zone these three zones are not mutually exclusive and in fact typically overlap as illustrated in fig 4 for example when the fire ep lands in a cell it will deplete some moisture when a second ep lands in the cell if it does not land in the same location a new moisture depletion center and spatial standard deviation can be calculated based on the accumulating ep destination locations thus creating a rectangular moisture depletion zone as subsequent eps are absorbed by the fuel in the cell where there is remaining moisture the moisture depletion center is again adjusted along with the standard deviations all the while tracking the total moisture that is evaporated from within the fuel depletion zone alternatively if an ep is absorbed within the previously mapped out moisture depletion zone one of two cases occurs 1 more moisture is evaporated and the new destination point is used to update the moisture depletion zone center and standard deviations or 2 the moisture is completely depleted within this zone in which case the moisture depletion zone is left unchanged but the energy of the ep is used to consume some fuel and contribute to the development of the active fire zone if eps continue to land within the moisture depletion zone eventually the moisture will be depleted within this zone and subsequent eps can initiate active fire this methodology would allow eps being lofted short distances compared to the width of a cell into a cell for example from the right to develop a dry zone along the cells right side eventually it will be possible to develop an active fire zone that is entirely inside the moisture depletion zone once the active fire zone is established ep s are emitted from this zone which has the potential to send eps beyond the current moisture depletion zone further expanding it to the left as solid fuel inside the active fire zone is consumed a fuel depletion zone is established creating a region where some but not all of the fuel has been consumed the center and extent of the moisture and fuel depletion zones were calculated based on simple running statistics based on where the moisture and fuel depleting eps land during the history of activity in a cell the active burning region is more complex as fire activity is transient at any specific fire location the center of the active fire zone is thus weighted most heavily by the locations where the heat release rate is greatest when a new ignition location is established its heat release rate is approximated with an initial maximum followed by a linear decline with time over t b u r n o u t the implications for this subgrid zone approach have yet to be fully explored but they likely depend on the size of the cell in comparison to the energy transport distances or the depth of the fireline for scenarios where the values of l are on the same order or larger than the cell size the zones can rapidly fill the entire cell as absorbed eps are distributed widely across the cell in this case the value modeling the three zones is minimal but in scenarios where l is small compared to the cell size the zones will have more importance the assumption of homogeneity within respective zones is the subject of ongoing discussions as it is certainly not applicable to all fuel beds by using a more complicated distribution within each zone we would be able to represent the gradients within each zone but at this point we believe that even with a top hat distribution of depletions and fire activity within their zones the subgrid heterogeneity provided by the zones themselves in the zone or out of a zone helps capture the impacts of some of the subgrid variability 2 7 fate of ep in quic fire when an ep reaches its destination location it evaporates moisture ignites fuel in the new location or heats the atmosphere the fate of each ep is determined by the state of the destination fuel a series of checks are performed regarding the probability of the presence of fuel and moisture level of the fuel fuel densities and surface area per unit volume are used to determine the probability of an ep landing on viable fuel using the following equation p v i a b l e f u e l 1 e ρ f l o c a l c i g s c a v u h 2 w 2 a v n o r m u h 2 ρ f l o c a l ρ f 0 in this equation a v is the surface area per unit volume of the local fuel and a v n o r m is a parameter used to normalize surface area per unit volume such that a v a v n o r m is unitless a v n o r m is currently chosen to be 4000 m 1 which is a typical value for grass ρ f l o c a l is the local bulk fuel density where the ep lands and ρ f 0 is the bulk density of the fuel in that computational cell at the beginning of the simulation u h is horizontal wind velocity and w is the local vertical velocity interpolated to cell centers c i g s c kg m 3 is a constant currently set to 6 kg m 3 that represents a theoretical density where the fuels are thick enough that an ep traveling 1 m through a cell with a v a v n o r m will a have 63 chance of being absorbed by fuel and not going to the atmosphere this use of this constant can be thought of as the influence of the fuel density on the optical path length and on the spatial gradient of heat absorption into the fuel as hot gases blow through cooler fuel both of which take on exponential forms under some circumstances the value of c i g s c which is currently set to 6 kg m 3 does not currently have a firm basis yet but could be evaluated as part of future refinements the portion of the equation in square brackets describes the effects local wind and fuel characteristics on the probability of the ep finding viable fuel if we assume that the fuels remain evenly distributed this term describes something like the effects of fuel on mean free path of an ep the factor on the right of the expression accounts for the increased spatial heterogeneity and gaps between fuel elements as the fuel gets depleted if the ep lands within the fuel depletion zone then the locally depleted fuel density of the zone fuel depletion concentrated in fuel depletion zone is used as ρ f l o c a l as fuel is depleted in the depletion zone its receptivity to new eps approaches zero if an ep does not get absorbed by the fuel then its energy is released to the atmosphere and goes on to generate a buoyant plume if it is absorbed by fuel then it can either heat ignite some of the fuel or heat and evaporate water if the fuels at the destination are wet if the ambient fuel bed is not completely dry 0 moisture at the beginning of the simulation the ep destination location will either fall within a moisture depletion zone or not if the ep was absorbed by fuel outside the current moisture depletion zone its energy will be used to heat a fraction of the moisture to vaporization additionally the location of the ep within the cell was used to grow the moisture depletion zone of the cell if an ep was absorbed by fuel inside the current moisture depletion zone then its energy will be used to evaporate any water remaining within the zone if there was no moisture within the zone then the energy of the ep was used to ignite fuel and this was considered a successful ignition 3 simulations to demonstrate model performance we use two case study comparisons between firetec and quic fire simulations the goal of these comparisons is to assess if the reduced order representations of processes and fire atmospheric feedbacks in quic fire at cell levels aggregate to produce overall fire behavior and response to fire conditions that are like what a full cfd fire atmosphere model provides one advantage to comparing quic fire with firetec is that they rely on nearly identical 3d representations of vegetation in the fuelbed allowing for direct comparison of outputs to the cfd based firetec model 3 1 case study 1 long line vs short line ignition linn and cunningham 2005 hereafter lc2005 conducted a series of simulations using higrad firetec to investigate how fire behavior in grasslands was influenced by the initial length of the fireline and the ambient winds initial line length was found to influence both the rate of spread and fire shape due to differences in the coupled fire atmosphere interactions overall the head fire spread rates of lc2005 were found to be in good agreement with both field experiments cheney and gould 1995 cheney et al 1993 and empirical model estimates cheney et al 1998 the simulations of lc2005 provide an ideal test case for initial evaluation of whether the simplified representations of local fire phenomena in quic fire combine to capture reasonable overall fire atmosphere feedbacks and thus fire behavior as they provide a very simple fire environment that highlights the role of fire atmosphere coupling for the simulations of lc2005 a domain of uniform fuels with no topographic slope was used the fuels represented tall grass with a height of 0 7 m and an aerial distribution of 0 7 kg m 2 the computational domain was discretized into a uniform horizontal grid of 400 m 400 m with 2 m horizontal resolution and a stretched vertical grid with the near ground spacing starting at 1 m and stretching to 25 m spacing at a height of 150 m the initial fuel moisture fraction i e the mass of water divided by the mass of fuel was specified as 0 05 in the quic fire simulation ambient winds are prescribed to be in the positive x direction and are initialized with a log profile that assumes neutral atmospheric stability for this study we consider wind speeds of 1 3 6 and 12 m s 1 as measured at a height of 10 m in lc2005 the initial and inflow profiles are of the same speeds but a uniform profile was assumed and vegetative drag could develop the near surface shear layer to account for potential differences between our log profile and the profile of lc2005 we used mid flame wind speeds reported in lc2005 and adjust these to a reference height of 10 m using a log profile using a roughness length of 0 1 m for tall grass we consider ignition line lengths of 16 and 100 m hereafter referred to as short and long lines respectively for quic fire ignition was accomplished by removing any moisture and adding energy to the grid cells designated for ignition for these cases the amount of energy added was 2 eps in lc2005 ignition was accomplished by removing any initial fuel moisture from the ignition cells and increasing the temperature from 300 k to 1000 k over a span of 2 s difference in ignition method should only impact very early stages of fire spread and thus we focus comparisons on times after the first 60 s of spread 3 2 case study 2 prescribed fire multiline ignition to demonstrate quic fire s ability to work with three dimensional fuels and complex ignition patterns we simulated a typical burn pattern for a prescribed fire at eglin air force base eafb ignited by 5 strip head fire ignition lines moving perpendicular to the wind direction the rates of ignition across the unit reflect eafb use of all terrain vehicles atvs to ignite units of this size the 3d fuels were parameterized for fire maintained longleaf pine pinus palustris turkey oak quercus laevis sandhill ecosystems using fire effects monitoring data hiers et al 2007 and supplemented by fuel data from the prescribed fire combustion and atmospheric dynamics experiment rxcadre ottmar et al 2016 we analyze each simulation for fuel consumption in different forest strata which are common foundational data for prescribed fire objectives the burn unit modeled for this comparison was 40 ha with full model domain extending 500 m 1200 m in planer dimensions and 614 m in height with 2 m horizontal resolution and 1 m vertical resolution we define surface fuels as those fuels in the lower most model layer height 1 5 m midstory fuels occupy layers 2 5 height 1 5 3 m and canopy fuels as any fuels above layer 3 height 3 m this simulation was compared with firetec using the same fuel inputs ignition pattern rate of ignition and mean input windspeed of 5 m s 1 4 results 4 1 comparisons to grass fires fire perimeters as a function of time for wind speeds of 1 3 6 and 12 m s 1 are shown in figs 5 and 6 for the short and long line cases respectively for the short ignition line and 1 m s 1 winds upper left panel of fig 2 the pattern of fire spread is predominantly circular and as wind speeds increase the fire perimeters become elongated and focused to a sharp point downwind of the ignition the longer ignition lines in fig 6 lack the circular pattern at low speeds and have a more lobed pattern due to increased competition for indrafts to feed the larger number of updrafts required for the longer line fig 7 shows the downwind propagation of the fire front with time for the short line cases for the quic fire simulations spread rates are nearly constant for each wind speed after a period of adjustment during the first 60 s as the initial ignition line builds into a sustainable fire front the impact of this adjustment period on rate of spread was evident when estimated via a linear fit as performed in lc2005 table 1 limiting the data in the linear fit to the period after the 60 s adjustment period improves the agreement with lc2005 spread rates for the 1 and 12 m s 1 cases the long line cases also exhibit near constant spread rates outside of the ignition influenced period fig 8 as for the short line cases the quic fire simulation spread rates are slower than those of lc2005 table 2 adjusting the log wind profile used in quic fire such that the near surface winds more closely match the winds in the lowest model level of lc2005 greatly reduces the differences in spread rates for the higher wind speed cases the adjusted 10 m wind speeds are 1 1 3 5 7 6 and 14 5 m s 1 while adjusting the wind profile improves the agreement between quic fire and lc2005 other sources of potential disagreement such as differences in turbulence levels are not as easily adjusted overall the quic fire results are consistent with those of lc2005 in that longer ignition lines result in faster rates of spread than shorter lines for a given wind speed as noted above the results of lc2005 are generally consistent with the field experiments described by cheney et al 1993 and cheney and gould 1995 the one exception to this consistency was lc2005 finding the influence of line length being stronger for lower winds where cheney and gould 1995 indicate that the effect was greatest at high winds low wind speeds are where quic fire and lc2005 differ the greatest with quic fire response to line length being weaker at low winds than lc2005 table 3 this result however shows quic fire simulations to be more consistent with the findings of cheney and gould 1995 lateral spread was significantly higher in quic fire than lc2005 while lc2005 yielded length to breadth ratios lower than those of alexander 1985 discrepancies were potentially attributable to an overly simplified treatment of radiation or turbulent heat exchange or under resolution of the fuel bed unlike lc2005 quic fire yielded length to breadth ratios larger than those of alexander 1985 the low lateral spread rates from quic fire could be caused by the absence of natural variability in the input wind field which was artificially constrained in these comparisons alternatively the low lateral spread rates from quic fire could also be a manifestation of our assumed gaussian and isotropic velocity distributions as alexander 1985 does not provide variability statistics on winds it was difficult to replicate subtle but potentially important variation driving lateral spread 4 2 prescribed fire multiline ignition simulation of an operational prescribed fire using multiple ignition lines shows good qualitative agreement between quic fire and firetec fig 9 five firelines were ignited in the simulation moving from right to left in fig 9 and the ambient wind of 5 m s 1 was blowing from the image foreground towards the background after 120 s of simulation time top 2 images in fig 9 the surface fire was beginning to ignite the midstory as ignition lines converge to create strong convective updrafts by 300 s bottom 2 images in fig 9 the fire was actively consuming fuel within all three fuel strata by 420 s not shown consumption rates decline as fire from early in the simulation entered a smoldering phase and the left flank continues flaming combustion within the last zone of merged firelines overall in the quic fire simulation 98 6 of surface fuels were consumed along with 33 6 of the canopy here surface fuels are those fuels within the first vertical level of the model and canopy fuels are those fuels above the fifth vertical level these consumption values are considered typical for aggressive ignition patterns like strip head fire that target midstory consumption and often at the expense of some canopy consumption ottmar et al 2016 comparison of fuel consumption time series with the firetec simulations reveal similar trends fig 10 firetec ramps up to a constant rate of consumption for surface fuels more quickly than quic fire but the rates of consumption are similar and the overall difference in the percentage of fuel consumed is only 3 2 for canopy consumption firetec is again quicker to initiate canopy consumption but the rates and total consumption are quite similar for quic fire differing by 2 6 in total canopy consumed 5 discussion the new coupled fire fuel atmosphere model quic fire shows great promise as an emerging planning tool for complex ignitions as these two test case studies illustrate quic fire is capable of faster than real time simulations of complex fire atmospheric feedbacks resulting from variation in ignition patterns and produced results like the cfd solution for these scenarios at 1 2000 the computational cost due to nearly identical fuel inputs quic fire performance can be analyzed against the numerical cfd model higrad firetec under a range of wind speed conditions in these two ignition line length scenarios quic fire showed similar trends to the firetec spread rates while its performance diverged at lower windspeeds its results showed closer agreement than firetec to empirical data on which the lc2005 simulation was built cheney et al 1993 this realistic representation of the interaction of wind speed length of ignition line and fire spread was a critical test of the ability of quic fire to capture basic phenomena that emerge from coupled fire atmospheric processes the comparison of quic fire to firetec using a complex ignition pattern common for prescribed fire operations in the southeastern united states further illustrates potential utility in capturing altered flows resulting from both the 3d fuel structure and the fire induced buoyant plume in planning prescribed fires a manager routinely relies upon experience to distill what combination of ranges of wind speeds fuel moistures and ignitions patterns yield fire intensities that are suitable for achieving desirable ecological outcomes models commonly used in prescription development nwcg 2017 cannot account for the multiple interacting firelines associated with most prescribed fire ignitions as they assume free burning fire yedinak et al 2018 interacting firelines on prescribed fires drive areas of enhanced convection that migrate progressively across the burn unit forming complex patterns as firelines merge and burn out this simulation shows the promise of quic fire to rapidly account for how winds fuel conditions and ignition pattern interact to develop a complex mosaic of fire interactions across a burn unit for models to be useful in evaluating prescribed fire ignition scenarios the impacts of the fire atmosphere interaction must be effectively resolved in scenario 2 the convection convergence driven by interacting firelines was similarly represented by both quic fire and firetec put simply as heated air parcels rise above the fires other air parcels were drawn towards the rising air to fill the volume vacated by the rising air as intensity increased greater volumes of air were pulled in under the rising plume nearby fires pulled on one another with the net effect that different firelines were drawn together in addition it should be noted that the fire generated flow is not aligned with the mean ambient wind flow this phenomenon must be adequately described in terms of the underlying fire dynamics for a model to be useful in situations where complex patterns of multiple firelines exist such as most prescribed fires in addition to being able to capture the impact of heterogeneous and dynamically placed ignitions quic fire accounted for influences of fuel structure and ambient winds that change during the course a fire despite the recent focus of fire science on the importance of convective heat transfer canfield et al 2014 finney et al 2015 only cfd representations could reliably capture buoyancy driven fire atmospheric feedbacks linn and cunningham 2005 mell et al 2009 parsons et al 2011 yet such tools remain inaccessible due to intense computational demands and are largely applied in research contexts cruz and alexander 2012 mell et al 2018 sullivan 2009a thus managers continually rely on radiation driven semi empirical fire spread models for operational planning andrews 1986 finney 1998 tymstra et al 2010 that were originally designed to predict forward fire spread and employed in evaluating firefighter safety unfortunately these types of operational models are inappropriate for prescribed fire scenarios where the importance of fire atmosphere interaction is amplified both prescribed fire ignition patterns and wildfire tactical ignitions are determined primarily by highly variable wind fields and fuel characteristics e g moisture content and loading all of which change in both space and time heilman et al 2015 it is also common for prescribed fires to be performed in the context of high structural variability in vegetation that occurs across scales furman 2018 for example both fine scale fuel breaks and adjacent large burned areas can alter patterns of air flow and can facilitate enhanced inflows that have a large impact on fire behavior linn et al 2012 alternatively planned burned units are often bordered by denser vegetation which is more resistant to ambient wind flows and alters patterns of fire spread lashley et al 2014 the combined effects of vegetation structural influences on wind will influence fire behavior in ways that may help or hinder in meeting burn objectives therefore a useful model must be able to account for the effects of vegetation on both fuels and wind flows while quic fire does not include a detailed treatment of the turbulent processes associated with vegetation or fire it has compared favorably to the more physically detailed firetec model and thus shows potential for being a useful tool for fire managers quic fire represents a critical innovation in efforts to create a prescribed fire modeling tool that could be widely applied by fire managers including the essential coupled fire atmospheric interaction without large computational demands quic fire provides the ability to manipulate fuels winds and ignition patterns and then compare predicted fire behavior among those scenarios the computational efficiency allows for ensemble runs to explore for example fire behavior predictions resulting from variations in wind speed gustiness or fuel moisture the reliance on 3d fuels already available for firetec is a critical strength of this tool as quic fire predictions can be tested against a cfd model solution to examine the underlying physics when predictions do not match expectations 6 conclusions here we describe the conceptual basis basic formulation and initial demonstration of a new fast running modeling tool quic fire that can be applied to prescribed fire planning quic fire is the integration of a phenomenologically based fire spread model with the fast running wind solver quic urb and provides a self determining fire prediction capability that represents the critical coupled fire atmosphere feedbacks at scales relevant for prescribed fire although the development of this model is in the nascent stages of development initial results show an encouraging capability to capture basic trends in fire behavior response of fire spread to size of fire and consumption of canopy fuels in prescribed fire scenarios based on the results of this initial assessment the combination of the quic urb wind solver with a phenomenologically based cellular automata approach to modeling the fire spread shows promising potential and warrants continued development quic fire is currently being used for simulations of fire on relatively flat terrains and most prescribed fires in the us occur under minimal topographic relief melvin 2015 however current efforts to adapt this coupled fire atmospheric modeling framework for topography are expected to extend quic fire s application to prescribed fire in complex terrain quic fire s ability to represent fire atmospheric feedbacks such as those that govern prescribed fire behavior at relevant scales while running on a basic laptop is intended to begin providing a capability for a broader set of users to explore prescribed fire behavior its ability to model response to both ignition patterns and a temporally and spatially variable fire environment without computational expense of cfd solutions is a critical design feature advancing this initial effort however will require continued refinement and validation against observations for example the representation of near fire turbulence as influenced by ambient winds vegetation and the fire itself is a focus of model advancement efforts ambient and fire influenced as the limitations of the current smagorinsky style approach could have implications for the model s representation of fires in some circumstances additionally efforts to refine representations of heat transfer length scales directions and efficiency representation of backing and flanking fire spread phenomenology as well as vegetation drag and fire atmosphere feedbacks will continue declaration of competing interest authors declare no conflicts of interest aknowledgements funding was provided in part by the department of defense through the strategic environmental research and development program project rc 2643 and environmental security technology certification program project rc 201303 the los alamos national laboratory ldrd program the usda forest service southern research station usda forest service rocky mountain research station and usda forest service r d washington office through the national fire plan and tall timbers research station we are appreciative of the efforts of james furman brett williams and participants in the prescribed fire science consortium for their support and encouragement in developing this tool appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104616 
26069,coupled fire atmospheric modeling tools are increasingly used to understand the complex and dynamic behavior of wildland fires multiple research tools linking combustion to fluid flow use navier stokes numerical solutions coupled to a thermodynamic model to understand fire atmospheric feedbacks but these computational fluid dynamics approaches require high performance computing resources we present a new simulation tool called quic fire to rapidly solve these feedbacks by coupling the mature 3 d rapid wind solver quic urb to a physics based cellular automata fire spread model fire ca quic fire uses 3 d fuels inputs similar to the cfd model firetec allowing this tool to simulate effects of fuel structure on local winds and fire behavior results comparing fire behavior metrics to the computational fluid dynamic model firetec show strong agreement quic fire is the first tool intended to provide an opportunity for prescribed fire planners to compare evaluate and design burn plans including complex ignition patterns and coupled fire atmospheric feedbacks keywords fire behavior model fire atmospheric feedbacks cellular automata prescribed fire model 1 introduction wildland fire behavior and subsequent fire effects are largely driven by complex heterogenous and dynamic fire atmospheric feedbacks hilton et al 2015 linn et al 2013 understanding the complex interactions of ignition pattern heterogeneous vegetation and dynamic fire environmental conditions are critical to accurately predicting fire behavior hoffman et al 2018 and subsequent fire effects o brien et al 2018 many fire behavior models have been developed to predict fire spread energy release and fire effects using a variety of approaches sullivan 2009a 2009b but ultimately all wildland fire models must balance representing the complexity of fire fuel atmospheric feedbacks and the speed of predictions hilton et al 2018 in addition firefighter safety and rapid assessments of fire spread have dominated the objectives for much of the modeling to date leading to adoption of tools producing rapid outputs as a preference over models that capture complex wildland fire dynamics this bias for speed over accuracy is particularly problematic for prescribed fire practitioners who must account for complex ignition patterns and dynamic environmental conditions i e wind and moisture to plan and execute burning treatments intentional ignitions are used to manage 5 million hectares annually in the us melvin 2015 with many millions more burned globally bond and keeley 2005 guyette et al 2017 ichoku et al 2008 planning and development of legal prescriptions for prescribed fire practices to meet objectives depends on anticipating the fire s response to variation in fuels and weather conditions chiodi et al 2018 o brien et al 2018 wade et al 1989 one complication to the prediction of prescribed fire behavior compared to wildfire scenarios is the significant influence of the rates and patterns of ignition on fire behavior also the interaction between the fire environment and practitioner designed ignition practices depends on landscape scale mean fuel and weather conditions as well as their localized spatial and temporal variability canfield et al 2014 furman 2018 in managed fire regimes surface fire behavior is further complicated by three dimensional turbulent flows within the canopy parsons et al 2011 pimont et al 2011 and by surrounding edge effects such as those produced by roads or adjacent previously burned blocks linn et al 2012 in this case fuel structure influences wind flow through the canopy in the vicinity of the surface fires affecting air entrainment among firelines and draw between fires to cope with such complexity and ultimately the spread patterns and intensity potential of fires managers are forced to rely exclusively on experience or rules of thumb cruz and alexander 2019 as the simplified models of forward fire spread available to plan fire activities do not account for these kinds of critical processes mell et al 2018 there are a variety of models that have been developed to generate rapid wildfire behavior predictions such as the fire spread model of rothermel 1972 and its derivatives including behave andrews 1986 farsite finney 1998 and prometheus tymstra et al 2010 these models are either empirically derived from algebraic regressions of fire behavior observations or semi empirical solutions employing simplified physical bases for their algebraic functional forms with the addition of calibration constants calculated from fire behavior observations sullivan 2009a 2009b the algebraic functional forms of these models describe the spread of wildfires as functions of mean local ambient conditions such as wind speed fuel conditions and topographic slope they do not however resolve the individual processes that produced the fire behavior such approaches extrapolate the single fireline behavior to complex patterns of firelines ignoring the effect of the fire and atmosphere interactions thus misrepresenting the spreading behavior and the fire intensity in fact multiple ignitions commonly used in prescribed fires or wildfire suppression tactics violate several key assumptions of most simplified wildfire spread models such as steady state fire spread non interacting firelines yedinak et al 2018 and homogeneity of fuels hiers et al 2009 improving the effectiveness of prescribed fire management requires new modeling tools designed to capture the complex dynamics of multiple firelines heterogeneous fuels and variable environmental conditions hoffman et al 2018 o brien et al 2018 to represent the influences of the dynamic interaction between fire fuel and atmosphere fire behavior models have been linked to computational fluid dynamics cfd models that characterize the movement of the atmosphere linn et al 2002 mell et al 2007 current models that explicitly represent some aspect of fire behavior and associated atmospheric response to fires fall into two categories 1 empirical fire spread models coupled to an atmospheric fluid dynamics model e g wrf fire coen et al 2013 wrf sfire mandel et al 2011 cawfe coen 2013 and 2 process based fire models coupled to an atmospheric fluid dynamics model e g wfds mell et al 2007 firetec linn et al 2002 firestar morvan et al 2007 the challenge for the use of the former class of fire atmosphere models for prescribed fire is that they are designed to run at horizontal resolutions 20 m where the representations of the fire and atmospheric perturbations are smoothed to larger scales in order to be consistent with the underpinning empirical basis of the fire models these scales are then too coarse to simulate prescribed fire dynamics or the fuel heterogeneity that drives prescribed fire behavior which are often smaller than 10 m the challenge for the process based coupled fire atmosphere models is the computational demands of these models which typically exceed the time or computer resources for practical use in planning prescribed fires achtemeier 2013 used a cellular automata ca based approach to model the movement of fire based on a set of rules attached to observed or theoretical phenomenology because ca models attempt to represent some of the local phenomenology of fires which is dominated by local winds it is possible to couple them to flow models to capture some of the two way feedbacks between fires and the atmosphere achtemeier et al 2012 coupled their ca model to a simplified wind model and produced promising results using only a limited set of rules based on the expert opinion of the developer such 2d fire spread can then be coupled to potential flow to rapidly predict fire spread hilton et al 2018 while the model of achtemeier 2013 used a simple depth averaged wind flow to model fire spread through a simplified localized spot driven model its ruleset was not directly tied to physical processes which has led to limited adoption outside of its use within the daysmoke model achtemeier et al 2012 additionally fuel representation relied on 2d models of forest type with a rule based influence of fuelbed height while 2d representations are common and can simplify predictions e g wrf s fire hilton et al 2018 such simplifications limit potentially important fire atmospheric interactions driven by 3d fuel variation i e locally increased surface flows in stem spaces downwind of a canopy gap in surface fire regimes parsons et al 2011 pimont et al 2011 for realistic representation of fire behavior that drives fire effects in a prescribed fire setting 3d time resolved fire behavior must be coupled to more accurate representations of fuels o brien et al 2018 this is also critically important to understanding prescribed fire behavior which results from complex ignition patterns furman 2018 as the response of fires to these ignition patterns depends on the local vertical and horizontal vegetation structure the ability of a simplified model to rapidly and accurately represent fire atmospheric feedbacks enabling representation of the interaction between multiple ignitions and 3d fuel structure would represent a crucial advancement here we describe a recently developed wildland fire modeling tool quic fire that uses a ca coupled fire atmospheric modeling approach that builds on the approach by achtemeier 2013 but applies physical process based rules to achieve a more generalized fire spread model onto 3d fuel data in the following sections model formulation results of preliminary simulation results lessons learned concerning the impacts of fire atmosphere coupling and a discussion of a path forward for this model are described 2 methods 2 1 quic fire model description quic fire is a wildland fire simulation tool designed to capture the coupled fire atmosphere interactions that are essential for simulating prescribed fire without extreme computational demands its design allows for wider spread use and ensemble calculations to cover for instance a range of possible weather or fuel conditions since we developed quic fire as a prescribed fire planning tool it was purposefully designed to be able to represent the fire atmosphere feedbacks that determine the behavior of prescribed fires as well as the influences of heterogeneous vegetation this was accomplished by exploiting the capabilities of the quic urb wind field solver pardyjak and brown 2003 singh et al 2008 coupled with a new ca based fire spread model referred to here as fire ca quic urb is a fast running wind field solver that was originally designed for computing flow fields around buildings in urban settings pardyjak and brown 2003 it combines mass conservation constraints with observation based wind field phenomenology algorithms to quickly compute 3d flow fields that include the influence of both structure driven drag and now the dynamic buoyancy sources of wildland fires the coupling between the extended quic urb wind field solver and fire ca was executed by passing a 3d vegetation and fire influenced wind field associated turbulence intensity from quic urb to drive fire ca fire ca then feeds back the spatially resolved and evolving three dimensional vegetation and heat release distributions to quic urb 2 2 extension of quic urb for wildland fire application the presence of the fire alters the wind fields in two ways through the buoyancy driven flows and through its effects on the vegetation and resulting impacts on aerodynamic drag in order to extend quic urb for wildland fire scenarios a representation of the strong buoyancy induced flows was implemented part of the heat produced by the flames warms up the surrounding atmosphere generating a buoyantly driven vertical movement of heated air in turn the displacement of the heated air draws in adjacent air to fill the vacated volume producing a convergence zone ultimately the resulting flow pattern influences the fire spread achtemeier et al 2012 as the hot air travels upwards in an isolated plume it mixes with fresh air and the volume that it influences increases with height while its upward velocity decreases with sustained heat release the rising parcels of air form a plume that increases in diameter with height and can lean downwind depending on local wind speed to capture these phenomena the plume trajectory and lateral expansion are parameterized with the briggs theory davidson 1989 with the briggs theory the plume radius and centerline updraft are recalculated at each point on the trajectory for this approach we initially assumed that there is non zero local horizontal wind speed and thus as hot air rises it is also translating some horizontal distance x from the spot of plume origination the centerline updraft w c is a function of the heat released by the fire and the horizontal downwind distance from the point of origin davidson 1989 w c 1 β 2 3 f b w s z z v e r t here β is an empirical constant z v e r t is an offset for a virtual plume origin and f b is a buoyancy term in m4 s 3 z v e r t is related to cell size since it compensates for the fact that the plume is starting with an area that is the same as the horizontal area for the cell z v e r t a c e l l π β f b is defined by f b g t a e ρ a c p a where g is the acceleration due to gravity t a ρ a and c p a are the ambient air temperature density and specific heat of the undisturbed air and e is the heat released by the fire in any given computational cell into the atmosphere in watts computationally plumes are initiated from each quic urb cell where heat is generated plume radii at the plumes origin are computed from the horizontal area a c e l l of the computational cell the plumes develop over time under the influence of their own buoyancy and surrounding winds which are influenced by the presence of other plumes as well plumes also influence each other for instance for two perfectly vertical nearby plumes their trajectories will bend toward one another in particular the plume with the smaller buoyancy will bend more toward the one with higher buoyancy this phenomenon requires o n 2 calculations where n is the number of plumes hence tracking three dimensional plumes emanating from every heated cell is very computationally intensive however the plumes of hot gases rising from nearby locations eventually merge thus in order to streamline computations two plumes are merged when their trajectories are closer than the sum of their radii that is when the plumes start to overlap the updraft of the merged plume is calculated based on lai and lee 2012 and the merged plume radius recalculated to guarantee mass conservation a gaussian similarity profile is used to describe the radial updraft decay this updraft is superimposed onto the quic urb solution while the resulting horizontal flow is established by imposing mass consistency as fire consumes fuels drag is reduced and the horizontal wind speed increases quic urb incorporates a cionco type correlation to parameterize the vegetation drag cionco 1965 nelson et al 2009 here we applied a novel solution to the drag dynamics by linearly interpolating wind speed between the calculated cionco profile representing the unburnt canopy and the unimpeded flow representing a fully burnt canopy in particular the wind speed is a function of the reduction in vegetation due to fire consumption for example in the absence of fire induced flows the wind profile in the absence of vegetation is a log law profile as is typical of the atmosphere sutton 1953 but in the presence of the vegetation canopy the wind profile within the canopy is slower than would be predicted by the log law fig 1 as the canopy burns the wind speed value moves along the black line in fig 1 the drag impact on local wind in each cell that initially has fuel in it is adjusted as the fire progresses depending on the local fuel consumption at a specific time in the context of a spreading fire that is consuming vegetation this modification in quic urb successfully captured the higher wind speeds in areas where fuels were consumed the procedure also enables the wind field to reflect some of the influences of the spatial distribution of fuel treatments and fuel heterogeneity 2 3 fire ca fire ca is built on a conceptual model that leverages the work of achtemeier et al 2012 and achtemeier 2013 where the energy transfer from one location to another is accomplished through a cellular automata ca approach in fire ca energy packets eps are moved from one location to another based on local wind environments eps either evaporate moisture start new fires intensify existing fires or transfer their energy to the atmosphere based on the fuel conditions in their destination cells each ep represents a fixed amount of energy per unit time e e p and the number of eps produced by each cell n e p in a given time step is determined based on an estimate of the reaction rate within that cell 2 4 reaction rate mass loss rate and heat release rate in each burning cell the local reaction rate average reaction rate within the cell is described in terms of the change in bulk density of fine fuel particles ρ f with time and is captured by the following equation ρ f t c m ρ f ρ o ψ σ λ where c m is a dimensionless reaction rate constant ρ f and ρ o are the local bulk density kg m 3 of fine solid fuel fine scale biomass particles such as foliage or small twigs and oxygen respectively ψ is the fraction of the fuel in the computational cell that is actively burning σ is the turbulent mixing coefficient and λ is the dimensionless stoichiometry factor the oxygen density is modeled based on the local reaction rate through the following expression ρ o ρ g a s 0 21 γ o f l o o r e c o 2 ρ f t n o 2 v o l σ γ o f l o o r where ρ g a s is the combined density of the all gases in a computational cell γ o f l o o r is an assumed minimum bulk oxygen concentration that is possible within a burning cell and is set to 0 1 kg o2 kg air but could be adjusted to be a function of fire ca cell volume v o l the dimensionless constant c o 2 in the exponential is initially set to 2 based on cursory analysis of cfd calculations which will be refined through further explorations in subsequent studies n o 2 is a stoichiometry coefficient normalized by the mass of the total products for oxygen in the combustion of wood in a simple wood burning model n f w o o d n o 2 o x y g e n p r o d u c t s h e a t this chemical equation involving a solid fuel and oxygen is a significant simplification compared to the multistage chain of reactions involving pyrolysis and many gas phase reactions and it should be noted that this approach assumes that the rate of combustion is mixing limited the rate of pyrolysis is implicitly assumed to be limited by the availability of heat from the reaction which is limited by the mixing processes bringing oxygen to the pyrolyzing fuel the value of n o 2 5448 and n f 4552 based on drysdale s 2011 stoichiometry for net combustion of wood the stoichiometry factor λ is adjusted to prevent maximum burning with fuel or oxygen rich scenarios linn 1997 λ ρ f ρ o ρ f n f ρ o n o 2 2 the parameter ψ is calculated based on the number of eps n e p that have been absorbed by dry fuel in the previous time step using the following relationship ψ n e p e e p t b u r n o u t ρ f v o l h w o o d c p w o o d t c r i t t a m b i e n t where ψ is constrained to be between 0 and 1 and e e p is the energy per unit time per ep which can be specified by the user the value of e e p can be thought of as degree of energy resolution with large values leading to fewer eps for the same reaction rate and thus less computational cost but the representation of the distribution of energy transferred to surroundings is reduced smaller values provide better finer grain representation of energy transport but add computational cost e e p is set to 50 kw as a balance between computational cost and representation of the energy transport in the simulations for this manuscript t b u r n o u t is the assumed time for fine fuel particles to burn out which is currently estimated to be 30 s though the sensitivity of the modeled behavior to this parameter will be explored in future work t c r i t and t a m b i e n t are the temperature k where fuel is assumed to combust and the ambient air respectively h w o o d is the heat of combustion of wood which is taken to be 18620 kj kg 1 drysdale 2011 and c p w o o d is the specific heat of wood which is taken to be 1 7 kj kg 1 k 1 the turbulent mixing parameter σ is computed in a customary manner for turbulent diffusion coefficients σ 0 09 s ρ g a s k where ρ g a s is the density of the ambient air while the length scale s and turbulent kinetic energy k are based on grid cell size and a smagorinsky style formulation following germano et al 1991 the reaction rate r explicitly describes the reduction in fuel within a computational cell kg m 3 s 1 per computational time step which is currently taken to be 1 s in the context of this text the total energy release rate kj m 3 s 1 q t o t a l can be formulated in terms of the heat of combustion of wood h w o o d q t o t a l r h w o o d because it takes energy to heat fuel from ambient temperature to a nominal temperature of combustion t c r i t we compute the net energy release from the fuel as q n e t r h w o o d c p w o o d t c r i t t a m b i e n t a fraction of q n e t is assumed to be lost upward to the distant atmosphere c r a d l o s s but to avoid a complex radiation calculation to determine the precise amount of energy lost to the sky versus absorbed by surroundings we estimated c r a d l o s s to be 0 2 20 of the net energy this preliminary percentage estimate was based on consideration of radiation view factors and cursory examination of cfd calculations further analysis of detailed physics calculations and experiments could be used estimate as a function of fire or environmental factors in the future the number of eps that are available to be transported to other locations is computed as n e p 1 c r a d l o s s q n e t e e p 2 5 transporting energy energy transfer to unburned fuel and thus spreading fire is accomplished by moving eps from a location where they are produced to another location energy from wildfires is transported to surroundings through a variety of processes that occur over different scales for the purposes of this simplified representation of the energy transport the heat transfer processes are grouped into two different categories which are referred to as 1 wind dominated and 2 creeping in the context of this model and this simplified categorization the wind dominated category is intended to capture the combined heating effects of convective bursts that blow transport heat in the direction of the local wind and radiative heating from flames that are also influenced by the local wind this category of energy movement can be associated with fires in surface or elevated shrubs or trees fuels and is assumed to be the dominant mechanisms for head fire spread it is true that radiative heating is not constrained to the direction of the wind but the radiative heating that is contrary to the wind direction is often not effective at spreading the fire since it is quickly offset by convective cooling the creeping category includes the much slower influences of heat transfer processes occurring locally within the surface fuel beds including those associated with contact between adjacent fuel particles and fine scale mixing that happens within the fuel bed these surface fire mechanisms are assumed in the model to be key factors in flanking and backing fire spread portions of a fire moving perpendicular and contrary to the direction of fastest fire spread and evaluated against high resolution observations of experimental burns in 2017 2018 at tall timbers research station unpublished data using methods similar to loudermilk et al 2014 these two categories of heat transfer process wind dominated and creeping can be thought of as associated with fireline scales on the order of the width of the fireline in the direction spread or fuelbed scales on the order of the depth of the surface fuel bed these notional scale ranges are not mutually exclusive and they depend on the fire environment as the high intensity or wind driven fires tend to have much wider or deeper firelines than low intensity prescribed fires however fireline scales can be significantly larger than fuelbed scales and thus can contribute to much larger rates of fire spread the cellular automata aspect of this model represents the net influences of the movement of energy by wind dominated or creeping processes through the transport of energy packets per unit time or eps ep s are simply a conceptualized quantity of energy that can move from one location to another per unit time this could be the energy associated with hot gases or flames or radiant energy each ep represents a fixed amount of energy that is moved within a unit of time e e p and they move through space but are not associated with specific volumes the notion of dilution or diffusion of energy as it moves away from its source is accomplished by decreasing the ep density per unit volume as distance from source increases since a typical power for an ep is 50 kw there are often many of eps being emitted in a given time step depending on the fire intensity at a given moment and location currently the transport rate or velocity of the ep while being transported is not computed and thus the eps move from one location to another the movement of eps associated with wind dominated processes is achieved by deriving a direction and distance for the travel of each ep based on phenomenology observed in laboratory and field experiments as well as much higher fidelity coupled fire atmosphere simulations for each ep within a computational cell a location for production is chosen randomly from within the zone that is taken to be actively burning within a computational cell described in sub grid zones active fire fuel depletion moisture depletion subsection below a possible length scale for the ep travel is estimated based on the observed flame length relationships described in nelson et al 2012 in order to use their relationship a local fire intensity is estimated by i n e p e e p a a c t i v e f i r e z o n e where a a c t i v e f i r e z o n e is the area of the sub grid zone that is actively burning the flame height is then estimated by h f l a m e h f u e l 0 0155 i 2 3 where h f u e l is the height of the surface fuels and is taken to be zero for fuels in cells above the lowest vertical level nelson et al 2012 also provided an expression for the fire updraft speed within a flame w which is not the same as a cell average vertical velocity w 0 377 i the flame updraft speed w is concentrated in the location where the ep is emitted and is not necessarily the same as the plume velocity described above as the plume is associated with the collective buoyancy averaged over a cell we used the flame height and fire updraft speed to estimate a length scale for the distance of heat transfer l in calculating l we assume that the minimum length scale occurs at a horizontal wind speed of 0 but is stretched as horizontal wind components increases l h f l a m e m a x i m u m 1 3 u 2 v 2 w 2 1 4 where u and v are the local horizontal wind components this expression is effectively the scaled square root of a convective froude number using this expression when horizonal winds exceed three times the flame induced vertical wind speed the length scale starts to get stretched both the coefficient 3 and the square root of the froude number elements of this expression are preliminary and based on phenomenological expectations but will be verified or refined through comparison with observations in future work this length scale is used to describe the upper limit of the potential distance that an ep could move the length scale is then adjusted based on whether it is more aligned with the horizontal or vertical winds and the magnitude of those winds l l 2 α h w π 2 α h u 2 v 2 π u 2 v 2 w 2 where α h is the angle of the ep trajectory with respect to the horizontal and is described below in this equation w is a vertical wind component that is adjusted if only part of the cell is known to be on fire factoring in for the concentration of an updraft the distance that an ep moves is taken randomly from a triangular distribution ranging from 0 to l 0 being the location of maximum probability thus the travel distance for an ep d becomes d l 1 1 r n d 1 where rnd1 as well as rnd2 rnd3 rnd4 rnd5 and rnd6 below are a random number between 0 and 1 which are chosen for each ep rnd1 through rnd4 are used at convective scales and rnd 5 and rnd6 are used at creeping scales described below the ep trajectory angle relative to the x axis in the horizontal plane α x is determined by adding a perturbation to the u and v wind components as illustrated in fig 2 the strength of the perturbation is determined using a gaussian distribution with a standard deviation of k 2 where k is the local turbulent kinetic energy α x t a n 1 u k 2 erf 2 r n d 2 1 v k 2 erf 2 r n d 3 1 use of a gaussian function to represent turbulent fluctuations within a forest canopy is an oversimplification as velocity distributions within forest canopies are known to be non gaussian and skewed shaw and seginer 1987 mueller et al 2014 error induced by this oversimplification could be further amplified by the presence of a fire as heilman et al 2017 revealed that fires can further enhance the skewness of the velocity distributions the gaussian function was chosen for simplicity and evaluation of non gaussian velocity distributions will be investigated in future work the angle from the horizontal α h is determined by first computing a metric ϕ describing the local dominance of the fire induced updraft over the horizontal wind ϕ w w u 2 v 2 ϕ approaches one when the updraft dominates over the horizontal wind and goes to zero when the horizontal wind is strong and the updraft is minimal this metric is used to set the median angle of the ep trajectory above the horizontal plane where the median ep trajectory is vertical when the updrafts dominate see fig 3 α h π 2 ϕ based on wind tunnel and field observations as well as high fidelity simulations the movement of heat from the active fire is heavily influenced by a tower and trough structure finney et al 2015 the tower and trough structure tends to bifurcate the projection of the movement of hot air on a vertical plane aligned with α x into two general paths one with a strong horizontal component and one with a strong vertical component the result is that heat advection from the fire often does not follow the median fireline wind trajectory but instead follows either a path below the average trajectory or a more vertical path above the average trajectory based on this phenomenology as well as unsuccessful explorations where energy followed the median trajectory or local instantaneous mean wind direction quic fire bifurcates the movement of heat along either of two paths based on the value of α h in this initial version of quic fire we have made the approximation that ratio of α h to vertical π 2 determines the probability that an ep will follow a vertical path both this linear probability dependence function on α h and the strictly vertical and horizontal trajectories will be the subject of follow up research through detailed analysis of laboratory and field observations and cfd based calculations but for this preliminary version we use α h π 2 i f 2 π α h r n d 4 1 a n d α h 0 i f 2 π α h r n d 4 1 the transport of eps to represent the creeping category of heat transfer process is meant to be only within the surface fuels and thus unlike the wind dominated process model the travel direction for creeping movement of eps is currently constrained to be in a horizontal plane and only in the computational cells closest to the ground the creeping travel distance and direction are estimated based on observations of some of the mechanisms by which flanking and backing fires spread 1 short duration turbulent fluctuations 2 particle to particle spread and 3 small scale reverse flows caused by turbulence driven by surface fuel roughness the net result of these three mechanisms is a slow spread in directions that are not aligned with the mean wind these are parameterized with the following expressions for the direction relative to the x coordinate axis α x c r e e p creeping length scale l c r e e p and a triangular distribution of transport distances d c r e e p relative to the creeping length scale α x c r e e p 2 π r n d 5 l c r e e p l c r e e p 2 d t 1 u c o s α x c r e e p v s i n α x c r e e p u 2 x 2 d c r e e p l c r e e p 1 r n d 6 the creeping length scale depends on a length scale parameter l c r e e p which is meant to be a function of the fuel bed geometry height packing ratio etc and turbulence intensity but is currently estimated to be 2 m based loosely on the structure of grasses initially simulated and length time scales of local wind fluctuations observed in backing fires we intend to fully resolve this parameter in future versions of quic fire the ratio in the parentheses is effectively the projection of the unit vector of the horizontal wind speed on the creeping direction and thus l c r e e p is a maximum of 2 m when the creeping direction is in the opposite direction of the wind with this formulation d c r e e p is chosen from a triangular distribution of the length scales ranging from 0 to l c r e e p with the highest probability being 0 2 6 sub grid zones active fire fuel depletion moisture depletion for quic fire to rapidly simulate fires on a landscape scale kilometers to tens of kilometers with minimal computational resources we expect grid cell dimensions to be on the order of a meter or larger this is not the minimum model resolution but at finer scales the model would be unable to explicitly resolve the details of fire perimeters and fire widths of low intensity fires since our goal was to produce a model useful for prescribed fire it was important to be able to adequately capture the behavior of low intensity fires where the fire activity or fuel conditions are heterogeneous within computational cells for this reason quic fire models three subgrid spatial zones within each computational cell the active fire zone fuel depletion zone and the moisture depletion zone fig 4 for this initial version of the quic fire model we described the zones in two horizontal dimensions x and y this simplification was made for computational ease because in our current application the vertical dimensions were often about half the size of the horizontal grid dimensions i e 2 2 1 m and in cells closest to the ground the fire and depletion activity were limited to the height of the fuels it is important to note that by not tracking the vertical extent of the active fire zone we were not adding the vertical cell dimension to our vertical energy transport as eps were still emitted from random vertical positions from within the fuel bed we will explore the implications of a three dimensional zone description in future versions these zones active fire fuel depletion and moisture depletion shaded red brown and grey in fig 4 were described with a zone center a x a y f x f y d x d y and a standard deviation in the x and y direction indicated with σ in fig 4 for simplification purposes we assumed that within each of the three zones the associated properties of the fuels and fire activity were homogeneous in other words fuels and fuel moisture were homogenously distributed within their respective depletion zones we also assumed that no fuel or moisture was depleted outside of their respective zones fire activity was treated as uniform within the active fire zone and thus eps were assumed to be emitted from random locations within this zone these three zones are not mutually exclusive and in fact typically overlap as illustrated in fig 4 for example when the fire ep lands in a cell it will deplete some moisture when a second ep lands in the cell if it does not land in the same location a new moisture depletion center and spatial standard deviation can be calculated based on the accumulating ep destination locations thus creating a rectangular moisture depletion zone as subsequent eps are absorbed by the fuel in the cell where there is remaining moisture the moisture depletion center is again adjusted along with the standard deviations all the while tracking the total moisture that is evaporated from within the fuel depletion zone alternatively if an ep is absorbed within the previously mapped out moisture depletion zone one of two cases occurs 1 more moisture is evaporated and the new destination point is used to update the moisture depletion zone center and standard deviations or 2 the moisture is completely depleted within this zone in which case the moisture depletion zone is left unchanged but the energy of the ep is used to consume some fuel and contribute to the development of the active fire zone if eps continue to land within the moisture depletion zone eventually the moisture will be depleted within this zone and subsequent eps can initiate active fire this methodology would allow eps being lofted short distances compared to the width of a cell into a cell for example from the right to develop a dry zone along the cells right side eventually it will be possible to develop an active fire zone that is entirely inside the moisture depletion zone once the active fire zone is established ep s are emitted from this zone which has the potential to send eps beyond the current moisture depletion zone further expanding it to the left as solid fuel inside the active fire zone is consumed a fuel depletion zone is established creating a region where some but not all of the fuel has been consumed the center and extent of the moisture and fuel depletion zones were calculated based on simple running statistics based on where the moisture and fuel depleting eps land during the history of activity in a cell the active burning region is more complex as fire activity is transient at any specific fire location the center of the active fire zone is thus weighted most heavily by the locations where the heat release rate is greatest when a new ignition location is established its heat release rate is approximated with an initial maximum followed by a linear decline with time over t b u r n o u t the implications for this subgrid zone approach have yet to be fully explored but they likely depend on the size of the cell in comparison to the energy transport distances or the depth of the fireline for scenarios where the values of l are on the same order or larger than the cell size the zones can rapidly fill the entire cell as absorbed eps are distributed widely across the cell in this case the value modeling the three zones is minimal but in scenarios where l is small compared to the cell size the zones will have more importance the assumption of homogeneity within respective zones is the subject of ongoing discussions as it is certainly not applicable to all fuel beds by using a more complicated distribution within each zone we would be able to represent the gradients within each zone but at this point we believe that even with a top hat distribution of depletions and fire activity within their zones the subgrid heterogeneity provided by the zones themselves in the zone or out of a zone helps capture the impacts of some of the subgrid variability 2 7 fate of ep in quic fire when an ep reaches its destination location it evaporates moisture ignites fuel in the new location or heats the atmosphere the fate of each ep is determined by the state of the destination fuel a series of checks are performed regarding the probability of the presence of fuel and moisture level of the fuel fuel densities and surface area per unit volume are used to determine the probability of an ep landing on viable fuel using the following equation p v i a b l e f u e l 1 e ρ f l o c a l c i g s c a v u h 2 w 2 a v n o r m u h 2 ρ f l o c a l ρ f 0 in this equation a v is the surface area per unit volume of the local fuel and a v n o r m is a parameter used to normalize surface area per unit volume such that a v a v n o r m is unitless a v n o r m is currently chosen to be 4000 m 1 which is a typical value for grass ρ f l o c a l is the local bulk fuel density where the ep lands and ρ f 0 is the bulk density of the fuel in that computational cell at the beginning of the simulation u h is horizontal wind velocity and w is the local vertical velocity interpolated to cell centers c i g s c kg m 3 is a constant currently set to 6 kg m 3 that represents a theoretical density where the fuels are thick enough that an ep traveling 1 m through a cell with a v a v n o r m will a have 63 chance of being absorbed by fuel and not going to the atmosphere this use of this constant can be thought of as the influence of the fuel density on the optical path length and on the spatial gradient of heat absorption into the fuel as hot gases blow through cooler fuel both of which take on exponential forms under some circumstances the value of c i g s c which is currently set to 6 kg m 3 does not currently have a firm basis yet but could be evaluated as part of future refinements the portion of the equation in square brackets describes the effects local wind and fuel characteristics on the probability of the ep finding viable fuel if we assume that the fuels remain evenly distributed this term describes something like the effects of fuel on mean free path of an ep the factor on the right of the expression accounts for the increased spatial heterogeneity and gaps between fuel elements as the fuel gets depleted if the ep lands within the fuel depletion zone then the locally depleted fuel density of the zone fuel depletion concentrated in fuel depletion zone is used as ρ f l o c a l as fuel is depleted in the depletion zone its receptivity to new eps approaches zero if an ep does not get absorbed by the fuel then its energy is released to the atmosphere and goes on to generate a buoyant plume if it is absorbed by fuel then it can either heat ignite some of the fuel or heat and evaporate water if the fuels at the destination are wet if the ambient fuel bed is not completely dry 0 moisture at the beginning of the simulation the ep destination location will either fall within a moisture depletion zone or not if the ep was absorbed by fuel outside the current moisture depletion zone its energy will be used to heat a fraction of the moisture to vaporization additionally the location of the ep within the cell was used to grow the moisture depletion zone of the cell if an ep was absorbed by fuel inside the current moisture depletion zone then its energy will be used to evaporate any water remaining within the zone if there was no moisture within the zone then the energy of the ep was used to ignite fuel and this was considered a successful ignition 3 simulations to demonstrate model performance we use two case study comparisons between firetec and quic fire simulations the goal of these comparisons is to assess if the reduced order representations of processes and fire atmospheric feedbacks in quic fire at cell levels aggregate to produce overall fire behavior and response to fire conditions that are like what a full cfd fire atmosphere model provides one advantage to comparing quic fire with firetec is that they rely on nearly identical 3d representations of vegetation in the fuelbed allowing for direct comparison of outputs to the cfd based firetec model 3 1 case study 1 long line vs short line ignition linn and cunningham 2005 hereafter lc2005 conducted a series of simulations using higrad firetec to investigate how fire behavior in grasslands was influenced by the initial length of the fireline and the ambient winds initial line length was found to influence both the rate of spread and fire shape due to differences in the coupled fire atmosphere interactions overall the head fire spread rates of lc2005 were found to be in good agreement with both field experiments cheney and gould 1995 cheney et al 1993 and empirical model estimates cheney et al 1998 the simulations of lc2005 provide an ideal test case for initial evaluation of whether the simplified representations of local fire phenomena in quic fire combine to capture reasonable overall fire atmosphere feedbacks and thus fire behavior as they provide a very simple fire environment that highlights the role of fire atmosphere coupling for the simulations of lc2005 a domain of uniform fuels with no topographic slope was used the fuels represented tall grass with a height of 0 7 m and an aerial distribution of 0 7 kg m 2 the computational domain was discretized into a uniform horizontal grid of 400 m 400 m with 2 m horizontal resolution and a stretched vertical grid with the near ground spacing starting at 1 m and stretching to 25 m spacing at a height of 150 m the initial fuel moisture fraction i e the mass of water divided by the mass of fuel was specified as 0 05 in the quic fire simulation ambient winds are prescribed to be in the positive x direction and are initialized with a log profile that assumes neutral atmospheric stability for this study we consider wind speeds of 1 3 6 and 12 m s 1 as measured at a height of 10 m in lc2005 the initial and inflow profiles are of the same speeds but a uniform profile was assumed and vegetative drag could develop the near surface shear layer to account for potential differences between our log profile and the profile of lc2005 we used mid flame wind speeds reported in lc2005 and adjust these to a reference height of 10 m using a log profile using a roughness length of 0 1 m for tall grass we consider ignition line lengths of 16 and 100 m hereafter referred to as short and long lines respectively for quic fire ignition was accomplished by removing any moisture and adding energy to the grid cells designated for ignition for these cases the amount of energy added was 2 eps in lc2005 ignition was accomplished by removing any initial fuel moisture from the ignition cells and increasing the temperature from 300 k to 1000 k over a span of 2 s difference in ignition method should only impact very early stages of fire spread and thus we focus comparisons on times after the first 60 s of spread 3 2 case study 2 prescribed fire multiline ignition to demonstrate quic fire s ability to work with three dimensional fuels and complex ignition patterns we simulated a typical burn pattern for a prescribed fire at eglin air force base eafb ignited by 5 strip head fire ignition lines moving perpendicular to the wind direction the rates of ignition across the unit reflect eafb use of all terrain vehicles atvs to ignite units of this size the 3d fuels were parameterized for fire maintained longleaf pine pinus palustris turkey oak quercus laevis sandhill ecosystems using fire effects monitoring data hiers et al 2007 and supplemented by fuel data from the prescribed fire combustion and atmospheric dynamics experiment rxcadre ottmar et al 2016 we analyze each simulation for fuel consumption in different forest strata which are common foundational data for prescribed fire objectives the burn unit modeled for this comparison was 40 ha with full model domain extending 500 m 1200 m in planer dimensions and 614 m in height with 2 m horizontal resolution and 1 m vertical resolution we define surface fuels as those fuels in the lower most model layer height 1 5 m midstory fuels occupy layers 2 5 height 1 5 3 m and canopy fuels as any fuels above layer 3 height 3 m this simulation was compared with firetec using the same fuel inputs ignition pattern rate of ignition and mean input windspeed of 5 m s 1 4 results 4 1 comparisons to grass fires fire perimeters as a function of time for wind speeds of 1 3 6 and 12 m s 1 are shown in figs 5 and 6 for the short and long line cases respectively for the short ignition line and 1 m s 1 winds upper left panel of fig 2 the pattern of fire spread is predominantly circular and as wind speeds increase the fire perimeters become elongated and focused to a sharp point downwind of the ignition the longer ignition lines in fig 6 lack the circular pattern at low speeds and have a more lobed pattern due to increased competition for indrafts to feed the larger number of updrafts required for the longer line fig 7 shows the downwind propagation of the fire front with time for the short line cases for the quic fire simulations spread rates are nearly constant for each wind speed after a period of adjustment during the first 60 s as the initial ignition line builds into a sustainable fire front the impact of this adjustment period on rate of spread was evident when estimated via a linear fit as performed in lc2005 table 1 limiting the data in the linear fit to the period after the 60 s adjustment period improves the agreement with lc2005 spread rates for the 1 and 12 m s 1 cases the long line cases also exhibit near constant spread rates outside of the ignition influenced period fig 8 as for the short line cases the quic fire simulation spread rates are slower than those of lc2005 table 2 adjusting the log wind profile used in quic fire such that the near surface winds more closely match the winds in the lowest model level of lc2005 greatly reduces the differences in spread rates for the higher wind speed cases the adjusted 10 m wind speeds are 1 1 3 5 7 6 and 14 5 m s 1 while adjusting the wind profile improves the agreement between quic fire and lc2005 other sources of potential disagreement such as differences in turbulence levels are not as easily adjusted overall the quic fire results are consistent with those of lc2005 in that longer ignition lines result in faster rates of spread than shorter lines for a given wind speed as noted above the results of lc2005 are generally consistent with the field experiments described by cheney et al 1993 and cheney and gould 1995 the one exception to this consistency was lc2005 finding the influence of line length being stronger for lower winds where cheney and gould 1995 indicate that the effect was greatest at high winds low wind speeds are where quic fire and lc2005 differ the greatest with quic fire response to line length being weaker at low winds than lc2005 table 3 this result however shows quic fire simulations to be more consistent with the findings of cheney and gould 1995 lateral spread was significantly higher in quic fire than lc2005 while lc2005 yielded length to breadth ratios lower than those of alexander 1985 discrepancies were potentially attributable to an overly simplified treatment of radiation or turbulent heat exchange or under resolution of the fuel bed unlike lc2005 quic fire yielded length to breadth ratios larger than those of alexander 1985 the low lateral spread rates from quic fire could be caused by the absence of natural variability in the input wind field which was artificially constrained in these comparisons alternatively the low lateral spread rates from quic fire could also be a manifestation of our assumed gaussian and isotropic velocity distributions as alexander 1985 does not provide variability statistics on winds it was difficult to replicate subtle but potentially important variation driving lateral spread 4 2 prescribed fire multiline ignition simulation of an operational prescribed fire using multiple ignition lines shows good qualitative agreement between quic fire and firetec fig 9 five firelines were ignited in the simulation moving from right to left in fig 9 and the ambient wind of 5 m s 1 was blowing from the image foreground towards the background after 120 s of simulation time top 2 images in fig 9 the surface fire was beginning to ignite the midstory as ignition lines converge to create strong convective updrafts by 300 s bottom 2 images in fig 9 the fire was actively consuming fuel within all three fuel strata by 420 s not shown consumption rates decline as fire from early in the simulation entered a smoldering phase and the left flank continues flaming combustion within the last zone of merged firelines overall in the quic fire simulation 98 6 of surface fuels were consumed along with 33 6 of the canopy here surface fuels are those fuels within the first vertical level of the model and canopy fuels are those fuels above the fifth vertical level these consumption values are considered typical for aggressive ignition patterns like strip head fire that target midstory consumption and often at the expense of some canopy consumption ottmar et al 2016 comparison of fuel consumption time series with the firetec simulations reveal similar trends fig 10 firetec ramps up to a constant rate of consumption for surface fuels more quickly than quic fire but the rates of consumption are similar and the overall difference in the percentage of fuel consumed is only 3 2 for canopy consumption firetec is again quicker to initiate canopy consumption but the rates and total consumption are quite similar for quic fire differing by 2 6 in total canopy consumed 5 discussion the new coupled fire fuel atmosphere model quic fire shows great promise as an emerging planning tool for complex ignitions as these two test case studies illustrate quic fire is capable of faster than real time simulations of complex fire atmospheric feedbacks resulting from variation in ignition patterns and produced results like the cfd solution for these scenarios at 1 2000 the computational cost due to nearly identical fuel inputs quic fire performance can be analyzed against the numerical cfd model higrad firetec under a range of wind speed conditions in these two ignition line length scenarios quic fire showed similar trends to the firetec spread rates while its performance diverged at lower windspeeds its results showed closer agreement than firetec to empirical data on which the lc2005 simulation was built cheney et al 1993 this realistic representation of the interaction of wind speed length of ignition line and fire spread was a critical test of the ability of quic fire to capture basic phenomena that emerge from coupled fire atmospheric processes the comparison of quic fire to firetec using a complex ignition pattern common for prescribed fire operations in the southeastern united states further illustrates potential utility in capturing altered flows resulting from both the 3d fuel structure and the fire induced buoyant plume in planning prescribed fires a manager routinely relies upon experience to distill what combination of ranges of wind speeds fuel moistures and ignitions patterns yield fire intensities that are suitable for achieving desirable ecological outcomes models commonly used in prescription development nwcg 2017 cannot account for the multiple interacting firelines associated with most prescribed fire ignitions as they assume free burning fire yedinak et al 2018 interacting firelines on prescribed fires drive areas of enhanced convection that migrate progressively across the burn unit forming complex patterns as firelines merge and burn out this simulation shows the promise of quic fire to rapidly account for how winds fuel conditions and ignition pattern interact to develop a complex mosaic of fire interactions across a burn unit for models to be useful in evaluating prescribed fire ignition scenarios the impacts of the fire atmosphere interaction must be effectively resolved in scenario 2 the convection convergence driven by interacting firelines was similarly represented by both quic fire and firetec put simply as heated air parcels rise above the fires other air parcels were drawn towards the rising air to fill the volume vacated by the rising air as intensity increased greater volumes of air were pulled in under the rising plume nearby fires pulled on one another with the net effect that different firelines were drawn together in addition it should be noted that the fire generated flow is not aligned with the mean ambient wind flow this phenomenon must be adequately described in terms of the underlying fire dynamics for a model to be useful in situations where complex patterns of multiple firelines exist such as most prescribed fires in addition to being able to capture the impact of heterogeneous and dynamically placed ignitions quic fire accounted for influences of fuel structure and ambient winds that change during the course a fire despite the recent focus of fire science on the importance of convective heat transfer canfield et al 2014 finney et al 2015 only cfd representations could reliably capture buoyancy driven fire atmospheric feedbacks linn and cunningham 2005 mell et al 2009 parsons et al 2011 yet such tools remain inaccessible due to intense computational demands and are largely applied in research contexts cruz and alexander 2012 mell et al 2018 sullivan 2009a thus managers continually rely on radiation driven semi empirical fire spread models for operational planning andrews 1986 finney 1998 tymstra et al 2010 that were originally designed to predict forward fire spread and employed in evaluating firefighter safety unfortunately these types of operational models are inappropriate for prescribed fire scenarios where the importance of fire atmosphere interaction is amplified both prescribed fire ignition patterns and wildfire tactical ignitions are determined primarily by highly variable wind fields and fuel characteristics e g moisture content and loading all of which change in both space and time heilman et al 2015 it is also common for prescribed fires to be performed in the context of high structural variability in vegetation that occurs across scales furman 2018 for example both fine scale fuel breaks and adjacent large burned areas can alter patterns of air flow and can facilitate enhanced inflows that have a large impact on fire behavior linn et al 2012 alternatively planned burned units are often bordered by denser vegetation which is more resistant to ambient wind flows and alters patterns of fire spread lashley et al 2014 the combined effects of vegetation structural influences on wind will influence fire behavior in ways that may help or hinder in meeting burn objectives therefore a useful model must be able to account for the effects of vegetation on both fuels and wind flows while quic fire does not include a detailed treatment of the turbulent processes associated with vegetation or fire it has compared favorably to the more physically detailed firetec model and thus shows potential for being a useful tool for fire managers quic fire represents a critical innovation in efforts to create a prescribed fire modeling tool that could be widely applied by fire managers including the essential coupled fire atmospheric interaction without large computational demands quic fire provides the ability to manipulate fuels winds and ignition patterns and then compare predicted fire behavior among those scenarios the computational efficiency allows for ensemble runs to explore for example fire behavior predictions resulting from variations in wind speed gustiness or fuel moisture the reliance on 3d fuels already available for firetec is a critical strength of this tool as quic fire predictions can be tested against a cfd model solution to examine the underlying physics when predictions do not match expectations 6 conclusions here we describe the conceptual basis basic formulation and initial demonstration of a new fast running modeling tool quic fire that can be applied to prescribed fire planning quic fire is the integration of a phenomenologically based fire spread model with the fast running wind solver quic urb and provides a self determining fire prediction capability that represents the critical coupled fire atmosphere feedbacks at scales relevant for prescribed fire although the development of this model is in the nascent stages of development initial results show an encouraging capability to capture basic trends in fire behavior response of fire spread to size of fire and consumption of canopy fuels in prescribed fire scenarios based on the results of this initial assessment the combination of the quic urb wind solver with a phenomenologically based cellular automata approach to modeling the fire spread shows promising potential and warrants continued development quic fire is currently being used for simulations of fire on relatively flat terrains and most prescribed fires in the us occur under minimal topographic relief melvin 2015 however current efforts to adapt this coupled fire atmospheric modeling framework for topography are expected to extend quic fire s application to prescribed fire in complex terrain quic fire s ability to represent fire atmospheric feedbacks such as those that govern prescribed fire behavior at relevant scales while running on a basic laptop is intended to begin providing a capability for a broader set of users to explore prescribed fire behavior its ability to model response to both ignition patterns and a temporally and spatially variable fire environment without computational expense of cfd solutions is a critical design feature advancing this initial effort however will require continued refinement and validation against observations for example the representation of near fire turbulence as influenced by ambient winds vegetation and the fire itself is a focus of model advancement efforts ambient and fire influenced as the limitations of the current smagorinsky style approach could have implications for the model s representation of fires in some circumstances additionally efforts to refine representations of heat transfer length scales directions and efficiency representation of backing and flanking fire spread phenomenology as well as vegetation drag and fire atmosphere feedbacks will continue declaration of competing interest authors declare no conflicts of interest aknowledgements funding was provided in part by the department of defense through the strategic environmental research and development program project rc 2643 and environmental security technology certification program project rc 201303 the los alamos national laboratory ldrd program the usda forest service southern research station usda forest service rocky mountain research station and usda forest service r d washington office through the national fire plan and tall timbers research station we are appreciative of the efforts of james furman brett williams and participants in the prescribed fire science consortium for their support and encouragement in developing this tool appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104616 
