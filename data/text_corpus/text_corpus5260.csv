index,text
26300,air temperature is a significant meteorological variable that affects social activities and economic sectors in this paper a non parametric and a parametric approach are used to forecast hourly air temperature up to 24 h in advance the former is a regression model in the functional data analysis framework the nonlinear regression operator is estimated using a kernel function the smoothing parameter is obtained by a cross validation procedure and used for the selection of the optimal number of closest curves the other method applied is a seasonal autoregressive moving average sarma model the order of which is determined by the bayesian information criterion the obtained forecasts are combined using weights calculated based on the forecast errors the results show that sarma has a better performance for the first 6 forecasted hours after which the non parametric functional data analysis npfda model provides superior results forecast pooling improves the accuracy of the forecasts keywords functional data analysis sarma time series air temperature forecasting 1 introduction accurate modelling and forecasting of hydrometeorological variables is important since weather can affect different economic and social activities weather forecasts provide useful knowledge that can prevent damages life and property losses by predicting extreme events and increase the efficiency of operations al matarneh et al 2014 air temperature is crucial for the agricultural sector since it affects the crop growth e g webber et al 2016 and the soil temperature and moisture high temperatures result in a faster development of the crops that leads to lower yield potential hatfield and prueger 2015 tourism is also highly influenced by temperature and precipitation de freitas 2003 electricity load is considerably driven by air temperature especially in arid regions such as uae where high temperatures are the norm however the response of electricity demand to changes in temperature is not uniform across the different sectors and taking into account this discrepancy can result in improved long term forecasts of electricity demand and energy planning moral carcedo and pérez garcía 2015 the predictions of weather related variables are also widely used for speculation in the rapidly growing weather derivatives markets campbell and diebold 2005 temperature is frequently used as an explanatory variable for forecasting other variables and processes such as streamflow sene 2009 electricity loading liu et al 2014 river water temperature st hilaire et al 2012 cheng and wiley 2016 surface energy fluxes woolway et al 2015 or solar radiation shamshirband et al 2015 mohammadi et al 2016 hence accurate prediction of temperature can provide higher forecasting accuracy of other variables in the literature the two basic techniques used to forecast climate variables are physical models based on numerical weather prediction e g lei et al 2009 and statistical methods which is the focus of this study among various statistical approaches implemented by researchers are time series models e g modarres and ouarda 2013 introduced by box and jenkins 1976 machine learning models such as artificial neural networks e g ouarda and shu 2009 support vector machine methods e g bautu and barbulescu 2013 and markov chains e g d amico et al 2012 the literature regarding temperature modelling and forecasting using box jenkins methods is considerable modarres and ouarda 2014 applied a multivariate generalized autoregressive conditional heteroscedasticity mgarch approach to model the second order moment variance for temperature downscaling in choon and chuin 2008 ann was found to provide more accurate monthly temperature forecasts compared to a sarima model air temperature was used as an explanatory variable to model stream water temperature in caissie et al 1998 and box jenkins model was found to have similar performance with a multiple regression model and a second order markov process temperature was used as an exogenous variable in an arima in order to model peak ozone concentrations in taiwan liu 2007 tabari et al 2015 used air temperature forecasts obtained by artificial neural networks ann to predict soil temperature seasonal long memory models have been commonly adopted for the modelling of climatic hydrological and environmental time series see for instance ooms and franses 2001 and reisen et al 2014 grimaldi 2004 showed that fractional arma farma models are more appropriate for time series that exhibit long term memory compared to the standard short memory models hybrid approaches have also been extensively used for air temperature forecasting for example deihimi et al 2013 combined wavelets transform with echo state networks wesn to predict temperature and used the forecasts to predict electricity load according to some other studies the combination of neural networks with wavelet methods sharma and agarwal 2012 eynard et al 2011 provides improved results except for the combinations between statistical approaches stochastic models are also used for postprocessing of the forecast ensembles obtained from deterministic numerical models möller and gross 2015 baran et al 2013 functional data analysis fda is a framework of increasing interest in the scientific community since being introduced by ramsay 1982 various classical statistical tools have been developed and extended to the context of fda ramsay and silverman 2005 ferraty and vieu 2006 dabo niang and ferraty 2008 it has been applied in many different fields such as chemometrics burba et al 2009 medicine ratcliffe et al 2002 and genomics krämer et al 2008 but the applications in hydrometeorology are limited chebana et al 2012 used fda to perform frequency analysis on flood hydrographs and ternynck et al 2016 for their classification el nino data series were used for quantile estimate by ferraty et al 2011 and for year ahead forecasting by besse et al 2000 meiring 2007 studied the patterns of the ozone variations depending on the altitude masselot et al 2016 applied linear functional regression models to forecast streamflow in this study we use a nonparametric functional data analysis approach to forecast air temperature time series nonparametric methods have the advantages of being free of assumptions about the distribution of the variables and the linearity of their relationship e g st hilaire et al 2012 this paper is organized as follows the npfda and the box jenkins models and the evaluation methodology are introduced in section 2 in section 3 the data used in the study is presented the results are discussed in sections 4 and 5 and section 6 concludes the study 2 methodology this section presents the methods used in this study initially data smoothing as a preliminary step for the npfda model is introduced and the regression model used in the fda framework is described then the sarma model is discussed and the forecasts combination technique is presented finally the evaluation procedure is explained 2 1 data smoothing by definition a random functional variable x takes its values in an infinite dimensional space a time series consisting of discrete measurements cannot describe data of such nature therefore it must be initially approximated by functions as a preparation step of the data to be used in the fda context a function x t can be constructed using the basis function expansion 1 x t k 1 k c k ϕ k t where t ω ϕ k k 1 k are basis functions linearly combined and c 1 c k are the coefficients of the expansion the most commonly used bases are fourier for periodic data see chebana et al 2012 for application in hydrology and splines for non periodic data e g ternynck et al 2016 there are also other basis systems that can be used such as wavelets e g giacofci et al 2013 or exponential e g mossaiby et al 2016 the optimal number of basis functions ϕ k and coefficients c k are estimated using a penalized residual sum of squares criterion that trades off between the roughness of the curve and the data underfitting and is given by 2 p e n n s e λ j y j x t j 2 λ d 2 x t 2 d t where d 2 x t is the second derivative of a function x t y j are the observations and λ is the smoothing parameter that gives weight to the second term which penalizes the roughness of the curve detailed explanation and discussion on the smoothing methods can be found in ramsay and silverman 2005 the periodicity of the air temperature series used in this study could justify the choice of fourier basis however the use of b splines provides slightly better forecast results and is preferred the order of the b splines is four by default and is equivalent to cubic polynomial since the degree of the polynomial is one less than its order b splines with two equispaced interior knots are considered to best capture the diurnal variation of the temperature 2 2 non parametric fda regression model univariate time series analysis consists in modelling the behavior of the past in order to predict future values of the same series a starting point is to determine the amount of information that will be taken into consideration for prediction i e what is the explanatory variable x that will be used to forecast a response y in the fda context a continuous section of the process is considered as a predictor variable assume a time series z t t ℝ observed at some equispaced points n n τ for some n ℕ and τ 0 considering the observations denoted by z 1 z 2 z n the objective is to predict future values z n s for some s 0 forecast horizon in order to construct a set of functional data the original series is initially divided into n 1 segments of length τ such that 3 x i z t i 1 τ t i τ for i 1 n 1 these functional observations correspond to the explanatory variables with responses that can be either scalar or of functional nature and are defined as 4 y i z i τ s here the scalar responses are estimated using a regression model 5 y i r x i ε i where ε i is the error term satisfying e ε i x i 0 i and r is a nonlinear regression function for the regression function r the following functional kernel estimator is considered ferraty and vieu 2006 6 r ˆ x i 1 n y i k h 1 d x x i i 1 n k h 1 d x x i where k is a local weighting kernel function each scalar response y i is attributed a weight according to the distance d between each functional random variable x i and a fixed functional variable x in other words for the x i that are closer to x the respective y i are assigned higher weights although many different types of kernel functions can be used e g box triangle quadratic gaussian the choice of the kernel has little impact on the quality of the estimator bosq 1998 in this study the positive asymmetrical quadratic kernel epanechnikov is implemented 7 k u 3 4 1 u 2 1 0 1 u the calculation of the distances between functional objects is a critical issue since they are defined in an infinite dimensional space and classical approaches such as euclidean norms are no longer appropriate as discussed in chapter 3 of ferraty and vieu 2006 there are considerable advantages in using semi metrics as proximity measures rather than metrics or norms one can select between various families of semi metrics depending mainly on the roughness of the functional observations and the purpose of the analysis the curves used in this study are relatively smooth hence semi metrics based on derivatives are applied 8 d q d e r i v x x i 2 x q t x i q t 2 d t where x q is the q t h derivative of x the stability issues that arise from the calculation of consecutive derivatives are solved by using b splines for curve fitting de boor 1978 schumaker 1982 in this study such problems are not encountered since the classical l2 norm was used as a proximity measure by setting the number of derivatives to zero the smoothing parameter bandwidth h represents the threshold for which the kernel k assigns weights to each y i for every calculated semi metric d x x i i e the estimator r ˆ x considers only the y i s for which the proximity between the respective x and x i is not larger than h the bandwidth can be substituted by an optimal number of closest curves using the k nearest neighbors method and obtained by a cross validation procedure ferraty and vieu 2006 9 k o p t arg min k g c v k where 10 g c v k i 1 n 1 y i r i k n n x i 2 with 11 r i k n n x j 1 j i n 1 y j k d x j x h k x j 1 j i n 1 k d x j x h k x 2 3 sarma model the second approach used in this study is a seasonal autoregressive integrated moving average sarima model also known as box jenkins 1976 model box jenkins approaches are probably the most popular methods for time series forecasting and have been extensively applied to model and predict temperatures among other hydro meteorological variables a major difference compared to the fda regression model is that sarma belongs to a different class of statistical procedures since it is a parametric approach the general form of a sarima p d q p d q s is denoted as 12 φ b s ϕ b 1 b d 1 b s d y t θ b s θ b a t where y t is the original data a t is the innovation term ϕ b is the non seasonal autoregressive ar polynomial φ b s is the seasonal autoregressive sar polynomial θ b is the non seasonal moving average ma polynomial θ b s is the seasonal moving average sma polynomial s is the seasonality term d is the order of non seasonal differencing d is the order of seasonal differencing and b is the backward shift operator due to the diurnal variation of the temperature the series is differenced at lag d 24 the annual seasonality is not considered since a subset of the data is used for training and the forecast horizon of 24 h is unlikely to be influenced by it the statistical significance of the seasonal ar and ma parameters is tested by calculating the t values the non seasonal ar and ma parameters are assigned values from 1 to 5 in all the possible combinations and the goodness of the models is evaluated by computing the bayesian information criterion aic calculated as 13 b i c m n l n σ a 2 m l n n where m is the number of ar and ma parameters to estimate σ a 2 is the variance of the residuals and n the number of observations the model that provides the lowest bic value is chosen as a parsimonious one 2 4 combination of forecasts as a result of the advances in time series analysis and growing computing capability several predictions for the same variable can be obtained forecasts pooling was initially introduced by bates and granger 1969 and has been widely applied since then e g marcellino 2004 according to various studies the combination of forecasts from different sources can provide better estimates than any single method johansson et al 2015 numerous pooling techniques can be implemented such as simple averaging linear combinations using weights or median forecast selection in the literature there is no general rule or diagnostic test that would indicate which pooling method is more appropriate the combination does not always provide improved forecasts and the results depend on the number and types of used models the forecast horizon and the type of the time series based on the promising results obtained in marcellino 2004 for linear combinations of forecasts from linear and non linear models the forecasts are combined linearly in this study 14 y ˆ t l m 1 m k m l t y ˆ t l m where l is the forecast horizon m denotes the models and k m l t are the weights calculated using the inverse mean square forecast error msfe ratio defined as 15 k m l t 1 m s f e m l t w j 1 m 1 m s f e j l t w such that m 1 m k m l t 1 for w 0 each forecast is assigned equal weight and for w 1 the weight of each forecast is inversely proportional to its error we consider w 5 in order to assign higher weights to the model that provides better results 2 5 evaluation procedure a jackknife evaluation procedure miller 1964 shao and tu 1995 is applied to assess the performance of the models the main strengths of the jackknife is that the models accuracy is independent of the calibration data and the loss in the sample data is minimal mccuen 2005 since jackknife is an iterative process and the number of hourly observations is 254 208 a subset of the entire series is used for training it consists of a moving window of 20 days 10 days before and 10 days after the day used for validation extended for all the years for example when forecasting the 15th of january 12 a m 11 p m the training dataset includes observations from 5th to 25th of january from 1982 to 2010 leaving out the 15th of january of each year the models are calibrated with the remaining data and then we forecast 1 24 h ahead at the next iteration the process starts with the next hour e g 15th of january 1 a m 12 a m and is repeated until the whole year is covered at each iteration a set of five indices is used to assess the performance of the proposed models for every forecast horizon specifically the indices are the mean squared error mse the root mean squared error rmse the relative root mean squared error rmser the mean bias bias and the relative mean bias biasr they are calculated using the following equations 16 m s e i 1 n y i y ˆ i 2 n 17 r m s e 1 n i 1 n y i y ˆ i 2 18 r m s e r 1 n i 1 n y i y ˆ i y i 2 19 b i a s 1 n i 1 n y i y ˆ i 20 b i a s r 1 n i 1 n y i y ˆ i y i where generically y i is the observed value and y ˆ i the estimated or forecasted value 3 case study the approaches described in the previous section are applied to the data presented in this section the time series used in this study consists of hourly measurements of air temperature oc and covers a period of 29 years from 1982 to 2010 fig 1 for the analysis in the fda framework the series are divided into segments of length 24 h thus every functional curve corresponds to the daily temperature fig 2 the data was collected from the meteorological station located at the abu dhabi international airport under the patronage of the national center of meteorology and seismology ncms of the uae it is situated at a height of 27 m at latitude 24 26 and longitude 54 39 the geographical location of the station is shown in fig 3 due to its location uae receives high amount of radiation during the summer and is one of the hottest regions in the world as can be observed from table 1 august is the month with the highest average temperature of 35 2 c and the highest maximum 48 8 c and minimum 24 c temperature the hottest period is from may until september during which the mean and the median temperatures are above 30 c the maximum temperature is higher than 46 c and may 17 c is the only month of the hot period that exhibits minimum temperature below 20 c during the rest of the season the range of the mean temperature is between 18 5 c january and 29 c october the cool period is during the winter months when the lowest minimum temperatures are observed december 7 9 c january 6 3 c and february 6 6 c as well as the lowest maximum temperature december 33 c january 33 4 c and february 37 7 c the highest variability of the temperature is observed from february until may during this period the range of the data is above 30 c and variance standard deviation and skewness are higher compared to the other months except for june that exhibits a higher variance than february but a smaller range the literature concerning studies of temperature and other climatic variables in the middle east me region is limited but has been increasing during the last years one of them was conducted by chandran et al 2015 who studied the relationship between climate oscillations and temperature precipitation in the uae and found significant correlation between the north atlantic oscillation index nao and temperature naizghi and ouarda 2017 carried out a similar study for wind speed in the uae basha et al 2015 forecasted temperatures in the uae by projecting non stationary oscillation nso processes obtained from ensemble empirical mode decomposition eemd results indicate a significant increase in the temperature in abu dhabi over the next 30 years 4 results this section initially presents the results of configuration of the srama model and then the results of applying the two proposed models are discussed 4 1 specification of the sarma model during the process of estimating the parameters of the model unlike seasonal ar parameters sma parameters are found to be statistically significant t 1 96 and are included in the model the bic values for the tested models are presented in table 2 the minimum bic is achieved by the model with five ar and two ma parameters a non seasonal differencing at lag d 1 results in higher aic values providing higher variance of the residuals for the same number of parameters and thus is not considered after the specification of the various components a sarma 5 0 2 0 1 1 24 model is eventually selected 4 2 speed of the processes the training process of the sarma model requires 6 76 min and forecasting 0 04 s of computation time the fitting and forecasting processes of the npfda model are performed in one command that includes all the functions and has an overall computation time of 2 7 s however only one forecast is obtained at each run of the npfda model so in order to forecast 24 h ahead 64 68 s are required it is difficult to calculate or estimate the average and total computation time mainly because parallel computing was applied on desktops with different characteristics as stated in the methodology section for the calculation of every error index the forecast of every year at each horizon was used in order to reduce the computation time each year s forecast was performed on a different core and the parallel utilization of all the processors exhausted the computing capability of the used pc consequently the running speed depended on the characteristics of each desktop 4 3 performance of the models the results in terms of the performance indices corresponding to one hour ahead forecast are illustrated in fig 4 fig 5 and fig 6 sarma outperforms the npfda model as the latter has higher errors for all the calculated indices the combination of their forecasts provides slightly better results that both models the mse fig 4 rmse fig 5a and rmser fig 5b obtained with the forecasts done with the npfda model have the highest values during the winter and the spring and have a negative trend as we move towards summer and autumn this can be explained by the fact that the variability of the temperature during these seasons is the highest as discussed in section 3 thus it is more difficult to accurately forecast future values according to bias fig 6a and biasr fig 6b the performance of the two models is relatively consistent throughout the year the averaged error indices over all the months for forecast horizon of 1 24 h are displayed in fig 7 and fig 8 where the superior performance of the sarma model for one hour ahead forecast is confirmed this behavior continues until the sixth hour after which mse rmse fig 7a and rmser fig 7b of npfda become lower than sarma s these indices show that the pooling of the forecasts outperforms both models for all the forecast horizons the biggest improvement is observed at the sixth hour when mse is reduced by 17 57 rmse by 8 85 and rmser by 8 48 compared to the respective indices obtained by sarma when forecasting one hour ahead bias fig 8a is the lowest for sarma model and has a positive trend for increasing forecast horizon the functional model has a negative bias trend and becomes negative at the 9th hour the combination of the predictions has a positive bias trend for the first forecasted hours following the behavior of sarma the forecasts of which are assigned higher weights for this forecast horizon after the sixth hour the trend becomes negative similarly to npfda model and results in the lowest absolute bias value for a day ahead forecast since both models have a negative biasr fig 8b which keeps decreasing as the moment of the forecast moves away from the last available observation their combination follows the same behavior the evolution of the error indices for all the months when forecasting 1 24 h in advance are presented in fig 9 and fig 10 the increase in mse rmse fig 9a and rmser fig 9b for longer forecast horizons is the highest during february march and april both models and the combination of 24 h ahead forecasts result in mse and rmse that peak in march and in rmser that reaches its highest point in february similar to one hour ahead forecast the poor performance during these months can be justified by the high variability of the temperature the results are considerably improved during the period may november and the highest accuracy is observed for october and november when mse is less than 2 c for all the forecast horizons the only exception is for the month of september when the increase in the errors is larger compared to neighboring months especially regarding the rmser npfda provides mainly positive bias fig 10a values for one hour ahead forecast and a negative trend as the hours of forecast increase these results are in consistency with the average of the bias over the entire year this index also indicates a positively biased sarma model however its evolution shows that sarma is positively biased from february until august and gives negative bias values during the period september january forecasts combination has a rather mixed behavior it provides bias with the same pattern as sarma throughout the year with lower values and a declining trend for some of the months which is a characteristic of the functional model biasr fig 10b is negative during all the months when forecasting with the npfda model whereas it is positive for sarma from april until july except for the average of the error indices it is also interesting to investigate their variance as can be observed from the boxplots depicted in fig 11 and fig 12 sarma model has a lower median smaller interquartile range and smaller outliers of mse rmse fig 11a and rmser fig 11b than the npfda model until the 5th hour after which the behavior is reversed and the same results apply for the npfda model the combination of the forecasts results in lower median and outliers values and smaller interquartile range than both models for the first 9 forecasted hours but in a similar behavior with the npfda model for longer forecast horizon the improvements in the bias fig 12a and biasr fig 12b when combining the forecasts is achieved for most of the forecasts horizons 5 discussion the comparison of the results obtained in this paper with other studies in the literature is difficult for several reasons in general the use of functional data analysis to forecast meteorological variables is novel furthermore the combination of forecasts based on non parametric non linear fda and classical parametric linear time series approaches has not been tempted before most studies in the field of air temperature forecasting use time series of daily or monthly temporal resolution a search of the literature revealed few studies that use hourly air temperature however in these papers air temperature is used as a predictor for forecasting other variables e g water temperature and streamflow and the results of the air temperature forecasting are either not available or not reported previous studies regarding analysis of temperature in the region of me again use different temporal resolution or have a different scope such as the effects of climate oscillations trend analysis and analysis of extremes the series used in this study was obtained from a location with a very specific and unique climate therefore it cannot be claimed that all the obtained results can represent a general case however due to the nature of the used data and the applied approaches some useful conclusions can be deducted the basis of the fda approach is the approximation of series by functions and this explains the superior performance for longer forecast horizon for data that exhibits seasonal characteristics i e the diurnal variation of the temperature the low hour to hour variability of the temperature is a possible explanation of the fact that a linear model can capture the dynamics of a natural phenomenon and provide improved results for very short term forecasting future research activities should focus on applying the fda approach to other climatic variables and for other regions characterized by different climates due to the uncertainties associated with weather forecasting one would expect statistical methods to be popular in hydro meteorological variables forecasting however weather forecasting research has been dominated by deterministic models since the introduction of the computers more than 50 years ago despite their initial simplicity the physically based weather forecast models have shown wide success in predicting air temperature and other climatic variables and have since been developed and improved however statistical techniques have become increasingly popular in the recent years they have been mainly used in the areas of data assimilation and for the estimation of uncertainty by analyzing statistically the ensemble forecasts while the present work focuses on the development of the fda approach future work should consider a comparison of the performances of the fda approach and some physically based models this work could be continued in several ways the application of the proposed techniques could be further advanced with the inclusion of exogenous variables such as air pressure the addition of explanatory variables in the autoregressive model sarimax is very common and can be implemented in a straightforward way using any mainstream software for data analysis and modelling the literature on the inclusion of exogenous variables in the functional framework e g damon and guillas 2002 is considerably more limited compared to box jenkinson methods and a possible extension of this study could be in that direction it would also be interesting to investigate whether and how the different characteristics of the air temperature measured in other regions would affect the performance of the models 6 conclusions in this paper two approaches are presented to forecast air temperature time series namely npfda and sarma models the 1 24 h ahead predictions obtained from both models are also combined linearly using weights calculated based on the forecast performance of each model the application of the models shows that sarma model provides more accurate results for a forecast period of 1 6 h and npfda model has a better performance for forecast horizon of 7 24 h both models showed consistently good results from may until november and inferior performance in february and march when temperature variability is higher forecasts pooling improved the results for four out of five performance indices providing smaller averages and smaller variances of the error measures for almost all the forecast horizons data and software availability the data set used in this study is collected checked and processed by the meteorological department of the national center of meteorology and seismology of the uae ncms reserves the property rights and strictly prohibits its unauthorized sharing it can be made available under strict conditions to uae s universities and scientific research institutions in order to be used for research purposes this study was conducted in the context of the thesis presented to the masdar institute of science and technology in partial fulfillment of the requirements for the msc degree all the methods described in this paper were applied in the r statistical environment r 2016 the r software that was used for the implementation of the fda analysis is available online link and can be downloaded freely the names and the contact details of the developers are frédéric ferraty université toulouse le mirail ufr ses dépt math info 5 allées antonio machado 31058 toulouse cedex france ferraty univ tlse2 fr 33 0 5 61 50 46 06 philippe vieu université paul sabatier lab statistique and probabilités cnrs umr c55830 31062 toulouse cedex 09 france vieu cict fr 33 0 5 61 55 60 22 the box jenkins methods were applied by using the forecast package from the r statistical software hyndman r j 2017 hyndman r j and y khandakar 2008 the maintainer of the package is rob j hyndman monash university department of econometrics business statistics vic 3800 australia rob hyndman monash edu 03 9905 5141 acknowledgements financial support for this study was provided by masdar institute of science and technology the authors thank the national center of meteorology and seismology of the uae for the data of high quality we also thank frédéric ferraty and philippe vieu for the development and free distribution of the r tools used in fda finally the authors wish to thank the editor and three anonymous reviewers for their useful comments which led to considerable improvements in the paper appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 017 
26300,air temperature is a significant meteorological variable that affects social activities and economic sectors in this paper a non parametric and a parametric approach are used to forecast hourly air temperature up to 24 h in advance the former is a regression model in the functional data analysis framework the nonlinear regression operator is estimated using a kernel function the smoothing parameter is obtained by a cross validation procedure and used for the selection of the optimal number of closest curves the other method applied is a seasonal autoregressive moving average sarma model the order of which is determined by the bayesian information criterion the obtained forecasts are combined using weights calculated based on the forecast errors the results show that sarma has a better performance for the first 6 forecasted hours after which the non parametric functional data analysis npfda model provides superior results forecast pooling improves the accuracy of the forecasts keywords functional data analysis sarma time series air temperature forecasting 1 introduction accurate modelling and forecasting of hydrometeorological variables is important since weather can affect different economic and social activities weather forecasts provide useful knowledge that can prevent damages life and property losses by predicting extreme events and increase the efficiency of operations al matarneh et al 2014 air temperature is crucial for the agricultural sector since it affects the crop growth e g webber et al 2016 and the soil temperature and moisture high temperatures result in a faster development of the crops that leads to lower yield potential hatfield and prueger 2015 tourism is also highly influenced by temperature and precipitation de freitas 2003 electricity load is considerably driven by air temperature especially in arid regions such as uae where high temperatures are the norm however the response of electricity demand to changes in temperature is not uniform across the different sectors and taking into account this discrepancy can result in improved long term forecasts of electricity demand and energy planning moral carcedo and pérez garcía 2015 the predictions of weather related variables are also widely used for speculation in the rapidly growing weather derivatives markets campbell and diebold 2005 temperature is frequently used as an explanatory variable for forecasting other variables and processes such as streamflow sene 2009 electricity loading liu et al 2014 river water temperature st hilaire et al 2012 cheng and wiley 2016 surface energy fluxes woolway et al 2015 or solar radiation shamshirband et al 2015 mohammadi et al 2016 hence accurate prediction of temperature can provide higher forecasting accuracy of other variables in the literature the two basic techniques used to forecast climate variables are physical models based on numerical weather prediction e g lei et al 2009 and statistical methods which is the focus of this study among various statistical approaches implemented by researchers are time series models e g modarres and ouarda 2013 introduced by box and jenkins 1976 machine learning models such as artificial neural networks e g ouarda and shu 2009 support vector machine methods e g bautu and barbulescu 2013 and markov chains e g d amico et al 2012 the literature regarding temperature modelling and forecasting using box jenkins methods is considerable modarres and ouarda 2014 applied a multivariate generalized autoregressive conditional heteroscedasticity mgarch approach to model the second order moment variance for temperature downscaling in choon and chuin 2008 ann was found to provide more accurate monthly temperature forecasts compared to a sarima model air temperature was used as an explanatory variable to model stream water temperature in caissie et al 1998 and box jenkins model was found to have similar performance with a multiple regression model and a second order markov process temperature was used as an exogenous variable in an arima in order to model peak ozone concentrations in taiwan liu 2007 tabari et al 2015 used air temperature forecasts obtained by artificial neural networks ann to predict soil temperature seasonal long memory models have been commonly adopted for the modelling of climatic hydrological and environmental time series see for instance ooms and franses 2001 and reisen et al 2014 grimaldi 2004 showed that fractional arma farma models are more appropriate for time series that exhibit long term memory compared to the standard short memory models hybrid approaches have also been extensively used for air temperature forecasting for example deihimi et al 2013 combined wavelets transform with echo state networks wesn to predict temperature and used the forecasts to predict electricity load according to some other studies the combination of neural networks with wavelet methods sharma and agarwal 2012 eynard et al 2011 provides improved results except for the combinations between statistical approaches stochastic models are also used for postprocessing of the forecast ensembles obtained from deterministic numerical models möller and gross 2015 baran et al 2013 functional data analysis fda is a framework of increasing interest in the scientific community since being introduced by ramsay 1982 various classical statistical tools have been developed and extended to the context of fda ramsay and silverman 2005 ferraty and vieu 2006 dabo niang and ferraty 2008 it has been applied in many different fields such as chemometrics burba et al 2009 medicine ratcliffe et al 2002 and genomics krämer et al 2008 but the applications in hydrometeorology are limited chebana et al 2012 used fda to perform frequency analysis on flood hydrographs and ternynck et al 2016 for their classification el nino data series were used for quantile estimate by ferraty et al 2011 and for year ahead forecasting by besse et al 2000 meiring 2007 studied the patterns of the ozone variations depending on the altitude masselot et al 2016 applied linear functional regression models to forecast streamflow in this study we use a nonparametric functional data analysis approach to forecast air temperature time series nonparametric methods have the advantages of being free of assumptions about the distribution of the variables and the linearity of their relationship e g st hilaire et al 2012 this paper is organized as follows the npfda and the box jenkins models and the evaluation methodology are introduced in section 2 in section 3 the data used in the study is presented the results are discussed in sections 4 and 5 and section 6 concludes the study 2 methodology this section presents the methods used in this study initially data smoothing as a preliminary step for the npfda model is introduced and the regression model used in the fda framework is described then the sarma model is discussed and the forecasts combination technique is presented finally the evaluation procedure is explained 2 1 data smoothing by definition a random functional variable x takes its values in an infinite dimensional space a time series consisting of discrete measurements cannot describe data of such nature therefore it must be initially approximated by functions as a preparation step of the data to be used in the fda context a function x t can be constructed using the basis function expansion 1 x t k 1 k c k ϕ k t where t ω ϕ k k 1 k are basis functions linearly combined and c 1 c k are the coefficients of the expansion the most commonly used bases are fourier for periodic data see chebana et al 2012 for application in hydrology and splines for non periodic data e g ternynck et al 2016 there are also other basis systems that can be used such as wavelets e g giacofci et al 2013 or exponential e g mossaiby et al 2016 the optimal number of basis functions ϕ k and coefficients c k are estimated using a penalized residual sum of squares criterion that trades off between the roughness of the curve and the data underfitting and is given by 2 p e n n s e λ j y j x t j 2 λ d 2 x t 2 d t where d 2 x t is the second derivative of a function x t y j are the observations and λ is the smoothing parameter that gives weight to the second term which penalizes the roughness of the curve detailed explanation and discussion on the smoothing methods can be found in ramsay and silverman 2005 the periodicity of the air temperature series used in this study could justify the choice of fourier basis however the use of b splines provides slightly better forecast results and is preferred the order of the b splines is four by default and is equivalent to cubic polynomial since the degree of the polynomial is one less than its order b splines with two equispaced interior knots are considered to best capture the diurnal variation of the temperature 2 2 non parametric fda regression model univariate time series analysis consists in modelling the behavior of the past in order to predict future values of the same series a starting point is to determine the amount of information that will be taken into consideration for prediction i e what is the explanatory variable x that will be used to forecast a response y in the fda context a continuous section of the process is considered as a predictor variable assume a time series z t t ℝ observed at some equispaced points n n τ for some n ℕ and τ 0 considering the observations denoted by z 1 z 2 z n the objective is to predict future values z n s for some s 0 forecast horizon in order to construct a set of functional data the original series is initially divided into n 1 segments of length τ such that 3 x i z t i 1 τ t i τ for i 1 n 1 these functional observations correspond to the explanatory variables with responses that can be either scalar or of functional nature and are defined as 4 y i z i τ s here the scalar responses are estimated using a regression model 5 y i r x i ε i where ε i is the error term satisfying e ε i x i 0 i and r is a nonlinear regression function for the regression function r the following functional kernel estimator is considered ferraty and vieu 2006 6 r ˆ x i 1 n y i k h 1 d x x i i 1 n k h 1 d x x i where k is a local weighting kernel function each scalar response y i is attributed a weight according to the distance d between each functional random variable x i and a fixed functional variable x in other words for the x i that are closer to x the respective y i are assigned higher weights although many different types of kernel functions can be used e g box triangle quadratic gaussian the choice of the kernel has little impact on the quality of the estimator bosq 1998 in this study the positive asymmetrical quadratic kernel epanechnikov is implemented 7 k u 3 4 1 u 2 1 0 1 u the calculation of the distances between functional objects is a critical issue since they are defined in an infinite dimensional space and classical approaches such as euclidean norms are no longer appropriate as discussed in chapter 3 of ferraty and vieu 2006 there are considerable advantages in using semi metrics as proximity measures rather than metrics or norms one can select between various families of semi metrics depending mainly on the roughness of the functional observations and the purpose of the analysis the curves used in this study are relatively smooth hence semi metrics based on derivatives are applied 8 d q d e r i v x x i 2 x q t x i q t 2 d t where x q is the q t h derivative of x the stability issues that arise from the calculation of consecutive derivatives are solved by using b splines for curve fitting de boor 1978 schumaker 1982 in this study such problems are not encountered since the classical l2 norm was used as a proximity measure by setting the number of derivatives to zero the smoothing parameter bandwidth h represents the threshold for which the kernel k assigns weights to each y i for every calculated semi metric d x x i i e the estimator r ˆ x considers only the y i s for which the proximity between the respective x and x i is not larger than h the bandwidth can be substituted by an optimal number of closest curves using the k nearest neighbors method and obtained by a cross validation procedure ferraty and vieu 2006 9 k o p t arg min k g c v k where 10 g c v k i 1 n 1 y i r i k n n x i 2 with 11 r i k n n x j 1 j i n 1 y j k d x j x h k x j 1 j i n 1 k d x j x h k x 2 3 sarma model the second approach used in this study is a seasonal autoregressive integrated moving average sarima model also known as box jenkins 1976 model box jenkins approaches are probably the most popular methods for time series forecasting and have been extensively applied to model and predict temperatures among other hydro meteorological variables a major difference compared to the fda regression model is that sarma belongs to a different class of statistical procedures since it is a parametric approach the general form of a sarima p d q p d q s is denoted as 12 φ b s ϕ b 1 b d 1 b s d y t θ b s θ b a t where y t is the original data a t is the innovation term ϕ b is the non seasonal autoregressive ar polynomial φ b s is the seasonal autoregressive sar polynomial θ b is the non seasonal moving average ma polynomial θ b s is the seasonal moving average sma polynomial s is the seasonality term d is the order of non seasonal differencing d is the order of seasonal differencing and b is the backward shift operator due to the diurnal variation of the temperature the series is differenced at lag d 24 the annual seasonality is not considered since a subset of the data is used for training and the forecast horizon of 24 h is unlikely to be influenced by it the statistical significance of the seasonal ar and ma parameters is tested by calculating the t values the non seasonal ar and ma parameters are assigned values from 1 to 5 in all the possible combinations and the goodness of the models is evaluated by computing the bayesian information criterion aic calculated as 13 b i c m n l n σ a 2 m l n n where m is the number of ar and ma parameters to estimate σ a 2 is the variance of the residuals and n the number of observations the model that provides the lowest bic value is chosen as a parsimonious one 2 4 combination of forecasts as a result of the advances in time series analysis and growing computing capability several predictions for the same variable can be obtained forecasts pooling was initially introduced by bates and granger 1969 and has been widely applied since then e g marcellino 2004 according to various studies the combination of forecasts from different sources can provide better estimates than any single method johansson et al 2015 numerous pooling techniques can be implemented such as simple averaging linear combinations using weights or median forecast selection in the literature there is no general rule or diagnostic test that would indicate which pooling method is more appropriate the combination does not always provide improved forecasts and the results depend on the number and types of used models the forecast horizon and the type of the time series based on the promising results obtained in marcellino 2004 for linear combinations of forecasts from linear and non linear models the forecasts are combined linearly in this study 14 y ˆ t l m 1 m k m l t y ˆ t l m where l is the forecast horizon m denotes the models and k m l t are the weights calculated using the inverse mean square forecast error msfe ratio defined as 15 k m l t 1 m s f e m l t w j 1 m 1 m s f e j l t w such that m 1 m k m l t 1 for w 0 each forecast is assigned equal weight and for w 1 the weight of each forecast is inversely proportional to its error we consider w 5 in order to assign higher weights to the model that provides better results 2 5 evaluation procedure a jackknife evaluation procedure miller 1964 shao and tu 1995 is applied to assess the performance of the models the main strengths of the jackknife is that the models accuracy is independent of the calibration data and the loss in the sample data is minimal mccuen 2005 since jackknife is an iterative process and the number of hourly observations is 254 208 a subset of the entire series is used for training it consists of a moving window of 20 days 10 days before and 10 days after the day used for validation extended for all the years for example when forecasting the 15th of january 12 a m 11 p m the training dataset includes observations from 5th to 25th of january from 1982 to 2010 leaving out the 15th of january of each year the models are calibrated with the remaining data and then we forecast 1 24 h ahead at the next iteration the process starts with the next hour e g 15th of january 1 a m 12 a m and is repeated until the whole year is covered at each iteration a set of five indices is used to assess the performance of the proposed models for every forecast horizon specifically the indices are the mean squared error mse the root mean squared error rmse the relative root mean squared error rmser the mean bias bias and the relative mean bias biasr they are calculated using the following equations 16 m s e i 1 n y i y ˆ i 2 n 17 r m s e 1 n i 1 n y i y ˆ i 2 18 r m s e r 1 n i 1 n y i y ˆ i y i 2 19 b i a s 1 n i 1 n y i y ˆ i 20 b i a s r 1 n i 1 n y i y ˆ i y i where generically y i is the observed value and y ˆ i the estimated or forecasted value 3 case study the approaches described in the previous section are applied to the data presented in this section the time series used in this study consists of hourly measurements of air temperature oc and covers a period of 29 years from 1982 to 2010 fig 1 for the analysis in the fda framework the series are divided into segments of length 24 h thus every functional curve corresponds to the daily temperature fig 2 the data was collected from the meteorological station located at the abu dhabi international airport under the patronage of the national center of meteorology and seismology ncms of the uae it is situated at a height of 27 m at latitude 24 26 and longitude 54 39 the geographical location of the station is shown in fig 3 due to its location uae receives high amount of radiation during the summer and is one of the hottest regions in the world as can be observed from table 1 august is the month with the highest average temperature of 35 2 c and the highest maximum 48 8 c and minimum 24 c temperature the hottest period is from may until september during which the mean and the median temperatures are above 30 c the maximum temperature is higher than 46 c and may 17 c is the only month of the hot period that exhibits minimum temperature below 20 c during the rest of the season the range of the mean temperature is between 18 5 c january and 29 c october the cool period is during the winter months when the lowest minimum temperatures are observed december 7 9 c january 6 3 c and february 6 6 c as well as the lowest maximum temperature december 33 c january 33 4 c and february 37 7 c the highest variability of the temperature is observed from february until may during this period the range of the data is above 30 c and variance standard deviation and skewness are higher compared to the other months except for june that exhibits a higher variance than february but a smaller range the literature concerning studies of temperature and other climatic variables in the middle east me region is limited but has been increasing during the last years one of them was conducted by chandran et al 2015 who studied the relationship between climate oscillations and temperature precipitation in the uae and found significant correlation between the north atlantic oscillation index nao and temperature naizghi and ouarda 2017 carried out a similar study for wind speed in the uae basha et al 2015 forecasted temperatures in the uae by projecting non stationary oscillation nso processes obtained from ensemble empirical mode decomposition eemd results indicate a significant increase in the temperature in abu dhabi over the next 30 years 4 results this section initially presents the results of configuration of the srama model and then the results of applying the two proposed models are discussed 4 1 specification of the sarma model during the process of estimating the parameters of the model unlike seasonal ar parameters sma parameters are found to be statistically significant t 1 96 and are included in the model the bic values for the tested models are presented in table 2 the minimum bic is achieved by the model with five ar and two ma parameters a non seasonal differencing at lag d 1 results in higher aic values providing higher variance of the residuals for the same number of parameters and thus is not considered after the specification of the various components a sarma 5 0 2 0 1 1 24 model is eventually selected 4 2 speed of the processes the training process of the sarma model requires 6 76 min and forecasting 0 04 s of computation time the fitting and forecasting processes of the npfda model are performed in one command that includes all the functions and has an overall computation time of 2 7 s however only one forecast is obtained at each run of the npfda model so in order to forecast 24 h ahead 64 68 s are required it is difficult to calculate or estimate the average and total computation time mainly because parallel computing was applied on desktops with different characteristics as stated in the methodology section for the calculation of every error index the forecast of every year at each horizon was used in order to reduce the computation time each year s forecast was performed on a different core and the parallel utilization of all the processors exhausted the computing capability of the used pc consequently the running speed depended on the characteristics of each desktop 4 3 performance of the models the results in terms of the performance indices corresponding to one hour ahead forecast are illustrated in fig 4 fig 5 and fig 6 sarma outperforms the npfda model as the latter has higher errors for all the calculated indices the combination of their forecasts provides slightly better results that both models the mse fig 4 rmse fig 5a and rmser fig 5b obtained with the forecasts done with the npfda model have the highest values during the winter and the spring and have a negative trend as we move towards summer and autumn this can be explained by the fact that the variability of the temperature during these seasons is the highest as discussed in section 3 thus it is more difficult to accurately forecast future values according to bias fig 6a and biasr fig 6b the performance of the two models is relatively consistent throughout the year the averaged error indices over all the months for forecast horizon of 1 24 h are displayed in fig 7 and fig 8 where the superior performance of the sarma model for one hour ahead forecast is confirmed this behavior continues until the sixth hour after which mse rmse fig 7a and rmser fig 7b of npfda become lower than sarma s these indices show that the pooling of the forecasts outperforms both models for all the forecast horizons the biggest improvement is observed at the sixth hour when mse is reduced by 17 57 rmse by 8 85 and rmser by 8 48 compared to the respective indices obtained by sarma when forecasting one hour ahead bias fig 8a is the lowest for sarma model and has a positive trend for increasing forecast horizon the functional model has a negative bias trend and becomes negative at the 9th hour the combination of the predictions has a positive bias trend for the first forecasted hours following the behavior of sarma the forecasts of which are assigned higher weights for this forecast horizon after the sixth hour the trend becomes negative similarly to npfda model and results in the lowest absolute bias value for a day ahead forecast since both models have a negative biasr fig 8b which keeps decreasing as the moment of the forecast moves away from the last available observation their combination follows the same behavior the evolution of the error indices for all the months when forecasting 1 24 h in advance are presented in fig 9 and fig 10 the increase in mse rmse fig 9a and rmser fig 9b for longer forecast horizons is the highest during february march and april both models and the combination of 24 h ahead forecasts result in mse and rmse that peak in march and in rmser that reaches its highest point in february similar to one hour ahead forecast the poor performance during these months can be justified by the high variability of the temperature the results are considerably improved during the period may november and the highest accuracy is observed for october and november when mse is less than 2 c for all the forecast horizons the only exception is for the month of september when the increase in the errors is larger compared to neighboring months especially regarding the rmser npfda provides mainly positive bias fig 10a values for one hour ahead forecast and a negative trend as the hours of forecast increase these results are in consistency with the average of the bias over the entire year this index also indicates a positively biased sarma model however its evolution shows that sarma is positively biased from february until august and gives negative bias values during the period september january forecasts combination has a rather mixed behavior it provides bias with the same pattern as sarma throughout the year with lower values and a declining trend for some of the months which is a characteristic of the functional model biasr fig 10b is negative during all the months when forecasting with the npfda model whereas it is positive for sarma from april until july except for the average of the error indices it is also interesting to investigate their variance as can be observed from the boxplots depicted in fig 11 and fig 12 sarma model has a lower median smaller interquartile range and smaller outliers of mse rmse fig 11a and rmser fig 11b than the npfda model until the 5th hour after which the behavior is reversed and the same results apply for the npfda model the combination of the forecasts results in lower median and outliers values and smaller interquartile range than both models for the first 9 forecasted hours but in a similar behavior with the npfda model for longer forecast horizon the improvements in the bias fig 12a and biasr fig 12b when combining the forecasts is achieved for most of the forecasts horizons 5 discussion the comparison of the results obtained in this paper with other studies in the literature is difficult for several reasons in general the use of functional data analysis to forecast meteorological variables is novel furthermore the combination of forecasts based on non parametric non linear fda and classical parametric linear time series approaches has not been tempted before most studies in the field of air temperature forecasting use time series of daily or monthly temporal resolution a search of the literature revealed few studies that use hourly air temperature however in these papers air temperature is used as a predictor for forecasting other variables e g water temperature and streamflow and the results of the air temperature forecasting are either not available or not reported previous studies regarding analysis of temperature in the region of me again use different temporal resolution or have a different scope such as the effects of climate oscillations trend analysis and analysis of extremes the series used in this study was obtained from a location with a very specific and unique climate therefore it cannot be claimed that all the obtained results can represent a general case however due to the nature of the used data and the applied approaches some useful conclusions can be deducted the basis of the fda approach is the approximation of series by functions and this explains the superior performance for longer forecast horizon for data that exhibits seasonal characteristics i e the diurnal variation of the temperature the low hour to hour variability of the temperature is a possible explanation of the fact that a linear model can capture the dynamics of a natural phenomenon and provide improved results for very short term forecasting future research activities should focus on applying the fda approach to other climatic variables and for other regions characterized by different climates due to the uncertainties associated with weather forecasting one would expect statistical methods to be popular in hydro meteorological variables forecasting however weather forecasting research has been dominated by deterministic models since the introduction of the computers more than 50 years ago despite their initial simplicity the physically based weather forecast models have shown wide success in predicting air temperature and other climatic variables and have since been developed and improved however statistical techniques have become increasingly popular in the recent years they have been mainly used in the areas of data assimilation and for the estimation of uncertainty by analyzing statistically the ensemble forecasts while the present work focuses on the development of the fda approach future work should consider a comparison of the performances of the fda approach and some physically based models this work could be continued in several ways the application of the proposed techniques could be further advanced with the inclusion of exogenous variables such as air pressure the addition of explanatory variables in the autoregressive model sarimax is very common and can be implemented in a straightforward way using any mainstream software for data analysis and modelling the literature on the inclusion of exogenous variables in the functional framework e g damon and guillas 2002 is considerably more limited compared to box jenkinson methods and a possible extension of this study could be in that direction it would also be interesting to investigate whether and how the different characteristics of the air temperature measured in other regions would affect the performance of the models 6 conclusions in this paper two approaches are presented to forecast air temperature time series namely npfda and sarma models the 1 24 h ahead predictions obtained from both models are also combined linearly using weights calculated based on the forecast performance of each model the application of the models shows that sarma model provides more accurate results for a forecast period of 1 6 h and npfda model has a better performance for forecast horizon of 7 24 h both models showed consistently good results from may until november and inferior performance in february and march when temperature variability is higher forecasts pooling improved the results for four out of five performance indices providing smaller averages and smaller variances of the error measures for almost all the forecast horizons data and software availability the data set used in this study is collected checked and processed by the meteorological department of the national center of meteorology and seismology of the uae ncms reserves the property rights and strictly prohibits its unauthorized sharing it can be made available under strict conditions to uae s universities and scientific research institutions in order to be used for research purposes this study was conducted in the context of the thesis presented to the masdar institute of science and technology in partial fulfillment of the requirements for the msc degree all the methods described in this paper were applied in the r statistical environment r 2016 the r software that was used for the implementation of the fda analysis is available online link and can be downloaded freely the names and the contact details of the developers are frédéric ferraty université toulouse le mirail ufr ses dépt math info 5 allées antonio machado 31058 toulouse cedex france ferraty univ tlse2 fr 33 0 5 61 50 46 06 philippe vieu université paul sabatier lab statistique and probabilités cnrs umr c55830 31062 toulouse cedex 09 france vieu cict fr 33 0 5 61 55 60 22 the box jenkins methods were applied by using the forecast package from the r statistical software hyndman r j 2017 hyndman r j and y khandakar 2008 the maintainer of the package is rob j hyndman monash university department of econometrics business statistics vic 3800 australia rob hyndman monash edu 03 9905 5141 acknowledgements financial support for this study was provided by masdar institute of science and technology the authors thank the national center of meteorology and seismology of the uae for the data of high quality we also thank frédéric ferraty and philippe vieu for the development and free distribution of the r tools used in fda finally the authors wish to thank the editor and three anonymous reviewers for their useful comments which led to considerable improvements in the paper appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 017 
26301,this paper proposes a model for risk analysis of real time flood control operation of a multi reservoir system using a dynamic bayesian network the proposed model consists of three components monte carlo simulations dynamic bayesian network establishing and risk informed inference for decision making the monte carlo simulations provide basic data inputs for the dynamic bayesian network establishing using the historical floods and operation models of the multi reservoir system the dynamic bayesian network is built with expert knowledge and the relationships among the uncertainties the component of risk informed inference for decision making is to provide risk information about the operation schedules using the trained dynamic bayesian network we apply the proposed model to a multi reservoir system in china the results show that the proposed method has a capability for bi directional inferences and can be served as a risk informed decision making tool under uncertainties in the real time flood control operation of a multi reservoir system keywords reservoir flood control system uncertainty assessment risk analysis real time operation bayesian network 1 introduction real time flood control operation of multi reservoir systems is one of the important non engineering measures for flood mitigation in a river basin there are significant uncertainties involved in the real time flood control operation of multi reservoir systems wang 2009 kriauciuniene et al 2013 tung and wong 2013 yan et al 2014 these uncertainties generally fall into two categories i the internal uncertainties due to the randomness characteristics of the hydrologic variables themselves and ii the external uncertainties arising from model structural uncertainty as well as errors in parameter estimation of those models the traditional real time flood control operation models generally provide the operation schedules with deterministic results however the uncertainties lead to the deviation between the calculated deterministic results and the actual occurrence creating risks in flood control decision making due to this concern it is important to assess the uncertainties and provide decision makers with risk information risk analysis is one of the most important ways to quantify the uncertainties and their effects fema 2015 wu et al 2011 it can help the decision makers understand the mechanisms and evolution processes of the risk events it not only predicts the risk events and the possibilities of their occurrence but also identifies the possible reasons for the occurrence of the risk events therefore this study conducts research on risk analysis for the real time flood control operation of multi reservoir systems and performs the probabilistic reasoning under uncertainties many researchers have developed risk analysis methods for flood control systems zhu et al 2018 delenne et al 2012 hsu et al 2010 mcanally et al 2014 rudiger and bernd 1977 these methods generally fall into four categories i the return period method which characterizes the risk of a system by the frequency of floods ii the analytical method which calculates the risks through the probability density functions of the uncertainty factors such as stochastic differential equation method iii the reliability analysis method such as the first order second moment method mfosm the improved first order second order method afosm joint committee jc method and second moment method so iv the stochastic simulation method based on monte caro diao and wang 2010 li et al 2012 melching 1992 zhou et al 2016 however these traditional methods are not efficient to be applied to the risk analysis for real time flood control operations of multi reservoir systems the main problems include the following aspects i most of the traditional methods calculate the risks through statistical approaches based on the prior probabilities of the uncertainties however the uncertainties in real time flood control operations change over time the real time flood control operation processes of multi reservoir systems have to be modelled dynamically ii most of the traditional methods are difficult to obtain the high dimensional and nonlinear joint probability distribution function of the multiple uncertainty factors bayesian network bn was first proposed by pearl in 1988 it is a kind of probabilistic reasoning network based on bayesian conditional probability and graph theory hanea et al 2013 researchers have made great achievements in bayesian network structure learning parameter learning and probabilistic reasoning algorithms and so on malekmohammadi et al 2009 mesbah et al 2009 williams and cole 2013 bayesian network has become popular in the areas of dam risk analysis reliability assessment of electrical power system fault diagnosis of mechanical system and water management assessment chan et al 2010 wang et al 2009 morrison and stone 2014 zhang et al 2002 he 2012 the above published researches mainly focus on the application of static bayesian networks in flood control systems and flood risk management there have been few applications of dynamic bayesian networks in the risk analysis of real time flood control operations of multi reservoir systems however the static bayesian networks have some problems to be applied to the real time flood control operations firstly the uncertainties in the real time flood control operations are characterized by dynamic for example the forecast errors of reservoir inflows change with time and flood magnitude the forecast errors in different stages of a flood are also varied the flood risk is related to the real time reservoir storage which also changes over time thus the uncertainties evolve over time in the real time flood control operations of multi reservoir systems these uncertainties not only have the randomness characteristic themselves but also have the feature of time variability secondly the uncertainties in the real time flood control operations are transmitted in spatial dimension due to the complex topological structure of multi reservoir systems for example the reservoir discharges are transformed into a component of flood flows at the downstream control points thus the uncertainties evolve over space in the real time flood control operations of multi reservoir systems these uncertainties not only have the feature of time variability but also change in spatial dimension therefore we have to establish a dynamic risk analysis method for the real time flood control operations of multi reservoir systems a dynamic bayesian network dbn is a bn extended with temporal dimensions to enable us to model systems dynamically where the variables change over time molina et al 2013 dbns not only inherit the advantages of static bns but also graphically reveal the temporal and spatial evolution processes of the uncertainties the changes of node states in the dbns reflect the evolution of the stochastic variables over time a dbn can be decomposed into a number of static bns and the transfer networks between adjacent time slices the transfer network represents the connection between consecutive times for example the reservoir storage of the current moment is calculated from that of the previous moment therefore most of the modelling methods for static systems are not appropriate for dynamic flood control systems because of the connection between consecutive times however reviewing the previous works that dbns have been application as risk analysis for real time flood control operations is so limited we can only find one published application of dynamic bayesian networks in hydrological forecasting and probabilistic reasoning li 2009 studied the application of the dynamic bayesian network in hydrologic forecasting and proved its effectiveness from uncertainty selecting data preprocessing and model establishing the risk analysis of a flood control system can be considered as the problem for solving the joint probability distribution function of the uncertainties however the complexity of the high dimension and non linearity joint probability distribution function increases exponentially with the number of uncertainty factors bayesian network has opened a new way for the risk analysis of flood control systems because it has a good ability to solve the joint probability distribution function through prior distributions and conditional distributions of the uncertainty factors therefore this study for the first time applies dynamic bayesian networks to risk analysis for real time flood control operations of multi reservoir systems the whole proposed method is to have a general reliability assessment of the real time flood control operation process the proposed method includes the following three major modules 1 a monte carlo simulation model that prepares basic data inputs for the dynamic bayesian network establishing using the historical floods and the operation models of the multi reservoir system 2 a dynamic bayesian network model which is established with expert knowledge and the relationships among the uncertainties and 3 a risk informed inference model for decision making to predict risk information about the operation schedules using the observed data and the trained dynamic bayesian network the rest of this paper is organized as follows section 2 introduces the basic theory of bns and dbns section 3 proposes the methodology including monte carlo simulations dynamic bayesian network model establishing and risk informed inference for decision making section 4 presents the data of a case study as well as the results and discussions of the proposed methodology finally we provide the conclusions of this work in section 5 2 introduction to bayesian network 2 1 static bayesian network bayesian network is a kind of probabilistic reasoning network based on bayesian conditional probability and graph theory hanea et al 2013 nodes in bayesian networks represent stochastic variables and arcs represent the causal relationships direct influences between the variables the uncertainties of stochastic variables are represented by the prior probabilities of the root nodes and the conditional probabilities of the non root nodes therefore the risk event is expressed as a directed acyclic graph dag composed of nodes and directed arcs more detailed definitions about bns can be found in the paper by pearl 1988 the risk analysis of a flood control system can be considered as the problem for the joint probability distribution function pdf i e the probability of the possible risk event defined by the stochastic variables uncertainties according to the basic probability theory the pdf of the stochastic variables is expressed as 1 f x 1 x n f 1 x 1 i 2 n f i 1 i 1 x i x 1 x i 1 where x i represents the stochastic variable n is the number of the stochastic variables f x 1 x n is the joint probability density function of all the variables x 1 x n f i is the marginal probability density function of x i f i 1 i 1 is the conditional probability density function of x i given x 1 x i 1 according to eq 1 the complexity of the joint probability distribution function f x 1 x n increases exponentially with the number of the stochastic variables the traditional risk analysis methods are not suitable to solve the above high dimension and non linearity joint probability distribution function however bayesian networks can achieve the compactness and reduce the dimension of the complex joint pdf by the independence and conditional independence between the stochastic variables from bayesian networks theory the joint probability density function of x 1 x n in eq 1 is simplified as 2 f x 1 x n i 1 n p x i π i g where π i represents the parent nodes of x i g refers to the directed acyclic graph of bayesian network p x i π i g is the conditional probability distribution function of x i given its parents π i and the directed acyclic graph g 2 2 dynamic bayesian network a static bayesian network models a system at a fixed time dynamic bayesian network extends the concept for the systems that change with time it provides a great deal of flexibility in modelling the probabilistic causal relationships among the stochastic variables over time a dynamic bayesian network consists of a number of static bayesian networks and the transfer networks between the static bns li 2009 the transfer network between two adjacent static bns is expresses as 3 p x t x t 1 i 1 n p x i t π i t where x t x 1 t x n t representing the set of n stochastic variables that compose the static bayesian network at time t x i t represents the node variable i at time t π i t represents the parent nodes of x i t at time t the parents of x i t not only exist in the current static bn but also exist in the previous static bn the full joint probability distribution of the nodes in a dynamic bayesian network is given by the product 4 f x 1 t t 1 t i 1 n p x i t π i t g where x is the set of n stochastic variables x 1 x n that compose the dynamic bayesian network t is the number of static bns 3 methodology as the real time floods and flood control requirements change with time the status of each node in the bayesian network also varies over time therefore the real time flood control operation processes of multi reservoir systems have to be modelled dynamically in this paper we propose a dynamic bayesian network for the multi reservoir system to have a general risk assessment of the real time flood control operation processes the status changes of the nodes in dynamic bayesian networks represent the processes that the stochastic variables change over time the framework of the proposed model is shown in fig 1 as presented in fig 1 the proposed model includes three components monte carlo simulations dynamic bayesian network establishing and risk informed inference for decision making the monte carlo simulations prepare basic data inputs for the dynamic bayesian network establishing based on the historical floods and operation models of the multi reservoir system the dynamic bayesian network is established with the expert knowledge and the results of the monte carlo simulations the component of risk informed inference for decision making is to perform probabilistic reasoning using the observed data and trained dynamic bayesian network considering the uncertainties the description for the components of the proposed model is shown in the following sections 3 1 monte carlo simulations the monte carlo simulations prepare basic data inputs for establishing the dynamic bayesian network the steps are as follows 1 generate the random samples of the stochastic reservoir inflows q i t and lateral inflows i c i t at downstream control points according to their probability distribution functions using the latin hypercube sampling lhs method manache and melching 2004 2 calculate the samples of the reservoir water levels storages and releases through reservoir flood routing based on the random samples of the reservoir inflows shown in step 1 the reservoir flood routing is carried out using the optimal flood control operation models of the reservoirs 3 obtain the samples of the flood flows at downstream control points through river flood routing using the muskingum method based on the samples of the lateral inflows shown in step 1 and the results achieved in step 2 3 2 dynamic bayesian network establishing the establishing of dynamic bayesian networks consists of two steps structure learning and parameter learning structure learning establishes the graph structures of the bayesian networks considering the dependence between the stochastic variables parameter learning is to estimate the prior probability distributions of the root nodes and the conditional probability distributions of the non root nodes in bayesian networks the description for the methods of the two steps is shown in the following sections 3 2 1 structure learning of dynamic bayesian networks the structure learning methods for bayesian networks generally fall into two categories i establishing the graph structures by expert knowledge and ii searching the space of possible graph structures using the training data as the relationship among the variables in the real time flood control operation of the multi reservoir system is explicit this study establishes the graph structures of the dynamic bayesian networks by expert knowledge the steps are as follows 1 selecting the stochastic variables as the nodes of dynamic bayesian networks the nodes of dynamic bayesian networks are chosen according to the topological structure of the flood control system and flood propagation in the system this study selects the following six kinds of stochastic variables as the nodes of the dynamic bayesian network the inflows q i t of reservoir i at time t the storages v i t of reservoir i at time t the discharges q i t of reservoir i at time t the water levels h i t of reservoir i at time t the lateral inflows i c i t at downstream control point i at time t and the flood flows q c i t at downstream control point i at time t 2 determining the relationships between the nodes in dynamic bayesian network based on the topological structure of the reservoir groups and flood propagation in the multi reservoir system the order of the nodes and the causal relationships between the nodes stochastic variables are determined 3 drawing the nodes and arcs according to the order in step 2 the nodes are added to the bayesian networks and the directed arcs are drawn according to the causality between the nodes stochastic variables 4 checking the proposed graph structures the established graph structures of the bayesian network are examined according to the characteristics of bayesian networks such as conditional independence 3 2 2 parameter learning of dynamic bayesian networks the prior probability distributions of the root nodes and the conditional probability distributions or tables cpts of the non root nodes in bayesian networks are learned from data achieved in the monte carlo simulations there are various parameter learning methods for bns such as the expectation maximization em algorithm conjugate gradient descent method and so on this paper uses the maximum likelihood estimation mle method for the parameter learning given the observed samples x x 1 x 2 x m the mle method calculates the parameters through maximizing the likelihood function 5 θ ˆ a r g m a x θ l x θ a r g m a x θ ln p x θ where l x θ is the likelihood function l x θ ln p x θ θ refers to the parameter θ ˆ is the maximum likelihood estimation of the parameter θ usually the continuous variables are problematic in dbns due to computational complexity because it is difficult to capture the relationships between the continuous variables other than the gaussian distribution dan and heckerman 1994 therefore the classical ways to deal with continuous variables in bns and dbns are assuming a joint parametric gaussian distribution or discretizing the variables monti and cooper 1998 most often the discretization methods are used because many continuous variables in the real world do not follow the gaussian distributions there are many discretization methods for the continuous variables in bns and dbns such as equal distance normalized equal distance equal frequency k means tree and so on we use the equal distance method which is one of the most commonly used approaches for data discretization especially for obtaining a discrete representation of the probability density function of the variable the equal distance method divides the range of the stochastic variable into m x i intervals and the intervals are equal sized the width w x i of the intervals is expressed as 6 w x i x i max x i min m x i where m x i is the number of the divided intervals of the variable x i x i max is the maximum in the range of the variable x i x i min is the minimum in the range of the variable x i 3 3 risk informed inference for decision making the component of risk informed inference for decision making is to perform probabilistic reasoning and risk calculation for the flood control operations under uncertainties based on the observed data and the trained dynamic bayesian network 3 3 1 risk informed inference using the dynamic bayesian network given an evidence on any of the nodes dbns can perform inference regardless of the directions of the arcs the inference of dbns usually consists of prediction simulation and diagnosis 1 prediction simulation is to infer from cause to effect given the observed values of some nodes in the dynamic bayesian network it predicts the values and their probabilities of other nodes for example given the forecasted reservoir inflows and lateral inflows it calculates the reservoir water levels and flood flows at downstream control points and it also simultaneously provides their possibilities of occurrence 2 diagnosis is to infer from effect to cause given the observed results it diagnoses the causes of the results for example given the risk event of reservoir overtopping it finds the possible reasons for the event prediction and diagnosis are identical computations in dbns they both compute the posterior probabilities of the nodes in dbns conditional upon evidence conrady and jouffe 2015 the underlying computational mechanism is based on bayes theory expressed as 7 p x e p e x p x p e where e refers to the evidence i e the observed values of the nodes in dynamic bayesian networks p x is the prior probability of x p e x is the likelihood probability p x e is the posterior probability i e the conditional probability of x given e this paper also uses the mle method for the inference algorithm given the observations evidence of some nodes dbns achieve the values of other nodes by updating the network at time steps therefore the real time flood control operation processes of multi reservoir systems are dynamically modelled by the updating of dbns 3 3 2 uncertainties and risk calculation this paper considers the uncertainty of reservoir inflows and lateral inflows of the downstream control points arising from the forecast errors reservoir storage capacity curve errors and reservoir discharge curve errors jiang 1998 chen et al 2017 the above uncertainties result in the randomness of the reservoir storages releases water levels and lateral inflows of downstream control points the randomness makes reservoir storages releases water levels and lateral inflows stochastic in occurrence and magnitude therefore the water level considering the influence of the uncertainties is called stochastic reservoir water level the flood flow of the downstream control point is defined as the flow that combines the upstream reservoir releases and lateral inflows of the downstream control point the upstream reservoir releases and lateral inflows of the downstream control point are stochastic due to the uncertainties which result in the randomness of the flood flow at the downstream control point therefore the flood flow of the downstream control point considering the influence of the uncertainties is called stochastic flood flow the stochastic reservoir inflows storages releases and lateral inflows of downstream control points are expressed as 8 q i t q i t ξ q i t v i t v i t ξ v i t q i t q i t ξ q i t i c i t i c i t ξ i c i t where ξ q i t is the forecasting error of reservoir i at time t q i t is the mean value of the stochastic inflow of reservoir i at time t ξ v i t is the storage error of reservoir i at time t v i t is the mean value of the stochastic storage of reservoir i at time t ξ q i t is the discharge error of reservoir i at time t q i t is the mean value of the stochastic release of reservoir i at time t ξ i c i t is the forecasting error of control point i at time t i c i t is the mean value of the stochastic lateral inflow of downstream control point i at time t the stochastic reservoir water levels and the stochastic flood flows at downstream control points are expressed as 9 h i t h i t ξ h i t q c i t q c i t ξ q c i t where ξ h i t is the water level error of reservoir i at time t h i t is the mean value of the stochastic water level of reservoir i at time t ξ q c i t is the flood flow error of control point i at time t q c i t is the mean value of the stochastic flood flow of control point i at time t the above uncertainties create risks in real time flood control decisions of multi reservoir systems in this study we focus on the risk of reservoir overtopping and the risk of flooding at downstream control points tung and mays 1981 chen et al 2017 expressed as 10 p r i t p h i t h r i t where p r i t is the risk of reservoir overtopping at time t h r i t is the selected safe water level of reservoir i at time t which can be the reservoir design flood water level or the reservoir dam crest height 11 p c i t p q c i t q c r i t where p c i t refers to the risk of flooding at downstream control point i at time t q c r i t is the selected safe flow at the downstream control point i at time t and it can be the safety flood flow the continuous variables water level and flood flow are discretized in the dbns for parameter learning the discretization is to replace a continuous variable by a number of discrete states the term h j i t represents the jth discrete state of the continuous variable water level of reservoir i at time t where j 1 to m h according to the inference of the dbns we can obtain the occurrence probability of each discrete state of the continuous variable expressed as p j h j i t the occurrence probability p j h j i t means there is a p j h j i t chance that the water level of reservoir i at time t is h j i t therefore the probability in eq 10 that the reservoir water level h i t is larger than the selected safe water level h r i t is equal to the summation of the occurrence probabilities whose discrete states are larger than h r i t expressed as 12 p r i t p h i t h r i t j 1 m h p j h j i t h r i t where m h is the number counted as h j i t h r i t j 1 to m h m h m h p j h j i t h r i t is the corresponding probability of h j i t h r i t i e p j h j i t where h j i t h r i t similarity the risk in eq 11 is transformed to 13 p c i t p q c i t q c r i t j 1 m q c p j q c j i t q c r i t where q c j i t represents the jth discrete state of the continuous variable flood flow at downstream control point i at time t where j 1 to m q c p j q c j i t is the corresponding occurrence probability of the discrete state q c j i t of the continuous variable q c i t p j q c j i t q c r i t is the corresponding probability of q c j i t q c r i t i e p j q c j i t where q c j i t q c r i t m q c is the number counted as q c j i t q c r i t j 1 to m q c m q c m q c therefore based on eq 12 and eq 13 the risk of reservoir overtopping and the risk of flooding at control points can be calculated using the inference results of dbns the main steps are as follows 1 obtain the forecasted reservoir inflows and lateral inflows considering forecast errors 2 find the discrete values corresponding to the above reservoir inflows and lateral inflows and then input them to the corresponding nodes of the dynamic bayesian network 3 perform the probabilistic reasoning using the inference algorithm of dynamic bayesian networks and obtain the reservoir water levels and flood flows at control points and also achieve their corresponding probabilities of occurrence 4 calculate the risk of reservoir overtopping and the risk of flooding at control points according to eq 12 and eq 13 respectively given the evidence of reservoir overtopping and flooding at control points the possible reasons can be founded through the following steps 1 obtain the evidence of reservoir overtopping or flooding at control points e g the values of reservoir water levels and flood flows 2 find the discrete values corresponding to the above evidence and then input them to the corresponding nodes of the dynamic bayesian network 3 perform the probabilistic reasoning using the inference algorithm of dynamic bayesian networks 4 obtain the values of all the other nodes in dbns and their corresponding probabilities of occurrence and then analyze the possible reasons for the risk event 4 case study 4 1 introduction to the study area the case study is performed in the middle reaches of the huaihe river basin in china as presented in fig 2 the huaihe river basin located about midway between the yellow river and yangtze is characterized by both a continental climate and a semi humid monsoon climate the multi reservoir system in the middle reaches of the huaihe river basin is composed of four large reservoirs nianyushan meishan xianghongdian and foziling and four downstream control points jiangjiaji hengpaitou runheji and zhengyangguan the generalized configuration of the flood control system is shown in fig 3 the main features of the reservoirs are shown in table 1 the safety flood flows of the jiangjiaji hengpaitou runheji and zhengyangguan control points are 3580 m3 s 8400 m3 s 8000 m3 s and 10000 m3 s respectively 4 2 data for the proposed methodology 1 inflows to the reservoirs and lateral inflows we use an observed flood in 2003 for the case study fig 4 shows the inflows to the reservoirs and lateral inflows between reservoirs and downstream control points the time interval is 1 h and the time duration of the flood is 168 according to the monte carlo simulation steps in section 3 1 generate 1000 random samples of reservoir inflows and lateral inflows according to their probability distributions in this paper the forecast errors of reservoir inflows and lateral inflows are assumed to follow the normal distributions it is because that the parameters of the hydrological forecast model are acquired from the statistical methods whose estimation techniques are optimal in the mean square error sense therefore the forecast errors are assumed to follow the normal distributions which have been evaluated in many literature and recommended by the ministry of water resources mwr of china mwr 2000 li et al 2010 the relative forecast errors of reservoir inflows and lateral inflows are expressed as n 0 0 20 2 2 nodes of the dynamic bayesian networks according to the steps in section 3 2 we select the following stochastic variables as the nodes of the dynamic bayesian network for the multi reservoir system as shown in table 2 the data for the nodes in table 2 are discretized for parameter learning according to the equal distance method shown in section 3 2 4 3 results 4 3 1 establishing of the dynamic bayesian networks as table 2 shows there are 24 nodes at time t for the risk analysis of real time flood control operation of the multi reservoir system therefore it is difficult to obtain the joint probability distribution of these stochastic variables by the traditional risk analysis methods we resolve the problem through dynamic bayesian networks the graph structures of the dynamic bayesian network are established according to the steps in section 3 2 as presented in fig 5 as fig 5 shows the dynamic bayesian network is composed of t time slices and t represents the time duration of the whole flood process each time slice represents a static bayesian network the solid arrows in the networks show the direct connections between the nodes in the same time slice the dotted arrows refer to the connections between the nodes in adjacent time slices the time between consecutive time slice is 1 h therefore the whole flood event from time 1 to time t is represented by the dynamic bayesian network the parameters i e the prior probabilities of the root nodes and the cpts of the non root nodes are learned from data using the bayesialab software based on mle method bayesialab is an automatic learning software for bns developed by dr lionel jouffe and dr paul munteanu the users can download the software from http www bayesia com 4 3 2 numerical experiments using the established dbn we conduct six numerical experiments to show the risk informed inference using the established dbn this study also uses bayesialab for implementing the inference the detailed information and results of the six numerical experiments are shown as follows 1 prediction experiment 1 assessing the impact of uncertainty of reservoir inflows and lateral inflows of the control points in this study the mean values of reservoir inflows and lateral inflows at control points are as follows q 1 t p 144 m3 s q 2 t p 242 m3 s q 3 t p 118 m3 s q 4 t p 109 m3 s i c 5 t p 917 m3 s i c 6 t p 884 m3 s i c 7 t p 858 m3 s and i c 8 t p 143 m3 s taking the time t p of the largest flood flow at zhengyangguan control point as an example according to eq 8 it is assumed that the forecast errors of reservoir inflows and lateral inflows are equal to their standard deviations i e q 1 t p q 1 t p σ q 1 t p 172 m3 s q 2 t p 290 m3 s q 3 t p 142 m3 s q 4 t p 130 m3 s i c 5 t p i c 5 t p σ i c 5 t p 1100 m3 s i c 6 t p 1060 m3 s i c 7 t p 1030 m3 s and i c 8 t p 171 m3 s given these forecast errors of reservoir inflows and lateral inflows the water levels and flood flows at downstream control points are predicted as well as their probabilities of occurrence we input the above reservoir inflows and lateral inflows with forecast errors to the corresponding nodes of the dynamic bayesian network and perform the inference fig 6 shows the reservoir water levels and flood flows of control point with their probabilities of occurrence taking xianghongdian reservoir and hengpaitou flood control point as examples according to fig 6 the risk of reservoir overtopping and the risk of flooding at control points are calculated based on eq 12 and eq 13 as shown in fig 7 experiment 2 assessing the impact of reservoir release uncertainty in this study the mean values of reservoir releases are as follows q 1 t p 486 m3 s q 2 t p 1490 m3 s q 3 t p 582 m3 s and q 4 t p 261 m3 s taking the time t p of the largest flood flow at zhengyangguan control point as an example according to eq 8 it is assumed that the reservoir release error is equal to its standard deviation i e q 1 t p q 1 t p σ q 1 t p 490 m3 s q 2 t p 1510 m3 s q 3 t p 591 m3 s and q 4 t p 263 m3 s we input the above values to the corresponding nodes of the dynamic bayesian network and perform the inference fig 8 shows the reservoir water levels and flood flows of control point with their probabilities of occurrence taking xianghongdian reservoir and hengpaitou flood control point as examples according to fig 8 the risk of reservoir overtopping and the risk of flooding at control points are calculated based on eq 12 and eq 13 as shown in fig 9 experiment 3 assessing the impact of reservoir storage uncertainty in this study the mean values of reservoir storages are as follows v 1 t p 627 84 106 m3 v 2 t p 1830 62 106 m3 v 3 t p 2004 22 106 m3 and v 4 t p 381 58 106 m3 taking the time t p of the largest flood flow at zhengyangguan control point as an example according to eq 8 it is assumed that the reservoir storage error is equal to its standard deviation i e v 1 t p v 1 t p σ v 1 t p 628 48 106 m3 v 2 t p 1833 99 106 m3 v 3 t p 2014 40 106 m3 and v 4 t p 382 01 106 m3 we input the above values to the corresponding nodes of the dynamic bayesian network and perform the inference fig 10 shows the reservoir water levels and flood flows of control point with their probabilities of occurrence taking xianghongdian reservoir and hengpaitou flood control point as examples according to fig 10 the risk of reservoir overtopping and the risk of flooding at control points are calculated based on eq 12 and eq 13 as shown in fig 11 experiment 4 evaluating the impact of the combination of all the uncertainties we input all the uncertainties in experiments 1 2 and 3 to the corresponding nodes of the dynamic bayesian network and perform the inference fig 12 shows the reservoir water levels and flood flows of control point with their probabilities of occurrence taking xianghongdian reservoir and hengpaitou flood control point as examples according to fig 12 the risk of reservoir overtopping and the risk of flooding at control points are calculated based on eq 12 and eq 13 as shown in fig 13 2 diagnosis experiment 5 given the reservoir water level predicting the inflows according to eq 9 it is assumed that the reservoir water level errors are equal to their standard deviations i e h 1 t p h 1 t p σ h 1 t p 109 46 m h 2 t p 134 80 m h 3 t p 136 48 m and h 4 t p 124 89 m we input the above values to the corresponding nodes of the dynamic bayesian network and perform the inference fig 14 a shows the reservoir inflows with their probabilities of occurrence taking nianyushan reservoir as an example the risks that the reservoir inflows exceed the selected inflows are in shown in fig 14 b experiment 6 given the flood flows at control points predicting the reservoir inflows and lateral inflows according to eq 9 it is assumed that the flood flow errors are equal to their standard deviations i e q c 5 t p q c 5 t p σ q c 5 t p 2660 m3 s q c 6 t p 1620 m3 s q c 7 t p 3200 m3 s and q c 8 t p 3580 m3 s we input the above values to the corresponding nodes of the dynamic bayesian network and perform the inference fig 15 a shows the reservoir inflows with their probabilities of occurrence taking nianyushan reservoir as an example the risks that the reservoir inflows exceed the selected inflows are in shown in fig 15 b 4 3 3 effect of uncertainty level on the results in order to evaluate the effect of the uncertainty level on the results we assume that the forecast errors of reservoir inflows and lateral inflows are equal to zero s1 their standard deviations s2 twice of their standard deviations s3 respectively we input the values with the above uncertainties to the corresponding nodes of the dynamic bayesian network and perform the inference respectively fig 16 shows the risks of flooding at control points with different uncertainty levels taking hengpaitou as an example 4 4 discussions 4 4 1 discussions of the results according to figs 6 8 10 and 12 we can know the reservoir water levels and flood flows at control points and their corresponding probabilities of occurrence considering the forecast errors of reservoir inflows and lateral inflows reservoir release uncertainty reservoir storage uncertainty or the combination of these uncertainties for example the probability that the flood flow of hengpaitou control point at time t p is between 1480 and 1610 m3 s is 48 7 as shown in fig 6 that means there is a 48 7 chance that the flood flow is between 1480 and 1610 m3 s similarity there is a 20 7 chance that the flood flow at hengpaitou control point is between 1610 and 1730 m3 s and there is a 16 2 chance that the flood flow is between 1730 and 1850 m3 s according to figs 7 9 11 and 13 we can estimate the risk of reservoir overtopping and the risk of flooding at control points for example the probability that the flood flow of hengpaitou control point exceeds 1730 m3 s is 16 8 due to the forecast errors of reservoir inflows and lateral inflows as shown in fig 7 it is noted that the selected safe values of reservoir water levels and flood flows affect the risk of reservoir overtopping and the risk of flooding at control points the risk of reservoir overtopping decreases as the selected water level increases the risk of flooding at control points also reduces when the selected flood flows increases according to figs 14 and 15 we can calculate the probability that the stochastic reservoir inflows are above the selected inflows given the risk event that the stochastic reservoir water level exceeds the selected water level or the flood flow at the control point exceeds the selected flood flow for example given the risk event that the stochastic water level of nianyushan reservoir exceeds its mean value h 1 t p h 1 t p σ h 1 t p 109 46 m the probability that the inflows of nianyushan reservoir exceed 219 m3 s is 0 5 as shown in fig 14 the uncertainty level affects the risk of reservoir overtopping and the risk of flooding at control points for example the risk of flooding at hengpaitou control point increases with the uncertainty level of reservoir inflows and lateral inflows under the same selected flood flows as shown in fig 16 4 4 2 comparison with static bns and monte carlo method in order to evaluate the efficiency of the proposed method we compare the risks obtained by dbns with the results from static bns and monte carlo mc method fig 17 shows the comparison of reservoir overtopping risks derived from dbns and bns taking xianghongdian reservoir as an example as fig 17 shows the risks of reservoir overtopping under the low water level design flood water level and overtopping flood water level are all the same for bn and dbn but the risk of dbn is smaller than that of bn under medium water level it is because that dbn method performs inference with more information than the bn approach i e the connections between adjacent time slices fig 18 shows the comparison of the curves between reservoir overtopping risks and selected water levels obtained from dbns and mc method taking xianghongdian reservoir as an example as fig 18 shows the risks obtained from the dbn method approximate to that of the mc method the maximum deviation is 2 8 as shown in the points b 136 35 52 3 and a 136 35 49 5 the difference is mainly due to the discretization precision of the continuous variables in the dbns and the sample size of the mc method in addition most of the risks obtained from the dbn method are slightly larger than that of the mc method it means that the mc method may underestimate the risks which is not safe for the flood control operations and decision making 4 4 3 application and limitations of the proposed model the proposed method can be served as a risk informed decision making tool under uncertainty in the real time flood control operations of a multi reservoir system the risk analysis results provide fundamental risk information to the decision makers through the prediction and diagnosis of the established dynamic bayesian network the risk information includes the probability of reservoir overtopping the probability of flooding at control points and the integrated risk of the whole multi reservoir system under the uncertainties and the possible causes for given risk events the decision makers can have an insight into the vulnerabilities of the multi reservoir systems and improve the real time flood control operations according to the risk information for example if the probability of reservoir overtopping is greater than the risk acceptable to the decision makers then they can improve the operating schedules further research can be conducted on the effect of the discretization precision of the continuous variables in the dbns xue et al 2016 the other problem of the discretization methods is to balance the desire for high accuracy of the risks with the calculation burden to obtain the results the type of evidence can also be evaluated e g the usage of probabilistic evidence or soft evidence 5 conclusions there are many uncertainties in the real time operation of multi reservoir systems which create risks in flood control decisions we proposed a method for risk analysis of the real time flood control operation of a multi reservoir system using a dynamic bayesian network we applied the method to a multi reservoir flood control system in the middle reaches of the huaihe river basin in china the main points are summarized as follows 1 we established a dynamic bayesian network for the risk analysis of the real time flood control operation of a multi reservoir system the dynamic bayesian network was built with expert knowledge and parameter learning using the data from the monte carlo simulations 2 we performed the risk informed inference of prediction for decision making using the established dynamic bayesian network the prediction inference results of the dbn can provide the possible reservoir water levels and flood flows at control points and their corresponding probabilities of occurrence considering the forecast errors of reservoir inflows and lateral inflows reservoir release uncertainty reservoir storage uncertainty or the combination of these uncertainties the inference results can also estimate the risk of reservoir overtopping and the risk of flooding at control points 3 we performed the risk informed inference of diagnosis for decision making using the proposed dbn the diagnosis results can provide the probability that the stochastic reservoir inflows are above the selected inflows given the risk event that the stochastic reservoir water level exceeds the selected water level or the flood flow at the control point exceeds the selected flood flow 4 the uncertainty level as well as the selected water levels and flood flows affect the risk of reservoir overtopping and the risk of flooding at control points the risks increase with the uncertainty level and decrease with the selected water levels and flood flows 5 the proposed method can be served as a risk informed decision making tool under uncertainty in the real time flood control operations of a multi reservoir system for example if the probability of reservoir overtopping is greater than the risk acceptable to the decision makers then they can improve the operating schedules 6 further research can be conducted on the effect of the discretization precision of the continuous variables in the dbns and on the impact of the comprehensive uncertainty factors in real time flood control operations acknowledgments this study is supported by the national key r d program of china china grant 2017yfc0405606 national natural science foundation of china china grant 51579068 fundamental research funds for the central universities china grant 2018b18214 national natural science foundation of jiangsu province china grant bk20180509 china postdoctoral science foundation china grant 2017m621612 and postdoctoral science foundation of jiangsu province china grant 2018k127c the users can access the data used in this paper from https www zenodo org record 1164432 wnq6kvmdhjc appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 007 
26301,this paper proposes a model for risk analysis of real time flood control operation of a multi reservoir system using a dynamic bayesian network the proposed model consists of three components monte carlo simulations dynamic bayesian network establishing and risk informed inference for decision making the monte carlo simulations provide basic data inputs for the dynamic bayesian network establishing using the historical floods and operation models of the multi reservoir system the dynamic bayesian network is built with expert knowledge and the relationships among the uncertainties the component of risk informed inference for decision making is to provide risk information about the operation schedules using the trained dynamic bayesian network we apply the proposed model to a multi reservoir system in china the results show that the proposed method has a capability for bi directional inferences and can be served as a risk informed decision making tool under uncertainties in the real time flood control operation of a multi reservoir system keywords reservoir flood control system uncertainty assessment risk analysis real time operation bayesian network 1 introduction real time flood control operation of multi reservoir systems is one of the important non engineering measures for flood mitigation in a river basin there are significant uncertainties involved in the real time flood control operation of multi reservoir systems wang 2009 kriauciuniene et al 2013 tung and wong 2013 yan et al 2014 these uncertainties generally fall into two categories i the internal uncertainties due to the randomness characteristics of the hydrologic variables themselves and ii the external uncertainties arising from model structural uncertainty as well as errors in parameter estimation of those models the traditional real time flood control operation models generally provide the operation schedules with deterministic results however the uncertainties lead to the deviation between the calculated deterministic results and the actual occurrence creating risks in flood control decision making due to this concern it is important to assess the uncertainties and provide decision makers with risk information risk analysis is one of the most important ways to quantify the uncertainties and their effects fema 2015 wu et al 2011 it can help the decision makers understand the mechanisms and evolution processes of the risk events it not only predicts the risk events and the possibilities of their occurrence but also identifies the possible reasons for the occurrence of the risk events therefore this study conducts research on risk analysis for the real time flood control operation of multi reservoir systems and performs the probabilistic reasoning under uncertainties many researchers have developed risk analysis methods for flood control systems zhu et al 2018 delenne et al 2012 hsu et al 2010 mcanally et al 2014 rudiger and bernd 1977 these methods generally fall into four categories i the return period method which characterizes the risk of a system by the frequency of floods ii the analytical method which calculates the risks through the probability density functions of the uncertainty factors such as stochastic differential equation method iii the reliability analysis method such as the first order second moment method mfosm the improved first order second order method afosm joint committee jc method and second moment method so iv the stochastic simulation method based on monte caro diao and wang 2010 li et al 2012 melching 1992 zhou et al 2016 however these traditional methods are not efficient to be applied to the risk analysis for real time flood control operations of multi reservoir systems the main problems include the following aspects i most of the traditional methods calculate the risks through statistical approaches based on the prior probabilities of the uncertainties however the uncertainties in real time flood control operations change over time the real time flood control operation processes of multi reservoir systems have to be modelled dynamically ii most of the traditional methods are difficult to obtain the high dimensional and nonlinear joint probability distribution function of the multiple uncertainty factors bayesian network bn was first proposed by pearl in 1988 it is a kind of probabilistic reasoning network based on bayesian conditional probability and graph theory hanea et al 2013 researchers have made great achievements in bayesian network structure learning parameter learning and probabilistic reasoning algorithms and so on malekmohammadi et al 2009 mesbah et al 2009 williams and cole 2013 bayesian network has become popular in the areas of dam risk analysis reliability assessment of electrical power system fault diagnosis of mechanical system and water management assessment chan et al 2010 wang et al 2009 morrison and stone 2014 zhang et al 2002 he 2012 the above published researches mainly focus on the application of static bayesian networks in flood control systems and flood risk management there have been few applications of dynamic bayesian networks in the risk analysis of real time flood control operations of multi reservoir systems however the static bayesian networks have some problems to be applied to the real time flood control operations firstly the uncertainties in the real time flood control operations are characterized by dynamic for example the forecast errors of reservoir inflows change with time and flood magnitude the forecast errors in different stages of a flood are also varied the flood risk is related to the real time reservoir storage which also changes over time thus the uncertainties evolve over time in the real time flood control operations of multi reservoir systems these uncertainties not only have the randomness characteristic themselves but also have the feature of time variability secondly the uncertainties in the real time flood control operations are transmitted in spatial dimension due to the complex topological structure of multi reservoir systems for example the reservoir discharges are transformed into a component of flood flows at the downstream control points thus the uncertainties evolve over space in the real time flood control operations of multi reservoir systems these uncertainties not only have the feature of time variability but also change in spatial dimension therefore we have to establish a dynamic risk analysis method for the real time flood control operations of multi reservoir systems a dynamic bayesian network dbn is a bn extended with temporal dimensions to enable us to model systems dynamically where the variables change over time molina et al 2013 dbns not only inherit the advantages of static bns but also graphically reveal the temporal and spatial evolution processes of the uncertainties the changes of node states in the dbns reflect the evolution of the stochastic variables over time a dbn can be decomposed into a number of static bns and the transfer networks between adjacent time slices the transfer network represents the connection between consecutive times for example the reservoir storage of the current moment is calculated from that of the previous moment therefore most of the modelling methods for static systems are not appropriate for dynamic flood control systems because of the connection between consecutive times however reviewing the previous works that dbns have been application as risk analysis for real time flood control operations is so limited we can only find one published application of dynamic bayesian networks in hydrological forecasting and probabilistic reasoning li 2009 studied the application of the dynamic bayesian network in hydrologic forecasting and proved its effectiveness from uncertainty selecting data preprocessing and model establishing the risk analysis of a flood control system can be considered as the problem for solving the joint probability distribution function of the uncertainties however the complexity of the high dimension and non linearity joint probability distribution function increases exponentially with the number of uncertainty factors bayesian network has opened a new way for the risk analysis of flood control systems because it has a good ability to solve the joint probability distribution function through prior distributions and conditional distributions of the uncertainty factors therefore this study for the first time applies dynamic bayesian networks to risk analysis for real time flood control operations of multi reservoir systems the whole proposed method is to have a general reliability assessment of the real time flood control operation process the proposed method includes the following three major modules 1 a monte carlo simulation model that prepares basic data inputs for the dynamic bayesian network establishing using the historical floods and the operation models of the multi reservoir system 2 a dynamic bayesian network model which is established with expert knowledge and the relationships among the uncertainties and 3 a risk informed inference model for decision making to predict risk information about the operation schedules using the observed data and the trained dynamic bayesian network the rest of this paper is organized as follows section 2 introduces the basic theory of bns and dbns section 3 proposes the methodology including monte carlo simulations dynamic bayesian network model establishing and risk informed inference for decision making section 4 presents the data of a case study as well as the results and discussions of the proposed methodology finally we provide the conclusions of this work in section 5 2 introduction to bayesian network 2 1 static bayesian network bayesian network is a kind of probabilistic reasoning network based on bayesian conditional probability and graph theory hanea et al 2013 nodes in bayesian networks represent stochastic variables and arcs represent the causal relationships direct influences between the variables the uncertainties of stochastic variables are represented by the prior probabilities of the root nodes and the conditional probabilities of the non root nodes therefore the risk event is expressed as a directed acyclic graph dag composed of nodes and directed arcs more detailed definitions about bns can be found in the paper by pearl 1988 the risk analysis of a flood control system can be considered as the problem for the joint probability distribution function pdf i e the probability of the possible risk event defined by the stochastic variables uncertainties according to the basic probability theory the pdf of the stochastic variables is expressed as 1 f x 1 x n f 1 x 1 i 2 n f i 1 i 1 x i x 1 x i 1 where x i represents the stochastic variable n is the number of the stochastic variables f x 1 x n is the joint probability density function of all the variables x 1 x n f i is the marginal probability density function of x i f i 1 i 1 is the conditional probability density function of x i given x 1 x i 1 according to eq 1 the complexity of the joint probability distribution function f x 1 x n increases exponentially with the number of the stochastic variables the traditional risk analysis methods are not suitable to solve the above high dimension and non linearity joint probability distribution function however bayesian networks can achieve the compactness and reduce the dimension of the complex joint pdf by the independence and conditional independence between the stochastic variables from bayesian networks theory the joint probability density function of x 1 x n in eq 1 is simplified as 2 f x 1 x n i 1 n p x i π i g where π i represents the parent nodes of x i g refers to the directed acyclic graph of bayesian network p x i π i g is the conditional probability distribution function of x i given its parents π i and the directed acyclic graph g 2 2 dynamic bayesian network a static bayesian network models a system at a fixed time dynamic bayesian network extends the concept for the systems that change with time it provides a great deal of flexibility in modelling the probabilistic causal relationships among the stochastic variables over time a dynamic bayesian network consists of a number of static bayesian networks and the transfer networks between the static bns li 2009 the transfer network between two adjacent static bns is expresses as 3 p x t x t 1 i 1 n p x i t π i t where x t x 1 t x n t representing the set of n stochastic variables that compose the static bayesian network at time t x i t represents the node variable i at time t π i t represents the parent nodes of x i t at time t the parents of x i t not only exist in the current static bn but also exist in the previous static bn the full joint probability distribution of the nodes in a dynamic bayesian network is given by the product 4 f x 1 t t 1 t i 1 n p x i t π i t g where x is the set of n stochastic variables x 1 x n that compose the dynamic bayesian network t is the number of static bns 3 methodology as the real time floods and flood control requirements change with time the status of each node in the bayesian network also varies over time therefore the real time flood control operation processes of multi reservoir systems have to be modelled dynamically in this paper we propose a dynamic bayesian network for the multi reservoir system to have a general risk assessment of the real time flood control operation processes the status changes of the nodes in dynamic bayesian networks represent the processes that the stochastic variables change over time the framework of the proposed model is shown in fig 1 as presented in fig 1 the proposed model includes three components monte carlo simulations dynamic bayesian network establishing and risk informed inference for decision making the monte carlo simulations prepare basic data inputs for the dynamic bayesian network establishing based on the historical floods and operation models of the multi reservoir system the dynamic bayesian network is established with the expert knowledge and the results of the monte carlo simulations the component of risk informed inference for decision making is to perform probabilistic reasoning using the observed data and trained dynamic bayesian network considering the uncertainties the description for the components of the proposed model is shown in the following sections 3 1 monte carlo simulations the monte carlo simulations prepare basic data inputs for establishing the dynamic bayesian network the steps are as follows 1 generate the random samples of the stochastic reservoir inflows q i t and lateral inflows i c i t at downstream control points according to their probability distribution functions using the latin hypercube sampling lhs method manache and melching 2004 2 calculate the samples of the reservoir water levels storages and releases through reservoir flood routing based on the random samples of the reservoir inflows shown in step 1 the reservoir flood routing is carried out using the optimal flood control operation models of the reservoirs 3 obtain the samples of the flood flows at downstream control points through river flood routing using the muskingum method based on the samples of the lateral inflows shown in step 1 and the results achieved in step 2 3 2 dynamic bayesian network establishing the establishing of dynamic bayesian networks consists of two steps structure learning and parameter learning structure learning establishes the graph structures of the bayesian networks considering the dependence between the stochastic variables parameter learning is to estimate the prior probability distributions of the root nodes and the conditional probability distributions of the non root nodes in bayesian networks the description for the methods of the two steps is shown in the following sections 3 2 1 structure learning of dynamic bayesian networks the structure learning methods for bayesian networks generally fall into two categories i establishing the graph structures by expert knowledge and ii searching the space of possible graph structures using the training data as the relationship among the variables in the real time flood control operation of the multi reservoir system is explicit this study establishes the graph structures of the dynamic bayesian networks by expert knowledge the steps are as follows 1 selecting the stochastic variables as the nodes of dynamic bayesian networks the nodes of dynamic bayesian networks are chosen according to the topological structure of the flood control system and flood propagation in the system this study selects the following six kinds of stochastic variables as the nodes of the dynamic bayesian network the inflows q i t of reservoir i at time t the storages v i t of reservoir i at time t the discharges q i t of reservoir i at time t the water levels h i t of reservoir i at time t the lateral inflows i c i t at downstream control point i at time t and the flood flows q c i t at downstream control point i at time t 2 determining the relationships between the nodes in dynamic bayesian network based on the topological structure of the reservoir groups and flood propagation in the multi reservoir system the order of the nodes and the causal relationships between the nodes stochastic variables are determined 3 drawing the nodes and arcs according to the order in step 2 the nodes are added to the bayesian networks and the directed arcs are drawn according to the causality between the nodes stochastic variables 4 checking the proposed graph structures the established graph structures of the bayesian network are examined according to the characteristics of bayesian networks such as conditional independence 3 2 2 parameter learning of dynamic bayesian networks the prior probability distributions of the root nodes and the conditional probability distributions or tables cpts of the non root nodes in bayesian networks are learned from data achieved in the monte carlo simulations there are various parameter learning methods for bns such as the expectation maximization em algorithm conjugate gradient descent method and so on this paper uses the maximum likelihood estimation mle method for the parameter learning given the observed samples x x 1 x 2 x m the mle method calculates the parameters through maximizing the likelihood function 5 θ ˆ a r g m a x θ l x θ a r g m a x θ ln p x θ where l x θ is the likelihood function l x θ ln p x θ θ refers to the parameter θ ˆ is the maximum likelihood estimation of the parameter θ usually the continuous variables are problematic in dbns due to computational complexity because it is difficult to capture the relationships between the continuous variables other than the gaussian distribution dan and heckerman 1994 therefore the classical ways to deal with continuous variables in bns and dbns are assuming a joint parametric gaussian distribution or discretizing the variables monti and cooper 1998 most often the discretization methods are used because many continuous variables in the real world do not follow the gaussian distributions there are many discretization methods for the continuous variables in bns and dbns such as equal distance normalized equal distance equal frequency k means tree and so on we use the equal distance method which is one of the most commonly used approaches for data discretization especially for obtaining a discrete representation of the probability density function of the variable the equal distance method divides the range of the stochastic variable into m x i intervals and the intervals are equal sized the width w x i of the intervals is expressed as 6 w x i x i max x i min m x i where m x i is the number of the divided intervals of the variable x i x i max is the maximum in the range of the variable x i x i min is the minimum in the range of the variable x i 3 3 risk informed inference for decision making the component of risk informed inference for decision making is to perform probabilistic reasoning and risk calculation for the flood control operations under uncertainties based on the observed data and the trained dynamic bayesian network 3 3 1 risk informed inference using the dynamic bayesian network given an evidence on any of the nodes dbns can perform inference regardless of the directions of the arcs the inference of dbns usually consists of prediction simulation and diagnosis 1 prediction simulation is to infer from cause to effect given the observed values of some nodes in the dynamic bayesian network it predicts the values and their probabilities of other nodes for example given the forecasted reservoir inflows and lateral inflows it calculates the reservoir water levels and flood flows at downstream control points and it also simultaneously provides their possibilities of occurrence 2 diagnosis is to infer from effect to cause given the observed results it diagnoses the causes of the results for example given the risk event of reservoir overtopping it finds the possible reasons for the event prediction and diagnosis are identical computations in dbns they both compute the posterior probabilities of the nodes in dbns conditional upon evidence conrady and jouffe 2015 the underlying computational mechanism is based on bayes theory expressed as 7 p x e p e x p x p e where e refers to the evidence i e the observed values of the nodes in dynamic bayesian networks p x is the prior probability of x p e x is the likelihood probability p x e is the posterior probability i e the conditional probability of x given e this paper also uses the mle method for the inference algorithm given the observations evidence of some nodes dbns achieve the values of other nodes by updating the network at time steps therefore the real time flood control operation processes of multi reservoir systems are dynamically modelled by the updating of dbns 3 3 2 uncertainties and risk calculation this paper considers the uncertainty of reservoir inflows and lateral inflows of the downstream control points arising from the forecast errors reservoir storage capacity curve errors and reservoir discharge curve errors jiang 1998 chen et al 2017 the above uncertainties result in the randomness of the reservoir storages releases water levels and lateral inflows of downstream control points the randomness makes reservoir storages releases water levels and lateral inflows stochastic in occurrence and magnitude therefore the water level considering the influence of the uncertainties is called stochastic reservoir water level the flood flow of the downstream control point is defined as the flow that combines the upstream reservoir releases and lateral inflows of the downstream control point the upstream reservoir releases and lateral inflows of the downstream control point are stochastic due to the uncertainties which result in the randomness of the flood flow at the downstream control point therefore the flood flow of the downstream control point considering the influence of the uncertainties is called stochastic flood flow the stochastic reservoir inflows storages releases and lateral inflows of downstream control points are expressed as 8 q i t q i t ξ q i t v i t v i t ξ v i t q i t q i t ξ q i t i c i t i c i t ξ i c i t where ξ q i t is the forecasting error of reservoir i at time t q i t is the mean value of the stochastic inflow of reservoir i at time t ξ v i t is the storage error of reservoir i at time t v i t is the mean value of the stochastic storage of reservoir i at time t ξ q i t is the discharge error of reservoir i at time t q i t is the mean value of the stochastic release of reservoir i at time t ξ i c i t is the forecasting error of control point i at time t i c i t is the mean value of the stochastic lateral inflow of downstream control point i at time t the stochastic reservoir water levels and the stochastic flood flows at downstream control points are expressed as 9 h i t h i t ξ h i t q c i t q c i t ξ q c i t where ξ h i t is the water level error of reservoir i at time t h i t is the mean value of the stochastic water level of reservoir i at time t ξ q c i t is the flood flow error of control point i at time t q c i t is the mean value of the stochastic flood flow of control point i at time t the above uncertainties create risks in real time flood control decisions of multi reservoir systems in this study we focus on the risk of reservoir overtopping and the risk of flooding at downstream control points tung and mays 1981 chen et al 2017 expressed as 10 p r i t p h i t h r i t where p r i t is the risk of reservoir overtopping at time t h r i t is the selected safe water level of reservoir i at time t which can be the reservoir design flood water level or the reservoir dam crest height 11 p c i t p q c i t q c r i t where p c i t refers to the risk of flooding at downstream control point i at time t q c r i t is the selected safe flow at the downstream control point i at time t and it can be the safety flood flow the continuous variables water level and flood flow are discretized in the dbns for parameter learning the discretization is to replace a continuous variable by a number of discrete states the term h j i t represents the jth discrete state of the continuous variable water level of reservoir i at time t where j 1 to m h according to the inference of the dbns we can obtain the occurrence probability of each discrete state of the continuous variable expressed as p j h j i t the occurrence probability p j h j i t means there is a p j h j i t chance that the water level of reservoir i at time t is h j i t therefore the probability in eq 10 that the reservoir water level h i t is larger than the selected safe water level h r i t is equal to the summation of the occurrence probabilities whose discrete states are larger than h r i t expressed as 12 p r i t p h i t h r i t j 1 m h p j h j i t h r i t where m h is the number counted as h j i t h r i t j 1 to m h m h m h p j h j i t h r i t is the corresponding probability of h j i t h r i t i e p j h j i t where h j i t h r i t similarity the risk in eq 11 is transformed to 13 p c i t p q c i t q c r i t j 1 m q c p j q c j i t q c r i t where q c j i t represents the jth discrete state of the continuous variable flood flow at downstream control point i at time t where j 1 to m q c p j q c j i t is the corresponding occurrence probability of the discrete state q c j i t of the continuous variable q c i t p j q c j i t q c r i t is the corresponding probability of q c j i t q c r i t i e p j q c j i t where q c j i t q c r i t m q c is the number counted as q c j i t q c r i t j 1 to m q c m q c m q c therefore based on eq 12 and eq 13 the risk of reservoir overtopping and the risk of flooding at control points can be calculated using the inference results of dbns the main steps are as follows 1 obtain the forecasted reservoir inflows and lateral inflows considering forecast errors 2 find the discrete values corresponding to the above reservoir inflows and lateral inflows and then input them to the corresponding nodes of the dynamic bayesian network 3 perform the probabilistic reasoning using the inference algorithm of dynamic bayesian networks and obtain the reservoir water levels and flood flows at control points and also achieve their corresponding probabilities of occurrence 4 calculate the risk of reservoir overtopping and the risk of flooding at control points according to eq 12 and eq 13 respectively given the evidence of reservoir overtopping and flooding at control points the possible reasons can be founded through the following steps 1 obtain the evidence of reservoir overtopping or flooding at control points e g the values of reservoir water levels and flood flows 2 find the discrete values corresponding to the above evidence and then input them to the corresponding nodes of the dynamic bayesian network 3 perform the probabilistic reasoning using the inference algorithm of dynamic bayesian networks 4 obtain the values of all the other nodes in dbns and their corresponding probabilities of occurrence and then analyze the possible reasons for the risk event 4 case study 4 1 introduction to the study area the case study is performed in the middle reaches of the huaihe river basin in china as presented in fig 2 the huaihe river basin located about midway between the yellow river and yangtze is characterized by both a continental climate and a semi humid monsoon climate the multi reservoir system in the middle reaches of the huaihe river basin is composed of four large reservoirs nianyushan meishan xianghongdian and foziling and four downstream control points jiangjiaji hengpaitou runheji and zhengyangguan the generalized configuration of the flood control system is shown in fig 3 the main features of the reservoirs are shown in table 1 the safety flood flows of the jiangjiaji hengpaitou runheji and zhengyangguan control points are 3580 m3 s 8400 m3 s 8000 m3 s and 10000 m3 s respectively 4 2 data for the proposed methodology 1 inflows to the reservoirs and lateral inflows we use an observed flood in 2003 for the case study fig 4 shows the inflows to the reservoirs and lateral inflows between reservoirs and downstream control points the time interval is 1 h and the time duration of the flood is 168 according to the monte carlo simulation steps in section 3 1 generate 1000 random samples of reservoir inflows and lateral inflows according to their probability distributions in this paper the forecast errors of reservoir inflows and lateral inflows are assumed to follow the normal distributions it is because that the parameters of the hydrological forecast model are acquired from the statistical methods whose estimation techniques are optimal in the mean square error sense therefore the forecast errors are assumed to follow the normal distributions which have been evaluated in many literature and recommended by the ministry of water resources mwr of china mwr 2000 li et al 2010 the relative forecast errors of reservoir inflows and lateral inflows are expressed as n 0 0 20 2 2 nodes of the dynamic bayesian networks according to the steps in section 3 2 we select the following stochastic variables as the nodes of the dynamic bayesian network for the multi reservoir system as shown in table 2 the data for the nodes in table 2 are discretized for parameter learning according to the equal distance method shown in section 3 2 4 3 results 4 3 1 establishing of the dynamic bayesian networks as table 2 shows there are 24 nodes at time t for the risk analysis of real time flood control operation of the multi reservoir system therefore it is difficult to obtain the joint probability distribution of these stochastic variables by the traditional risk analysis methods we resolve the problem through dynamic bayesian networks the graph structures of the dynamic bayesian network are established according to the steps in section 3 2 as presented in fig 5 as fig 5 shows the dynamic bayesian network is composed of t time slices and t represents the time duration of the whole flood process each time slice represents a static bayesian network the solid arrows in the networks show the direct connections between the nodes in the same time slice the dotted arrows refer to the connections between the nodes in adjacent time slices the time between consecutive time slice is 1 h therefore the whole flood event from time 1 to time t is represented by the dynamic bayesian network the parameters i e the prior probabilities of the root nodes and the cpts of the non root nodes are learned from data using the bayesialab software based on mle method bayesialab is an automatic learning software for bns developed by dr lionel jouffe and dr paul munteanu the users can download the software from http www bayesia com 4 3 2 numerical experiments using the established dbn we conduct six numerical experiments to show the risk informed inference using the established dbn this study also uses bayesialab for implementing the inference the detailed information and results of the six numerical experiments are shown as follows 1 prediction experiment 1 assessing the impact of uncertainty of reservoir inflows and lateral inflows of the control points in this study the mean values of reservoir inflows and lateral inflows at control points are as follows q 1 t p 144 m3 s q 2 t p 242 m3 s q 3 t p 118 m3 s q 4 t p 109 m3 s i c 5 t p 917 m3 s i c 6 t p 884 m3 s i c 7 t p 858 m3 s and i c 8 t p 143 m3 s taking the time t p of the largest flood flow at zhengyangguan control point as an example according to eq 8 it is assumed that the forecast errors of reservoir inflows and lateral inflows are equal to their standard deviations i e q 1 t p q 1 t p σ q 1 t p 172 m3 s q 2 t p 290 m3 s q 3 t p 142 m3 s q 4 t p 130 m3 s i c 5 t p i c 5 t p σ i c 5 t p 1100 m3 s i c 6 t p 1060 m3 s i c 7 t p 1030 m3 s and i c 8 t p 171 m3 s given these forecast errors of reservoir inflows and lateral inflows the water levels and flood flows at downstream control points are predicted as well as their probabilities of occurrence we input the above reservoir inflows and lateral inflows with forecast errors to the corresponding nodes of the dynamic bayesian network and perform the inference fig 6 shows the reservoir water levels and flood flows of control point with their probabilities of occurrence taking xianghongdian reservoir and hengpaitou flood control point as examples according to fig 6 the risk of reservoir overtopping and the risk of flooding at control points are calculated based on eq 12 and eq 13 as shown in fig 7 experiment 2 assessing the impact of reservoir release uncertainty in this study the mean values of reservoir releases are as follows q 1 t p 486 m3 s q 2 t p 1490 m3 s q 3 t p 582 m3 s and q 4 t p 261 m3 s taking the time t p of the largest flood flow at zhengyangguan control point as an example according to eq 8 it is assumed that the reservoir release error is equal to its standard deviation i e q 1 t p q 1 t p σ q 1 t p 490 m3 s q 2 t p 1510 m3 s q 3 t p 591 m3 s and q 4 t p 263 m3 s we input the above values to the corresponding nodes of the dynamic bayesian network and perform the inference fig 8 shows the reservoir water levels and flood flows of control point with their probabilities of occurrence taking xianghongdian reservoir and hengpaitou flood control point as examples according to fig 8 the risk of reservoir overtopping and the risk of flooding at control points are calculated based on eq 12 and eq 13 as shown in fig 9 experiment 3 assessing the impact of reservoir storage uncertainty in this study the mean values of reservoir storages are as follows v 1 t p 627 84 106 m3 v 2 t p 1830 62 106 m3 v 3 t p 2004 22 106 m3 and v 4 t p 381 58 106 m3 taking the time t p of the largest flood flow at zhengyangguan control point as an example according to eq 8 it is assumed that the reservoir storage error is equal to its standard deviation i e v 1 t p v 1 t p σ v 1 t p 628 48 106 m3 v 2 t p 1833 99 106 m3 v 3 t p 2014 40 106 m3 and v 4 t p 382 01 106 m3 we input the above values to the corresponding nodes of the dynamic bayesian network and perform the inference fig 10 shows the reservoir water levels and flood flows of control point with their probabilities of occurrence taking xianghongdian reservoir and hengpaitou flood control point as examples according to fig 10 the risk of reservoir overtopping and the risk of flooding at control points are calculated based on eq 12 and eq 13 as shown in fig 11 experiment 4 evaluating the impact of the combination of all the uncertainties we input all the uncertainties in experiments 1 2 and 3 to the corresponding nodes of the dynamic bayesian network and perform the inference fig 12 shows the reservoir water levels and flood flows of control point with their probabilities of occurrence taking xianghongdian reservoir and hengpaitou flood control point as examples according to fig 12 the risk of reservoir overtopping and the risk of flooding at control points are calculated based on eq 12 and eq 13 as shown in fig 13 2 diagnosis experiment 5 given the reservoir water level predicting the inflows according to eq 9 it is assumed that the reservoir water level errors are equal to their standard deviations i e h 1 t p h 1 t p σ h 1 t p 109 46 m h 2 t p 134 80 m h 3 t p 136 48 m and h 4 t p 124 89 m we input the above values to the corresponding nodes of the dynamic bayesian network and perform the inference fig 14 a shows the reservoir inflows with their probabilities of occurrence taking nianyushan reservoir as an example the risks that the reservoir inflows exceed the selected inflows are in shown in fig 14 b experiment 6 given the flood flows at control points predicting the reservoir inflows and lateral inflows according to eq 9 it is assumed that the flood flow errors are equal to their standard deviations i e q c 5 t p q c 5 t p σ q c 5 t p 2660 m3 s q c 6 t p 1620 m3 s q c 7 t p 3200 m3 s and q c 8 t p 3580 m3 s we input the above values to the corresponding nodes of the dynamic bayesian network and perform the inference fig 15 a shows the reservoir inflows with their probabilities of occurrence taking nianyushan reservoir as an example the risks that the reservoir inflows exceed the selected inflows are in shown in fig 15 b 4 3 3 effect of uncertainty level on the results in order to evaluate the effect of the uncertainty level on the results we assume that the forecast errors of reservoir inflows and lateral inflows are equal to zero s1 their standard deviations s2 twice of their standard deviations s3 respectively we input the values with the above uncertainties to the corresponding nodes of the dynamic bayesian network and perform the inference respectively fig 16 shows the risks of flooding at control points with different uncertainty levels taking hengpaitou as an example 4 4 discussions 4 4 1 discussions of the results according to figs 6 8 10 and 12 we can know the reservoir water levels and flood flows at control points and their corresponding probabilities of occurrence considering the forecast errors of reservoir inflows and lateral inflows reservoir release uncertainty reservoir storage uncertainty or the combination of these uncertainties for example the probability that the flood flow of hengpaitou control point at time t p is between 1480 and 1610 m3 s is 48 7 as shown in fig 6 that means there is a 48 7 chance that the flood flow is between 1480 and 1610 m3 s similarity there is a 20 7 chance that the flood flow at hengpaitou control point is between 1610 and 1730 m3 s and there is a 16 2 chance that the flood flow is between 1730 and 1850 m3 s according to figs 7 9 11 and 13 we can estimate the risk of reservoir overtopping and the risk of flooding at control points for example the probability that the flood flow of hengpaitou control point exceeds 1730 m3 s is 16 8 due to the forecast errors of reservoir inflows and lateral inflows as shown in fig 7 it is noted that the selected safe values of reservoir water levels and flood flows affect the risk of reservoir overtopping and the risk of flooding at control points the risk of reservoir overtopping decreases as the selected water level increases the risk of flooding at control points also reduces when the selected flood flows increases according to figs 14 and 15 we can calculate the probability that the stochastic reservoir inflows are above the selected inflows given the risk event that the stochastic reservoir water level exceeds the selected water level or the flood flow at the control point exceeds the selected flood flow for example given the risk event that the stochastic water level of nianyushan reservoir exceeds its mean value h 1 t p h 1 t p σ h 1 t p 109 46 m the probability that the inflows of nianyushan reservoir exceed 219 m3 s is 0 5 as shown in fig 14 the uncertainty level affects the risk of reservoir overtopping and the risk of flooding at control points for example the risk of flooding at hengpaitou control point increases with the uncertainty level of reservoir inflows and lateral inflows under the same selected flood flows as shown in fig 16 4 4 2 comparison with static bns and monte carlo method in order to evaluate the efficiency of the proposed method we compare the risks obtained by dbns with the results from static bns and monte carlo mc method fig 17 shows the comparison of reservoir overtopping risks derived from dbns and bns taking xianghongdian reservoir as an example as fig 17 shows the risks of reservoir overtopping under the low water level design flood water level and overtopping flood water level are all the same for bn and dbn but the risk of dbn is smaller than that of bn under medium water level it is because that dbn method performs inference with more information than the bn approach i e the connections between adjacent time slices fig 18 shows the comparison of the curves between reservoir overtopping risks and selected water levels obtained from dbns and mc method taking xianghongdian reservoir as an example as fig 18 shows the risks obtained from the dbn method approximate to that of the mc method the maximum deviation is 2 8 as shown in the points b 136 35 52 3 and a 136 35 49 5 the difference is mainly due to the discretization precision of the continuous variables in the dbns and the sample size of the mc method in addition most of the risks obtained from the dbn method are slightly larger than that of the mc method it means that the mc method may underestimate the risks which is not safe for the flood control operations and decision making 4 4 3 application and limitations of the proposed model the proposed method can be served as a risk informed decision making tool under uncertainty in the real time flood control operations of a multi reservoir system the risk analysis results provide fundamental risk information to the decision makers through the prediction and diagnosis of the established dynamic bayesian network the risk information includes the probability of reservoir overtopping the probability of flooding at control points and the integrated risk of the whole multi reservoir system under the uncertainties and the possible causes for given risk events the decision makers can have an insight into the vulnerabilities of the multi reservoir systems and improve the real time flood control operations according to the risk information for example if the probability of reservoir overtopping is greater than the risk acceptable to the decision makers then they can improve the operating schedules further research can be conducted on the effect of the discretization precision of the continuous variables in the dbns xue et al 2016 the other problem of the discretization methods is to balance the desire for high accuracy of the risks with the calculation burden to obtain the results the type of evidence can also be evaluated e g the usage of probabilistic evidence or soft evidence 5 conclusions there are many uncertainties in the real time operation of multi reservoir systems which create risks in flood control decisions we proposed a method for risk analysis of the real time flood control operation of a multi reservoir system using a dynamic bayesian network we applied the method to a multi reservoir flood control system in the middle reaches of the huaihe river basin in china the main points are summarized as follows 1 we established a dynamic bayesian network for the risk analysis of the real time flood control operation of a multi reservoir system the dynamic bayesian network was built with expert knowledge and parameter learning using the data from the monte carlo simulations 2 we performed the risk informed inference of prediction for decision making using the established dynamic bayesian network the prediction inference results of the dbn can provide the possible reservoir water levels and flood flows at control points and their corresponding probabilities of occurrence considering the forecast errors of reservoir inflows and lateral inflows reservoir release uncertainty reservoir storage uncertainty or the combination of these uncertainties the inference results can also estimate the risk of reservoir overtopping and the risk of flooding at control points 3 we performed the risk informed inference of diagnosis for decision making using the proposed dbn the diagnosis results can provide the probability that the stochastic reservoir inflows are above the selected inflows given the risk event that the stochastic reservoir water level exceeds the selected water level or the flood flow at the control point exceeds the selected flood flow 4 the uncertainty level as well as the selected water levels and flood flows affect the risk of reservoir overtopping and the risk of flooding at control points the risks increase with the uncertainty level and decrease with the selected water levels and flood flows 5 the proposed method can be served as a risk informed decision making tool under uncertainty in the real time flood control operations of a multi reservoir system for example if the probability of reservoir overtopping is greater than the risk acceptable to the decision makers then they can improve the operating schedules 6 further research can be conducted on the effect of the discretization precision of the continuous variables in the dbns and on the impact of the comprehensive uncertainty factors in real time flood control operations acknowledgments this study is supported by the national key r d program of china china grant 2017yfc0405606 national natural science foundation of china china grant 51579068 fundamental research funds for the central universities china grant 2018b18214 national natural science foundation of jiangsu province china grant bk20180509 china postdoctoral science foundation china grant 2017m621612 and postdoctoral science foundation of jiangsu province china grant 2018k127c the users can access the data used in this paper from https www zenodo org record 1164432 wnq6kvmdhjc appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 007 
26302,this paper investigates the applicability of model emulation to speed up simulation time of cpu intensive environmental models polynomial chaos expansion pce emulators are constructed for three case studies of increasing complexity the level of emulator training and the order of polynomial necessary to sufficiently build accurate emulators for each model are investigated although the pce emulators shown here do not approximate well the outputs of parameter rich models 80 parameters results demonstrate that the emulators mimic closely outputs of relatively simple low dimensional simulation models 15 parameters or less furthermore the pce emulators are tested with applications such as global sensitivity analysis gsa results illustrate the advantages and drawbacks of using classical pce emulators for treating computational limitation of complex environmental models keywords model emulation surrogate modeling polynomial chaos expansion sensitivity analysis 1 introduction and scope environmental models are dynamic representations of the atmosphere ocean ice and land described in coupled systems to analyze and predict short and long term earth system behavior claussen et al 2002 wood et al 2011 hurrell et al 2013 typically obtaining better accuracy in these models comes at the expense of dimensionality and complexity and thus a reduced computational efficiency dunne et al 2012 arora et al 2013 state of the art weather and climate prediction models contain a large number of parameters to simulate the exchange of energy momentum and mass between the land surface and overlying atmosphere noilhan and planton 1989 bastidas et al 1999 masson et al 2003 sargsyan et al 2014 assessment of the model parameters is a necessary step before the model can be ran for operational use gupta et al 1999 rosolem et al 2013 yet the cpu time of simulation models can vary from less than a second for simple dynamic models to many hours of calculation for spatially explicit models therefore there is an immense cpu and time cost associated with investigating model parameters since many thousands of runs are typically required for using methods such as global sensitivity analysis or parameter estimation with markov chain monte carlo mcmc certainly computational limitations still remain a major obstacle to the effective use of environ mental models in decision making ratto et al 2012 and emulation methods have been developed in an attempt to reduce the time cost of such models emulation is an important and growing field of research that signifies a major achievement in the study of complex mathematical models ratto et al 2012 razavi et al 2012 these methods define surrogate responses of the original model outputs at a strongly reduced computational cost the concept of emulation dates back to blanning 1975 and kleijnen 1975 and has been greatly evolving in recent years the central idea is that the original complex model has a response surface for its outputs for a pre defined parameter range and this response surface has a mathematical representation that the emulation methods are built to capture presented and implemented here are polynomial chaos expansion pce emulators ghanem and spanos 1991 xiu and karniadakis 2002 le mâıtre and knio 2010 which utilize orthogonal polynomials to mimic the parameter effects on model outputs and surrogate models are built for various environmental models of increasing complexity there are various algorithms and methods other than pce used for emulation and surrogate modeling and for many different types of applications the set of methods referred to as the design and analysis of computer experiments dace is very popular sacks et al 1989 santner et al 2013 levy and steinberg 2010 and books reviews and journal special issues have been written on this subject van gigch 1991 fang et al 2005 kuhnt and steinberg 2010 kleijnen 2010 the dace method is also known as gaussian process gp or kriging metamodeling di pierro et al 2009 for example conti and o hagan 2010 applied a gp emulator to mimic a multi output and dynamic computer simulation model the sheffield dynamic global vegetation model and machac et al 2016 used a similar method to create a surrogate model for an urban hydrodynamic drainage simulator another emulation method uses radial basis functions rbf such as the techniques used in bliznyuk et al 2012 and regis and shoemaker 2007a 2007b furthermore artificial neural networks ann and support vector machines svm are also emulation techniques and have been used in many studies such as the emulation of a hydrologic model zhang et al 2009 the identification of controlling mechanisms in transpiration of a pine tree vrugt et al 2002 or the estimation of regional variation in particulate organic carbon to nitrogen ratio in the surface ocean martiny et al 2013 in keating et al 2010 prediction uncertainty of a highly complex cpu intensive groundwater model was explored using a simplified surrogate model emulators have been used for data assimilation and parameter estimation using a probabilistic collocation based kalman filter pckf which combines the kalman filter with pce emulators saad and ghanem 2009 man et al 2016 fan et al 2016 additionally laloy et al 2013 jointly use a pce emulator of the original model and a dimensionality reduction using the karhunen love tranform of the parameter space to speed up the calibration of a groundwater model zeng et al 2016 used similar methods to build pce emulators of a groundwater model using stochastic collocation and sparse grids a goal of the study is to investigate the applicability of pce based emulators for mimicking complex environmental and earth system models esms first two environmental models of low and medium complexity are considered these two cases involve a conceptual rainfall runoff model hmodel shown in schoups and vrugt 2010 and a soil tree atmosphere continuum stac model that simulates hydrodynamic processes of a single tree shown in rings et al 2013 finally an emulator is created for an esm a version of the community land model clm4 5 ed shown in fisher et al 2015 for all case studies the polynomial emulators are built using training parameters sampled from sparse grid approximation smolyak 1963 xiu and hesthaven 2005 nobile et al 2008a 2008b sparse grids are numerical techniques to represent integrate or interpolate high dimensional functions and can be useful in the application of pce emulators to sample the locations of the training runs the user chooses the level of sparse grid approximation where an increasing level denotes an increased density of sampling which usually indicates a stronger emulator tested are 1st and higher level sparse grids as well as 1st and higher order polynomial emulators and the level of training and order of polynomial necessary to sufficiently build accurate emulators is investigated this study analyzes the accuracy efficiency and fidelity of the emulators compared to their original counterparts the remainder of this paper is structured as follows section 2 describes the pce method used to build the emulators section 3 introduces the models and data of each case study and report the corresponding results of the pce emulators this is followed in section 4 by a general discussion of the main implications of the collective results finally section 5 presents a summary of the most important findings and offers concluding remarks 2 materials and methods 2 1 model formulation this section briefly reviews the building blocks of the pce method that is used herein to emulate the output of different ecosystem models consider a n vector of measurements y y 1 y n observed at discrete times t 1 n that summarizes the response of an environmental system s to k temporally variant control inputs b b 1 b n with column elements b t b t1 b tk consider a computer model f to explain the observed data 1 y f x ζ 0 b e where x x 1 x d is the d 1 vector of model parameters ζ 0 stores the values of the state variables at the start of simulation b signifies the control matrix with temporal measurements of the forcing variables and e e 1 e n is a vector of residuals 2 e f x ζ 0 b y y f x ζ 0 b the index t for time takes on strictly positive integer values in the remainder of this paper t 1 n n yet may take on real values t 0 n r in the actual system model f to resolve for continuous time processes wherein the simulated output at t 0 is defined completely by ζ 0 a convenient assumption is made that the forcing data are observed without measurement error and that errors in the initial states b pose no harm as their impact on the simulated output y diminishes rapidly with advancing time this latter assumption is certainly appropriate for environmental systems controlled by negative or degenerative feedback the assumptions of perfect input data and initial states due to spin up period are common to environmental modeling this ideal case leaves as the only unknowns the model parameters without further loss of generality restrict the model parameters to a closed space χ equivalent to a d dimensional hypercube x χ r d called the feasible parameter space note that the tilde operator is used for the initial state and forcing data to signify explicit use of measured values the large computational requirements of complex system models complicates tremendously tasks such as parameter estimation via nonlinear optimization or statistical inference as the resulting inverse problem may require many successive executions of the model this becomes particularly cumbersome for parameter rich models emulation methods may help simplify parameter estimation by using a fast running surrogate model of the original complex cpu intensive process model the next section reviews one of such emulation methods namely pce note curly brackets are used to differentiate between random variables and their actual sampled values thus x 1 x t stores a sequence of t different training points draws of the model parameters x 2 2 model emulation using generalized polynomial chaos expansion the pce method emulates the output y of the original model in equation 1 via polynomial approximation this emulator can be re written as follows 3 y f p x where p signifies the order of the polynomial e g p 1 for a first order polynomial emulator as detailed in the previous section ignore measurement uncertainty of the initial states and forcing variables and thus assume that the model s ability to describe the observed data y is determined only by the d values of the model parameters x consequently only the parameter values are needed as input by the emulator to replicate the response of the original model the pce emulator approximates the model output in the following manner 4 y f p x j 1 m a j ψ j x where a a 1 a m is a m vector of yet unknown deterministic expansion coefficients m signifies the number of d dimensional orthogonal polynomials and each ψ j x ψ j 1 x 1 ψ j d x d is the product of one dimensional polynomials for expansion terms j 1 m thus each model parameter x i impacts ψ j in their own specific way the number of orthogonal polynomials m depends on the polynomial order p of the emulator and the dimensionality d of the parameter space χ according to m p d p d where the symbol denotes factorial table 1 shows the number of polynomials for each emulator considered in this study in this study the values of the coefficients a are estimated via spectral projection of the original model response y x against each individual basis function e g see xiu 2007 5 a j k 1 t w k ψ j x k f x k ζ 0 b ψ j 2 x where x x 1 x t is a t d matrix of t different realizations of the parameter vector x and the w k s denote the corresponding weights of an integration a cubature rule on the feasible parameter space χ on r d the denominator ψ2 x in equation 5 is equivalent to the inner product of the jth multivariate orthogonal polynomial and can be computed as follows 6 ψ j 2 x i 1 d ψ j i 2 where j 1 m the d univariate inner products ψ 2 of each jth polynomial where i 1 d right hand side of equation 6 have a simple closed form analytic solution therefore the computational cost of pce is determined by the time it takes to evaluate the forward model f x k ζ 0 b for each of the t training parameter vectors x k of x in equation 5 where k 1 t now combine equations 5 and 6 the approximate pce emulator reads as follows 7 f p x j 1 m a j ψ j x interested readers are referred to xiu 2007 and laloy et al 2013 for a more detailed description of spectral projection methods within the context of pce the use of a cubature integration rule in equation 5 causes f p x in equation 4 to differ from ζ 0 in equation 7 this is also referred to as the aliasing error xiu 2007 and decreases with t in equation 5 in theory the larger the values of p and t the better the emulator output y y f x ζ 0 b e should approximate the simulations y x of the original forward simulator f x ζ 0 ζ 0 for random parameter vector x the rate of convergence of y x to y x depends on the regularity of f x ζ 0 ζ 0 the smoother the response of the forward model the faster y x will approximate y x in practice however computational requirements impose restrictions on the values of p and t thus the available computational budget dictates the accuracy of the surrogate model f p to maximize the capabilities of the emulator it would be desirable to select wisely x the matrix of t training samples spectral projection has the advantage that it requires far fewer parameter vectors than linear regression for a well posed solution of the pce expansion coefficients in equation 7 the results section briefly shows the sparse grid method used to generate the matrix of training realizations 2 3 sensitivity analysis of model parameters global sensitivity analysis aims at quantifying the contributions of input variables to the variance of the outputs of a physical model among the abundant literature on global sensitivity analysis sobol s method sobol 1990 has received much attention since it provides accurate information when calculating the importance index of model parameters however the full description requires the evaluation of 2 n monte carlo integrals sudret 2008 which is not practically feasible unless n is low n here represents the dimensionality of the model or the number of active parameters the fourier amplitude sensitivity test fast cukier et al 1973 is another popular sensitivity analysis technique that is computationally efficient and can be used effectively for nonlinear and nonmonotonic models sudret 2008 xu and gertner 2011 fast uses a periodic approach to sample the parameter space and a fourier transformation to decompose the variance of a model output into partial variances contributed by different model parameters the fast methodology was used in massoud et al 2019 scientific reports in review to evaluate the sensitivity index of the 87 parameters of the clm model used in this study in their sensitivity analysis the top 10 sensitive parameters were pinpointed for each model output and this information will be used in the case study here to reduce the dimensionality of the model in order to build more efficient emulators more on this in the results section the fidelity of the emulators to their original model counterpart is investigated where the sensitivity of the model parameters in the emulators is compared to those of the original models in theory if the performance of the emulator is identical to that of the original model then the parameters of the emulator should have the same sensitivity index as those of the original model many studies assume that emulators provide efficient sensitivity analysis for large and expensive models crestaux et al 2009 ratto et al 2012 however this may not always be true as emulators may provide anomalous parameter sensitivity estimates in one study the authors investigated the joint application of emulators and their respective parameter sensitivities and showed that with enough training the emulator will match the original model in the parameter sensitivity rank and magnitudes borgonovo et al 2012 however the environmental model used in borgonovo et al 2012 only contained 12 parameters other studies have demonstrated the use of emulation for applications of sensitivity analysis or parameter calibration of higher order models e g lu et al 2018 ricciuto et al 2018 but the comparison of the results obtained from the emulators to those obtained from the original model was not investigated in this study the claim that it is safe to assume the sensitivity index obtained from the emulator represents the true importance of that parameter in the original model is investigated this will indicate that the processes represented in the original model are being replicated properly in the emulator the fast method of xu and gertner 2007 2011 is used to evaluate the parameter sensitivities of the original model and the emulators therefore other than simply checking the accuracy of the emulators the total sensitivity of each models parameters to the parameters in the surrogate models is compared to test the fidelity of the emulators to their original counterparts 3 results the results of each case study is presented separately each discussing the results of a specific model first the results of the sparse grid approximations are shown which ultimately determine the location of the training runs for each emulator then the emulation results from each case study is presented the model of interest is first discussed in each subsection followed by a description of the model parameters finally the emulation results using different polynomial emulators and sparse grid levels are presented for each case study 3 1 sparse grid approximation sparse grid approximation methods reduce drastically the required number of parameter vectors to construct an emulator while preserving a high accuracy for moderately large dimensional parameter spaces smolyak 1963 xiu and hesthaven 2005 nobile et al 2008a nobile et al 2008b sinsbeck and nowak a pth order pce emulator can be built with sparse grids of different levels l and thus variable number of training data points thus a sparse grid of level l does not necessarily equate to a p lth order pce emulator in general the larger the order of the polynomial emulator the more training points are required for its calibration yet this does not guarantee that the emulator will closely approximate the output of the original forward model thus emulators of different polynomial orders using a range of sparse grid levels are built tested and contrasted fig 1 depicts graphically training data pairs x 1 x 2 for a hypothetical two parameter model a gauss patterson sparse grid is used and the results for levels l 1 up to l 6 are all presented the higher the order l of the sparse grid the more training data points indeed with l 1 only tr 5 samples are required to build the pce emulator and this number increases to tr 769 for l 6 see fig 1 table 2 lists the number of training samples associated with sparse grids of levels l 1 l 2 and l 3 for the three forward models considered in this paper details of these models appear in the case study section and readers are referred to this part of the paper for further information the first case study considers a d 7 parameter conceptual watershed model named hmodel the first level sparse grid of this model corresponds to t 15 tr s whereas a second and third level grid increases the number of training points to t 127 and t 799 respectively the cpu efficiency of the model warrants emulation using all three different grid levels the second case study involves simulation of water transport using a soil tree atmosphere continuum or stac model this model has d 15 different parameters a first level sparse grid would require only t 31 tr s and this number increases rapidly to t 511 and t 5 925 for a second and third order level due to the somewhat large cpu demands of stac construction of the emulator is limited to a first and second level sparse grid lastly the third case study considers clm this model is rather complex yet the emulators consider herein only the d 87 parameters of interest to dynamic vegetation modeling a first order sparse grid equates to t 175 tr s whereas the second and third level grids necessitate 15 487 and t 2 044 416 executions of clm therefore the deliberate choice for this model is l 1 as the second and third grid levels demand too much time indeed a single forward run of clm takes about 8 h using serial processing also considered separately is a refined clm parameterization in which only the ten most sensitive parameters are allowed to vary this alternative parameterization coined clm 10 is desirable as it requires far fewer model evaluations for each of the three sparse grids the sensitive parameters were chosen through a global sensitivity analysis in which the parameters with the top 10 highest sensitivity index were chosen for each output of interest in summary all three levels of the sparse grid are tested for the hmodel l 1 and l 2 for the stac model and only l 1 for clm furthermore for the hmodel and stac model polynomial emulators of first second and third order p 1 2 3 are examined these emulators are coined lxpyy where x denotes the sparse grid level and yy signifies the polynomial order of the emulator thus l1p1 signifies a first order polynomial emulator built using a sparse grid of level one l2p3 thus is equivalent to a third order polynomial emulation using a second order sparse grid 3 2 case study 1 the rainfall runoff transformation the first case study simulates the rainfall runoff transformation of the guadalupe river at spring branch in texas using the hmodel conceptual watershed model the model transforms rainfall into discharge at the watershed outlet using explicit process descriptions of interception throughfall evaporation surface runoff percolation and surface and subsurface routing a detailed description of the hmodel structure and processes representations can be found in schoups and vrugt 2010 their study also summarizes the d 7 parameters of the hmodel and their prior uncertainty ranges a six year record of daily discharge mm day mean areal precipitation mm day and mean areal potential evapotranspiration mm day of the guadalupe river derived from the mopex data set duan et al 2006 was used for this case study details about the basin experimental data and prior distribution can be found in schoups and vrugt 2010 table 3 summarizes the results of this analysis and lists the cpu time and the root mean square error rmse and correlation values using first second and third order polynomial emulators with sparse grids of level one two and three the hmodel l3p3 emulator and the original hmodel are compared with observed data in fig 2 using 100 random parameter sets from the prior distribution both the original model and the emulators were able to capture the dynamics in the observed data quite well note a truncated pce is known to approximate the variance from below and this is neatly visible in the plot i e original model variance matches emulator variance although this holds true only for representing the prior variance and not the posterior variance since the posterior may be located in a totally different region of the parameter space and thus might result in a different variance for the simulations 3 3 case study 2 soil tree atmosphere continuum stac model the second case study involves the stac model which is a physically based nonlinear modeling framework that simulates water flow in the combined soil and tree systems rings et al 2013 the model discretizes the system domain and couples the soil with the tree domain simulating the soil roots and tree trunk as a continuum the stac model utilizes the hydrus model simuunek et al 2008 where water flow through the soil and the tree root system and stem is driven by the evaporative demand and soil available water leading to a gradient in soil and xylem water potentials throughout the system the stac model contains 15 parameters and emulators with 1st and 2nd level sparse grid sampling are built table 2 there are at least m 4 outputs for the stac model including sapflux stem potential soil storage and tree storage however the focus of this study is solely on the sapflux output in cm day 1 in an attempt to emulate the response for t 769 time steps representing 30 min intervals during a 17 day period measured data include soil water content and water potential in three spatial dimensions in the root zone tree stem water content and sapflux canopy water potential and atmospheric variables such as net radiation air temperature and humidity providing the necessary information for potential tree evapotranspiration the data was collected in and around a mature douglas fir abies concolor in a 99 ha subcatchment p301 of the king s river experimental watershed krew located in the rain snow transition zone of the southern sierra nevada mountain range in california the data collection was part of the southern sierra critical zone observatory czo established to improve understanding of surface and subsurface processes along a gradient of elevation energy water and soil bales et al 2011a 2011b the douglas fir czt 1 is located along a ridge in a relatively open area of the forest at an elevation of 2018 m bales et al 2011b near a meteorological station the mature tree is about 30 m high with a trunk diameter of 0 55 m table 4 summarizes the results of this analysis and lists the cpu time and the rmse and correlation values using first second and third order polynomial emulators with sparse grids of level one and two the stac model l1p1 and l2p1 emulators as well as the original stac model are compared with observed data in fig 3 left using a parameter set that is representative of a mature douglas fir then 100 random parameter sets are drawn from the prior distribution to compare the l1p1 emulator with the original stac model fig 3 right both the original model and the emulators were able to capture the dynamics in the observed data quite well with a bit of noise being introduced in the l2p1 simulations light blue curve in fig 3 3 4 case study 3 the community land model with ecosystem demography clm4 5 ed the esm considered in this study is the clm4 5 ed clm is a community based open source model that is widely used for understanding climate vegetation interactions clm is the land surface model used within various esms including the community earth system model cesm and the norwegian earth system model noresm lawrence et al 2011 bonan et al 2011 the ecosystem demography ed concept is a method for scaling the behavior of forest ecosystems by aggregating individual trees into representative cohorts based on their size and plant type or pft and by aggregating groups of cohorts into representative patches conceptually similar to a forest plot which explicitly tracks the time between disturbances moorcroft et al 2001 the ed component is the most advanced dgvm incorporated into the clm framework the main property of the ed concept that differs from most commonly used big leaf models is the capacity to predict distributions and compositions of plants directly from their given physiological traits described by the model parameterization fisher et al 2015 clm ed simulates growth by integrating photosynthesis across different leaf layers for each cohort and mechanistic mortality is simulated based on plant carbon starvation and hydraulic failure in addition to a background mortality rate mortality from tree fall impacts and fire the model allocates photosynthetic carbon to different tissues such as leaf root and stem based on the allometry of different tree species clm ed can be simulated at different modes including point mode for sites regional mode for watershed or regional scale and global mode for continental and global scale see supplementary model description in fisher et al 2015 for details on specific components of the model structure the clm ed version considered here contains 87 parameters making it only acceptable to build an emulator with 1st level sparse grid sampling but for a 2nd level sparse grid emulator the number of training runs already become too large for this analysis the runs are initialized i e ζ 0 with a bare ground or a state with no vegetation the climate conditions i e b for this site are from qian et al 2006 representative of data from 1948 to 1972 the model is simulated for a site in the state of par a the amazon brazil 7 s 55 w there are dozens of possible outputs for the clm and here the focus is on m 3 outputs including gross primary production gpp kgc m 2 yr 1 leaf area index lai and overall biomass biomass kgc m 2 in an attempt to emulate the response for t 960 time steps representing 1 month intervals during an 80 year period fig 4 shows the original model and the emulator simulations fig 5 shows the simulations for the clm for the entire simulation period of 80 years each of the emulators is compared to the original model 3 4 1 87 parameter emulator an l1p1 emulator is built for the 87 parameter clm requiring a total of 175 training sets the simulations spanned 80 years and the cpu cost of the original model is roughly 8 h therefore the emulator training cost a total of 1400 cpu hrs performed in parallel simulations on the conejo supercomputer at the los alamos national laboratory after obtaining the training runs a separate emulator is created for each output since any model output ordinarily has its own response surface to test the emulator the original model simulation is compared with its surrogate counterpart using a parameter set representing the default values for broadleaf evergreen tropical trees currently implemented in clm note the parameter prior range used to construct the emulator and to sample the 100 random parameter sets is a space that occupies 15 from the default parameter values the various emulators built for gpp lai and biomass are shown with the original model simulations in fig 4 the black dotted line in fig 4 shows the emulation results for the 87 parameter emulator which significantly deviate from the original model outputs shown with a red line for the simulations showing the full time period see fig 5 the 87 parameter emulator of the clm model does not capture the dynamics of the original model and it seems that at many time steps the emulator outputs are largely affected by noise fig 4 also shows scatter plots of 100 various simulations with randomly drawn parameter values using both the original clm and the 87 parameter emulator the fit between the two models is not good for any of the outputs shown in the scatter plots with black dots these simulations have correlations of 0 50 for gpp 0 29 for lai and 0 47 for biomass clearly this emulator will not serve the purposes of building a surrogate model for the clm 3 4 2 reducing to a 10 parameter emulator the emulator results shown with the black dotted line in figs 4 5 are created by applying pce emulators for a 87 parameter model and it is hypothesized that reducing the dimensionality of the emulator will reduce the noise at each projection which might allow the emulator to better mimc the original model according to massoud et al 2018 james in review only a handful of parameters generally control the outputs of this clm version and shown in their study are the 10 most sensitive parameters for various outputs in this study the emulator for clm is re constructed with a focus on the 10 most influential parameters rather than the complete set of 87 parameters thus l1p1 and l2p1 emulators are built using training samples from the 10 most sensitive parameters requiring a total of 21 and 241 training runs respectively the dimensionality reduction applied to the emulators significantly improves their outputs when compared to the original model the results of the 10 parameter l1p1 blue and l2p1 green emulators for clm are shown in figs 4 5 again these simulations use the default parameter set for the broadleaf evergreen tropical tree and the results of these emulators follow the original model outputs much closer than the 87 parameter emulator the right panel of fig 4 shows scatter plots of 100 various simulations with randomly drawn parameter values using both the original clm and the 10 parameter emulators note these 100 parameter vectors draw randomly the 10 influential parameters and keeps the remaining 77 non influential parameters at their default value the fit between the two models is fairly good for all of the outputs with correlations of 0 93 for gpp 0 70 for lai and 0 94 for biomass for the l1p1 emulator and with correlations of 0 87 for gpp 0 49 for lai and 0 89 for biomass for the l2p1 emulator clearly these emulators perform much better than the 87 parameter emulator and can possibly serve the purposes of building a surrogate model for the clm but only for the sensitive parameters that are considered in building the emulator 4 discussion 4 1 applicability of pce emulators for environmental models in this paper the applicability of emulation for complex environmental models is investigated to address this questions raised for each case study included 1 what is the level of emulator training and 2 the order of polynomial necessary to sufficiently build accurate emulators for each model to answer this factors such as how well the pce emulators approximate the original model are analyzed e g using default parameterization after training and to what degree does this performance degrade away from the parameter combinations used in training e g 100 random samples then the process parameterizations of the model dynamics are investigated through a global sensitivity analysis in essence this section aims to discuss the accuracy efficiency and fidelity of the surrogate models to their original counterparts 4 1 1 accuracy of emulators to investigate the accuracy of the emulators compared to the original models a parameter set from the training locations is used the default parameterizations then to test the degree to which the emulator performance degrades away from the parameter combinations used in training 100 random samples from the parameter prior range are used and compared with simulations from the original model for the hmodel all emulators that were built had very good performance for the stac and clm models the emulators created with level one sampling i e l1pyy had better performance than the ones created with level two sampling i e l2pyy there is no clear answer as to why the emulator performance degrades at the level two sampling i e table 4 showing correlations of stac l1p1 is a stronger emulator than l2p1 or fig 4 showing the clm l1p1 emulator has higher correlation than the l2p1 emulator the best explanation is that these correlations are made for the 100 parameter sets that are randomly drawn from the prior space and the emulator is mostly built around the default parameter set therefore this deviation from the default values might cause a noticeable bias in the emulator performance for models of high dimensions n 10 4 1 2 efficiency of emulators it is clear in this study that building emulators to speed up simulation time of cpu intensive environmental models can be beneficial as shown in tables 3 5 in regards to efficiency emulators can be a very viable option for research questions of high complexity that require unmanageable computational cost the emulators built in this study used a plain vanilla recipe for sparse grid sampling as well as for integrating the basis functions it is important to note that the sparse grid methodology used in this study is rudimentary relative to the methods displayed in the uncertainty quantification literature such as in sinsbeck and nowak for example the authors in sinsbeck and nowak utilized an optimized stochastic collocation osc method where the user can specify the number of integration points independently of the problem dimension and pce expansion order this allows one to reduce the number of model evaluations and still achieve a high accuracy through the approximation of optimized integration points however for the purposes of this study a generic sparse grid approximation recipe is followed that does not distinguish between sensitive and non sensitive dimensions of the problem which allows us to consider all the parameters possible effect on the emulators the emulation methods considered in this study are fundamentally simple and there are much more sophisticated emulation methods as well as methods for sampling the training points one could argue that most of the properties and drawbacks found for the current pce version will change and disappear to some degree if more elaborate pce methods or sampling strategies are used for example constantine et al 2012 provided theoretical justification on how to choose the order of the polynomial for a given sparse grid quadrature rule by minimizing aliasing error this would theoretically remove the need to numerically study what polynomial degree to use for a given sparse grid rule for example as shown in table 2 a dimension adaptive version of this rule is given in conrad and marzouk 2013 additionally here the dimensionality of the model is reduced to make construction of surrogates more feasible it should be noted that if such an approach works then dimension adaptive pseudo spectral projection should also work conrad and marzouk 2013 the latter is preferable because it makes no a priori assumption of the importance of each model parameter but rather determines the importance adaptively moreover the algorithm will add samples in a manner that greedily minimizes the approximation error the method also adds points in a much more granular manner than the level by level approach used by isotropic sparse grids furthermore computing the coefficients of the emulators using compressive sensing is often better than sparse grid collocation see results in doostan and owhadi 2011 or sargsyan et al 2014 collocation and pseudo spectral projection are equivalent for certain quadrature rule and polynomial type combinations constantine et al 2012 lastly surrogates were built for parameter spaces that spanned over the entire prior distribution this can be inefficient however and other works have focused on building surrogates which are accurate only over regions of high prior probability for example in conrad et al 2016 or li and marzouk 2014 4 1 3 fidelity of emulators a model surrogate may be accurate in the simulation outputs but it is possible that the pro cess representation may not be correct to investigate this for the emulators built in this study information from global sensitivity analysis is utilized to conduct a fidelity test of each emulator to its original counterpart fig 6 shows the sensitivity index of the top 5 parameters for each model and for their l1p1 and l2p1 emulators for the hmodel all emulators show a high fidelity since the simulations all represented the original model very well parameters that represent this model include i max α e and k s or the maximum interception the evaporation parameter and the time constant of the slow reservoir respectively this high fidelity is not seen for the stac model however since each emulator slightly differs from the original model in the magnitude and rank of the 5 most sensitive parameters for the stac model the parameters included are several van genuchten parameters van genuchten 1980 that represent the hydraulic properties of the tree layer in the soil tree atmosphere continuum domain i e tree α tree k s tree θ s and tree n as well as a feddes hydraulic stress parameter feddes et al 1978 i e tree p3 for clm some of the top 10 most sensitive parameters that were included to build the clm 10 emulators are v c max25 or the parameter that controls the photosynthetic capacity target carbon storage top of canopy specific leaf area sla maintenance respiration and stem allometry or carbon allocation to stem the l2p1 emulator sensitivities are shaped almost identically like the original model s yet with somewhat of a bias this bias could be attributed to the fact that the original model has 87 parameters and the emulator only considers 10 parameters which allows each respective parameter to have more influence and thus sensitivity on the model outputs however it can be assumed there is high fidelity between the original model and the emulator since the rank of the parameter sensitivities is identical between the two 5 conclusion environmental models are useful for analyzing the earth and all its processes many of these models are cpu intensive and contain dozens of parameters that control the model output making them rather difficult for tests such as sensitivity analysis or parameter estimation here emulators of various models were created using the pce method in search of a strategy for efficient simulation of complex environmental models to investigate this further the accuracy of the simulations the efficiency of each method and the fidelity of each emulator to its original counterpart were examined emulators were built using sparse grid levels 1 3 and polynomial orders 1 3 the performance of the emulators seemed to degrade with the dimensionality of the problem in the first case study a 7 dimensional hydrologic model was mimicked fairly well then for a 15 parameter ecohydrologic model the performance of the emulators were still strong but further degradation occurred for the emulators with higher level sampling in the final case study an emulator for the 87 parameter clm was built and the high dimensionality of this emulator caused the performance to degrade significantly therefore the dimensionality of the emulator was reduced to 10 parameters this dimensionality reduction allowed the emulators to achieve a much stronger fit requiring only 21 l1p1 and 241 l2p1 forward runs from the original model to train the emulators most of the properties and drawbacks found for the current pce version will change and disappear to some degree if more elaborate pce methods or sampling strategies are used in general due to the interplay between truncation error and numerical integration error it might be best to choose a lower order pce and a more accurate integration scheme than vice versa overall this paper provides guidance for simple and efficient emulation of complex models and promotes the use of the pce method for efficient and accurate exploration of the parameter space software availability https figshare com articles polynomial chaos expansion pce example 4907300 acknowledgments the author gratefully acknowledges support from the scgsr fellowship of the department of energy and the uc lab fees research program award 237285 this research was performed at the university of california irvine and at the jet propulsion laboratory california institute of technology under a contract with the national aeronautics space administration the author thanks dr j vrugt and dr c xu for assistance with the models used in the paper and dr e laloy elaloy sckcen be for assistance with the emulation software special thanks are given to los alamos national laboratory for allowing the clm simulations to be made on the conejo supercomputing facility the source code of the pce method is written in matlab and available upon request appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 008 
26302,this paper investigates the applicability of model emulation to speed up simulation time of cpu intensive environmental models polynomial chaos expansion pce emulators are constructed for three case studies of increasing complexity the level of emulator training and the order of polynomial necessary to sufficiently build accurate emulators for each model are investigated although the pce emulators shown here do not approximate well the outputs of parameter rich models 80 parameters results demonstrate that the emulators mimic closely outputs of relatively simple low dimensional simulation models 15 parameters or less furthermore the pce emulators are tested with applications such as global sensitivity analysis gsa results illustrate the advantages and drawbacks of using classical pce emulators for treating computational limitation of complex environmental models keywords model emulation surrogate modeling polynomial chaos expansion sensitivity analysis 1 introduction and scope environmental models are dynamic representations of the atmosphere ocean ice and land described in coupled systems to analyze and predict short and long term earth system behavior claussen et al 2002 wood et al 2011 hurrell et al 2013 typically obtaining better accuracy in these models comes at the expense of dimensionality and complexity and thus a reduced computational efficiency dunne et al 2012 arora et al 2013 state of the art weather and climate prediction models contain a large number of parameters to simulate the exchange of energy momentum and mass between the land surface and overlying atmosphere noilhan and planton 1989 bastidas et al 1999 masson et al 2003 sargsyan et al 2014 assessment of the model parameters is a necessary step before the model can be ran for operational use gupta et al 1999 rosolem et al 2013 yet the cpu time of simulation models can vary from less than a second for simple dynamic models to many hours of calculation for spatially explicit models therefore there is an immense cpu and time cost associated with investigating model parameters since many thousands of runs are typically required for using methods such as global sensitivity analysis or parameter estimation with markov chain monte carlo mcmc certainly computational limitations still remain a major obstacle to the effective use of environ mental models in decision making ratto et al 2012 and emulation methods have been developed in an attempt to reduce the time cost of such models emulation is an important and growing field of research that signifies a major achievement in the study of complex mathematical models ratto et al 2012 razavi et al 2012 these methods define surrogate responses of the original model outputs at a strongly reduced computational cost the concept of emulation dates back to blanning 1975 and kleijnen 1975 and has been greatly evolving in recent years the central idea is that the original complex model has a response surface for its outputs for a pre defined parameter range and this response surface has a mathematical representation that the emulation methods are built to capture presented and implemented here are polynomial chaos expansion pce emulators ghanem and spanos 1991 xiu and karniadakis 2002 le mâıtre and knio 2010 which utilize orthogonal polynomials to mimic the parameter effects on model outputs and surrogate models are built for various environmental models of increasing complexity there are various algorithms and methods other than pce used for emulation and surrogate modeling and for many different types of applications the set of methods referred to as the design and analysis of computer experiments dace is very popular sacks et al 1989 santner et al 2013 levy and steinberg 2010 and books reviews and journal special issues have been written on this subject van gigch 1991 fang et al 2005 kuhnt and steinberg 2010 kleijnen 2010 the dace method is also known as gaussian process gp or kriging metamodeling di pierro et al 2009 for example conti and o hagan 2010 applied a gp emulator to mimic a multi output and dynamic computer simulation model the sheffield dynamic global vegetation model and machac et al 2016 used a similar method to create a surrogate model for an urban hydrodynamic drainage simulator another emulation method uses radial basis functions rbf such as the techniques used in bliznyuk et al 2012 and regis and shoemaker 2007a 2007b furthermore artificial neural networks ann and support vector machines svm are also emulation techniques and have been used in many studies such as the emulation of a hydrologic model zhang et al 2009 the identification of controlling mechanisms in transpiration of a pine tree vrugt et al 2002 or the estimation of regional variation in particulate organic carbon to nitrogen ratio in the surface ocean martiny et al 2013 in keating et al 2010 prediction uncertainty of a highly complex cpu intensive groundwater model was explored using a simplified surrogate model emulators have been used for data assimilation and parameter estimation using a probabilistic collocation based kalman filter pckf which combines the kalman filter with pce emulators saad and ghanem 2009 man et al 2016 fan et al 2016 additionally laloy et al 2013 jointly use a pce emulator of the original model and a dimensionality reduction using the karhunen love tranform of the parameter space to speed up the calibration of a groundwater model zeng et al 2016 used similar methods to build pce emulators of a groundwater model using stochastic collocation and sparse grids a goal of the study is to investigate the applicability of pce based emulators for mimicking complex environmental and earth system models esms first two environmental models of low and medium complexity are considered these two cases involve a conceptual rainfall runoff model hmodel shown in schoups and vrugt 2010 and a soil tree atmosphere continuum stac model that simulates hydrodynamic processes of a single tree shown in rings et al 2013 finally an emulator is created for an esm a version of the community land model clm4 5 ed shown in fisher et al 2015 for all case studies the polynomial emulators are built using training parameters sampled from sparse grid approximation smolyak 1963 xiu and hesthaven 2005 nobile et al 2008a 2008b sparse grids are numerical techniques to represent integrate or interpolate high dimensional functions and can be useful in the application of pce emulators to sample the locations of the training runs the user chooses the level of sparse grid approximation where an increasing level denotes an increased density of sampling which usually indicates a stronger emulator tested are 1st and higher level sparse grids as well as 1st and higher order polynomial emulators and the level of training and order of polynomial necessary to sufficiently build accurate emulators is investigated this study analyzes the accuracy efficiency and fidelity of the emulators compared to their original counterparts the remainder of this paper is structured as follows section 2 describes the pce method used to build the emulators section 3 introduces the models and data of each case study and report the corresponding results of the pce emulators this is followed in section 4 by a general discussion of the main implications of the collective results finally section 5 presents a summary of the most important findings and offers concluding remarks 2 materials and methods 2 1 model formulation this section briefly reviews the building blocks of the pce method that is used herein to emulate the output of different ecosystem models consider a n vector of measurements y y 1 y n observed at discrete times t 1 n that summarizes the response of an environmental system s to k temporally variant control inputs b b 1 b n with column elements b t b t1 b tk consider a computer model f to explain the observed data 1 y f x ζ 0 b e where x x 1 x d is the d 1 vector of model parameters ζ 0 stores the values of the state variables at the start of simulation b signifies the control matrix with temporal measurements of the forcing variables and e e 1 e n is a vector of residuals 2 e f x ζ 0 b y y f x ζ 0 b the index t for time takes on strictly positive integer values in the remainder of this paper t 1 n n yet may take on real values t 0 n r in the actual system model f to resolve for continuous time processes wherein the simulated output at t 0 is defined completely by ζ 0 a convenient assumption is made that the forcing data are observed without measurement error and that errors in the initial states b pose no harm as their impact on the simulated output y diminishes rapidly with advancing time this latter assumption is certainly appropriate for environmental systems controlled by negative or degenerative feedback the assumptions of perfect input data and initial states due to spin up period are common to environmental modeling this ideal case leaves as the only unknowns the model parameters without further loss of generality restrict the model parameters to a closed space χ equivalent to a d dimensional hypercube x χ r d called the feasible parameter space note that the tilde operator is used for the initial state and forcing data to signify explicit use of measured values the large computational requirements of complex system models complicates tremendously tasks such as parameter estimation via nonlinear optimization or statistical inference as the resulting inverse problem may require many successive executions of the model this becomes particularly cumbersome for parameter rich models emulation methods may help simplify parameter estimation by using a fast running surrogate model of the original complex cpu intensive process model the next section reviews one of such emulation methods namely pce note curly brackets are used to differentiate between random variables and their actual sampled values thus x 1 x t stores a sequence of t different training points draws of the model parameters x 2 2 model emulation using generalized polynomial chaos expansion the pce method emulates the output y of the original model in equation 1 via polynomial approximation this emulator can be re written as follows 3 y f p x where p signifies the order of the polynomial e g p 1 for a first order polynomial emulator as detailed in the previous section ignore measurement uncertainty of the initial states and forcing variables and thus assume that the model s ability to describe the observed data y is determined only by the d values of the model parameters x consequently only the parameter values are needed as input by the emulator to replicate the response of the original model the pce emulator approximates the model output in the following manner 4 y f p x j 1 m a j ψ j x where a a 1 a m is a m vector of yet unknown deterministic expansion coefficients m signifies the number of d dimensional orthogonal polynomials and each ψ j x ψ j 1 x 1 ψ j d x d is the product of one dimensional polynomials for expansion terms j 1 m thus each model parameter x i impacts ψ j in their own specific way the number of orthogonal polynomials m depends on the polynomial order p of the emulator and the dimensionality d of the parameter space χ according to m p d p d where the symbol denotes factorial table 1 shows the number of polynomials for each emulator considered in this study in this study the values of the coefficients a are estimated via spectral projection of the original model response y x against each individual basis function e g see xiu 2007 5 a j k 1 t w k ψ j x k f x k ζ 0 b ψ j 2 x where x x 1 x t is a t d matrix of t different realizations of the parameter vector x and the w k s denote the corresponding weights of an integration a cubature rule on the feasible parameter space χ on r d the denominator ψ2 x in equation 5 is equivalent to the inner product of the jth multivariate orthogonal polynomial and can be computed as follows 6 ψ j 2 x i 1 d ψ j i 2 where j 1 m the d univariate inner products ψ 2 of each jth polynomial where i 1 d right hand side of equation 6 have a simple closed form analytic solution therefore the computational cost of pce is determined by the time it takes to evaluate the forward model f x k ζ 0 b for each of the t training parameter vectors x k of x in equation 5 where k 1 t now combine equations 5 and 6 the approximate pce emulator reads as follows 7 f p x j 1 m a j ψ j x interested readers are referred to xiu 2007 and laloy et al 2013 for a more detailed description of spectral projection methods within the context of pce the use of a cubature integration rule in equation 5 causes f p x in equation 4 to differ from ζ 0 in equation 7 this is also referred to as the aliasing error xiu 2007 and decreases with t in equation 5 in theory the larger the values of p and t the better the emulator output y y f x ζ 0 b e should approximate the simulations y x of the original forward simulator f x ζ 0 ζ 0 for random parameter vector x the rate of convergence of y x to y x depends on the regularity of f x ζ 0 ζ 0 the smoother the response of the forward model the faster y x will approximate y x in practice however computational requirements impose restrictions on the values of p and t thus the available computational budget dictates the accuracy of the surrogate model f p to maximize the capabilities of the emulator it would be desirable to select wisely x the matrix of t training samples spectral projection has the advantage that it requires far fewer parameter vectors than linear regression for a well posed solution of the pce expansion coefficients in equation 7 the results section briefly shows the sparse grid method used to generate the matrix of training realizations 2 3 sensitivity analysis of model parameters global sensitivity analysis aims at quantifying the contributions of input variables to the variance of the outputs of a physical model among the abundant literature on global sensitivity analysis sobol s method sobol 1990 has received much attention since it provides accurate information when calculating the importance index of model parameters however the full description requires the evaluation of 2 n monte carlo integrals sudret 2008 which is not practically feasible unless n is low n here represents the dimensionality of the model or the number of active parameters the fourier amplitude sensitivity test fast cukier et al 1973 is another popular sensitivity analysis technique that is computationally efficient and can be used effectively for nonlinear and nonmonotonic models sudret 2008 xu and gertner 2011 fast uses a periodic approach to sample the parameter space and a fourier transformation to decompose the variance of a model output into partial variances contributed by different model parameters the fast methodology was used in massoud et al 2019 scientific reports in review to evaluate the sensitivity index of the 87 parameters of the clm model used in this study in their sensitivity analysis the top 10 sensitive parameters were pinpointed for each model output and this information will be used in the case study here to reduce the dimensionality of the model in order to build more efficient emulators more on this in the results section the fidelity of the emulators to their original model counterpart is investigated where the sensitivity of the model parameters in the emulators is compared to those of the original models in theory if the performance of the emulator is identical to that of the original model then the parameters of the emulator should have the same sensitivity index as those of the original model many studies assume that emulators provide efficient sensitivity analysis for large and expensive models crestaux et al 2009 ratto et al 2012 however this may not always be true as emulators may provide anomalous parameter sensitivity estimates in one study the authors investigated the joint application of emulators and their respective parameter sensitivities and showed that with enough training the emulator will match the original model in the parameter sensitivity rank and magnitudes borgonovo et al 2012 however the environmental model used in borgonovo et al 2012 only contained 12 parameters other studies have demonstrated the use of emulation for applications of sensitivity analysis or parameter calibration of higher order models e g lu et al 2018 ricciuto et al 2018 but the comparison of the results obtained from the emulators to those obtained from the original model was not investigated in this study the claim that it is safe to assume the sensitivity index obtained from the emulator represents the true importance of that parameter in the original model is investigated this will indicate that the processes represented in the original model are being replicated properly in the emulator the fast method of xu and gertner 2007 2011 is used to evaluate the parameter sensitivities of the original model and the emulators therefore other than simply checking the accuracy of the emulators the total sensitivity of each models parameters to the parameters in the surrogate models is compared to test the fidelity of the emulators to their original counterparts 3 results the results of each case study is presented separately each discussing the results of a specific model first the results of the sparse grid approximations are shown which ultimately determine the location of the training runs for each emulator then the emulation results from each case study is presented the model of interest is first discussed in each subsection followed by a description of the model parameters finally the emulation results using different polynomial emulators and sparse grid levels are presented for each case study 3 1 sparse grid approximation sparse grid approximation methods reduce drastically the required number of parameter vectors to construct an emulator while preserving a high accuracy for moderately large dimensional parameter spaces smolyak 1963 xiu and hesthaven 2005 nobile et al 2008a nobile et al 2008b sinsbeck and nowak a pth order pce emulator can be built with sparse grids of different levels l and thus variable number of training data points thus a sparse grid of level l does not necessarily equate to a p lth order pce emulator in general the larger the order of the polynomial emulator the more training points are required for its calibration yet this does not guarantee that the emulator will closely approximate the output of the original forward model thus emulators of different polynomial orders using a range of sparse grid levels are built tested and contrasted fig 1 depicts graphically training data pairs x 1 x 2 for a hypothetical two parameter model a gauss patterson sparse grid is used and the results for levels l 1 up to l 6 are all presented the higher the order l of the sparse grid the more training data points indeed with l 1 only tr 5 samples are required to build the pce emulator and this number increases to tr 769 for l 6 see fig 1 table 2 lists the number of training samples associated with sparse grids of levels l 1 l 2 and l 3 for the three forward models considered in this paper details of these models appear in the case study section and readers are referred to this part of the paper for further information the first case study considers a d 7 parameter conceptual watershed model named hmodel the first level sparse grid of this model corresponds to t 15 tr s whereas a second and third level grid increases the number of training points to t 127 and t 799 respectively the cpu efficiency of the model warrants emulation using all three different grid levels the second case study involves simulation of water transport using a soil tree atmosphere continuum or stac model this model has d 15 different parameters a first level sparse grid would require only t 31 tr s and this number increases rapidly to t 511 and t 5 925 for a second and third order level due to the somewhat large cpu demands of stac construction of the emulator is limited to a first and second level sparse grid lastly the third case study considers clm this model is rather complex yet the emulators consider herein only the d 87 parameters of interest to dynamic vegetation modeling a first order sparse grid equates to t 175 tr s whereas the second and third level grids necessitate 15 487 and t 2 044 416 executions of clm therefore the deliberate choice for this model is l 1 as the second and third grid levels demand too much time indeed a single forward run of clm takes about 8 h using serial processing also considered separately is a refined clm parameterization in which only the ten most sensitive parameters are allowed to vary this alternative parameterization coined clm 10 is desirable as it requires far fewer model evaluations for each of the three sparse grids the sensitive parameters were chosen through a global sensitivity analysis in which the parameters with the top 10 highest sensitivity index were chosen for each output of interest in summary all three levels of the sparse grid are tested for the hmodel l 1 and l 2 for the stac model and only l 1 for clm furthermore for the hmodel and stac model polynomial emulators of first second and third order p 1 2 3 are examined these emulators are coined lxpyy where x denotes the sparse grid level and yy signifies the polynomial order of the emulator thus l1p1 signifies a first order polynomial emulator built using a sparse grid of level one l2p3 thus is equivalent to a third order polynomial emulation using a second order sparse grid 3 2 case study 1 the rainfall runoff transformation the first case study simulates the rainfall runoff transformation of the guadalupe river at spring branch in texas using the hmodel conceptual watershed model the model transforms rainfall into discharge at the watershed outlet using explicit process descriptions of interception throughfall evaporation surface runoff percolation and surface and subsurface routing a detailed description of the hmodel structure and processes representations can be found in schoups and vrugt 2010 their study also summarizes the d 7 parameters of the hmodel and their prior uncertainty ranges a six year record of daily discharge mm day mean areal precipitation mm day and mean areal potential evapotranspiration mm day of the guadalupe river derived from the mopex data set duan et al 2006 was used for this case study details about the basin experimental data and prior distribution can be found in schoups and vrugt 2010 table 3 summarizes the results of this analysis and lists the cpu time and the root mean square error rmse and correlation values using first second and third order polynomial emulators with sparse grids of level one two and three the hmodel l3p3 emulator and the original hmodel are compared with observed data in fig 2 using 100 random parameter sets from the prior distribution both the original model and the emulators were able to capture the dynamics in the observed data quite well note a truncated pce is known to approximate the variance from below and this is neatly visible in the plot i e original model variance matches emulator variance although this holds true only for representing the prior variance and not the posterior variance since the posterior may be located in a totally different region of the parameter space and thus might result in a different variance for the simulations 3 3 case study 2 soil tree atmosphere continuum stac model the second case study involves the stac model which is a physically based nonlinear modeling framework that simulates water flow in the combined soil and tree systems rings et al 2013 the model discretizes the system domain and couples the soil with the tree domain simulating the soil roots and tree trunk as a continuum the stac model utilizes the hydrus model simuunek et al 2008 where water flow through the soil and the tree root system and stem is driven by the evaporative demand and soil available water leading to a gradient in soil and xylem water potentials throughout the system the stac model contains 15 parameters and emulators with 1st and 2nd level sparse grid sampling are built table 2 there are at least m 4 outputs for the stac model including sapflux stem potential soil storage and tree storage however the focus of this study is solely on the sapflux output in cm day 1 in an attempt to emulate the response for t 769 time steps representing 30 min intervals during a 17 day period measured data include soil water content and water potential in three spatial dimensions in the root zone tree stem water content and sapflux canopy water potential and atmospheric variables such as net radiation air temperature and humidity providing the necessary information for potential tree evapotranspiration the data was collected in and around a mature douglas fir abies concolor in a 99 ha subcatchment p301 of the king s river experimental watershed krew located in the rain snow transition zone of the southern sierra nevada mountain range in california the data collection was part of the southern sierra critical zone observatory czo established to improve understanding of surface and subsurface processes along a gradient of elevation energy water and soil bales et al 2011a 2011b the douglas fir czt 1 is located along a ridge in a relatively open area of the forest at an elevation of 2018 m bales et al 2011b near a meteorological station the mature tree is about 30 m high with a trunk diameter of 0 55 m table 4 summarizes the results of this analysis and lists the cpu time and the rmse and correlation values using first second and third order polynomial emulators with sparse grids of level one and two the stac model l1p1 and l2p1 emulators as well as the original stac model are compared with observed data in fig 3 left using a parameter set that is representative of a mature douglas fir then 100 random parameter sets are drawn from the prior distribution to compare the l1p1 emulator with the original stac model fig 3 right both the original model and the emulators were able to capture the dynamics in the observed data quite well with a bit of noise being introduced in the l2p1 simulations light blue curve in fig 3 3 4 case study 3 the community land model with ecosystem demography clm4 5 ed the esm considered in this study is the clm4 5 ed clm is a community based open source model that is widely used for understanding climate vegetation interactions clm is the land surface model used within various esms including the community earth system model cesm and the norwegian earth system model noresm lawrence et al 2011 bonan et al 2011 the ecosystem demography ed concept is a method for scaling the behavior of forest ecosystems by aggregating individual trees into representative cohorts based on their size and plant type or pft and by aggregating groups of cohorts into representative patches conceptually similar to a forest plot which explicitly tracks the time between disturbances moorcroft et al 2001 the ed component is the most advanced dgvm incorporated into the clm framework the main property of the ed concept that differs from most commonly used big leaf models is the capacity to predict distributions and compositions of plants directly from their given physiological traits described by the model parameterization fisher et al 2015 clm ed simulates growth by integrating photosynthesis across different leaf layers for each cohort and mechanistic mortality is simulated based on plant carbon starvation and hydraulic failure in addition to a background mortality rate mortality from tree fall impacts and fire the model allocates photosynthetic carbon to different tissues such as leaf root and stem based on the allometry of different tree species clm ed can be simulated at different modes including point mode for sites regional mode for watershed or regional scale and global mode for continental and global scale see supplementary model description in fisher et al 2015 for details on specific components of the model structure the clm ed version considered here contains 87 parameters making it only acceptable to build an emulator with 1st level sparse grid sampling but for a 2nd level sparse grid emulator the number of training runs already become too large for this analysis the runs are initialized i e ζ 0 with a bare ground or a state with no vegetation the climate conditions i e b for this site are from qian et al 2006 representative of data from 1948 to 1972 the model is simulated for a site in the state of par a the amazon brazil 7 s 55 w there are dozens of possible outputs for the clm and here the focus is on m 3 outputs including gross primary production gpp kgc m 2 yr 1 leaf area index lai and overall biomass biomass kgc m 2 in an attempt to emulate the response for t 960 time steps representing 1 month intervals during an 80 year period fig 4 shows the original model and the emulator simulations fig 5 shows the simulations for the clm for the entire simulation period of 80 years each of the emulators is compared to the original model 3 4 1 87 parameter emulator an l1p1 emulator is built for the 87 parameter clm requiring a total of 175 training sets the simulations spanned 80 years and the cpu cost of the original model is roughly 8 h therefore the emulator training cost a total of 1400 cpu hrs performed in parallel simulations on the conejo supercomputer at the los alamos national laboratory after obtaining the training runs a separate emulator is created for each output since any model output ordinarily has its own response surface to test the emulator the original model simulation is compared with its surrogate counterpart using a parameter set representing the default values for broadleaf evergreen tropical trees currently implemented in clm note the parameter prior range used to construct the emulator and to sample the 100 random parameter sets is a space that occupies 15 from the default parameter values the various emulators built for gpp lai and biomass are shown with the original model simulations in fig 4 the black dotted line in fig 4 shows the emulation results for the 87 parameter emulator which significantly deviate from the original model outputs shown with a red line for the simulations showing the full time period see fig 5 the 87 parameter emulator of the clm model does not capture the dynamics of the original model and it seems that at many time steps the emulator outputs are largely affected by noise fig 4 also shows scatter plots of 100 various simulations with randomly drawn parameter values using both the original clm and the 87 parameter emulator the fit between the two models is not good for any of the outputs shown in the scatter plots with black dots these simulations have correlations of 0 50 for gpp 0 29 for lai and 0 47 for biomass clearly this emulator will not serve the purposes of building a surrogate model for the clm 3 4 2 reducing to a 10 parameter emulator the emulator results shown with the black dotted line in figs 4 5 are created by applying pce emulators for a 87 parameter model and it is hypothesized that reducing the dimensionality of the emulator will reduce the noise at each projection which might allow the emulator to better mimc the original model according to massoud et al 2018 james in review only a handful of parameters generally control the outputs of this clm version and shown in their study are the 10 most sensitive parameters for various outputs in this study the emulator for clm is re constructed with a focus on the 10 most influential parameters rather than the complete set of 87 parameters thus l1p1 and l2p1 emulators are built using training samples from the 10 most sensitive parameters requiring a total of 21 and 241 training runs respectively the dimensionality reduction applied to the emulators significantly improves their outputs when compared to the original model the results of the 10 parameter l1p1 blue and l2p1 green emulators for clm are shown in figs 4 5 again these simulations use the default parameter set for the broadleaf evergreen tropical tree and the results of these emulators follow the original model outputs much closer than the 87 parameter emulator the right panel of fig 4 shows scatter plots of 100 various simulations with randomly drawn parameter values using both the original clm and the 10 parameter emulators note these 100 parameter vectors draw randomly the 10 influential parameters and keeps the remaining 77 non influential parameters at their default value the fit between the two models is fairly good for all of the outputs with correlations of 0 93 for gpp 0 70 for lai and 0 94 for biomass for the l1p1 emulator and with correlations of 0 87 for gpp 0 49 for lai and 0 89 for biomass for the l2p1 emulator clearly these emulators perform much better than the 87 parameter emulator and can possibly serve the purposes of building a surrogate model for the clm but only for the sensitive parameters that are considered in building the emulator 4 discussion 4 1 applicability of pce emulators for environmental models in this paper the applicability of emulation for complex environmental models is investigated to address this questions raised for each case study included 1 what is the level of emulator training and 2 the order of polynomial necessary to sufficiently build accurate emulators for each model to answer this factors such as how well the pce emulators approximate the original model are analyzed e g using default parameterization after training and to what degree does this performance degrade away from the parameter combinations used in training e g 100 random samples then the process parameterizations of the model dynamics are investigated through a global sensitivity analysis in essence this section aims to discuss the accuracy efficiency and fidelity of the surrogate models to their original counterparts 4 1 1 accuracy of emulators to investigate the accuracy of the emulators compared to the original models a parameter set from the training locations is used the default parameterizations then to test the degree to which the emulator performance degrades away from the parameter combinations used in training 100 random samples from the parameter prior range are used and compared with simulations from the original model for the hmodel all emulators that were built had very good performance for the stac and clm models the emulators created with level one sampling i e l1pyy had better performance than the ones created with level two sampling i e l2pyy there is no clear answer as to why the emulator performance degrades at the level two sampling i e table 4 showing correlations of stac l1p1 is a stronger emulator than l2p1 or fig 4 showing the clm l1p1 emulator has higher correlation than the l2p1 emulator the best explanation is that these correlations are made for the 100 parameter sets that are randomly drawn from the prior space and the emulator is mostly built around the default parameter set therefore this deviation from the default values might cause a noticeable bias in the emulator performance for models of high dimensions n 10 4 1 2 efficiency of emulators it is clear in this study that building emulators to speed up simulation time of cpu intensive environmental models can be beneficial as shown in tables 3 5 in regards to efficiency emulators can be a very viable option for research questions of high complexity that require unmanageable computational cost the emulators built in this study used a plain vanilla recipe for sparse grid sampling as well as for integrating the basis functions it is important to note that the sparse grid methodology used in this study is rudimentary relative to the methods displayed in the uncertainty quantification literature such as in sinsbeck and nowak for example the authors in sinsbeck and nowak utilized an optimized stochastic collocation osc method where the user can specify the number of integration points independently of the problem dimension and pce expansion order this allows one to reduce the number of model evaluations and still achieve a high accuracy through the approximation of optimized integration points however for the purposes of this study a generic sparse grid approximation recipe is followed that does not distinguish between sensitive and non sensitive dimensions of the problem which allows us to consider all the parameters possible effect on the emulators the emulation methods considered in this study are fundamentally simple and there are much more sophisticated emulation methods as well as methods for sampling the training points one could argue that most of the properties and drawbacks found for the current pce version will change and disappear to some degree if more elaborate pce methods or sampling strategies are used for example constantine et al 2012 provided theoretical justification on how to choose the order of the polynomial for a given sparse grid quadrature rule by minimizing aliasing error this would theoretically remove the need to numerically study what polynomial degree to use for a given sparse grid rule for example as shown in table 2 a dimension adaptive version of this rule is given in conrad and marzouk 2013 additionally here the dimensionality of the model is reduced to make construction of surrogates more feasible it should be noted that if such an approach works then dimension adaptive pseudo spectral projection should also work conrad and marzouk 2013 the latter is preferable because it makes no a priori assumption of the importance of each model parameter but rather determines the importance adaptively moreover the algorithm will add samples in a manner that greedily minimizes the approximation error the method also adds points in a much more granular manner than the level by level approach used by isotropic sparse grids furthermore computing the coefficients of the emulators using compressive sensing is often better than sparse grid collocation see results in doostan and owhadi 2011 or sargsyan et al 2014 collocation and pseudo spectral projection are equivalent for certain quadrature rule and polynomial type combinations constantine et al 2012 lastly surrogates were built for parameter spaces that spanned over the entire prior distribution this can be inefficient however and other works have focused on building surrogates which are accurate only over regions of high prior probability for example in conrad et al 2016 or li and marzouk 2014 4 1 3 fidelity of emulators a model surrogate may be accurate in the simulation outputs but it is possible that the pro cess representation may not be correct to investigate this for the emulators built in this study information from global sensitivity analysis is utilized to conduct a fidelity test of each emulator to its original counterpart fig 6 shows the sensitivity index of the top 5 parameters for each model and for their l1p1 and l2p1 emulators for the hmodel all emulators show a high fidelity since the simulations all represented the original model very well parameters that represent this model include i max α e and k s or the maximum interception the evaporation parameter and the time constant of the slow reservoir respectively this high fidelity is not seen for the stac model however since each emulator slightly differs from the original model in the magnitude and rank of the 5 most sensitive parameters for the stac model the parameters included are several van genuchten parameters van genuchten 1980 that represent the hydraulic properties of the tree layer in the soil tree atmosphere continuum domain i e tree α tree k s tree θ s and tree n as well as a feddes hydraulic stress parameter feddes et al 1978 i e tree p3 for clm some of the top 10 most sensitive parameters that were included to build the clm 10 emulators are v c max25 or the parameter that controls the photosynthetic capacity target carbon storage top of canopy specific leaf area sla maintenance respiration and stem allometry or carbon allocation to stem the l2p1 emulator sensitivities are shaped almost identically like the original model s yet with somewhat of a bias this bias could be attributed to the fact that the original model has 87 parameters and the emulator only considers 10 parameters which allows each respective parameter to have more influence and thus sensitivity on the model outputs however it can be assumed there is high fidelity between the original model and the emulator since the rank of the parameter sensitivities is identical between the two 5 conclusion environmental models are useful for analyzing the earth and all its processes many of these models are cpu intensive and contain dozens of parameters that control the model output making them rather difficult for tests such as sensitivity analysis or parameter estimation here emulators of various models were created using the pce method in search of a strategy for efficient simulation of complex environmental models to investigate this further the accuracy of the simulations the efficiency of each method and the fidelity of each emulator to its original counterpart were examined emulators were built using sparse grid levels 1 3 and polynomial orders 1 3 the performance of the emulators seemed to degrade with the dimensionality of the problem in the first case study a 7 dimensional hydrologic model was mimicked fairly well then for a 15 parameter ecohydrologic model the performance of the emulators were still strong but further degradation occurred for the emulators with higher level sampling in the final case study an emulator for the 87 parameter clm was built and the high dimensionality of this emulator caused the performance to degrade significantly therefore the dimensionality of the emulator was reduced to 10 parameters this dimensionality reduction allowed the emulators to achieve a much stronger fit requiring only 21 l1p1 and 241 l2p1 forward runs from the original model to train the emulators most of the properties and drawbacks found for the current pce version will change and disappear to some degree if more elaborate pce methods or sampling strategies are used in general due to the interplay between truncation error and numerical integration error it might be best to choose a lower order pce and a more accurate integration scheme than vice versa overall this paper provides guidance for simple and efficient emulation of complex models and promotes the use of the pce method for efficient and accurate exploration of the parameter space software availability https figshare com articles polynomial chaos expansion pce example 4907300 acknowledgments the author gratefully acknowledges support from the scgsr fellowship of the department of energy and the uc lab fees research program award 237285 this research was performed at the university of california irvine and at the jet propulsion laboratory california institute of technology under a contract with the national aeronautics space administration the author thanks dr j vrugt and dr c xu for assistance with the models used in the paper and dr e laloy elaloy sckcen be for assistance with the emulation software special thanks are given to los alamos national laboratory for allowing the clm simulations to be made on the conejo supercomputing facility the source code of the pce method is written in matlab and available upon request appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 008 
26303,an integrated watershed model is a key tool for assessing the fate and transport of organic chemicals we developed and integrated an organic chemical simulation module into soil water assessment tool swat model and used it for polycyclic aromatic hydrocarbons pahs simulation in muskeg river watershed in athabasca oil sands region western canada the model uses a three phase partition process to simulate the chemicals partitioning to dissolved organic carbon doc in both soil water and streams the simulation performances for phenanthrene and pyrene loads for a period of 2015 2017 were assessed to be of satisfactory quality the model simulation results indicated that the summer season is the hot moment and the rainfall runoff event is the driving force for the pahs transport in the watershed the developed simulation module could be a useful modeling tool to simulate and asess the pathway fate and transport of organic chemicals at the watershed scale graphical abstract image 1 keywords swat model organic chemical simulation three phase partition polycyclic aromatic hydrocarbons pahs software availability name of software swat organic chemical simulation module developer xinzhong du contact address 1 university dr athabasca ab t9s 3a3 canada and e mail xinzhong du gmail com year first available 2018 program language fortran availability contact developer by e mail 1 introduction organic chemicals can cause local and regional losses of freshwater biodiversity and ecosystem services of the 223 chemicals monitored globally pesticides tributyltin polycyclic aromatic hydrocarbons pahs and brominated flame retardants were the major contributors to the chemical risk malaj et al 2014 particularly organic chemicals can impair the designated water use if their concentrations exceed water quality standards they can be exported to stream and other water bodies by point source or transport from landscape environment oil sands are found in about 70 countries and the largest deposits of oil sands exist in canada and venezuela with smaller resources in russia europe and the u s hirsch et al 2005 the oil production in oil sands region could pose threat to the local environment the athabasca oil sands or tar sands are large deposits of bitumen or extremely heavy crude oil located in northeastern alberta canada which have been subject of various environmental studies since suncor energy inc began its operation in 1967 evans et al 2016 since then the oil sands industry has expanded exponentially and there are environmental concerns from the industry activities associated with surface mining in situ recovery and upgrading of bitumen kurek et al 2013 kelly et al 2009 conducted an independent and accessible assessment of the loading of polycyclic aromatic compounds pac to the athabasca river and its tributaries and found that the oil sands industry is a far greater source of regional pac contamination than previously realized due to substantial loadings of airborne pac they pointed out that controls on waterborne pac are critical because concentrations at tributary mouths and at one site on the athabasca river are already within the range toxic to fish embryos kelly et al 2009 moreover kelly et al 2010 found that oil sands industry substantially increases the loadings of toxic metal elements to the athabasca river and its tributaries via air and water pathways and recommended a detailed long term monitoring to distinguish the sources of the contaminates and control the potential impacts collectively these industrial activities yield significant landscape disturbance and habitat loss and lead to the controversy water quantity and quality issues kurek et al 2013 monitoring methods for organic chemicals have been conducted to assess the impact on water quality indentify pollution source and evaluate the efficiency of pollution mitigation technique bourgeault and gourlay france 2013 imfeld et al 2009 morandi et al 2015 however it is very expensive and time consuming to conduct an intensive and long term monitoring program du et al 2016 on the other hand an integrated watershed model is a cost effective and powerful tool for quantitatively predicting the fate and transport of organic chemicals in both terrestrial and aquatic environments at the watershed scale to support environmental exposure and risk assessments pollution control planning and decision making chen et al 2017 therefore models have been used for simulation of pahs in the main stream of lower athabasca river using one dimensional shakibaeinia et al 2017 and two dimensional kashyap et al 2017 hydrodynamic and transport models however while these models were successful to represent the organic chemical processes in the river system they were lack of description of the transport of the chemicals from terrestrial to aquatic environment a watershed scale model the soil and water assessment tool swat arnold et al 2012 has been widely used for simulating streamflow shrestha et al 2017 zhang et al 2008 water temperature du et al 2017 sediment and nutrient loads jha et al 2004 on different temporal and spatial scales the swat model is also able to simulate the pesticides which is one of specific organic chemicals and it has been used for modeling pesticides in different watersheds chen et al 2017 fohrer et al 2014 larose et al 2007 however organic pollutants are not limited to the pesticides a module should also be able to simulate other organic chemicals such as pahs and naphthenic acids understanding the fate and effects of organic chemicals at the watershed scale requires an understanding of partitioning behavior both in the soil and water column the partitioning process of the organic chemical can be very important in governing the environment fate and transport and the toxicity of the chemicals from the modeling perspective partitioning process has to be simulated before the transport process since it determines the fractions of different chemical phases the swat model used a two phases dissolved and particular phases partitioning for pesticides modeling and the impact of dissolved organic carbon doc on chemical partitioning has not been included ligaray et al 2017 the partitioning of organic chemicals to doc was indicated to be important in determining the bioavailability fate transformation and transport of the chemicals tronczynski 1992 therefore the organic chemicals partitioning to doc has to be modeled to better represent the chemical partition process ligaray et al 2016 used swat to model pahs at the watershed scale and the partitioning of pahs to doc in the soil was included to have a three phases partitioning simulation later they used the same approach to model the pesticide in the pagsanjan lumban basin philippines with three phases partitioning however the chemical partitioning to doc in the water column is not considered in our study the organic chemicals partitioning to doc in soil layers and water column were both simulated in addition organic chemical load that is transported to the stream by the groundwater has not been simulated by previous modeling studies organic chemicals like pahs have been detected in the groundwater aquifers in different regions brindha and elango 2014 mackay and gschwend 2001 and thus it is possible that groundwater flow can contain organic chemicals and transports the chemicals to the stream and other water bodies therefore the organic chemical load transported by groundwater flow should be modeled to improve the existing model by considering the contribution from groundwater flow to the total organic chemical load for this region the oil sands land is the main feature and activities that has not been taken into account in the existing hydrological models therefore oil sands land is explicitly depicted as a land use type to setup a swat model and the hydrologically closed circuited areas are characterized in swat model of this study the athabasca oil sands region is located in the cold climatic region where snowmelt and rainfall runoff could export the organic chemicals from landscape to the aquatic environment and an integrated modeling study is helpful for understanding the roles of different hydrological process in pollutants transport at watershed scale no studies have been reported about modeling organic chemicals at a watershed scale for both terrestrial and aquatic environments in cold climate condition like athabasca oil sands region this study tried to address the gap to have a watershed scale pahs simulation at muskeg river watershed in athabasca oil sands region by developing and integrating an organic chemical simulation into swat model the goals of this study are twofold firstly we aim to develop an organic chemical simulation module and to integrate it into the widely used swat model to provide a modeling a tool for simulating the fate and transport of organic chemicals at the watershed scale secondly the developed model is used for watershed scale pahs modeling in athabasca oil sands region to understand and assess the pathway fate and transport of organic pollutants at watershed scale in this region 2 materials and methods 2 1 study area the muskeg river watershed mrw is located in the athabasca oil sands region in lower part of athabasca river basin of northern alberta within the regional municipality of wood buffalo the river is a tributary to the athabasca river and drains an extensive area of boreal forest wetlands the area of the mrw is about 1433 km2 and there are several major tributaries including jackpine muskeg and wapasu creeks the mrw is underlain by mineable oil sands which are very important to the economics of alberta the mrw contains four operating oil sands mines which have modified the watershed landuse composition dramatically eum et al 2016 the land change area related to oil sands development in 2015 in the mrw is 238 9 km2 accounting for 16 7 of the total watershed area streamflow is monitored at the station of muskeg river near fort mckay station number 07da008 and can be downloaded from environmental canada and climate change website moreover sediment and water quality concentration such as nutrients metals and organic chemicals are also monitored in this station fig 1 a shows the location and extent of mrw and the observed stations 2 2 soil and water assessment tool swat swat model is watershed scale model developing for modeling the impact of land management practices on water sediment and agricultural chemical yields like nutrients and pesticides in large complex watersheds with varying landuse and soil conditions over long periods of time neitsch et al 2011 the hydrological processes in swat model include canopy interception infiltration surface runoff lateral flow snowmelt flow evapotranspiration deep percolation baseflow and water routing in the aqutic systems in addition soil erosion for each hru is simulated by modified universal soil loss equation musle and sediment routing is model by simulating sediment deposition and degradation using different model options in addition nutrient pesticides and bacterial are also modeled by simulating the chemical fate transport processes in both landscape and aquatic environments more detailed description of swat model processes is available from neitsch et al 2011 2 3 doc simulation in swat model doc concentrations in soil water and water column are needed to simulate a three phase partitioning for organic chemicals for doc concentration in soil water we used the organic matter om and a parameter as the fraction of doc to calculate the concentrations of different layers ligaray et al 2016 the equation of soil doc concentration was given as 1 c d o c s o l f d o c o m where c doc soil is the doc concentration in soil water mg l f doc is the fraction of doc model parameter and om is the concentration of organic matter mg l using swat parameters om was determined by dividing the mass of soil organic carbon in soil organic matter with the soil water content ligaray et al 2016 here we used the swat c model zhang et al 2013 which already has been incorporated into swat code to simulate the soil organic carbon cycle in soil layers and the mass of soil organic carbon at each daily time step are from swat c simulation then the doc concentration in soil water was estimated based on the simulated organic carbon mass and the parameter f doc in order to simulate three phases partitioning in the streams a simple regression model based on streamflow was used to estimate the doc concentrations in the stream the exploratory data analysis and inspection of the residuals suggest a linear model that relates log load to log streamflow runkel et al 2004 as 2 l o g load a 0 a 1 l o g flow where load is the observed daily doc load in the streams kg day flow is the observed daily streamflow m3 s and a o and a 1 are the two coefficient need to be estimated the linear regression between observed log load and log streamflow using the 22 doc measurements from 2015 to 2017 table s1 was showed in fig 2 once a o and a 1 were estimated the doc load can be estimated by the exponential equation from equation 2 and then the doc concentrations are estimated by the estimated doc load and the streamflow simulated by swat it s worth mentioning that the coefficients a o and a 1 have to be re calibrated using the observed data if it is applied in a different watershed 2 4 organic chemical simulation module in swat the organic chemical simulation module models the movement and fate of the organic pollutants at a watershed scale including the processes both in the landscape and aquatic system the landscape routine simulates the degradation and transport of the organic chemical in the soil layers of hur units in swat the aquatic routine receives the chemical loads simulated by the landscape routine and models the transport and fate of organic chemicals in the stream and other water bodies lake or reservoir from upstream to downstream 2 4 1 organic chemical processes in the landscape fig 3 shows the main processes modeled for the organic chemical processes in the landscape unit the landscape routine models the chemical processes including partition decay and transport to the aquatic system the modeled transport processes consist of chemical amount removed by soil erosion surface runoff lateral flow and groundwater in addition the leaching of dissolved and doc sorbed chemical from upper layer to the underlying layer is also simulated 2 4 1 1 organic chemical partitioning in soil layers partitioning also called sorption is the bonding of dissolved chemicals onto solid phases such as suspended solids biological material and sometimes dissolved or colloidal organic material multiple phase partitioning dissolved in water sorbed to doc and sorbed to solids is simulated in the soil layers sorption reactions are usually fast relative to other environmental processes and linear equilibrium partitioning is assumed in this module the fraction of different chemical phases is calculated as 3 f d s o i l c d s o i l c t s o i l 10 6 θ r s o i l 4 f p s o i l c p s o i l c t s o i l k p s o i l d s o i l r s o i l 5 f d o c s o i l c d o c s o i l c t s o i l k d o c s o i l θ c d o c s o i l r s o i l 6 r s o i l 10 6 θ k d o c s o i l θ c d o c s o i l k p s o i l d s o i l where c t soil is total concentration of the organic chemical in soil layers ug l 1 f d soil is the fraction of dissolved phase in soil layers f p soil is the fraction of solid sorbed phase in soil layers f doc soil is the fraction of doc sorbed phase in soil layers c doc soil is doc concentration in soil water simulated by the incorporated doc simulation module mg l 1 k p soil is partition coefficient for soil solids l kg 1 k doc soil is the doc partition coefficient l kg 1 d soil is the density of soil solid in soil layers mg l 1 θ is the soil water content expressed as a fraction of total soil volume the density of soil solid d soil mg l in each soil layer can be calculated by soil bulk density which is a swat soil property parameter soil water content θ can be calculated by the simulated soil moisture because the partitioning of organic chemical is dependent upon on the amount of organic matter in the soil the soil solids partition coefficient k p soil input to the model is normalized for soil organic carbon content 7 k p s o i l k o c o r g c 100 where k oc is the soil solid partition coefficient normalized for soil carbon content and orgc is the organic carbon content in the soil layers k oc and k doc soil are the two parameters for organic chemical partition process in the soil once the partition is simulated the amounts and concentrations of different chemical phases are determined and then the transport processes for different phases are modeled via different transport pathways 2 4 1 2 organic chemical decay in soil layers the reaction processes in soil layers include degradation photolysis hydrolysis volatilization and extra reaction in this simulation module these reactions are lumped together using a first order decay equation to calculate the total amount of chemical reactions in each soil layer for each simulation time step the landscape routine calculates the decay amount and mass changes of the chemicals in soil layers this decay process of chemicals whether they are dissolved or adsorbed to particles is modeled using the same equation and decay rate chemical decay amount in soil layers is computed as 8 m s o i l d e c t k d e c s o i l t f d s o i l f d o c s o i l f p s o i l m s o i l t where m soil dec t is chemical decay amount in the soil layers g ha 1 day 1 on a given day k dec soil t is the decay rate for organic chemical in the soil layers day 1 m soil is the total chemical amount in soil layers g ha 1 which is the model state variable in soil layers the decay rate is related to the half life as follows 9 k d e c s o i l 0 693 t 1 2 s where t 1 2 s is organic chemical half life in the soil d 1 which is a model parameter for the simulation module 2 4 1 3 organic chemical transport and leaching in soil layers the landscape routine simulates organic chemicals movement into stream network or other water bodies via sediment surface runoff lateral subsurface flow and groundwater flow chemicals transported by surface runoff lateral subsurface flow and groundwater flow are considered as dissolved and doc sorbed phase sediment transport of chemicals is considered as particular phase in addition dissolved and doc sorbed chemicals moving into underlying soil layers by leaching or percolation are simulated the chemical amounts contained in surface runoff lateral flow and percolation are estimated as products of mobile water volume and the average concentration of dissolved phase and doc sorbed in the soil layer the particular phase chemicals transported with sediments is simulated with a loading function of swat which is also used to calculate particular nutrient loads the loading function can estimate daily particular phase chemicals loss based on the concentration of particular phase chemicals in the top soil layer the sediment yield and the enrichment ratio which is the concentration of particular chemicals in the sediment divided by that in the soil the equations for organic chemical transport processes are as below 1 deposition a user defined flux rate is used to describe the atmospheric deposition of the organic chemicals the atmospheric deposition is added to the top soil layer which is given as 10 m a t m 10 2 l 0 where m atm is the deposition amount of organic chemical on a given day g ha and l 0 is areal deposition rate of the organic chemical ug m 2 day 1 which is a model parameter needs to be estimated by model calibration 2 particular organic chemical transported by the sediment organic chemicals attached to soil particles may be transported by sediment through soil erosion process to the main channel this phase of organic chemical is associated with sediment loading from swat hru unit and changes in sediment loading will impact the loading of particular phase chemicals the amount of organic chemicals transported with sediment to the stream is calculated using the loading function as follows neitsch et al 2011 11 m s o i l s e d c p s e d s e d a r e a h r u ξ c o n s e d where m soil sed is the amount of sorbed chemical transported from the top soil layer on a given day g ha c p sed is the concentration of organic chemical on sediment in the top soil g metric ton sed is the sediment yield on a given day metric tons area hru is the hru area ha and ξ con sed is chemical enrichment ratio the enrichment ratio is defined as the ratio of the concentration of sorbed chemical transported with sediment to the concentration in the top soil layer in swat enrichment ratio can be calculated for each storm event using empirical equation or be defined as a parameter that is used for all storm events during simulation period in this study enrichment ratio is a user define parameter which can be estimated by model calibration the concentration of particular chemical on sediment c p sed g ton in the top soil can be calculated from partitioning concentration in the top soil layer 12 c p s e d 10 3 c p s o i l s o l b d where c p soil ug l is the solid chemical concentration in top soil layer based on equilibrium partitioning sol bd is soil bulk density g cm3 of the soil layer which is the soil parameter in swat 3 dissolved and doc sorbed organic chemical transported by runoff components in order to calculate the chemical amounts transport by surface runoff and lateral flow the concentration of organic chemical dissolved and doc sorbed in mobile water is first calculated as 13 o r g c h m c o n m o b i l e o r g c h m l y f d s o i l f d o c s o i l 1 exp w m o b i l e s a t l y w m o b i l e where orgchm con mobile is the organic chemical concentration in the mobile water including the dissolved and doc sorbed phase g ha mm orgchm ly is the total amount of organic chemical in the soil layer g ha f d soil and f doc soil are the fractions of the dissolved and doc sorbed phase chemical calculated by the partitioning process then transported chemical loads are calculated by multiplying surface runoff and lateral flow volumes simulated by swat and the concentration in mobile water in addition the dissolved and doc sorbed organic chemicals could be moved downwardly to lower soil layers by the leaching processes and the percolation water volume simulated by swat and the retardation factor were used for the leaching simulation the equations used for simulating the chemical leaching can be found from equations s1 to s12 in the supplementary material in addition to the chemical transport by surface runoff and lateral flow the dissolved and doc sorbed chemicals can leach into the shallow aquifer and be transported to the stream by the groundwater flow a constant concentration of chemical concentration in the groundwater flow is used to calculate the chemical load transported with groundwater flow 14 m g w f c o n g w f q g w 10 5 where m gwf is the chemical amount transported by groundwater flow g ha con gwf is the chemical concentration in groundwater which is a model parameter subject to model calibration ng l and q gw is groundwater flow mm simulated by swat 2 4 2 organic chemical simulation in aquatic system the aquatic routine of organic chemical simulation module models the transport and transformation of the organic chemicals in stream and other water bodies the aquatic routine assumes a well mixed layer of water overlying a bed sediment layer fig 4 illustrates the processes included in the aquatic routine the processes simulated in the aquatic routine include chemical partitioning in the water and sediment layer chemical decay in both water and sediment layer chemical diffusion between water and sediment layer settling of particular chemical into sediment layer resuspension from sediment layer to water column and chemical transport with inflow and outflow 2 4 2 1 chemical partitioning process in water column three phase partitioning dissolved in water sorbed to doc and sorbed to solids is simulated in water column linear equilibrium partitioning is assumed in the water column and the fractions of different chemical phases is calculated as 15 f d w a t e r c d w a t e r c t w a t e r 10 6 r w a t e r 16 f p w a t e r c p w a t e r c t w a t e r k p w a t e r s e d c o n r w a t e r 17 f d o c w a t e r c d o c w a t e r c t w a t e r k d o c w a t e r d o c c o n w a t e r r w a t e r 18 r w a t e r 10 6 k d o c w a t e r d o c c o n w a t e r k p w a t e r s e d c o n where f d water is the fraction of dissolved phase in water column c t water is total concentration of organic chemical in water column ug l 1 f p water is the fraction of solid sorbed phase in water column c p water is the concentration of particular organic chemical in water column ug l 1 k p water is the partition coefficient for suspend solids in water column l kg 1 sed con is the suspend sediment concentration in water column simulated by swat mg l 1 f doc water is the fraction of doc sorbed phase in water column c doc water is the concentration of doc sorbed organic chemical in water column ug l 1 k doc water is the doc partition coefficient in water column l kg 1 and doc con water is doc concentration in water column mg l 1 simulated by the doc simulation module k p water and k doc water are two parameters for organic chemical partitioning processes in the water column 2 4 2 2 organic chemical decay and transport processes in water column the reaction processes in water column include degradation photolysis hydrolysis volatilization and extra reaction in this study these reactions are lumped together using a first order decay equation to calculate the total amount of chemical reactions in the water column chemical reaction amount in water column is computed as 19 m w t r d e c t k d e c w t r t f d w a t e r f d o c w a t e r f p w a t e r m w a t e r where m wtr dec t is chemical decay amount in the water column k dec wtr t is decay rate for organic chemical in water column d 1 and m water is total chemical amount in the water column ug the decay rate in water column is related to the half life as follows 20 k d e c w t r 0 693 t 1 2 w where t 1 2 w is organic chemical half life in the water column d 1 which is a model parameter for the simulation module the mass balance for organic chemical in water column is calculated as 21 δ m w a t e r min m o u t m w t r d e c m w t r s t l m s e d r s p m d i f where δm water is the change in chemical mass in the water column mg m in is the chemical amount added to the water column via inflow mg m out is the chemical amount removed via outflow mg m wtr dec is the amount of organic chemical removed from the water via decay mg m wtr stl is the amount of organic chemical removed from the water due to settling mg m sed rsp is the amount of organic chemical added to the water from resuspension from the sediment layer and m dif is the amount of organic chemical transferred between the water and sediment layer by diffusion the particular phase organic chemical might be removed from the water column by settling which transfers organic chemicals from the water to the sediment layer the amount of chemical that is removed from the water via the settling is calculated as 22 m w t r s t l v s t l d e p t h f p w a t e r m w a t e r t t where m wtr stl is the settling amount of the particular organic chemical mg v stl is the settling velocity m day which is a model parameter depth is the water depth simulated by swat routing routine m f p water is the fraction of organic chemical in the particular phase calculated by the partition process m water is the amount of organic chemical in the water and tt is the flow travel time days 2 4 2 3 organic chemical processes in the sediment layer in order to simulate the interaction resuspension and diffusion between water column and the sediment layer the organic chemical processes in the sediment layer underlying the water column are simulated the chemical amount in the sediment is increased through addition of mass by settling and diffusion from the water the chemical amount in the sediment is reduced by resuspension decay and diffusion into the overlying water column and the burial the mass balance for organic chemical in the sediment layer is calculated as 23 δ m s e d m w t r s t l m s e d d e c m s e d r s p m s e d b u r m d i f where δm sed is the change in organic chemical mass in the sediment layer mg m wtr stl is the amount of organic chemical added to the sediment layer from the setting of particular chemical in the water column m sed dec is the sediment decay amount mg m sed rsp is the amount of organic chemical removed from sediment layer via resuspension mg m sed bur is the amount of organic chemical removed via burial and m dif is the amount of organic chemical transferred between the water and sediment layer by diffusion the amount of organic chemical that is removed from sediment layer via decay process is calculated as 24 m s e d d e c k d e c s e d m s e d where m sed dec is the amount of organic chemical removed from sediment layer by the decay process mg k dec sed is the rate constant for chemical decay in the sediment mg and m sed is the amount of organic chemical in the sediment layer mg the decay rate constant is related to the sediment half life 25 k d e c s e d 0 693 t 1 2 s e d where t 1 2 sed is the sediment half life for organic chemical which is a model parameter subject to model calibration in addition the equations for calculating resupension sediment burial and diffusion between the water and sediment layer can be found from equation s12 to s15 in the supplementary material 2 5 swat model setup spatial and meteorological data are needed for setting up the swat model in the mrb the required spatial datasets are a digital elevation model dem landuse and soil data maps the dem data was obtained from geospatial data extraction of natural resources canada with the resolution of 0 75 arc second 18 7m 18 7m the 30m 30m landuse map from aafc http www agr gc ca atlas landu and oil sands land map http www rampalberta org data map mapdata aspx from regional aquatics monitoring program ramp were used to create landuse map of mrw the 1 1 million scale soil map from the agriculture and agri food canada aafc were used as model input to build soil database the dem was used to delineate subbasin and stream networks and 30 subbasins were delineated in the mrw a slope map is derived from the dem and divided into 3 groups breaks at 1 2 and 3 9 to define the hru 10 different landuse classes and 6 different soil types were defined for setting up the model in addition a threshold of 5 5 and 5 for land use soil and slope respectively were used to definite hrus and a total of 362 hrus were obtained as to meteorological input data daily precipitation maximum and minimum air temperature were obtained from 2 stations recorded by environmental canada and climate change solar radiation relative humidity and wind speed data at 5 stations recorded by climate forecast system reanalysis dile and srinivasan 2014 were also used as model input in addition we also defined 5 elevation bands in each subbasin to enable the model simulate snow melt processes in a distributed way and adjust the precipitation and temperature according to the elevation differences with weather gauge station 6 2 6 swat modification for oil sands land the oil sands mines and tailing ponds constructed in the mrw are now dramatic features of the watershed landscape the changes to the landscape by oil sands development indirectly affect the natural patterns of water and pollutants movement to reflect the oil sands impact on mrw oil sands lands are defined as the landuse type to setup swat model specifically the oil sands land map from ramp was overlapped with the aafc landuse map to define oil sands in the landuse classifications fig 1b shows the landuse distribution in the mrw and two types of oil sands landuses were defined according the different hydrological features if the oil sands areas have no natural exchange of water with the rest of the watershed e g tailings ponds they are designated as hydrologically closed circuited if the oil sands areas have natural exchange of water with the rest of the watershed they are designated as not hydrologically closed circuited in the mrw there are 73 7 km2 hydrologically closed circuited oil sands area and 146 7 km2 not hydrologically closed circuited which account for 4 9 and 9 8 percentage of the total watershed area respectively there are 101 hrus for oil sands area for swat setup in the mrw and 30 of them are hydrologically closed circuited since there is no hydrological interaction between hydrologically closed oil sands area and the rest of watershed the swat model was modified to reflect this unique feature specifily the water yield surface runoff lateral flow and groundwater flow sediment and organic chemical loads of hydrologically closed oil sands hru are not routed to the stream as such 3 swat original subroutines and 1 organic chemical subroutine were modified for hydrologically closed oil sands hrus 2 7 model calibration and validation since the pahs observed data are only available for a period of 2015 2017 we calibrated and validated the streamflow sediment and pahs simulations using the same time period on a daily time scale as such the year of 2016 was selected as the calibration period because the pahs measurements covered the whole year from january to december the years of 2015 and 2017 were selected as the validation period as there were only 10 measurements during the validation period 7 measurements in 2015 from may to december and 3 measurements in 2017 from january to march a three year warm up period 2012 2014 was used to attenuate the impact of initial conditions on the model simulations first daily streamflow data station number 07da008 at the outlet station collected from environmental canada and climate change was used for the streamflow calibration and validation the observed sediment and two pahs phenanthrene and pyrene concentrations at the outlet station and the sampling dates were showed in table s1 those concentrations were obtained from the ramp monitoring datasets and were measured once a month from june 2015 to march 2017 there were 20 measurements for sediment concentration with an average concentration of 3 6 mg l and there were 22 measurements for phenanthrene phe and pyrene pyr with the average concentrations of 1 6 and 0 8 ng l respectively the observed daily streamflow from 2015 to 2017 range from 0 15 to 14 4 m3 s and the streamflow of the 22 pahs sampling dates range from 0 18 to 14 4 m3 s which indicates that the measured pahs concentration could represent the pahs variability that may exist in the mrw from 2015 to 2017 we used the measured sediment and pahs concentrations and the daily observed streamflow of the same day to estimate the loads next the estimated periodic loads were used for sediment and pahs calibration and validation the model calibration was then conducted by combining the automatic calibration using swat cup abbaspour 2013 and manual calibration in the following first the parameter sensitivity analysis results were obtained by the global sensitivity analysis algorithm of swat cup for automatic calibration sufi 2 algorithm from swat cup was used based on 2000 simulations with the defined parameter rages then we manually adjusted the one or two most sensitive parameters based on the best parameter set from the automatic calibration to get better model performance statistics we found that manually adjusting the most sensitive parameters after the automatic calibration was very useful to improve the model performances since the new parameters added for organic chemical simulation module are not in the swat cup parameter list we borrowed swat pesticide parameters to perform pahs parameters calibrations in swat cup specifically we wrote the scripts pah parameters equal to the pesticide parameters in the code and used the columns of pesticides in output rch file to output pahs simulation results then we compiled an exe file based on the organic chemical module code to replace the original swat exe file in swat cup directory the pesticide parameters and pesticide column number in output rch file were used to setup sufi 2 project for pahs calibrations in swat cup so when pesticide parameters are changed in swat cup the pahs parameter values and simulation results will change accordingly we admitted that this is not a direct way to facilitate the automatic calibration in swat cup but is able to indirectly perform the pahs automatic calibration and sensitivity analysis in this study the root mean square error rmse was used as the objective function and the model parameters were calibrated to minimize the value of rmse during the calibration period du et al 2014 in addition percentage biases pbias coefficient of determination r2 and nash sutcliffe efficiency nse were also used as the measures of model performance zhang et al 2013 the objective function rmse and the model performance measures are defined as 26 r m s e t 1 n y s i m t y o b s t 2 n 27 p b i a s t 1 n y s i m t y o b s t t 1 n y o b s t 100 28 r 2 t 1 n y s i m t y s i m y o b s t y o b s t 1 n y s i m t y s i m 2 t 1 n y s i m t y s i m 2 2 29 n s e 1 t 1 n y o b s t y s i m t 2 t 1 n y o b s t y o b s 2 where y t sim is the simulated value at a time unit t y t obs is the observed value at a time unit t n is the total data number y s i m is the mean observed value for the simulated period and y o b s is the mean simulated value of the simulated period pbias measures the average tendency of the simulated data to be larger or smaller than the observed data and is sensitive to the systematic error chen et al 2017 r2 ranges from 0 to 1 and represents the proportion of the total variance in the observed data that is explained by the model zhang et al 2013 nse ranges from negative infinity to 1 and assesses how well the model to data plot fits the 1 1 line in addition we also assessed the model simulations based on the performance evaluation criteria for watershed models summarized by moriasi et al 2015 which evaluates the model performance from unsatisfactory to very good based on the range of values of chosen goodness of fit statistics as to model uncertainties analysis the 95 prediction uncertainty band obtained from sufi 2 algorithm was used to quantify the simulation uncertainties the simulation uncertainties were assessed by the p factor and r factor chen et al 2017 the p factor is the percentage of the observed data bracketed by the uncertainty band and the r factor is the normalized width of the band 3 results and discussion 3 1 hydrological calibration and streamflow simulation to accurately simulate the transport of organic chemicals the hydrological processes simulated by the swat model should confirm to actual hydrological cycle of the watershed therefore it is imperative to perform the hydrologic calibration before calibrating the organic chemicals a total of 25 parameters were used for streamflow calibration and the parameter ranges and calibrated values of the top 10 most sensitive parameters were showed in table 1 the parameter cn2 scs curve number for moisture condition ii was found to be the most sensitive parameter which controls the surface runoff yield and the water amount infiltrating into the soil layers moreover the parameter surlag which controls the surface runoff routing in the subbasin scale was found to be the second most sensitive parameter which indicates that the surface runoff is the important hydrological process in the mrw two snowmelt parameters sftmp and smtmp were also among the top 10 sensitive parameters list further validating the need to accurately represent snow related process in such a cold climate region watershed this is indeed expected in such a cold region watershed because snowmelt is the dominant hydrological process in the spring season other sensitive parameters include the parameters affecting the processes of channel routing ch k2 groundwater flow gw delay and revapmn and evaporation esco the model performance statistics for daily streamflow are shown in table 2 the comparison between simulated and observed daily steamflow was shown in fig 5 a the visual plot indicated that the swat hydrology model could provide acceptable simulation results as the simulated streamflow tend to follow a similar trend to the observed data and are capable to fairly capture both the diurnal as well as the seasonal variations as per moriasi et al 2015 evaluation criteria the quality of daily stream flow is of satisfactory quality based on r2 value r2 0 5 based on the nse values the daily flow simulations of the calibration and whole period were evaluated as satisfactory but unsatisfactory for the validation period however the nse value 0 41 of the validation period is much greater than the mean nse value 0 13 of daily streamflow simulation applications summarized by moriasi et al 2015 in addition the performance rating during the calibration period is of very good quality based on the pbias value 1 1 however a high pbias value 54 2 during the validation period points to the unsatisfactory quality of the model simulation it is indeed true that there could be conflicting performance ratings during calibration and validation periods as pointed out by chen et al 2017 when using swat model as different goodness of fit statistics tend to capture distinct aspects of model performances furthermore such overestimation of daily streamflow during the validation period was mainly caused by the overestimation of low base flow season in the year of 2017 however the low flow of the years 2015 and 2016 were simulated quite well fig 5a as the groundwater flow is the dominant runoff component in such low flow seasons we aimed to adjust the groundwater parameters heuristically to match the streamflow hydrograph in the low flow season of the year 2017 and such low flow can be only matched when the groundwater flow was adjusted as zero this indicates possible unreliable observations and points to the need of using more streamflow data for hydrological calibration however in order to keep the calibration and validation period of streamflow same as to that of pah we used the streamflow data from for the years 2015 2017 the statistics for streamflow simulation uncertainties were showed in table 3 on average 45 of the variation in streamflow was bracketed by the 95 uncertainty band and r factor 0 75 was less than 1 suggesting acceptable thickness of the band 3 2 sediment load simulation after the streamflow calibration and validation the sediment load was calibrated and validated since sediment directly affects the transport amount of particular organic chemicals 9 sediment related parameters were chosen for model calibration the sensitivity ranking and calibrated values of these parameters were shown in table 4 the parameter prf bsn is found to be the most sensitive sediment parameter which determines the maximum amount of sediment that can be transported through a reach the second and third most sediment parameters were other two parameters used for calculating the transported maximum sediment load through a reach overall the reach sediment routing parameters ranking from first to fifth were found to be more sensitive than the soil erosion parameters ranking from sixth to ninth this indicates that the sediment routing process in the reach is more influential in the mrw than the upland soil erosion and transport processes various researchers e g shrestha and wang 2018 and zabaleta et al 2014 etc also reported similar findings the model performance statistics of sediment load was shown in table 2 and the comparison between the simulated and observed daily sediment load was shown in fig 5b the simulation performance of sediment load for the whole simulation period 2015 2017 was found to be of good quality while considering the r2 0 74 of very good while considering the pbias 8 5 value and of satisfactory while considering the nse 0 67 value the simulation performance for the calibration period was of good quality as per r2 and of satisfactory as per pbias and nse values however the simulation performance for the validation period was of unsatisfactory quality as per r2 pbias and nse values in which the sediment load was clearly overestimated this was partially caused by the overestimation of streamflow during the validation period furthermore limited number of data available to use for sediment calibration and validation only 20 measurements also played a significant role only 10 daily sediment loads were available during calibration period 2016 which ranged from 0 03 to 16 2 ton day similarly only10 daily sediment loads were available during validation period 2015 and 2017 which ranged from 0 03 to 1 3 ton day it is clear that the sediment loads used for calibration were much larger than those used for validation therefore the sediment loads during the validation period were overestimated since the parameters were tuned to capture those much larger sediment loads observed during the calibration period ideally the observation data used for model calibration should include the data observed during different streamflow conditions such as those during low flow normal flow and high flow condition such data would ensure a more robust model calibration and validation however in our case the observed sediment and pahs data in the mrw were rather limited because the ramp water quality monitoring program in athabasca oil sands region only started from june 2015 and the water quality measurements are only collected at the monthly frequency as to sediment load simulation uncertainties table 3 70 of the measured data were bracketed by the 95 uncertainty band during the simulation period 2015 2017 and r factor 0 76 was also less than 1 3 3 model simulation for two pahs in muskeg river watershed the periodic daily pahs loads at the watershed outlet were used for calibrating and validating the organic chemical simulation module 19 pahs related parameters with sensitive ranking their parameter ranges and optimized values were shown in table 5 of all the most sensitive parameter was found to be the deposition rate which is the pollution source parameter determining the amount of the pahs depositing to the top soil layers the second most sensitive parameter was found to be the percolation coefficient for the pahs transported by surface runoff which further highlights the importance of the surface runoff as the pathway of the pahs in the mrw besides these two most sensitive parameters 4 other parameters affecting the pahs interactions between water column and sediment layer were ranked from third to sixth which equally reflect the importance of interactions between water column and sediment layer for the pahs fate and transport in the mrw in addition the doc partition coefficient in water column was ranked as seventh most sensitive parameter which indicates the necessity to simulate the pah partitioning to the doc the model performance statistics of the pahs loads were shown in table 2 and the comparisons between the simulated and observed daily pahs loads were showed in fig 5c and d in this study as already depicted the performance evaluation criteria for nutrient simulation as summarized by moriasi et al 2015 were used to assess the pahs simulation performance the simulation performance of phe load for the whole simulation period 2015 2017 was found to be of very good quality as per r2 0 73 and nse 0 71 values and pbias 5 8 values the simulation performance of phe load for the calibration period was rated to be of very good quality as per r2 and nse values and of good quality as per pbias value the simulation performance of phe load for the validation period was rated to be of satisfactory quality as per r2 but of unsatisfactory as per pbias and nse values the simulation performance of pyr load for the whole simulation period 2015 2017 was found to be of very good quality as per r2 0 73 and of good quality as per nse 0 64 and pbias 16 8 value the simulation performance of pyr load during the calibration period was rated to be of very good as per r2 of very good as per nse value and of satisfactory quality as per pbias value similar to phe load simulation performance for the validation period the pyr load simulation was also of satisfactory quality as per r2 but of unsatisfactory as per nse and pbias values the phe and pyr loads were overestimated during the validation period and such overestimation was partially caused by the overestimations of both streamflow and sediment load as pointed out in the previous sections as identified for the sediment load the overestimation was also caused by the limited data available to be used for pahs calibration and validation since the loads used for the calibration were much larger than those used during the validation period in addition the peak values of the pah loads on june 22 2016 were severely underestimated fig 5 and this caused underestimations of streamflow and sediment on this very day as a continuous model with daily step the swat model would underestimate streamflow sediment and chemical loads for the days with short intense storm events our results are consistent with other studies conduction in the mrw for instance eum et al 2016 used the vic variable infiltration capacity model to simulate streamflow in the mrw and results showed similar underestimation of the peak flows during high flow seasons in addition we also check the impact of peak values in 2016 on the pahs model performances during calibration period in 2016 specifically we compared the model performances statistics of the observed data excluding peak values with original performances statistics with the peak values the results showed that the model performances statistics deteriorated a lot when the peak values were excluded for calculating model performances statistics for example the nse values decreased to 0 30 and 0 40 for phe and pyr simulation and r2 decreased to 0 37 and 0 41 this is because the best parameter set was calibrated by including the peak values and the parameter values were obtained towards matching the peak values even though the model underestimated the peaks the peak values were still useful to constraint the model and parameter values for uncertainty analysis 77 of the observed phe and pyr loads were bracketed by the 95 uncertainties bands table 3 and the r factors were both less than 1 suggesting acceptable thickness of the bands in addition to daily pahs loads we also simulated monthly and annual pahs loads to further analyze the characteristics of the pahs fate and transport in the mrw the annual phe loads for the years 2015 2017 were found to be 139 9 194 6 and 202 7 g year respectively the annual pyr loads for the years 2015 2017 were found to be 70 7 98 5 and 103 5 g year respectively the pahs loads in the years 2016 and 2017 were much larger than that of 2015 because these two year were relatively wet years compared to 2015 for the monthly pahs loads the maximum pah loads would be delivered in june accounting almost 40 and 20 g phe and pyr in addition the summer season june to september accounted for 65 2 and 66 2 of the annual phe and pyr loads this indicates that the summer season is the hot moment for pahs transport in the mrw 4 conclusion an organic chemical simulation module was developed and integrated into one of the widely used watershed model the soil and water assessment tool swat to simulate the fate and transport processes of the organic chemicals at watershed scale the chemical module considers three phase portioning process by simulating chemicals portioning to dissolved organic carbon doc in soil water and streams we demonstrated the module s capability by simulating the dynamics of the two polycyclic aromatic hydrocarbons pahs in a cold climate region watershed muskeg river watershed in the athabasca oil sands region of western canada where the organic chemicals pose a threat to the water quality status of receiving waters the hydrological and sediment dynamics were first calibrated and validated before pahs calibrations using daily streamflow and periodic daily sediment loads for a period of 2015 2017 at the watershed outlet the simulation performances for the pahs loads for the whole simulation period 2015 2017 were assessed as satisfactory however the simulation performances in the validation period showed conflicting results the model simulation results indicate that the summer season is the hot moment for the pahs transport mainly driving by the intense rainfall runoff events in this season the developed simulation module could be a useful modeling tool to simulate the fate and transport of organic chemicals at a watershed scale however more model applications are required to further verify the model capability in other climatic regions of the world currently the doc simulations in soil water and streams are the simplifications of the complex carbon cycle processes in the watershed therefore a process based watershed scale doc model coupling with swat organic chemical simulation module holds the potential to further improve the simulations of chemical partitioning processes and fate and transport in the watershed acknowledgements the authors would like to thank the alberta economic development and trade for the campus alberta innovates program research chair no rcp 12 001 bcaip appendix a supplementary data the following is the supplementary data to this article sm revised2 v2 sm revised2 v2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 011 
26303,an integrated watershed model is a key tool for assessing the fate and transport of organic chemicals we developed and integrated an organic chemical simulation module into soil water assessment tool swat model and used it for polycyclic aromatic hydrocarbons pahs simulation in muskeg river watershed in athabasca oil sands region western canada the model uses a three phase partition process to simulate the chemicals partitioning to dissolved organic carbon doc in both soil water and streams the simulation performances for phenanthrene and pyrene loads for a period of 2015 2017 were assessed to be of satisfactory quality the model simulation results indicated that the summer season is the hot moment and the rainfall runoff event is the driving force for the pahs transport in the watershed the developed simulation module could be a useful modeling tool to simulate and asess the pathway fate and transport of organic chemicals at the watershed scale graphical abstract image 1 keywords swat model organic chemical simulation three phase partition polycyclic aromatic hydrocarbons pahs software availability name of software swat organic chemical simulation module developer xinzhong du contact address 1 university dr athabasca ab t9s 3a3 canada and e mail xinzhong du gmail com year first available 2018 program language fortran availability contact developer by e mail 1 introduction organic chemicals can cause local and regional losses of freshwater biodiversity and ecosystem services of the 223 chemicals monitored globally pesticides tributyltin polycyclic aromatic hydrocarbons pahs and brominated flame retardants were the major contributors to the chemical risk malaj et al 2014 particularly organic chemicals can impair the designated water use if their concentrations exceed water quality standards they can be exported to stream and other water bodies by point source or transport from landscape environment oil sands are found in about 70 countries and the largest deposits of oil sands exist in canada and venezuela with smaller resources in russia europe and the u s hirsch et al 2005 the oil production in oil sands region could pose threat to the local environment the athabasca oil sands or tar sands are large deposits of bitumen or extremely heavy crude oil located in northeastern alberta canada which have been subject of various environmental studies since suncor energy inc began its operation in 1967 evans et al 2016 since then the oil sands industry has expanded exponentially and there are environmental concerns from the industry activities associated with surface mining in situ recovery and upgrading of bitumen kurek et al 2013 kelly et al 2009 conducted an independent and accessible assessment of the loading of polycyclic aromatic compounds pac to the athabasca river and its tributaries and found that the oil sands industry is a far greater source of regional pac contamination than previously realized due to substantial loadings of airborne pac they pointed out that controls on waterborne pac are critical because concentrations at tributary mouths and at one site on the athabasca river are already within the range toxic to fish embryos kelly et al 2009 moreover kelly et al 2010 found that oil sands industry substantially increases the loadings of toxic metal elements to the athabasca river and its tributaries via air and water pathways and recommended a detailed long term monitoring to distinguish the sources of the contaminates and control the potential impacts collectively these industrial activities yield significant landscape disturbance and habitat loss and lead to the controversy water quantity and quality issues kurek et al 2013 monitoring methods for organic chemicals have been conducted to assess the impact on water quality indentify pollution source and evaluate the efficiency of pollution mitigation technique bourgeault and gourlay france 2013 imfeld et al 2009 morandi et al 2015 however it is very expensive and time consuming to conduct an intensive and long term monitoring program du et al 2016 on the other hand an integrated watershed model is a cost effective and powerful tool for quantitatively predicting the fate and transport of organic chemicals in both terrestrial and aquatic environments at the watershed scale to support environmental exposure and risk assessments pollution control planning and decision making chen et al 2017 therefore models have been used for simulation of pahs in the main stream of lower athabasca river using one dimensional shakibaeinia et al 2017 and two dimensional kashyap et al 2017 hydrodynamic and transport models however while these models were successful to represent the organic chemical processes in the river system they were lack of description of the transport of the chemicals from terrestrial to aquatic environment a watershed scale model the soil and water assessment tool swat arnold et al 2012 has been widely used for simulating streamflow shrestha et al 2017 zhang et al 2008 water temperature du et al 2017 sediment and nutrient loads jha et al 2004 on different temporal and spatial scales the swat model is also able to simulate the pesticides which is one of specific organic chemicals and it has been used for modeling pesticides in different watersheds chen et al 2017 fohrer et al 2014 larose et al 2007 however organic pollutants are not limited to the pesticides a module should also be able to simulate other organic chemicals such as pahs and naphthenic acids understanding the fate and effects of organic chemicals at the watershed scale requires an understanding of partitioning behavior both in the soil and water column the partitioning process of the organic chemical can be very important in governing the environment fate and transport and the toxicity of the chemicals from the modeling perspective partitioning process has to be simulated before the transport process since it determines the fractions of different chemical phases the swat model used a two phases dissolved and particular phases partitioning for pesticides modeling and the impact of dissolved organic carbon doc on chemical partitioning has not been included ligaray et al 2017 the partitioning of organic chemicals to doc was indicated to be important in determining the bioavailability fate transformation and transport of the chemicals tronczynski 1992 therefore the organic chemicals partitioning to doc has to be modeled to better represent the chemical partition process ligaray et al 2016 used swat to model pahs at the watershed scale and the partitioning of pahs to doc in the soil was included to have a three phases partitioning simulation later they used the same approach to model the pesticide in the pagsanjan lumban basin philippines with three phases partitioning however the chemical partitioning to doc in the water column is not considered in our study the organic chemicals partitioning to doc in soil layers and water column were both simulated in addition organic chemical load that is transported to the stream by the groundwater has not been simulated by previous modeling studies organic chemicals like pahs have been detected in the groundwater aquifers in different regions brindha and elango 2014 mackay and gschwend 2001 and thus it is possible that groundwater flow can contain organic chemicals and transports the chemicals to the stream and other water bodies therefore the organic chemical load transported by groundwater flow should be modeled to improve the existing model by considering the contribution from groundwater flow to the total organic chemical load for this region the oil sands land is the main feature and activities that has not been taken into account in the existing hydrological models therefore oil sands land is explicitly depicted as a land use type to setup a swat model and the hydrologically closed circuited areas are characterized in swat model of this study the athabasca oil sands region is located in the cold climatic region where snowmelt and rainfall runoff could export the organic chemicals from landscape to the aquatic environment and an integrated modeling study is helpful for understanding the roles of different hydrological process in pollutants transport at watershed scale no studies have been reported about modeling organic chemicals at a watershed scale for both terrestrial and aquatic environments in cold climate condition like athabasca oil sands region this study tried to address the gap to have a watershed scale pahs simulation at muskeg river watershed in athabasca oil sands region by developing and integrating an organic chemical simulation into swat model the goals of this study are twofold firstly we aim to develop an organic chemical simulation module and to integrate it into the widely used swat model to provide a modeling a tool for simulating the fate and transport of organic chemicals at the watershed scale secondly the developed model is used for watershed scale pahs modeling in athabasca oil sands region to understand and assess the pathway fate and transport of organic pollutants at watershed scale in this region 2 materials and methods 2 1 study area the muskeg river watershed mrw is located in the athabasca oil sands region in lower part of athabasca river basin of northern alberta within the regional municipality of wood buffalo the river is a tributary to the athabasca river and drains an extensive area of boreal forest wetlands the area of the mrw is about 1433 km2 and there are several major tributaries including jackpine muskeg and wapasu creeks the mrw is underlain by mineable oil sands which are very important to the economics of alberta the mrw contains four operating oil sands mines which have modified the watershed landuse composition dramatically eum et al 2016 the land change area related to oil sands development in 2015 in the mrw is 238 9 km2 accounting for 16 7 of the total watershed area streamflow is monitored at the station of muskeg river near fort mckay station number 07da008 and can be downloaded from environmental canada and climate change website moreover sediment and water quality concentration such as nutrients metals and organic chemicals are also monitored in this station fig 1 a shows the location and extent of mrw and the observed stations 2 2 soil and water assessment tool swat swat model is watershed scale model developing for modeling the impact of land management practices on water sediment and agricultural chemical yields like nutrients and pesticides in large complex watersheds with varying landuse and soil conditions over long periods of time neitsch et al 2011 the hydrological processes in swat model include canopy interception infiltration surface runoff lateral flow snowmelt flow evapotranspiration deep percolation baseflow and water routing in the aqutic systems in addition soil erosion for each hru is simulated by modified universal soil loss equation musle and sediment routing is model by simulating sediment deposition and degradation using different model options in addition nutrient pesticides and bacterial are also modeled by simulating the chemical fate transport processes in both landscape and aquatic environments more detailed description of swat model processes is available from neitsch et al 2011 2 3 doc simulation in swat model doc concentrations in soil water and water column are needed to simulate a three phase partitioning for organic chemicals for doc concentration in soil water we used the organic matter om and a parameter as the fraction of doc to calculate the concentrations of different layers ligaray et al 2016 the equation of soil doc concentration was given as 1 c d o c s o l f d o c o m where c doc soil is the doc concentration in soil water mg l f doc is the fraction of doc model parameter and om is the concentration of organic matter mg l using swat parameters om was determined by dividing the mass of soil organic carbon in soil organic matter with the soil water content ligaray et al 2016 here we used the swat c model zhang et al 2013 which already has been incorporated into swat code to simulate the soil organic carbon cycle in soil layers and the mass of soil organic carbon at each daily time step are from swat c simulation then the doc concentration in soil water was estimated based on the simulated organic carbon mass and the parameter f doc in order to simulate three phases partitioning in the streams a simple regression model based on streamflow was used to estimate the doc concentrations in the stream the exploratory data analysis and inspection of the residuals suggest a linear model that relates log load to log streamflow runkel et al 2004 as 2 l o g load a 0 a 1 l o g flow where load is the observed daily doc load in the streams kg day flow is the observed daily streamflow m3 s and a o and a 1 are the two coefficient need to be estimated the linear regression between observed log load and log streamflow using the 22 doc measurements from 2015 to 2017 table s1 was showed in fig 2 once a o and a 1 were estimated the doc load can be estimated by the exponential equation from equation 2 and then the doc concentrations are estimated by the estimated doc load and the streamflow simulated by swat it s worth mentioning that the coefficients a o and a 1 have to be re calibrated using the observed data if it is applied in a different watershed 2 4 organic chemical simulation module in swat the organic chemical simulation module models the movement and fate of the organic pollutants at a watershed scale including the processes both in the landscape and aquatic system the landscape routine simulates the degradation and transport of the organic chemical in the soil layers of hur units in swat the aquatic routine receives the chemical loads simulated by the landscape routine and models the transport and fate of organic chemicals in the stream and other water bodies lake or reservoir from upstream to downstream 2 4 1 organic chemical processes in the landscape fig 3 shows the main processes modeled for the organic chemical processes in the landscape unit the landscape routine models the chemical processes including partition decay and transport to the aquatic system the modeled transport processes consist of chemical amount removed by soil erosion surface runoff lateral flow and groundwater in addition the leaching of dissolved and doc sorbed chemical from upper layer to the underlying layer is also simulated 2 4 1 1 organic chemical partitioning in soil layers partitioning also called sorption is the bonding of dissolved chemicals onto solid phases such as suspended solids biological material and sometimes dissolved or colloidal organic material multiple phase partitioning dissolved in water sorbed to doc and sorbed to solids is simulated in the soil layers sorption reactions are usually fast relative to other environmental processes and linear equilibrium partitioning is assumed in this module the fraction of different chemical phases is calculated as 3 f d s o i l c d s o i l c t s o i l 10 6 θ r s o i l 4 f p s o i l c p s o i l c t s o i l k p s o i l d s o i l r s o i l 5 f d o c s o i l c d o c s o i l c t s o i l k d o c s o i l θ c d o c s o i l r s o i l 6 r s o i l 10 6 θ k d o c s o i l θ c d o c s o i l k p s o i l d s o i l where c t soil is total concentration of the organic chemical in soil layers ug l 1 f d soil is the fraction of dissolved phase in soil layers f p soil is the fraction of solid sorbed phase in soil layers f doc soil is the fraction of doc sorbed phase in soil layers c doc soil is doc concentration in soil water simulated by the incorporated doc simulation module mg l 1 k p soil is partition coefficient for soil solids l kg 1 k doc soil is the doc partition coefficient l kg 1 d soil is the density of soil solid in soil layers mg l 1 θ is the soil water content expressed as a fraction of total soil volume the density of soil solid d soil mg l in each soil layer can be calculated by soil bulk density which is a swat soil property parameter soil water content θ can be calculated by the simulated soil moisture because the partitioning of organic chemical is dependent upon on the amount of organic matter in the soil the soil solids partition coefficient k p soil input to the model is normalized for soil organic carbon content 7 k p s o i l k o c o r g c 100 where k oc is the soil solid partition coefficient normalized for soil carbon content and orgc is the organic carbon content in the soil layers k oc and k doc soil are the two parameters for organic chemical partition process in the soil once the partition is simulated the amounts and concentrations of different chemical phases are determined and then the transport processes for different phases are modeled via different transport pathways 2 4 1 2 organic chemical decay in soil layers the reaction processes in soil layers include degradation photolysis hydrolysis volatilization and extra reaction in this simulation module these reactions are lumped together using a first order decay equation to calculate the total amount of chemical reactions in each soil layer for each simulation time step the landscape routine calculates the decay amount and mass changes of the chemicals in soil layers this decay process of chemicals whether they are dissolved or adsorbed to particles is modeled using the same equation and decay rate chemical decay amount in soil layers is computed as 8 m s o i l d e c t k d e c s o i l t f d s o i l f d o c s o i l f p s o i l m s o i l t where m soil dec t is chemical decay amount in the soil layers g ha 1 day 1 on a given day k dec soil t is the decay rate for organic chemical in the soil layers day 1 m soil is the total chemical amount in soil layers g ha 1 which is the model state variable in soil layers the decay rate is related to the half life as follows 9 k d e c s o i l 0 693 t 1 2 s where t 1 2 s is organic chemical half life in the soil d 1 which is a model parameter for the simulation module 2 4 1 3 organic chemical transport and leaching in soil layers the landscape routine simulates organic chemicals movement into stream network or other water bodies via sediment surface runoff lateral subsurface flow and groundwater flow chemicals transported by surface runoff lateral subsurface flow and groundwater flow are considered as dissolved and doc sorbed phase sediment transport of chemicals is considered as particular phase in addition dissolved and doc sorbed chemicals moving into underlying soil layers by leaching or percolation are simulated the chemical amounts contained in surface runoff lateral flow and percolation are estimated as products of mobile water volume and the average concentration of dissolved phase and doc sorbed in the soil layer the particular phase chemicals transported with sediments is simulated with a loading function of swat which is also used to calculate particular nutrient loads the loading function can estimate daily particular phase chemicals loss based on the concentration of particular phase chemicals in the top soil layer the sediment yield and the enrichment ratio which is the concentration of particular chemicals in the sediment divided by that in the soil the equations for organic chemical transport processes are as below 1 deposition a user defined flux rate is used to describe the atmospheric deposition of the organic chemicals the atmospheric deposition is added to the top soil layer which is given as 10 m a t m 10 2 l 0 where m atm is the deposition amount of organic chemical on a given day g ha and l 0 is areal deposition rate of the organic chemical ug m 2 day 1 which is a model parameter needs to be estimated by model calibration 2 particular organic chemical transported by the sediment organic chemicals attached to soil particles may be transported by sediment through soil erosion process to the main channel this phase of organic chemical is associated with sediment loading from swat hru unit and changes in sediment loading will impact the loading of particular phase chemicals the amount of organic chemicals transported with sediment to the stream is calculated using the loading function as follows neitsch et al 2011 11 m s o i l s e d c p s e d s e d a r e a h r u ξ c o n s e d where m soil sed is the amount of sorbed chemical transported from the top soil layer on a given day g ha c p sed is the concentration of organic chemical on sediment in the top soil g metric ton sed is the sediment yield on a given day metric tons area hru is the hru area ha and ξ con sed is chemical enrichment ratio the enrichment ratio is defined as the ratio of the concentration of sorbed chemical transported with sediment to the concentration in the top soil layer in swat enrichment ratio can be calculated for each storm event using empirical equation or be defined as a parameter that is used for all storm events during simulation period in this study enrichment ratio is a user define parameter which can be estimated by model calibration the concentration of particular chemical on sediment c p sed g ton in the top soil can be calculated from partitioning concentration in the top soil layer 12 c p s e d 10 3 c p s o i l s o l b d where c p soil ug l is the solid chemical concentration in top soil layer based on equilibrium partitioning sol bd is soil bulk density g cm3 of the soil layer which is the soil parameter in swat 3 dissolved and doc sorbed organic chemical transported by runoff components in order to calculate the chemical amounts transport by surface runoff and lateral flow the concentration of organic chemical dissolved and doc sorbed in mobile water is first calculated as 13 o r g c h m c o n m o b i l e o r g c h m l y f d s o i l f d o c s o i l 1 exp w m o b i l e s a t l y w m o b i l e where orgchm con mobile is the organic chemical concentration in the mobile water including the dissolved and doc sorbed phase g ha mm orgchm ly is the total amount of organic chemical in the soil layer g ha f d soil and f doc soil are the fractions of the dissolved and doc sorbed phase chemical calculated by the partitioning process then transported chemical loads are calculated by multiplying surface runoff and lateral flow volumes simulated by swat and the concentration in mobile water in addition the dissolved and doc sorbed organic chemicals could be moved downwardly to lower soil layers by the leaching processes and the percolation water volume simulated by swat and the retardation factor were used for the leaching simulation the equations used for simulating the chemical leaching can be found from equations s1 to s12 in the supplementary material in addition to the chemical transport by surface runoff and lateral flow the dissolved and doc sorbed chemicals can leach into the shallow aquifer and be transported to the stream by the groundwater flow a constant concentration of chemical concentration in the groundwater flow is used to calculate the chemical load transported with groundwater flow 14 m g w f c o n g w f q g w 10 5 where m gwf is the chemical amount transported by groundwater flow g ha con gwf is the chemical concentration in groundwater which is a model parameter subject to model calibration ng l and q gw is groundwater flow mm simulated by swat 2 4 2 organic chemical simulation in aquatic system the aquatic routine of organic chemical simulation module models the transport and transformation of the organic chemicals in stream and other water bodies the aquatic routine assumes a well mixed layer of water overlying a bed sediment layer fig 4 illustrates the processes included in the aquatic routine the processes simulated in the aquatic routine include chemical partitioning in the water and sediment layer chemical decay in both water and sediment layer chemical diffusion between water and sediment layer settling of particular chemical into sediment layer resuspension from sediment layer to water column and chemical transport with inflow and outflow 2 4 2 1 chemical partitioning process in water column three phase partitioning dissolved in water sorbed to doc and sorbed to solids is simulated in water column linear equilibrium partitioning is assumed in the water column and the fractions of different chemical phases is calculated as 15 f d w a t e r c d w a t e r c t w a t e r 10 6 r w a t e r 16 f p w a t e r c p w a t e r c t w a t e r k p w a t e r s e d c o n r w a t e r 17 f d o c w a t e r c d o c w a t e r c t w a t e r k d o c w a t e r d o c c o n w a t e r r w a t e r 18 r w a t e r 10 6 k d o c w a t e r d o c c o n w a t e r k p w a t e r s e d c o n where f d water is the fraction of dissolved phase in water column c t water is total concentration of organic chemical in water column ug l 1 f p water is the fraction of solid sorbed phase in water column c p water is the concentration of particular organic chemical in water column ug l 1 k p water is the partition coefficient for suspend solids in water column l kg 1 sed con is the suspend sediment concentration in water column simulated by swat mg l 1 f doc water is the fraction of doc sorbed phase in water column c doc water is the concentration of doc sorbed organic chemical in water column ug l 1 k doc water is the doc partition coefficient in water column l kg 1 and doc con water is doc concentration in water column mg l 1 simulated by the doc simulation module k p water and k doc water are two parameters for organic chemical partitioning processes in the water column 2 4 2 2 organic chemical decay and transport processes in water column the reaction processes in water column include degradation photolysis hydrolysis volatilization and extra reaction in this study these reactions are lumped together using a first order decay equation to calculate the total amount of chemical reactions in the water column chemical reaction amount in water column is computed as 19 m w t r d e c t k d e c w t r t f d w a t e r f d o c w a t e r f p w a t e r m w a t e r where m wtr dec t is chemical decay amount in the water column k dec wtr t is decay rate for organic chemical in water column d 1 and m water is total chemical amount in the water column ug the decay rate in water column is related to the half life as follows 20 k d e c w t r 0 693 t 1 2 w where t 1 2 w is organic chemical half life in the water column d 1 which is a model parameter for the simulation module the mass balance for organic chemical in water column is calculated as 21 δ m w a t e r min m o u t m w t r d e c m w t r s t l m s e d r s p m d i f where δm water is the change in chemical mass in the water column mg m in is the chemical amount added to the water column via inflow mg m out is the chemical amount removed via outflow mg m wtr dec is the amount of organic chemical removed from the water via decay mg m wtr stl is the amount of organic chemical removed from the water due to settling mg m sed rsp is the amount of organic chemical added to the water from resuspension from the sediment layer and m dif is the amount of organic chemical transferred between the water and sediment layer by diffusion the particular phase organic chemical might be removed from the water column by settling which transfers organic chemicals from the water to the sediment layer the amount of chemical that is removed from the water via the settling is calculated as 22 m w t r s t l v s t l d e p t h f p w a t e r m w a t e r t t where m wtr stl is the settling amount of the particular organic chemical mg v stl is the settling velocity m day which is a model parameter depth is the water depth simulated by swat routing routine m f p water is the fraction of organic chemical in the particular phase calculated by the partition process m water is the amount of organic chemical in the water and tt is the flow travel time days 2 4 2 3 organic chemical processes in the sediment layer in order to simulate the interaction resuspension and diffusion between water column and the sediment layer the organic chemical processes in the sediment layer underlying the water column are simulated the chemical amount in the sediment is increased through addition of mass by settling and diffusion from the water the chemical amount in the sediment is reduced by resuspension decay and diffusion into the overlying water column and the burial the mass balance for organic chemical in the sediment layer is calculated as 23 δ m s e d m w t r s t l m s e d d e c m s e d r s p m s e d b u r m d i f where δm sed is the change in organic chemical mass in the sediment layer mg m wtr stl is the amount of organic chemical added to the sediment layer from the setting of particular chemical in the water column m sed dec is the sediment decay amount mg m sed rsp is the amount of organic chemical removed from sediment layer via resuspension mg m sed bur is the amount of organic chemical removed via burial and m dif is the amount of organic chemical transferred between the water and sediment layer by diffusion the amount of organic chemical that is removed from sediment layer via decay process is calculated as 24 m s e d d e c k d e c s e d m s e d where m sed dec is the amount of organic chemical removed from sediment layer by the decay process mg k dec sed is the rate constant for chemical decay in the sediment mg and m sed is the amount of organic chemical in the sediment layer mg the decay rate constant is related to the sediment half life 25 k d e c s e d 0 693 t 1 2 s e d where t 1 2 sed is the sediment half life for organic chemical which is a model parameter subject to model calibration in addition the equations for calculating resupension sediment burial and diffusion between the water and sediment layer can be found from equation s12 to s15 in the supplementary material 2 5 swat model setup spatial and meteorological data are needed for setting up the swat model in the mrb the required spatial datasets are a digital elevation model dem landuse and soil data maps the dem data was obtained from geospatial data extraction of natural resources canada with the resolution of 0 75 arc second 18 7m 18 7m the 30m 30m landuse map from aafc http www agr gc ca atlas landu and oil sands land map http www rampalberta org data map mapdata aspx from regional aquatics monitoring program ramp were used to create landuse map of mrw the 1 1 million scale soil map from the agriculture and agri food canada aafc were used as model input to build soil database the dem was used to delineate subbasin and stream networks and 30 subbasins were delineated in the mrw a slope map is derived from the dem and divided into 3 groups breaks at 1 2 and 3 9 to define the hru 10 different landuse classes and 6 different soil types were defined for setting up the model in addition a threshold of 5 5 and 5 for land use soil and slope respectively were used to definite hrus and a total of 362 hrus were obtained as to meteorological input data daily precipitation maximum and minimum air temperature were obtained from 2 stations recorded by environmental canada and climate change solar radiation relative humidity and wind speed data at 5 stations recorded by climate forecast system reanalysis dile and srinivasan 2014 were also used as model input in addition we also defined 5 elevation bands in each subbasin to enable the model simulate snow melt processes in a distributed way and adjust the precipitation and temperature according to the elevation differences with weather gauge station 6 2 6 swat modification for oil sands land the oil sands mines and tailing ponds constructed in the mrw are now dramatic features of the watershed landscape the changes to the landscape by oil sands development indirectly affect the natural patterns of water and pollutants movement to reflect the oil sands impact on mrw oil sands lands are defined as the landuse type to setup swat model specifically the oil sands land map from ramp was overlapped with the aafc landuse map to define oil sands in the landuse classifications fig 1b shows the landuse distribution in the mrw and two types of oil sands landuses were defined according the different hydrological features if the oil sands areas have no natural exchange of water with the rest of the watershed e g tailings ponds they are designated as hydrologically closed circuited if the oil sands areas have natural exchange of water with the rest of the watershed they are designated as not hydrologically closed circuited in the mrw there are 73 7 km2 hydrologically closed circuited oil sands area and 146 7 km2 not hydrologically closed circuited which account for 4 9 and 9 8 percentage of the total watershed area respectively there are 101 hrus for oil sands area for swat setup in the mrw and 30 of them are hydrologically closed circuited since there is no hydrological interaction between hydrologically closed oil sands area and the rest of watershed the swat model was modified to reflect this unique feature specifily the water yield surface runoff lateral flow and groundwater flow sediment and organic chemical loads of hydrologically closed oil sands hru are not routed to the stream as such 3 swat original subroutines and 1 organic chemical subroutine were modified for hydrologically closed oil sands hrus 2 7 model calibration and validation since the pahs observed data are only available for a period of 2015 2017 we calibrated and validated the streamflow sediment and pahs simulations using the same time period on a daily time scale as such the year of 2016 was selected as the calibration period because the pahs measurements covered the whole year from january to december the years of 2015 and 2017 were selected as the validation period as there were only 10 measurements during the validation period 7 measurements in 2015 from may to december and 3 measurements in 2017 from january to march a three year warm up period 2012 2014 was used to attenuate the impact of initial conditions on the model simulations first daily streamflow data station number 07da008 at the outlet station collected from environmental canada and climate change was used for the streamflow calibration and validation the observed sediment and two pahs phenanthrene and pyrene concentrations at the outlet station and the sampling dates were showed in table s1 those concentrations were obtained from the ramp monitoring datasets and were measured once a month from june 2015 to march 2017 there were 20 measurements for sediment concentration with an average concentration of 3 6 mg l and there were 22 measurements for phenanthrene phe and pyrene pyr with the average concentrations of 1 6 and 0 8 ng l respectively the observed daily streamflow from 2015 to 2017 range from 0 15 to 14 4 m3 s and the streamflow of the 22 pahs sampling dates range from 0 18 to 14 4 m3 s which indicates that the measured pahs concentration could represent the pahs variability that may exist in the mrw from 2015 to 2017 we used the measured sediment and pahs concentrations and the daily observed streamflow of the same day to estimate the loads next the estimated periodic loads were used for sediment and pahs calibration and validation the model calibration was then conducted by combining the automatic calibration using swat cup abbaspour 2013 and manual calibration in the following first the parameter sensitivity analysis results were obtained by the global sensitivity analysis algorithm of swat cup for automatic calibration sufi 2 algorithm from swat cup was used based on 2000 simulations with the defined parameter rages then we manually adjusted the one or two most sensitive parameters based on the best parameter set from the automatic calibration to get better model performance statistics we found that manually adjusting the most sensitive parameters after the automatic calibration was very useful to improve the model performances since the new parameters added for organic chemical simulation module are not in the swat cup parameter list we borrowed swat pesticide parameters to perform pahs parameters calibrations in swat cup specifically we wrote the scripts pah parameters equal to the pesticide parameters in the code and used the columns of pesticides in output rch file to output pahs simulation results then we compiled an exe file based on the organic chemical module code to replace the original swat exe file in swat cup directory the pesticide parameters and pesticide column number in output rch file were used to setup sufi 2 project for pahs calibrations in swat cup so when pesticide parameters are changed in swat cup the pahs parameter values and simulation results will change accordingly we admitted that this is not a direct way to facilitate the automatic calibration in swat cup but is able to indirectly perform the pahs automatic calibration and sensitivity analysis in this study the root mean square error rmse was used as the objective function and the model parameters were calibrated to minimize the value of rmse during the calibration period du et al 2014 in addition percentage biases pbias coefficient of determination r2 and nash sutcliffe efficiency nse were also used as the measures of model performance zhang et al 2013 the objective function rmse and the model performance measures are defined as 26 r m s e t 1 n y s i m t y o b s t 2 n 27 p b i a s t 1 n y s i m t y o b s t t 1 n y o b s t 100 28 r 2 t 1 n y s i m t y s i m y o b s t y o b s t 1 n y s i m t y s i m 2 t 1 n y s i m t y s i m 2 2 29 n s e 1 t 1 n y o b s t y s i m t 2 t 1 n y o b s t y o b s 2 where y t sim is the simulated value at a time unit t y t obs is the observed value at a time unit t n is the total data number y s i m is the mean observed value for the simulated period and y o b s is the mean simulated value of the simulated period pbias measures the average tendency of the simulated data to be larger or smaller than the observed data and is sensitive to the systematic error chen et al 2017 r2 ranges from 0 to 1 and represents the proportion of the total variance in the observed data that is explained by the model zhang et al 2013 nse ranges from negative infinity to 1 and assesses how well the model to data plot fits the 1 1 line in addition we also assessed the model simulations based on the performance evaluation criteria for watershed models summarized by moriasi et al 2015 which evaluates the model performance from unsatisfactory to very good based on the range of values of chosen goodness of fit statistics as to model uncertainties analysis the 95 prediction uncertainty band obtained from sufi 2 algorithm was used to quantify the simulation uncertainties the simulation uncertainties were assessed by the p factor and r factor chen et al 2017 the p factor is the percentage of the observed data bracketed by the uncertainty band and the r factor is the normalized width of the band 3 results and discussion 3 1 hydrological calibration and streamflow simulation to accurately simulate the transport of organic chemicals the hydrological processes simulated by the swat model should confirm to actual hydrological cycle of the watershed therefore it is imperative to perform the hydrologic calibration before calibrating the organic chemicals a total of 25 parameters were used for streamflow calibration and the parameter ranges and calibrated values of the top 10 most sensitive parameters were showed in table 1 the parameter cn2 scs curve number for moisture condition ii was found to be the most sensitive parameter which controls the surface runoff yield and the water amount infiltrating into the soil layers moreover the parameter surlag which controls the surface runoff routing in the subbasin scale was found to be the second most sensitive parameter which indicates that the surface runoff is the important hydrological process in the mrw two snowmelt parameters sftmp and smtmp were also among the top 10 sensitive parameters list further validating the need to accurately represent snow related process in such a cold climate region watershed this is indeed expected in such a cold region watershed because snowmelt is the dominant hydrological process in the spring season other sensitive parameters include the parameters affecting the processes of channel routing ch k2 groundwater flow gw delay and revapmn and evaporation esco the model performance statistics for daily streamflow are shown in table 2 the comparison between simulated and observed daily steamflow was shown in fig 5 a the visual plot indicated that the swat hydrology model could provide acceptable simulation results as the simulated streamflow tend to follow a similar trend to the observed data and are capable to fairly capture both the diurnal as well as the seasonal variations as per moriasi et al 2015 evaluation criteria the quality of daily stream flow is of satisfactory quality based on r2 value r2 0 5 based on the nse values the daily flow simulations of the calibration and whole period were evaluated as satisfactory but unsatisfactory for the validation period however the nse value 0 41 of the validation period is much greater than the mean nse value 0 13 of daily streamflow simulation applications summarized by moriasi et al 2015 in addition the performance rating during the calibration period is of very good quality based on the pbias value 1 1 however a high pbias value 54 2 during the validation period points to the unsatisfactory quality of the model simulation it is indeed true that there could be conflicting performance ratings during calibration and validation periods as pointed out by chen et al 2017 when using swat model as different goodness of fit statistics tend to capture distinct aspects of model performances furthermore such overestimation of daily streamflow during the validation period was mainly caused by the overestimation of low base flow season in the year of 2017 however the low flow of the years 2015 and 2016 were simulated quite well fig 5a as the groundwater flow is the dominant runoff component in such low flow seasons we aimed to adjust the groundwater parameters heuristically to match the streamflow hydrograph in the low flow season of the year 2017 and such low flow can be only matched when the groundwater flow was adjusted as zero this indicates possible unreliable observations and points to the need of using more streamflow data for hydrological calibration however in order to keep the calibration and validation period of streamflow same as to that of pah we used the streamflow data from for the years 2015 2017 the statistics for streamflow simulation uncertainties were showed in table 3 on average 45 of the variation in streamflow was bracketed by the 95 uncertainty band and r factor 0 75 was less than 1 suggesting acceptable thickness of the band 3 2 sediment load simulation after the streamflow calibration and validation the sediment load was calibrated and validated since sediment directly affects the transport amount of particular organic chemicals 9 sediment related parameters were chosen for model calibration the sensitivity ranking and calibrated values of these parameters were shown in table 4 the parameter prf bsn is found to be the most sensitive sediment parameter which determines the maximum amount of sediment that can be transported through a reach the second and third most sediment parameters were other two parameters used for calculating the transported maximum sediment load through a reach overall the reach sediment routing parameters ranking from first to fifth were found to be more sensitive than the soil erosion parameters ranking from sixth to ninth this indicates that the sediment routing process in the reach is more influential in the mrw than the upland soil erosion and transport processes various researchers e g shrestha and wang 2018 and zabaleta et al 2014 etc also reported similar findings the model performance statistics of sediment load was shown in table 2 and the comparison between the simulated and observed daily sediment load was shown in fig 5b the simulation performance of sediment load for the whole simulation period 2015 2017 was found to be of good quality while considering the r2 0 74 of very good while considering the pbias 8 5 value and of satisfactory while considering the nse 0 67 value the simulation performance for the calibration period was of good quality as per r2 and of satisfactory as per pbias and nse values however the simulation performance for the validation period was of unsatisfactory quality as per r2 pbias and nse values in which the sediment load was clearly overestimated this was partially caused by the overestimation of streamflow during the validation period furthermore limited number of data available to use for sediment calibration and validation only 20 measurements also played a significant role only 10 daily sediment loads were available during calibration period 2016 which ranged from 0 03 to 16 2 ton day similarly only10 daily sediment loads were available during validation period 2015 and 2017 which ranged from 0 03 to 1 3 ton day it is clear that the sediment loads used for calibration were much larger than those used for validation therefore the sediment loads during the validation period were overestimated since the parameters were tuned to capture those much larger sediment loads observed during the calibration period ideally the observation data used for model calibration should include the data observed during different streamflow conditions such as those during low flow normal flow and high flow condition such data would ensure a more robust model calibration and validation however in our case the observed sediment and pahs data in the mrw were rather limited because the ramp water quality monitoring program in athabasca oil sands region only started from june 2015 and the water quality measurements are only collected at the monthly frequency as to sediment load simulation uncertainties table 3 70 of the measured data were bracketed by the 95 uncertainty band during the simulation period 2015 2017 and r factor 0 76 was also less than 1 3 3 model simulation for two pahs in muskeg river watershed the periodic daily pahs loads at the watershed outlet were used for calibrating and validating the organic chemical simulation module 19 pahs related parameters with sensitive ranking their parameter ranges and optimized values were shown in table 5 of all the most sensitive parameter was found to be the deposition rate which is the pollution source parameter determining the amount of the pahs depositing to the top soil layers the second most sensitive parameter was found to be the percolation coefficient for the pahs transported by surface runoff which further highlights the importance of the surface runoff as the pathway of the pahs in the mrw besides these two most sensitive parameters 4 other parameters affecting the pahs interactions between water column and sediment layer were ranked from third to sixth which equally reflect the importance of interactions between water column and sediment layer for the pahs fate and transport in the mrw in addition the doc partition coefficient in water column was ranked as seventh most sensitive parameter which indicates the necessity to simulate the pah partitioning to the doc the model performance statistics of the pahs loads were shown in table 2 and the comparisons between the simulated and observed daily pahs loads were showed in fig 5c and d in this study as already depicted the performance evaluation criteria for nutrient simulation as summarized by moriasi et al 2015 were used to assess the pahs simulation performance the simulation performance of phe load for the whole simulation period 2015 2017 was found to be of very good quality as per r2 0 73 and nse 0 71 values and pbias 5 8 values the simulation performance of phe load for the calibration period was rated to be of very good quality as per r2 and nse values and of good quality as per pbias value the simulation performance of phe load for the validation period was rated to be of satisfactory quality as per r2 but of unsatisfactory as per pbias and nse values the simulation performance of pyr load for the whole simulation period 2015 2017 was found to be of very good quality as per r2 0 73 and of good quality as per nse 0 64 and pbias 16 8 value the simulation performance of pyr load during the calibration period was rated to be of very good as per r2 of very good as per nse value and of satisfactory quality as per pbias value similar to phe load simulation performance for the validation period the pyr load simulation was also of satisfactory quality as per r2 but of unsatisfactory as per nse and pbias values the phe and pyr loads were overestimated during the validation period and such overestimation was partially caused by the overestimations of both streamflow and sediment load as pointed out in the previous sections as identified for the sediment load the overestimation was also caused by the limited data available to be used for pahs calibration and validation since the loads used for the calibration were much larger than those used during the validation period in addition the peak values of the pah loads on june 22 2016 were severely underestimated fig 5 and this caused underestimations of streamflow and sediment on this very day as a continuous model with daily step the swat model would underestimate streamflow sediment and chemical loads for the days with short intense storm events our results are consistent with other studies conduction in the mrw for instance eum et al 2016 used the vic variable infiltration capacity model to simulate streamflow in the mrw and results showed similar underestimation of the peak flows during high flow seasons in addition we also check the impact of peak values in 2016 on the pahs model performances during calibration period in 2016 specifically we compared the model performances statistics of the observed data excluding peak values with original performances statistics with the peak values the results showed that the model performances statistics deteriorated a lot when the peak values were excluded for calculating model performances statistics for example the nse values decreased to 0 30 and 0 40 for phe and pyr simulation and r2 decreased to 0 37 and 0 41 this is because the best parameter set was calibrated by including the peak values and the parameter values were obtained towards matching the peak values even though the model underestimated the peaks the peak values were still useful to constraint the model and parameter values for uncertainty analysis 77 of the observed phe and pyr loads were bracketed by the 95 uncertainties bands table 3 and the r factors were both less than 1 suggesting acceptable thickness of the bands in addition to daily pahs loads we also simulated monthly and annual pahs loads to further analyze the characteristics of the pahs fate and transport in the mrw the annual phe loads for the years 2015 2017 were found to be 139 9 194 6 and 202 7 g year respectively the annual pyr loads for the years 2015 2017 were found to be 70 7 98 5 and 103 5 g year respectively the pahs loads in the years 2016 and 2017 were much larger than that of 2015 because these two year were relatively wet years compared to 2015 for the monthly pahs loads the maximum pah loads would be delivered in june accounting almost 40 and 20 g phe and pyr in addition the summer season june to september accounted for 65 2 and 66 2 of the annual phe and pyr loads this indicates that the summer season is the hot moment for pahs transport in the mrw 4 conclusion an organic chemical simulation module was developed and integrated into one of the widely used watershed model the soil and water assessment tool swat to simulate the fate and transport processes of the organic chemicals at watershed scale the chemical module considers three phase portioning process by simulating chemicals portioning to dissolved organic carbon doc in soil water and streams we demonstrated the module s capability by simulating the dynamics of the two polycyclic aromatic hydrocarbons pahs in a cold climate region watershed muskeg river watershed in the athabasca oil sands region of western canada where the organic chemicals pose a threat to the water quality status of receiving waters the hydrological and sediment dynamics were first calibrated and validated before pahs calibrations using daily streamflow and periodic daily sediment loads for a period of 2015 2017 at the watershed outlet the simulation performances for the pahs loads for the whole simulation period 2015 2017 were assessed as satisfactory however the simulation performances in the validation period showed conflicting results the model simulation results indicate that the summer season is the hot moment for the pahs transport mainly driving by the intense rainfall runoff events in this season the developed simulation module could be a useful modeling tool to simulate the fate and transport of organic chemicals at a watershed scale however more model applications are required to further verify the model capability in other climatic regions of the world currently the doc simulations in soil water and streams are the simplifications of the complex carbon cycle processes in the watershed therefore a process based watershed scale doc model coupling with swat organic chemical simulation module holds the potential to further improve the simulations of chemical partitioning processes and fate and transport in the watershed acknowledgements the authors would like to thank the alberta economic development and trade for the campus alberta innovates program research chair no rcp 12 001 bcaip appendix a supplementary data the following is the supplementary data to this article sm revised2 v2 sm revised2 v2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 011 
26304,toxicant concentrations in surface waters are of environmental concern due to their potential impacts to humans and wildlife numerical models provide system insight support management decisions and provide scenario testing on the impacts of toxicants the water quality analysis simulation program wasp is a widely used framework for developing site specific models for simulating toxicant concentrations in surface waters and sediments over a range of complexities and temporal and spatial scales wasp8 with the advanced toxicant module has been recently released incorporating a complete architecture redesign for an increased number of state variables and different state variable types wasp8 incorporates a new structure for simulating light intensity and photoreactions in the water column including the distinction of 10 different wavelength bands and nanoparticle heteroaggregation to solids we present a hypothetical case study using the cape fear river north carolina as a representative example for simulating solute chemicals nanoparticles and solids to demonstrate the new and updated capabilities of wasp8 keywords surface waters numerical modeling nanoparticles toxicants sediments software availability program title water quality analysis simulation program version 8 developers tim wool robert b ambrose jr address united states environmental protection agency water quality planning branch data and information analysis section region 4 atlanta ga 30303 us tel 404 562 9260 e mail wool tim epa gov first available july 7 2017 hardware 64 bit windows 64 bit mac osc yosemite or higher 64 bit linux built on ubuntu software wasp 8 2 executable https www epa gov ceam water quality analysis simulation program wasp http epawasp twool com language fortran 95 c size executable is 3 mb files require 205 mb 1 introduction exposure to toxic contaminants in surface waters may pose a threat to the health of humans and wildlife in the united states us the us environmental protection agency epa has passed a series of acts to address managing toxic contaminants in rivers and streams such as the clean water act cwa the toxic substances control act tsca the federal insecticide fungicide and rodenticide act fifra and the comprehensive environmental response compensation and liability act cercla superfund the implementation of environmental models is critical to the regulatory decision making process spatial and temporal scales linking environmental controls and environmental quality generally do not allow an observational approach to understand the relationship between economic activity and environmental quality nrc 2007 environmental models are used in the regulatory process such as assisting in the development of total maximum daily loads under cwa evaluating different feasible remediation strategies cercla or evaluating the possible impacts of the release of newly developed chemicals review under tsca the water quality analysis simulation program wasp is a differential spatially resolved mass balance fate and transport modeling framework structured to allow users to simulate concentrations of environmental contaminants in surface waters and sediments wasp was originally developed in the 1980s and has undergone a series of improvements and versions over the decades it is now widely used throughout the us and the world wasp has been applied to a range of different surface water systems addressing a range of environmental contaminants wasp has been previously implemented for nutrient loading on water quality in tampa bay florida us wang et al 1999 mercury fate and transport in the carson river nevada us carroll et al 2000 nutrients tmdl for neuse river estuary north carolina us wool et al 2003 mercury remediation strategies in the sudbury river massachusetts us knightes 2010 impacts of climate change on water quality of chungju lake south korea park et al 2013 the fate and transport of multi walled carbon nanotubes in brier creek georgia us bouchard et al 2017 and as part of an uncertainty and risk analysis for a margin of safety for a nutrient tmdl for sawgrass lake florida us camacho et al 2018 most recently wasp8 v 8 2 was released which incorporates an overhaul of the wasp architecture and a restructuring and upgrading of the toxicant module which is the focus of this paper wasp8 updated the underlying code from fortran 77 to fortran 95 and incorporated a new wasp interface linkage to a post processor water resources database wrdb v 6 1 www wrdb com and the ability to run on windows mac os or linux wasp8 contains two modules advanced eutrophication and advanced toxicant over the past several years wasp development has focused on the eutrophication module martin et al 2006 wool et al 2013 and much modeling research has focused on eutrophication gargallo et al 2018 nguyen et al 2018 sadeghian et al 2018 less work over the years has focused on the toxicant module routines with limited development on the toxicant module since the 1990s with the updated architecture of wasp8 recent work has focused on upgrading the toxicant module routines one driver of this effort was interest in emerging contaminants and their potential exposure concentrations particularly nanomaterials once released into the environment previous versions of wasp were not structured in a way to handle the introduction of these new contaminants consequently the wasp8 advanced toxicant module completely restructured its state variables as distinct arrays to permit for simulation of any number of chemical solutes solid particles and nanoparticles this is a departure from the original architecture additional functionality has also been added to incorporate nanoparticle specific processes specifically particle attachment kinetics in this paper we present the newly developed publicly available wasp8 advanced toxicant module https www epa gov ceam water quality analysis simulation program wasp epawasp twool com the structure of the underlying software and modeling framework and how it fits within an overall multi media system of models is discussed the general structure of the wasp8 advanced toxicant module with details on the changes from the previous wasp toxicant modules particularly regarding the change in the structure of simulating state variables and the available state variables is described and specific state variables and processes are explained the three main state variables are solid particles solute chemicals and nanoparticles first the solids module is addressed as solids are an important factor for sufficiently simulating toxicants particularly regarding contaminants attaching to particles and being transported by them next partitioning and particle attachment are discussed and the different processes of equilibrium partitioning kinetic partitioning and heteroaggregation light attenuation and phototransformations are then presented followed by a focus on the processes governing solute chemicals and then nanoparticles each of these sections discusses the options for different processes available to the user from descriptive to mechanistic discussion of the different processes concludes with a hypothetical application of wasp8 using the cape fear river as a case study the goal of this application is to demonstrate how the updated wasp can be applied for the different state variables and their governing processes the application is structured to simulate flow and four different solids using both the descriptive and mechanistic solids algorithms to this system there is a hypothetical release of a solute chemical which transforms into another solute chemical and a nanoparticle which attaches to the different solid phases the system uses time varying flow conditions simulated using kinematic wave routing light is simulated temporally over the day and year with attenuation and phototransformation of the nanoparticle 2 modeling framework wasp is a surface water modeling framework designed to create site specific models for simulating environmental concentrations in the water column and sediment layers of different aquatic ecosystems over space and time the wasp framework is used to develop a water quality model for a specific system of interest and contaminants of concern and is structured using a series of different module components fig 1 shows the overall wasp modeling framework generally the user interacts with wasp through the interface to create the wasp input file wif wasp reads the wif to run a simulation wasp has a series of hydrodynamic modules incorporated into its architecture e g flow routing kinematic wave diffusive wave and dispersive wave ambrose and wool 2017 and can import data files from other hydrodynamic models e g efdc via a hydrodynamic file hyd file the science modules are shared libraries for kinetic parameters and transformation processes for the specific state variables stored data files include data base time series information for time functions e g solar radiation air temperature boundary concentrations inflows and loads once the wasp simulation is performed an out file a text file of parameters and outputs and a bmd binary data file are produced the bmd is readable by wrdb which is a tool that works with wasp to manage data and provide graphical outputs wrdb can also be used to generate figures or files for other programs to read e g csv for further data analysis wasp can also be used in the greater context of multi media modeling taking output from watershed models e g swat nietsch et al 2011 hspf bicknell et al 2001 and feeding forward as input into bioaccumulation models e g bass barber 2006 gobas model arnot and gobas 2004 as was done to link atmospheric deposition to fish tissue concentrations knightes et al 2009 and can be a part of a multi media modeling framework like the framework for risk analysis in multimedia systems frames johnston et al 2011 whelan et al 2014 wasp can be structured with different spatial resolutions as required by the user to simulate zero dimensional lakes and ponds one dimensional streams with or without branching two dimensional rivers and stratified lakes and three dimensional estuaries or large lakes the wasp model domain is constructed with wasp segments surface water or subsurface water water segments and surface benthic or subsurface benthic sediment segments segments are assumed to be uniformly mixed whether they are a water column segment or a sediment segment within each segment each state variable is simulated for every time step the user defines how the segments are laid out in space through the interface this allows for three dimensional model domains with multiple sediment layers as needed wasp incorporates simple hydrodynamic modules ambrose and wool 2017 which include general constant flow routing stream routing using hydrogeometry kinematic wave routing using slope and roughness and dynamic wave for tidal influences based on dynhyd5 ambrose et al 1993 these modules are best suited for one dimensional systems including branching for larger lakes and estuaries a hydrodynamic model can be used to provide details on flow which can be entered manually using flow routing or via hydrodynamic model linkage efdc can be used to simulate the hydrodynamics of the system which is then linked to the wasp model for simulating contaminant fate and transport which has been demonstrated successfully for a nutrient tmdl in the neuse river nc usa wool et al 2003 for algal blooms caused by increased residence time by construction of instream structures in the nakdong river korea seo et al 2012 and simulating water quality in yongdam lake korea seo et al 2009 wasp currently contains two kinetics modules advanced eutrophication and advanced toxicants example state variables available in both modules are presented in table 1 the advanced eutrophication module has explicitly incorporated state variables e g nitrate ammonia orthophosphate martin et al 2006 wool et al 2013 and has been reviewed in the context of other eutrophication models sharma and kansal 2013 the advanced toxicant module is structured differently than the advanced eutrophication module by having state variable classes e g solute chemicals nanoparticles solids this provides the user with the flexibility of constructing a wasp model designed for a range of different contaminants of interest by defining and parameterizing governing processes the user effectively defines that state variable e g a solute chemical is defined as atrazine by its parameterization each state variable and process has different options for how it is described governing equations for concentrations follow the advection dispersion reaction equation e g gem little et al 2018 ce qual w2 sadeghian et al 2018 ce qual icm cerco et al 2010 the science module and the transport module are solved using differential mass balance equations for each segment for each time step the user may select different solution techniques but generally the forward euler numerical approximation approach is used with varying time steps designed to minimize numerical instability 3 wasp8 advanced toxicant module the advanced toxicant module has been updated as part of wasp8 to simulate an increased number of specific state variables table 2 presents some of the changes from wasp7 to wasp8 with respect to the advanced toxicant module originally the wasp toxicant module had a single class of state variables and wasp kept track of whether it was a solid type or a dissolved chemical type this legacy architecture made it challenging to incorporate increased numbers of state variables as well as different types of state variables using a new architecture state variables have been separated into their own classes with up to 5 or 10 state variables per state variable class these numbers can be increased by recompiling after adjusting a compile parameter now that wasp8 has separate state variables the structure of the advanced toxicant module has been redesigned and additional processes and options have been added the new design also facilitates updates in the science and algorithms moving forward with wasp future development table 3 presents an overview of the processes for different state variables in wasp8 as part of the development of the wasp8 advanced toxicant module model verification was performed the solids light particle attachment and nanomaterial and chemical solute reaction algorithms were isolated and simple simulations were performed and compared to analytical solutions to verify that the algorithms and modules were performing properly ambrose et al 2017 4 solids module solids are an important environmental component of water quality modeling suspended solid concentrations can be of concern on their own by reducing light penetration or blanketing benthic spawning areas or because environmental contaminants attach to the particles and their transport becomes linked to the movement of the particles in previous wasp releases up to three solid particles could be simulated e g sand fines clay silt and particulate organic matter in wasp8 there can be up to 10 solids types as defined by the user such that clays and silts can now be separate state variables this allows for increased refinement in solids modeling with different settling and resuspension rates as well as different partition coefficients increased numbers of particle size classes have been shown to affect the distribution and accumulation of simulated nanomaterials in surface waters quik et al 2015 each solid type state variable is defined by the way the user constructs the model and parameterizes the solids state variable solids in the advanced toxicant module can either be organic materials e g plankton algae detritus or inorganic e g sand silt clay the solids state variable captures the processes for all these solids types in a general framework the user parameterizes each solid type to reflect its characteristics and properties fig 2 there are four options for simulating solids in wasp solid flow fields descriptive mechanistic van rijn equations van rijn 1984 or mechanistic roberts equations roberts et al 1998 for all options all solids move via advection dispersion may also move solids if the exchange option is turned on and dispersion rates are defined between surface water segments particle density is set for each particle type 4 1 solid flow fields the solid flow fields option is the legacy option used in wasp7 and earlier versions this option treats each solid state variable type separately movement of solids from one segment to another is explicitly defined by using a flow field of a specific flow velocity and the cross sectional area of the interface flow fields are generic and the user establishes whether it is resuspension or settling via the direction of the flow field as each solid flow field is linked to a solids type and is from the legacy structure the flow fields are limited to three 4 2 descriptive solids transport the descriptive solids transport fig 2a option uses constant settling and resuspension velocities which do not change with stream velocity for this option the settling velocity v s m s 1 and resuspension velocity v r m s 1 are entered for each solid type for each segment as part of the model domain set up in the segment definition the user enters which segment is below a given segment wasp then knows for a given segment where both settling and resuspension is going to and coming from therefore if a model has sand silt and clay the settling and resuspension velocities for descriptive transport are determined by the user and entered directly into the interface 4 3 mechanistic solids transport the mechanistic solids transport fig 2b options simulate dynamic settling deposition erosion and resuspension velocities based on a set of parameters and the simulated flow velocity of the surface water settling is the process of solids moving downward through the water column and is a function of particle size and density deposition erosion bedload and resuspension are functions of the shear stress on the stream bed deposition is when a solid particle moves from the water column to the surface sediment erosion is when solids move from the surface sediment layer into the overlying water boundary layer resuspension is when the solid particle leaves the boundary layer and enters the water column erosion and resuspension additionally depend on whether the sediment layer is acting cohesively or non cohesively for a non cohesive sediment layer all particles act independently when a sediment layer is cohesive all particles 0 10 mm diameter fines erode as a single unit the user defines the critical cohesive sediment fraction for which the sediments act cohesively the default is 0 2 so when fines comprise 20 or more of the bed the bed acts cohesively the inclusion of the mechanistic solids transport allows for erosional events during high flows and for depositional events and zones for slower flows 4 3 1 settling for the mechanistic option wasp8 calculates the settling velocity for different ranges of particle diameter d m as a function of particle density ρ s kg m 3 water density ρ w kg m 3 and absolute viscosity μ kg m 1 s 1 van rijn 1984 1 v s r d 18 g d d 100 μ m 10 r d 1 0 01 r d 2 1 g d 100 μ m d 1 m m 1 1 g d d 1 m m where r d is the sediment particle densitometric reynolds number r d d g d μ ρ w unitless and g g ρ s ρ w 1 m s 2 where g is the acceleration of gravity 9 807 m s 2 4 3 2 deposition for the mechanistic option wasp8 calculates the deposition velocity v d m s 1 as v d v s α d where α d unitless is the probability that a solid will deposit and is a function of bottom shear stress as well as the lower and upper critical shear stress thresholds τ c d 1 n m 2 and τ c d 2 n m 2 krone 1963 where 2 α d 1 τ b τ c d 1 τ c d 2 τ b τ c d 2 τ c d 1 γ d τ c d 1 τ b τ c d 2 0 τ b τ c d 2 and γ d is a dimensionless exponent default 1 this relationship captures the fact that all settling particles deposit onto the sediment layer when shear stress is low as shear stress increases fewer particles deposit until flow conditions reach a regime where the shear stress is so high that no particles deposit this allows the system to shift between deposition or no deposition 4 3 3 non cohesive erosion the model domain does not differentiate the boundary layer fig 2b as a separate wasp8 segment from the overlying water column the boundary layer is used conceptually to separate the processes of erosion and resuspension in wasp8 when the sediment is acting non cohesively fine fraction is less than critical cohesive fraction all particles act non cohesively in cohesive sediment segments only sands and larger particles 0 1 mm diameter are subject to non cohesive erosion and resuspension wasp8 calculates the non cohesive erosion velocity for each solids type using equations either developed by van rijn or roberts depending on the solids option the van rijn equation calculates the gross deposition rate v s m s 1 where v e e v s using a proportionality constant e where 3 e 0 015 γ e d k s r d 0 2 τ η and γ e is a user specified multiplier default 1 η is a user specified exponent default 1 5 and τ is the non dimensional shear stress given by 4 τ τ b τ c e τ c e τ b τ c e 0 τ b τ c e where τ c e is the critical shear stress for erosion n m 2 see appendix eqn a 3 for the roberts equation 5 v e γ e a ρ b m τ b n where ρ b is the bulk density kg m 3 and coefficient a and exponents m and n are determined experimentally for different particle sizes from fine silt 5 7 nm to coarse sand 1 25 mm roberts et al 1998 4 3 4 non cohesive resuspension particles that erode from the sediment layer enter the boundary layer at that point particles can resuspend and enter the overlying water segment or travel downstream via bedload eroded particles move in the boundary layer via bedload when the shear stress is less than the critical shear stress for resuspension τ c r n m 2 see appendix when τ b τ c r particles resuspend with a resuspension velocity v r f r v e m s 1 where 6 f r 0 τ b τ c r l n u v s l n u c r v s l n 4 l n u c r v s τ b τ c r 1 u 4 v s and u τ b ρ w is the shear velocity m s 1 and u c r τ c r ρ w is the critical shear velocity for resuspension m s jones and lick 2001 hydroqual 2007 4 3 5 non cohesive bedload bedload is the transport of non cohesive solid particles downstream through the boundary mobile layer when τ b τ c e bedload may begin the bedload flux per unit width g b l g m 1 s 1 is given as 7 g b l α b l ρ s uh d 50 h 1 2 u u c e ρ s ρ w ρ w g d 50 η where d 50 is the mean particle diameter m α b l is a fitted coefficient u is stream velocity m s 1 h is water depth m η is a fitted exponent and u c e is the critical stream velocity for erosion see appendix suggested values for the fitted parameters are α b l 15 and η 1 5 van rijn 2007 4 3 6 cohesive resuspension when the surface sediment layer is in the cohesive regime all cohesive solids erode at the erosion velocity v e m s 1 the cohesive erosion flux e coh g m 2 s 1 is given by the excess shear stress power law formulation lick et al 1994 e c o h f c o h m τ n where m is the shear stress multiplier g m 2 s 1 n is the shear stress exponent f coh is the fraction of the surface bed that is cohesive and τ is the excess shear stress n s 1 defined in section 4 3 3 the shear stress multiplier varies between 0 1 and 100 g m 2 s 1 with a default value of 5 the shear stress exponent varies between 1 6 and 4 with a default value of 3 the critical shear stress for erosion varies between 0 5 and 8 n m 2 with a default value of 2 wasp8 is structured so that the shear stress multiplier exponent and the critical shear stress for erosion can vary spatially if desired 4 3 7 organic solids production and dissolution additional processes are available if the user wants to classify a solid particle as an organic solid such as algae or particulate organic matter pom for any given solid the net production rate r p g m 3 d 3 is given as 8 r p r p s r p t θ p t 20 where r p s g m 3 d 1 is the spatially variable production rate by wasp segment r p t g m 3 d 1 is the time variable production rate defined by providing a wasp time function and θ p is the temperature correction coefficient organic matter may also decay and leave behind ash and dissolved organic matter using a similar functionality the loss rate of an organic solid particle via dissolution is given by the dissolution rate constant k d d 1 9 k d k d s k d t θ d t 20 where k d s d 1 is the spatially variable dissolution rate by wasp segment k d t d 1 is the time variable dissolution rate defined by providing a wasp time function and θ d is the temperature correction coefficient when an organic solid type dissolves a fraction of the particle leaves behind residue described as the ash dry weight residue the mass associated with the fraction of residue then is added to a different solid type as identified by the user similarly the organic fraction may dissolve into one of the doc state variables for example as pom dissolves a fraction may form silt and another fraction may form doc the advanced toxicant module has these simple equations for organic solids if a more rigorous representation of organic matter is desired then the advanced eutrophication module can be run to simulate nutrients and algae growth of 3 types of phytoplankton the output can then be pulled directly into the advanced toxicant as an input file wool et al 2013 4 3 8 solids burial and erosion wasp segments are classified as either water or sediments beneath the water column the user can structure one or multiple sediment segments for each sediment layer segment the initial solids concentration in each segment establishes the reference bulk density g m 3 and porosity m3 void space m 3 as the simulation progresses solid particles are depositing eroding growing and dissolving for each sediment segment wasp8 conducts a solids mass balance with two approaches for representing the sediment bed volume static constant volume and dynamic constant bulk density the dynamic runs using the benthic time step δt b d operates distinctly from the wasp time step for the surface sediment layer solids may be deposited so that total mass in the segment increasing or eroded resulting in mass increasing for the static option the volume is held constant so the density of the segment changes to account for the change in mass and solids aren t moved between segment layers for the dynamic option the volume changes and the density is held constant if there is only one layer one segment in the sediment then deposition can cause the layer to increase in depth and volume indefinitely and there will be no burial no sediments are pushed out the bottom of the segment conversely erosion can result in loss of volume under the dynamic option however erosion is halted after segment volume reaches 5 of the initial value when there are multiple sediment layers in the dynamic option the surface sediment segment volume and depth are reset to initial values at each benthic time step wasp8 calculates the segment volume that needs to move v b m3 and its associated depth d b m to return to the initial volume and depth the mass of contaminants and solids move with v b m d 1 the burial rate burial fluxes f b i g m 2 s 1 of solid or pollutant i are given by f b i c i v b where c i g m 3 is the concentration of the constituent of interest in the sediment layer negative burial rates reflect erosion under erosion conditions the burial rate is reversed and wasp8 recruits the necessary volume v b from the sediment layer below the mass moved then reflects the concentration of the sediment layer below the segment of interest in the subsurface sediment layers at each benthic time step mass is either moved upward or downward under deposition conditions the subsurface sediment segment receives solids from the overlying sediment segment at each time step if the subsurface sediment segment has a higher bulk density v b and d b are compressed and pore water is pushed upward the compressed volume is then passed downward to the next sediment layer or out of the system solids and pollutants in this volume are similarly passed downward maintaining the initial bulk density or porosity under erosion conditions the sediment segment receives the erosion volume from the underlying sediment layer along with the associated solids and pollutant masses this process is continued downward until all underlying sediment segments have been adjusted 5 toxicant partitioning and attachment to particles one of the most important processes governing the fate and transport of toxicant chemicals is their interaction with particles wasp8 currently incorporates equilibrium partitioning between solute chemicals and solid particles kinetic sorption and heteroaggregation kinetics of nanoparticles and solid particles previously wasp only allowed for equilibrium partitioning this section presents the modeling structures available for simulating the interactions of both solute chemicals and nanoparticles with particles 5 1 equilibrium partitioning for organic chemicals partitioning onto sediment organic matter partitioning kinetics are usually faster than the time steps simulated in wasp under these conditions or as a simplifying assumption instantaneous equilibrium partitioning is often used in water quality modeling of toxicant contaminants for environmental concentrations 10 5 m or one half of water solubility equilibrium sorption is linear with dissolved concentration karickhoff 1984 as c s i k d i c w i for a pollutant i and a particular solid s c s i mg l 1 is the chemical concentration on the solid phase c w i mg l 1 is the concentration in the water phase and k d i l kg 1 is the partition coefficient when there are multiple solid phases present this relationship holds for all constituents and the overall fraction of each chemical on each phase is given by solving all equations simultaneously so that 10 f i j k d i j s j 1 i 1 n k d i j s j where f i j is the fraction of a solute chemical i on the given solid phase j e g silt sand clay n is the number of solid phases present j and s j mg l 1 is the specific solid concentration complexation with doc is simulated using the same functionality 5 2 kinetic sorption in some cases equilibrium partitioning is not a good representation of the system for example solute chemicals may exhibit sorption and desorption hysteresis where kinetic sorption may be a more appropriate representation because the system is dynamic wasp8 uses separate state variables to represent the freely dissolved chemical c w i mg l 1 and the sorbed chemical c s i mg l 1 this model structure requires n 1 state variables where n is the number of solids present in the system and s mg l 1 is the concentration of the specific solid type the sorption process is described as 11 d c w i dt k for c w i s k rev c w i where k f o r l mg 1 d 1 is the forward reaction rate constant sorption and k r e v d 1 is the reverse reaction rate constant desorption 5 3 nanoparticle heteroaggregation kinetics attachment of nanoparticles to solid particles is simulated using equations based on colloidal theory using heteroaggregation kinetics arvidsson et al 2011 praetorius et al 2012 the overall heteroaggregation rate is defined by 12 k het i α k coll i j n s j where α unitless varies from 0 to 1 is the attachment efficiency which is a user entered segment specific parameter k coll i j m3 d 1 is the rate of collision calculated internally by wasp8 between the nanoparticle i and a solid particle j and n s j m 3 is the number concentration of suspended particles an explicitly simulated state variable the collision rate is comprised of three components brownian motion fluid motion and differential settling 13 k coll 2 k b t w 3 μ w r np r s 2 r np r s 4 3 g r np r s 3 π r np r s 2 v np s v s s where k b 1 38 10 20 m2 g s 2 k 1 is the boltzmann constant and μ w g m 1 s 1 is the dynamic viscosity of water calculated as a function of temperature t w kelvin is the temperature of water g s 1 is the shear rate r np m is the radius of the nanoparticle r s m is the radius of the solid particle v n p s m s 1 is the settling velocity of the nanoparticle and v s s m s 1 is the settling velocity of the solid particle the k c o l l is calculated internally in wasp8 for each segment at each time step t can be directly simulated or entered for each segment or as a time function the user enters r np and r s the v n p s is user defined per segment the v s s is defined based on the solids option the user chooses section 4 the shear rate g is calculated as g u h wasp8 requires n 1 nanoparticle state variables for simulating heteroaggregation for each nanoparticle type where n is the number of solid particles simulated in the system 6 light module and phototransformations solar radiation is an important factor in simulating environmental constituents in surface waters solute chemicals schwarzenbach et al 1993 metals stumm and morgan 1981 morel and hering 1993 and nanoparticles hou et al 2015 may undergo photochemical reactions resulting in their degradation and or transformation pathogens and viruses may be deactivated by different wavelengths of light to effectively simulate phototransformations requires the light intensity that reaches the surface of the water body the amount of light that penetrates through the water column and the photoreaction itself the amount of light reaching the surface depends on cloud cover shading and the solar zenith angle solar radiation is comprised of different wavelengths each wavelength affects different environmental constituents in different ways attenuates differently as it passes through the water column and comprises a different fraction of solar radiation wasp8 accounts for ultraviolet 295 379 nm visible 380 749 nm and infrared 750 2500 nm ultraviolet and visible light are both divided into five wavelength bands see appendix this section provides details on the different options for incorporating input total radiation attenuation of solar radiation before reaching the water surface light attenuation in the water column and phototransformations 6 1 input total solar radiation wasp8 has 3 different options for quantifying solar radiation i input w m 2 h 1 reaching the water surface internally calculated diel light user input diel light or user input daily light with calculated diel light for the first option wasp8 will calculate i input based on the site latitude and longitude varying by date and time of day for the second option i input as one or more up to four time functions is entered so different locations can have different solar radiation at the surface as a function of time for the third option the sum of all radiation for a given day w m 2 h 1 is entered then wasp8 calculates i input over the course of the day and year 6 2 solar radiation to water surface as solar radiation passes through the atmosphere it attenuates due to different processes if wasp8 is calculating i input atmospheric attenuation is not automatically incorporated if solar radiation is entered then the included attenuation processes needs to be known so the others can be accounted for wasp8 incorporates four attenuation processes specifically cloud cover canopy shading water surface reflectance and attenuation in ice cloud cover and canopy shading are incorporated by using time functions and reflectance is a constant these factors adjust i input as 14 i o 0 65 i input 1 cloud 2 1 shade 1 reflectance and the development of ice on the surface will further attenuate solar radiation as 15 i o ice i o 1 α 2 1 β e γ h where α is albedo β is surface absorption γ is ice extinction coefficient and h m is the ice thickness edinger et al 1974 cole and buchak 1994 ice thickness can be calculated by wasp8 by simulating water temperature or included by using a time function 6 3 light attenuation through water column wasp8 simulates light attenuation through the water column following the beer lambert equation i z i o e k e λ z where k e λ is the light extinction coefficient m 1 for a given wavelength index λ and z is depth below the surface m wasp8 allows for a segment specific general light extinction coefficient or for a wasp calculated light extinction coefficient wasp8 calculates the light extinction coefficient specific to each wavelength as a summation of contributions from background water algal chlorophyll a chl μg l 1 raised to a chlorophyll exponent a λ unitless doc mg l 1 and total suspended solids tss mg l 1 as 16 k e λ k w λ k chl λ chl a λ k doc λ doc k solid λ tss additional information on the values for these wavelengths divisions of wavelength by class and the division of wavelengths into wave bands as a function of latitude are provided in the appendix 6 4 photoreactions solute chemicals and nanoparticles may undergo photoreactions wasp8 simulates photoreactions using first order kinetics with the photoreaction rate constant k photo d 1 which accounts for photoreactions due to ultraviolet and visible light the wavelength specific reaction rate constant k λ d 1 w 1 m2 due to each wavelength band is multiplied by the associated wavelength specific radiation in the segment i λ w m 2 and summed to provide the total rate constant as k p h o t o λ 1 10 k λ i λ 7 additional solute chemicals processes distribution and concentrations of dissolved contaminants in surface waters are significantly influenced by interactions between contaminants and the physical and chemical components of aquatic environments these fate processes must be fully assessed during the evaluation of contaminant fate and transport to accurately simulate the behavior of contaminants in surface waters wasp8 includes biodegradation oxidation reduction and photochemical reactions the previous wasp version wasp7 can simulate these four chemical reactions for up to three chemical state variables while wasp8 allows simulation of up to 10 variables including transformation of one chemical into another a chemical into nanoparticles as well as nanoparticles forming chemicals these advances allow for increasing the speed of modeling multiple contaminants at once e g simulating 4 metals at once as well as for simulating more complex biodegradation or transformation pathways e g simulating an organic chemical with 4 daughter products fig 3 presents an overview of the processes available for solute chemicals in wasp8 oxidation reduction and biodegradation reactions are simulated use a generic transformation reaction rate 17 k reaction k rate x temp x seg x segtype x phase x monod subs x monod env constructed by multiplying the base reaction rate constant k rate d 1 by a series of unitless factors the temperature correction factor x t e m p allows for the reaction rate to change as a function of temperature as x t e m p θ t 20 where θ unitless is the temperature activity coefficient and t celsius is temperature the segment multiplication factor x s e g allows each individual segment to have a different rate constant by multiplying it by this factor with a different value for each reaction process i e biodegradation oxidation reduction the segment type multiplication factor x s e g t y p e is set to affect all segments of a general type i e water column surface sediment subsurface sediment in this way reactions can be set binarily to either happen or not in the water or sediments or to be faster or slower depending on the segment type the phase multiplication factor x p h a s e is described as x p h a s e i 3 f i x i where f i is the mass fraction of the contaminant in phase i of three possible phases the freely dissolved phase doc complexed phase or sorbed onto a solid phase and x i are phase multiplication factors associated with each phase the phase multiplier effectively allows control for when a reaction takes place for dissolved doc complexed or sorbed species previously wasp only allowed for first order kinetics wasp8 now includes first order second order and monod kinetics to represent different kinetics formulations e g schnoor 1996 monod kinetics can be incorporated with the rate expressed as a product of a maximum rate constant μ max and rate limiting factors for the substrate concentration and additional environmental concentrations e g electron acceptors these factors are given by x m o n o d s u b s 1 k s c and x m o n o d e n v c e k e c where k s mg l 1 is the half saturation coefficient for the substrate c is the concentration of the substrate mg l 1 k e mg l 1 is the half saturation coefficient for an environmental concentration and c e mg l 1 is the environmental concentration of interest e g oxygen if monod substrate kinetics are used then k rate μ max x monod subs x monod env mg l 1 d 1 time functions can be used to apply time varying concentrations of reactants 8 additional processes for nanoparticles nanoparticles are new state variables in wasp8 routinely defined as materials sized between 1 and 100 nm engineered nanoparticles have been applied in all areas of our daily lives and their production has increased appreciably in recent years such rapid expansion of nanoparticle production increases the likelihood of nanoparticles being released into the environment besides the heteroaggregation particle attachment process in surface waters described in section 5 nanoparticles may undergo phototransformations sulfidation oxidation and dissolution see fig 4 similar formulations describing nanoparticle fate and transport processes have been presented praetorius et al 2012 quik et al 2015 photoreactions are modeled as described in section 6 4 for other reactions wasp8 uses a general formulation using the same general chemical reaction eqn 17 that is used for solute chemicals as is described in section 7 a general first order decay rate process is also available a nanoparticle can be set to transform into a daughter product which can be either a solute chemical or another nanoparticle with the yield of each given by a user defined yield coefficient 9 additional state variables in addition to the state variables and processes already discussed the wasp8 advanced toxicant module also has doc salinity temperature pathogens and mercury as available state variables in earlier versions of wasp doc was a segment specific parameter now wasp8 permits explicit simulation of 1 5 doc types which may oxidize or decay as first order processes and organic solids that may decay to form doc salinity temperature and pathogens were originally available in a separate wasp module heat with the option of setting the output from heat to be linked to the eutrophication or toxicant module mercury was previously available in wasp7 as an overlay using the early toxicant module kinetics module toxi with a separate mercury module merc merc simulated elemental mercury divalent mercury and methylmercury explicitly with specific transformation processes outlined for these species merc has been incorporated into the wasp8 advanced toxicant module with the additional state variable of recalcitrant divalent mercury so that historically contaminated mercury sites can explicitly simulate the difference between labile and recalcitrant mercury 10 example wasp8 model application hypothetical case study of the cape fear river the goal of this paper is to present the details on the advances implemented in wasp8 to provide details on a wasp8 model to demonstrate the new and updated capabilities an example wasp8 model was constructed using a hypothetical case study of the cape fear river north carolina nc us fig 5 first the model is developed to simulate flow and the simulated flow at the outflow is compared to observations next hypothetical simulation of solids concentrations is performed using realistic but arbitrary boundary concentrations the solids state variables are sand silt clay and particulate organic matter pom then a load of a solute chemical and then a nanomaterial are added representing a hypothetical release from a waste water treatment plant or industrial outflow the solute chemical transforms into a second state variable and the nanomaterial attaches to the different solids forming additional state variables i e nano silt nano clay nano pom additionally light intensity in the water column and doc are simulated explicitly physical chemical properties of solute and nanomaterial properties are presented in table 4 further details on solids solutes and nanomaterials are available in appendix tables a7 a9 the model domain consists of a 90 km stretch of the cape fear river downstream from usgs gage 02105500 to usgs gage 02105769 consisting of 45 water column segments each 2 km long and 92 m wide overlying 45 surface sediment segments 0 05 m depth and 45 subsurface sediment segments 0 15 m depth that is each 2 km reach of the river consists of three stacked layers water column surface sediment and subsurface sediment upstream of this section the cape fear is in the piedmont region of nc this section of the cape fear river is in the coastal plain region and is used as a source of drinking water at several locations and there are no substantial tributaries flow routing is simulated using the kinematic wave functionality and depth width velocity and volume are simulated per time step for each water column segment usgs 02105500 is used as the upstream inflow condition details on simulating flow for the cape fear river are presented in the appendix wasp8 has different methods for simulating flow in surface waters and for simulating simple hydrodynamics in 1 dimensional systems wasp8 itself is not a hydrodynamics model and relies on other more rigorous models for more complex hydrodynamics here a 1 dimensional kinematic wave is appropriate using the upstream boundary and the parameters we compared the outflow to the matching usgs gage 02105769 with an r2 0 92 and a nash sutcliffe efficiency 0 91 the simulated average travel time is 5 days with a range of 2 40 days further details on simulating flow are presented in the appendix further details on using wasp8 for stream routing are available in ambrose et al 2017 once stream flow is running solids and doc are simulated solids are simulated using two different approaches the descriptive option using constant settling and resuspension and the mechanistic option using dynamic settling deposition erosion and resuspension suspended solids have an upstream boundary condition of 6 mg l 1 silt 4 mg l 1 clay 2 mg l 1 pom and 8 mg l 1 doc initial concentrations of solids are zero in the water column and sand of 472 540 g m 3 silt of 250 030 g m 3 clay 356 500 g m 3 and pom 30 860 g m 3 in the sediment layers once the solids are running a load of 0 1 kg d 1 of the nanoparticle is added to the first segment the nanoparticle is set to attach to silt clay and pom particles via heteroaggregation kinetics forming new state variables nano silt nano clay and nano pom these particle attached nanoparticles then follow the processes of the solid to which they are attached the non attached nanoparticle photo degrades light is simulated using the option in which wasp8 calculates solar radiation based on the latitude and longitude of the cape fear location providing changes in surface light intensity with time of day and day of year next a load of 10 kg d 1 of chemical 1 is added to the first segment chemical 1 biodegrades into chemical 2 both solute chemicals volatilize attach to solids assuming equilibrium partitioning and complex with doc initial concentrations of solute chemicals nanoparticles and doc are zero everywhere the simulation was run for 15 years further details on parameterization are provided in the appendix the simulations presented here are not meant to provide a complete representation of all wasp8 capabilities but rather these example simulations are meant to provide details on some of the processes available in wasp8 and information on how one could construct a model depending on the questions to be answered and the goals to be met this example simulates four solid particle types where wasp7 could only simulate 3 nanoparticles are a completely new state variable which can form 3 other distinct other state variables as they attach to solid particles further the enhanced light and transformation routines are incorporated as part of its regular simulations wasp8 performs a mass check to confirm that mass balance is being upheld which wasp outputs live to the screen as it runs as well as part of its output files a perfect mass balance 1 for these simulations the mass check ranged from 0 999 to 1 0 10 1 solids the cape fear river wasp model was run using two different approaches for simulating solids a descriptive approach and a mechanistic approach as described in section 4 this is done to demonstrate the different results that one would expect with these different approaches the simulation results are presented in fig 6 due to the high settling velocity of sand there is no sand observed in the water column in the descriptive solids simulation in the water column silt clay and pom fluctuate as the flow and velocity of the segment fluctuate clay is the dominant solid type in the water column as it has the slowest settling velocity the solid concentrations in the surface sediment layer remains relatively constant with silt slightly increasing as it settles from the water column to the sediment layer the subsurface sediment layer remains relatively constant throughout the duration of the simulation for the mechanistic solids the water column concentrations of silt clay and pom vary over the course of the simulation generally the water column concentrations are lower than for the descriptive solids option reflecting a deposition dominated process where resuspension is small or negligible the spikes in water column solids concentration occur during times of high bottom shear stresses which result in erosional events in response to the increased solids concentrations in the water column the sediment layer loses mass of the finer particles of silt clay and pom wasp8 then moves solids from the deep sediments to replace the lost solids which results in enriching the surface sediment layer with sand this result is reflected by the jump in sand concentration during the largest erosional event this comparison of the simulated solids concentration trends using the descriptive and mechanistic approaches illustrates the fact that the mechanistic approach results in greater variation in suspended solids concentrations as a function of time 10 2 light fig 7 shows an example of light simulation in wasp8 the ten different bands of light attenuate differently through the water column the light intensity at noon is shown for all 10 wavelength bands for each day for the entire year for the length of the simulation the total light intensity changes over the course of the year highest in summer and lowest in winter the fraction of the total light intensity is demonstrated by the thickness of each band as the concentration of solids changes in the water column the amount of attenuation of each band of light changes the total photoreaction rate constant is the sum of the contributions due to each wavelength band the simulation is parameterized to have higher wavelength specific base rate constants for uvb with decreasing rate constants as the wavelength increases this translates to the pattern observed in fig 7 where the shorter wavelengths have a larger contribution to the total photoreaction rate constant even though they comprise a smaller fraction of the total light intensity for each time step and each location wasp is calculating the attenuation of light based on the concentrations of particles and doc and depth and then calculating the total reaction rate constant which is then applied to any state variable with a light driven process 10 3 nanoparticles fig 8 presents concentrations of the different phases of nanoparticles in two representative segments segment 20 and 40 wasp simulates concentrations in all segments for all time steps and these two segments are presented for illustration the descriptive and mechanistic solids options are both presented to provide an example for how these choices can affect the fate and transport of the nanoparticles in the water column the total nanoparticle concentrations vary inversely with flow as the load is held constant and flow varies over time the nanoparticle is present predominantly in the free non attached form free nano and concentrations decrease moving downstream as nanoparticles attach and settle to the sediments in the sediment layer the dominant form is the nanoparticle attached to clay nano clay followed by nanoparticle attached to silt nano silt with a small fraction attached to pom nano pom this is the case for both the descriptive solids and mechanistic solids case for the descriptive solids the concentrations in the sediment steadily accumulate over time in the mechanistic solids case there is less deposition to the sediment layer and thus the concentrations accumulate less halfway through the simulation there is an erosional event as seen in fig 6 halfway through the simulation there is a spike of solids in the water column and a jump in the surface sediments solid concentration at the same time the concentration of nano clay in the surface sediments decreases suddenly following this event the concentration of nano clay increases for the deep sediment layer the descriptive solids option shows an accumulation of nanoparticle concentrations increasing slower than in the surface layer for the mechanistic solids option there is minimal deposition from the surface sediments to the deep sediment layer in this example application therefore no nanoparticles enter this layer 10 4 solute chemicals fig 9 presents the concentrations of the two chemicals for the mechanistic solids option moving downstream in segment 10 20 30 and 40 these figures focus on the mechanistic solids to illustrate how the concentrations change moving along the length of the river in this scenario chemical 1 biodegrades into chemical 2 thus upstream the concentration of chemical 1 is larger than chemical 2 moving downstream chemical 2 increases in concentration as chemical 1 decreases the model is structured using equilibrium partitioning of the chemicals and the solid phases with chemical 1 being more hydrophilic than chemical 2 the difference in hydrophilicity is evidenced in that chemical 2 increases in concentration in the sediments moving downstream 11 conclusions the wasp8 advanced toxicant module has undergone a significant redesign and upgrade from previous wasp versions this upgrade includes the introduction of more state variables and different types of state variables particularly with the introduction of the nanoparticle state variable and the expansion of how light attenuation and photoreaction processes are handled with different options for simulating solids which are a critical aspect of toxicant fate and transport modeling over the years wasp has been increasingly used to investigate and address different contaminant fate and transport problems we hope that with these advances in the toxicant module that wasp8 will provide a modeling framework for a range of users that is flexible and rigorous enough to address the expanding range of problems posed by both historic and emerging contaminants disclaimer this paper has been reviewed in accordance with the u s environmental protection agency s peer and administrative review policies and approved for publication mention of trade names or commercial products does not constitute endorsement or recommendation for use the views expressed in this article are those of the authors and do not necessarily represent the views or policies of the us epa acknowledgements we would like to acknowledge the efforts of lourdes prieto for gathering the data necessary for constructing the cape fear river wasp model and the helpful review and comments by caroline stevens and craig barber as well as by an anonymous reviewer appendix supplementary data the following is the supplementary data to this article knightesetal2018 knightesetal2018 appendix supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 012 funding this work was funded through the us epa s office of research and development s chemical safety for sustainability research program this research did not receive any specific grant from funding agencies in the commercial or not for profit sectors 
26304,toxicant concentrations in surface waters are of environmental concern due to their potential impacts to humans and wildlife numerical models provide system insight support management decisions and provide scenario testing on the impacts of toxicants the water quality analysis simulation program wasp is a widely used framework for developing site specific models for simulating toxicant concentrations in surface waters and sediments over a range of complexities and temporal and spatial scales wasp8 with the advanced toxicant module has been recently released incorporating a complete architecture redesign for an increased number of state variables and different state variable types wasp8 incorporates a new structure for simulating light intensity and photoreactions in the water column including the distinction of 10 different wavelength bands and nanoparticle heteroaggregation to solids we present a hypothetical case study using the cape fear river north carolina as a representative example for simulating solute chemicals nanoparticles and solids to demonstrate the new and updated capabilities of wasp8 keywords surface waters numerical modeling nanoparticles toxicants sediments software availability program title water quality analysis simulation program version 8 developers tim wool robert b ambrose jr address united states environmental protection agency water quality planning branch data and information analysis section region 4 atlanta ga 30303 us tel 404 562 9260 e mail wool tim epa gov first available july 7 2017 hardware 64 bit windows 64 bit mac osc yosemite or higher 64 bit linux built on ubuntu software wasp 8 2 executable https www epa gov ceam water quality analysis simulation program wasp http epawasp twool com language fortran 95 c size executable is 3 mb files require 205 mb 1 introduction exposure to toxic contaminants in surface waters may pose a threat to the health of humans and wildlife in the united states us the us environmental protection agency epa has passed a series of acts to address managing toxic contaminants in rivers and streams such as the clean water act cwa the toxic substances control act tsca the federal insecticide fungicide and rodenticide act fifra and the comprehensive environmental response compensation and liability act cercla superfund the implementation of environmental models is critical to the regulatory decision making process spatial and temporal scales linking environmental controls and environmental quality generally do not allow an observational approach to understand the relationship between economic activity and environmental quality nrc 2007 environmental models are used in the regulatory process such as assisting in the development of total maximum daily loads under cwa evaluating different feasible remediation strategies cercla or evaluating the possible impacts of the release of newly developed chemicals review under tsca the water quality analysis simulation program wasp is a differential spatially resolved mass balance fate and transport modeling framework structured to allow users to simulate concentrations of environmental contaminants in surface waters and sediments wasp was originally developed in the 1980s and has undergone a series of improvements and versions over the decades it is now widely used throughout the us and the world wasp has been applied to a range of different surface water systems addressing a range of environmental contaminants wasp has been previously implemented for nutrient loading on water quality in tampa bay florida us wang et al 1999 mercury fate and transport in the carson river nevada us carroll et al 2000 nutrients tmdl for neuse river estuary north carolina us wool et al 2003 mercury remediation strategies in the sudbury river massachusetts us knightes 2010 impacts of climate change on water quality of chungju lake south korea park et al 2013 the fate and transport of multi walled carbon nanotubes in brier creek georgia us bouchard et al 2017 and as part of an uncertainty and risk analysis for a margin of safety for a nutrient tmdl for sawgrass lake florida us camacho et al 2018 most recently wasp8 v 8 2 was released which incorporates an overhaul of the wasp architecture and a restructuring and upgrading of the toxicant module which is the focus of this paper wasp8 updated the underlying code from fortran 77 to fortran 95 and incorporated a new wasp interface linkage to a post processor water resources database wrdb v 6 1 www wrdb com and the ability to run on windows mac os or linux wasp8 contains two modules advanced eutrophication and advanced toxicant over the past several years wasp development has focused on the eutrophication module martin et al 2006 wool et al 2013 and much modeling research has focused on eutrophication gargallo et al 2018 nguyen et al 2018 sadeghian et al 2018 less work over the years has focused on the toxicant module routines with limited development on the toxicant module since the 1990s with the updated architecture of wasp8 recent work has focused on upgrading the toxicant module routines one driver of this effort was interest in emerging contaminants and their potential exposure concentrations particularly nanomaterials once released into the environment previous versions of wasp were not structured in a way to handle the introduction of these new contaminants consequently the wasp8 advanced toxicant module completely restructured its state variables as distinct arrays to permit for simulation of any number of chemical solutes solid particles and nanoparticles this is a departure from the original architecture additional functionality has also been added to incorporate nanoparticle specific processes specifically particle attachment kinetics in this paper we present the newly developed publicly available wasp8 advanced toxicant module https www epa gov ceam water quality analysis simulation program wasp epawasp twool com the structure of the underlying software and modeling framework and how it fits within an overall multi media system of models is discussed the general structure of the wasp8 advanced toxicant module with details on the changes from the previous wasp toxicant modules particularly regarding the change in the structure of simulating state variables and the available state variables is described and specific state variables and processes are explained the three main state variables are solid particles solute chemicals and nanoparticles first the solids module is addressed as solids are an important factor for sufficiently simulating toxicants particularly regarding contaminants attaching to particles and being transported by them next partitioning and particle attachment are discussed and the different processes of equilibrium partitioning kinetic partitioning and heteroaggregation light attenuation and phototransformations are then presented followed by a focus on the processes governing solute chemicals and then nanoparticles each of these sections discusses the options for different processes available to the user from descriptive to mechanistic discussion of the different processes concludes with a hypothetical application of wasp8 using the cape fear river as a case study the goal of this application is to demonstrate how the updated wasp can be applied for the different state variables and their governing processes the application is structured to simulate flow and four different solids using both the descriptive and mechanistic solids algorithms to this system there is a hypothetical release of a solute chemical which transforms into another solute chemical and a nanoparticle which attaches to the different solid phases the system uses time varying flow conditions simulated using kinematic wave routing light is simulated temporally over the day and year with attenuation and phototransformation of the nanoparticle 2 modeling framework wasp is a surface water modeling framework designed to create site specific models for simulating environmental concentrations in the water column and sediment layers of different aquatic ecosystems over space and time the wasp framework is used to develop a water quality model for a specific system of interest and contaminants of concern and is structured using a series of different module components fig 1 shows the overall wasp modeling framework generally the user interacts with wasp through the interface to create the wasp input file wif wasp reads the wif to run a simulation wasp has a series of hydrodynamic modules incorporated into its architecture e g flow routing kinematic wave diffusive wave and dispersive wave ambrose and wool 2017 and can import data files from other hydrodynamic models e g efdc via a hydrodynamic file hyd file the science modules are shared libraries for kinetic parameters and transformation processes for the specific state variables stored data files include data base time series information for time functions e g solar radiation air temperature boundary concentrations inflows and loads once the wasp simulation is performed an out file a text file of parameters and outputs and a bmd binary data file are produced the bmd is readable by wrdb which is a tool that works with wasp to manage data and provide graphical outputs wrdb can also be used to generate figures or files for other programs to read e g csv for further data analysis wasp can also be used in the greater context of multi media modeling taking output from watershed models e g swat nietsch et al 2011 hspf bicknell et al 2001 and feeding forward as input into bioaccumulation models e g bass barber 2006 gobas model arnot and gobas 2004 as was done to link atmospheric deposition to fish tissue concentrations knightes et al 2009 and can be a part of a multi media modeling framework like the framework for risk analysis in multimedia systems frames johnston et al 2011 whelan et al 2014 wasp can be structured with different spatial resolutions as required by the user to simulate zero dimensional lakes and ponds one dimensional streams with or without branching two dimensional rivers and stratified lakes and three dimensional estuaries or large lakes the wasp model domain is constructed with wasp segments surface water or subsurface water water segments and surface benthic or subsurface benthic sediment segments segments are assumed to be uniformly mixed whether they are a water column segment or a sediment segment within each segment each state variable is simulated for every time step the user defines how the segments are laid out in space through the interface this allows for three dimensional model domains with multiple sediment layers as needed wasp incorporates simple hydrodynamic modules ambrose and wool 2017 which include general constant flow routing stream routing using hydrogeometry kinematic wave routing using slope and roughness and dynamic wave for tidal influences based on dynhyd5 ambrose et al 1993 these modules are best suited for one dimensional systems including branching for larger lakes and estuaries a hydrodynamic model can be used to provide details on flow which can be entered manually using flow routing or via hydrodynamic model linkage efdc can be used to simulate the hydrodynamics of the system which is then linked to the wasp model for simulating contaminant fate and transport which has been demonstrated successfully for a nutrient tmdl in the neuse river nc usa wool et al 2003 for algal blooms caused by increased residence time by construction of instream structures in the nakdong river korea seo et al 2012 and simulating water quality in yongdam lake korea seo et al 2009 wasp currently contains two kinetics modules advanced eutrophication and advanced toxicants example state variables available in both modules are presented in table 1 the advanced eutrophication module has explicitly incorporated state variables e g nitrate ammonia orthophosphate martin et al 2006 wool et al 2013 and has been reviewed in the context of other eutrophication models sharma and kansal 2013 the advanced toxicant module is structured differently than the advanced eutrophication module by having state variable classes e g solute chemicals nanoparticles solids this provides the user with the flexibility of constructing a wasp model designed for a range of different contaminants of interest by defining and parameterizing governing processes the user effectively defines that state variable e g a solute chemical is defined as atrazine by its parameterization each state variable and process has different options for how it is described governing equations for concentrations follow the advection dispersion reaction equation e g gem little et al 2018 ce qual w2 sadeghian et al 2018 ce qual icm cerco et al 2010 the science module and the transport module are solved using differential mass balance equations for each segment for each time step the user may select different solution techniques but generally the forward euler numerical approximation approach is used with varying time steps designed to minimize numerical instability 3 wasp8 advanced toxicant module the advanced toxicant module has been updated as part of wasp8 to simulate an increased number of specific state variables table 2 presents some of the changes from wasp7 to wasp8 with respect to the advanced toxicant module originally the wasp toxicant module had a single class of state variables and wasp kept track of whether it was a solid type or a dissolved chemical type this legacy architecture made it challenging to incorporate increased numbers of state variables as well as different types of state variables using a new architecture state variables have been separated into their own classes with up to 5 or 10 state variables per state variable class these numbers can be increased by recompiling after adjusting a compile parameter now that wasp8 has separate state variables the structure of the advanced toxicant module has been redesigned and additional processes and options have been added the new design also facilitates updates in the science and algorithms moving forward with wasp future development table 3 presents an overview of the processes for different state variables in wasp8 as part of the development of the wasp8 advanced toxicant module model verification was performed the solids light particle attachment and nanomaterial and chemical solute reaction algorithms were isolated and simple simulations were performed and compared to analytical solutions to verify that the algorithms and modules were performing properly ambrose et al 2017 4 solids module solids are an important environmental component of water quality modeling suspended solid concentrations can be of concern on their own by reducing light penetration or blanketing benthic spawning areas or because environmental contaminants attach to the particles and their transport becomes linked to the movement of the particles in previous wasp releases up to three solid particles could be simulated e g sand fines clay silt and particulate organic matter in wasp8 there can be up to 10 solids types as defined by the user such that clays and silts can now be separate state variables this allows for increased refinement in solids modeling with different settling and resuspension rates as well as different partition coefficients increased numbers of particle size classes have been shown to affect the distribution and accumulation of simulated nanomaterials in surface waters quik et al 2015 each solid type state variable is defined by the way the user constructs the model and parameterizes the solids state variable solids in the advanced toxicant module can either be organic materials e g plankton algae detritus or inorganic e g sand silt clay the solids state variable captures the processes for all these solids types in a general framework the user parameterizes each solid type to reflect its characteristics and properties fig 2 there are four options for simulating solids in wasp solid flow fields descriptive mechanistic van rijn equations van rijn 1984 or mechanistic roberts equations roberts et al 1998 for all options all solids move via advection dispersion may also move solids if the exchange option is turned on and dispersion rates are defined between surface water segments particle density is set for each particle type 4 1 solid flow fields the solid flow fields option is the legacy option used in wasp7 and earlier versions this option treats each solid state variable type separately movement of solids from one segment to another is explicitly defined by using a flow field of a specific flow velocity and the cross sectional area of the interface flow fields are generic and the user establishes whether it is resuspension or settling via the direction of the flow field as each solid flow field is linked to a solids type and is from the legacy structure the flow fields are limited to three 4 2 descriptive solids transport the descriptive solids transport fig 2a option uses constant settling and resuspension velocities which do not change with stream velocity for this option the settling velocity v s m s 1 and resuspension velocity v r m s 1 are entered for each solid type for each segment as part of the model domain set up in the segment definition the user enters which segment is below a given segment wasp then knows for a given segment where both settling and resuspension is going to and coming from therefore if a model has sand silt and clay the settling and resuspension velocities for descriptive transport are determined by the user and entered directly into the interface 4 3 mechanistic solids transport the mechanistic solids transport fig 2b options simulate dynamic settling deposition erosion and resuspension velocities based on a set of parameters and the simulated flow velocity of the surface water settling is the process of solids moving downward through the water column and is a function of particle size and density deposition erosion bedload and resuspension are functions of the shear stress on the stream bed deposition is when a solid particle moves from the water column to the surface sediment erosion is when solids move from the surface sediment layer into the overlying water boundary layer resuspension is when the solid particle leaves the boundary layer and enters the water column erosion and resuspension additionally depend on whether the sediment layer is acting cohesively or non cohesively for a non cohesive sediment layer all particles act independently when a sediment layer is cohesive all particles 0 10 mm diameter fines erode as a single unit the user defines the critical cohesive sediment fraction for which the sediments act cohesively the default is 0 2 so when fines comprise 20 or more of the bed the bed acts cohesively the inclusion of the mechanistic solids transport allows for erosional events during high flows and for depositional events and zones for slower flows 4 3 1 settling for the mechanistic option wasp8 calculates the settling velocity for different ranges of particle diameter d m as a function of particle density ρ s kg m 3 water density ρ w kg m 3 and absolute viscosity μ kg m 1 s 1 van rijn 1984 1 v s r d 18 g d d 100 μ m 10 r d 1 0 01 r d 2 1 g d 100 μ m d 1 m m 1 1 g d d 1 m m where r d is the sediment particle densitometric reynolds number r d d g d μ ρ w unitless and g g ρ s ρ w 1 m s 2 where g is the acceleration of gravity 9 807 m s 2 4 3 2 deposition for the mechanistic option wasp8 calculates the deposition velocity v d m s 1 as v d v s α d where α d unitless is the probability that a solid will deposit and is a function of bottom shear stress as well as the lower and upper critical shear stress thresholds τ c d 1 n m 2 and τ c d 2 n m 2 krone 1963 where 2 α d 1 τ b τ c d 1 τ c d 2 τ b τ c d 2 τ c d 1 γ d τ c d 1 τ b τ c d 2 0 τ b τ c d 2 and γ d is a dimensionless exponent default 1 this relationship captures the fact that all settling particles deposit onto the sediment layer when shear stress is low as shear stress increases fewer particles deposit until flow conditions reach a regime where the shear stress is so high that no particles deposit this allows the system to shift between deposition or no deposition 4 3 3 non cohesive erosion the model domain does not differentiate the boundary layer fig 2b as a separate wasp8 segment from the overlying water column the boundary layer is used conceptually to separate the processes of erosion and resuspension in wasp8 when the sediment is acting non cohesively fine fraction is less than critical cohesive fraction all particles act non cohesively in cohesive sediment segments only sands and larger particles 0 1 mm diameter are subject to non cohesive erosion and resuspension wasp8 calculates the non cohesive erosion velocity for each solids type using equations either developed by van rijn or roberts depending on the solids option the van rijn equation calculates the gross deposition rate v s m s 1 where v e e v s using a proportionality constant e where 3 e 0 015 γ e d k s r d 0 2 τ η and γ e is a user specified multiplier default 1 η is a user specified exponent default 1 5 and τ is the non dimensional shear stress given by 4 τ τ b τ c e τ c e τ b τ c e 0 τ b τ c e where τ c e is the critical shear stress for erosion n m 2 see appendix eqn a 3 for the roberts equation 5 v e γ e a ρ b m τ b n where ρ b is the bulk density kg m 3 and coefficient a and exponents m and n are determined experimentally for different particle sizes from fine silt 5 7 nm to coarse sand 1 25 mm roberts et al 1998 4 3 4 non cohesive resuspension particles that erode from the sediment layer enter the boundary layer at that point particles can resuspend and enter the overlying water segment or travel downstream via bedload eroded particles move in the boundary layer via bedload when the shear stress is less than the critical shear stress for resuspension τ c r n m 2 see appendix when τ b τ c r particles resuspend with a resuspension velocity v r f r v e m s 1 where 6 f r 0 τ b τ c r l n u v s l n u c r v s l n 4 l n u c r v s τ b τ c r 1 u 4 v s and u τ b ρ w is the shear velocity m s 1 and u c r τ c r ρ w is the critical shear velocity for resuspension m s jones and lick 2001 hydroqual 2007 4 3 5 non cohesive bedload bedload is the transport of non cohesive solid particles downstream through the boundary mobile layer when τ b τ c e bedload may begin the bedload flux per unit width g b l g m 1 s 1 is given as 7 g b l α b l ρ s uh d 50 h 1 2 u u c e ρ s ρ w ρ w g d 50 η where d 50 is the mean particle diameter m α b l is a fitted coefficient u is stream velocity m s 1 h is water depth m η is a fitted exponent and u c e is the critical stream velocity for erosion see appendix suggested values for the fitted parameters are α b l 15 and η 1 5 van rijn 2007 4 3 6 cohesive resuspension when the surface sediment layer is in the cohesive regime all cohesive solids erode at the erosion velocity v e m s 1 the cohesive erosion flux e coh g m 2 s 1 is given by the excess shear stress power law formulation lick et al 1994 e c o h f c o h m τ n where m is the shear stress multiplier g m 2 s 1 n is the shear stress exponent f coh is the fraction of the surface bed that is cohesive and τ is the excess shear stress n s 1 defined in section 4 3 3 the shear stress multiplier varies between 0 1 and 100 g m 2 s 1 with a default value of 5 the shear stress exponent varies between 1 6 and 4 with a default value of 3 the critical shear stress for erosion varies between 0 5 and 8 n m 2 with a default value of 2 wasp8 is structured so that the shear stress multiplier exponent and the critical shear stress for erosion can vary spatially if desired 4 3 7 organic solids production and dissolution additional processes are available if the user wants to classify a solid particle as an organic solid such as algae or particulate organic matter pom for any given solid the net production rate r p g m 3 d 3 is given as 8 r p r p s r p t θ p t 20 where r p s g m 3 d 1 is the spatially variable production rate by wasp segment r p t g m 3 d 1 is the time variable production rate defined by providing a wasp time function and θ p is the temperature correction coefficient organic matter may also decay and leave behind ash and dissolved organic matter using a similar functionality the loss rate of an organic solid particle via dissolution is given by the dissolution rate constant k d d 1 9 k d k d s k d t θ d t 20 where k d s d 1 is the spatially variable dissolution rate by wasp segment k d t d 1 is the time variable dissolution rate defined by providing a wasp time function and θ d is the temperature correction coefficient when an organic solid type dissolves a fraction of the particle leaves behind residue described as the ash dry weight residue the mass associated with the fraction of residue then is added to a different solid type as identified by the user similarly the organic fraction may dissolve into one of the doc state variables for example as pom dissolves a fraction may form silt and another fraction may form doc the advanced toxicant module has these simple equations for organic solids if a more rigorous representation of organic matter is desired then the advanced eutrophication module can be run to simulate nutrients and algae growth of 3 types of phytoplankton the output can then be pulled directly into the advanced toxicant as an input file wool et al 2013 4 3 8 solids burial and erosion wasp segments are classified as either water or sediments beneath the water column the user can structure one or multiple sediment segments for each sediment layer segment the initial solids concentration in each segment establishes the reference bulk density g m 3 and porosity m3 void space m 3 as the simulation progresses solid particles are depositing eroding growing and dissolving for each sediment segment wasp8 conducts a solids mass balance with two approaches for representing the sediment bed volume static constant volume and dynamic constant bulk density the dynamic runs using the benthic time step δt b d operates distinctly from the wasp time step for the surface sediment layer solids may be deposited so that total mass in the segment increasing or eroded resulting in mass increasing for the static option the volume is held constant so the density of the segment changes to account for the change in mass and solids aren t moved between segment layers for the dynamic option the volume changes and the density is held constant if there is only one layer one segment in the sediment then deposition can cause the layer to increase in depth and volume indefinitely and there will be no burial no sediments are pushed out the bottom of the segment conversely erosion can result in loss of volume under the dynamic option however erosion is halted after segment volume reaches 5 of the initial value when there are multiple sediment layers in the dynamic option the surface sediment segment volume and depth are reset to initial values at each benthic time step wasp8 calculates the segment volume that needs to move v b m3 and its associated depth d b m to return to the initial volume and depth the mass of contaminants and solids move with v b m d 1 the burial rate burial fluxes f b i g m 2 s 1 of solid or pollutant i are given by f b i c i v b where c i g m 3 is the concentration of the constituent of interest in the sediment layer negative burial rates reflect erosion under erosion conditions the burial rate is reversed and wasp8 recruits the necessary volume v b from the sediment layer below the mass moved then reflects the concentration of the sediment layer below the segment of interest in the subsurface sediment layers at each benthic time step mass is either moved upward or downward under deposition conditions the subsurface sediment segment receives solids from the overlying sediment segment at each time step if the subsurface sediment segment has a higher bulk density v b and d b are compressed and pore water is pushed upward the compressed volume is then passed downward to the next sediment layer or out of the system solids and pollutants in this volume are similarly passed downward maintaining the initial bulk density or porosity under erosion conditions the sediment segment receives the erosion volume from the underlying sediment layer along with the associated solids and pollutant masses this process is continued downward until all underlying sediment segments have been adjusted 5 toxicant partitioning and attachment to particles one of the most important processes governing the fate and transport of toxicant chemicals is their interaction with particles wasp8 currently incorporates equilibrium partitioning between solute chemicals and solid particles kinetic sorption and heteroaggregation kinetics of nanoparticles and solid particles previously wasp only allowed for equilibrium partitioning this section presents the modeling structures available for simulating the interactions of both solute chemicals and nanoparticles with particles 5 1 equilibrium partitioning for organic chemicals partitioning onto sediment organic matter partitioning kinetics are usually faster than the time steps simulated in wasp under these conditions or as a simplifying assumption instantaneous equilibrium partitioning is often used in water quality modeling of toxicant contaminants for environmental concentrations 10 5 m or one half of water solubility equilibrium sorption is linear with dissolved concentration karickhoff 1984 as c s i k d i c w i for a pollutant i and a particular solid s c s i mg l 1 is the chemical concentration on the solid phase c w i mg l 1 is the concentration in the water phase and k d i l kg 1 is the partition coefficient when there are multiple solid phases present this relationship holds for all constituents and the overall fraction of each chemical on each phase is given by solving all equations simultaneously so that 10 f i j k d i j s j 1 i 1 n k d i j s j where f i j is the fraction of a solute chemical i on the given solid phase j e g silt sand clay n is the number of solid phases present j and s j mg l 1 is the specific solid concentration complexation with doc is simulated using the same functionality 5 2 kinetic sorption in some cases equilibrium partitioning is not a good representation of the system for example solute chemicals may exhibit sorption and desorption hysteresis where kinetic sorption may be a more appropriate representation because the system is dynamic wasp8 uses separate state variables to represent the freely dissolved chemical c w i mg l 1 and the sorbed chemical c s i mg l 1 this model structure requires n 1 state variables where n is the number of solids present in the system and s mg l 1 is the concentration of the specific solid type the sorption process is described as 11 d c w i dt k for c w i s k rev c w i where k f o r l mg 1 d 1 is the forward reaction rate constant sorption and k r e v d 1 is the reverse reaction rate constant desorption 5 3 nanoparticle heteroaggregation kinetics attachment of nanoparticles to solid particles is simulated using equations based on colloidal theory using heteroaggregation kinetics arvidsson et al 2011 praetorius et al 2012 the overall heteroaggregation rate is defined by 12 k het i α k coll i j n s j where α unitless varies from 0 to 1 is the attachment efficiency which is a user entered segment specific parameter k coll i j m3 d 1 is the rate of collision calculated internally by wasp8 between the nanoparticle i and a solid particle j and n s j m 3 is the number concentration of suspended particles an explicitly simulated state variable the collision rate is comprised of three components brownian motion fluid motion and differential settling 13 k coll 2 k b t w 3 μ w r np r s 2 r np r s 4 3 g r np r s 3 π r np r s 2 v np s v s s where k b 1 38 10 20 m2 g s 2 k 1 is the boltzmann constant and μ w g m 1 s 1 is the dynamic viscosity of water calculated as a function of temperature t w kelvin is the temperature of water g s 1 is the shear rate r np m is the radius of the nanoparticle r s m is the radius of the solid particle v n p s m s 1 is the settling velocity of the nanoparticle and v s s m s 1 is the settling velocity of the solid particle the k c o l l is calculated internally in wasp8 for each segment at each time step t can be directly simulated or entered for each segment or as a time function the user enters r np and r s the v n p s is user defined per segment the v s s is defined based on the solids option the user chooses section 4 the shear rate g is calculated as g u h wasp8 requires n 1 nanoparticle state variables for simulating heteroaggregation for each nanoparticle type where n is the number of solid particles simulated in the system 6 light module and phototransformations solar radiation is an important factor in simulating environmental constituents in surface waters solute chemicals schwarzenbach et al 1993 metals stumm and morgan 1981 morel and hering 1993 and nanoparticles hou et al 2015 may undergo photochemical reactions resulting in their degradation and or transformation pathogens and viruses may be deactivated by different wavelengths of light to effectively simulate phototransformations requires the light intensity that reaches the surface of the water body the amount of light that penetrates through the water column and the photoreaction itself the amount of light reaching the surface depends on cloud cover shading and the solar zenith angle solar radiation is comprised of different wavelengths each wavelength affects different environmental constituents in different ways attenuates differently as it passes through the water column and comprises a different fraction of solar radiation wasp8 accounts for ultraviolet 295 379 nm visible 380 749 nm and infrared 750 2500 nm ultraviolet and visible light are both divided into five wavelength bands see appendix this section provides details on the different options for incorporating input total radiation attenuation of solar radiation before reaching the water surface light attenuation in the water column and phototransformations 6 1 input total solar radiation wasp8 has 3 different options for quantifying solar radiation i input w m 2 h 1 reaching the water surface internally calculated diel light user input diel light or user input daily light with calculated diel light for the first option wasp8 will calculate i input based on the site latitude and longitude varying by date and time of day for the second option i input as one or more up to four time functions is entered so different locations can have different solar radiation at the surface as a function of time for the third option the sum of all radiation for a given day w m 2 h 1 is entered then wasp8 calculates i input over the course of the day and year 6 2 solar radiation to water surface as solar radiation passes through the atmosphere it attenuates due to different processes if wasp8 is calculating i input atmospheric attenuation is not automatically incorporated if solar radiation is entered then the included attenuation processes needs to be known so the others can be accounted for wasp8 incorporates four attenuation processes specifically cloud cover canopy shading water surface reflectance and attenuation in ice cloud cover and canopy shading are incorporated by using time functions and reflectance is a constant these factors adjust i input as 14 i o 0 65 i input 1 cloud 2 1 shade 1 reflectance and the development of ice on the surface will further attenuate solar radiation as 15 i o ice i o 1 α 2 1 β e γ h where α is albedo β is surface absorption γ is ice extinction coefficient and h m is the ice thickness edinger et al 1974 cole and buchak 1994 ice thickness can be calculated by wasp8 by simulating water temperature or included by using a time function 6 3 light attenuation through water column wasp8 simulates light attenuation through the water column following the beer lambert equation i z i o e k e λ z where k e λ is the light extinction coefficient m 1 for a given wavelength index λ and z is depth below the surface m wasp8 allows for a segment specific general light extinction coefficient or for a wasp calculated light extinction coefficient wasp8 calculates the light extinction coefficient specific to each wavelength as a summation of contributions from background water algal chlorophyll a chl μg l 1 raised to a chlorophyll exponent a λ unitless doc mg l 1 and total suspended solids tss mg l 1 as 16 k e λ k w λ k chl λ chl a λ k doc λ doc k solid λ tss additional information on the values for these wavelengths divisions of wavelength by class and the division of wavelengths into wave bands as a function of latitude are provided in the appendix 6 4 photoreactions solute chemicals and nanoparticles may undergo photoreactions wasp8 simulates photoreactions using first order kinetics with the photoreaction rate constant k photo d 1 which accounts for photoreactions due to ultraviolet and visible light the wavelength specific reaction rate constant k λ d 1 w 1 m2 due to each wavelength band is multiplied by the associated wavelength specific radiation in the segment i λ w m 2 and summed to provide the total rate constant as k p h o t o λ 1 10 k λ i λ 7 additional solute chemicals processes distribution and concentrations of dissolved contaminants in surface waters are significantly influenced by interactions between contaminants and the physical and chemical components of aquatic environments these fate processes must be fully assessed during the evaluation of contaminant fate and transport to accurately simulate the behavior of contaminants in surface waters wasp8 includes biodegradation oxidation reduction and photochemical reactions the previous wasp version wasp7 can simulate these four chemical reactions for up to three chemical state variables while wasp8 allows simulation of up to 10 variables including transformation of one chemical into another a chemical into nanoparticles as well as nanoparticles forming chemicals these advances allow for increasing the speed of modeling multiple contaminants at once e g simulating 4 metals at once as well as for simulating more complex biodegradation or transformation pathways e g simulating an organic chemical with 4 daughter products fig 3 presents an overview of the processes available for solute chemicals in wasp8 oxidation reduction and biodegradation reactions are simulated use a generic transformation reaction rate 17 k reaction k rate x temp x seg x segtype x phase x monod subs x monod env constructed by multiplying the base reaction rate constant k rate d 1 by a series of unitless factors the temperature correction factor x t e m p allows for the reaction rate to change as a function of temperature as x t e m p θ t 20 where θ unitless is the temperature activity coefficient and t celsius is temperature the segment multiplication factor x s e g allows each individual segment to have a different rate constant by multiplying it by this factor with a different value for each reaction process i e biodegradation oxidation reduction the segment type multiplication factor x s e g t y p e is set to affect all segments of a general type i e water column surface sediment subsurface sediment in this way reactions can be set binarily to either happen or not in the water or sediments or to be faster or slower depending on the segment type the phase multiplication factor x p h a s e is described as x p h a s e i 3 f i x i where f i is the mass fraction of the contaminant in phase i of three possible phases the freely dissolved phase doc complexed phase or sorbed onto a solid phase and x i are phase multiplication factors associated with each phase the phase multiplier effectively allows control for when a reaction takes place for dissolved doc complexed or sorbed species previously wasp only allowed for first order kinetics wasp8 now includes first order second order and monod kinetics to represent different kinetics formulations e g schnoor 1996 monod kinetics can be incorporated with the rate expressed as a product of a maximum rate constant μ max and rate limiting factors for the substrate concentration and additional environmental concentrations e g electron acceptors these factors are given by x m o n o d s u b s 1 k s c and x m o n o d e n v c e k e c where k s mg l 1 is the half saturation coefficient for the substrate c is the concentration of the substrate mg l 1 k e mg l 1 is the half saturation coefficient for an environmental concentration and c e mg l 1 is the environmental concentration of interest e g oxygen if monod substrate kinetics are used then k rate μ max x monod subs x monod env mg l 1 d 1 time functions can be used to apply time varying concentrations of reactants 8 additional processes for nanoparticles nanoparticles are new state variables in wasp8 routinely defined as materials sized between 1 and 100 nm engineered nanoparticles have been applied in all areas of our daily lives and their production has increased appreciably in recent years such rapid expansion of nanoparticle production increases the likelihood of nanoparticles being released into the environment besides the heteroaggregation particle attachment process in surface waters described in section 5 nanoparticles may undergo phototransformations sulfidation oxidation and dissolution see fig 4 similar formulations describing nanoparticle fate and transport processes have been presented praetorius et al 2012 quik et al 2015 photoreactions are modeled as described in section 6 4 for other reactions wasp8 uses a general formulation using the same general chemical reaction eqn 17 that is used for solute chemicals as is described in section 7 a general first order decay rate process is also available a nanoparticle can be set to transform into a daughter product which can be either a solute chemical or another nanoparticle with the yield of each given by a user defined yield coefficient 9 additional state variables in addition to the state variables and processes already discussed the wasp8 advanced toxicant module also has doc salinity temperature pathogens and mercury as available state variables in earlier versions of wasp doc was a segment specific parameter now wasp8 permits explicit simulation of 1 5 doc types which may oxidize or decay as first order processes and organic solids that may decay to form doc salinity temperature and pathogens were originally available in a separate wasp module heat with the option of setting the output from heat to be linked to the eutrophication or toxicant module mercury was previously available in wasp7 as an overlay using the early toxicant module kinetics module toxi with a separate mercury module merc merc simulated elemental mercury divalent mercury and methylmercury explicitly with specific transformation processes outlined for these species merc has been incorporated into the wasp8 advanced toxicant module with the additional state variable of recalcitrant divalent mercury so that historically contaminated mercury sites can explicitly simulate the difference between labile and recalcitrant mercury 10 example wasp8 model application hypothetical case study of the cape fear river the goal of this paper is to present the details on the advances implemented in wasp8 to provide details on a wasp8 model to demonstrate the new and updated capabilities an example wasp8 model was constructed using a hypothetical case study of the cape fear river north carolina nc us fig 5 first the model is developed to simulate flow and the simulated flow at the outflow is compared to observations next hypothetical simulation of solids concentrations is performed using realistic but arbitrary boundary concentrations the solids state variables are sand silt clay and particulate organic matter pom then a load of a solute chemical and then a nanomaterial are added representing a hypothetical release from a waste water treatment plant or industrial outflow the solute chemical transforms into a second state variable and the nanomaterial attaches to the different solids forming additional state variables i e nano silt nano clay nano pom additionally light intensity in the water column and doc are simulated explicitly physical chemical properties of solute and nanomaterial properties are presented in table 4 further details on solids solutes and nanomaterials are available in appendix tables a7 a9 the model domain consists of a 90 km stretch of the cape fear river downstream from usgs gage 02105500 to usgs gage 02105769 consisting of 45 water column segments each 2 km long and 92 m wide overlying 45 surface sediment segments 0 05 m depth and 45 subsurface sediment segments 0 15 m depth that is each 2 km reach of the river consists of three stacked layers water column surface sediment and subsurface sediment upstream of this section the cape fear is in the piedmont region of nc this section of the cape fear river is in the coastal plain region and is used as a source of drinking water at several locations and there are no substantial tributaries flow routing is simulated using the kinematic wave functionality and depth width velocity and volume are simulated per time step for each water column segment usgs 02105500 is used as the upstream inflow condition details on simulating flow for the cape fear river are presented in the appendix wasp8 has different methods for simulating flow in surface waters and for simulating simple hydrodynamics in 1 dimensional systems wasp8 itself is not a hydrodynamics model and relies on other more rigorous models for more complex hydrodynamics here a 1 dimensional kinematic wave is appropriate using the upstream boundary and the parameters we compared the outflow to the matching usgs gage 02105769 with an r2 0 92 and a nash sutcliffe efficiency 0 91 the simulated average travel time is 5 days with a range of 2 40 days further details on simulating flow are presented in the appendix further details on using wasp8 for stream routing are available in ambrose et al 2017 once stream flow is running solids and doc are simulated solids are simulated using two different approaches the descriptive option using constant settling and resuspension and the mechanistic option using dynamic settling deposition erosion and resuspension suspended solids have an upstream boundary condition of 6 mg l 1 silt 4 mg l 1 clay 2 mg l 1 pom and 8 mg l 1 doc initial concentrations of solids are zero in the water column and sand of 472 540 g m 3 silt of 250 030 g m 3 clay 356 500 g m 3 and pom 30 860 g m 3 in the sediment layers once the solids are running a load of 0 1 kg d 1 of the nanoparticle is added to the first segment the nanoparticle is set to attach to silt clay and pom particles via heteroaggregation kinetics forming new state variables nano silt nano clay and nano pom these particle attached nanoparticles then follow the processes of the solid to which they are attached the non attached nanoparticle photo degrades light is simulated using the option in which wasp8 calculates solar radiation based on the latitude and longitude of the cape fear location providing changes in surface light intensity with time of day and day of year next a load of 10 kg d 1 of chemical 1 is added to the first segment chemical 1 biodegrades into chemical 2 both solute chemicals volatilize attach to solids assuming equilibrium partitioning and complex with doc initial concentrations of solute chemicals nanoparticles and doc are zero everywhere the simulation was run for 15 years further details on parameterization are provided in the appendix the simulations presented here are not meant to provide a complete representation of all wasp8 capabilities but rather these example simulations are meant to provide details on some of the processes available in wasp8 and information on how one could construct a model depending on the questions to be answered and the goals to be met this example simulates four solid particle types where wasp7 could only simulate 3 nanoparticles are a completely new state variable which can form 3 other distinct other state variables as they attach to solid particles further the enhanced light and transformation routines are incorporated as part of its regular simulations wasp8 performs a mass check to confirm that mass balance is being upheld which wasp outputs live to the screen as it runs as well as part of its output files a perfect mass balance 1 for these simulations the mass check ranged from 0 999 to 1 0 10 1 solids the cape fear river wasp model was run using two different approaches for simulating solids a descriptive approach and a mechanistic approach as described in section 4 this is done to demonstrate the different results that one would expect with these different approaches the simulation results are presented in fig 6 due to the high settling velocity of sand there is no sand observed in the water column in the descriptive solids simulation in the water column silt clay and pom fluctuate as the flow and velocity of the segment fluctuate clay is the dominant solid type in the water column as it has the slowest settling velocity the solid concentrations in the surface sediment layer remains relatively constant with silt slightly increasing as it settles from the water column to the sediment layer the subsurface sediment layer remains relatively constant throughout the duration of the simulation for the mechanistic solids the water column concentrations of silt clay and pom vary over the course of the simulation generally the water column concentrations are lower than for the descriptive solids option reflecting a deposition dominated process where resuspension is small or negligible the spikes in water column solids concentration occur during times of high bottom shear stresses which result in erosional events in response to the increased solids concentrations in the water column the sediment layer loses mass of the finer particles of silt clay and pom wasp8 then moves solids from the deep sediments to replace the lost solids which results in enriching the surface sediment layer with sand this result is reflected by the jump in sand concentration during the largest erosional event this comparison of the simulated solids concentration trends using the descriptive and mechanistic approaches illustrates the fact that the mechanistic approach results in greater variation in suspended solids concentrations as a function of time 10 2 light fig 7 shows an example of light simulation in wasp8 the ten different bands of light attenuate differently through the water column the light intensity at noon is shown for all 10 wavelength bands for each day for the entire year for the length of the simulation the total light intensity changes over the course of the year highest in summer and lowest in winter the fraction of the total light intensity is demonstrated by the thickness of each band as the concentration of solids changes in the water column the amount of attenuation of each band of light changes the total photoreaction rate constant is the sum of the contributions due to each wavelength band the simulation is parameterized to have higher wavelength specific base rate constants for uvb with decreasing rate constants as the wavelength increases this translates to the pattern observed in fig 7 where the shorter wavelengths have a larger contribution to the total photoreaction rate constant even though they comprise a smaller fraction of the total light intensity for each time step and each location wasp is calculating the attenuation of light based on the concentrations of particles and doc and depth and then calculating the total reaction rate constant which is then applied to any state variable with a light driven process 10 3 nanoparticles fig 8 presents concentrations of the different phases of nanoparticles in two representative segments segment 20 and 40 wasp simulates concentrations in all segments for all time steps and these two segments are presented for illustration the descriptive and mechanistic solids options are both presented to provide an example for how these choices can affect the fate and transport of the nanoparticles in the water column the total nanoparticle concentrations vary inversely with flow as the load is held constant and flow varies over time the nanoparticle is present predominantly in the free non attached form free nano and concentrations decrease moving downstream as nanoparticles attach and settle to the sediments in the sediment layer the dominant form is the nanoparticle attached to clay nano clay followed by nanoparticle attached to silt nano silt with a small fraction attached to pom nano pom this is the case for both the descriptive solids and mechanistic solids case for the descriptive solids the concentrations in the sediment steadily accumulate over time in the mechanistic solids case there is less deposition to the sediment layer and thus the concentrations accumulate less halfway through the simulation there is an erosional event as seen in fig 6 halfway through the simulation there is a spike of solids in the water column and a jump in the surface sediments solid concentration at the same time the concentration of nano clay in the surface sediments decreases suddenly following this event the concentration of nano clay increases for the deep sediment layer the descriptive solids option shows an accumulation of nanoparticle concentrations increasing slower than in the surface layer for the mechanistic solids option there is minimal deposition from the surface sediments to the deep sediment layer in this example application therefore no nanoparticles enter this layer 10 4 solute chemicals fig 9 presents the concentrations of the two chemicals for the mechanistic solids option moving downstream in segment 10 20 30 and 40 these figures focus on the mechanistic solids to illustrate how the concentrations change moving along the length of the river in this scenario chemical 1 biodegrades into chemical 2 thus upstream the concentration of chemical 1 is larger than chemical 2 moving downstream chemical 2 increases in concentration as chemical 1 decreases the model is structured using equilibrium partitioning of the chemicals and the solid phases with chemical 1 being more hydrophilic than chemical 2 the difference in hydrophilicity is evidenced in that chemical 2 increases in concentration in the sediments moving downstream 11 conclusions the wasp8 advanced toxicant module has undergone a significant redesign and upgrade from previous wasp versions this upgrade includes the introduction of more state variables and different types of state variables particularly with the introduction of the nanoparticle state variable and the expansion of how light attenuation and photoreaction processes are handled with different options for simulating solids which are a critical aspect of toxicant fate and transport modeling over the years wasp has been increasingly used to investigate and address different contaminant fate and transport problems we hope that with these advances in the toxicant module that wasp8 will provide a modeling framework for a range of users that is flexible and rigorous enough to address the expanding range of problems posed by both historic and emerging contaminants disclaimer this paper has been reviewed in accordance with the u s environmental protection agency s peer and administrative review policies and approved for publication mention of trade names or commercial products does not constitute endorsement or recommendation for use the views expressed in this article are those of the authors and do not necessarily represent the views or policies of the us epa acknowledgements we would like to acknowledge the efforts of lourdes prieto for gathering the data necessary for constructing the cape fear river wasp model and the helpful review and comments by caroline stevens and craig barber as well as by an anonymous reviewer appendix supplementary data the following is the supplementary data to this article knightesetal2018 knightesetal2018 appendix supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 012 funding this work was funded through the us epa s office of research and development s chemical safety for sustainability research program this research did not receive any specific grant from funding agencies in the commercial or not for profit sectors 
