index,text
26170,one major challenge in applying crop simulation models at the regional or global scale is the lack of available global gridded soil profile data we developed a 10 km resolution global soil profile dataset at 2 m depth compatible with dssat using soilgrids1km several soil physical and chemical properties required by dssat were directly extracted from soilgrids1km pedo transfer functions were used to derive soil hydraulic properties other soil parameters not available from soilgrids1km were estimated from harvestchoice hc27 generic soil profiles the newly developed soil profile dataset was evaluated in different regions of the globe using independent soil databases from other sources in general we found that the derived soil properties matched well with data from other soil data sources an ex ante assessment for maize intensification in tanzania is provided to show the potential regional to global uses of the new gridded soil profile dataset keywords global gridded soil profile dataset crop simulation modeling dssat 1 introduction the needs for assessing impacts of global environmental change on agriculture and food security stimulated the use of process based crop models at the regional to global scales basso et al 2018 challinor et al 2010 jones et al 2017 nelson et al 2010 rabbinge and van diepen 2000 rosenzweig et al 2014 rosenzweig et al 2013 wu et al 2010 xiong et al 2008 crop models have been widely used also in developing early warnings and decision support systems asfaw et al 2018 gyawali et al 2018 despite the known issues of scaling up point scale processed based crop models onto larger spatial scales faivre et al 2004 hansen and jones 2000 they have been applied at larger scales beyond the spatial footprint which they were initially developed this is because crop models allow us to better understand the dynamic interactions between crops and the changing environment which statistical models are limited from doing applications of well validated process based crop models at the regional or global scale require a gridded framework elliott et al 2014 2015 folberth et al 2013 liu 2009 considering that a complex crop model developed at a field scale requires a considerable amount of data inputs including climate soil and crop management information a gridded crop modeling framework requires even more information with spatial consistency and it is inevitable to make assumptions at the aggregate scales among others soil data are particularly problematic because it is difficult and expensive to estimate representative soil properties at the aggregate scale through field surveys lagacherie et al 2000 while climate data have been increasingly available from various sources that include those from weather stations remote sensing and reanalysis data the availability of spatially contiguous gridded soil datasets in a crop model specific format is still a major challenge for gridded crop simulations wu et al 2010 there is a critical need to develop practical solutions to bridge this gap to date there have been several efforts in the development of global soil databases to name a few harmonized world soil database hwsd nachtergaele et al 2008 international soil reference and information centre isric world inventory of soil emission potentials wise derived soil properties batjes 2012 and recently isric world soil information service wosis ribeiro et al 2015 these soil databases were converted into three dimensional digital soil maps by interpolating field measurements with other environmental covariates hengl et al 2014 sanchez et al 2009 the resulting digital soil maps are published in a standard format grid based and at high resolution for example hengl et al 2017 and hengl et al 2014 developed global gridded soil maps at 250 m and 1 km resolution respectively these digital soil maps however cannot be readily used in crop models because most process based models require more soil information than what these soil databases provide and soil input must be tailored to with the model s specific format crop models in the decision support system for agro technology transfer dssat hoogenboom et al 2017 jones et al 2003 have been used in several global gridded crop model inter comparisons for assessing climate change risks in agriculture elliott et al 2014 2015 rosenzweig et al 2013 2014 dssat has been integrated in impact model for policy analysis by the international food policy research institute robinson et al 2015 dssat is also used by harvestchoice http harvestchoice org to simulate scenarios for improving livelihoods of smallholder farmers in sub saharan africa e g rosenzweig et al 2014 the main motivation of the present study was to develop soil inputs readily available for gridded dssat applications at a high resolution 5 arc minute 10 km which is a target resolution of harvestchoice s analysis system we envision that the new global soil profile dataset can be used by other gridded modeling systems thereafter note that previous works for gridded dssat applications were done at coarser resolutions based on isric wise database gijsman et al 2007 and romero et al 2012 generated a global point scale soil profile dataset wi sol compatible with dssat crop models wi sol has a very sparse soil profile data especially in the developing countries for example only one soil profile is available in ethiopia on the other hand isric s global soil information system called soilgrids provides gridded soil properties at various resolutions from 250 m to larger scales 1 km hengl et al 2017 soilgrids opened up an opportunity to develop a gridded soil input for dssat in this study we developed a 10 km resolution gridded global soil profile dataset compatible with dssat from soilgrids datasets datasets made in april 2015 at 1 km resolution we evaluated the newly developed gridded soil profile dataset using several soil data from independent sources then we introduced a maize intensification assessment study in tanzania to showcase the utility of the newly developed gridded soil profile dataset note that our target spatial resolution 10 km was determined to be seamlessly ingested with harvestchoice s on line database called cell5m ssa which contains hundreds of biophysical and socio economic indicators for sub saharan africa that enables analyses of spatial relationships between climate environment agriculture nutrition poverty and health harvestchoice and minnesota 2018 2 data 2 1 isric soilgrids recently the international community has paid an increasing attention to improving legacy soil data resources in support of sustainable development folberth et al 2016 montanarella and vargas 2012 sanchez et al 2009 in order to contribute to the global soil partnership initiative montanarella and vargas 2012 isric in collaboration with several international agencies has developed a gridded global soil database called soilgrids1km a global 3d soil information system at 30 arc second 1 km resolution hengl et al 2014 soilgrids1km provides representative soil properties at six standard depth intervals specified by the globalsoilmap net project 0 5 5 15 15 30 30 60 60 100 and 100 200 cm the soil properties include soil organic carbon g kg 1 soil ph sand silt and clay fractions cation exchange capacity cmol kg 1 bulk density kg m 3 coarse fragments soil taxonomy based on the world reference base classification system and usda soil taxonomy suborders after compiling multiple major international soil profile databases and selecting global environmental covariates reflecting soil formation factors hengl et al 2014 developed global spatial prediction models 2d or 3d regression and or regression kriging to derive these soil properties the soilgrids1km data used in this study was the data published in 2015 http soilgrids org a newer version of the dataset called soilgrids250 m has now been released since then hengl et al 2017 the 250 m product uses machine learning unlike the 1 km product which used regression kriging all of our discussions here however are all based on the 2015 soilgrids1km the soilgrids250 m data will be treated in a separate study 2 2 harvestchoice hc27 harvestchoice developed generic proto typical soil profiles called hc27 which classifies soil profiles by only three criteria soil texture organic carbon content as a proxy for soil fertility and soil rooting depth as a proxy for soil water availability as shown in table 1 harvestchoice 2010 koo and dimes 2010 hc27 aims to help overcome the limitation of traditional location specific soil database such as wise or hwsd when gridded simulations are needed and to make it possible to apply crop simulation models in larger areas where detailed soil information is not available particularly in africa hc27 provides 27 different soil profiles in a dssat compatible format in this study hc27 was used to derive soil characteristics not available from the soilgrids1km see section 3 3 methodology 3 1 estimating soil properties for dssat table 2 shows the soil physical and chemical properties required by dssat the soil data must be written in a specific format shown in fig a1 appendix if not available and not an essential soil property dssat will fill in a missing data with a default value during run time in this study soil horizons were patterned using the six standard layers of soilgrids1km although soilgrids1km provides some of the essential soil properties for dssat e g bulk density sbdm organic carbon sloc and fraction of silt slsi and clay slcl the remaining parameters in table 2 must be estimated the third column in table 2 shows how we estimated each soil property general workflow for processing soilgirds1km and deriving soil properties for dssat input file sol is shown in fig 1 the parameters in the first 10 rows of table 2 from scom to smke inherit their values from corresponding hc27 soil profiles determined by soil properties from soilgrids1km detailed procedures for deriving values of other soil properties are described in the following sections more technical details on data processing and formatting can be found in the appendix and han et al 2015 3 1 1 estimation of the soil hydraulic properties soil hydraulic properties are important soil data inputs needed to run any crop models the soil hydraulic properties determine its capacity to hold and release water needed in the calculation of the soil water balance in order to simulate soil water movement dssat requires minimum inputs to characterize soil hydraulic properties e g soil water content at field capacity sdul at wilting point slll and saturation ssat gijsman et al 2002 these soil properties can be measured in the laboratory and the field but it is practically impossible to measure them at the aggregate scale gijsman et al 2002 at the aggregate scale indirect ways of estimating these properties are essential many efforts have been done to derive hydraulic soil properties using more easy to measure properties such as texture or proportion of sand silt and clay several pedo transfer functions ptfs have been developed to relate soil hydraulic properties and other soil physical and chemical properties rawls et al 1991 and timlin et al 1996 reviewed more than 49 ptfs there are several approaches used for developing ptfs including multiple regression physico empirical models and estimating parameters of equations to relate soil water contents with soil water potentials gijsman et al 2002 some studies showed that their ptfs performed well e g wösten et al 2013 however gijsman et al 2002 noted that it is difficult to recommend a certain well performing ptfs due to big discrepancies between methods in estimating water retention parameters nonetheless they found that the ptfs of saxton et al 1986 performed better compared with other 7 ptfs they examined in terms of estimating field capacity wilting point and available water holding capacity for certain soil types in the u s in addition romero et al 2012 adapted the ptf approach by saxton et al 1986 to estimate soil hydraulic properties for the reanalysis of isric wise soil database and for the development of dssat compatible soil input file wi sol however very sandy or very clayey soils were excluded from the method of saxton et al 1986 and rawls et al 1982 in this study we used the ptfs in saxton and rawls 2006 which updated the equations of saxton et al 1986 based on the usda soil database such that soil hydraulic properties can be estimated using texture and organic matter content table 3 soil texture can be directly obtained from soilgrids1km organic carbon from soilgrids1km was converted to organic matter to be ingested in the equations as shown in table 3 we multiplied a factor of 2 to the organic carbon to estimate the organic matter content pribyl 2010 concluded that conversion factor 2 is more accurate than the conventional value of 1 724 to convert soil organic carbon to soil organic matter even though we selected the best available ptfs for this study one should be aware about the limitations of ptfs saxton and rawls 2006 developed their equations based on usda soil database and there may be regions in the world where the equations will not perform well most studies evaluated certain ptfs by comparing results with field or laboratory measured soil water retention data however wilting point or field capacity measured from laboratory may not be always applicable at the field or larger scales gijsman et al 2002 once these fractions of sand and clay and organic matter content are determined the soil water content at wilting point slll field capacity sdul and saturation ssat and saturated hydraulic conductivity ssks can be computed using the ptfs in table 3 at the soilgrids1km resolution 3 1 2 estimation of the soil root growth factor soil root growth factor srgf is an important parameter in simulating crop growth because it affects the potential rooting depth and root density of plants which determines plants water uptake and eventually biomass accumulation in spite of its importance there is no concrete guideline to estimating srgf ma et al 2009 because it is an empirical parameter srgf was often calibrated arbitrarily to improve the predicted soil water content or crop yield calmon et al 1999 fang et al 2008 leenaars et al 2018 derived spatially coherent rootable depth in sub saharan africa for maize by parameterizing several soil factors with a rootability index they also argued that the absence of observed rootability or rootable depth data hindered a quantitative validation of the rootable depths map in this study srgf is estimated based on the available water storage capacity awc and the corresponding soil profile in hc27 awc is defined as the difference between water content at field capacity and wilting point as follows 1 awc 1000 sdul slll zr where awc is total available soil water in the root zone mm sdul the water content at field capacity m3 m 3 slll the water content at wilting point m3 m 3 zr the rooting depth m the fao s hwsd classified the soil units based on awc in 1 m soil depth as shown in table 4 nachtergaele et al 2008 therefore once a soil texture is known the awc mm m 1 of the soil can be a good indicator of a possible soil rooting depth e g deep or shallow harvestchoice used the awc information to determine a soil as deep medium or shallow as shown in table 4 we also adapted a similar approach to determine a soil rootable depth from awc and soil texture from table 4 once the soil depth soil fertility and texture are determined for a target soilgrids1km grid a corresponding hc27 soil profile can be selected then srgf of the selected hc27 soil type is assigned as the srgf of that target pixel 4 assign srgf for each soil layer from the corresponding hc27 soil profile since soil depth specifications in hc27 are 0 10 10 30 30 60 60 90 90 120 120 150 150 180 cm as opposed to 0 5 5 15 15 30 30 60 60 100 and 100 200 cm of soilgrids1km different definitions of soil depths for shallow medium deep and weighted averages of srgfs are allocated to the standard soilgrids1km layers as shown in table 5 3 2 evaluation of the gridded soil profile dataset at 10 km scale evaluating the accuracy and quality of the gridded soil dataset at 5 arc minute resolution 10 km as introduced in this study is not straightforward the new gridded soil profile dataset contains soil parameters derived from multiple sources uncertainties of the parameters can come from many factors e g measurement errors incurred in the original survey data by local agencies standardization harmonization process by compiling all available measurements and spatial predictions by statistical models and aggregation to target resolutions heuvelink 2014 issues with accuracy assessment of digital soil mapping have been addressed in several studies bishop et al 2015 heuvelink 2007 leopold et al 2006 typical approach to evaluate the quality of model predicted soil parameters is to compare them with independent point observations however as heuvelink 2007 and leopold et al 2006 argued that differences in measurement supports between point observations and model predictions at larger scales should be taken into account bishop et al 2015 used three different spatial supports point 48 m blocks and soil land use complexes in order to evaluate model predicted clay content maps and concluded that validating digital soil maps at the point support is likely to be the worst case they argued that various spatial supports ranging from a point to larger block supports are required for better validation of digital soil maps in this study the target resolution of the soil dataset is 10 km it is difficult to have an independent data at a similar scale support for evaluating the output soil dataset for the world the best possible database of global coverage is the isric wosis ribeiro et al 2015 however wosis comprises point profile observations thus not free from the issue of scale support discrepancy with our target resolution moreover few point observations that fall within the target grid 10 km is not enough to aggregate to make an average representation of the target resolution despite all of these we took wosis as one of the validation data and compared our derived soil hydraulic properties at 10 km resolution the effect of different scales will be discussed in section 4 due to the scale issue described above the evaluation of the new soil profile dataset relied only on visual comparisons among other datasets in different regions of the globe the derived soil hydraulic properties were evaluated using similar maps from literature e g leenaars et al 2018 other soil database i e ssurgo of the u s and crop land distributions fritz et al 2015 regardless of these efforts it must be reiterated that there is no true value to quantitatively assess the quality of the derived soil profile dataset we can only speculate based on logic furthermore a crop simulation study is presented in section 4 2 which showed the application of the output soil profile dataset in modeling crop yield responses over tanzania it can be considered an indirect way of evaluating the quality of our output soil dataset gijsman et al 2007 and romero et al 2012 tested the sensitivities of the soil parameters they developed using crop simulation results without quantitative comparisons from other soil data sources 4 results the final 10 km resolution gridded soil profile dataset formatted in dssat soil input file sol were created separately for each 225 countries with its unique iso a2 as a filename e g ke sol for kenya the dataset is accessible and freely available from https doi org 10 7910 dvn 1peey0 iri et al 2018 the generated soil profile dataset and some applications are described below here we only show some representative soil parameters as a way of evaluating the quality of the new model friendly soil profile dataset 4 1 derived soil hydraulic properties 4 1 1 soil water contents at wilting point and field capacity maps of soil water content at wilting point slll at 10 km resolution are shown in figs 2 and 3 fig 2 shows a depth aggregated slll for the top 15 cm depth i e weighted average of the first two layers for the world in order to evaluate the quality of the estimated slll we compared the derived soil property for africa with a permanent wilting point map from afsis gyga functional soil information for sub saharan africa rz pawhc ssa fig 3 the afsis gyga data was introduced by leenaars et al 2018 and is available from www isric org we used a map moisture content volumetric of the soil fine earth at permanent wilting point defined at pf 4 2 and aggregated over the top 30 cm of the soil the slll in fig 3b by leenaars et al 2018 was created based on afsoilgrids250 m hengl et al 2015 and different pedo transfer functions they used pedo transfer functions particularly developed for tropical soils that need input data e g bulk density cation exchange capacity and ph besides the other soil properties that we used in this study i e sand silt clay and organic carbon content in general the sllls from afsis gyga are slightly higher than what we produced this may be due to the different input soil database at different resolution afsoilgrids250 m vs soilgrids1km and different pedo transfer functions used however a permanent wilting point of 0 44 cm3 cm 3 in ethiopia fig 3b seems to be too high in reality from afsys gyga overall the spatial distributions of sllls are comparable fig 3 the soil water contents at wilting point slll and field capacity sdul of the new soil profile dataset this study were compared with isric wosis database isric wosis contains standardized soil profile data collected by many international soil data providers for a fair comparison we filtered out data from isric wosis to remove soil properties of depths deeper than 15 cm the locations of wosis slll and sdul soil profiles can be found in fig a3 appendix a total of 8643 sdul values were compared but 96 of them were from the continental u s in the case of slll however data from the u s only takes 6 2 of total 1662 slll values in the comparison sdul data greater than 0 95 cm3cm 3 were excluded from wosis note that there are scale discrepancies between the wosis data and our derived slll and sdul see section 3 2 instead of calculating error statistics we compared the kernel density plots of the two soil parameters slll and sdul from wosis and the parameters we produced from this study fig 4 the slll values from wosis are more positively skewed and have a lesser mean compared with the sllls we derived the distributions of both slll and sdul produced in this study are narrower than the ones from wosis this could be due to the smoothing effect when values are aggregated at 10 km scale the pedo transfer functions applied in this study may have also partially contributed to the narrower distribution however considering that crop simulation models will be highly likely applied to areas suitable for agriculture the sdul or slll values in extreme ranges too low or high such as sdul greater than 0 6 cm3cm 3 are not appropriate to be used for evaluation in general the slll and sdul values we derived from this study are within reasonable ranges for agricultural soils 4 1 2 available water content fig 5 shows the available water content awc of the first soil layer 0 5 cm at 10 km resolution as expected derived awc is very low in areas where the world s major deserts are located e g sahara kalahari and namib in africa pantagonia in south america western australia and northern india derived awc with a maximum value of 0 19 cm3 cm 3 seems to be low compared with laboratory measurements which could be due to the smoothing effect of spatial aggregation awc of the 2nd soil layer 5 15 cm showed a similar spatial distribution as in fig 5 not shown due to the difficulties of collecting global awc maps we relied on well documented soil database ssurgo of the u s for additional evaluation of the derived soil hydraulic properties the gridded ssurgo gssurgo soil survey staff 2014 provides a wide range of soil information including available water storage mm of top 25 cm of soil depth an example in california usa the awc values derived from soilgrids1km sdul minus slll at 10 km resolution in this study fig 6 b were compared with awcs from ssurgo in fig 6a and weighted averages of awcs at an aggregated 10 km resolution from soilgrids250 m hengl et al 2017 in fig 6c note that awcs in fig 6b and c represents value at 15 cm soil depth through weighted averages of two standard soil layers 0 5 and 5 10 cm while the ssurgo awc values in fig 6a represent 25 cm of soil depth awcs from soilgrid1km have a narrower range of distribution i e maximum value of 13 and minimum value of 7 compared to ssurgo this narrow range may be attributed to the narrower range of sdul distribution as shown in section 4 1 1 in comparison with wosis other reasons could be the aggregation effect and the ptfs applied gijsman et al 2002 compared 8 different methods to derive soil hydraulic properties and demonstrated that the estimated awc values vary significantly not only between methods and soil types but just as well within soil types therefore it is recommended to test sensitivities of the selected ptfs in future studies despite of the narrower ranges in the magnitudes of awcs the awcs derived from this study show a very similar distribution with ssurgo i e major agricultural areas along the great valley have relatively higher awc values while forest areas along the sierra nevada have the lower values interestingly awc map from soilgrids250 m shows an opposite distribution between ssurgo and this study as an indirect evaluation of the derived awc from this study a cropland map was compared with the derived awc map of africa we used the most recently released global cropland percentage map developed by the international institute for applied systems analysis iiasa international food policy research institute ifpri fritz et al 2015 fig 7 suggests that many cropland areas in africa are consistent with areas of relatively higher awc values fig 7b see red circles however there are also some areas that do not match as indicated in black circles fig 7b as shown in section 4 1 1 we also compared the derived awc with another derived awc map from afsis gyga leenaars et al 2018 the afsis gyga s awc map see fig 6b of leenaars et al 2018 shows a somewhat different spatial distribution from our results in fig 7 specifically for relatively higher awc regions throughout the savanna zone in africa and very low awc in the gezira sudan not shown here see han et al 2015 leenaars et al 2018 attributed the strange patterns in their results to the limited availability of bulk density data 4 2 case study ex ante assessment of maize yield response potential to fertilizer hybrid seeds and improved agronomy this section shows an example of how the new gridded soil profile database can be used for real world applications southern highlands area in tanzania is often referred to as one of the breadbasket in the region yet smallholder farmers productivity is still very low latest national statistics data from faostat shows the national average yield of maize as 1 3 t ha 1 in 2013 with annual growth rate of 2 71 to assess how much maize productivity increases can be achieved by future investment on intensification options a grid based crop modeling framework was developed using the soilgrids1km based soil profiles and used for each 10 km grid cell in the area from the calibrated baseline yield levels with subsistence farming system where farmers use traditional variety with no fertilizer application with inadequate agronomic information three step wise strategies were simulated 1 increased nitrogen n fertilizer application 2 improved variety and 3 optimum planting window specifically the ex ante study aimed to identify the feasibility of achieving the target yields of 3 t ha 1 from the simulated intensification strategies simulation results showed overall that the target yield level could be readily achieved through the intensifications with agronomic interventions e g optimum planting density and planting window spatial variability of yield response potentials were demonstrated across the region in fig 8 5 discussion this study presents a new global gridded soil profile dataset at a 10 km resolution developed for dssat despite recent advances in digital soil mapping there is still a gap in utilizing the available soil databases for crop models and eventually for practical decision making by policy makers for food security under a changing climate and environment the gap is mainly because of the models required soil inputs that are not provided by typical soil survey data especially for gridded crop model applications availability of soil input data at the right format and resolution has been one of the big challenges in this study we bridge this gap by developing a 10 km global gridded soil profile dataset by translating a high resolution gridded soil database soilgrids1km into model ready formats and specificity this work aligns with carré et al 2007 and mcbratney et al 2012 that asserted the concept of digital soil assessments beyond digital soil mapping linking crop models with soil inputs is a critical step in assessing biomass production e g crop yield as one of the important soil functions as well as bringing the value of digital soil maps to benefit society the soil profile dataset developed in this study has been used operationally since its first release in 2015 andreadis et al 2017 developed the regional hydrologic extremes assessment system rheas and coupled it with dssat to monitor and predict drought and maize productivity in east africa rheas used the gridded soil profile data from https doi org 10 7910 dvn 1peey0 for regional dssat modeling komarek et al 2018 applied a gridded dssat using the soil profiles of han et al 2015 to simulate and analyze economic effects of different seed cultivars and fertilizer intensification in tanzania the developed soil profile dataset as it is now has a great potential to be used for developing agriculture related decision support tools or any analysis framework at regional or global scale for researchers or policy makers the methods described in this study and han et al 2015 can be used as precedence to developing other gridded soil datasets for other crop models like agricultural production systems simulator apsim or environmental policy integrated climate epic model in this study we applied the pedo transfer function of saxton and rawls 2006 to derive the required soil hydraulic properties for dssat however there are other pedo transfer functions that can be used for this purpose likewise there are several ways to estimate soil properties that are not available from soilgrids1km to complete the model friendly soil profile dataset although here we used the generic soil profiles in hc27 these are potential sources of uncertainties in the dataset further work is needed to quantify uncertainties and sensitivities of the applied methods to the final product uncertainty quantification has received a great attention particularly by the globalsoilmap community heuvelink 2014 because of its importance to the end users as well as to the map makers uncertainties of the derived soil profile dataset can come from different sources moreover additional uncertainties can come also from subsequent processes when applying pedo transfer functions and when aggregating the data from 1 km to 10 km resolution han et al 2015 earlier quantified uncertainties of derived soil hydraulic properties based on the assumptions of independence from each pixel or normality of soil properties following heuvelink 1999 and hengl et al 2014 assumptions like this may not be always valid in realistic more sophisticated approach for quantifying uncertainty can be employed e g statistical approaches to quantify probability distributions of all possible sources of uncertainties heuvelink 2014 this statistical approach however can be computationally expensive and requires sufficiently large sample size to characterize uncertainties thus warrants as a separate study the product of this study has a final resolution of 10 km derived from a 1 km resolution dataset the question of how to aggregate data from 1 km to 10 km arises this is related to defining the support of the soil properties an approach to finding a representative soil property at a 10 km resolution is to select a dominant soil pixel profile in the target 10 km grid as wu et al 2010 however selecting a dominant soil type is not always the best approach heuvelink 2007 and lagacherie et al 2000 argued that selecting a representative profile of a dominant mapping unit can lead to neglect a substantial part of the mapping unit in addition soil covariates used for interpolating point measurements are not always from a dominant soil profile hengl et al 2017 applied a digital elevation model and a wide range of remote sensing data e g modis enhanced vegetation index as soil covariates to creating soil database at 250 m resolution these gridded soil covariates are representative values and not necessarily from a dominant soil type of the pixel in this study we employed an arithmetic averaging of soil properties to aggregate 1 km resolution data to a 10 km resolution the reasoning behind using the simple average is that we assumed that our target pixel is a homogeneous unit having a hypothetical representative soil property which may be neither a simple average of a few ground measured values nor values from one dominant soil profile when we apply dssat to a grid of a 10 km resolution this is similar to the approach of mohanty and zhu 2007 regarding the issue of defining a representative value for a grid cell shangguan et al 2014 addressed the problem of finding one to one relation between a grid cell and soil mapping units in reality there are one to n relations between a soil mapping unit or soil map polygon or a grid they tested three methods area weighted dominant soil type and dominant binned soil attribute method but offered results of the areas weighted method which is similar to our simple averaging of 1 km pixel values simple averaging approach has been used commonly in remote sensing community but not in soil science community further investigation is needed to better understand the effects of different aggregation schemes 6 conclusions this study developed a set of dssat compatible global soil profiles at 10 km grid based from a recently released soil database soilgrids1km based on the soil properties from soilgrids1km soil hydraulic properties were derived using pedo transfer functions other required variables were derived from harvestchoice s hc27 dataset the final product is provided for each country in a sol file which is a standard format of the dssat soil input since it is not possible to have a reference data close to true values representing a 10 km resolution our evaluation of the output soil dataset had to rely on more indirect methods qualitative approach rather than quantitative visual inspections were conducted in comparison with i other maps i e iiasa ifpri cropland map and afsis gyga s wilting point and available water content maps and ii ssurgo soil database for california in the u s in general distributions of the soil properties areas with relatively higher or lower values matched well with the data in comparison when the output was compared with wosis soil database which contains global harmonized soil profiles smoothing effect due to the aggregation was observed as an application the soil profile datasets developed in this study was tested in an ex ante modeling study to assess the potential of agricultural investments in tanzania s southern highlands area uncertainties incurred from the input soil database and during the data processing should be quantified to better assess the quality of the final product future work will be done on quantifying the impacts of selected pedo transfer functions type of input soil database soilgrids250 m rather than soilgrids1km and sensitivities of the estimated soil parameters on the crop yield predictions the soil profile dataset developed in this study will contribute to the advances of gridded crop modeling applications at regional or global scale the approach used in this study can be easily applied for other soil databases e g soilgrids250 m for gridded application of crop models at higher resolution data availability name of data set global high resolution soil profile database for crop modeling applications developers international research institute for climate and society columbia university ny 10964 usa michigan state university east lansing mi usa harvestchoice international food policy research institute ifpri washington d c usa contacts eunjin han international research institute for climate and society columbia university ny 10964 usa email eunjin iri columbia edu amor vm ines michigan state university mi usa email inesamor msu edu jawoo koo international food policy research institute ifpri washington d c usa email j koo cgiar org year first available 2015 availability the soil dataset product is available at https doi org 10 7910 dvn 1peey0 under a creative commons cc by nc attribution non commercial 4 0 license via dataverse fortran codes for deriving soil parameters are also freely available via github https github com agro climate global gridded soil data for dssat at 10km acknowledgement we acknowledge bill and melinda gates foundation for funding this research project through international food policy research institute ifpri harvestchoice project phase ii oppgd1450 we also acknowledge funding from nasa servir nnn13d788t and nnh15zda001n servir 15 servir15 2 0089 we thank the editor and reviewers for helping us improve the quality of the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 012 appendix a 1 data processing for output format dssat compatible soil profile data sol were saved separately for each country the file names were derived from two letter standard country code iso a2 of iso international organization for standardization for example ke sol file contains all available soil profiles at 10 km resolution in kenya two different country codes iso a2 and iso a3 three letter code were also written in each profile as shown in the line 1 ken and 3 ke in fig a1 complete list of iso country code and corresponding country boundary map was obtained from natural earth http www naturalearthdata com figure a 1 example sol output for kenya figure a 1 soil profile id e g ke04430591 in fig a1 is determined from harvestchoice s standard grid cell id at 10 km spatial resolution cell5m harvestchoice and minnesota 2018 fig a2 shows a sample of cell5m grid cell ids across kenya as an example naming soil profile id based on cell5m ids is intended to streamline grid based crop modeling operation across large areas in a batch within the harvestchoice s modeling framework creating country mask at 10 km resolution is not easy due to mismatches between curved country boundaries or coastal lines and 10 km grids as shown in fig a2 we used ifpri s concordance look up table to assign a country id for each 10 km grid harvestchoice and minnesota 2018 figure a 2 harvestchoice s standard grid cell id cell5m and corresponding country masks over kenya ethiopia and somalia figure a 2 soil texture classification in the first line of sol e g sandclayl in fig a1 is determined based on the proportion of sand and clay in a given grid using usda scs soil triangle gerakis and baer 1999 due to limitation in space the soil texture classification was truncated to shorter names when needed see table a1 table a 1 short names for soil texture classifications table a 1 soil texture classification short name silt loam siltloam sand sand silty clay loam siltclayl loam loam clay loam clayloam sandy loam sandyloam silty clay siltyclay sandy clay loam sandclayl loamy sand loamysand clay clay silt silt sandy clay sandyclay except for four soil hydraulic properties slll sdul ssat ssks derived from ptfs other unknown soil properties are extracted from hc27 as described in table 2 based on the given classified soil texture root depth and fertility organic carbon one soil profile out of 27 profiles was selected the only exception is for slmh master horizon as it was pre determined for each standardized soil layers so that it can be similar with hc27 for the first two layers a is assigned and ab ba b bc are assigned to the rest 3rd to 6th layers respectively figure a 3 locations of wosis soil profiles which have soil water content at wilting point top and at field capacity bottom in soil depth up to 15 cm from the july 2016 wosis data downloaded in august 2018 figure a 3 
26170,one major challenge in applying crop simulation models at the regional or global scale is the lack of available global gridded soil profile data we developed a 10 km resolution global soil profile dataset at 2 m depth compatible with dssat using soilgrids1km several soil physical and chemical properties required by dssat were directly extracted from soilgrids1km pedo transfer functions were used to derive soil hydraulic properties other soil parameters not available from soilgrids1km were estimated from harvestchoice hc27 generic soil profiles the newly developed soil profile dataset was evaluated in different regions of the globe using independent soil databases from other sources in general we found that the derived soil properties matched well with data from other soil data sources an ex ante assessment for maize intensification in tanzania is provided to show the potential regional to global uses of the new gridded soil profile dataset keywords global gridded soil profile dataset crop simulation modeling dssat 1 introduction the needs for assessing impacts of global environmental change on agriculture and food security stimulated the use of process based crop models at the regional to global scales basso et al 2018 challinor et al 2010 jones et al 2017 nelson et al 2010 rabbinge and van diepen 2000 rosenzweig et al 2014 rosenzweig et al 2013 wu et al 2010 xiong et al 2008 crop models have been widely used also in developing early warnings and decision support systems asfaw et al 2018 gyawali et al 2018 despite the known issues of scaling up point scale processed based crop models onto larger spatial scales faivre et al 2004 hansen and jones 2000 they have been applied at larger scales beyond the spatial footprint which they were initially developed this is because crop models allow us to better understand the dynamic interactions between crops and the changing environment which statistical models are limited from doing applications of well validated process based crop models at the regional or global scale require a gridded framework elliott et al 2014 2015 folberth et al 2013 liu 2009 considering that a complex crop model developed at a field scale requires a considerable amount of data inputs including climate soil and crop management information a gridded crop modeling framework requires even more information with spatial consistency and it is inevitable to make assumptions at the aggregate scales among others soil data are particularly problematic because it is difficult and expensive to estimate representative soil properties at the aggregate scale through field surveys lagacherie et al 2000 while climate data have been increasingly available from various sources that include those from weather stations remote sensing and reanalysis data the availability of spatially contiguous gridded soil datasets in a crop model specific format is still a major challenge for gridded crop simulations wu et al 2010 there is a critical need to develop practical solutions to bridge this gap to date there have been several efforts in the development of global soil databases to name a few harmonized world soil database hwsd nachtergaele et al 2008 international soil reference and information centre isric world inventory of soil emission potentials wise derived soil properties batjes 2012 and recently isric world soil information service wosis ribeiro et al 2015 these soil databases were converted into three dimensional digital soil maps by interpolating field measurements with other environmental covariates hengl et al 2014 sanchez et al 2009 the resulting digital soil maps are published in a standard format grid based and at high resolution for example hengl et al 2017 and hengl et al 2014 developed global gridded soil maps at 250 m and 1 km resolution respectively these digital soil maps however cannot be readily used in crop models because most process based models require more soil information than what these soil databases provide and soil input must be tailored to with the model s specific format crop models in the decision support system for agro technology transfer dssat hoogenboom et al 2017 jones et al 2003 have been used in several global gridded crop model inter comparisons for assessing climate change risks in agriculture elliott et al 2014 2015 rosenzweig et al 2013 2014 dssat has been integrated in impact model for policy analysis by the international food policy research institute robinson et al 2015 dssat is also used by harvestchoice http harvestchoice org to simulate scenarios for improving livelihoods of smallholder farmers in sub saharan africa e g rosenzweig et al 2014 the main motivation of the present study was to develop soil inputs readily available for gridded dssat applications at a high resolution 5 arc minute 10 km which is a target resolution of harvestchoice s analysis system we envision that the new global soil profile dataset can be used by other gridded modeling systems thereafter note that previous works for gridded dssat applications were done at coarser resolutions based on isric wise database gijsman et al 2007 and romero et al 2012 generated a global point scale soil profile dataset wi sol compatible with dssat crop models wi sol has a very sparse soil profile data especially in the developing countries for example only one soil profile is available in ethiopia on the other hand isric s global soil information system called soilgrids provides gridded soil properties at various resolutions from 250 m to larger scales 1 km hengl et al 2017 soilgrids opened up an opportunity to develop a gridded soil input for dssat in this study we developed a 10 km resolution gridded global soil profile dataset compatible with dssat from soilgrids datasets datasets made in april 2015 at 1 km resolution we evaluated the newly developed gridded soil profile dataset using several soil data from independent sources then we introduced a maize intensification assessment study in tanzania to showcase the utility of the newly developed gridded soil profile dataset note that our target spatial resolution 10 km was determined to be seamlessly ingested with harvestchoice s on line database called cell5m ssa which contains hundreds of biophysical and socio economic indicators for sub saharan africa that enables analyses of spatial relationships between climate environment agriculture nutrition poverty and health harvestchoice and minnesota 2018 2 data 2 1 isric soilgrids recently the international community has paid an increasing attention to improving legacy soil data resources in support of sustainable development folberth et al 2016 montanarella and vargas 2012 sanchez et al 2009 in order to contribute to the global soil partnership initiative montanarella and vargas 2012 isric in collaboration with several international agencies has developed a gridded global soil database called soilgrids1km a global 3d soil information system at 30 arc second 1 km resolution hengl et al 2014 soilgrids1km provides representative soil properties at six standard depth intervals specified by the globalsoilmap net project 0 5 5 15 15 30 30 60 60 100 and 100 200 cm the soil properties include soil organic carbon g kg 1 soil ph sand silt and clay fractions cation exchange capacity cmol kg 1 bulk density kg m 3 coarse fragments soil taxonomy based on the world reference base classification system and usda soil taxonomy suborders after compiling multiple major international soil profile databases and selecting global environmental covariates reflecting soil formation factors hengl et al 2014 developed global spatial prediction models 2d or 3d regression and or regression kriging to derive these soil properties the soilgrids1km data used in this study was the data published in 2015 http soilgrids org a newer version of the dataset called soilgrids250 m has now been released since then hengl et al 2017 the 250 m product uses machine learning unlike the 1 km product which used regression kriging all of our discussions here however are all based on the 2015 soilgrids1km the soilgrids250 m data will be treated in a separate study 2 2 harvestchoice hc27 harvestchoice developed generic proto typical soil profiles called hc27 which classifies soil profiles by only three criteria soil texture organic carbon content as a proxy for soil fertility and soil rooting depth as a proxy for soil water availability as shown in table 1 harvestchoice 2010 koo and dimes 2010 hc27 aims to help overcome the limitation of traditional location specific soil database such as wise or hwsd when gridded simulations are needed and to make it possible to apply crop simulation models in larger areas where detailed soil information is not available particularly in africa hc27 provides 27 different soil profiles in a dssat compatible format in this study hc27 was used to derive soil characteristics not available from the soilgrids1km see section 3 3 methodology 3 1 estimating soil properties for dssat table 2 shows the soil physical and chemical properties required by dssat the soil data must be written in a specific format shown in fig a1 appendix if not available and not an essential soil property dssat will fill in a missing data with a default value during run time in this study soil horizons were patterned using the six standard layers of soilgrids1km although soilgrids1km provides some of the essential soil properties for dssat e g bulk density sbdm organic carbon sloc and fraction of silt slsi and clay slcl the remaining parameters in table 2 must be estimated the third column in table 2 shows how we estimated each soil property general workflow for processing soilgirds1km and deriving soil properties for dssat input file sol is shown in fig 1 the parameters in the first 10 rows of table 2 from scom to smke inherit their values from corresponding hc27 soil profiles determined by soil properties from soilgrids1km detailed procedures for deriving values of other soil properties are described in the following sections more technical details on data processing and formatting can be found in the appendix and han et al 2015 3 1 1 estimation of the soil hydraulic properties soil hydraulic properties are important soil data inputs needed to run any crop models the soil hydraulic properties determine its capacity to hold and release water needed in the calculation of the soil water balance in order to simulate soil water movement dssat requires minimum inputs to characterize soil hydraulic properties e g soil water content at field capacity sdul at wilting point slll and saturation ssat gijsman et al 2002 these soil properties can be measured in the laboratory and the field but it is practically impossible to measure them at the aggregate scale gijsman et al 2002 at the aggregate scale indirect ways of estimating these properties are essential many efforts have been done to derive hydraulic soil properties using more easy to measure properties such as texture or proportion of sand silt and clay several pedo transfer functions ptfs have been developed to relate soil hydraulic properties and other soil physical and chemical properties rawls et al 1991 and timlin et al 1996 reviewed more than 49 ptfs there are several approaches used for developing ptfs including multiple regression physico empirical models and estimating parameters of equations to relate soil water contents with soil water potentials gijsman et al 2002 some studies showed that their ptfs performed well e g wösten et al 2013 however gijsman et al 2002 noted that it is difficult to recommend a certain well performing ptfs due to big discrepancies between methods in estimating water retention parameters nonetheless they found that the ptfs of saxton et al 1986 performed better compared with other 7 ptfs they examined in terms of estimating field capacity wilting point and available water holding capacity for certain soil types in the u s in addition romero et al 2012 adapted the ptf approach by saxton et al 1986 to estimate soil hydraulic properties for the reanalysis of isric wise soil database and for the development of dssat compatible soil input file wi sol however very sandy or very clayey soils were excluded from the method of saxton et al 1986 and rawls et al 1982 in this study we used the ptfs in saxton and rawls 2006 which updated the equations of saxton et al 1986 based on the usda soil database such that soil hydraulic properties can be estimated using texture and organic matter content table 3 soil texture can be directly obtained from soilgrids1km organic carbon from soilgrids1km was converted to organic matter to be ingested in the equations as shown in table 3 we multiplied a factor of 2 to the organic carbon to estimate the organic matter content pribyl 2010 concluded that conversion factor 2 is more accurate than the conventional value of 1 724 to convert soil organic carbon to soil organic matter even though we selected the best available ptfs for this study one should be aware about the limitations of ptfs saxton and rawls 2006 developed their equations based on usda soil database and there may be regions in the world where the equations will not perform well most studies evaluated certain ptfs by comparing results with field or laboratory measured soil water retention data however wilting point or field capacity measured from laboratory may not be always applicable at the field or larger scales gijsman et al 2002 once these fractions of sand and clay and organic matter content are determined the soil water content at wilting point slll field capacity sdul and saturation ssat and saturated hydraulic conductivity ssks can be computed using the ptfs in table 3 at the soilgrids1km resolution 3 1 2 estimation of the soil root growth factor soil root growth factor srgf is an important parameter in simulating crop growth because it affects the potential rooting depth and root density of plants which determines plants water uptake and eventually biomass accumulation in spite of its importance there is no concrete guideline to estimating srgf ma et al 2009 because it is an empirical parameter srgf was often calibrated arbitrarily to improve the predicted soil water content or crop yield calmon et al 1999 fang et al 2008 leenaars et al 2018 derived spatially coherent rootable depth in sub saharan africa for maize by parameterizing several soil factors with a rootability index they also argued that the absence of observed rootability or rootable depth data hindered a quantitative validation of the rootable depths map in this study srgf is estimated based on the available water storage capacity awc and the corresponding soil profile in hc27 awc is defined as the difference between water content at field capacity and wilting point as follows 1 awc 1000 sdul slll zr where awc is total available soil water in the root zone mm sdul the water content at field capacity m3 m 3 slll the water content at wilting point m3 m 3 zr the rooting depth m the fao s hwsd classified the soil units based on awc in 1 m soil depth as shown in table 4 nachtergaele et al 2008 therefore once a soil texture is known the awc mm m 1 of the soil can be a good indicator of a possible soil rooting depth e g deep or shallow harvestchoice used the awc information to determine a soil as deep medium or shallow as shown in table 4 we also adapted a similar approach to determine a soil rootable depth from awc and soil texture from table 4 once the soil depth soil fertility and texture are determined for a target soilgrids1km grid a corresponding hc27 soil profile can be selected then srgf of the selected hc27 soil type is assigned as the srgf of that target pixel 4 assign srgf for each soil layer from the corresponding hc27 soil profile since soil depth specifications in hc27 are 0 10 10 30 30 60 60 90 90 120 120 150 150 180 cm as opposed to 0 5 5 15 15 30 30 60 60 100 and 100 200 cm of soilgrids1km different definitions of soil depths for shallow medium deep and weighted averages of srgfs are allocated to the standard soilgrids1km layers as shown in table 5 3 2 evaluation of the gridded soil profile dataset at 10 km scale evaluating the accuracy and quality of the gridded soil dataset at 5 arc minute resolution 10 km as introduced in this study is not straightforward the new gridded soil profile dataset contains soil parameters derived from multiple sources uncertainties of the parameters can come from many factors e g measurement errors incurred in the original survey data by local agencies standardization harmonization process by compiling all available measurements and spatial predictions by statistical models and aggregation to target resolutions heuvelink 2014 issues with accuracy assessment of digital soil mapping have been addressed in several studies bishop et al 2015 heuvelink 2007 leopold et al 2006 typical approach to evaluate the quality of model predicted soil parameters is to compare them with independent point observations however as heuvelink 2007 and leopold et al 2006 argued that differences in measurement supports between point observations and model predictions at larger scales should be taken into account bishop et al 2015 used three different spatial supports point 48 m blocks and soil land use complexes in order to evaluate model predicted clay content maps and concluded that validating digital soil maps at the point support is likely to be the worst case they argued that various spatial supports ranging from a point to larger block supports are required for better validation of digital soil maps in this study the target resolution of the soil dataset is 10 km it is difficult to have an independent data at a similar scale support for evaluating the output soil dataset for the world the best possible database of global coverage is the isric wosis ribeiro et al 2015 however wosis comprises point profile observations thus not free from the issue of scale support discrepancy with our target resolution moreover few point observations that fall within the target grid 10 km is not enough to aggregate to make an average representation of the target resolution despite all of these we took wosis as one of the validation data and compared our derived soil hydraulic properties at 10 km resolution the effect of different scales will be discussed in section 4 due to the scale issue described above the evaluation of the new soil profile dataset relied only on visual comparisons among other datasets in different regions of the globe the derived soil hydraulic properties were evaluated using similar maps from literature e g leenaars et al 2018 other soil database i e ssurgo of the u s and crop land distributions fritz et al 2015 regardless of these efforts it must be reiterated that there is no true value to quantitatively assess the quality of the derived soil profile dataset we can only speculate based on logic furthermore a crop simulation study is presented in section 4 2 which showed the application of the output soil profile dataset in modeling crop yield responses over tanzania it can be considered an indirect way of evaluating the quality of our output soil dataset gijsman et al 2007 and romero et al 2012 tested the sensitivities of the soil parameters they developed using crop simulation results without quantitative comparisons from other soil data sources 4 results the final 10 km resolution gridded soil profile dataset formatted in dssat soil input file sol were created separately for each 225 countries with its unique iso a2 as a filename e g ke sol for kenya the dataset is accessible and freely available from https doi org 10 7910 dvn 1peey0 iri et al 2018 the generated soil profile dataset and some applications are described below here we only show some representative soil parameters as a way of evaluating the quality of the new model friendly soil profile dataset 4 1 derived soil hydraulic properties 4 1 1 soil water contents at wilting point and field capacity maps of soil water content at wilting point slll at 10 km resolution are shown in figs 2 and 3 fig 2 shows a depth aggregated slll for the top 15 cm depth i e weighted average of the first two layers for the world in order to evaluate the quality of the estimated slll we compared the derived soil property for africa with a permanent wilting point map from afsis gyga functional soil information for sub saharan africa rz pawhc ssa fig 3 the afsis gyga data was introduced by leenaars et al 2018 and is available from www isric org we used a map moisture content volumetric of the soil fine earth at permanent wilting point defined at pf 4 2 and aggregated over the top 30 cm of the soil the slll in fig 3b by leenaars et al 2018 was created based on afsoilgrids250 m hengl et al 2015 and different pedo transfer functions they used pedo transfer functions particularly developed for tropical soils that need input data e g bulk density cation exchange capacity and ph besides the other soil properties that we used in this study i e sand silt clay and organic carbon content in general the sllls from afsis gyga are slightly higher than what we produced this may be due to the different input soil database at different resolution afsoilgrids250 m vs soilgrids1km and different pedo transfer functions used however a permanent wilting point of 0 44 cm3 cm 3 in ethiopia fig 3b seems to be too high in reality from afsys gyga overall the spatial distributions of sllls are comparable fig 3 the soil water contents at wilting point slll and field capacity sdul of the new soil profile dataset this study were compared with isric wosis database isric wosis contains standardized soil profile data collected by many international soil data providers for a fair comparison we filtered out data from isric wosis to remove soil properties of depths deeper than 15 cm the locations of wosis slll and sdul soil profiles can be found in fig a3 appendix a total of 8643 sdul values were compared but 96 of them were from the continental u s in the case of slll however data from the u s only takes 6 2 of total 1662 slll values in the comparison sdul data greater than 0 95 cm3cm 3 were excluded from wosis note that there are scale discrepancies between the wosis data and our derived slll and sdul see section 3 2 instead of calculating error statistics we compared the kernel density plots of the two soil parameters slll and sdul from wosis and the parameters we produced from this study fig 4 the slll values from wosis are more positively skewed and have a lesser mean compared with the sllls we derived the distributions of both slll and sdul produced in this study are narrower than the ones from wosis this could be due to the smoothing effect when values are aggregated at 10 km scale the pedo transfer functions applied in this study may have also partially contributed to the narrower distribution however considering that crop simulation models will be highly likely applied to areas suitable for agriculture the sdul or slll values in extreme ranges too low or high such as sdul greater than 0 6 cm3cm 3 are not appropriate to be used for evaluation in general the slll and sdul values we derived from this study are within reasonable ranges for agricultural soils 4 1 2 available water content fig 5 shows the available water content awc of the first soil layer 0 5 cm at 10 km resolution as expected derived awc is very low in areas where the world s major deserts are located e g sahara kalahari and namib in africa pantagonia in south america western australia and northern india derived awc with a maximum value of 0 19 cm3 cm 3 seems to be low compared with laboratory measurements which could be due to the smoothing effect of spatial aggregation awc of the 2nd soil layer 5 15 cm showed a similar spatial distribution as in fig 5 not shown due to the difficulties of collecting global awc maps we relied on well documented soil database ssurgo of the u s for additional evaluation of the derived soil hydraulic properties the gridded ssurgo gssurgo soil survey staff 2014 provides a wide range of soil information including available water storage mm of top 25 cm of soil depth an example in california usa the awc values derived from soilgrids1km sdul minus slll at 10 km resolution in this study fig 6 b were compared with awcs from ssurgo in fig 6a and weighted averages of awcs at an aggregated 10 km resolution from soilgrids250 m hengl et al 2017 in fig 6c note that awcs in fig 6b and c represents value at 15 cm soil depth through weighted averages of two standard soil layers 0 5 and 5 10 cm while the ssurgo awc values in fig 6a represent 25 cm of soil depth awcs from soilgrid1km have a narrower range of distribution i e maximum value of 13 and minimum value of 7 compared to ssurgo this narrow range may be attributed to the narrower range of sdul distribution as shown in section 4 1 1 in comparison with wosis other reasons could be the aggregation effect and the ptfs applied gijsman et al 2002 compared 8 different methods to derive soil hydraulic properties and demonstrated that the estimated awc values vary significantly not only between methods and soil types but just as well within soil types therefore it is recommended to test sensitivities of the selected ptfs in future studies despite of the narrower ranges in the magnitudes of awcs the awcs derived from this study show a very similar distribution with ssurgo i e major agricultural areas along the great valley have relatively higher awc values while forest areas along the sierra nevada have the lower values interestingly awc map from soilgrids250 m shows an opposite distribution between ssurgo and this study as an indirect evaluation of the derived awc from this study a cropland map was compared with the derived awc map of africa we used the most recently released global cropland percentage map developed by the international institute for applied systems analysis iiasa international food policy research institute ifpri fritz et al 2015 fig 7 suggests that many cropland areas in africa are consistent with areas of relatively higher awc values fig 7b see red circles however there are also some areas that do not match as indicated in black circles fig 7b as shown in section 4 1 1 we also compared the derived awc with another derived awc map from afsis gyga leenaars et al 2018 the afsis gyga s awc map see fig 6b of leenaars et al 2018 shows a somewhat different spatial distribution from our results in fig 7 specifically for relatively higher awc regions throughout the savanna zone in africa and very low awc in the gezira sudan not shown here see han et al 2015 leenaars et al 2018 attributed the strange patterns in their results to the limited availability of bulk density data 4 2 case study ex ante assessment of maize yield response potential to fertilizer hybrid seeds and improved agronomy this section shows an example of how the new gridded soil profile database can be used for real world applications southern highlands area in tanzania is often referred to as one of the breadbasket in the region yet smallholder farmers productivity is still very low latest national statistics data from faostat shows the national average yield of maize as 1 3 t ha 1 in 2013 with annual growth rate of 2 71 to assess how much maize productivity increases can be achieved by future investment on intensification options a grid based crop modeling framework was developed using the soilgrids1km based soil profiles and used for each 10 km grid cell in the area from the calibrated baseline yield levels with subsistence farming system where farmers use traditional variety with no fertilizer application with inadequate agronomic information three step wise strategies were simulated 1 increased nitrogen n fertilizer application 2 improved variety and 3 optimum planting window specifically the ex ante study aimed to identify the feasibility of achieving the target yields of 3 t ha 1 from the simulated intensification strategies simulation results showed overall that the target yield level could be readily achieved through the intensifications with agronomic interventions e g optimum planting density and planting window spatial variability of yield response potentials were demonstrated across the region in fig 8 5 discussion this study presents a new global gridded soil profile dataset at a 10 km resolution developed for dssat despite recent advances in digital soil mapping there is still a gap in utilizing the available soil databases for crop models and eventually for practical decision making by policy makers for food security under a changing climate and environment the gap is mainly because of the models required soil inputs that are not provided by typical soil survey data especially for gridded crop model applications availability of soil input data at the right format and resolution has been one of the big challenges in this study we bridge this gap by developing a 10 km global gridded soil profile dataset by translating a high resolution gridded soil database soilgrids1km into model ready formats and specificity this work aligns with carré et al 2007 and mcbratney et al 2012 that asserted the concept of digital soil assessments beyond digital soil mapping linking crop models with soil inputs is a critical step in assessing biomass production e g crop yield as one of the important soil functions as well as bringing the value of digital soil maps to benefit society the soil profile dataset developed in this study has been used operationally since its first release in 2015 andreadis et al 2017 developed the regional hydrologic extremes assessment system rheas and coupled it with dssat to monitor and predict drought and maize productivity in east africa rheas used the gridded soil profile data from https doi org 10 7910 dvn 1peey0 for regional dssat modeling komarek et al 2018 applied a gridded dssat using the soil profiles of han et al 2015 to simulate and analyze economic effects of different seed cultivars and fertilizer intensification in tanzania the developed soil profile dataset as it is now has a great potential to be used for developing agriculture related decision support tools or any analysis framework at regional or global scale for researchers or policy makers the methods described in this study and han et al 2015 can be used as precedence to developing other gridded soil datasets for other crop models like agricultural production systems simulator apsim or environmental policy integrated climate epic model in this study we applied the pedo transfer function of saxton and rawls 2006 to derive the required soil hydraulic properties for dssat however there are other pedo transfer functions that can be used for this purpose likewise there are several ways to estimate soil properties that are not available from soilgrids1km to complete the model friendly soil profile dataset although here we used the generic soil profiles in hc27 these are potential sources of uncertainties in the dataset further work is needed to quantify uncertainties and sensitivities of the applied methods to the final product uncertainty quantification has received a great attention particularly by the globalsoilmap community heuvelink 2014 because of its importance to the end users as well as to the map makers uncertainties of the derived soil profile dataset can come from different sources moreover additional uncertainties can come also from subsequent processes when applying pedo transfer functions and when aggregating the data from 1 km to 10 km resolution han et al 2015 earlier quantified uncertainties of derived soil hydraulic properties based on the assumptions of independence from each pixel or normality of soil properties following heuvelink 1999 and hengl et al 2014 assumptions like this may not be always valid in realistic more sophisticated approach for quantifying uncertainty can be employed e g statistical approaches to quantify probability distributions of all possible sources of uncertainties heuvelink 2014 this statistical approach however can be computationally expensive and requires sufficiently large sample size to characterize uncertainties thus warrants as a separate study the product of this study has a final resolution of 10 km derived from a 1 km resolution dataset the question of how to aggregate data from 1 km to 10 km arises this is related to defining the support of the soil properties an approach to finding a representative soil property at a 10 km resolution is to select a dominant soil pixel profile in the target 10 km grid as wu et al 2010 however selecting a dominant soil type is not always the best approach heuvelink 2007 and lagacherie et al 2000 argued that selecting a representative profile of a dominant mapping unit can lead to neglect a substantial part of the mapping unit in addition soil covariates used for interpolating point measurements are not always from a dominant soil profile hengl et al 2017 applied a digital elevation model and a wide range of remote sensing data e g modis enhanced vegetation index as soil covariates to creating soil database at 250 m resolution these gridded soil covariates are representative values and not necessarily from a dominant soil type of the pixel in this study we employed an arithmetic averaging of soil properties to aggregate 1 km resolution data to a 10 km resolution the reasoning behind using the simple average is that we assumed that our target pixel is a homogeneous unit having a hypothetical representative soil property which may be neither a simple average of a few ground measured values nor values from one dominant soil profile when we apply dssat to a grid of a 10 km resolution this is similar to the approach of mohanty and zhu 2007 regarding the issue of defining a representative value for a grid cell shangguan et al 2014 addressed the problem of finding one to one relation between a grid cell and soil mapping units in reality there are one to n relations between a soil mapping unit or soil map polygon or a grid they tested three methods area weighted dominant soil type and dominant binned soil attribute method but offered results of the areas weighted method which is similar to our simple averaging of 1 km pixel values simple averaging approach has been used commonly in remote sensing community but not in soil science community further investigation is needed to better understand the effects of different aggregation schemes 6 conclusions this study developed a set of dssat compatible global soil profiles at 10 km grid based from a recently released soil database soilgrids1km based on the soil properties from soilgrids1km soil hydraulic properties were derived using pedo transfer functions other required variables were derived from harvestchoice s hc27 dataset the final product is provided for each country in a sol file which is a standard format of the dssat soil input since it is not possible to have a reference data close to true values representing a 10 km resolution our evaluation of the output soil dataset had to rely on more indirect methods qualitative approach rather than quantitative visual inspections were conducted in comparison with i other maps i e iiasa ifpri cropland map and afsis gyga s wilting point and available water content maps and ii ssurgo soil database for california in the u s in general distributions of the soil properties areas with relatively higher or lower values matched well with the data in comparison when the output was compared with wosis soil database which contains global harmonized soil profiles smoothing effect due to the aggregation was observed as an application the soil profile datasets developed in this study was tested in an ex ante modeling study to assess the potential of agricultural investments in tanzania s southern highlands area uncertainties incurred from the input soil database and during the data processing should be quantified to better assess the quality of the final product future work will be done on quantifying the impacts of selected pedo transfer functions type of input soil database soilgrids250 m rather than soilgrids1km and sensitivities of the estimated soil parameters on the crop yield predictions the soil profile dataset developed in this study will contribute to the advances of gridded crop modeling applications at regional or global scale the approach used in this study can be easily applied for other soil databases e g soilgrids250 m for gridded application of crop models at higher resolution data availability name of data set global high resolution soil profile database for crop modeling applications developers international research institute for climate and society columbia university ny 10964 usa michigan state university east lansing mi usa harvestchoice international food policy research institute ifpri washington d c usa contacts eunjin han international research institute for climate and society columbia university ny 10964 usa email eunjin iri columbia edu amor vm ines michigan state university mi usa email inesamor msu edu jawoo koo international food policy research institute ifpri washington d c usa email j koo cgiar org year first available 2015 availability the soil dataset product is available at https doi org 10 7910 dvn 1peey0 under a creative commons cc by nc attribution non commercial 4 0 license via dataverse fortran codes for deriving soil parameters are also freely available via github https github com agro climate global gridded soil data for dssat at 10km acknowledgement we acknowledge bill and melinda gates foundation for funding this research project through international food policy research institute ifpri harvestchoice project phase ii oppgd1450 we also acknowledge funding from nasa servir nnn13d788t and nnh15zda001n servir 15 servir15 2 0089 we thank the editor and reviewers for helping us improve the quality of the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 012 appendix a 1 data processing for output format dssat compatible soil profile data sol were saved separately for each country the file names were derived from two letter standard country code iso a2 of iso international organization for standardization for example ke sol file contains all available soil profiles at 10 km resolution in kenya two different country codes iso a2 and iso a3 three letter code were also written in each profile as shown in the line 1 ken and 3 ke in fig a1 complete list of iso country code and corresponding country boundary map was obtained from natural earth http www naturalearthdata com figure a 1 example sol output for kenya figure a 1 soil profile id e g ke04430591 in fig a1 is determined from harvestchoice s standard grid cell id at 10 km spatial resolution cell5m harvestchoice and minnesota 2018 fig a2 shows a sample of cell5m grid cell ids across kenya as an example naming soil profile id based on cell5m ids is intended to streamline grid based crop modeling operation across large areas in a batch within the harvestchoice s modeling framework creating country mask at 10 km resolution is not easy due to mismatches between curved country boundaries or coastal lines and 10 km grids as shown in fig a2 we used ifpri s concordance look up table to assign a country id for each 10 km grid harvestchoice and minnesota 2018 figure a 2 harvestchoice s standard grid cell id cell5m and corresponding country masks over kenya ethiopia and somalia figure a 2 soil texture classification in the first line of sol e g sandclayl in fig a1 is determined based on the proportion of sand and clay in a given grid using usda scs soil triangle gerakis and baer 1999 due to limitation in space the soil texture classification was truncated to shorter names when needed see table a1 table a 1 short names for soil texture classifications table a 1 soil texture classification short name silt loam siltloam sand sand silty clay loam siltclayl loam loam clay loam clayloam sandy loam sandyloam silty clay siltyclay sandy clay loam sandclayl loamy sand loamysand clay clay silt silt sandy clay sandyclay except for four soil hydraulic properties slll sdul ssat ssks derived from ptfs other unknown soil properties are extracted from hc27 as described in table 2 based on the given classified soil texture root depth and fertility organic carbon one soil profile out of 27 profiles was selected the only exception is for slmh master horizon as it was pre determined for each standardized soil layers so that it can be similar with hc27 for the first two layers a is assigned and ab ba b bc are assigned to the rest 3rd to 6th layers respectively figure a 3 locations of wosis soil profiles which have soil water content at wilting point top and at field capacity bottom in soil depth up to 15 cm from the july 2016 wosis data downloaded in august 2018 figure a 3 
26171,is it possible to predict the execution time of a spatially distributed hydrological model by only examining the mesh this article investigates this question by using a benchmark mesh with the penn state integrated hydrologic model pihm the benchmark mesh triangles are reordered using ten different graph search algorithms that treat each mesh triangle as a graph root to select the remaining triangles in the watershed domain pihm then executed these graph reordered meshes to create performance datasets to find which graph search algorithm and triangle root combinations improved pihm s execution time the performance datasets were used to train and classify seven different machine learning ml models to predict the fastest execution times analyzing these ml results facilitated a strategy for end users of the hydroterre expert system to choose meshes that improve execution times for their hydrological science research with pihm graphical abstract image 1 keywords meshes hydroterre pihm machine learning models graph search algorithms distributed hydrological model 1 introduction to accurately represent water dynamics within watersheds that incorporates landscape features such as green infrastructure designs in distributed hydrologic models often computational meshes need to be 1 m or less to represent these features leonard et al 2019 this leads to a large number of triangles 10 000 s to 10 000 000 triangles even for a small study watershed that increases the overall computational costs leonard 2018 these costs are compounded further when watersheds need to be calibrated numerous times to match observation datasets hence modelers initially choose to use coarser resolutions with low triangle counts to minimize the computational costs and refine progressively the mesh to match the capabilities of their high performance computing hpc resources what if the modeler could predict which mesh is more efficient before executing their distributed hydrological model these same issues affect expert systems such as hydroterre http www hydroterre psu edu leonard and duffy 2013 leonard 2015 leonard and duffy 2016 to improve the performance of hydroterre requires strategies to create high quality meshes before using the meshes in a variety of environmental modeling this research is a step towards addressing these common modeling issues by devising strategies to predict the hydrological model execution time or running time based on mesh characteristics alone before using the mesh here the penn state integrated hydrologic model pihm qu 2004 qu and duffy 2007 pihm and penn 2014 is used to demonstrate issues with meshes found amongst other spatially distributed hydrological models using polygon meshes as input datasets a watershed model calibrated to observation datasets by an expert modeler will serve as the benchmark mesh the mesh file is reordered by graph search algorithms with the remaining input datasets fixed to measure the reorder impact on execution times with pihm this is followed by using machine learning ml models to train and classify datasets to predict which graph ordered meshes generated the fastest execution times for pihm 1 1 the following are the primary research contributions using a benchmark unstructured grid mesh ten different graph search algorithms were used to reorder the mesh triangles to identify which of the six different triangle types that form the mesh improved the execution time of the pihm model this research identified that the watershed outlet triangle s is not necessarily the graph root triangle with the lowest execution times but graph root triangles that touch or form the stream network had consistently the best running times all ten different graph search algorithms improved the execution times of pihm as reordering triangles improved cache memory access from the ten graph algorithms studied breadth first search bfs implicit edge depth first search iedfs reverse depth first search dfs reverse and edge depth first search edfs reverse consistently reordered meshes with the lowest execution times execution times from the ten different graph search algorithms combined with the benchmark mesh properties were used to train and classify seven different machine learning models to predict the lowest execution times four trial datasets were examined removing variables to understand their influence to predicting running times from the seven machine learning models studied combined with the four best graph search algorithms naïve bayes linear regression and deep learning models are recommended to predict the lowest execution times this article is structured as follows section 1 summarizes the hydrological model used to evaluate meshes reordered by graph search algorithms and how this research is coupled with the hydroterre expert system section 2 explains the methods used to modify the benchmark mesh using graph search algorithms followed by describing the performance measurements recorded for predicting mesh performance with ml models section 3 discusses the performance results of modifying the meshes and summarizes the ability for ml models to predict execution times with these graph reordered meshes section 4 demonstrates a strategy for end users to choose from different mesh properties graph search algorithms and ml models to predict with three accuracy ranges an appropriate graph reordered mesh for their hydrological modeling science objective 1 2 spatially distributed watershed models pihm uses a spatially distributed unstructured grid with a finite volume formulation pihm s unstructured grid is kept as a plain text mesh input file that can be transformed and be used by a variety of environmental models the strategies demonstrated in this research are also applicable to other hydrological models including rhessys leonard et al 2019 tague and band 2004 rhessys 2017 that use a mesh to represent the land surface for the governing multi physics equations used by pihm to calculate water and energy refer to qu 2004 qu and duffy 2007 pihm and penn 2014 for further details applying these equations to a finite volume strategy leads to a semi discrete system of local ordinary equations ode assembled together to form a global system of odes that is then solved using cvode solver in the sundials suite of solvers hindmarsh et al 2005 designed for stiff nonlinear systems within pihm the unstructured mesh and domain decomposition uses triangle software shewchuk 1996 shewchuk 1997a shewchuk 1997b cheng et al 2012a for pihm users to construct quality numerical meshes that can be constrained to follow or preserve important features of the watershed domain for pihm this includes preserving watershed or catchment boundaries for example level 12 hydrological unit codes huc the smallest watershed delineated by usgs leonard and duffy 2014a usgs 2013 nhd 2013 mckay et al 2012 and to capture stream networks such as those available from the national hydrography dataset nhd nhd 2013 mckay et al 2012 using these datasets with triangle results in a triangle irregular network tin whose edges are constrained by the watershed boundary and stream network for pihm users to capture the watershed domain and stream network with fine detail the tin triangles need to be small as demonstrated in leonard 2018 to capture stream curvature and other land use details such as green infrastructure designs that result in large multi million triangle counts these large triangle counts are problematic with models such as pihm as they are computationally expensive to solve and difficult to predict compute times if they are possible to resolve at all therefore most pihm modelers create watershed tin domains with a minimum number of triangles i e hundreds to a few thousand triangles that are faster to compute to meet their hydrological science objective based on their previous experiences and expertise hence for models such as pihm to use multi million triangles it is necessary to investigate computational strategies to create optimal hydrological meshes with choices so modelers can select meshes with thousands to multi million of triangles appropriate to their scientific objective the focus of this research is to improve running time performance by developing automated strategies with meshes before using them in hydrological models doing so will reduce time spent by hydrological modelers to identify mesh issues and is a step towards predicting compute times for models such as pihm automating these strategies is important as manual strategies often taken by pihm modelers are not feasible for meshes containing hundreds of millions of triangles as demonstrated in leonard 2018 these large triangle count meshes are a key step towards evaluating hydrological models from hill slopes to major river basins which is a major objective of the hydroterre expert system and is explained next 1 3 hydroterre overview hydroterre is a heterogeneous distributed compute environment using end to end workflows finely tuned fig 1 a to retrieve essential terrestrial variables etv s including elevation soils climate geology rivers land use at multiple level hydrological unit code huc scales to create calibrate and visualize watersheds leonard and duffy 2013 leonard 2015 leonard and duffy 2016 hydroterre provides rapid web service access to 250 terabytes of etv national datasets at a level 12 huc scale within a minute leonard et al 2016 the etv data workflow fig 1b creates a downloadable data bundle that are the minimum geographic information system gis datasets for many hydrological models and are appropriate datasets for other environmental models leonard 2015 leonard and duffy 2016 leonard et al 2018 the data model workflow fig 1c retrieves the etv data bundle and transforms the gis datasets to pihm model inputs based on four parameters that control the watershed boundary stream topology and in turn controls the quality of the unstructured mesh leonard 2018 leonard and duffy 2014b using the hydroterre expert system pihm models for the 83 016 level 12 hucs delineated within the continental united states conus have been executed millions of times with the end to end workflows fig 1a e by analyzing these workflow results pihm failed due to delineation issues with the watershed boundaries e g clipping streams and digitization problems with the stream networks e g inconsistent flow directions etv transformation of these two datasets accounted for 31 91 failure within the data model workflow leonard 2015 within the model workflow that retrieves the data model workflow to execute pihm using distributed compute resources 71 17 failed due to a poor quality mesh created with stream geometry where pihm s physics never converged using the cvode solver leonard and duffy 2014b leonard and duffy 2014a leonard and duffy 2014c leonard 2015 to improve these hydroterre end to end workflow results and to create services that will enable users to select dynamic number of level 12 hucs anywhere in the conus the large graph visual analytic workflow fig 1f enables users to validate watershed boundaries and stream networks using hydroterre web services leonard and duffy 2014a leonard et al 2017 the multilevel mesh workflow fig 1g retrieves elevation datasets validated stream networks and huc graphs from hydroterre services to create quality meshes that minimize sliver creation and other mesh issues specific to hydrological modeling leonard 2018 the focus of this manuscript fig 1h j is to provide hydroterre users strategies to evaluate whether these meshes improves running times for hydrological modeling using the methods outlined in the next section 1 4 related work ml models are becoming well established in the hydrological science community for a variety of science objectives including to predict hydrological flow series using extreme learning machine elm deo and sahin 2016 deo and şahin 2016 atiquzzaman 2018 atiquzzaman and kandasamy 2018 worland et al 2018 worland et al 2018 compared eight ml models e g gradient boosting machine gbm random forest rf and four baseline models to improve predictions of hydrological low flow indices in ungauged basins duerr et al 2018 duerr et al 2018 used gbm rf and bayesian additive regression trees bart with time series datasets to improve forecasting of household level water demand deep learning dl models have been used to forecast daily reservoir inflow bai et al 2016 calibration testing on hydrological numerical data and benchmarks marçais and de dreuzy 2017 and used for rainfall runoff simulations hu et al 2018 for a review on dl models and its relevance for water resource science see shen 2018 shen et al 2018 mesh generation is an extensively studied field and used by many disciplines for a description on delaunay mesh generation algorithms the readers are directed to research by shewchuk 2008 shewchuk 2008 and cheng 2012 cheng et al 2012b with research on mesh generation and spatially distributed hydrological modeling readers are directed to heinzer et al 2012 heinzer et al 2012 who used triangle software to generate tins for the integrated water flow model iwfm 2019 marsh et al 2018 marsh et al 2018 used multi objective unstructured mesh generation to capture and preserve spatial heterogeneity of etvs e g soil vegetation required for spatially distributed hydrological modeling graph search algorithms is broadly used and for related research in using these algorithms in hydrology readers are referred to castronova 2014 castronova and goodall 2014 who used graphs to process watershed boundaries and nhd stream networks cui et al 2009 cui et al 2009 applied graph theory to design and identify major flow paths for river channel networks halverson 2015 halverson and fleming 2015 applied network analysis tools to an assortment of streamflow gauges to inform streamflow network monitoring design 2 methods to predict reordered graph mesh performance using machine learning models this section explains the methods used to create and modify hydrological meshes used as pihm mesh inputs section 2 1 describes the benchmark mesh used for refinement and analysis section 2 2 outlines the mesh reordering process using graph search algorithms before pihm execution section 2 3 discusses the measurements and data collected when executing pihm with these graph reordered meshes the ml models will use this running time data to predict pihm execution times as summarized in section 2 4 the last section describes the procedure to predict which triangle types and graph search algorithms are the best to use for hydrological modeling using unstructured grid meshes 2 1 benchmark mesh to examine the execution times of the meshes reordered by graph algorithms a benchmark mesh is necessary for comparison this research uses the mesh generated by dr lele shu using triangle software shu 2017 the mesh was used to investigate the conestoga watershed 1270 sq km located in pennsylvania to study land use change with pihm shu 2017 triangle is the method used in the data model workflow fig 1c to create pihm meshes constrained by catchment boundaries and stream networks dr shu manually fine tuned and calibrated his water research with this conestoga mesh against observation datasets after many time consuming iterations dr shu tuned the pihm input files and all these input files remained fixed in this research except the mesh and river input files fig 1h in this manuscript reordering the mesh implies modifying the river file as well keeping the input files fixed is important to understand the impact of only changing the mesh the motivation is to understand the impact of reordering the mesh triangles and to device strategies to handle multi million triangle counts generated by the multilevel mesh workflow fig 1g the results described in this manuscript will guide further research with the large triangle count meshes without executing pihm numerous times the benchmark mesh contains 8042 triangles and 1412 river segments as shown in appendix a and is considered a large triangle count compared to previous pihm studies pihm and penn 2014 the pihm input datasets are available at this link 1 1 https github com happynotes conestogadata tree master mesh 8042 the mesh triangles in appendix a and with the detail in fig 2 have unique identification integer numbers 1 8042 and at first appear random at the watershed scale with closer examination most triangles along the stream network have lower identification numbers that are not consecutive in order meaning triangle identifications adjacent to each other are not sequential the non sequential labelling of triangles is expected behavior from triangle software as triangle first creates triangles along the stream network a break line constraint that meets user supplied arguments to control angles and area sizes followed by recursively creating triangles along the stream network before creating triangles within the remaining watershed domain often triangle identifications are large in central areas of the watershed that are the furthest away from the watershed boundary and stream networks to understand the impact of these triangle formations and discover any potential strategies in reducing execution times the triangles were classified table 1 by how they form or do not form along the stream networks briefly these triangle types include the number of nodes touching the stream network and the number of stream edges formed by the triangle fig 2 shows these classification triangle types near the watershed outlet of the benchmark mesh triangles shaded in green have no nodes touching the stream network while triangles with one or more nodes touching the stream network are blue appendix a shows these triangle classifications for the entire watershed domain 2 2 reordering mesh with graph algorithms as fig 2 demonstrates and was explained in the previous sub section the mesh unique identification numbers are not sequential when comparing the triangle neighbors in the benchmark mesh file the triangles are written in sequential order 1 8042 but as shown in fig 2 a triangle neighbors identification are often not similar to have neighboring triangle identifications close to each other in the mesh file graph search algorithms reorder the triangles in the mesh input file for example from fig 2 a possible sequence would be 105 117 4135 106 and so on in the graph reordered mesh file to evaluate the impact of changing these sequences each of the benchmark triangles were designated as the root or origin triangle and the graph search algorithm selects the remaining triangles assigning distance to triangles from the root triangle for example selecting triangle unique identification 105 watershed outlet in fig 2 as the graph root the remaining 8041 triangles are searched for with distances calculated to the origin the original mesh geometry e g vertices nodes edges and triangle connections remains the same for example triangle 106 its neighbors or triangle connections are 4135 3779 and 103 these shared edge connections remain the same what changes is how the graph reordered mesh is stored in the mesh data file and consequently how the data is loaded into memory this graph reordering step has been calculated for each of the 8042 triangles to compare the impact of various graph roots and graph search algorithms to the execution times of pihm to address the following questions does it matter if the triangle identifications with its neighbors are close to each other in the mesh input file should the graph root be a watershed outlet triangle which triangle types perform better than others to reorder the mesh each triangle is a graph node or vertex with each triangle neighbor s stored as graph edges with each triangle the following five graph search algorithms were used to select the remaining triangles briefly these algorithms are 1 breadth first search bfs the root node is expanded first then all the successors of the root node are expanded next then their successors and so on all the graph nodes are expanded at a given depth in the search tree before the nodes at the next level are expanded russell and norvig 2010 2 depth first search dfs dfs always expands the deepest node in the current edge of the search tree the search proceeds immediately to the deepest level of the search tree where the nodes have no successors as those nodes are expanded they are dropped and the search steps back to the next deepest node that has unexplored successors russell and norvig 2010 3 edge depth first search edfs edfs is a variant of dfs algorithm where edges are selected rather than the node 4 implicit depth first search idfs idfs is a dfs algorithm for implicit directed graphs the implicit directed graph is derived by the mesh triangles 5 implicit edge depth first search iedfs iedfs is an edfs algorithm for implicit directed graphs the implicit directed graph is derived by the mesh triangle edge connections furthermore the above five graph search algorithms were searched in reverse order meaning the selected triangle graph root was the furthest distance to compare pihm running times fig 3 illustrates how the five different graph search algorithms and their reverse counterpart used in this research affect triangle selections at the watershed outlet based on triangle 105 shown in fig 2 outlet location appendix b has web links to access movies demonstrating the graph search algorithm impact to the complete watershed domain for all combinations access to the graph reordered meshes used in this manuscript is available here 2 2 https github com leonard psu machine learning models tree master graphsearchalgorithms data examples conestoga 2 3 measurements recorded when executing pihm with reordered meshes to measure the execution time impact of graph search algorithms that reordered mesh triangles in memory the average number of instructions retired per cycle ipc were tracked when executing pihm all these tests executed on one server as described in appendix c to simplify performance comparison cycle refers to the central processing unit cpu cycle and is the time required for retrieving and executing one simple machine instruction an ipc value of one is generally considered an acceptable value for hpc software applications furthermore to gain insight how the graph algorithms performed when executing pihm measuring the following time measurements was necessary 1 total time representing the total amount of time from beginning to end to execute pihm this includes read pihm input files execute the hydrological physics equations and write result files each test uses the same input files att forc ibc soil geol lc para calib init except the mesh and river input files mesh riv that the graph search algorithms modified as described in section 2 2 2 read time time spent reading the pihm input files the total amount of memory required is identical with each test as only the mesh and river files data are rearranged furthermore all tests used the same disk appendix c therefore the total read time should be nearly identical to other read tests with only minor differences any test results where significant differences occurred i e greater than a few seconds were redone as there are outside influences these outside influences include operating system services i e virus scanner update checks etc disk latency or disk failures and are difficult to block 3 write time time spent writing pihm output results again each test result should be similar to other write tests test results with significant differences were redone to reduce outside influences 4 cvode time the amount of time spent computing pihm s hydrological physics using the cvode solver by comparing the original benchmark mesh result time with the graph modified meshes low cvode execution times indicate that modified meshes influence pihm in a positive way while high cvode execution times indicate a negative influence similar times indicate no influence by the modified graph algorithm meshes ideally adding the read write and cvode times should be identical to the total time recorded due to cpu and disk latency and operating system influences i e memory leaks other software there are minor time differences to minimize disk and cpu latency a timeout of 3 s was imposed between tests in execution scripts to allow briefly cpu idle status any erroneous tests i e negative values or large values with about 100 occurring were re evaluated based on the following formula 5 number of graph algorithms 2 forward and reverse search 8042 number of triangles in mesh domain a total of 80 420 pihm graph reordered mesh tests with re evaluations took a month on one server to calculate results each test executed pihm with one week of simulation data it was enough data and simulation period to stress the cpus on the server and generate results but fast enough to run each pihm test within a minute to understand these running time results for the graph reordered meshes generated by the graph search algorithms section 2 2 with ml models section 2 4 a new dataset is necessary based on the benchmark mesh that combines multiple static properties of pihm input files see pihm and penn 2014 leonard and duffy 2014b for further details to be joined based on the unique triangle identification with the above running time results table 2 summarizes these properties briefly these include geometry features not required by pihm input files but created for these trials calc method and physical attributes in method applied to pihm triangles i e soil geology landcover and climate attributes there will be four trial datasets used by the ml models as described in the next sub sections 2 4 machine learning ml models with the datasets collected as described in section 2 3 ml models are necessary to identify patterns patterns may occur within individual graph search algorithm results and those that may happen when comparing different graph algorithms and triangle types using the trial datasets these algorithm and ml model types are necessary to search for answers to the following questions is it possible to predict cvode times based on a graph reordered mesh are there any specific triangles that outperform i e lowest cvode execution times other triangles consistently if so what features or attributes do they share for example a common land use type or geometry properties see table 2 do the ml models behave differently between the pihm graph search algorithm results if there are differences will they affect the hydrological modeling decision process the following seven ml models were used to train classify and analyze the 80 420 tests to predict cvode execution times with meshes reordered by the graph search algorithm techniques 1 naïve bayes a simple and fast probabilistic classifier based on bayes theorem where the fundamental assumption given the value of any attribute is independent of the value of any other attribute this assumption is rarely true but is computationally inexpensive and the algorithm often works well the gaussian probability density is used to model the attribute data russell and norvig 2010 2 generalized linear model glm generalization of linear regression models estimate regression models for outcomes following exponential distributions cook 2017 h2o ai 2018 nykodym et al 2016 3 logistic regression uses glm with a binary classification h2o ai 2018 nykodym et al 2016 4 deep learning based on a multi layer feed forward artificial neural network trained using a stochastic gradient descent using back propagation for learning non linear relationships cook 2017 phan et al 2017 deshpande and kotu 2015 5 decision tree finds simple tree like models which are easy to understand the structure resembles a tree with a collection of nodes intended to create a decision on values relationship to a class or an estimate of a numerical goal value cook 2017 6 random forest is an ensemble algorithm creating more than one tree model with the model results combined cook 2017 7 gradient boosted machine complex model using ensembles of decision trees using forward learning ensemble methods to obtain predictions using gradually improved estimations click et al 2017 2 5 procedure to predict which triangles and graph algorithms are the best for hydrological modeling fig 4 summarizes the procedure to predict which triangles have the lowest cvode execution times based on the graph reordered mesh as described in section 2 2 reordering the benchmark mesh takes place using graph search algorithms before executing pihm fig 4a each mesh triangle served as the graph root to select the remaining triangles as explained in section 2 3 pihm performance data for each mesh will help explain graph search algorithm achievements fig 4b these performance datasets are joined with mesh properties to create new datasets that are partitioned by triangle type formed along stream networks or not section 2 1 as summarized in fig 4c initial analysis of the variables collected indicate strong correlations that do not improve the ml models and these variables required refinement fig 4d for example low cvode execution times indicate higher ipc values and lower total times recorded hence the last two variables are not necessary to improve ml model performance table 2 identifies which variables the four trial datasets fig 4e used and their meanings the ml models used these four trial datasets to predict cvode execution times the first trial used all 22 refined variables the second trial selected the highest correlated variables to support the lowest cvode execution times again removal of variables occurred if their correlation did not improve the ml models for example soil and geol have the same impact thus geology was not necessary and removed the third trial removed triangle identification often the highest correlation in trials one and two to compare the impact of this variable to predict cvode execution times finally the last trial only used cvode time and triangle identification variables for each of these four trial datasets tables were sorted by cvode times in ascending order these tables did not match the benchmark sequence of triangles 1 8042 recall the objective is to evaluate how well the ml models can predict the lowest cvode execution times in other words which mesh input improved pihm running time the ml models section 2 4 evaluated used two strategies to train 80 of dataset and classify the datasets to fit the parameters of the classifier with remaining 20 of dataset fig 4f 1 two equal sizes with no data clipping of high outlier cvode times with the first partition containing the lowest cvode execution time values and the second partition has the remaining values 2 two equal ranges with no data clipping this means to split cvode times between the lowest and highest cvode times recorded to create two dataset partitions hence the number of items in the first range does not necessarily have the same number of items in the second range with both strategies documentation included ml model classification accuracy to predict the lowest cvode execution times fig 4g and whether the ml model predicted range one lowest cvode times without optimization this is followed by optimizing the ml model to predict range one fig 4h to simplify summarizing these results full results in appendices described in the next section the partition strategy used was based on which strategy that best captured the diversity of cvode execution times fig 4i for example if high cvode time outliers shifted the division location of two equal ranges meaning range one had the majority of data often reflecting high model accuracy then method one two equal sizes was selected for discussion in situations where neither strategy was sufficient data clipping of high cvode time outliers was necessary to re evaluate the model accuracy there was no clipping of low cvode time outliers as the objective of this research is to understand why this happens finally with the trial four datasets the ranges of triangle identifications representing the most accurate result by the ml model was identified to analysis the spatial location of these triangles within the watershed fig 4j due to the substantial number of combinations there was no documenting of triangle ranges for trials 1 3 datasets as discussed further in the next section 3 results and discussion section 3 1 summarizes the benchmark mesh results and performance of meshes using graph search algorithms with pihm the ml models analyzed the trial datasets to predict the best performing triangle type and graph search algorithm combinations before and after optimization section 3 2 discusses the overall performance while section 3 3 focuses on specific model and graph highlights to formulate a plan for end users to determine an appropriate course of action for their hydrological modeling objective section 3 4 3 1 benchmark mesh result and summary of using graph search algorithms the total time to execute pihm with the triangle benchmark mesh where triangle identifications in the mesh input file are written sequentially 1 8042 but triangles listed above or below in the mesh input file are often not adjacent triangles ranges from 30 to 39 s the ipc ratio range was 0 69 0 76 and the cvode running time ranges from 23 71 to 28 16 s as expected these results have time variability as explained in section 2 3 while using the methods described in section 2 2 graph search algorithms reordered the benchmark mesh before executed with pihm the transformation changed the mesh and river files required by pihm while the other pihm input files remained fixed each mesh triangle a total of 8042 served as the graph root node to search for the remaining triangles fig 5 summarizes the cvode execution times measured fig 4b using the techniques described in section 2 3 the edfs reverse algorithm recorded the lowest total time of 18 09 s and dfs reverse algorithm recorded the longest total time of 28 44 s the average total time with all graph algorithms was 20 92 s 10 s faster than the benchmark mesh result the lowest cvode execution time recorded was 13 82 s with the idfs reverse algorithm about 10 s faster than the benchmark the largest cvode execution time recorded was 22 1 s with the dfs reverse algorithm approaching the benchmark mesh results the average cvode execution time with all graph search algorithms was 16 37 s about 7 s faster than the benchmark mesh results factoring in time variability these averaged results show that the graph search algorithms have improved pihm execution times recall pihm used one week of simulation data and assuming this improvement is consistent for long simulation periods e g hundreds of years it will reduce many hours to days of compute resources required by pihm table 3 examines the best performing individual triangle for each of the graph search algorithms the left side shows the highest ipc values and the right side the lowest cvode execution times two of the graph search algorithms the bfs reverse and dfs reverse algorithms had matching triangle identifications and performance for high ipc and lowest cvode execution times ideally all graphs would match as described in section 2 5 there is a positive correlation between high ipc values and low cvode execution times but the highest ipc values do not necessary produce the lowest cvode times for example the bfs graph search algorithm at triangle 5323 had the highest ipc value of all 80 420 pihm mesh tests but was 2 74 s slower than the best bfs result examining timed tasks within pihm for example printing values to files are slightly higher than other results due to operating system interference that accumulate to the slower result therefore as described in section 2 5 the ml models fig 4d did not use the total time and ipc values instead the results focused on cvode execution times where most of hydrological physics calculations occur and has the largest contribution to the total time to execute pihm furthermore the triangle type section 2 1 mapped to each of the triangle identifications in table 3 indicates from this sample no specific triangle type preference for the best performing cvode execution times per graph search algorithm instead further examination of all triangles per graph is necessary the first technique used to analyze all triangle types after sorting cvode execution times in ascending order was to mark the index location after finding all the triangle types index location one had the lowest cvode execution time and location 8042 has the worst cvode execution time for example there are 104 3n 1e triangle types if 3n 1e dominated the lowest times then with a best case scenario the index location would be close to 104 a worst case scenario would be close to the total number of triangles briefly all graph search algorithms selected nt 1n 0e and 2n 1e triangle types after 8000 indicating this type is not clustered early these three triangle types dominate the watershed count 7449 hence it makes sense these are not clustered for the remaining triangle types a total of 593 the 2n 0e triangle type index location was 7604 for the iedfs reverse graph algorithm with dfs reverse having the worst index location at 8040 with the 3n 1e triangle type the best result was from the idfs graph algorithm which found all these triangle types at index 4605 finally the 3n 2e triangle type the idfs graph algorithm found all these types at index 4965 unfortunately these results do not distinguish any graph search algorithm and triangle type combination that would support end users for an optimal mesh the second technique was to understand the distribution of triangle types as they change by clipping the sample size of the lowest cvode execution times the lowest sample clipped was 258 to match the 2n 0e triangle count see table 1 in a best case scenario either one of the 2n 0e 3n 1e and 3n 2e triangle types would dominate this sample as their totals are less than or equal to 258 the next clipping sample sizes are 500 1000 2000 3000 and ending with 4021 being half of the triangle total and match range one see section 2 5 these results are available in appendix d and fig 6 summarizes these results using parallel coordinate diagrams per triangle type based on the nt triangle type fig 6 the edfs and iedfs graph search algorithms consistently have a high ratio from 258 to 2000 index locations after index 2000 bfs reverse had the highest ratio while the remaining graph algorithms ratio became lower with the 1n 0e triangle type the ratios vary with edfs the ratio peaks at 500 then decreases while with the dfs and iedfs ratios 1n 0e is high initially decreases at index 500 then increases again both the idfs iedfs reverse graph algorithms have high ratios with the 2n 0e triangle type with the edfs and iedfs graphs the ratio is low from 258 to 2000 and slowly increases the ratios by the graph search algorithms in the 2n 1e triangle type is the opposite to the nt triangle type the ratios are low for the edfs increases after index 2000 iedfs bfs and bfs reverse lowest at 2000 graphs the other graph search algorithms have a high ratio that were low in the nt triangle type this pattern occurs due to how the graph search algorithms select the triangles and with average cvode execution times described in the next subsections are important for selecting the best performing graph search algorithm the same pattern occurs with the 3n 1e and 3n 2e triangle types with the edfs reverse graph having the highest ratios preferably all triangle types would be identical with the lowest cvode execution times and other mesh characteristics see table 2 such as the angles that form the triangle as they do not ml models are necessary to clarify which variables are important to classify and predict the lowest cvode execution times fig 4e these results demonstrate it is advantageous to use graph search algorithms to reorder neighboring triangle identifications in the mesh input file rather than the sequential method used in the benchmark mesh see section 2 1 which does not write neighbor triangles adjacent to each other in the mesh file a major contribution to improving pihm s performance is storing a triangle s neighboring triangle data adjacent in memory thus the physics equations can minimize memory misses in cache memory while accessing neighbor triangles data hager and wellein 2010 however the positive correlation between ipc cvode and total times has variability that do not distinguish graph search algorithms for end users to select on performance merit alone the next section examines the impact of triangle classification to predict cvode execution times using ml models 3 2 summary of predicting cvode execution times the results for all graph search algorithms and ml model combinations per triangle types for the trials 1 4 datasets is accessible at appendix e this section focuses on the average accuracy of how well the ml models after optimization predicted the graph reordered mesh lowest cvode execution times before optimization results and discussion are available in appendix e fig 7 a summarizes the trials 1 4 results for triangle types against the graph search algorithms after optimization the overall average accuracy increased to 86 99 from before optimization of 67 15 the range of average accuracy varies from a minimum of 76 5 using 2n 1e at trial 3 towards 93 39 using 3n 2e with the trial 1 dataset comparing all triangle types per trial trial 1 uses twenty two variables had the highest accuracy value of 92 52 the optimization procedure took advantage of minor correlations in the multiple variables to help improve ml model accuracy with the trial 1 datasets this contrasts with the results before optimization where trial 4 with two variables had the best results triangle identification often had the highest correlation illustrated by the better results in trials 1 2 and 4 in both before and after optimization by examining the triangle identification locations with the highest accuracy in the trial 4 datasets see appendix f there was a pattern to the ml model accuracy with dfs and dfs reverse graph algorithms most ml model accuracy was higher at low triangle identifications often triangles at streams with bfs most higher ml accuracy values occurred with lower identifications with some occurring in central and end locations with bfs reverse higher accuracy ml values occurred at high triangle identifications which means the triangle graph root started in the upper central watershed locations but often triangles near the outlet are written first in the mesh file and loaded into memory first see appendix b the other graph search algorithms behavior with similar patterns overall there is an advantage to select lower triangle identifications often found along the stream networks see sections 2 1 and 2 2 for the lowest cvode execution times and at times kept in lower memory locations improving compute calculations although the trial 3 dataset with no triangle identification had the lowest average results in both before and after optimization it shows the positive and important correlation between the outletzmin and outletzmax variables see table 2 that represent hydraulic and elevation heads of the watershed these issues in the future direction section will be re examined since these ml models have been optimized for range one rather than determining the number of ml models that selected range one the number of 100 accuracy results were tabulated appendix e9 overall 24 49 generated 100 results with all trials and triangle type combinations trial 3 with 2n 1e triangle type had the lowest result of 2 86 and trial 1 3n 2e triangle type recorded the largest number of 100 with 55 71 the 3n 2e result is misleading as the number of triangles is small and has low triangle identifications meaning these triangles are often formed early by triangle 2n 1e has identifications ranging from 4 to 8038 indicating these triangle types are created throughout the triangle process hence supporting the idea of selecting triangles along the stream network is important that may not necessarily have low triangle identifications but this depends on the graph search algorithm used again trial 1 performed the best 48 78 and trial 3 the worst with 9 8 number of 100 accuracy results indicating that the extra variables are necessary to end users for choosing a graph search technique to improve predicting the lowest cvode execution times fig 7 b summarizes the trials 1 4 results for ml models against triangle types after optimization the average accuracy per ml model varied from 82 85 rf to 93 15 using the gbt algorithm the lowest average of 76 50 occurred at trial 3 with the decision tree dt algorithm and the highest value of 99 61 happened at trial 1 using the dl algorithm appendix e10 compares the number of 100 accuracy results per ml model zero number of 100 results were found for trial 1 using the rf algorithm and naïve bayes at trial 3 the highest result recorded was 91 43 using lr for trial 1 again this demonstrates the need to strategize when choosing between triangle type graph search algorithm and ml models 3 3 ml and graph search algorithm highlights to predict cvode execution times the results from section 3 2 focused on the overall averages after optimization this section focuses on finding specific highlights about which ml models and graph search algorithms to use or not for each trial dataset per triangle type these highlights are necessary to direct end users to which methods to use after optimization as summarized in section 3 5 a discussion of before optimization results with highlights is available in appendix e after optimization the ml models predict range one with an assortment of accuracies see appendix e2 10 to simplify explaining these results defining three accuracy ranges will address most end users modeling needs as shown in fig 8 the first range is a perfect score of 100 highlighted in blue the second range is 95 100 shown in green and the last range is 90 95 displayed as orange the cells colored pink indicate scores below 90 for these combinations of ml models against the triangle types per trial dataset the trial 1 dataset is the only dataset with a perfect score using three ml models nb lr dl with all triangle types except the nt triangle type six of the thirteen perfect scores recorded used the lr model however lr scores below 90 occur with the remaining trial datasets and triangle type combinations hence end users wanting to use the logistic regression lr model should do so only with the trial 1 dataset the remaining perfect scores occurred with the deep learning total of four and naïve bayes total of three models twenty two of the machine learning models and trial dataset combinations resulted in range two 95 100 both the naïve bayes nb and gradient boosted trees gbt models generated seven range two results with nb generating these scores in trials 1 and 2 only trials 3 and 4 scored below 90 gbt scores between 95 and 100 across all trial datasets and is the only algorithm to do so in trials 3 and 4 the generalized linear model glm recorded four range twos deep learning dl three logistic regression lr one all using the trial 1 dataset twenty of the thirty eight range three 90 95 results occurred with the gradient boosted trees model gbt is the only model to score above 90 in all trail and triangle type combinations except in trial 4 with triangle type 3n 1e the remaining range three results happened with deep learning a total of eight seven in the trial 2 dataset both the decision tree and random forest only with trial 4 dataset resulting with four range threes and finally two using glm with the trial 1 dataset at first with a total of 27 ranges 1 to 3 scores with most trial and triangle type combinations using gbt is encouraging for users however 20 of these occurred in range three only while the deep learning model had 4 perfect scores three 95 100 scores and eight 90 95 scores with trials 1 to 3 datasets a total of 15 ranges 1 to 3 finally the naïve bayes model with a total of ten ranges 1 to 3 had three perfect scores and seven range two scores both the deep learning and naïve bayes models are suitable dependent on the trial dataset used by the end user the following reviews the optimized results with individual graph search algorithms against the triangle types per trial dataset range one blue cells does not apply here with no perfect score recorded additionally the iedfs graph algorithm did not generate scores higher than 90 for any triangle type and trial dataset combination highlighted in pink within fig 9 therefore the recommendation is to not use the iedfs graph by users nor would the bfs and dfs reverse graphs with the trials 3 and 4 datasets the idfs edfs reverse and iedfs reverse graph search algorithms using the nt datasets recorded range two accuracies 95 100 green cells with all trial datasets furthermore the bfs reverse graph algorithm resulted with range two with the 3n 1e and 3n 2e triangle types for all trial datasets contradicting the before optimization results hence these four graph and triangle types after optimization are appropriate to end users range three 90 95 total cell count results are similar amongst the graphs however range two total cell counts are distinguishable idfs has the highest total of 19 followed by the bfs reverse dfs iedfs reverse and edfs reverse graphs in descending order 4 strategies to predict the lowest cvode execution times as the previous sections have discussed there are many choices for end users to select by triangle type graph search algorithm and ml models that predict cvode executable times with acceptable accuracy ranges to narrow user options the graphs overall performance was ranked by selecting the lowest cvode execution times at different index locations see table 3 from ten choices to four graphs the four graphs selected based on consistently low times green lowest at index locations are bfs iedfs reverse dfs reverse and edfs reverse fig 10 summarizes the combinations for these four graph search algorithms against triangle types and ml models per trial dataset appendix g shows all combinations as described in section 3 2 these four graph search algorithms have a higher ratio of 2n 1e triangle types that form the stream network with the lowest execution times it should be noted this graph selection does not include the idfs reverse graph algorithm which recorded the lowest cvode execution time but was not consistently high at other index locations however the best performing triangle for idfs reverse graph is a 2n 1e triangle type near the watershed outlet a pattern common with all the graph algorithms hence to answer the questions posed in sections 2 2 and 2 4 using graph search algorithms to reorder triangles with the triangle neighbors adjacent in the mesh file and loaded in memory is important for ml model performance to predict the lowest cvode execution times the graph root does not necessarily need to be the watershed outlet but triangle types 2n 1e that form the stream network often improve the cvode performance to predict cvode execution times where most of hydrological physics computation performance occurs within pihm it is possible with acceptable accuracy ranges to use ml models dependent on the variable selection with the trial 4 dataset only two of the four graphs edfs reverse and iedfs reverse met these criteria against all triangle types except 3n 1e and 2n 2e using the decision tree dt random forest rf and gradient boosted tree gbt models using the trial 4 dataset is the simplest way to predict cvode execution times and reflects the importance of triangle types with the lowest identifications often being triangles at streams however by comparing the overall accuracy ranges with trial 3 it is not the only factor in predicting cvode times the trial 3 dataset had comparable results to trial 4 with only four triangle types to select and two ml models gbt was the only model with 95 100 accuracy green line with the 1n 0e triangle type and edfs reverse and iedfs reverse graphs the trial 3 dataset had the overall worst performance amongst the four trial datasets as triangle identifications the highest correlated variable was missing however the ability of the ml models to predict cvode execution times is not poor either due to including the elevation head variables hence the trial 3 dataset requires further investigation as explained in the future direction section recall the only difference between trials 2 and 3 datasets is trial 2 contains the triangle identification variable however the trial 2 dataset overall performance is higher than trial 3 indicating triangle types often lower identifications indicate triangles at streams improves predicting lowest cvode execution times using the trial 2 dataset with these four chosen graphs meant no 95 100 model accuracy between ml model and triangle type combinations while with 90 95 accuracy orange line the naïve bayes nb deep learning dl and gbt models with most triangle types except all is appropriate with the four selected graphs the trial 1 dataset with twenty two variables has many combinations compared to the other three trial datasets the blue connections representing 100 accuracy occur using the nb linear regression lr and dl models with all triangle types except not touching nt with the four best performing graph search algorithms this is problematic as three of the best four graphs of the lowest cvode execution times were nt types to match table 3 nt results requires using the nb generalized linear model glm lr and dl models with a 95 100 accuracy for the iedfs reverse dfs reverse and edfs reverse graphs algorithms however the bfs and nt combination occurs with a ml model accuracy less than 90 for all the trial datasets this raises new challenges described in the future direction section 5 limitations and constraints this research focused on comparing all graph search algorithm combinations with triangle types which meant tests needed to be small and fast however this constraint means it is unknown if the performance discussed here will persist for the entire model simulation furthermore the supplied calibration files that remain fixed for these trials which did produce identical results for short simulations may no longer be valid for the graph reordered meshes for long simulations initial testing of longer simulation periods indicates larger cvode execution time differences between graph combinations and is part of future direction research to provide improved options to end users of hydroterre expert system as discussed in section 2 measuring cvode execution times is sensitive to the compute environment with appropriate compute resources executing these evaluations from hundreds to thousands of times would certainly improve these results the results presented here serve as a benchmark to understand the impact on different graph reordered combinations to improve meshes for hydrological modeling 6 conclusion the goal of this research is to improve the workflows within the hydroterre expert system that prepares input datasets to execute pihm with distributed compute resources to achieve this goal mesh qualities need refinement to scale from hill slopes to major river basins this research has focused on using a benchmark calibrated mesh with a few thousand triangles to understand the performance impact of reordering the mesh with graph search algorithms a total of ten graphs using five graph search algorithms and their reverse counterparts were evaluated to identify which graphs generated the lowest cvode execution times all graph search algorithms improved the overall performance of pihm as the mesh input file was reordered to have mesh triangles with their triangle neighbors near each other which improves cache memory access when computing the hydrological physics to distinguish which graph search algorithms had better performance than others it was necessary to classify triangle types a total of six to find the best graph root locations root triangles near the watershed outlet those that formed the stream network and graphs that selected these regions first had better execution or running times however this is not always the case as it also depends on the graph search algorithm as the best performing time reported was with a triangle not touching the stream network and selected the inner portion of the watershed first nevertheless based on using the machine learning models to understand patterns and predict the lowest cvode execution times choosing triangles near the stream network will improve performance after collecting pihm performance datasets from the ten graph search algorithms the datasets were partitioned by triangle types to train and classify seven machine learning ml models four trial datasets were used per triangle type to identify which ml models predicted pihm execution times the first trial dataset with 22 variables generated the highest accuracy the triangle identification was the most important and highly correlated variable supporting the concept of using graphs to reorder triangles and triangle types influences execution times although removing this variable trial 3 reduced the ml accuracy the elevation heads served as another important variable in predicting the lowest cvode execution times after examining different graph search algorithms triangle types and ml model combinations the end user is recommended to use four graph search algorithms breadth first search bfs implicit edge depth first search iedfs reverse depth first search dfs reverse and edge depth first search edfs reverse as these graphs generated the lowest execution times consistently the appropriate ml model to use with these four graph search algorithms depends on the trail dataset to generate the highest accuracy levels using the first trail dataset naïve bayes linear regression and deep learning models are recommended to predict the lowest cvode execution times using these strategies will improve the meshes generated by the hydroterre expert system and they are important steps towards analyzing spatially distributed unstructured grid watersheds with multi millions of triangles 7 future direction this research focused on demonstrating that graph search algorithm selection has an impact on pihm s execution times and finding appropriate machine learning models to predict where to define a graph root with a mesh containing a few thousand triangles and a short simulation test meant results can be generated fast for a large combination of trials triangle types graph search algorithms and machine learning models based on these results the next phase is to refine the methodology used here to address these issues 1 do the best performing graph search algorithms improvement to pihm continue with decade long simulation periods will this filter the graph search algorithm choices for end users 2 will the best performing graph search algorithm choices remain the same after minor changes to elevation values to the benchmark mesh by keeping the triangle total fix but replicating edits often done by modelers will the trial 3 performance results improve 3 do the best performing graph search algorithms apply to other watershed locations if they do not which variables are significant can these variables be used to classify watersheds and optimize graph reordered meshes 4 do these techniques apply to other models such as rhessys if they do are the same variables in 3 applicable to address whether computational properties are more important to improve model performance versus selecting by hydrology physical attributes 5 what happens with dynamic graph changes for example evaluating meshes modified by green infrastructure simulations and models such as le pihm and pihm wetland where streams are dynamically changing both spatially and with time zhang et al 2015 zhangli et al 2018 zhang et al 2018 6 will model physics during computational rigorous events such as a major storm affect graph performance will the variables in the trial 3 dataset be more important changing which graph and machine learning model combinations to use for end users of hydroterre 7 does changing the triangle count alter the graph search algorithm performance for example using mesh decomposition techniques to increase triangle counts to simulate a major storm path 8 are there performance improvements to pihm execution times by creating graph searches that allow users to specify start and end designations for example a graph that selects the lowest elevation to the highest elevation first before selecting the remaining watershed triangles in the watershed domain furthermore another approach to optimize meshes for hydrological modeling is to analyze the images representing the graph spatial distances accessible in appendix b movies can combining image recognition techniques with visual analytics and machine learning models be used to select meshes without executing hydrological models finally by investigating the above issues the results will guide the next generation of multilevel mesh workflows leonard 2018 to scale the hydroterre expert system from hill slopes to major river basins the following are the supplementary data related to this article bfs leonard mov1 iedfs rev leonard2 edfs rev leonard mov3 dfs rev leonard mov4 edfs 15fps edfs rev 15fps idfs 15fps idfs rev 15fps iedfs 15fps iedfs rev 15fps appendix docx supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2019 03 023 software availability name hydroterre developer dr lorne leonard department of civil engineering department of computer science and engineering and institutes of energy and the environment the pennsylvania state university contact information lorne leonard 245 agricultural sciences and industries building the pennsylvania state university university park pa 16802 usa software required internet browser later versions are recommended program language c c python arcgis availability and cost any user can access hydroterre web applications at no cost at http www hydroterre psu edu contact developer availability with software benchmark calibrated watershed model supplied by dr lele shu postdoctoral researcher dept land air and water resources university of california davis available here https github com happynotes conestogadata tree master mesh 8042 meshes reordered by graph search algorithms developed by dr lorne leonard available here https github com leonard psu machine learning models tree master graphsearchalgorithms data examples conestoga acknowledgements i thank dr lele shu postdoctoral scholar at uc davis for sharing the conestoga watershed inputs that served as the benchmark mesh also thank you to the three anonymous reviewers appendix a supplementary data the following is are the supplementary data to this article appendix appendix appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 023 
26171,is it possible to predict the execution time of a spatially distributed hydrological model by only examining the mesh this article investigates this question by using a benchmark mesh with the penn state integrated hydrologic model pihm the benchmark mesh triangles are reordered using ten different graph search algorithms that treat each mesh triangle as a graph root to select the remaining triangles in the watershed domain pihm then executed these graph reordered meshes to create performance datasets to find which graph search algorithm and triangle root combinations improved pihm s execution time the performance datasets were used to train and classify seven different machine learning ml models to predict the fastest execution times analyzing these ml results facilitated a strategy for end users of the hydroterre expert system to choose meshes that improve execution times for their hydrological science research with pihm graphical abstract image 1 keywords meshes hydroterre pihm machine learning models graph search algorithms distributed hydrological model 1 introduction to accurately represent water dynamics within watersheds that incorporates landscape features such as green infrastructure designs in distributed hydrologic models often computational meshes need to be 1 m or less to represent these features leonard et al 2019 this leads to a large number of triangles 10 000 s to 10 000 000 triangles even for a small study watershed that increases the overall computational costs leonard 2018 these costs are compounded further when watersheds need to be calibrated numerous times to match observation datasets hence modelers initially choose to use coarser resolutions with low triangle counts to minimize the computational costs and refine progressively the mesh to match the capabilities of their high performance computing hpc resources what if the modeler could predict which mesh is more efficient before executing their distributed hydrological model these same issues affect expert systems such as hydroterre http www hydroterre psu edu leonard and duffy 2013 leonard 2015 leonard and duffy 2016 to improve the performance of hydroterre requires strategies to create high quality meshes before using the meshes in a variety of environmental modeling this research is a step towards addressing these common modeling issues by devising strategies to predict the hydrological model execution time or running time based on mesh characteristics alone before using the mesh here the penn state integrated hydrologic model pihm qu 2004 qu and duffy 2007 pihm and penn 2014 is used to demonstrate issues with meshes found amongst other spatially distributed hydrological models using polygon meshes as input datasets a watershed model calibrated to observation datasets by an expert modeler will serve as the benchmark mesh the mesh file is reordered by graph search algorithms with the remaining input datasets fixed to measure the reorder impact on execution times with pihm this is followed by using machine learning ml models to train and classify datasets to predict which graph ordered meshes generated the fastest execution times for pihm 1 1 the following are the primary research contributions using a benchmark unstructured grid mesh ten different graph search algorithms were used to reorder the mesh triangles to identify which of the six different triangle types that form the mesh improved the execution time of the pihm model this research identified that the watershed outlet triangle s is not necessarily the graph root triangle with the lowest execution times but graph root triangles that touch or form the stream network had consistently the best running times all ten different graph search algorithms improved the execution times of pihm as reordering triangles improved cache memory access from the ten graph algorithms studied breadth first search bfs implicit edge depth first search iedfs reverse depth first search dfs reverse and edge depth first search edfs reverse consistently reordered meshes with the lowest execution times execution times from the ten different graph search algorithms combined with the benchmark mesh properties were used to train and classify seven different machine learning models to predict the lowest execution times four trial datasets were examined removing variables to understand their influence to predicting running times from the seven machine learning models studied combined with the four best graph search algorithms naïve bayes linear regression and deep learning models are recommended to predict the lowest execution times this article is structured as follows section 1 summarizes the hydrological model used to evaluate meshes reordered by graph search algorithms and how this research is coupled with the hydroterre expert system section 2 explains the methods used to modify the benchmark mesh using graph search algorithms followed by describing the performance measurements recorded for predicting mesh performance with ml models section 3 discusses the performance results of modifying the meshes and summarizes the ability for ml models to predict execution times with these graph reordered meshes section 4 demonstrates a strategy for end users to choose from different mesh properties graph search algorithms and ml models to predict with three accuracy ranges an appropriate graph reordered mesh for their hydrological modeling science objective 1 2 spatially distributed watershed models pihm uses a spatially distributed unstructured grid with a finite volume formulation pihm s unstructured grid is kept as a plain text mesh input file that can be transformed and be used by a variety of environmental models the strategies demonstrated in this research are also applicable to other hydrological models including rhessys leonard et al 2019 tague and band 2004 rhessys 2017 that use a mesh to represent the land surface for the governing multi physics equations used by pihm to calculate water and energy refer to qu 2004 qu and duffy 2007 pihm and penn 2014 for further details applying these equations to a finite volume strategy leads to a semi discrete system of local ordinary equations ode assembled together to form a global system of odes that is then solved using cvode solver in the sundials suite of solvers hindmarsh et al 2005 designed for stiff nonlinear systems within pihm the unstructured mesh and domain decomposition uses triangle software shewchuk 1996 shewchuk 1997a shewchuk 1997b cheng et al 2012a for pihm users to construct quality numerical meshes that can be constrained to follow or preserve important features of the watershed domain for pihm this includes preserving watershed or catchment boundaries for example level 12 hydrological unit codes huc the smallest watershed delineated by usgs leonard and duffy 2014a usgs 2013 nhd 2013 mckay et al 2012 and to capture stream networks such as those available from the national hydrography dataset nhd nhd 2013 mckay et al 2012 using these datasets with triangle results in a triangle irregular network tin whose edges are constrained by the watershed boundary and stream network for pihm users to capture the watershed domain and stream network with fine detail the tin triangles need to be small as demonstrated in leonard 2018 to capture stream curvature and other land use details such as green infrastructure designs that result in large multi million triangle counts these large triangle counts are problematic with models such as pihm as they are computationally expensive to solve and difficult to predict compute times if they are possible to resolve at all therefore most pihm modelers create watershed tin domains with a minimum number of triangles i e hundreds to a few thousand triangles that are faster to compute to meet their hydrological science objective based on their previous experiences and expertise hence for models such as pihm to use multi million triangles it is necessary to investigate computational strategies to create optimal hydrological meshes with choices so modelers can select meshes with thousands to multi million of triangles appropriate to their scientific objective the focus of this research is to improve running time performance by developing automated strategies with meshes before using them in hydrological models doing so will reduce time spent by hydrological modelers to identify mesh issues and is a step towards predicting compute times for models such as pihm automating these strategies is important as manual strategies often taken by pihm modelers are not feasible for meshes containing hundreds of millions of triangles as demonstrated in leonard 2018 these large triangle count meshes are a key step towards evaluating hydrological models from hill slopes to major river basins which is a major objective of the hydroterre expert system and is explained next 1 3 hydroterre overview hydroterre is a heterogeneous distributed compute environment using end to end workflows finely tuned fig 1 a to retrieve essential terrestrial variables etv s including elevation soils climate geology rivers land use at multiple level hydrological unit code huc scales to create calibrate and visualize watersheds leonard and duffy 2013 leonard 2015 leonard and duffy 2016 hydroterre provides rapid web service access to 250 terabytes of etv national datasets at a level 12 huc scale within a minute leonard et al 2016 the etv data workflow fig 1b creates a downloadable data bundle that are the minimum geographic information system gis datasets for many hydrological models and are appropriate datasets for other environmental models leonard 2015 leonard and duffy 2016 leonard et al 2018 the data model workflow fig 1c retrieves the etv data bundle and transforms the gis datasets to pihm model inputs based on four parameters that control the watershed boundary stream topology and in turn controls the quality of the unstructured mesh leonard 2018 leonard and duffy 2014b using the hydroterre expert system pihm models for the 83 016 level 12 hucs delineated within the continental united states conus have been executed millions of times with the end to end workflows fig 1a e by analyzing these workflow results pihm failed due to delineation issues with the watershed boundaries e g clipping streams and digitization problems with the stream networks e g inconsistent flow directions etv transformation of these two datasets accounted for 31 91 failure within the data model workflow leonard 2015 within the model workflow that retrieves the data model workflow to execute pihm using distributed compute resources 71 17 failed due to a poor quality mesh created with stream geometry where pihm s physics never converged using the cvode solver leonard and duffy 2014b leonard and duffy 2014a leonard and duffy 2014c leonard 2015 to improve these hydroterre end to end workflow results and to create services that will enable users to select dynamic number of level 12 hucs anywhere in the conus the large graph visual analytic workflow fig 1f enables users to validate watershed boundaries and stream networks using hydroterre web services leonard and duffy 2014a leonard et al 2017 the multilevel mesh workflow fig 1g retrieves elevation datasets validated stream networks and huc graphs from hydroterre services to create quality meshes that minimize sliver creation and other mesh issues specific to hydrological modeling leonard 2018 the focus of this manuscript fig 1h j is to provide hydroterre users strategies to evaluate whether these meshes improves running times for hydrological modeling using the methods outlined in the next section 1 4 related work ml models are becoming well established in the hydrological science community for a variety of science objectives including to predict hydrological flow series using extreme learning machine elm deo and sahin 2016 deo and şahin 2016 atiquzzaman 2018 atiquzzaman and kandasamy 2018 worland et al 2018 worland et al 2018 compared eight ml models e g gradient boosting machine gbm random forest rf and four baseline models to improve predictions of hydrological low flow indices in ungauged basins duerr et al 2018 duerr et al 2018 used gbm rf and bayesian additive regression trees bart with time series datasets to improve forecasting of household level water demand deep learning dl models have been used to forecast daily reservoir inflow bai et al 2016 calibration testing on hydrological numerical data and benchmarks marçais and de dreuzy 2017 and used for rainfall runoff simulations hu et al 2018 for a review on dl models and its relevance for water resource science see shen 2018 shen et al 2018 mesh generation is an extensively studied field and used by many disciplines for a description on delaunay mesh generation algorithms the readers are directed to research by shewchuk 2008 shewchuk 2008 and cheng 2012 cheng et al 2012b with research on mesh generation and spatially distributed hydrological modeling readers are directed to heinzer et al 2012 heinzer et al 2012 who used triangle software to generate tins for the integrated water flow model iwfm 2019 marsh et al 2018 marsh et al 2018 used multi objective unstructured mesh generation to capture and preserve spatial heterogeneity of etvs e g soil vegetation required for spatially distributed hydrological modeling graph search algorithms is broadly used and for related research in using these algorithms in hydrology readers are referred to castronova 2014 castronova and goodall 2014 who used graphs to process watershed boundaries and nhd stream networks cui et al 2009 cui et al 2009 applied graph theory to design and identify major flow paths for river channel networks halverson 2015 halverson and fleming 2015 applied network analysis tools to an assortment of streamflow gauges to inform streamflow network monitoring design 2 methods to predict reordered graph mesh performance using machine learning models this section explains the methods used to create and modify hydrological meshes used as pihm mesh inputs section 2 1 describes the benchmark mesh used for refinement and analysis section 2 2 outlines the mesh reordering process using graph search algorithms before pihm execution section 2 3 discusses the measurements and data collected when executing pihm with these graph reordered meshes the ml models will use this running time data to predict pihm execution times as summarized in section 2 4 the last section describes the procedure to predict which triangle types and graph search algorithms are the best to use for hydrological modeling using unstructured grid meshes 2 1 benchmark mesh to examine the execution times of the meshes reordered by graph algorithms a benchmark mesh is necessary for comparison this research uses the mesh generated by dr lele shu using triangle software shu 2017 the mesh was used to investigate the conestoga watershed 1270 sq km located in pennsylvania to study land use change with pihm shu 2017 triangle is the method used in the data model workflow fig 1c to create pihm meshes constrained by catchment boundaries and stream networks dr shu manually fine tuned and calibrated his water research with this conestoga mesh against observation datasets after many time consuming iterations dr shu tuned the pihm input files and all these input files remained fixed in this research except the mesh and river input files fig 1h in this manuscript reordering the mesh implies modifying the river file as well keeping the input files fixed is important to understand the impact of only changing the mesh the motivation is to understand the impact of reordering the mesh triangles and to device strategies to handle multi million triangle counts generated by the multilevel mesh workflow fig 1g the results described in this manuscript will guide further research with the large triangle count meshes without executing pihm numerous times the benchmark mesh contains 8042 triangles and 1412 river segments as shown in appendix a and is considered a large triangle count compared to previous pihm studies pihm and penn 2014 the pihm input datasets are available at this link 1 1 https github com happynotes conestogadata tree master mesh 8042 the mesh triangles in appendix a and with the detail in fig 2 have unique identification integer numbers 1 8042 and at first appear random at the watershed scale with closer examination most triangles along the stream network have lower identification numbers that are not consecutive in order meaning triangle identifications adjacent to each other are not sequential the non sequential labelling of triangles is expected behavior from triangle software as triangle first creates triangles along the stream network a break line constraint that meets user supplied arguments to control angles and area sizes followed by recursively creating triangles along the stream network before creating triangles within the remaining watershed domain often triangle identifications are large in central areas of the watershed that are the furthest away from the watershed boundary and stream networks to understand the impact of these triangle formations and discover any potential strategies in reducing execution times the triangles were classified table 1 by how they form or do not form along the stream networks briefly these triangle types include the number of nodes touching the stream network and the number of stream edges formed by the triangle fig 2 shows these classification triangle types near the watershed outlet of the benchmark mesh triangles shaded in green have no nodes touching the stream network while triangles with one or more nodes touching the stream network are blue appendix a shows these triangle classifications for the entire watershed domain 2 2 reordering mesh with graph algorithms as fig 2 demonstrates and was explained in the previous sub section the mesh unique identification numbers are not sequential when comparing the triangle neighbors in the benchmark mesh file the triangles are written in sequential order 1 8042 but as shown in fig 2 a triangle neighbors identification are often not similar to have neighboring triangle identifications close to each other in the mesh file graph search algorithms reorder the triangles in the mesh input file for example from fig 2 a possible sequence would be 105 117 4135 106 and so on in the graph reordered mesh file to evaluate the impact of changing these sequences each of the benchmark triangles were designated as the root or origin triangle and the graph search algorithm selects the remaining triangles assigning distance to triangles from the root triangle for example selecting triangle unique identification 105 watershed outlet in fig 2 as the graph root the remaining 8041 triangles are searched for with distances calculated to the origin the original mesh geometry e g vertices nodes edges and triangle connections remains the same for example triangle 106 its neighbors or triangle connections are 4135 3779 and 103 these shared edge connections remain the same what changes is how the graph reordered mesh is stored in the mesh data file and consequently how the data is loaded into memory this graph reordering step has been calculated for each of the 8042 triangles to compare the impact of various graph roots and graph search algorithms to the execution times of pihm to address the following questions does it matter if the triangle identifications with its neighbors are close to each other in the mesh input file should the graph root be a watershed outlet triangle which triangle types perform better than others to reorder the mesh each triangle is a graph node or vertex with each triangle neighbor s stored as graph edges with each triangle the following five graph search algorithms were used to select the remaining triangles briefly these algorithms are 1 breadth first search bfs the root node is expanded first then all the successors of the root node are expanded next then their successors and so on all the graph nodes are expanded at a given depth in the search tree before the nodes at the next level are expanded russell and norvig 2010 2 depth first search dfs dfs always expands the deepest node in the current edge of the search tree the search proceeds immediately to the deepest level of the search tree where the nodes have no successors as those nodes are expanded they are dropped and the search steps back to the next deepest node that has unexplored successors russell and norvig 2010 3 edge depth first search edfs edfs is a variant of dfs algorithm where edges are selected rather than the node 4 implicit depth first search idfs idfs is a dfs algorithm for implicit directed graphs the implicit directed graph is derived by the mesh triangles 5 implicit edge depth first search iedfs iedfs is an edfs algorithm for implicit directed graphs the implicit directed graph is derived by the mesh triangle edge connections furthermore the above five graph search algorithms were searched in reverse order meaning the selected triangle graph root was the furthest distance to compare pihm running times fig 3 illustrates how the five different graph search algorithms and their reverse counterpart used in this research affect triangle selections at the watershed outlet based on triangle 105 shown in fig 2 outlet location appendix b has web links to access movies demonstrating the graph search algorithm impact to the complete watershed domain for all combinations access to the graph reordered meshes used in this manuscript is available here 2 2 https github com leonard psu machine learning models tree master graphsearchalgorithms data examples conestoga 2 3 measurements recorded when executing pihm with reordered meshes to measure the execution time impact of graph search algorithms that reordered mesh triangles in memory the average number of instructions retired per cycle ipc were tracked when executing pihm all these tests executed on one server as described in appendix c to simplify performance comparison cycle refers to the central processing unit cpu cycle and is the time required for retrieving and executing one simple machine instruction an ipc value of one is generally considered an acceptable value for hpc software applications furthermore to gain insight how the graph algorithms performed when executing pihm measuring the following time measurements was necessary 1 total time representing the total amount of time from beginning to end to execute pihm this includes read pihm input files execute the hydrological physics equations and write result files each test uses the same input files att forc ibc soil geol lc para calib init except the mesh and river input files mesh riv that the graph search algorithms modified as described in section 2 2 2 read time time spent reading the pihm input files the total amount of memory required is identical with each test as only the mesh and river files data are rearranged furthermore all tests used the same disk appendix c therefore the total read time should be nearly identical to other read tests with only minor differences any test results where significant differences occurred i e greater than a few seconds were redone as there are outside influences these outside influences include operating system services i e virus scanner update checks etc disk latency or disk failures and are difficult to block 3 write time time spent writing pihm output results again each test result should be similar to other write tests test results with significant differences were redone to reduce outside influences 4 cvode time the amount of time spent computing pihm s hydrological physics using the cvode solver by comparing the original benchmark mesh result time with the graph modified meshes low cvode execution times indicate that modified meshes influence pihm in a positive way while high cvode execution times indicate a negative influence similar times indicate no influence by the modified graph algorithm meshes ideally adding the read write and cvode times should be identical to the total time recorded due to cpu and disk latency and operating system influences i e memory leaks other software there are minor time differences to minimize disk and cpu latency a timeout of 3 s was imposed between tests in execution scripts to allow briefly cpu idle status any erroneous tests i e negative values or large values with about 100 occurring were re evaluated based on the following formula 5 number of graph algorithms 2 forward and reverse search 8042 number of triangles in mesh domain a total of 80 420 pihm graph reordered mesh tests with re evaluations took a month on one server to calculate results each test executed pihm with one week of simulation data it was enough data and simulation period to stress the cpus on the server and generate results but fast enough to run each pihm test within a minute to understand these running time results for the graph reordered meshes generated by the graph search algorithms section 2 2 with ml models section 2 4 a new dataset is necessary based on the benchmark mesh that combines multiple static properties of pihm input files see pihm and penn 2014 leonard and duffy 2014b for further details to be joined based on the unique triangle identification with the above running time results table 2 summarizes these properties briefly these include geometry features not required by pihm input files but created for these trials calc method and physical attributes in method applied to pihm triangles i e soil geology landcover and climate attributes there will be four trial datasets used by the ml models as described in the next sub sections 2 4 machine learning ml models with the datasets collected as described in section 2 3 ml models are necessary to identify patterns patterns may occur within individual graph search algorithm results and those that may happen when comparing different graph algorithms and triangle types using the trial datasets these algorithm and ml model types are necessary to search for answers to the following questions is it possible to predict cvode times based on a graph reordered mesh are there any specific triangles that outperform i e lowest cvode execution times other triangles consistently if so what features or attributes do they share for example a common land use type or geometry properties see table 2 do the ml models behave differently between the pihm graph search algorithm results if there are differences will they affect the hydrological modeling decision process the following seven ml models were used to train classify and analyze the 80 420 tests to predict cvode execution times with meshes reordered by the graph search algorithm techniques 1 naïve bayes a simple and fast probabilistic classifier based on bayes theorem where the fundamental assumption given the value of any attribute is independent of the value of any other attribute this assumption is rarely true but is computationally inexpensive and the algorithm often works well the gaussian probability density is used to model the attribute data russell and norvig 2010 2 generalized linear model glm generalization of linear regression models estimate regression models for outcomes following exponential distributions cook 2017 h2o ai 2018 nykodym et al 2016 3 logistic regression uses glm with a binary classification h2o ai 2018 nykodym et al 2016 4 deep learning based on a multi layer feed forward artificial neural network trained using a stochastic gradient descent using back propagation for learning non linear relationships cook 2017 phan et al 2017 deshpande and kotu 2015 5 decision tree finds simple tree like models which are easy to understand the structure resembles a tree with a collection of nodes intended to create a decision on values relationship to a class or an estimate of a numerical goal value cook 2017 6 random forest is an ensemble algorithm creating more than one tree model with the model results combined cook 2017 7 gradient boosted machine complex model using ensembles of decision trees using forward learning ensemble methods to obtain predictions using gradually improved estimations click et al 2017 2 5 procedure to predict which triangles and graph algorithms are the best for hydrological modeling fig 4 summarizes the procedure to predict which triangles have the lowest cvode execution times based on the graph reordered mesh as described in section 2 2 reordering the benchmark mesh takes place using graph search algorithms before executing pihm fig 4a each mesh triangle served as the graph root to select the remaining triangles as explained in section 2 3 pihm performance data for each mesh will help explain graph search algorithm achievements fig 4b these performance datasets are joined with mesh properties to create new datasets that are partitioned by triangle type formed along stream networks or not section 2 1 as summarized in fig 4c initial analysis of the variables collected indicate strong correlations that do not improve the ml models and these variables required refinement fig 4d for example low cvode execution times indicate higher ipc values and lower total times recorded hence the last two variables are not necessary to improve ml model performance table 2 identifies which variables the four trial datasets fig 4e used and their meanings the ml models used these four trial datasets to predict cvode execution times the first trial used all 22 refined variables the second trial selected the highest correlated variables to support the lowest cvode execution times again removal of variables occurred if their correlation did not improve the ml models for example soil and geol have the same impact thus geology was not necessary and removed the third trial removed triangle identification often the highest correlation in trials one and two to compare the impact of this variable to predict cvode execution times finally the last trial only used cvode time and triangle identification variables for each of these four trial datasets tables were sorted by cvode times in ascending order these tables did not match the benchmark sequence of triangles 1 8042 recall the objective is to evaluate how well the ml models can predict the lowest cvode execution times in other words which mesh input improved pihm running time the ml models section 2 4 evaluated used two strategies to train 80 of dataset and classify the datasets to fit the parameters of the classifier with remaining 20 of dataset fig 4f 1 two equal sizes with no data clipping of high outlier cvode times with the first partition containing the lowest cvode execution time values and the second partition has the remaining values 2 two equal ranges with no data clipping this means to split cvode times between the lowest and highest cvode times recorded to create two dataset partitions hence the number of items in the first range does not necessarily have the same number of items in the second range with both strategies documentation included ml model classification accuracy to predict the lowest cvode execution times fig 4g and whether the ml model predicted range one lowest cvode times without optimization this is followed by optimizing the ml model to predict range one fig 4h to simplify summarizing these results full results in appendices described in the next section the partition strategy used was based on which strategy that best captured the diversity of cvode execution times fig 4i for example if high cvode time outliers shifted the division location of two equal ranges meaning range one had the majority of data often reflecting high model accuracy then method one two equal sizes was selected for discussion in situations where neither strategy was sufficient data clipping of high cvode time outliers was necessary to re evaluate the model accuracy there was no clipping of low cvode time outliers as the objective of this research is to understand why this happens finally with the trial four datasets the ranges of triangle identifications representing the most accurate result by the ml model was identified to analysis the spatial location of these triangles within the watershed fig 4j due to the substantial number of combinations there was no documenting of triangle ranges for trials 1 3 datasets as discussed further in the next section 3 results and discussion section 3 1 summarizes the benchmark mesh results and performance of meshes using graph search algorithms with pihm the ml models analyzed the trial datasets to predict the best performing triangle type and graph search algorithm combinations before and after optimization section 3 2 discusses the overall performance while section 3 3 focuses on specific model and graph highlights to formulate a plan for end users to determine an appropriate course of action for their hydrological modeling objective section 3 4 3 1 benchmark mesh result and summary of using graph search algorithms the total time to execute pihm with the triangle benchmark mesh where triangle identifications in the mesh input file are written sequentially 1 8042 but triangles listed above or below in the mesh input file are often not adjacent triangles ranges from 30 to 39 s the ipc ratio range was 0 69 0 76 and the cvode running time ranges from 23 71 to 28 16 s as expected these results have time variability as explained in section 2 3 while using the methods described in section 2 2 graph search algorithms reordered the benchmark mesh before executed with pihm the transformation changed the mesh and river files required by pihm while the other pihm input files remained fixed each mesh triangle a total of 8042 served as the graph root node to search for the remaining triangles fig 5 summarizes the cvode execution times measured fig 4b using the techniques described in section 2 3 the edfs reverse algorithm recorded the lowest total time of 18 09 s and dfs reverse algorithm recorded the longest total time of 28 44 s the average total time with all graph algorithms was 20 92 s 10 s faster than the benchmark mesh result the lowest cvode execution time recorded was 13 82 s with the idfs reverse algorithm about 10 s faster than the benchmark the largest cvode execution time recorded was 22 1 s with the dfs reverse algorithm approaching the benchmark mesh results the average cvode execution time with all graph search algorithms was 16 37 s about 7 s faster than the benchmark mesh results factoring in time variability these averaged results show that the graph search algorithms have improved pihm execution times recall pihm used one week of simulation data and assuming this improvement is consistent for long simulation periods e g hundreds of years it will reduce many hours to days of compute resources required by pihm table 3 examines the best performing individual triangle for each of the graph search algorithms the left side shows the highest ipc values and the right side the lowest cvode execution times two of the graph search algorithms the bfs reverse and dfs reverse algorithms had matching triangle identifications and performance for high ipc and lowest cvode execution times ideally all graphs would match as described in section 2 5 there is a positive correlation between high ipc values and low cvode execution times but the highest ipc values do not necessary produce the lowest cvode times for example the bfs graph search algorithm at triangle 5323 had the highest ipc value of all 80 420 pihm mesh tests but was 2 74 s slower than the best bfs result examining timed tasks within pihm for example printing values to files are slightly higher than other results due to operating system interference that accumulate to the slower result therefore as described in section 2 5 the ml models fig 4d did not use the total time and ipc values instead the results focused on cvode execution times where most of hydrological physics calculations occur and has the largest contribution to the total time to execute pihm furthermore the triangle type section 2 1 mapped to each of the triangle identifications in table 3 indicates from this sample no specific triangle type preference for the best performing cvode execution times per graph search algorithm instead further examination of all triangles per graph is necessary the first technique used to analyze all triangle types after sorting cvode execution times in ascending order was to mark the index location after finding all the triangle types index location one had the lowest cvode execution time and location 8042 has the worst cvode execution time for example there are 104 3n 1e triangle types if 3n 1e dominated the lowest times then with a best case scenario the index location would be close to 104 a worst case scenario would be close to the total number of triangles briefly all graph search algorithms selected nt 1n 0e and 2n 1e triangle types after 8000 indicating this type is not clustered early these three triangle types dominate the watershed count 7449 hence it makes sense these are not clustered for the remaining triangle types a total of 593 the 2n 0e triangle type index location was 7604 for the iedfs reverse graph algorithm with dfs reverse having the worst index location at 8040 with the 3n 1e triangle type the best result was from the idfs graph algorithm which found all these triangle types at index 4605 finally the 3n 2e triangle type the idfs graph algorithm found all these types at index 4965 unfortunately these results do not distinguish any graph search algorithm and triangle type combination that would support end users for an optimal mesh the second technique was to understand the distribution of triangle types as they change by clipping the sample size of the lowest cvode execution times the lowest sample clipped was 258 to match the 2n 0e triangle count see table 1 in a best case scenario either one of the 2n 0e 3n 1e and 3n 2e triangle types would dominate this sample as their totals are less than or equal to 258 the next clipping sample sizes are 500 1000 2000 3000 and ending with 4021 being half of the triangle total and match range one see section 2 5 these results are available in appendix d and fig 6 summarizes these results using parallel coordinate diagrams per triangle type based on the nt triangle type fig 6 the edfs and iedfs graph search algorithms consistently have a high ratio from 258 to 2000 index locations after index 2000 bfs reverse had the highest ratio while the remaining graph algorithms ratio became lower with the 1n 0e triangle type the ratios vary with edfs the ratio peaks at 500 then decreases while with the dfs and iedfs ratios 1n 0e is high initially decreases at index 500 then increases again both the idfs iedfs reverse graph algorithms have high ratios with the 2n 0e triangle type with the edfs and iedfs graphs the ratio is low from 258 to 2000 and slowly increases the ratios by the graph search algorithms in the 2n 1e triangle type is the opposite to the nt triangle type the ratios are low for the edfs increases after index 2000 iedfs bfs and bfs reverse lowest at 2000 graphs the other graph search algorithms have a high ratio that were low in the nt triangle type this pattern occurs due to how the graph search algorithms select the triangles and with average cvode execution times described in the next subsections are important for selecting the best performing graph search algorithm the same pattern occurs with the 3n 1e and 3n 2e triangle types with the edfs reverse graph having the highest ratios preferably all triangle types would be identical with the lowest cvode execution times and other mesh characteristics see table 2 such as the angles that form the triangle as they do not ml models are necessary to clarify which variables are important to classify and predict the lowest cvode execution times fig 4e these results demonstrate it is advantageous to use graph search algorithms to reorder neighboring triangle identifications in the mesh input file rather than the sequential method used in the benchmark mesh see section 2 1 which does not write neighbor triangles adjacent to each other in the mesh file a major contribution to improving pihm s performance is storing a triangle s neighboring triangle data adjacent in memory thus the physics equations can minimize memory misses in cache memory while accessing neighbor triangles data hager and wellein 2010 however the positive correlation between ipc cvode and total times has variability that do not distinguish graph search algorithms for end users to select on performance merit alone the next section examines the impact of triangle classification to predict cvode execution times using ml models 3 2 summary of predicting cvode execution times the results for all graph search algorithms and ml model combinations per triangle types for the trials 1 4 datasets is accessible at appendix e this section focuses on the average accuracy of how well the ml models after optimization predicted the graph reordered mesh lowest cvode execution times before optimization results and discussion are available in appendix e fig 7 a summarizes the trials 1 4 results for triangle types against the graph search algorithms after optimization the overall average accuracy increased to 86 99 from before optimization of 67 15 the range of average accuracy varies from a minimum of 76 5 using 2n 1e at trial 3 towards 93 39 using 3n 2e with the trial 1 dataset comparing all triangle types per trial trial 1 uses twenty two variables had the highest accuracy value of 92 52 the optimization procedure took advantage of minor correlations in the multiple variables to help improve ml model accuracy with the trial 1 datasets this contrasts with the results before optimization where trial 4 with two variables had the best results triangle identification often had the highest correlation illustrated by the better results in trials 1 2 and 4 in both before and after optimization by examining the triangle identification locations with the highest accuracy in the trial 4 datasets see appendix f there was a pattern to the ml model accuracy with dfs and dfs reverse graph algorithms most ml model accuracy was higher at low triangle identifications often triangles at streams with bfs most higher ml accuracy values occurred with lower identifications with some occurring in central and end locations with bfs reverse higher accuracy ml values occurred at high triangle identifications which means the triangle graph root started in the upper central watershed locations but often triangles near the outlet are written first in the mesh file and loaded into memory first see appendix b the other graph search algorithms behavior with similar patterns overall there is an advantage to select lower triangle identifications often found along the stream networks see sections 2 1 and 2 2 for the lowest cvode execution times and at times kept in lower memory locations improving compute calculations although the trial 3 dataset with no triangle identification had the lowest average results in both before and after optimization it shows the positive and important correlation between the outletzmin and outletzmax variables see table 2 that represent hydraulic and elevation heads of the watershed these issues in the future direction section will be re examined since these ml models have been optimized for range one rather than determining the number of ml models that selected range one the number of 100 accuracy results were tabulated appendix e9 overall 24 49 generated 100 results with all trials and triangle type combinations trial 3 with 2n 1e triangle type had the lowest result of 2 86 and trial 1 3n 2e triangle type recorded the largest number of 100 with 55 71 the 3n 2e result is misleading as the number of triangles is small and has low triangle identifications meaning these triangles are often formed early by triangle 2n 1e has identifications ranging from 4 to 8038 indicating these triangle types are created throughout the triangle process hence supporting the idea of selecting triangles along the stream network is important that may not necessarily have low triangle identifications but this depends on the graph search algorithm used again trial 1 performed the best 48 78 and trial 3 the worst with 9 8 number of 100 accuracy results indicating that the extra variables are necessary to end users for choosing a graph search technique to improve predicting the lowest cvode execution times fig 7 b summarizes the trials 1 4 results for ml models against triangle types after optimization the average accuracy per ml model varied from 82 85 rf to 93 15 using the gbt algorithm the lowest average of 76 50 occurred at trial 3 with the decision tree dt algorithm and the highest value of 99 61 happened at trial 1 using the dl algorithm appendix e10 compares the number of 100 accuracy results per ml model zero number of 100 results were found for trial 1 using the rf algorithm and naïve bayes at trial 3 the highest result recorded was 91 43 using lr for trial 1 again this demonstrates the need to strategize when choosing between triangle type graph search algorithm and ml models 3 3 ml and graph search algorithm highlights to predict cvode execution times the results from section 3 2 focused on the overall averages after optimization this section focuses on finding specific highlights about which ml models and graph search algorithms to use or not for each trial dataset per triangle type these highlights are necessary to direct end users to which methods to use after optimization as summarized in section 3 5 a discussion of before optimization results with highlights is available in appendix e after optimization the ml models predict range one with an assortment of accuracies see appendix e2 10 to simplify explaining these results defining three accuracy ranges will address most end users modeling needs as shown in fig 8 the first range is a perfect score of 100 highlighted in blue the second range is 95 100 shown in green and the last range is 90 95 displayed as orange the cells colored pink indicate scores below 90 for these combinations of ml models against the triangle types per trial dataset the trial 1 dataset is the only dataset with a perfect score using three ml models nb lr dl with all triangle types except the nt triangle type six of the thirteen perfect scores recorded used the lr model however lr scores below 90 occur with the remaining trial datasets and triangle type combinations hence end users wanting to use the logistic regression lr model should do so only with the trial 1 dataset the remaining perfect scores occurred with the deep learning total of four and naïve bayes total of three models twenty two of the machine learning models and trial dataset combinations resulted in range two 95 100 both the naïve bayes nb and gradient boosted trees gbt models generated seven range two results with nb generating these scores in trials 1 and 2 only trials 3 and 4 scored below 90 gbt scores between 95 and 100 across all trial datasets and is the only algorithm to do so in trials 3 and 4 the generalized linear model glm recorded four range twos deep learning dl three logistic regression lr one all using the trial 1 dataset twenty of the thirty eight range three 90 95 results occurred with the gradient boosted trees model gbt is the only model to score above 90 in all trail and triangle type combinations except in trial 4 with triangle type 3n 1e the remaining range three results happened with deep learning a total of eight seven in the trial 2 dataset both the decision tree and random forest only with trial 4 dataset resulting with four range threes and finally two using glm with the trial 1 dataset at first with a total of 27 ranges 1 to 3 scores with most trial and triangle type combinations using gbt is encouraging for users however 20 of these occurred in range three only while the deep learning model had 4 perfect scores three 95 100 scores and eight 90 95 scores with trials 1 to 3 datasets a total of 15 ranges 1 to 3 finally the naïve bayes model with a total of ten ranges 1 to 3 had three perfect scores and seven range two scores both the deep learning and naïve bayes models are suitable dependent on the trial dataset used by the end user the following reviews the optimized results with individual graph search algorithms against the triangle types per trial dataset range one blue cells does not apply here with no perfect score recorded additionally the iedfs graph algorithm did not generate scores higher than 90 for any triangle type and trial dataset combination highlighted in pink within fig 9 therefore the recommendation is to not use the iedfs graph by users nor would the bfs and dfs reverse graphs with the trials 3 and 4 datasets the idfs edfs reverse and iedfs reverse graph search algorithms using the nt datasets recorded range two accuracies 95 100 green cells with all trial datasets furthermore the bfs reverse graph algorithm resulted with range two with the 3n 1e and 3n 2e triangle types for all trial datasets contradicting the before optimization results hence these four graph and triangle types after optimization are appropriate to end users range three 90 95 total cell count results are similar amongst the graphs however range two total cell counts are distinguishable idfs has the highest total of 19 followed by the bfs reverse dfs iedfs reverse and edfs reverse graphs in descending order 4 strategies to predict the lowest cvode execution times as the previous sections have discussed there are many choices for end users to select by triangle type graph search algorithm and ml models that predict cvode executable times with acceptable accuracy ranges to narrow user options the graphs overall performance was ranked by selecting the lowest cvode execution times at different index locations see table 3 from ten choices to four graphs the four graphs selected based on consistently low times green lowest at index locations are bfs iedfs reverse dfs reverse and edfs reverse fig 10 summarizes the combinations for these four graph search algorithms against triangle types and ml models per trial dataset appendix g shows all combinations as described in section 3 2 these four graph search algorithms have a higher ratio of 2n 1e triangle types that form the stream network with the lowest execution times it should be noted this graph selection does not include the idfs reverse graph algorithm which recorded the lowest cvode execution time but was not consistently high at other index locations however the best performing triangle for idfs reverse graph is a 2n 1e triangle type near the watershed outlet a pattern common with all the graph algorithms hence to answer the questions posed in sections 2 2 and 2 4 using graph search algorithms to reorder triangles with the triangle neighbors adjacent in the mesh file and loaded in memory is important for ml model performance to predict the lowest cvode execution times the graph root does not necessarily need to be the watershed outlet but triangle types 2n 1e that form the stream network often improve the cvode performance to predict cvode execution times where most of hydrological physics computation performance occurs within pihm it is possible with acceptable accuracy ranges to use ml models dependent on the variable selection with the trial 4 dataset only two of the four graphs edfs reverse and iedfs reverse met these criteria against all triangle types except 3n 1e and 2n 2e using the decision tree dt random forest rf and gradient boosted tree gbt models using the trial 4 dataset is the simplest way to predict cvode execution times and reflects the importance of triangle types with the lowest identifications often being triangles at streams however by comparing the overall accuracy ranges with trial 3 it is not the only factor in predicting cvode times the trial 3 dataset had comparable results to trial 4 with only four triangle types to select and two ml models gbt was the only model with 95 100 accuracy green line with the 1n 0e triangle type and edfs reverse and iedfs reverse graphs the trial 3 dataset had the overall worst performance amongst the four trial datasets as triangle identifications the highest correlated variable was missing however the ability of the ml models to predict cvode execution times is not poor either due to including the elevation head variables hence the trial 3 dataset requires further investigation as explained in the future direction section recall the only difference between trials 2 and 3 datasets is trial 2 contains the triangle identification variable however the trial 2 dataset overall performance is higher than trial 3 indicating triangle types often lower identifications indicate triangles at streams improves predicting lowest cvode execution times using the trial 2 dataset with these four chosen graphs meant no 95 100 model accuracy between ml model and triangle type combinations while with 90 95 accuracy orange line the naïve bayes nb deep learning dl and gbt models with most triangle types except all is appropriate with the four selected graphs the trial 1 dataset with twenty two variables has many combinations compared to the other three trial datasets the blue connections representing 100 accuracy occur using the nb linear regression lr and dl models with all triangle types except not touching nt with the four best performing graph search algorithms this is problematic as three of the best four graphs of the lowest cvode execution times were nt types to match table 3 nt results requires using the nb generalized linear model glm lr and dl models with a 95 100 accuracy for the iedfs reverse dfs reverse and edfs reverse graphs algorithms however the bfs and nt combination occurs with a ml model accuracy less than 90 for all the trial datasets this raises new challenges described in the future direction section 5 limitations and constraints this research focused on comparing all graph search algorithm combinations with triangle types which meant tests needed to be small and fast however this constraint means it is unknown if the performance discussed here will persist for the entire model simulation furthermore the supplied calibration files that remain fixed for these trials which did produce identical results for short simulations may no longer be valid for the graph reordered meshes for long simulations initial testing of longer simulation periods indicates larger cvode execution time differences between graph combinations and is part of future direction research to provide improved options to end users of hydroterre expert system as discussed in section 2 measuring cvode execution times is sensitive to the compute environment with appropriate compute resources executing these evaluations from hundreds to thousands of times would certainly improve these results the results presented here serve as a benchmark to understand the impact on different graph reordered combinations to improve meshes for hydrological modeling 6 conclusion the goal of this research is to improve the workflows within the hydroterre expert system that prepares input datasets to execute pihm with distributed compute resources to achieve this goal mesh qualities need refinement to scale from hill slopes to major river basins this research has focused on using a benchmark calibrated mesh with a few thousand triangles to understand the performance impact of reordering the mesh with graph search algorithms a total of ten graphs using five graph search algorithms and their reverse counterparts were evaluated to identify which graphs generated the lowest cvode execution times all graph search algorithms improved the overall performance of pihm as the mesh input file was reordered to have mesh triangles with their triangle neighbors near each other which improves cache memory access when computing the hydrological physics to distinguish which graph search algorithms had better performance than others it was necessary to classify triangle types a total of six to find the best graph root locations root triangles near the watershed outlet those that formed the stream network and graphs that selected these regions first had better execution or running times however this is not always the case as it also depends on the graph search algorithm as the best performing time reported was with a triangle not touching the stream network and selected the inner portion of the watershed first nevertheless based on using the machine learning models to understand patterns and predict the lowest cvode execution times choosing triangles near the stream network will improve performance after collecting pihm performance datasets from the ten graph search algorithms the datasets were partitioned by triangle types to train and classify seven machine learning ml models four trial datasets were used per triangle type to identify which ml models predicted pihm execution times the first trial dataset with 22 variables generated the highest accuracy the triangle identification was the most important and highly correlated variable supporting the concept of using graphs to reorder triangles and triangle types influences execution times although removing this variable trial 3 reduced the ml accuracy the elevation heads served as another important variable in predicting the lowest cvode execution times after examining different graph search algorithms triangle types and ml model combinations the end user is recommended to use four graph search algorithms breadth first search bfs implicit edge depth first search iedfs reverse depth first search dfs reverse and edge depth first search edfs reverse as these graphs generated the lowest execution times consistently the appropriate ml model to use with these four graph search algorithms depends on the trail dataset to generate the highest accuracy levels using the first trail dataset naïve bayes linear regression and deep learning models are recommended to predict the lowest cvode execution times using these strategies will improve the meshes generated by the hydroterre expert system and they are important steps towards analyzing spatially distributed unstructured grid watersheds with multi millions of triangles 7 future direction this research focused on demonstrating that graph search algorithm selection has an impact on pihm s execution times and finding appropriate machine learning models to predict where to define a graph root with a mesh containing a few thousand triangles and a short simulation test meant results can be generated fast for a large combination of trials triangle types graph search algorithms and machine learning models based on these results the next phase is to refine the methodology used here to address these issues 1 do the best performing graph search algorithms improvement to pihm continue with decade long simulation periods will this filter the graph search algorithm choices for end users 2 will the best performing graph search algorithm choices remain the same after minor changes to elevation values to the benchmark mesh by keeping the triangle total fix but replicating edits often done by modelers will the trial 3 performance results improve 3 do the best performing graph search algorithms apply to other watershed locations if they do not which variables are significant can these variables be used to classify watersheds and optimize graph reordered meshes 4 do these techniques apply to other models such as rhessys if they do are the same variables in 3 applicable to address whether computational properties are more important to improve model performance versus selecting by hydrology physical attributes 5 what happens with dynamic graph changes for example evaluating meshes modified by green infrastructure simulations and models such as le pihm and pihm wetland where streams are dynamically changing both spatially and with time zhang et al 2015 zhangli et al 2018 zhang et al 2018 6 will model physics during computational rigorous events such as a major storm affect graph performance will the variables in the trial 3 dataset be more important changing which graph and machine learning model combinations to use for end users of hydroterre 7 does changing the triangle count alter the graph search algorithm performance for example using mesh decomposition techniques to increase triangle counts to simulate a major storm path 8 are there performance improvements to pihm execution times by creating graph searches that allow users to specify start and end designations for example a graph that selects the lowest elevation to the highest elevation first before selecting the remaining watershed triangles in the watershed domain furthermore another approach to optimize meshes for hydrological modeling is to analyze the images representing the graph spatial distances accessible in appendix b movies can combining image recognition techniques with visual analytics and machine learning models be used to select meshes without executing hydrological models finally by investigating the above issues the results will guide the next generation of multilevel mesh workflows leonard 2018 to scale the hydroterre expert system from hill slopes to major river basins the following are the supplementary data related to this article bfs leonard mov1 iedfs rev leonard2 edfs rev leonard mov3 dfs rev leonard mov4 edfs 15fps edfs rev 15fps idfs 15fps idfs rev 15fps iedfs 15fps iedfs rev 15fps appendix docx supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2019 03 023 software availability name hydroterre developer dr lorne leonard department of civil engineering department of computer science and engineering and institutes of energy and the environment the pennsylvania state university contact information lorne leonard 245 agricultural sciences and industries building the pennsylvania state university university park pa 16802 usa software required internet browser later versions are recommended program language c c python arcgis availability and cost any user can access hydroterre web applications at no cost at http www hydroterre psu edu contact developer availability with software benchmark calibrated watershed model supplied by dr lele shu postdoctoral researcher dept land air and water resources university of california davis available here https github com happynotes conestogadata tree master mesh 8042 meshes reordered by graph search algorithms developed by dr lorne leonard available here https github com leonard psu machine learning models tree master graphsearchalgorithms data examples conestoga acknowledgements i thank dr lele shu postdoctoral scholar at uc davis for sharing the conestoga watershed inputs that served as the benchmark mesh also thank you to the three anonymous reviewers appendix a supplementary data the following is are the supplementary data to this article appendix appendix appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 023 
26172,this study presents a dynamic forecast combination approach adapted to incorporate multiple sources of precipitation dynamic combination serves to utilise the varying merit each data source exhibits with time the dynamic model combination framework presented merges a nonparametric k nearest neighbour k nn estimation of radar precipitation with thin plate spline tps interpolated gauge precipitation since air temperature is an essential variable to discriminate the phase of the precipitation in cold climates this study uses radar precipitation and air temperature as the two variables in the dynamic combination algorithm the merging of k nn and tps estimates is shown to reduce the rmse by 25 compared to the original radar precipitation rates the usefulness of air temperature is found not to be as significant in the combination as it is in the formulation of the nonparametric radar precipitation fields for cold incident temperatures keywords merging radar and gauge precipitation dynamic model combination nonparametric radar precipitation k nearest neighbour spatial precipitation and temperature cold climate precipitation 1 introduction continuous simulations of streamflow and catchment water storage using distributed hydrological models require accurate precipitation input at high spatial resolution syed et al 2003 smith et al 2004 beven 2012 it is often shown that existing precipitation networks in many places are not dense enough to capture the spatial variation of precipitation events kirchner 2009 hwang et al 2012 precipitation measurement by remote sensing has a great potential to fulfill the need for distributed input data measured at a catchment scale krajewski and smith 2002 woldemeskel et al 2013 presently weather radars provide precipitation estimates with high spatial at a standard resolution of 1 km 1 km and temporal hourly and sub hourly resolution however weather radar measures precipitation indirectly using remote sensing techniques and measurements are subject to several sources of errors and uncertainties chumchean et al 2006b villarini and krajewski 2010 berne and krajewski 2013 abdella 2016 weather radar transmits electromagnetic waves and measures the energy backscattered by the hydrometeors in the atmosphere as reflectivity the reflectivity is then converted to ground precipitation rates typically parametric power law relationship often called as z r equation z a r b is used in the conversion of radar reflectivity z to ground precipitation rates r the constant values of a 200 and b 1 6 derived by marshall and palmer 1948 for rain are generally used regardless of climate region however these parameters are not constant wilson and brandes 1979 and they are related to drop size distribution of hydrometeors in the atmosphere drop size distribution varies with the type and phase of the precipitation joss et al 1990 uijlenhoet 2001 chumchean et al 2008 due to errors and uncertainties in the measurement of reflectivity as well as in the conversion of reflectivity to ground precipitation advances and advantages of radar precipitation data are not fully used in widespread hydrological applications so far chumchean et al 2003 berne and krajewski 2013 to model radar rainfall uncertainty ciach et al 2007 used the nonparametric kernel regression method such nonparametric approaches can be more effective than parametric alternatives as fewer assumptions about the processes being modelled needs invoking while sufficient observational data exist silverman 1986 mehrotra and sharma 2006 while ability to adapt to the data locally is a strength of the nonparametric approaches they can result in local biases due to outliers villarini et al 2008 compared their nonparametric approach with a copula based method and found that the performance was as equal or higher than copula hasan et al 2016b presented a kernel based nonparametric method to improve radar rainfall estimates using the conditional probability distribution of past observed reflectivity and gauged rainfall sivasubramaniam et al 2018 extended the hasan et al 2016b univariate nonparametric approach of radar precipitation estimates to bivariate with the use of near surface air temperature as a covariate to improve the radar precipitation estimates in cold climates accurate estimates of precipitation data are required for engineering design and management in small basins particularly with the intense local rainfall events where the hydrological processes impelled by rainfall rates play a relatively higher role than the hydraulic processes of flood wave propagation krajewski and smith 2002 weather radar with its extended spatial coverage can monitor many small catchments that otherwise remain ungauged without any precipitation observations berne and krajewski 2013 moreover as precipitation is an intermittent variable and various difficulties in measuring precipitation precisely accurate estimation of spatial distribution of precipitation in the river basin using sparse gauge network alone is a challenging task hwang et al 2012 as a result distributed and physically based hydrological models are often limited by the accurate spatial variability of precipitation input data syed et al 2003 to obtain the spatial distribution of precipitation in a catchment from available gauges which are often unevenly distributed spatial interpolation techniques are required a range of spatial interpolation techniques are available in the literature from simple methods e g arithmetic mean nearest neighbour thiessen polygon and inverse distance weighting to advanced and more complex approaches e g multiple linear regression thin plate smoothing splines kriging genetic algorithms conditional bias penalized kriging and copula a review of different interpolation methods is presented by hwang et al 2012 where the gauge is a single sensor of precipitation even with the use of advanced spatial interpolation techniques it is a challenging task to get a distributed precipitation field in a sparsely gauged area it is often shown that compared to gauges radars capture the spatial variation of precipitation relatively well despite errors in their quantitative information several studies chumchean et al 2006a haberlandt 2007 goudenhoofdt and delobbe 2009 have shown that merging of radar precipitation estimates with gauge precipitation can improve precipitation estimates the merging approach is as old as the arrival of weather radar data for hydrological applications the focus of earlier studies has been to correct the bias in the radar precipitation estimation using gauge observations mean field bias mfb correction is a simple bias correction method that is broadly used the mfb method assumes a uniform multiplicative error in the radar estimates and it estimates the ratio of the accumulated radar precipitation and accumulated gauge precipitation from a number of radar gauge pairs as a multiplicative adjustment factor brandes 1975 proposed a correction factor at each gauge location with subsequent interpolation over the radar field chumchean et al 2006a applied kalman filter to improve the mfb estimates subsequent focus on merging radar and gauge data has utilised the spatial variability of the radar to further improve the spatial interpolation of gauges several merging methods with the use of geostatistical techniques with different degree of complexity have been proposed some of the geostatistical merging methods in literature are cokriging krajewski 1987 kriging with external drift ked berndt et al 2014 conditional merging sinclair and pegram 2005 and copula based assimilation vogl et al 2012 the geostatistical methods generally consider gauge as a primary source and radar as secondary source for the merging goudenhoofdt and delobbe 2009 however rabiei and haberlandt 2015 showed that quality of the quantitative radar data is still important factor in conditional merging hasan et al 2016a b argue that if errors associated with the precipitation field derived from radar and gauge can be quantified correctly the two data sources can be merged without abandoning the quantitative information from the radar this approach can be promising in sparsely gauged regions because intensity information from radar is to be extracted by extending the error structure identified from gauged regions to ungauged regions combination of two sources of information using error variance has its basis in economic forecast combination bates and granger 1969 when two set of forecasts are combined the resulting forecast can have lower mean square error than both original forecasts bates and granger 1969 the forecast combination approach has been applied in hydrometeorology wasko et al 2013 along with other areas the forecast combination methodology is prevalent in combining seasonal forecasts from multiple climate models these studies have reported the usefulness of dynamic weighting in combination instead of simple static combination chowdhury and sharma 2009 2011 devineni and sankarasubramanian 2010 khan et al 2014 kim et al 2016 in the dynamic combination the combination weights change over time to capture temporal variation locally hasan et al 2016a evaluated different combination approaches in the context of merging radar and gauge rainfall data they found that covariance based methods gave better results compared to non covariance based methods and showed the usefulness of dynamic approach hasan et al 2016b presented a covariance based dynamic model combination framework to combine radar and gauge rainfall data sources in their study a kernel based nonparametric approach was used to estimate rainfall estimates and then nonparametric radar rainfall estimates were merged with copula based spatially interpolated rainfall field for a tropical climate in the dynamic model combination similar events were identified using a k nearest neighbour approach with reflectivity as a single variable to estimate the error covariance matrix and weights in cold climates precipitation occurs with different phases snow rain or a mixture of snow and rain and similar events can be specified not only based on the intensity information but also the phase of the precipitation several studies have shown that air temperature is intrinsic to the phase of the precipitation auer jr 1974 killingtveit 1976 rohrer 1989 fassnacht et al 2001 air temperature is an essential variable to differentiate two similar events with different phase snow or rain but the same intensity al sakka et al 2013 fassnacht et al 1999 2001 used the near surface air temperature observations to adjust the radar precipitation estimation sivasubramaniam et al 2018 showed that the use of air temperature as a second covariate in the k nearest neighbour k nn nonparametric model reduces the root mean squared error significantly and improves the radar precipitation estimates in colder temperatures as similar to the sivasubramaniam et al 2018 approach air temperature can also be used as an additional variable to identify similar events within the dynamic model combination framework to further improve the combination estimates in cold climates the overall objective of this study is to improve the precipitation estimates with high spatiotemporal resolution the primary focus is to merge radar precipitation field with existing gauge observations to generate improved continuous hourly precipitation field for the region for that first we adjust the hourly radar precipitation rates using the k nearest neighbour nonparametric method and then we use the dynamic model combination framework to merge the nonparametric estimates with spatially interpolated precipitation gauge data using thin plate spline tps interpolation we evaluate whether the use of air temperature as an additional variable can be useful for dynamic model combination as is found for the nonparametric estimation 2 material and methods 2 1 study area the study area for the current research is 100 km radius from hurum radar station in norway an area of about 31000 km2 as shown in fig 1 the hurum radar station is located at 59 63 n latitude and 10 56 e longitude and it is about 30 km from the norwegian capital city oslo it is a c band installation with a wavelength of 5 319 cm and a coverage radius of approximately 240 km the hurum radar has been in operation since november 2010 and monitors the southeastern part of norway and part of sweden the coverage area consists of six norwegian counties where nearly 40 of the population live within the study area 94 precipitation gauges are in operation with available hourly precipitation data for the study the precipitation gauges are tipping bucket gauges most of them are lambrecht and nivus rm 202 brands and weighing gauges geonor brand with an alter windshield fig 1 shows the precipitation gauge locations overlaid on the topography of the study area seven gauges were left from the computations in order to verify the approach at ungauged pixels in fig 1 the 87 gauges which were used in the computation are displayed with blue circles and the 7 gauges with red looking at fig 1 precipitation gauges are not evenly distributed a relatively dense network of gauges exists near urban areas along the coast and a sparse network of gauges exist in the rest of the area further it can be seen that the inland mountainous areas remain mostly ungauged without precipitation gauges with hourly measurements mean annual precipitation in the study area is between 1000 and 1500 mm based on the climatology for the period 1961 to 2017 http www senorge no the precipitation in the study region can broadly be divided into three categories frontal orographic and convective precipitation the convective precipitation is most dominant during the warmer summer months the annual mean temperature is in the range of 2 10 c in the study region the january mean is near freezing at the coast and down to 10 15 c inland the maximum summer temperature is mostly in the 20 25 c range 2 2 data at present the norwegian meteorological institute met no operates nine c band doppler weather radar installations including the hurum radar station the norwegian radar network covers the entire land surface of norway and they scan the atmosphere with a 7 5 min temporal resolution with this resolution being 15 min until june 2013 the raw radar volume scan from the radar stations are processed and quality controlled by met no and then met no generates and distributes various radar products to end users elo 2012 one of the products from met no is surface rainfall intensity sri the mosaics of nine radars sri data covering entire norway are available for the public to use in practical applications the quality of the measured reflectivity can vary from pixel to pixel due to the nature of remote sensing measurements topography and distance from the radar station affect the radar measurement of precipitation and introduce errors these include anomalous propagation ground clutter beam blocking and attenuation germann and joss 2004 in a mountainous region like norway the mountains can cause partial or total beam blocking abdella et al 2012 reported that beam blockage for hurum radar affects the eastern and south eastern part of its coverage fig 1 and the beam blockage is up to 30 moreover in high latitude cold climates non uniform vertical profile of reflectivity vpr and bright band effects in the vpr introduce major uncertainties in the radar precipitation estimation abdella 2016 koistinen and pohjola 2014 the reflectivity measurements go through a chain of processes to address the above mentioned errors and uncertainties the process at met no first removes clutter and other non meteorological echoes from the radar scan and then gaps in the data caused by clutter are reconstructed after that volumetric reflectivity data are segmented as a convective or stratiform type of precipitation next the processing algorithm computes vpr according to the precipitation type to generate sri product met no uses the lowest plan position indicator ppi here the aloft reflectivity data is projected down to a reference height 1 km near to the ground and the projection known as vpr correction that takes variability in the vertical profile of reflectivity vpr and bright band effect into account elo 2012 the met no applies a single z r relationship marshall and palmer 1948 relationship z 200 r 1 6 to convert the 7 5 min or 15 min before june 2013 reflectivities into precipitation rates the precipitation rates are then accumulated to hourly and distributed as end user hourly radar precipitation rate sri product the accumulated hourly radar precipitation rate product sri product was used in the present study the gauged precipitation data used in the study are from the gauges operated by met no the met no undertakes the calibration of gauges and essential measures to reduce the uncertainty in the measurements further met no performs routine quality control before being released to the data portal for the public however met no does not correct the precipitation data for wind induced undercatch the precipitation measurements from the gauges are available with varying length as some gauges have been operated since 2013 or later and there are missing values during their operation as well gridded air temperature and wind speed datasets covering norway are available from met no lussana et al 2016b spatially interpolated the past temperature observations from the norwegian meteorological stations using an optimal interpolation in a bayesian setting to develop a temperature dataset for norway the gridded hourly wind speed dataset was derived by statistical downscaling from the 10 km numerical dataset nora10 combined with the arome 2 5 km numerical dataset a local quantile regression method was used for this statistical downscaling the wind speed data were required in this study to correct wind induced undercatch of gauge precipition the datasets for the study were downloaded and prepared as follows radar and gauge precipitation and meteorological data air temperature and wind speed for the period from january 2011 to may 2015 were used for this study the gridded hourly radar precipitation rates air temperature and wind speed data with 1 km 1 km spatial resolution for the study area were downloaded from met no s thredds server http thredds met no the gridded data are in netcdf file format in utm33n projection the hourly precipitation gauge measurements and gauges meta information were obtained from met no s data portal eklima http eklima met no the gauge locations were overlaid on the 1 km 1 km regular grids of the gridded data and the pixel of 1 km 2 overlapping each gauge was located one location near oslo has three gauges within a 1 km 1 km pixel but except for that all pixels consist of a single gauge the pixel value for each hour was extracted and continuous hourly time series of radar precipitation rate air temperature and wind speed for all gauges were generated solid precipitation exhibits significant undercatch due to high wind conditions in high latitude and mountainous regions wolff et al 2015 in this study gauged precipitation data were corrected for wind induced undercatch by using the nordic precipitation undercatch correction model førland et al 1996 the correction model classifies the precipitation phase as solid liquid and a mixture of two phases using air temperature and the model uses two sets of equations one for the solid precipitation and other for the liquid precipitation an average value of the two equations is used for the mixed precipitation these equations require wind speed and air temperature at gauge location in this study the gridded hourly wind speed and air temperature data were used for the undercatch correction hereafter gauge precipitation refers to undercatch corrected precipitation throughout the study the precipitation intensities in the study area are relatively low consistent with intensities in high latitude boreal climates sivasubramaniam et al 2018 a study of statistical properties of precipitation intensities in mid norway showed that precipitation rates less than 6 mm h 1 yields 88 of the total precipitation volume while less than 1 76 mm h 1 yields 50 moreover the same analysis found that precipitation rates below 0 1 mm h 1 contributes little to the total precipitation volume and might be regarded as zero precipitation engeland et al 2014 in this study gauge precipitation and radar precipitation rate less than 0 1 mm h 1 were assumed as zero precipitation at each gauge location the timesteps with gauge precipitation or radar precipitation rate less than 0 1 mm h 1 were removed and an observed dataset of hourly gauge precipitation and corresponding radar precipitation rate and air temperature was prepared it can be noted that the length of dataset at each gauge location can vary due to the availability of gauge precipitation records to analyse the distribution of gauge and radar precipitation used in this study observed datasets gauge and radar precipitation and air temperature at gauge locations were pooled and histograms were plotted as shown in fig 2 looking at fig 2 it is visible that radar underestimates the precipitation compared to gauge observation moreover for intense events radar precipitation rates show a considerable negative bias for both temperatures colder than and warmer than 10 c it can be noted that high intensity hourly precipitation events are rare in the study region only 0 03 of the total gauge hours used in the analysis have gauge precipitation intensity above 20 mm h 1 while 0 31 have intensity above 10 mm h 1 in the pooled dataset from the gauge locations the maximum observed gauge precipitation was 45 mm h 1 while maximum radar precipitation rate was 35 mm h 1 however analysis of radar precipitation rates from the entire set of pixels 46656 showed that there are radar pixels with extremely high intensity values a histogram showing the distribution of radar precipitation rates pooled from the entire set of radar pixels is available in the supplement fig s1 the intensive values above 50 mm h 1 are likely due to hail or a mixture of very heavy precipitation and hail 2 3 methodology 2 3 1 framework the two sources of quantitative precipitation from the radar and gauges are combined within a dynamic model combination framework as an overall description the flow diagram in fig 3 illustrates the data used and the methods applied in the merging process merging with gauge precipitation must be applied as the last step in radar precipitation estimation process and all possible corrections should be applied first to improve the radar precipitation estimates before merging goudenhoofdt and delobbe 2009 the hourly radar precipitation rates were first adjusted within a nonparametric framework using gauge precipitation and air temperature observations sivasubramaniam et al 2018 since the gauge precipitation data are at gauge locations a spatial interpolation was applied to get precipitation values at grids precipitation values at grids from the two sources were then merged using the estimated combination weights within dynamic model combination framework each of the methods is described in detail in the following subsections 2 3 2 k nearest neighbour k nn estimation for the nonparametric estimation this study adopted the method presented by sivasubramaniam et al 2018 for the cold climatological setting where k nearest neighbour k nn nonparametric model with radar precipitation rate and air temperature as two covariates is used here a summary of the k nn method with air temperature as an additional covariate is presented for brevity more details of the k nn method of radar precipitation estimation can be found in sivasubramaniam et al 2018 readers are referred to sharma and mehrotra 2014 sharma et al 2016 hasan et al 2016b mehrotra and sharma 2006 for a description of the nonparametric framework conditional estimation of ground precipitation r e s t t for a given observed pair of radar precipitation rate r t and air temperature t t using past observed dataset of radar precipitation rate and air temperature and corresponding gauge precipitation within a k nn regression model is written as follows sivasubramaniam et al 2018 1 e r e s t t r t t t k 1 k g k k j 1 k 1 j in eq 1 k denotes the number of past observations radar precipitation rate and air temperature pairs which are considered similar to the current observation r t and g k denotes the gauge precipitation corresponding to the k t h neighbour in the past observations k is a maximum number of nearest neighbours permissible and it is taken as equal to the square root of the sample size as recommended by lall and sharma 1996 the order of k neighbours is ascertained using a weighted euclidean distance metric as expressed below 2 ξ i 2 β r r r i s r 2 β t t t i s t 2 where ξ i is the distance of the current observed pair r t to the i t h data point r i t i in the past observed dataset in a two dimensional space s r and s t denote sample standard deviations of the two predictors and β r and β t are partial weights of predictors and their summation is equal to one readers are referred to sivasubramaniam et al 2018 for further details about the use of partial weights in k nn estimation the k nn regression estimator available in the npred r package sharma et al 2016 available for downloading from http www hydrology unsw edu au download software npred was used for the nonparametric k nn estimation in this study an average partial weight of β r 0 68 β t 0 32 recommended by sivasubramaniam et al 2018 for the study region was used a continuous time series of radar precipitation rate and air temperature are on the regular 1 km 1 km grids for the k nn estimation at each pixel grid location the observed dataset gauge precipitation radar precipitation rate and air temperature from the three nearest gauges were pooled and used as past observations with the k nn regression to estimate precipitation k nn estimate for the timesteps with radar precipitation rate less than 0 1 mm h 1 the k nn estimates were set as zero 2 3 3 thin plate spline tps interpolation merging of radar precipitation field with gauges spatially in a regular grid require spatial interpolation of point gauge data for this study thin plate splines tps was chosen for the spatial interpolation of gauge precipitation data it can be noted that for spatially interpolated gauge precipitation to use in the combination method any of the interpolation techniques as listed in section 1 can be used if the estimates of variance in the fitted precipitation surface are available hasan et al 2016b the tps interpolation is a data driven nonparametric approach of locally weighted polynomial method the optimal number of neighbours around each target is determined using general cross validation gcv statistics hwang et al 2012 a number of studies hutchinson 1998a b tait et al 2006 woldemeskel et al 2013 have reported the application of tps method for spatial interpolation of gauge precipitation tps from the r package library fields v9 0 nychka et al 2017 available on the comprehensive r archive network cran was used to fit a thin plate spline precipitation surface from precipitation gauges in this study a description of the tps can be found in the r documentation https www rdocumentation org packages fields versions 9 0 topics tps in the tps the smoothing parameter is chosen by generalized cross validation 2 3 4 dynamic model combination at each pixel 1 km 1 km grid we have spatially estimated k nn radar precipitation estimates using a model with radar precipitation rate and air temperature as two predictor variables k nn for each hourly timestep we also interpolated gauge precipitation using tps to provide an alternate estimate that uses a different data source as a result all 46656 pixels in the study area have continuous time series of k nn and tps estimates the k nn and tps estimates have different estimation accuracy the estimation accuracy varies spatially from one pixel location to another as well as temporally at a given pixel location within a model combination framework the two estimates can be combined by weighting each method according to their past observed estimation accuracy bates and granger 1969 wasko et al 2013 hasan et al 2016b in this study the combination weight of each method is calculated from the error covariance matrix using residual errors corresponding to past observations to calculate the error we need a true observation of the estimated precipitation value at pixel locations coinciding with gauges corresponding gauge precipitation was taken as the true precipitation and k nn and tps errors were calculated this study used the leave one out cross validation loocv procedure to calculate these errors the k nn error at a gauge location was calculated by leaving out one observed response gauge precipitation from the k nn regression and estimating the expected response value for that observed response to obtain the error for each observation a matrix of observed gauge precipitation and corresponding precipitation rate and air temperature and k nn error e k n n was generated at each gauge location tps error was calculated using spatial loocv whereby leaving a gauge from spatial interpolation and estimating the tps response for each observed precipitation value allowed tps error to be calculated with reference to the observed precipitation value tps error matrix with observed precipitation and corresponding tps error e t p s was generated the k nn error matrix and tps error matrix were next merged to obtain an error matrix of radar precipitation rate and air temperature and corresponding k nn error e k n n and tps error e t p s at 87 gauge locations to merge the tps and k nn estimates combination weights were calculated at each pixel grid location x y for each hourly time step t as described follows each pixel can be associated with a radar precipitation rate r x y t air temperature t x y t and corresponding precipitation estimates using tps p t p s x y t and k nn p k n n x y t to estimate the combination weight error matrices from three nearest gauges for this pixel were pooled the pooled error matrix y r x y t x y e k n n x y e t p s x y consist of radar precipitation rate air temperature and corresponding estimated k nn and tps error from past observations the static combination approach uses entire observations to estimate error covariance matrix at a given pixel location in contrast the dynamic combination identifies a number of similar precipitation events to estimate error covariance matrix for each timestep at the pixel location here the k nearest neighbour k nn method with a euclidean distance metric was used to ascertain the neighbours and the number of maximum neighbours was equal to the square root of the sample size as recommended by lall and sharma 1996 hasan et al 2016b identified similar events using reflectivity as a single variable in contrast to their approach this study uses radar precipitation rate and air temperature as two variables to identify similar events using k nearest neighbour technique in the dynamic model combination algorithm to merge k nn and tps estimates the proposed combined product of k nn and tps is denoted as p c o m b r t for comparison a reference model combination of k nn and tps estimates using radar precipitation rate as a single variable to identify similar events in the dynamic model combination algorithm as similar to hasan et al 2016b were also developed p c o m b r for each time step for the given radar precipitation rate and air temperature pair at the pixel location r x y t x y t k number of similar r t pairs from the past observations in the matrix y were identified and hence corresponding k number of k nn error e k n n and tps error e t p s pairs were selected in the reference model for the given radar precipitation rate r x y t k number of similar r from the matrix y were identified to select k nn and tps error pairs an error covariance matrix e of the estimation errors was calculated for the selected error pairs e k n n e t p s and the covariance matrix can be written as follows 3 e σ t p s 2 ρ σ t p s σ k n n ρ σ t p s σ k n n σ k n n 2 here diagonals are the variance of the errors of each method and off diagonals represent covariance of the errors from the two methods where ρ is the correlation between errors from the two estimation methods the dynamic combination weight of the two estimates can be calculated by minimizing the quantity as shown in 4 such that the weights being constrained to lie between 0 and 1 and their summation is unity khan et al 2014 hasan et al 2016b for further details of the derivation of eq 4 readers are referred to khan et al 2014 4 min w e w here w w t p s w k n n and the summation w t p s w k n n is equal to 1 w t p s denotes the combination weight associated with tps estimates while the weight associated with k nn is denoted by w k n n the combined precipitation estimation p c o m b r t x y t is the weighted summation of k nn and tps estimates as follows 5 p c o m b r t x y t w k n n p k n n x y t w t p s p t p s x y t at pixel locations coinciding with gauges the estimates were obtained using leave one out cross validation loocv the loocv ensures the modelled outcomes p k n n p t p s p c o m b r and p c o m b r t are obtained from independent of gauge precipitation values those will be used to evaluate the modelled outcomes the k nn estimate was calculated by leaving out the current radar precipitation rate and air temperature and corresponding gauge precipitation from the past observed dataset in the k nn regression the tps estimate was obtained by leaving a gauge from spatial interpolation and estimating the tps response on that gauge location to calculate the weight for each timestep in the dynamic combination the error covariance matrix was formulated by excluding the k nn and tps error pair corresponding to the current k nn and tps estimates 2 3 5 model evaluation criteria several performance metrics have been used in literature to assess the performance of the models and compare them villarini et al 2008 hasan et al 2016b some of the metrics are root mean square error rmse mean absolute error mae and mean error me the rmse provides the overall performance measure of a predictive model hasan et al 2016b the study primarily used the rmse as a performance metric to evaluate the model performance to strengthen the evaluation additional performance metrics mae and me were also used definition of the performance metrics used in this study can be found in the published literature villarini et al 2008 bennett et al 2013 hasan et al 2016b 3 results 3 1 performance of dynamic combination the performance of the modelled outcomes was evaluated at gauge locations for the assessment we computed performance metrics rmse mae and me for the k nn tps and the combination estimates combr and combrt at 87 gauge locations for the timesteps with radar precipitation rate and gauge precipitation greater than 0 1 mm h 1 here gauge precipitation was taken as a true observed value to compute the performance metrics the performance metrics were also estimated for radar precipitation rates which was considered as a benchmark to compare the modelled estimates fig 4 shows a comparison of performance metrics computed at 87 gauge location for the precipitation estimates using different estimation methods and for the radar precipitation rates mp the nonparametric k nn estimation k nn leads to a considerable decrease in the rmse in the radar precipitation estimates the merging of k nn and tps estimates within a dynamic model combination framework reduces the rmse in the estimated precipitation field significantly the mean improvement in rmse for k nn is 15 0 from 1 3 to 1 1 mm h 1 while it is over 25 1 3 0 95 mm h 1 for the proposed dynamic model combination estimation combrt compared to the radar precipitation rates further almost all gauge locations exhibited clear improvement in estimates using dynamic combination approach looking at fig 4 tps estimates has a relatively smaller error than radar precipitation rates mp this displays the errors associated with the radar precipitation it can be noted that tps interpolation technique was the method of choice in this study and the focus of the study was not to assess the different interpolation methods to select for the model combination as shown in fig 4 c the mean error me of the radar precipitation rates mp which represents the bias in the mp was negative for almost all gauge locations this demonstrates the underestimation of radar precipitation compared to gauge precipitation the k nn estimation noticeably reduces the bias me in the radar precipitation to near zero from 0 44 to 0 01 mm h 1 while reducing the rmse and mae most of the gauge locations showed clear reduction in rmse and mae in the precipitation estimates by merging the two sources than the estimation k nn or tps using the data from a single sensor radar or gauge however the me associated with combination estimation mean value of 0 1 mm h 1 was slightly higher than k nn estimation while it is less than the tps estimates mean value of 0 2 mm h 1 the proposed merging method using air temperature as an additional variable in the dynamic combination algorithm combrt slightly improves the precipitation estimates compared to the reference model combr looking at fig 4 the improvement is not very high however more than 80 of the gauge locations rmse of combrt is lower than combr in addition we also evaluated the estimates at the 7 independent gauge locations to verify the results on ungauged pixel locations the gauge precipitation data from those control gauge locations have not been used in any of the estimations we found a similar result at these control gauge locations as for the study gauges a bar plot representing these performance metrics at 7 control gauge locations is shown in fig 5 while fig 4 presents the summary of performance metrics for the 87 gauges obtained by using loocv procedure the values of performance metrics estimated for each of those 7 control gauges are presented in fig 5 the combination approach reduces the rmse and mae for all seven gauge locations while it decreases the me in the radar precipitation rates for most of them and it resembles the summary result presented in fig 4 looking at fig 5 the magnitude of the performance metrics varies among gauges we investigated whether they are due to beam blockage however we could not find any spatial pattern in the magnitude of the errors at gauge locations here it can also be noted that the data length at each gauge location is not the same as mentioned in section 2 2 3 2 variation with temperature classes to investigate whether the improvement can vary with temperature ranges we computed performance metrics for the datasets with temperatures colder than or equal and warmer than 10 c and rmse and me are shown in fig 6 looking at fig 6 a for temperatures colder than or equal 10 c radar precipitation mp have a higher rmse than the tps interpolation this shows that the radar performance is poorer for colder temperatures than warmer there are relatively higher errors and uncertainties in the radar measurement of precipitation in cold temperatures for temperatures warmer than 10 c the overall rmse is higher for tps than for radar precipitation this can be due to tps spatial interpolation of gauge precipitation using available sparse gauges resulting in more error this is turn could be due to such events being local convective events where radars perform better fig 6 b shows the estimated bias me in the different estimation for the two temperature classes radar precipitation rates mp have substantial negative bias under estimation for both colder and warmer temperatures it can be noted that met no uses the single z r relationship marshall and palmer 1948 relationship for rain to convert the reflectivities to precipitation rates an inappropriate relationship z r relationship for rain instead of snow in the conversion can also result in phase dependent bias in the estimation for colder temperatures similar argument can be valid for the bias in the radar precipitation for temperatures warmer than 10 c where the single z r relationship cannot be appropriate for different types of rainfall orographic or convective as they have different raindrop size distribution uijlenhoet 2001 the nonparametric k nearest neighbour k nn estimation reduces the bias in the radar precipitation for both temperature classes looking at fig 6 b tps interpolation of gauge precipitation also resulted in negative bias at most of the gauge locations but it is considerably less than the bias in the radar precipitation for both estimates the magnitude of the bias is higher for warmer temperatures than colder this is because precipitation intensities of colder temperatures are relatively low and the values in the plot is not normalised the bias in the merged precipitation combr and combrt is lower than tps estimates however it is slightly higher than the k nn estimates the nonparametric k nearest neighbour model k nn reduces the rmse in the radar precipitation estimation significantly for temperatures colder than 10 c the k nn estimation still improves the radar precipitation estimation for warmer temperatures also both combination methods combr and combrt significantly reduces the rmse compared to any of the other estimations for both temperature classes for temperatures colder than 10 c the use of air temperature in the dynamic combination algorithm combrt results in marginal improvement compared to the reference model without air temperature combr and the performance is nearly same for temperatures warmer than 10 c 3 3 combination weights as mentioned earlier combination weights vary with space and time the dynamic assignment of weight ensures the provision of merit to the best method in the combination at any location for any particular time step hasan et al 2016b to comprehend the usefulness of dynamic model combination approach the spatiotemporal variation of combination weights was scrutinized fig 7 shows the spatial plot of combination weight associated with k nn at 87 gauge locations in the study area the circles represent the gauge locations and a discrete filled colour scale is used to show the combination weights assigned to k nn estimates it can be noted that the summation of weights w k n n w t p s is equal to 1 it is clearly visible from fig 7 that k nn gets lower weight yellow filled circles and hence tps gets more weight at densely gauged locations while k nn gets higher weight in the low density regions the circles filled with blue colour represents the gauged pixel locations where the k nn gets equal or higher weight compared to tps in the northern and western boundary of the study area with sparse gauges the highest average weight of more than 0 75 was assigned to the k nn estimates the result is consistent with hasan et al 2016b where they used a denser gauge network 282 tipping bucket gauges fig 7 shows the average combination weight for the entire timesteps at each gauge location in addition to spatial variation at any pixel location the combination weights also vary with time in this paper we illustrate the temporal variation for the four gauge locations which are marked with a b c d in fig 7 and the temporal variation is shown using a box and whisker plot in fig 8 the locations were chosen to represent the four classes of combination weight as listed in the legend of fig 7 the location a is in a densely gauged area and b is from a less dense gauged area while c and d are in a sparse gauged region looking at fig 8 the resulting combination weights of k nn are in a range of values between 0 and 1 for all four locations for the locations a and b the average weight of k nn is less than 0 5 but a number of hourly events still get a weight above 0 5 for k nn and vice versa for the locations c and d the dynamic model combination method assures the best data source k nn or tps in this case is chosen in the combination for each hourly event the proposed dynamic model combination uses air temperature as an additional variable to identify similar events to estimate the combination weight fig 9 illustrates the variation of combination weight assigned to the two estimates for the datasets with temperatures colder and warmer than 10 c at the gauge locations looking at fig 9 radar based k nn estimaton gets a relatively less weight than tps for air temperatures colder than 10 c for temperatures warmer than 10 c the k nn estimates get a higher average weight than tps for more than 50 of the gauge locations the overall aggregated average weight for k nn is 0 43 while 0 57 for tps 3 4 analysis of high intensity storm events fig 10 shows a histogram of radar and gauge precipitation and resulting estimates k nn and combination for high intensity hourly events at the gauge locations looking at fig 10 nonparametric k nn estimation results in lower estimation for the intensive events observed by gauges this can be because of lack of data points with intensive events in the past observed dataset to ascertain accurate estimates using a nonparametric method that is based on local regression enough number of past observations with similar intensities are required hasan et al 2016b the combination improves the precipitation estimates however combination estimates are also still lower than the gauge precipitation for higher precipitation rates to assess the modelled outcomes for individual storm events resulting raster images were made in order to visually inspect the usefulness and limitations of the different precipitation estimation methods the event shown in fig 11 was one of the recorded extreme hourly summer events by the gauging stations in oslo the spatial air temperature in the study area during the hourly event was in the range of 6 19 c over the pixels looking at fig 11 both radar and gauges tps observed the local extremes and shows pixels with high intensities comparing fig 11 a and b radar displays several pixels with intense precipitation while tps displays extremes at two locations further radar shows precipitation on the eastern part of the study area but tps did not show any precipitation because of very sparse gauges in this area and they could therefore not capture the event the advantage of extended spatial coverage of radar is clearly visible from this event spatial interpolation generally smooths out the spatial variation as it is visible on fig 11 b looking at fig 11 c the k nn did not show any high intense values intensity greater than 20 mmh 1 as described above if there are no similar intensive events in the past observations the k nn local regression estimation can result in underestimation the merging of k nn and tps estimates brought the quantitative information partially from gauges and radar while it reflects the spatial variation detected by radar in the resulting image of the combination estimate as shown in fig 11 d the radar image fig 11 a shows the pixels with high intense values in the northwest and northeast part of the study area however the resulting image in the combination does not display them as intense the more useful information from a radar is the spatial distribution of precipitation the usefulness of combination approach is demonstrated at the eastern part where the combination resulted in precipitation even though tps did not show any precipitation there winter precipitation intensities are relatively low in the study region consistent with intensities in boreal climates to compare the estimation methods for a winter event a 6 h winter storm event on 26 march 2015 is selected to present it can be noted that the gauges observed nearly similar hourly intensities during the 6 h period the hourly precipitation estimates using different estimation methods on 1 km 1 km pixels for the period from 06 00 to 11 00 utc were accumulated and displayed in fig 12 this winter storm disturbed the transport and other essential functions in the capital city oslo and neighbouring areas looking at fig 12 accumulated radar precipitation was very low as shown in the aggregated results in fig 4 radar underestimates the precipitation however it shows better the spatial variation in the precipitation the tps interpolation smooths out the spatial variation based on the available gauges observations the k nn estimates show higher precipitation than the tps interpolated values the nonparametric k nn estimation is based on the local regression of past similar observations it can be noted that the k nn estimation using air temperature as an additional covariate ascertains similar cold events as the nearest neighbours if such events in the past observations fig 12 d shows the resulting image of the accumulated hourly estimates of the combination of tps and k nn the combination resembles the spatial variation shown in the k nn estimation however it did not show higher precipitation for many pixels as in the k nn the dynamic combination approach provides more weight to the method with less error in the past estimation for this winter event the quantitative estimation from the radar k nn gets less weight than the estimation derived from gauges tps on these pixels 4 discussion water resources engineering and hydrological studies on a catchment scale are often limited by high spatiotemporal precipitation estimates available syed et al 2003 smith et al 2004 hailegeorgis et al 2016 in cold climates precipitation estimation using available sensors both gauges and radars are confronted with an additional challenge of phase dependent uncertainties saltikoff et al 2015 wolff et al 2015 the precipitation measurements by gauge network and weather radar have their own merits and shortcomings it is often shown that merging the two sources can result in improved precipitation estimates haberlandt 2007 berndt et al 2014 hasan et al 2016b in this study we combined the two precipitation data sources by taking advantage of their merits and rectifying their shortcomings the combination approach considerably decreased the negative bias in the radar precipitation while it reduced the mean squared error by one fourth of the errors associated with the original hourly radar precipitation rates as a result we generated an improved continuous hourly time series of gridded 1 km 1 km precipitation for the area with a radar coverage of radius 100 km this study has formulated a framework which adopted a nonparametric estimation model and a dynamic model combination approach to merge radar and gauge precipitation in cold climates in a typical radar gauge merging process radar precipitation rates from the processing chain clutter cancellation vpr correction and z r conversion are directly used for merging analysis of data showed that the radar precipitation rates have a noticeable bias including phase dependent bias in contrast to the traditional approach this study first adjusts the radar precipitation rates using the nonparametric k nn method before merging with gauge precipitation otherwise the bias could remain in the merged precipitation estimates we employed the nonparametric k nn method with air temperature as an additional covariate to adjust the radar precipitation for colder temperature conditions and then also for use within the dynamic model combination the use of air temperature in the dynamic model combination algorithm did not result in an improvement similar to the nonparametric estimation in sivasubramaniam et al 2018 however the proposed method marginally improved the estimates compared to the combination using radar precipitation rate alone when gridded air temperature data are available the use of air temperature in the dynamic model combination algorithm is inexpensive and that results in added value to the resulting precipitation estimates in cold climates geostatistical merging methods consider radar precipitation as a secondary information to improve spatially interpolated gauges goudenhoofdt and delobbe 2009 rabiei and haberlandt 2015 in contrast to those merging methods the dynamic variation of the weight in the model combination takes intensity information from radar precipitation as a potential source of equal importance to gauge precipitation the model combination approach can be more useful for a relatively less dense gauged area where quantitative information from the radar can be more accurate than the interpolated gauge value the results of the dynamic model combination found are comparable with the results of hasan et al 2016b in this study the dynamic model combination framework of hasan et al 2016b for a tropical setting was extended to a norwegian cold climatological context compared to hasan et al 2016b univariate radar reflectivity as a single variable kernel based nonparametric estimates this study employed a bivariate nonparametric k nearest neighbour model with radar precipitation rate and air temperature as two covariates to adjust the radar precipitation rates first further a thin plate spline tps was applied to spatially interpolate the gauge precipitation data to regular grids finally air temperature was used as an additional variable to find similar events to estimate the dynamic combination weights however in contrast to hasan et al 2016b this study aims to generate continuous hourly time series of improved radar based precipitation field for the region which can be readily useable with hydrological models hasan et al 2016b tested the combination method over the sydney region in australia and reported that the nonparametric estimation reduced the rmse in rainfall estimates by 10 and the model dynamic combination reduced by 20 compared to radar as a single sensor using a parametric z r relationship in this study nonparametric k nn estimation resulted in a mean reduction in rmse of 15 while a mean reduction in rmse of 25 was obtained using the dynamic combination method further hasan et al 2016b reported that the bias in parametric and nonparametric estimation was the same in contrast k nn estimation in this study resulted in a considerable reduction in bias the reason can be due to that hasan et al 2016b used a gauge adjusted parametric relationship for the study region and compared with nonparametric estimation while precipitation rates used in this study were estimated by met no using marshall and palmer 1948 relationship without any gauge adjustment work at the norwegian meteorological institute met no with the same objective is currently underway an experimental release from met no on this work reported merging of hourly radar precipitation rates with disaggregated daily gauged precipitation to hourly data using an optimal interpolation method lussana et al 2016a in contrast the present study merged the nonparametric estimation of radar precipitation with interpolated hourly gauge precipitation the findings from the present study can be an input for the ongoing norwegian national project of developing gridded radar based precipitation field the improved continuous hourly precipitation field obtained through the combination process can be a readily available data source for hydrological applications radar precipitation data have relatively smaller number of missing observations in addition radar covers a large geographical area however the radar precipitation field has relatively high errors these errors can potentially bias the calibration of hydrological models and water balances computations oke et al 2009 the current study exploited the advantages inherited in the radar data while it reduced the root mean squared error as well as the negative bias in the original radar precipitation field as a result improved continuous precipitation data are becoming available on the catchment scale for many small catchments the high spatial precipitation field could solve issues related to precipitation representativity of catchment scale hydrological modelling syed et al 2003 smith et al 2004 kirchner 2009 further the resulting long term continuous precipitation estimates can be led for deriving radar based climatology of precipitation for the region as similar to overeem et al 2009 the use of data with hydrological models to simulate the river flow and snow accumulations and reconstruct the extreme events would be an immediate and interesting next step for this work 5 conclusions the study developed a method to merge the radar and gauge precipitation observations further investigated the usefulness of air temperature as an additional factor in the combination process in cold climates an improvement of 25 in the root mean squared error was obtained using the dynamic model combination method compared to the original radar precipitation rates almost all gauge locations where we evaluated the modelled outcomes showed a significant improvement in the precipitation estimation air temperature as an additional variable in the combination algorithm marginally improve the precipitation estimates compared to the algorithm without air temperature however the improvement was not high as it yielded for the nonparametric estimation in cold climates given the need for high spatiotemporal precipitation data on the catchment scale and the availability of resulting data in remote areas in a continuous setting because of radar s extended coverage the above finding could be useful for practical hydrology software and data availability radar precipitation rate data used in the study are available in the norwegian meteorological institute s met no thredds server http thredds met no thredds catalog remotesensingradaraccr catalog html gauge precipitation data and gauges meta information can be obtained from met no s web portal eklima http eklima met no and access to the web portal is available upon request gridded hourly air temperature and wind speed data are obtained from met no s thredds server http thredds met no thredds catalog html npred programming tool is available as r package and it can be downloadable from the following link as follows http www hydrology unsw edu au download software npred the r package fields v9 0 is available on the comprehensive r archive network cran to install competing interests the authors declare that there are no competing interests acknowledgements the authors thankfully acknowledge the norwegian meteorological institute met no for providing radar and gauge precipitation and meteorological data for this study the authors would like to thank christoffer artturi elo and cristian lussana at met no for the assistance to get the gridded radar precipitation and meteorological data a great appreciation goes to water research centre university of new south wales unsw sydney australia for hosting the first author for research practicum the authors gratefully acknowledge the norwegian research council and norconsult for funding this research work under the industrial ph d scheme project no 255852 o30 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 013 
26172,this study presents a dynamic forecast combination approach adapted to incorporate multiple sources of precipitation dynamic combination serves to utilise the varying merit each data source exhibits with time the dynamic model combination framework presented merges a nonparametric k nearest neighbour k nn estimation of radar precipitation with thin plate spline tps interpolated gauge precipitation since air temperature is an essential variable to discriminate the phase of the precipitation in cold climates this study uses radar precipitation and air temperature as the two variables in the dynamic combination algorithm the merging of k nn and tps estimates is shown to reduce the rmse by 25 compared to the original radar precipitation rates the usefulness of air temperature is found not to be as significant in the combination as it is in the formulation of the nonparametric radar precipitation fields for cold incident temperatures keywords merging radar and gauge precipitation dynamic model combination nonparametric radar precipitation k nearest neighbour spatial precipitation and temperature cold climate precipitation 1 introduction continuous simulations of streamflow and catchment water storage using distributed hydrological models require accurate precipitation input at high spatial resolution syed et al 2003 smith et al 2004 beven 2012 it is often shown that existing precipitation networks in many places are not dense enough to capture the spatial variation of precipitation events kirchner 2009 hwang et al 2012 precipitation measurement by remote sensing has a great potential to fulfill the need for distributed input data measured at a catchment scale krajewski and smith 2002 woldemeskel et al 2013 presently weather radars provide precipitation estimates with high spatial at a standard resolution of 1 km 1 km and temporal hourly and sub hourly resolution however weather radar measures precipitation indirectly using remote sensing techniques and measurements are subject to several sources of errors and uncertainties chumchean et al 2006b villarini and krajewski 2010 berne and krajewski 2013 abdella 2016 weather radar transmits electromagnetic waves and measures the energy backscattered by the hydrometeors in the atmosphere as reflectivity the reflectivity is then converted to ground precipitation rates typically parametric power law relationship often called as z r equation z a r b is used in the conversion of radar reflectivity z to ground precipitation rates r the constant values of a 200 and b 1 6 derived by marshall and palmer 1948 for rain are generally used regardless of climate region however these parameters are not constant wilson and brandes 1979 and they are related to drop size distribution of hydrometeors in the atmosphere drop size distribution varies with the type and phase of the precipitation joss et al 1990 uijlenhoet 2001 chumchean et al 2008 due to errors and uncertainties in the measurement of reflectivity as well as in the conversion of reflectivity to ground precipitation advances and advantages of radar precipitation data are not fully used in widespread hydrological applications so far chumchean et al 2003 berne and krajewski 2013 to model radar rainfall uncertainty ciach et al 2007 used the nonparametric kernel regression method such nonparametric approaches can be more effective than parametric alternatives as fewer assumptions about the processes being modelled needs invoking while sufficient observational data exist silverman 1986 mehrotra and sharma 2006 while ability to adapt to the data locally is a strength of the nonparametric approaches they can result in local biases due to outliers villarini et al 2008 compared their nonparametric approach with a copula based method and found that the performance was as equal or higher than copula hasan et al 2016b presented a kernel based nonparametric method to improve radar rainfall estimates using the conditional probability distribution of past observed reflectivity and gauged rainfall sivasubramaniam et al 2018 extended the hasan et al 2016b univariate nonparametric approach of radar precipitation estimates to bivariate with the use of near surface air temperature as a covariate to improve the radar precipitation estimates in cold climates accurate estimates of precipitation data are required for engineering design and management in small basins particularly with the intense local rainfall events where the hydrological processes impelled by rainfall rates play a relatively higher role than the hydraulic processes of flood wave propagation krajewski and smith 2002 weather radar with its extended spatial coverage can monitor many small catchments that otherwise remain ungauged without any precipitation observations berne and krajewski 2013 moreover as precipitation is an intermittent variable and various difficulties in measuring precipitation precisely accurate estimation of spatial distribution of precipitation in the river basin using sparse gauge network alone is a challenging task hwang et al 2012 as a result distributed and physically based hydrological models are often limited by the accurate spatial variability of precipitation input data syed et al 2003 to obtain the spatial distribution of precipitation in a catchment from available gauges which are often unevenly distributed spatial interpolation techniques are required a range of spatial interpolation techniques are available in the literature from simple methods e g arithmetic mean nearest neighbour thiessen polygon and inverse distance weighting to advanced and more complex approaches e g multiple linear regression thin plate smoothing splines kriging genetic algorithms conditional bias penalized kriging and copula a review of different interpolation methods is presented by hwang et al 2012 where the gauge is a single sensor of precipitation even with the use of advanced spatial interpolation techniques it is a challenging task to get a distributed precipitation field in a sparsely gauged area it is often shown that compared to gauges radars capture the spatial variation of precipitation relatively well despite errors in their quantitative information several studies chumchean et al 2006a haberlandt 2007 goudenhoofdt and delobbe 2009 have shown that merging of radar precipitation estimates with gauge precipitation can improve precipitation estimates the merging approach is as old as the arrival of weather radar data for hydrological applications the focus of earlier studies has been to correct the bias in the radar precipitation estimation using gauge observations mean field bias mfb correction is a simple bias correction method that is broadly used the mfb method assumes a uniform multiplicative error in the radar estimates and it estimates the ratio of the accumulated radar precipitation and accumulated gauge precipitation from a number of radar gauge pairs as a multiplicative adjustment factor brandes 1975 proposed a correction factor at each gauge location with subsequent interpolation over the radar field chumchean et al 2006a applied kalman filter to improve the mfb estimates subsequent focus on merging radar and gauge data has utilised the spatial variability of the radar to further improve the spatial interpolation of gauges several merging methods with the use of geostatistical techniques with different degree of complexity have been proposed some of the geostatistical merging methods in literature are cokriging krajewski 1987 kriging with external drift ked berndt et al 2014 conditional merging sinclair and pegram 2005 and copula based assimilation vogl et al 2012 the geostatistical methods generally consider gauge as a primary source and radar as secondary source for the merging goudenhoofdt and delobbe 2009 however rabiei and haberlandt 2015 showed that quality of the quantitative radar data is still important factor in conditional merging hasan et al 2016a b argue that if errors associated with the precipitation field derived from radar and gauge can be quantified correctly the two data sources can be merged without abandoning the quantitative information from the radar this approach can be promising in sparsely gauged regions because intensity information from radar is to be extracted by extending the error structure identified from gauged regions to ungauged regions combination of two sources of information using error variance has its basis in economic forecast combination bates and granger 1969 when two set of forecasts are combined the resulting forecast can have lower mean square error than both original forecasts bates and granger 1969 the forecast combination approach has been applied in hydrometeorology wasko et al 2013 along with other areas the forecast combination methodology is prevalent in combining seasonal forecasts from multiple climate models these studies have reported the usefulness of dynamic weighting in combination instead of simple static combination chowdhury and sharma 2009 2011 devineni and sankarasubramanian 2010 khan et al 2014 kim et al 2016 in the dynamic combination the combination weights change over time to capture temporal variation locally hasan et al 2016a evaluated different combination approaches in the context of merging radar and gauge rainfall data they found that covariance based methods gave better results compared to non covariance based methods and showed the usefulness of dynamic approach hasan et al 2016b presented a covariance based dynamic model combination framework to combine radar and gauge rainfall data sources in their study a kernel based nonparametric approach was used to estimate rainfall estimates and then nonparametric radar rainfall estimates were merged with copula based spatially interpolated rainfall field for a tropical climate in the dynamic model combination similar events were identified using a k nearest neighbour approach with reflectivity as a single variable to estimate the error covariance matrix and weights in cold climates precipitation occurs with different phases snow rain or a mixture of snow and rain and similar events can be specified not only based on the intensity information but also the phase of the precipitation several studies have shown that air temperature is intrinsic to the phase of the precipitation auer jr 1974 killingtveit 1976 rohrer 1989 fassnacht et al 2001 air temperature is an essential variable to differentiate two similar events with different phase snow or rain but the same intensity al sakka et al 2013 fassnacht et al 1999 2001 used the near surface air temperature observations to adjust the radar precipitation estimation sivasubramaniam et al 2018 showed that the use of air temperature as a second covariate in the k nearest neighbour k nn nonparametric model reduces the root mean squared error significantly and improves the radar precipitation estimates in colder temperatures as similar to the sivasubramaniam et al 2018 approach air temperature can also be used as an additional variable to identify similar events within the dynamic model combination framework to further improve the combination estimates in cold climates the overall objective of this study is to improve the precipitation estimates with high spatiotemporal resolution the primary focus is to merge radar precipitation field with existing gauge observations to generate improved continuous hourly precipitation field for the region for that first we adjust the hourly radar precipitation rates using the k nearest neighbour nonparametric method and then we use the dynamic model combination framework to merge the nonparametric estimates with spatially interpolated precipitation gauge data using thin plate spline tps interpolation we evaluate whether the use of air temperature as an additional variable can be useful for dynamic model combination as is found for the nonparametric estimation 2 material and methods 2 1 study area the study area for the current research is 100 km radius from hurum radar station in norway an area of about 31000 km2 as shown in fig 1 the hurum radar station is located at 59 63 n latitude and 10 56 e longitude and it is about 30 km from the norwegian capital city oslo it is a c band installation with a wavelength of 5 319 cm and a coverage radius of approximately 240 km the hurum radar has been in operation since november 2010 and monitors the southeastern part of norway and part of sweden the coverage area consists of six norwegian counties where nearly 40 of the population live within the study area 94 precipitation gauges are in operation with available hourly precipitation data for the study the precipitation gauges are tipping bucket gauges most of them are lambrecht and nivus rm 202 brands and weighing gauges geonor brand with an alter windshield fig 1 shows the precipitation gauge locations overlaid on the topography of the study area seven gauges were left from the computations in order to verify the approach at ungauged pixels in fig 1 the 87 gauges which were used in the computation are displayed with blue circles and the 7 gauges with red looking at fig 1 precipitation gauges are not evenly distributed a relatively dense network of gauges exists near urban areas along the coast and a sparse network of gauges exist in the rest of the area further it can be seen that the inland mountainous areas remain mostly ungauged without precipitation gauges with hourly measurements mean annual precipitation in the study area is between 1000 and 1500 mm based on the climatology for the period 1961 to 2017 http www senorge no the precipitation in the study region can broadly be divided into three categories frontal orographic and convective precipitation the convective precipitation is most dominant during the warmer summer months the annual mean temperature is in the range of 2 10 c in the study region the january mean is near freezing at the coast and down to 10 15 c inland the maximum summer temperature is mostly in the 20 25 c range 2 2 data at present the norwegian meteorological institute met no operates nine c band doppler weather radar installations including the hurum radar station the norwegian radar network covers the entire land surface of norway and they scan the atmosphere with a 7 5 min temporal resolution with this resolution being 15 min until june 2013 the raw radar volume scan from the radar stations are processed and quality controlled by met no and then met no generates and distributes various radar products to end users elo 2012 one of the products from met no is surface rainfall intensity sri the mosaics of nine radars sri data covering entire norway are available for the public to use in practical applications the quality of the measured reflectivity can vary from pixel to pixel due to the nature of remote sensing measurements topography and distance from the radar station affect the radar measurement of precipitation and introduce errors these include anomalous propagation ground clutter beam blocking and attenuation germann and joss 2004 in a mountainous region like norway the mountains can cause partial or total beam blocking abdella et al 2012 reported that beam blockage for hurum radar affects the eastern and south eastern part of its coverage fig 1 and the beam blockage is up to 30 moreover in high latitude cold climates non uniform vertical profile of reflectivity vpr and bright band effects in the vpr introduce major uncertainties in the radar precipitation estimation abdella 2016 koistinen and pohjola 2014 the reflectivity measurements go through a chain of processes to address the above mentioned errors and uncertainties the process at met no first removes clutter and other non meteorological echoes from the radar scan and then gaps in the data caused by clutter are reconstructed after that volumetric reflectivity data are segmented as a convective or stratiform type of precipitation next the processing algorithm computes vpr according to the precipitation type to generate sri product met no uses the lowest plan position indicator ppi here the aloft reflectivity data is projected down to a reference height 1 km near to the ground and the projection known as vpr correction that takes variability in the vertical profile of reflectivity vpr and bright band effect into account elo 2012 the met no applies a single z r relationship marshall and palmer 1948 relationship z 200 r 1 6 to convert the 7 5 min or 15 min before june 2013 reflectivities into precipitation rates the precipitation rates are then accumulated to hourly and distributed as end user hourly radar precipitation rate sri product the accumulated hourly radar precipitation rate product sri product was used in the present study the gauged precipitation data used in the study are from the gauges operated by met no the met no undertakes the calibration of gauges and essential measures to reduce the uncertainty in the measurements further met no performs routine quality control before being released to the data portal for the public however met no does not correct the precipitation data for wind induced undercatch the precipitation measurements from the gauges are available with varying length as some gauges have been operated since 2013 or later and there are missing values during their operation as well gridded air temperature and wind speed datasets covering norway are available from met no lussana et al 2016b spatially interpolated the past temperature observations from the norwegian meteorological stations using an optimal interpolation in a bayesian setting to develop a temperature dataset for norway the gridded hourly wind speed dataset was derived by statistical downscaling from the 10 km numerical dataset nora10 combined with the arome 2 5 km numerical dataset a local quantile regression method was used for this statistical downscaling the wind speed data were required in this study to correct wind induced undercatch of gauge precipition the datasets for the study were downloaded and prepared as follows radar and gauge precipitation and meteorological data air temperature and wind speed for the period from january 2011 to may 2015 were used for this study the gridded hourly radar precipitation rates air temperature and wind speed data with 1 km 1 km spatial resolution for the study area were downloaded from met no s thredds server http thredds met no the gridded data are in netcdf file format in utm33n projection the hourly precipitation gauge measurements and gauges meta information were obtained from met no s data portal eklima http eklima met no the gauge locations were overlaid on the 1 km 1 km regular grids of the gridded data and the pixel of 1 km 2 overlapping each gauge was located one location near oslo has three gauges within a 1 km 1 km pixel but except for that all pixels consist of a single gauge the pixel value for each hour was extracted and continuous hourly time series of radar precipitation rate air temperature and wind speed for all gauges were generated solid precipitation exhibits significant undercatch due to high wind conditions in high latitude and mountainous regions wolff et al 2015 in this study gauged precipitation data were corrected for wind induced undercatch by using the nordic precipitation undercatch correction model førland et al 1996 the correction model classifies the precipitation phase as solid liquid and a mixture of two phases using air temperature and the model uses two sets of equations one for the solid precipitation and other for the liquid precipitation an average value of the two equations is used for the mixed precipitation these equations require wind speed and air temperature at gauge location in this study the gridded hourly wind speed and air temperature data were used for the undercatch correction hereafter gauge precipitation refers to undercatch corrected precipitation throughout the study the precipitation intensities in the study area are relatively low consistent with intensities in high latitude boreal climates sivasubramaniam et al 2018 a study of statistical properties of precipitation intensities in mid norway showed that precipitation rates less than 6 mm h 1 yields 88 of the total precipitation volume while less than 1 76 mm h 1 yields 50 moreover the same analysis found that precipitation rates below 0 1 mm h 1 contributes little to the total precipitation volume and might be regarded as zero precipitation engeland et al 2014 in this study gauge precipitation and radar precipitation rate less than 0 1 mm h 1 were assumed as zero precipitation at each gauge location the timesteps with gauge precipitation or radar precipitation rate less than 0 1 mm h 1 were removed and an observed dataset of hourly gauge precipitation and corresponding radar precipitation rate and air temperature was prepared it can be noted that the length of dataset at each gauge location can vary due to the availability of gauge precipitation records to analyse the distribution of gauge and radar precipitation used in this study observed datasets gauge and radar precipitation and air temperature at gauge locations were pooled and histograms were plotted as shown in fig 2 looking at fig 2 it is visible that radar underestimates the precipitation compared to gauge observation moreover for intense events radar precipitation rates show a considerable negative bias for both temperatures colder than and warmer than 10 c it can be noted that high intensity hourly precipitation events are rare in the study region only 0 03 of the total gauge hours used in the analysis have gauge precipitation intensity above 20 mm h 1 while 0 31 have intensity above 10 mm h 1 in the pooled dataset from the gauge locations the maximum observed gauge precipitation was 45 mm h 1 while maximum radar precipitation rate was 35 mm h 1 however analysis of radar precipitation rates from the entire set of pixels 46656 showed that there are radar pixels with extremely high intensity values a histogram showing the distribution of radar precipitation rates pooled from the entire set of radar pixels is available in the supplement fig s1 the intensive values above 50 mm h 1 are likely due to hail or a mixture of very heavy precipitation and hail 2 3 methodology 2 3 1 framework the two sources of quantitative precipitation from the radar and gauges are combined within a dynamic model combination framework as an overall description the flow diagram in fig 3 illustrates the data used and the methods applied in the merging process merging with gauge precipitation must be applied as the last step in radar precipitation estimation process and all possible corrections should be applied first to improve the radar precipitation estimates before merging goudenhoofdt and delobbe 2009 the hourly radar precipitation rates were first adjusted within a nonparametric framework using gauge precipitation and air temperature observations sivasubramaniam et al 2018 since the gauge precipitation data are at gauge locations a spatial interpolation was applied to get precipitation values at grids precipitation values at grids from the two sources were then merged using the estimated combination weights within dynamic model combination framework each of the methods is described in detail in the following subsections 2 3 2 k nearest neighbour k nn estimation for the nonparametric estimation this study adopted the method presented by sivasubramaniam et al 2018 for the cold climatological setting where k nearest neighbour k nn nonparametric model with radar precipitation rate and air temperature as two covariates is used here a summary of the k nn method with air temperature as an additional covariate is presented for brevity more details of the k nn method of radar precipitation estimation can be found in sivasubramaniam et al 2018 readers are referred to sharma and mehrotra 2014 sharma et al 2016 hasan et al 2016b mehrotra and sharma 2006 for a description of the nonparametric framework conditional estimation of ground precipitation r e s t t for a given observed pair of radar precipitation rate r t and air temperature t t using past observed dataset of radar precipitation rate and air temperature and corresponding gauge precipitation within a k nn regression model is written as follows sivasubramaniam et al 2018 1 e r e s t t r t t t k 1 k g k k j 1 k 1 j in eq 1 k denotes the number of past observations radar precipitation rate and air temperature pairs which are considered similar to the current observation r t and g k denotes the gauge precipitation corresponding to the k t h neighbour in the past observations k is a maximum number of nearest neighbours permissible and it is taken as equal to the square root of the sample size as recommended by lall and sharma 1996 the order of k neighbours is ascertained using a weighted euclidean distance metric as expressed below 2 ξ i 2 β r r r i s r 2 β t t t i s t 2 where ξ i is the distance of the current observed pair r t to the i t h data point r i t i in the past observed dataset in a two dimensional space s r and s t denote sample standard deviations of the two predictors and β r and β t are partial weights of predictors and their summation is equal to one readers are referred to sivasubramaniam et al 2018 for further details about the use of partial weights in k nn estimation the k nn regression estimator available in the npred r package sharma et al 2016 available for downloading from http www hydrology unsw edu au download software npred was used for the nonparametric k nn estimation in this study an average partial weight of β r 0 68 β t 0 32 recommended by sivasubramaniam et al 2018 for the study region was used a continuous time series of radar precipitation rate and air temperature are on the regular 1 km 1 km grids for the k nn estimation at each pixel grid location the observed dataset gauge precipitation radar precipitation rate and air temperature from the three nearest gauges were pooled and used as past observations with the k nn regression to estimate precipitation k nn estimate for the timesteps with radar precipitation rate less than 0 1 mm h 1 the k nn estimates were set as zero 2 3 3 thin plate spline tps interpolation merging of radar precipitation field with gauges spatially in a regular grid require spatial interpolation of point gauge data for this study thin plate splines tps was chosen for the spatial interpolation of gauge precipitation data it can be noted that for spatially interpolated gauge precipitation to use in the combination method any of the interpolation techniques as listed in section 1 can be used if the estimates of variance in the fitted precipitation surface are available hasan et al 2016b the tps interpolation is a data driven nonparametric approach of locally weighted polynomial method the optimal number of neighbours around each target is determined using general cross validation gcv statistics hwang et al 2012 a number of studies hutchinson 1998a b tait et al 2006 woldemeskel et al 2013 have reported the application of tps method for spatial interpolation of gauge precipitation tps from the r package library fields v9 0 nychka et al 2017 available on the comprehensive r archive network cran was used to fit a thin plate spline precipitation surface from precipitation gauges in this study a description of the tps can be found in the r documentation https www rdocumentation org packages fields versions 9 0 topics tps in the tps the smoothing parameter is chosen by generalized cross validation 2 3 4 dynamic model combination at each pixel 1 km 1 km grid we have spatially estimated k nn radar precipitation estimates using a model with radar precipitation rate and air temperature as two predictor variables k nn for each hourly timestep we also interpolated gauge precipitation using tps to provide an alternate estimate that uses a different data source as a result all 46656 pixels in the study area have continuous time series of k nn and tps estimates the k nn and tps estimates have different estimation accuracy the estimation accuracy varies spatially from one pixel location to another as well as temporally at a given pixel location within a model combination framework the two estimates can be combined by weighting each method according to their past observed estimation accuracy bates and granger 1969 wasko et al 2013 hasan et al 2016b in this study the combination weight of each method is calculated from the error covariance matrix using residual errors corresponding to past observations to calculate the error we need a true observation of the estimated precipitation value at pixel locations coinciding with gauges corresponding gauge precipitation was taken as the true precipitation and k nn and tps errors were calculated this study used the leave one out cross validation loocv procedure to calculate these errors the k nn error at a gauge location was calculated by leaving out one observed response gauge precipitation from the k nn regression and estimating the expected response value for that observed response to obtain the error for each observation a matrix of observed gauge precipitation and corresponding precipitation rate and air temperature and k nn error e k n n was generated at each gauge location tps error was calculated using spatial loocv whereby leaving a gauge from spatial interpolation and estimating the tps response for each observed precipitation value allowed tps error to be calculated with reference to the observed precipitation value tps error matrix with observed precipitation and corresponding tps error e t p s was generated the k nn error matrix and tps error matrix were next merged to obtain an error matrix of radar precipitation rate and air temperature and corresponding k nn error e k n n and tps error e t p s at 87 gauge locations to merge the tps and k nn estimates combination weights were calculated at each pixel grid location x y for each hourly time step t as described follows each pixel can be associated with a radar precipitation rate r x y t air temperature t x y t and corresponding precipitation estimates using tps p t p s x y t and k nn p k n n x y t to estimate the combination weight error matrices from three nearest gauges for this pixel were pooled the pooled error matrix y r x y t x y e k n n x y e t p s x y consist of radar precipitation rate air temperature and corresponding estimated k nn and tps error from past observations the static combination approach uses entire observations to estimate error covariance matrix at a given pixel location in contrast the dynamic combination identifies a number of similar precipitation events to estimate error covariance matrix for each timestep at the pixel location here the k nearest neighbour k nn method with a euclidean distance metric was used to ascertain the neighbours and the number of maximum neighbours was equal to the square root of the sample size as recommended by lall and sharma 1996 hasan et al 2016b identified similar events using reflectivity as a single variable in contrast to their approach this study uses radar precipitation rate and air temperature as two variables to identify similar events using k nearest neighbour technique in the dynamic model combination algorithm to merge k nn and tps estimates the proposed combined product of k nn and tps is denoted as p c o m b r t for comparison a reference model combination of k nn and tps estimates using radar precipitation rate as a single variable to identify similar events in the dynamic model combination algorithm as similar to hasan et al 2016b were also developed p c o m b r for each time step for the given radar precipitation rate and air temperature pair at the pixel location r x y t x y t k number of similar r t pairs from the past observations in the matrix y were identified and hence corresponding k number of k nn error e k n n and tps error e t p s pairs were selected in the reference model for the given radar precipitation rate r x y t k number of similar r from the matrix y were identified to select k nn and tps error pairs an error covariance matrix e of the estimation errors was calculated for the selected error pairs e k n n e t p s and the covariance matrix can be written as follows 3 e σ t p s 2 ρ σ t p s σ k n n ρ σ t p s σ k n n σ k n n 2 here diagonals are the variance of the errors of each method and off diagonals represent covariance of the errors from the two methods where ρ is the correlation between errors from the two estimation methods the dynamic combination weight of the two estimates can be calculated by minimizing the quantity as shown in 4 such that the weights being constrained to lie between 0 and 1 and their summation is unity khan et al 2014 hasan et al 2016b for further details of the derivation of eq 4 readers are referred to khan et al 2014 4 min w e w here w w t p s w k n n and the summation w t p s w k n n is equal to 1 w t p s denotes the combination weight associated with tps estimates while the weight associated with k nn is denoted by w k n n the combined precipitation estimation p c o m b r t x y t is the weighted summation of k nn and tps estimates as follows 5 p c o m b r t x y t w k n n p k n n x y t w t p s p t p s x y t at pixel locations coinciding with gauges the estimates were obtained using leave one out cross validation loocv the loocv ensures the modelled outcomes p k n n p t p s p c o m b r and p c o m b r t are obtained from independent of gauge precipitation values those will be used to evaluate the modelled outcomes the k nn estimate was calculated by leaving out the current radar precipitation rate and air temperature and corresponding gauge precipitation from the past observed dataset in the k nn regression the tps estimate was obtained by leaving a gauge from spatial interpolation and estimating the tps response on that gauge location to calculate the weight for each timestep in the dynamic combination the error covariance matrix was formulated by excluding the k nn and tps error pair corresponding to the current k nn and tps estimates 2 3 5 model evaluation criteria several performance metrics have been used in literature to assess the performance of the models and compare them villarini et al 2008 hasan et al 2016b some of the metrics are root mean square error rmse mean absolute error mae and mean error me the rmse provides the overall performance measure of a predictive model hasan et al 2016b the study primarily used the rmse as a performance metric to evaluate the model performance to strengthen the evaluation additional performance metrics mae and me were also used definition of the performance metrics used in this study can be found in the published literature villarini et al 2008 bennett et al 2013 hasan et al 2016b 3 results 3 1 performance of dynamic combination the performance of the modelled outcomes was evaluated at gauge locations for the assessment we computed performance metrics rmse mae and me for the k nn tps and the combination estimates combr and combrt at 87 gauge locations for the timesteps with radar precipitation rate and gauge precipitation greater than 0 1 mm h 1 here gauge precipitation was taken as a true observed value to compute the performance metrics the performance metrics were also estimated for radar precipitation rates which was considered as a benchmark to compare the modelled estimates fig 4 shows a comparison of performance metrics computed at 87 gauge location for the precipitation estimates using different estimation methods and for the radar precipitation rates mp the nonparametric k nn estimation k nn leads to a considerable decrease in the rmse in the radar precipitation estimates the merging of k nn and tps estimates within a dynamic model combination framework reduces the rmse in the estimated precipitation field significantly the mean improvement in rmse for k nn is 15 0 from 1 3 to 1 1 mm h 1 while it is over 25 1 3 0 95 mm h 1 for the proposed dynamic model combination estimation combrt compared to the radar precipitation rates further almost all gauge locations exhibited clear improvement in estimates using dynamic combination approach looking at fig 4 tps estimates has a relatively smaller error than radar precipitation rates mp this displays the errors associated with the radar precipitation it can be noted that tps interpolation technique was the method of choice in this study and the focus of the study was not to assess the different interpolation methods to select for the model combination as shown in fig 4 c the mean error me of the radar precipitation rates mp which represents the bias in the mp was negative for almost all gauge locations this demonstrates the underestimation of radar precipitation compared to gauge precipitation the k nn estimation noticeably reduces the bias me in the radar precipitation to near zero from 0 44 to 0 01 mm h 1 while reducing the rmse and mae most of the gauge locations showed clear reduction in rmse and mae in the precipitation estimates by merging the two sources than the estimation k nn or tps using the data from a single sensor radar or gauge however the me associated with combination estimation mean value of 0 1 mm h 1 was slightly higher than k nn estimation while it is less than the tps estimates mean value of 0 2 mm h 1 the proposed merging method using air temperature as an additional variable in the dynamic combination algorithm combrt slightly improves the precipitation estimates compared to the reference model combr looking at fig 4 the improvement is not very high however more than 80 of the gauge locations rmse of combrt is lower than combr in addition we also evaluated the estimates at the 7 independent gauge locations to verify the results on ungauged pixel locations the gauge precipitation data from those control gauge locations have not been used in any of the estimations we found a similar result at these control gauge locations as for the study gauges a bar plot representing these performance metrics at 7 control gauge locations is shown in fig 5 while fig 4 presents the summary of performance metrics for the 87 gauges obtained by using loocv procedure the values of performance metrics estimated for each of those 7 control gauges are presented in fig 5 the combination approach reduces the rmse and mae for all seven gauge locations while it decreases the me in the radar precipitation rates for most of them and it resembles the summary result presented in fig 4 looking at fig 5 the magnitude of the performance metrics varies among gauges we investigated whether they are due to beam blockage however we could not find any spatial pattern in the magnitude of the errors at gauge locations here it can also be noted that the data length at each gauge location is not the same as mentioned in section 2 2 3 2 variation with temperature classes to investigate whether the improvement can vary with temperature ranges we computed performance metrics for the datasets with temperatures colder than or equal and warmer than 10 c and rmse and me are shown in fig 6 looking at fig 6 a for temperatures colder than or equal 10 c radar precipitation mp have a higher rmse than the tps interpolation this shows that the radar performance is poorer for colder temperatures than warmer there are relatively higher errors and uncertainties in the radar measurement of precipitation in cold temperatures for temperatures warmer than 10 c the overall rmse is higher for tps than for radar precipitation this can be due to tps spatial interpolation of gauge precipitation using available sparse gauges resulting in more error this is turn could be due to such events being local convective events where radars perform better fig 6 b shows the estimated bias me in the different estimation for the two temperature classes radar precipitation rates mp have substantial negative bias under estimation for both colder and warmer temperatures it can be noted that met no uses the single z r relationship marshall and palmer 1948 relationship for rain to convert the reflectivities to precipitation rates an inappropriate relationship z r relationship for rain instead of snow in the conversion can also result in phase dependent bias in the estimation for colder temperatures similar argument can be valid for the bias in the radar precipitation for temperatures warmer than 10 c where the single z r relationship cannot be appropriate for different types of rainfall orographic or convective as they have different raindrop size distribution uijlenhoet 2001 the nonparametric k nearest neighbour k nn estimation reduces the bias in the radar precipitation for both temperature classes looking at fig 6 b tps interpolation of gauge precipitation also resulted in negative bias at most of the gauge locations but it is considerably less than the bias in the radar precipitation for both estimates the magnitude of the bias is higher for warmer temperatures than colder this is because precipitation intensities of colder temperatures are relatively low and the values in the plot is not normalised the bias in the merged precipitation combr and combrt is lower than tps estimates however it is slightly higher than the k nn estimates the nonparametric k nearest neighbour model k nn reduces the rmse in the radar precipitation estimation significantly for temperatures colder than 10 c the k nn estimation still improves the radar precipitation estimation for warmer temperatures also both combination methods combr and combrt significantly reduces the rmse compared to any of the other estimations for both temperature classes for temperatures colder than 10 c the use of air temperature in the dynamic combination algorithm combrt results in marginal improvement compared to the reference model without air temperature combr and the performance is nearly same for temperatures warmer than 10 c 3 3 combination weights as mentioned earlier combination weights vary with space and time the dynamic assignment of weight ensures the provision of merit to the best method in the combination at any location for any particular time step hasan et al 2016b to comprehend the usefulness of dynamic model combination approach the spatiotemporal variation of combination weights was scrutinized fig 7 shows the spatial plot of combination weight associated with k nn at 87 gauge locations in the study area the circles represent the gauge locations and a discrete filled colour scale is used to show the combination weights assigned to k nn estimates it can be noted that the summation of weights w k n n w t p s is equal to 1 it is clearly visible from fig 7 that k nn gets lower weight yellow filled circles and hence tps gets more weight at densely gauged locations while k nn gets higher weight in the low density regions the circles filled with blue colour represents the gauged pixel locations where the k nn gets equal or higher weight compared to tps in the northern and western boundary of the study area with sparse gauges the highest average weight of more than 0 75 was assigned to the k nn estimates the result is consistent with hasan et al 2016b where they used a denser gauge network 282 tipping bucket gauges fig 7 shows the average combination weight for the entire timesteps at each gauge location in addition to spatial variation at any pixel location the combination weights also vary with time in this paper we illustrate the temporal variation for the four gauge locations which are marked with a b c d in fig 7 and the temporal variation is shown using a box and whisker plot in fig 8 the locations were chosen to represent the four classes of combination weight as listed in the legend of fig 7 the location a is in a densely gauged area and b is from a less dense gauged area while c and d are in a sparse gauged region looking at fig 8 the resulting combination weights of k nn are in a range of values between 0 and 1 for all four locations for the locations a and b the average weight of k nn is less than 0 5 but a number of hourly events still get a weight above 0 5 for k nn and vice versa for the locations c and d the dynamic model combination method assures the best data source k nn or tps in this case is chosen in the combination for each hourly event the proposed dynamic model combination uses air temperature as an additional variable to identify similar events to estimate the combination weight fig 9 illustrates the variation of combination weight assigned to the two estimates for the datasets with temperatures colder and warmer than 10 c at the gauge locations looking at fig 9 radar based k nn estimaton gets a relatively less weight than tps for air temperatures colder than 10 c for temperatures warmer than 10 c the k nn estimates get a higher average weight than tps for more than 50 of the gauge locations the overall aggregated average weight for k nn is 0 43 while 0 57 for tps 3 4 analysis of high intensity storm events fig 10 shows a histogram of radar and gauge precipitation and resulting estimates k nn and combination for high intensity hourly events at the gauge locations looking at fig 10 nonparametric k nn estimation results in lower estimation for the intensive events observed by gauges this can be because of lack of data points with intensive events in the past observed dataset to ascertain accurate estimates using a nonparametric method that is based on local regression enough number of past observations with similar intensities are required hasan et al 2016b the combination improves the precipitation estimates however combination estimates are also still lower than the gauge precipitation for higher precipitation rates to assess the modelled outcomes for individual storm events resulting raster images were made in order to visually inspect the usefulness and limitations of the different precipitation estimation methods the event shown in fig 11 was one of the recorded extreme hourly summer events by the gauging stations in oslo the spatial air temperature in the study area during the hourly event was in the range of 6 19 c over the pixels looking at fig 11 both radar and gauges tps observed the local extremes and shows pixels with high intensities comparing fig 11 a and b radar displays several pixels with intense precipitation while tps displays extremes at two locations further radar shows precipitation on the eastern part of the study area but tps did not show any precipitation because of very sparse gauges in this area and they could therefore not capture the event the advantage of extended spatial coverage of radar is clearly visible from this event spatial interpolation generally smooths out the spatial variation as it is visible on fig 11 b looking at fig 11 c the k nn did not show any high intense values intensity greater than 20 mmh 1 as described above if there are no similar intensive events in the past observations the k nn local regression estimation can result in underestimation the merging of k nn and tps estimates brought the quantitative information partially from gauges and radar while it reflects the spatial variation detected by radar in the resulting image of the combination estimate as shown in fig 11 d the radar image fig 11 a shows the pixels with high intense values in the northwest and northeast part of the study area however the resulting image in the combination does not display them as intense the more useful information from a radar is the spatial distribution of precipitation the usefulness of combination approach is demonstrated at the eastern part where the combination resulted in precipitation even though tps did not show any precipitation there winter precipitation intensities are relatively low in the study region consistent with intensities in boreal climates to compare the estimation methods for a winter event a 6 h winter storm event on 26 march 2015 is selected to present it can be noted that the gauges observed nearly similar hourly intensities during the 6 h period the hourly precipitation estimates using different estimation methods on 1 km 1 km pixels for the period from 06 00 to 11 00 utc were accumulated and displayed in fig 12 this winter storm disturbed the transport and other essential functions in the capital city oslo and neighbouring areas looking at fig 12 accumulated radar precipitation was very low as shown in the aggregated results in fig 4 radar underestimates the precipitation however it shows better the spatial variation in the precipitation the tps interpolation smooths out the spatial variation based on the available gauges observations the k nn estimates show higher precipitation than the tps interpolated values the nonparametric k nn estimation is based on the local regression of past similar observations it can be noted that the k nn estimation using air temperature as an additional covariate ascertains similar cold events as the nearest neighbours if such events in the past observations fig 12 d shows the resulting image of the accumulated hourly estimates of the combination of tps and k nn the combination resembles the spatial variation shown in the k nn estimation however it did not show higher precipitation for many pixels as in the k nn the dynamic combination approach provides more weight to the method with less error in the past estimation for this winter event the quantitative estimation from the radar k nn gets less weight than the estimation derived from gauges tps on these pixels 4 discussion water resources engineering and hydrological studies on a catchment scale are often limited by high spatiotemporal precipitation estimates available syed et al 2003 smith et al 2004 hailegeorgis et al 2016 in cold climates precipitation estimation using available sensors both gauges and radars are confronted with an additional challenge of phase dependent uncertainties saltikoff et al 2015 wolff et al 2015 the precipitation measurements by gauge network and weather radar have their own merits and shortcomings it is often shown that merging the two sources can result in improved precipitation estimates haberlandt 2007 berndt et al 2014 hasan et al 2016b in this study we combined the two precipitation data sources by taking advantage of their merits and rectifying their shortcomings the combination approach considerably decreased the negative bias in the radar precipitation while it reduced the mean squared error by one fourth of the errors associated with the original hourly radar precipitation rates as a result we generated an improved continuous hourly time series of gridded 1 km 1 km precipitation for the area with a radar coverage of radius 100 km this study has formulated a framework which adopted a nonparametric estimation model and a dynamic model combination approach to merge radar and gauge precipitation in cold climates in a typical radar gauge merging process radar precipitation rates from the processing chain clutter cancellation vpr correction and z r conversion are directly used for merging analysis of data showed that the radar precipitation rates have a noticeable bias including phase dependent bias in contrast to the traditional approach this study first adjusts the radar precipitation rates using the nonparametric k nn method before merging with gauge precipitation otherwise the bias could remain in the merged precipitation estimates we employed the nonparametric k nn method with air temperature as an additional covariate to adjust the radar precipitation for colder temperature conditions and then also for use within the dynamic model combination the use of air temperature in the dynamic model combination algorithm did not result in an improvement similar to the nonparametric estimation in sivasubramaniam et al 2018 however the proposed method marginally improved the estimates compared to the combination using radar precipitation rate alone when gridded air temperature data are available the use of air temperature in the dynamic model combination algorithm is inexpensive and that results in added value to the resulting precipitation estimates in cold climates geostatistical merging methods consider radar precipitation as a secondary information to improve spatially interpolated gauges goudenhoofdt and delobbe 2009 rabiei and haberlandt 2015 in contrast to those merging methods the dynamic variation of the weight in the model combination takes intensity information from radar precipitation as a potential source of equal importance to gauge precipitation the model combination approach can be more useful for a relatively less dense gauged area where quantitative information from the radar can be more accurate than the interpolated gauge value the results of the dynamic model combination found are comparable with the results of hasan et al 2016b in this study the dynamic model combination framework of hasan et al 2016b for a tropical setting was extended to a norwegian cold climatological context compared to hasan et al 2016b univariate radar reflectivity as a single variable kernel based nonparametric estimates this study employed a bivariate nonparametric k nearest neighbour model with radar precipitation rate and air temperature as two covariates to adjust the radar precipitation rates first further a thin plate spline tps was applied to spatially interpolate the gauge precipitation data to regular grids finally air temperature was used as an additional variable to find similar events to estimate the dynamic combination weights however in contrast to hasan et al 2016b this study aims to generate continuous hourly time series of improved radar based precipitation field for the region which can be readily useable with hydrological models hasan et al 2016b tested the combination method over the sydney region in australia and reported that the nonparametric estimation reduced the rmse in rainfall estimates by 10 and the model dynamic combination reduced by 20 compared to radar as a single sensor using a parametric z r relationship in this study nonparametric k nn estimation resulted in a mean reduction in rmse of 15 while a mean reduction in rmse of 25 was obtained using the dynamic combination method further hasan et al 2016b reported that the bias in parametric and nonparametric estimation was the same in contrast k nn estimation in this study resulted in a considerable reduction in bias the reason can be due to that hasan et al 2016b used a gauge adjusted parametric relationship for the study region and compared with nonparametric estimation while precipitation rates used in this study were estimated by met no using marshall and palmer 1948 relationship without any gauge adjustment work at the norwegian meteorological institute met no with the same objective is currently underway an experimental release from met no on this work reported merging of hourly radar precipitation rates with disaggregated daily gauged precipitation to hourly data using an optimal interpolation method lussana et al 2016a in contrast the present study merged the nonparametric estimation of radar precipitation with interpolated hourly gauge precipitation the findings from the present study can be an input for the ongoing norwegian national project of developing gridded radar based precipitation field the improved continuous hourly precipitation field obtained through the combination process can be a readily available data source for hydrological applications radar precipitation data have relatively smaller number of missing observations in addition radar covers a large geographical area however the radar precipitation field has relatively high errors these errors can potentially bias the calibration of hydrological models and water balances computations oke et al 2009 the current study exploited the advantages inherited in the radar data while it reduced the root mean squared error as well as the negative bias in the original radar precipitation field as a result improved continuous precipitation data are becoming available on the catchment scale for many small catchments the high spatial precipitation field could solve issues related to precipitation representativity of catchment scale hydrological modelling syed et al 2003 smith et al 2004 kirchner 2009 further the resulting long term continuous precipitation estimates can be led for deriving radar based climatology of precipitation for the region as similar to overeem et al 2009 the use of data with hydrological models to simulate the river flow and snow accumulations and reconstruct the extreme events would be an immediate and interesting next step for this work 5 conclusions the study developed a method to merge the radar and gauge precipitation observations further investigated the usefulness of air temperature as an additional factor in the combination process in cold climates an improvement of 25 in the root mean squared error was obtained using the dynamic model combination method compared to the original radar precipitation rates almost all gauge locations where we evaluated the modelled outcomes showed a significant improvement in the precipitation estimation air temperature as an additional variable in the combination algorithm marginally improve the precipitation estimates compared to the algorithm without air temperature however the improvement was not high as it yielded for the nonparametric estimation in cold climates given the need for high spatiotemporal precipitation data on the catchment scale and the availability of resulting data in remote areas in a continuous setting because of radar s extended coverage the above finding could be useful for practical hydrology software and data availability radar precipitation rate data used in the study are available in the norwegian meteorological institute s met no thredds server http thredds met no thredds catalog remotesensingradaraccr catalog html gauge precipitation data and gauges meta information can be obtained from met no s web portal eklima http eklima met no and access to the web portal is available upon request gridded hourly air temperature and wind speed data are obtained from met no s thredds server http thredds met no thredds catalog html npred programming tool is available as r package and it can be downloadable from the following link as follows http www hydrology unsw edu au download software npred the r package fields v9 0 is available on the comprehensive r archive network cran to install competing interests the authors declare that there are no competing interests acknowledgements the authors thankfully acknowledge the norwegian meteorological institute met no for providing radar and gauge precipitation and meteorological data for this study the authors would like to thank christoffer artturi elo and cristian lussana at met no for the assistance to get the gridded radar precipitation and meteorological data a great appreciation goes to water research centre university of new south wales unsw sydney australia for hosting the first author for research practicum the authors gratefully acknowledge the norwegian research council and norconsult for funding this research work under the industrial ph d scheme project no 255852 o30 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 013 
26173,currently studies on constructing integrated modeling solutions in a web based environment primarily focus on the chaining of different model services while the flow of exploring solutions for complex geo problems geographically distributed issues has been less studied to address this gap a teamwork oriented integrated modeling approach to supporting geo problem solving based on the collaborative work of different modelers is proposed in this article with a workflow from conceptual logical and finally computational modeling in this approach conceptual logical and computational models are discussed and constructed using a graph based method a flowchart based method and a service configuration based method respectively by building the corresponding repositories for managing and comparing different versions of conceptual logical computational models dispersed modelers can propose their own ideas and explore appropriate solutions based on existing modeling resources a prototype modeling system was established and two experimental modeling cases were conducted to verify the feasibility and capability of the proposed approach keywords integrated modeling teamwork oriented modeling conceptual model logical model computational model workflow software availability architecture designer min chen songshan yue and yongning wen software developer songshan yue and tao hou address no 1 wenyuan road qixia district nanjing 210023 p r china e mail yss123yss 126 com first available 2018 hardware requirements 1 ghz cpu 1024 mb ram software requirements server requires java clients can be any computer with a supported web browser availability open source cost free program language javascript and java software access https github com yss123yss opengms teammodeling 1 introduction the earth s environmental system is a complex mix of natural and synthetic systems that includes massive and dynamic chemical biological and physical processes as well as complicated human activities gould et al 2008 walker and chapra 2014 kay et al 2015 tao et al 2019 due to this complexity various geo problems defined as problems that occur in geographic world need to be explored for example environmental issues such as global warming air pollution and desertification bopp et al 2005 kiesewetter et al 2015 xu et al 2016 decision making and planning issues such as shortest path finding shopping center site selection and urban expansion planning önüt et al 2010 shahzada and askar 2011 arsanjani et al 2013 and human activity issues such as traffic congestion information propagation via social media and indoor evacuation sheu et al 2004 he et al 2015 tang et al 2016 targeting the solution of different geo problems a range of geo analysis models have been studied and developed in different domains al sabhan et al 2003 selvam et al 2014 yanosky et al 2014 basra et al 2017 shahparakia et al 2017 a geo analysis model is the result of geographic modeling and can be employed to analyze geo problems chen et al 2018 zhang et al 2019 normally solving a complex geo problem requires interdisciplinary knowledge but a single model generally focuses on certain research areas and has limited functionalities thus integrated modeling is increasingly accepted as an effective method to help simulate and analyze the environment as a more holistic and comprehensive system carmona et al 2013 hurrell et al 2013 voinov and shugart 2013 voinov et al 2018 considerable progress has been made in integrated environmental modeling iem laniak et al 2013 in addition to studies on integrating specific individual models to achieve domain related modeling targets chen et al 2011 carmona et al 2013 foster et al 2016 harjeet kaur et al 2019 the development of integrated modeling frameworks imfs has gained widespread acceptance examples include the earth system modeling framework esmf the object modeling system oms and the open modeling interface openmi hill et al 2004 collins et al 2005 moore and tindall 2005 fotopoulos et al 2010 david et al 2013 formetta et al 2016 in an imf which can be used to organize and couple multidisciplinary models the involved models are normally wrapped according to the framework related programming specifications in this context a model can be regarded as a pluggable model component based on the standard invoking interface and data structure provided by the imf a model component serves as a software module and a modeling solution a solution for solving certain geo problems by integrating different models and data can be constructed by assembling different modules david et al 2013 whelan et al 2014 however the imf approach requires all involved model components to be organized in a standalone framework thus making it difficult for dispersed modelers to work together and integrate different imfs to solve complex geo problems to address these limitations the emergence and development of service oriented architecture soa have promoted the sharing and integration of different models as model services in a distributed web environment nativi et al 2013 walker and chapra 2014 in this article a model service represents a web based model component that can be invoked through message based commands many ideas and technical solutions regarding the web based integration of models have been proposed such as the model web geller and turner 2007 granell et al 2013 and model as a service maas li et al 2017 wen et al 2013 in addition a range of methods for converting an original model to a model service have been studied such as the web processing service wps based method proposed by the open geospatial consortium ogc castronova et al 2013 stasch et al 2016 the basic model interface bmi based method goodall and peckham 2016 jiang et al 2017 and other service oriented model encapsulation methods yue et al 2016 zhang et al 2019 in the web environment model services are invoked through web messaging commands and the integration of different models is achieved by orchestrating distributed model services meng et al 2009 de jesus et al 2012 giuliani et al 2012 sun et al 2012 belete et al 2017 with these services a scientific workflow that represents the logic and relations among different models can be constructed to help modelers solve complex geo problems schaeffer 2008 chen et al 2010 pratt et al 2010 however despite the advantages of using model services current research on integrating models in the web environment has mainly focused on technical solutions for chaining different model services the process of how to discover and collect appropriate existing models and data to solve geo problems received less attention to solve a complex geo problem which typically requires integrating knowledge and resources from multiple disciplines participating modelers must generally communicate ideas with each other and discuss whether the designed modeling solution is rational before linking detailed model services if the integration logic cannot correctly represent the modeling target or the scientific question the designed integrated modeling solution is not acceptable even if the corresponding model services can be chained together such a modeling process primarily depends on the teamwork of dispersed modelers and unambiguous communication is important for designing and constructing an integrated modeling solution in this article a teamwork oriented modeling approach is proposed to support modelers in collaboratively exploring modeling solutions in the web environment this approach is built based on the open geographic modeling and simulation opengms platform wang et al 2018 yue et al 2018 zhang et al 2019 and the open source mxgraph project https github com jgraph mxgraph is employed to help modelers design models through a graphic user interface gui three basic modeling phases are considered when using the proposed approach to design and construct an integrated modeling solution 1 a conceptual modeling phase to discuss and build graph based conceptual models that represent modeling targets at the conceptual level 2 a logical modeling phase to discuss and build flowchart based logical models that represent the organization and execution process of the designed integrated modeling solution and 3 a computational modeling phase to discuss and build service configuration based models that contain detailed model services and data services and that ultimately produce the modeling results within these three phases participating modelers in a team can analyze a geo problem from different perspectives and communicate personal ideas via the graph and flowchart that are interactively constructed or modified when all participating modelers agree with the collaboratively constructed conceptual logical and computational models the resulting modeling solution for the geo problem could be more reasonable and fit for purpose the remainder of this article is structured as follows the basic concept of the proposed approach is introduced in section 2 in section 3 the detailed conceptual logical and computational modeling method are explained the discussion process associated with using the proposed modeling approach is introduced in section 4 section 5 presents the designed prototype system and introduces experiments on using the proposed methods to support solving geo problems in the web environment finally the conclusion and discussion are presented in section 6 2 basic concept in this paper the process of exploring solutions for a complex geo problem is based on the collection of different modeling resources resources that are used for solving geo problems all required models data and computers for building a modeling solution are provided by participating modelers determining how to discover suitable modeling resources and integrate them to form a reasonable modeling solution depends on the communication and discussion among the participating modelers a participating modeler can be a provider or user of model resources data computational resources or others seeking answers to scientific or decision making questions due to the interdisciplinary characteristics embedded in a geo problem participants with different disciplinary backgrounds need to understand each other and offer their own domain related ideas clearly to make sure an integrated modeling solution is reasonable fig 1 illustrates the basic process of collaboratively building an integrated modeling solution to address a specific geo problem the research scope must first be identified after the research question is defined different participants with related disciplinary backgrounds are invited to form a collaborative modeling team according to the discussion among the modeling team the research scope can be refined and the collaborative modeling team can be modified once the research scope and the modeling team are constructed a related modeling resources e g models data computers etc need to be collected to support the integrated modeling work throughout the entire modeling process different modeling resources can be collaboratively added to the modeling solution by participating modelers b the modeling team analyzes the scientific question by constructing different conceptual models and the designed conceptual models will likely be discussed and modified by all participants c based on the constructed conceptual models the modeling team can design the corresponding integrated logic with the support of previously collected modeling resources according to the discussion results among different modelers if the designed solution is found to have errors modifications may be needed to revise the previously constructed conceptual models and the integrated model can also be refined d by configuring related models data with appropriate computers the designed integrated modeling solution can be implemented and executed and if errors or abnormal situations occur in the execution process the previously designed conceptual model and integrated logic require corresponding refinements e finally if the integrated model is executed successfully the modeling team can analyze the results to evaluate whether the proposed question has been solved the modeling process can be re undertaken to achieve more reasonable modeling results the flow of conceptual modeling logical modeling and computational modeling is designed to help modelers communicate and discuss ideas in the process of exploring modeling solutions an appropriate modeling solution can be arrived at through the discussions that arise within each of the three phases 1 when attempting to employ different models and data to solve a certain complex geo problem the conceptual principle s behind the integration must be confirmed as reasonable for instance a runoff model that is designed for wet areas should not be employed in the modeling of arid areas to support modelers with different disciplinary backgrounds and enable modelers to understand each other a graph based conceptual modeling method is designed in section 3 1 by attaching corresponding conceptual information to a graph modelers can collaboratively modify and improve the conceptual model to make sure the integrated modeling solution is conceptually rational 2 with a logical model the execution process of an integrated modeling solution can be described as a formalized workflow the execution process must be shown to be logically rational before chaining detailed models and data to execute the modeling solution for instance if a model s output data are another model s input data such data must have the appropriate data format spatial resolution spatial extent etc to support modelers in discussing the execution process of a modeling solution a flowchart based logical modeling method is designed in section 3 2 in the flowchart of a logical model the relationships among different models and data can be formulated and represented in a structured manner 3 to drive the integrated modeling solution to execution the detailed models and data involved published in different computers in the web environment as model and data services respectively should be able to interoperate with each other in the web environment such interoperability requires the integration of different services to support modelers discussions and to confirm a modeling solution is rational a service configuration based computational modeling method is designed in section 3 3 with which the involved model services can be configured to be executable and the involved data services can be configured to be accessible in the proposed method an object oriented analysis and design ooad approach is employed which has been used in other imf projects van ittersum et al 2008 knapen et al 2013 whelan et al 2014 this article proposes using the ooad strategy to help participants discuss geo problems structurally and collaboratively working together in the web environment these three types of models are not automatically binded together different participants can contribute their ideas in each modeling phase models constructed by one participating modeler can be modified by other modelers resulting in different versions of models discussions that arise through this process rational modeling solutions for solving certain geo problems can be built 3 detailed process of teamwork oriented modeling the method proposed in this article mainly focuses on providing a structured and understandable approach for modelers to collaboratively exchange ideas throughout the modeling process in the web environment the communication among dispersed modelers is based on two basic types of extensible markup language xml documents 1 a graph document which is collaboratively generated via the mxgraph based modeling gui and 2 a model document which is collaboratively generated with the following conceptual logical and computational modeling methods 3 1 graph based conceptual modeling method the conceptual modeling method is designed to help modelers discuss the conceptual idea of a modeling solution to help participating modelers express conceptual ideas clearly information of a conceptual model is presented through a graph with the layout of different graphics e g images text arrow lines etc a conceptual model can be represented visually compared with plain text using graphics to present a conceptual model can improve the ability of modelers with diverse cultural and disciplinary backgrounds to understand each other fig 2 provides an example of a conceptual model used to represent the precipitation process in the soil and water assessment tool model swat https swat tamu edu based on the graphics left part of fig 2 the precipitation process can be introduced to participants who have little knowledge about watershed modeling in addition different conceptual items right part of fig 2 are embedded in the graph a conceptual modeling scene is constructed based on these conceptual items and graphics in addition there are three basic types of conceptual items element system and relation 1 element describes an object involved in the modeling process according to different perspectives of a conceptual model the object can be used to represent a phenomenon that exists in the real world e g rainfall wind an abstraction of a geographic object e g cloud forest ocean or a generalization of geographic information processing approaches e g data assimilation sampling in fig 2 precipitation evaporation surface runoff lateral flow and base flow are elements contained in the sample conceptual scene 2 system describes a collection of items that can be regarded as a whole a system is mainly used by conceptual model designers to present information about a modeling scene more clearly a system has its own functionalities which are contributed by all the involved elements and relations as shown in fig 2 there are four systems the root layer system the unsaturated layer system the shallow aquifer system and the deep aquifer system each system contains its related elements and relations for example the element surface runoff belongs to the root layer system a system can also be composed of different subsystems and a subsystem is also a system such that systems form a hierarchical structure 3 relation describes the interactive relationship among different elements and systems in a modeling scene the relationship can be system system element element or system element in fig 2 infiltration is used to explain the relation between the root layer system and the unsaturated layer system percolation and capillary rise are used to explain the relation between the unsaturated layer system and the shallow aquifer system and another percolation is used to explain the relation between the shallow aquifer system and the deep aquifer system a conceptual model is formed based on the organization of element system relation and the explanatory content is expressed based on various concept items which are attached on elements systems and relations and a conceptual document is automatically constructed through the interactive operation of graphics in a conceptual document all of the involved elements systems and relations are presented as an individual xml node each element system and relation node owns a graph attribute to link with a graphic in the graph document base on the refinement and modification of the graph the conceptual document can be updated asynchronously and ideas from different modelers can be collaboratively integrated 3 2 flowchart based method for logical model building based on the constructed conceptual model the basic solution to geo problems can be formulated in the logical modeling phase participants need to determine which models and data are used to build the modeling solution and the execution process of integrated models should also be discussed to ensure rationality the flowchart based logical modeling method employs the workflow idea to express logical models the execution process is collaboratively constructed by different participants and each participant can interactively edit the execution process by modifying the visual flowchart as shown in fig 3 a flowchart contains a shape indicating the start of the execution process and a shape indicating the end of the execution process in addition there are five basic types of flowchart items for the construction of a logical model 1 model item used to indicate a model and includes metadata on its inputs and outputs 2 data item used to indicate data involved a data item can be provided by modelers or can be generated by the computation of a certain model and or by the data processing method 3 condition item used to indicate a judgment regarding the selection of different models or data in the execution process a condition item must be linked with at least one data item and multiple branches can be derived from a condition item for example a condition item can be used to describe the selection of different models according to the given data 4 operation item used to indicate a data processing method an operation item can be used to prepare data according to the demands of a model and can also be used to process the output data of a model according to the analysis requirements of modelers 5 dependency item used to indicate a relationship between two of the previously defined four types of flowchart items the arrow line points in a single direction and the flowchart item on the end side depends on the one on the start side a logical model can be constructed based on the combination of different flowchart items as shown in fig 4 the basic logic of a watershed delineation model which can be integrated with the swat model is employed to explain the flowchart based method the pitremove model is used to identify all pits in the digital elevation model dem and raises their elevation and the watersheddelineation model which depends on the calculation results of the pitremove model is used to identify all the subbasins in the study area there are four views with which to present a logical model 1 the integration view depicts the main structure of the integrated models with model items and condition items based on this view modelers can discuss the general integration logic among different models in fig 4 the pitremove model and two watersheddelineation models are linked together by a condition item to determine whether a stream network exists 2 the model dependency view depicts the relationship of a model item and different data items both input and output data are included based on this view modelers can discuss the logic for integrating a model with different required data in fig 4 the pitremove model depends on the originalgrid data and generates the pitremovedgrid data 3 the condition dependency view depicts the relationship of a condition item and its related data items and model items based on this view modelers can discuss the logic for selecting different models based on a defined condition in fig 4 the streamnetexist condition item is used to present the selection of different watersheddelineation models if the stream network does not exist then the watersheddelineation generatestreamnet model is used otherwise the watersheddelineation usingexistedstreamnet model is used 4 the data dependency view depicts the relationship of a data item and its related operation item an operation item can be linked to different data items as input and output based on this view modelers can discuss the logic for processing different data in fig 4 the data processing method operation clipgrid is used to clip pitremovedgrid with maskshape data and generate the maskedgrid data a logical document is organized based on the flowchart elements participating modelers can exchange ideas through the collaboratively modification of logical documents which are automatically generated with the mxgraph based gui and appropriate logical models can be interactively explored 3 3 service configuration based method for computational model building in the computational modeling phase detailed existing model services for invoking models data services for providing data and operation services for processing data are configured to implement and execute the logical model as shown in fig 5 the service configuration task is collaboratively conducted based on the flowchart of a logical model moreover these services are designed to be accessed in a rest representation state transfer manner the information pertaining to a model service data service or operation service must be organized via the serviceinstance object introduced in fig 5 a serviceinstance contains the following a unique identification the instanceid attribute a human readable name the instancename attribute a more detailed description the description attribute and restful web request information the request attribute which links to a servicerequest object in the servicerequest object the baseurl attribute indicates the location of the service and the parameterlist attribute indicates the query parameters for invoking the service with the key and the value attributes defined according to the restful specification in the requestparameter object through the interactive configuration of different model services data services and operation services a computational document can be formed the detailed information about how to invoke the involved model services data services and operation services is recorded as xml nodes in a computational document and this document is automatically generated and modified based on the operation in the modeling gui with this service configuration based method participating modelers can share ideas and configure the computation model based on the graph that is built in the logical modeling phase to execute a computational model which can be seen as an assembly of different services a service invoke engine is designed to load a computational model and invoke the involved model services data services and operation services according to the dependency information configured by modelers a model service list is built in the engine and the list will be iterated as long as it is not empty whether a model service can be invoked is judged based on the preparation status of its depended data services and operation services if a model service has been invoked it would be removed from the model service list once the model service list become empty the iterator will stop running and the execution of the integrated model finishes if participating modelers are not satisfied with the results modification can be made to the conceptual logical and computational models 4 discussing modeling solutions based on the proposed method to help dispersed modelers collaboratively discuss ideas different versions of the conceptual logical and computational models are organized in their respective repositories as shown in fig 6 participants can design different versions of these three types of models and determine which are the most suitable for the geo problem at hand in addition a solution repository is also offered for participants to combine these different models together for use different modeling solutions can also be discussed and compared by participants to seek better solutions for solving the problems examined when a modeling solution is approved by all participating modelers the detailed execution work can be conducted according to the execution results modelers can continually modify and refine the modeling solution in addition participating modelers can discuss the rationality of these versions of models through web communicating approaches e g email lists blogs message boards etc therefore teamwork among different modelers can be achieved by constructing and discussing different versions of modeling solutions 5 experiments to demonstrate the capability and feasibility of the proposed modeling approach a prototype system was developed and two modeling cases in the web environment were established 5 1 prototype system fig 7 illustrates the basic architecture of the prototype modeling system in this system a modeling group can be formed by the registration of geographically dispersed modelers through the web portal within a modeling group communication and discussion among different modelers occur inside the opengms platform through which the participating modelers can exchange their ideas and opinions a modeling group features a manager who proposes the theme of a modeling task a participant in a modeling group can provide modeling resources and other participants can reuse these modeling resources throughout the discussion process modeling resources in this prototype system are divided into three categories 1 conceptual modeling related resources shared resources in this category include icons used for drawing graphics of a conceptual model and concepts used for explaining the involved knowledge in a conceptual model 2 logical modeling related resources model data and operation items are the shared resources in this category these are used to describe the models the data and the operational resources such as the data processing methods involved in the modeling task 3 computational modeling related resources the principal resource in this category are the urls of model services which can be involved remotely and serve as chainable modules in the integrated model as well as the locations of data and data operation services individual supporting resources are stored in a repository for that particular class of resource item in other words model data and operation items are kept in their respective repositories as are other relevant resources and can be imported from these stores for reuse at a later date these resource stores and their relationship to the modeling process is depicted in figure 7 in addition three tools were designed to help modelers construct conceptual logical and computational models 1 the conceptual model builder 2 the logical model builder and 3 the computational model builder in the conceptual model builder as shown in fig 8 a various icons and other drawing shapes text arrow lines etc can be dragged and dropped into the sketchpad and the concept information can be linked to corresponding drawing shapes based on the concept linking and the layout of different drawing shapes the system element relation structure of a conceptual model can be formed in the logical model builder as shown in fig 8 b the predefined drawing shapes which represent a model item a data item a condition item an operation item or a dependency item can be dragged and dropped into the sketchpad each drawing shape in the diagramming sketchpad can be linked to corresponding model items data items and operation items in the computational model builder as shown in fig 8 c a logical model is imported to initialize the service integrating structure of the computational model the involved model items can be configured with model services the involved data items can be configured with data services or with the computing results of other model services or operation services and the involved operation items can be configured with operation services to drive the integrated modeling solution to execution a lightweight execution engine is developed based on the design introduced in section 3 3 5 2 modeling case 1 this modeling case concerns an environmental health issue in the geographic environment the occurrence of human disease can be influenced by various factors such as environmental attributes man made pollution and human activities in this case a demonstrative application scene involves one researcher proposing an issue in a certain study area neural tube defects ntds are reported frequently in newborn children and this researcher wants to know which factors are responsible for the ntds and what the importance of each factor is this modeling case and demonstrative data are mainly adapted from the study by wang et al 2010 first this researcher question raiser tries to build a team with other researchers to solve the proposed problem collaboratively in this team researchers hail from various fields such as disease control watershed modeling and geographic information systems gis different researchers have different opinions and understandings about the ntd problem with the help of the conceptual modeling builder participants in this team can communicate ideas and try to reach an agreement with each other for building a solution in fig 9 a sample conceptual model for detecting the relationship between a potential factor and the ntd ratio is presented the disease cases are distributed in region a and this region should be divided as a grid g g1 g2 gn assume the geographic attribute d is the factor that the researchers attempt to associate with the disease which is distributed in space as d1 d2 and d3 the proposed approach at the conceptual level is to overlay the disease ratio to d1 d2 and d3 and then use the geodetector model www geodetector cn to identify the relationship wang et al 2016 wang and xu 2017 then based on the agreement to use the geodetector model to solve this problem the corresponding logical model is collaboratively constructed via using the logical model builder in fig 10 an example logical model is presented which mainly includes four model items and six data items the model items are 1 risk detector analysis 2 factor detector analysis 3 ecological detector analysis and 4 interaction detector analysis the data items are 1 incidence rate of disease occurrence 2 elevation 3 soil type 4 watershed 5 vegetable production and 6 amount of fertilizer used these model and data items can be provided by different participants finally based on the logical model a computational model can be constructed by configuring corresponding services urls to each model item data item and operation item involved the xml document generated by the computational model builder representing the configuration is then parsed by the execution engine in the execution engine the configured data services are invoked to obtain data content and transfer them to the computers that publish model services then the model services can be invoked to obtain results a message indicating that certain result data have been generated is sent to each participant and participants can download and analyze the result data fig 11 depicts an example teamwork process based on the proposed method participant a b and c work together to solve the ntd case participant a proposed an idea and draft a logical model include the risk detector and factor detector analysis modules using the logical model builder the updated flowchart is simultaneously displayed to participant b and c as the changes occure participant b modify the logical model by adding the ecological and interaction detector analysis modules participant c receive documents and try to understand the logical model and provide some suggestions along with the design process once the logical model is accepted by all participants the team can continue to design the computational model collaboratively fig 12 provides snapshots of the result data based on the factor detector results presented in fig 12 b the watershed has the largest q statistic value which means rivers in the study area is the dominant factor determining the spatial distribution of ntds participants can also discuss the results and if the results are not suitable for solving the proposed issue modifications and improvements can be made to the previous conceptual logical and computational models 5 3 modeling case 2 a hydrology related modeling case is examined to introduce the designed methods and the prototype modeling system in this modeling case terrain analysis models for delineating watersheds a hydrological response unit hru generating model and the soil and water assessment tool swat are integrated to calculate hydrological characteristics of the study area the basic integration process of this modeling case uses terrain analysis models to delineate watersheds uses watershed delineation results to generate hrus and finally drives the swat model to execution see table 1 fig 13 presents part of the conceptual model constructed in this modeling case each graphic links to a corresponding concept item and the conceptual model can be interactively constructed with the help of this conceptual model builder participating modelers can share and exchange ideas in the logical modeling phase the integrated model is constructed by two modelers in the team based on their disciplinary knowledge and resources the first modeler is in charge of the watershed delineation model and the second modeler is in charge of the hru and swat model from the pitremove model to the streamnet model related terrain analysis models were integrated for the watersheddelineation model as shown in fig 14 a integration logic of the watersheddelineation model is designed by the first modeler and the corresponding logical document is send to the second modeler then the hru and swat model are added into the logical model by the second modeler the detailed model data dependency relations are designed based on the previous logical model as presented in fig 14 b these two participants can see the update process asynchronously via the web transfer of logical document based on the constructed logical model detailed model services data services and operation services are configured to form the corresponding computational model by loading the computational document the execution engine generates the model service list the execution engine is implemented as an independent tool which is not tightly bound with the conceptual logical computational modeling phases based on the execution results fig 15 presents certain computation results for this modeling case 6 conclusion and future work this article describes a teamwork oriented modeling approach to help dispersed modelers share knowledge and communicate ideas the proposed approach involves three modeling phases a conceptual modeling phase a logical modeling phase and a computational modeling phase in each phase the corresponding modeling method is designed a graph based method a flowchart based method and a service configuration based method are employed to build conceptual models logical models and computational models respectively based on the established prototype modeling system integrated modeling solutions can be discussed and constructed through two experimental modeling cases the feasibility and capability of the proposed modeling approach were verified however due to the complexity and heterogeneity of conducting environmental modeling work in different research areas future work on web based environmental modeling is necessary especially in the following respects 1 research into the real time collaboration method for constructing an integrated modeling solution in the web environment in this study the discussion and communication among different modelers are based on the version databases of models a modeler acquires a certain version of a model from the corresponding model database makes modifications to this model and posts a new version of the model in the model database that can be modified by other participating modelers in cases in which modelers must communicate modeling ideas in a timely manner for example emergency planning for certain ongoing accidents or disasters the collaborative process must be conducted in real time such a real time collaborative modeling method must satisfy the demands of the real time communication of modelers and account for the real time configuration of different modeling resources lin et al 2013a lin et al 2013b 2 a service recommendation method to help modelers explore appropriate modeling resources to solve geo problems in the web environment along with the development of web based modeling studies increasing numbers of model resources and data resources have been published as reusable services by building the conceptual and logical models the modeling resources required in an integrated modeling task can be described based on the description of these modeling resources studies on the automatic recommendation of appropriate services can largely promote the effective construction of a computational model 3 validation and comparison studies of the collaboratively constructed modeling solutions to holistically and synthetically solve geo problems the modeling process can be regarded as a process of seeking better answers in which different modeling solutions can be collaboratively constructed these modeling solutions must be validated to confirm whether they represent the involved modeling objects correctly the comparison of different modeling solutions can also help modelers discover limitations and explore better solutions therefore studies on the validation of an integrated modeling solution and the comparison of different integrated modeling solutions are necessary to help modelers conduct collaborative modeling tasks acknowledgments we appreciate the detailed suggestions and comments from the editor and the anonymous reviewers we would especially like to acknowledge takuya iwanaga for suggestions provided to improve this article the work described in this article involves many geo analysis models and was supported by the following research programs the national natural science foundation of china grant number 41701441 the nsf for excellent young scholars of china grant number 41622108 the national basic research program of china grant number 2015cb954103 the priority academic program development of jiangsu higher education institutions grant number 164320h116 and the outstanding innovation team in colleges and universities in jiangsu province 
26173,currently studies on constructing integrated modeling solutions in a web based environment primarily focus on the chaining of different model services while the flow of exploring solutions for complex geo problems geographically distributed issues has been less studied to address this gap a teamwork oriented integrated modeling approach to supporting geo problem solving based on the collaborative work of different modelers is proposed in this article with a workflow from conceptual logical and finally computational modeling in this approach conceptual logical and computational models are discussed and constructed using a graph based method a flowchart based method and a service configuration based method respectively by building the corresponding repositories for managing and comparing different versions of conceptual logical computational models dispersed modelers can propose their own ideas and explore appropriate solutions based on existing modeling resources a prototype modeling system was established and two experimental modeling cases were conducted to verify the feasibility and capability of the proposed approach keywords integrated modeling teamwork oriented modeling conceptual model logical model computational model workflow software availability architecture designer min chen songshan yue and yongning wen software developer songshan yue and tao hou address no 1 wenyuan road qixia district nanjing 210023 p r china e mail yss123yss 126 com first available 2018 hardware requirements 1 ghz cpu 1024 mb ram software requirements server requires java clients can be any computer with a supported web browser availability open source cost free program language javascript and java software access https github com yss123yss opengms teammodeling 1 introduction the earth s environmental system is a complex mix of natural and synthetic systems that includes massive and dynamic chemical biological and physical processes as well as complicated human activities gould et al 2008 walker and chapra 2014 kay et al 2015 tao et al 2019 due to this complexity various geo problems defined as problems that occur in geographic world need to be explored for example environmental issues such as global warming air pollution and desertification bopp et al 2005 kiesewetter et al 2015 xu et al 2016 decision making and planning issues such as shortest path finding shopping center site selection and urban expansion planning önüt et al 2010 shahzada and askar 2011 arsanjani et al 2013 and human activity issues such as traffic congestion information propagation via social media and indoor evacuation sheu et al 2004 he et al 2015 tang et al 2016 targeting the solution of different geo problems a range of geo analysis models have been studied and developed in different domains al sabhan et al 2003 selvam et al 2014 yanosky et al 2014 basra et al 2017 shahparakia et al 2017 a geo analysis model is the result of geographic modeling and can be employed to analyze geo problems chen et al 2018 zhang et al 2019 normally solving a complex geo problem requires interdisciplinary knowledge but a single model generally focuses on certain research areas and has limited functionalities thus integrated modeling is increasingly accepted as an effective method to help simulate and analyze the environment as a more holistic and comprehensive system carmona et al 2013 hurrell et al 2013 voinov and shugart 2013 voinov et al 2018 considerable progress has been made in integrated environmental modeling iem laniak et al 2013 in addition to studies on integrating specific individual models to achieve domain related modeling targets chen et al 2011 carmona et al 2013 foster et al 2016 harjeet kaur et al 2019 the development of integrated modeling frameworks imfs has gained widespread acceptance examples include the earth system modeling framework esmf the object modeling system oms and the open modeling interface openmi hill et al 2004 collins et al 2005 moore and tindall 2005 fotopoulos et al 2010 david et al 2013 formetta et al 2016 in an imf which can be used to organize and couple multidisciplinary models the involved models are normally wrapped according to the framework related programming specifications in this context a model can be regarded as a pluggable model component based on the standard invoking interface and data structure provided by the imf a model component serves as a software module and a modeling solution a solution for solving certain geo problems by integrating different models and data can be constructed by assembling different modules david et al 2013 whelan et al 2014 however the imf approach requires all involved model components to be organized in a standalone framework thus making it difficult for dispersed modelers to work together and integrate different imfs to solve complex geo problems to address these limitations the emergence and development of service oriented architecture soa have promoted the sharing and integration of different models as model services in a distributed web environment nativi et al 2013 walker and chapra 2014 in this article a model service represents a web based model component that can be invoked through message based commands many ideas and technical solutions regarding the web based integration of models have been proposed such as the model web geller and turner 2007 granell et al 2013 and model as a service maas li et al 2017 wen et al 2013 in addition a range of methods for converting an original model to a model service have been studied such as the web processing service wps based method proposed by the open geospatial consortium ogc castronova et al 2013 stasch et al 2016 the basic model interface bmi based method goodall and peckham 2016 jiang et al 2017 and other service oriented model encapsulation methods yue et al 2016 zhang et al 2019 in the web environment model services are invoked through web messaging commands and the integration of different models is achieved by orchestrating distributed model services meng et al 2009 de jesus et al 2012 giuliani et al 2012 sun et al 2012 belete et al 2017 with these services a scientific workflow that represents the logic and relations among different models can be constructed to help modelers solve complex geo problems schaeffer 2008 chen et al 2010 pratt et al 2010 however despite the advantages of using model services current research on integrating models in the web environment has mainly focused on technical solutions for chaining different model services the process of how to discover and collect appropriate existing models and data to solve geo problems received less attention to solve a complex geo problem which typically requires integrating knowledge and resources from multiple disciplines participating modelers must generally communicate ideas with each other and discuss whether the designed modeling solution is rational before linking detailed model services if the integration logic cannot correctly represent the modeling target or the scientific question the designed integrated modeling solution is not acceptable even if the corresponding model services can be chained together such a modeling process primarily depends on the teamwork of dispersed modelers and unambiguous communication is important for designing and constructing an integrated modeling solution in this article a teamwork oriented modeling approach is proposed to support modelers in collaboratively exploring modeling solutions in the web environment this approach is built based on the open geographic modeling and simulation opengms platform wang et al 2018 yue et al 2018 zhang et al 2019 and the open source mxgraph project https github com jgraph mxgraph is employed to help modelers design models through a graphic user interface gui three basic modeling phases are considered when using the proposed approach to design and construct an integrated modeling solution 1 a conceptual modeling phase to discuss and build graph based conceptual models that represent modeling targets at the conceptual level 2 a logical modeling phase to discuss and build flowchart based logical models that represent the organization and execution process of the designed integrated modeling solution and 3 a computational modeling phase to discuss and build service configuration based models that contain detailed model services and data services and that ultimately produce the modeling results within these three phases participating modelers in a team can analyze a geo problem from different perspectives and communicate personal ideas via the graph and flowchart that are interactively constructed or modified when all participating modelers agree with the collaboratively constructed conceptual logical and computational models the resulting modeling solution for the geo problem could be more reasonable and fit for purpose the remainder of this article is structured as follows the basic concept of the proposed approach is introduced in section 2 in section 3 the detailed conceptual logical and computational modeling method are explained the discussion process associated with using the proposed modeling approach is introduced in section 4 section 5 presents the designed prototype system and introduces experiments on using the proposed methods to support solving geo problems in the web environment finally the conclusion and discussion are presented in section 6 2 basic concept in this paper the process of exploring solutions for a complex geo problem is based on the collection of different modeling resources resources that are used for solving geo problems all required models data and computers for building a modeling solution are provided by participating modelers determining how to discover suitable modeling resources and integrate them to form a reasonable modeling solution depends on the communication and discussion among the participating modelers a participating modeler can be a provider or user of model resources data computational resources or others seeking answers to scientific or decision making questions due to the interdisciplinary characteristics embedded in a geo problem participants with different disciplinary backgrounds need to understand each other and offer their own domain related ideas clearly to make sure an integrated modeling solution is reasonable fig 1 illustrates the basic process of collaboratively building an integrated modeling solution to address a specific geo problem the research scope must first be identified after the research question is defined different participants with related disciplinary backgrounds are invited to form a collaborative modeling team according to the discussion among the modeling team the research scope can be refined and the collaborative modeling team can be modified once the research scope and the modeling team are constructed a related modeling resources e g models data computers etc need to be collected to support the integrated modeling work throughout the entire modeling process different modeling resources can be collaboratively added to the modeling solution by participating modelers b the modeling team analyzes the scientific question by constructing different conceptual models and the designed conceptual models will likely be discussed and modified by all participants c based on the constructed conceptual models the modeling team can design the corresponding integrated logic with the support of previously collected modeling resources according to the discussion results among different modelers if the designed solution is found to have errors modifications may be needed to revise the previously constructed conceptual models and the integrated model can also be refined d by configuring related models data with appropriate computers the designed integrated modeling solution can be implemented and executed and if errors or abnormal situations occur in the execution process the previously designed conceptual model and integrated logic require corresponding refinements e finally if the integrated model is executed successfully the modeling team can analyze the results to evaluate whether the proposed question has been solved the modeling process can be re undertaken to achieve more reasonable modeling results the flow of conceptual modeling logical modeling and computational modeling is designed to help modelers communicate and discuss ideas in the process of exploring modeling solutions an appropriate modeling solution can be arrived at through the discussions that arise within each of the three phases 1 when attempting to employ different models and data to solve a certain complex geo problem the conceptual principle s behind the integration must be confirmed as reasonable for instance a runoff model that is designed for wet areas should not be employed in the modeling of arid areas to support modelers with different disciplinary backgrounds and enable modelers to understand each other a graph based conceptual modeling method is designed in section 3 1 by attaching corresponding conceptual information to a graph modelers can collaboratively modify and improve the conceptual model to make sure the integrated modeling solution is conceptually rational 2 with a logical model the execution process of an integrated modeling solution can be described as a formalized workflow the execution process must be shown to be logically rational before chaining detailed models and data to execute the modeling solution for instance if a model s output data are another model s input data such data must have the appropriate data format spatial resolution spatial extent etc to support modelers in discussing the execution process of a modeling solution a flowchart based logical modeling method is designed in section 3 2 in the flowchart of a logical model the relationships among different models and data can be formulated and represented in a structured manner 3 to drive the integrated modeling solution to execution the detailed models and data involved published in different computers in the web environment as model and data services respectively should be able to interoperate with each other in the web environment such interoperability requires the integration of different services to support modelers discussions and to confirm a modeling solution is rational a service configuration based computational modeling method is designed in section 3 3 with which the involved model services can be configured to be executable and the involved data services can be configured to be accessible in the proposed method an object oriented analysis and design ooad approach is employed which has been used in other imf projects van ittersum et al 2008 knapen et al 2013 whelan et al 2014 this article proposes using the ooad strategy to help participants discuss geo problems structurally and collaboratively working together in the web environment these three types of models are not automatically binded together different participants can contribute their ideas in each modeling phase models constructed by one participating modeler can be modified by other modelers resulting in different versions of models discussions that arise through this process rational modeling solutions for solving certain geo problems can be built 3 detailed process of teamwork oriented modeling the method proposed in this article mainly focuses on providing a structured and understandable approach for modelers to collaboratively exchange ideas throughout the modeling process in the web environment the communication among dispersed modelers is based on two basic types of extensible markup language xml documents 1 a graph document which is collaboratively generated via the mxgraph based modeling gui and 2 a model document which is collaboratively generated with the following conceptual logical and computational modeling methods 3 1 graph based conceptual modeling method the conceptual modeling method is designed to help modelers discuss the conceptual idea of a modeling solution to help participating modelers express conceptual ideas clearly information of a conceptual model is presented through a graph with the layout of different graphics e g images text arrow lines etc a conceptual model can be represented visually compared with plain text using graphics to present a conceptual model can improve the ability of modelers with diverse cultural and disciplinary backgrounds to understand each other fig 2 provides an example of a conceptual model used to represent the precipitation process in the soil and water assessment tool model swat https swat tamu edu based on the graphics left part of fig 2 the precipitation process can be introduced to participants who have little knowledge about watershed modeling in addition different conceptual items right part of fig 2 are embedded in the graph a conceptual modeling scene is constructed based on these conceptual items and graphics in addition there are three basic types of conceptual items element system and relation 1 element describes an object involved in the modeling process according to different perspectives of a conceptual model the object can be used to represent a phenomenon that exists in the real world e g rainfall wind an abstraction of a geographic object e g cloud forest ocean or a generalization of geographic information processing approaches e g data assimilation sampling in fig 2 precipitation evaporation surface runoff lateral flow and base flow are elements contained in the sample conceptual scene 2 system describes a collection of items that can be regarded as a whole a system is mainly used by conceptual model designers to present information about a modeling scene more clearly a system has its own functionalities which are contributed by all the involved elements and relations as shown in fig 2 there are four systems the root layer system the unsaturated layer system the shallow aquifer system and the deep aquifer system each system contains its related elements and relations for example the element surface runoff belongs to the root layer system a system can also be composed of different subsystems and a subsystem is also a system such that systems form a hierarchical structure 3 relation describes the interactive relationship among different elements and systems in a modeling scene the relationship can be system system element element or system element in fig 2 infiltration is used to explain the relation between the root layer system and the unsaturated layer system percolation and capillary rise are used to explain the relation between the unsaturated layer system and the shallow aquifer system and another percolation is used to explain the relation between the shallow aquifer system and the deep aquifer system a conceptual model is formed based on the organization of element system relation and the explanatory content is expressed based on various concept items which are attached on elements systems and relations and a conceptual document is automatically constructed through the interactive operation of graphics in a conceptual document all of the involved elements systems and relations are presented as an individual xml node each element system and relation node owns a graph attribute to link with a graphic in the graph document base on the refinement and modification of the graph the conceptual document can be updated asynchronously and ideas from different modelers can be collaboratively integrated 3 2 flowchart based method for logical model building based on the constructed conceptual model the basic solution to geo problems can be formulated in the logical modeling phase participants need to determine which models and data are used to build the modeling solution and the execution process of integrated models should also be discussed to ensure rationality the flowchart based logical modeling method employs the workflow idea to express logical models the execution process is collaboratively constructed by different participants and each participant can interactively edit the execution process by modifying the visual flowchart as shown in fig 3 a flowchart contains a shape indicating the start of the execution process and a shape indicating the end of the execution process in addition there are five basic types of flowchart items for the construction of a logical model 1 model item used to indicate a model and includes metadata on its inputs and outputs 2 data item used to indicate data involved a data item can be provided by modelers or can be generated by the computation of a certain model and or by the data processing method 3 condition item used to indicate a judgment regarding the selection of different models or data in the execution process a condition item must be linked with at least one data item and multiple branches can be derived from a condition item for example a condition item can be used to describe the selection of different models according to the given data 4 operation item used to indicate a data processing method an operation item can be used to prepare data according to the demands of a model and can also be used to process the output data of a model according to the analysis requirements of modelers 5 dependency item used to indicate a relationship between two of the previously defined four types of flowchart items the arrow line points in a single direction and the flowchart item on the end side depends on the one on the start side a logical model can be constructed based on the combination of different flowchart items as shown in fig 4 the basic logic of a watershed delineation model which can be integrated with the swat model is employed to explain the flowchart based method the pitremove model is used to identify all pits in the digital elevation model dem and raises their elevation and the watersheddelineation model which depends on the calculation results of the pitremove model is used to identify all the subbasins in the study area there are four views with which to present a logical model 1 the integration view depicts the main structure of the integrated models with model items and condition items based on this view modelers can discuss the general integration logic among different models in fig 4 the pitremove model and two watersheddelineation models are linked together by a condition item to determine whether a stream network exists 2 the model dependency view depicts the relationship of a model item and different data items both input and output data are included based on this view modelers can discuss the logic for integrating a model with different required data in fig 4 the pitremove model depends on the originalgrid data and generates the pitremovedgrid data 3 the condition dependency view depicts the relationship of a condition item and its related data items and model items based on this view modelers can discuss the logic for selecting different models based on a defined condition in fig 4 the streamnetexist condition item is used to present the selection of different watersheddelineation models if the stream network does not exist then the watersheddelineation generatestreamnet model is used otherwise the watersheddelineation usingexistedstreamnet model is used 4 the data dependency view depicts the relationship of a data item and its related operation item an operation item can be linked to different data items as input and output based on this view modelers can discuss the logic for processing different data in fig 4 the data processing method operation clipgrid is used to clip pitremovedgrid with maskshape data and generate the maskedgrid data a logical document is organized based on the flowchart elements participating modelers can exchange ideas through the collaboratively modification of logical documents which are automatically generated with the mxgraph based gui and appropriate logical models can be interactively explored 3 3 service configuration based method for computational model building in the computational modeling phase detailed existing model services for invoking models data services for providing data and operation services for processing data are configured to implement and execute the logical model as shown in fig 5 the service configuration task is collaboratively conducted based on the flowchart of a logical model moreover these services are designed to be accessed in a rest representation state transfer manner the information pertaining to a model service data service or operation service must be organized via the serviceinstance object introduced in fig 5 a serviceinstance contains the following a unique identification the instanceid attribute a human readable name the instancename attribute a more detailed description the description attribute and restful web request information the request attribute which links to a servicerequest object in the servicerequest object the baseurl attribute indicates the location of the service and the parameterlist attribute indicates the query parameters for invoking the service with the key and the value attributes defined according to the restful specification in the requestparameter object through the interactive configuration of different model services data services and operation services a computational document can be formed the detailed information about how to invoke the involved model services data services and operation services is recorded as xml nodes in a computational document and this document is automatically generated and modified based on the operation in the modeling gui with this service configuration based method participating modelers can share ideas and configure the computation model based on the graph that is built in the logical modeling phase to execute a computational model which can be seen as an assembly of different services a service invoke engine is designed to load a computational model and invoke the involved model services data services and operation services according to the dependency information configured by modelers a model service list is built in the engine and the list will be iterated as long as it is not empty whether a model service can be invoked is judged based on the preparation status of its depended data services and operation services if a model service has been invoked it would be removed from the model service list once the model service list become empty the iterator will stop running and the execution of the integrated model finishes if participating modelers are not satisfied with the results modification can be made to the conceptual logical and computational models 4 discussing modeling solutions based on the proposed method to help dispersed modelers collaboratively discuss ideas different versions of the conceptual logical and computational models are organized in their respective repositories as shown in fig 6 participants can design different versions of these three types of models and determine which are the most suitable for the geo problem at hand in addition a solution repository is also offered for participants to combine these different models together for use different modeling solutions can also be discussed and compared by participants to seek better solutions for solving the problems examined when a modeling solution is approved by all participating modelers the detailed execution work can be conducted according to the execution results modelers can continually modify and refine the modeling solution in addition participating modelers can discuss the rationality of these versions of models through web communicating approaches e g email lists blogs message boards etc therefore teamwork among different modelers can be achieved by constructing and discussing different versions of modeling solutions 5 experiments to demonstrate the capability and feasibility of the proposed modeling approach a prototype system was developed and two modeling cases in the web environment were established 5 1 prototype system fig 7 illustrates the basic architecture of the prototype modeling system in this system a modeling group can be formed by the registration of geographically dispersed modelers through the web portal within a modeling group communication and discussion among different modelers occur inside the opengms platform through which the participating modelers can exchange their ideas and opinions a modeling group features a manager who proposes the theme of a modeling task a participant in a modeling group can provide modeling resources and other participants can reuse these modeling resources throughout the discussion process modeling resources in this prototype system are divided into three categories 1 conceptual modeling related resources shared resources in this category include icons used for drawing graphics of a conceptual model and concepts used for explaining the involved knowledge in a conceptual model 2 logical modeling related resources model data and operation items are the shared resources in this category these are used to describe the models the data and the operational resources such as the data processing methods involved in the modeling task 3 computational modeling related resources the principal resource in this category are the urls of model services which can be involved remotely and serve as chainable modules in the integrated model as well as the locations of data and data operation services individual supporting resources are stored in a repository for that particular class of resource item in other words model data and operation items are kept in their respective repositories as are other relevant resources and can be imported from these stores for reuse at a later date these resource stores and their relationship to the modeling process is depicted in figure 7 in addition three tools were designed to help modelers construct conceptual logical and computational models 1 the conceptual model builder 2 the logical model builder and 3 the computational model builder in the conceptual model builder as shown in fig 8 a various icons and other drawing shapes text arrow lines etc can be dragged and dropped into the sketchpad and the concept information can be linked to corresponding drawing shapes based on the concept linking and the layout of different drawing shapes the system element relation structure of a conceptual model can be formed in the logical model builder as shown in fig 8 b the predefined drawing shapes which represent a model item a data item a condition item an operation item or a dependency item can be dragged and dropped into the sketchpad each drawing shape in the diagramming sketchpad can be linked to corresponding model items data items and operation items in the computational model builder as shown in fig 8 c a logical model is imported to initialize the service integrating structure of the computational model the involved model items can be configured with model services the involved data items can be configured with data services or with the computing results of other model services or operation services and the involved operation items can be configured with operation services to drive the integrated modeling solution to execution a lightweight execution engine is developed based on the design introduced in section 3 3 5 2 modeling case 1 this modeling case concerns an environmental health issue in the geographic environment the occurrence of human disease can be influenced by various factors such as environmental attributes man made pollution and human activities in this case a demonstrative application scene involves one researcher proposing an issue in a certain study area neural tube defects ntds are reported frequently in newborn children and this researcher wants to know which factors are responsible for the ntds and what the importance of each factor is this modeling case and demonstrative data are mainly adapted from the study by wang et al 2010 first this researcher question raiser tries to build a team with other researchers to solve the proposed problem collaboratively in this team researchers hail from various fields such as disease control watershed modeling and geographic information systems gis different researchers have different opinions and understandings about the ntd problem with the help of the conceptual modeling builder participants in this team can communicate ideas and try to reach an agreement with each other for building a solution in fig 9 a sample conceptual model for detecting the relationship between a potential factor and the ntd ratio is presented the disease cases are distributed in region a and this region should be divided as a grid g g1 g2 gn assume the geographic attribute d is the factor that the researchers attempt to associate with the disease which is distributed in space as d1 d2 and d3 the proposed approach at the conceptual level is to overlay the disease ratio to d1 d2 and d3 and then use the geodetector model www geodetector cn to identify the relationship wang et al 2016 wang and xu 2017 then based on the agreement to use the geodetector model to solve this problem the corresponding logical model is collaboratively constructed via using the logical model builder in fig 10 an example logical model is presented which mainly includes four model items and six data items the model items are 1 risk detector analysis 2 factor detector analysis 3 ecological detector analysis and 4 interaction detector analysis the data items are 1 incidence rate of disease occurrence 2 elevation 3 soil type 4 watershed 5 vegetable production and 6 amount of fertilizer used these model and data items can be provided by different participants finally based on the logical model a computational model can be constructed by configuring corresponding services urls to each model item data item and operation item involved the xml document generated by the computational model builder representing the configuration is then parsed by the execution engine in the execution engine the configured data services are invoked to obtain data content and transfer them to the computers that publish model services then the model services can be invoked to obtain results a message indicating that certain result data have been generated is sent to each participant and participants can download and analyze the result data fig 11 depicts an example teamwork process based on the proposed method participant a b and c work together to solve the ntd case participant a proposed an idea and draft a logical model include the risk detector and factor detector analysis modules using the logical model builder the updated flowchart is simultaneously displayed to participant b and c as the changes occure participant b modify the logical model by adding the ecological and interaction detector analysis modules participant c receive documents and try to understand the logical model and provide some suggestions along with the design process once the logical model is accepted by all participants the team can continue to design the computational model collaboratively fig 12 provides snapshots of the result data based on the factor detector results presented in fig 12 b the watershed has the largest q statistic value which means rivers in the study area is the dominant factor determining the spatial distribution of ntds participants can also discuss the results and if the results are not suitable for solving the proposed issue modifications and improvements can be made to the previous conceptual logical and computational models 5 3 modeling case 2 a hydrology related modeling case is examined to introduce the designed methods and the prototype modeling system in this modeling case terrain analysis models for delineating watersheds a hydrological response unit hru generating model and the soil and water assessment tool swat are integrated to calculate hydrological characteristics of the study area the basic integration process of this modeling case uses terrain analysis models to delineate watersheds uses watershed delineation results to generate hrus and finally drives the swat model to execution see table 1 fig 13 presents part of the conceptual model constructed in this modeling case each graphic links to a corresponding concept item and the conceptual model can be interactively constructed with the help of this conceptual model builder participating modelers can share and exchange ideas in the logical modeling phase the integrated model is constructed by two modelers in the team based on their disciplinary knowledge and resources the first modeler is in charge of the watershed delineation model and the second modeler is in charge of the hru and swat model from the pitremove model to the streamnet model related terrain analysis models were integrated for the watersheddelineation model as shown in fig 14 a integration logic of the watersheddelineation model is designed by the first modeler and the corresponding logical document is send to the second modeler then the hru and swat model are added into the logical model by the second modeler the detailed model data dependency relations are designed based on the previous logical model as presented in fig 14 b these two participants can see the update process asynchronously via the web transfer of logical document based on the constructed logical model detailed model services data services and operation services are configured to form the corresponding computational model by loading the computational document the execution engine generates the model service list the execution engine is implemented as an independent tool which is not tightly bound with the conceptual logical computational modeling phases based on the execution results fig 15 presents certain computation results for this modeling case 6 conclusion and future work this article describes a teamwork oriented modeling approach to help dispersed modelers share knowledge and communicate ideas the proposed approach involves three modeling phases a conceptual modeling phase a logical modeling phase and a computational modeling phase in each phase the corresponding modeling method is designed a graph based method a flowchart based method and a service configuration based method are employed to build conceptual models logical models and computational models respectively based on the established prototype modeling system integrated modeling solutions can be discussed and constructed through two experimental modeling cases the feasibility and capability of the proposed modeling approach were verified however due to the complexity and heterogeneity of conducting environmental modeling work in different research areas future work on web based environmental modeling is necessary especially in the following respects 1 research into the real time collaboration method for constructing an integrated modeling solution in the web environment in this study the discussion and communication among different modelers are based on the version databases of models a modeler acquires a certain version of a model from the corresponding model database makes modifications to this model and posts a new version of the model in the model database that can be modified by other participating modelers in cases in which modelers must communicate modeling ideas in a timely manner for example emergency planning for certain ongoing accidents or disasters the collaborative process must be conducted in real time such a real time collaborative modeling method must satisfy the demands of the real time communication of modelers and account for the real time configuration of different modeling resources lin et al 2013a lin et al 2013b 2 a service recommendation method to help modelers explore appropriate modeling resources to solve geo problems in the web environment along with the development of web based modeling studies increasing numbers of model resources and data resources have been published as reusable services by building the conceptual and logical models the modeling resources required in an integrated modeling task can be described based on the description of these modeling resources studies on the automatic recommendation of appropriate services can largely promote the effective construction of a computational model 3 validation and comparison studies of the collaboratively constructed modeling solutions to holistically and synthetically solve geo problems the modeling process can be regarded as a process of seeking better answers in which different modeling solutions can be collaboratively constructed these modeling solutions must be validated to confirm whether they represent the involved modeling objects correctly the comparison of different modeling solutions can also help modelers discover limitations and explore better solutions therefore studies on the validation of an integrated modeling solution and the comparison of different integrated modeling solutions are necessary to help modelers conduct collaborative modeling tasks acknowledgments we appreciate the detailed suggestions and comments from the editor and the anonymous reviewers we would especially like to acknowledge takuya iwanaga for suggestions provided to improve this article the work described in this article involves many geo analysis models and was supported by the following research programs the national natural science foundation of china grant number 41701441 the nsf for excellent young scholars of china grant number 41622108 the national basic research program of china grant number 2015cb954103 the priority academic program development of jiangsu higher education institutions grant number 164320h116 and the outstanding innovation team in colleges and universities in jiangsu province 
26174,how interactive simulations can improve the support of environmental management lessons from the dutch peatlands h a van hardeveld a b p p j driessen a p p schot a m j wassen a a copernicus institute of sustainable development utrecht university p o box 80 115 3508 tc utrecht the netherlands copernicus institute of sustainable development utrecht university p o box 80 115 utrecht 3508 tc the netherlands copernicus institute of sustainable development utrecht university p o box 80 115 3508 tc utrecht the netherlands b hoogheemraadschap de stichtse rijnlanden p o box 550 3990 gj houten the netherlands hoogheemraadschap de stichtse rijnlanden p o box 550 houten 3990 gj the netherlands hoogheemraadschap de stichtse rijnlanden p o box 550 3990 gj houten the netherlands corresponding author copernicus institute of sustainable development utrecht university p o box 80 115 3508 tc utrecht netherlands copernicus institute of sustainable development utrecht university p o box 80 115 utrecht 3508 tc netherlands how interactive simulation systems can improve the support of environmental management is not fully understood we therefore cross analyzed questionnaires with logfiles and videos of workshops in which an interactive simulation system for peatland management was applied to derive an in depth perspective of its added values the workshop participants explored the physical system dynamics implementing measures and the social system dynamics brokering deals with other stakeholders the system enabled capacity building at individual and group level through iterative exploration of possible measures as a result cooperation among the stakeholders was enhanced and their understanding of problems and action perspectives regarding the peatlands was increased interventions that stimulated deliberation during the workshops were shown to prevent individualistic strategies and instead fostered cooperative attitudes the embeddedness in preceding science policy interfaces enhanced the credibility and legitimacy of the system whereas salience was strengthened by abundant detailed information realistic visual quality and short calculation times keywords interactive simulation serious gaming peatlands soil subsidence sustainable water management participatory modeling 1 introduction the sustainable management of social ecological systems is notoriously complex because management strategies must address a set of interrelated environmental political and economic variables with impacts across multiple spatial and temporal scales that are often nonlinear and highly uncertain walker et al 2002 ostrom 2009 it is therefore widely acknowledged that management strategies must move beyond panaceas instead adopting a perspective that embraces complexity folke 2006 ostrom 2007 to effectively harness science for this challenge interfaces are needed that promote communication and translation between experts and decision makers and enable mediation to avoid tradeoffs between the salience credibility and legitimacy of the scientific information cash et al 2003 these science policy interfaces are essentially social processes with the aim of enriching decision making van den hove 2007 they encompass a variety of typologies such as individual mediators processes of participatory knowledge development and boundary organizations van enst et al 2014 to bridge the gap between science and policy many science policy interfaces use boundary objects i e collaborative outputs that are both adaptable to different viewpoints and robust enough to maintain identity across them star and griesemer 1989 examples range from gis technology harvey and chrisman 1998 and simulation models white et al 2010 to multifaceted concepts like ecosystem services abson et al 2014 cash et al 2003 suggest that collaborative efforts to produce boundary objects are likely to result in credible legitimate and salient information however van enst et al 2014 point out that science policy interfaces may encounter several interaction problems that diminish their effectiveness in their paper they illustrate how operational misfits between the demand and supply of knowledge will reduce the salience of information and how strategic production and or use of knowledge will also negatively impact the credibility and legitimacy of knowledge the strategical interaction problems mainly occur when the knowledge is uncertain and or consensus on norms and values is lacking the operational misfits occur more often for example uran and janssen 2003 describe how many decision support systems failed to provide salient information for their users and were therefore not used as effective boundary objects leskens et al 2014a describe how simulation models for flood disaster management encountered similar predicaments mainly because the models needed experts to run them and could not keep pace with the speed of interactions in the decision making processes in an extended literature review mayer 2009 describes how operational misfits and the accompanying critiques stimulated many developers of simulation models to create more transparent and interactive models that are more likely to become effective boundary objects he argues that serious games can be regarded as the most promising exponent of this new generation of computer mediated support systems because they are able to integrate the technical physical and the social political complexities of policy problems in addition serious gaming is known to be an effective technique for learning and retention hofstede et al 2010 connolly et al 2012 wouters et al 2013 cheng et al 2017 with proven abilities to engage stakeholders and allow them to experience the complexity of collaborative management tasks bekebrede 2010 vervoort et al 2014 not surprisingly serious games are increasingly being used to support the management of social ecological systems e g van der wal et al 2016 voinov et al 2016 craven et al 2017 for similar reasons many contemporary decision support systems allow for interactive simulations too they include spatial decision support tools such as touch tables arciniegas et al 2013 eijkelboom and janssen 2013 pelzer et al 2016 and flood simulation models leskens et al 2014b for terminological clarity in this paper we will refer to all interactive computer mediated support systems as interactive simulation systems iss despite the potential benefits of iss it is unclear to what extent they effectively support management decisions because much iss research is hampered by one or more limitations first many iss are tested with the help of students instead of real world stakeholders e g hummel et al 2011 poplin 2012 arciniegas et al 2013 schulze et al 2015 this raises the issue of external validity it remains uncertain to what extent these settings reflect real world practices moreover the iss test results of students have been shown to differ significantly from those of professional stakeholders bekebrede et al 2015 second most studies consider only one or a limited number of workshops although this might provide valuable results it remains uncertain to what extent the results can be generalized to other circumstances third most studies focus on opinions voiced by the participants in workshops in which the iss was tested without considering logfiles and or video recordings of the workshops the disadvantage of stated opinions is that answers can be biased e g by socially preferred answers logfiles and or video recordings lack these possible biases because they reveal not what people say but how they actually behave in the workshops in which the iss is tested in this paper we report research that aimed to overcome the research limitations mentioned above we tested an iss with real world stakeholders in multiple workshops using questionnaires as well as logfiles and video recordings of the workshops the guiding research question was how can iss improve the support of environmental management 2 method 2 1 outline research the case we used for our research was the collective management of dutch peatlands at the turn of the century it was suggested to raise the surface water levels which would decrease the soil subsidence rates although profitable dairy farming would no longer be possible and large scale transitions from dairy farming to nature restoration would be necessary this disadvantage would be outweighed by a decrease of management costs van brouwers haven and lokker 2010 however projects aimed at a top down implementation of this strategy met with resistance from agricultural stakeholders a lock in situation developed which raised awareness that more effective stakeholder collaboration was needed to produce legitimate results and develop viable management strategies to aid this resolve various processes of participatory knowledge development have been instigated van brouwershaven and lokker 2010 a boundary organization for innovative peatland management was created and alternatives to a top down mode of environmental governance were explored den uyl and driessen 2015 nevertheless at most locations the soil subsidence rates have remained unsustainably high although it has been shown that innovative applications of field drains can reduce soil subsidence and improve the conditions for all stakeholders this requires a a clear understanding of their site specific impacts and b consensus on a fair distribution of their costs among the stakeholders van hardeveld et al 2018 these implementation challenges are not easily overcome because site specific collaborative management strategies to reduce soil subsidence have not become commonplace for this context we developed re peat an iss for the collaborative management of peatlands which accurately assessed the site specific impacts of management strategies and supported negotiation processes on goals means and implementation pathways next we applied re peat in ten workshops in which the participants faced the assignment of improving the future conditions of a specific site in the dutch peatlands all participants could influence the simulation by stakeholder specific actions and transactions with other stakeholders we used post workshop questionnaires to enquire about the workshop participants perceptions of the added values of re peat to overcome the aforementioned implementation challenges for collaborative management strategies to reveal how the participants used re peat we also enquired about their attitude and their strategies and recorded the workshop proceedings on logfiles and video in addition we experimented with different workshop settings and analyzed how these settings influenced the outcomes of the workshops the combined results of our experiment were used to derive an in depth perspective on how iss can improve the support of environmental management 2 2 developing re peat aided by several key experts on the dutch peatlands we developed an iss for peatland management the core of the iss consisted of a spatially and temporally explicit modeling framework that simulates the interrelated dynamics of surface water levels phreatic groundwater tables and soil subsidence as well as the ensuing effects on embankments and hydraulic structures real estate co2 emissions and crop yield van hardeveld et al 2017 following the cost benefit analysis approach of van hardeveld et al 2018 we combined the modeling framework with empirical economic data to simulate the investment sums and maintenance costs required for the water system field drainage real estate gardens and roads and sewers as well as the net value added of the agricultural production and the agricultural supply chain we combined the expanded modeling framework with the tygron geodesign platform an interactive software platform for accurate 3d modeling of spatial development projects warmerdam et al 2006 bekebrede et al 2015 the combination with the tygron geodesign platform transformed most scenario settings of the extended modeling framework e g the drainage strategy or the land use into actions that allowed users to influence the simulation in addition the tygron geodesign platform allowed for monetary transactions during the simulation as well as the levying of taxes as we wanted the resulting iss to reflect the entire range of land uses in dutch peatlands i e dairy farming and other forms of agriculture villages and nature reserves we expanded the iss with several additional effects that we deemed relevant for these land uses we used empirical data from water authorities so as to include the water supply required by drainage strategies the amounts of dredged material the numerous ditches and waterways in the dutch landscape must be dredged regularly and the amount of nutrients that drain to the water system due to soil subsidence and farm management the water quality was included by comparing the simulated nutrient loads with threshold values for nutrient loads above which ditches become choked with duckweed the threshold values were obtained by 1638 runs of the pcditch model janse and van puijenbroek 1998 allowing for variations in a soil properties b water depth determined by the surface water level and c water discharge drawing on the approach of sijtsma et al 2011 we included an ecological quality score derived from land use and groundwater tables which we extended to reflect the effects of water quality and farm management too we also included scores for urban quality and cultural heritage which we derived from stakeholder actions for example demolishing old real estate diminished the cultural heritage score and increasing the maintenance of gardens increased the village quality score flooding was included by using raster based rainfall runoff computations based on the diffusive wave approximation horritt and bates 2001 which adequately compared with the analytical solutions of overland flow presented by di giammarco et al 1998 with nash sutcliffe efficiencies exceeding 0 99 the resulting iss called re peat an acronym derived from platform for evaluating and anticipating trends in peatlands can be used iteratively to explore the myriad of options in collaborative environmental management it is shown in fig 1 we included five stakeholder roles in re peat 1 the municipality which manages the infrastructure of roads and sewers 2 the water authority which manages the water system 3 the collective farmers who own most of the rural area 4 the collective residents who own most of the real estate in the villages and 5 an ngo which manages the nature reserves note that in reality farmers and residents predominantly operate individually however because their individual stakes were similar for the sake of clarity and effectiveness they were included collectively all stakeholder roles had a main individual goal and several accompanying goals e g the main goal for the water authority was the reduction of management costs with as accompanying goals the improvement of water quality and the reduction of water demand and flood damage in addition all stakeholder roles shared the common goal of reducing soil subsidence several stakeholder roles shared accompanying goals as well e g improving the quality of the villages was important for both the municipality and the collective residents all stakeholder roles had a personalized graphical user interface available which contained action menus thematic maps information panels and a view on the 3d simulation from their own perspective fig 2 the information panels showed their budgets and the extent to which their goals were reached for all goals the panels compared the results with and without actions in addition an overall information panel showed a graph of their progress score throughout the simulation the progress score was derived from the main individual goal 40 the accompanying individual goals 30 and the common goal of reducing soil subsidence 30 2 3 comparing workshops we organized ten workshops in which we applied re peat from march 10th till november 29th 2016 the workshops were attended by a total of 89 participants who were professionally involved in the management of dutch peatlands in addition each workshop was attended by a facilitator who oversaw the overall workshop process and by 2 4 assistants who could provide technical support if needed all workshops alternated rounds of interactive simulation with plenary moments of instruction and reflection the time spent on these activities varied table 1 in general the workshops started with 30 90 min of plenary instruction followed by two rounds of interactive simulation which both lasted 30 45 min the rounds of interactive simulation were followed by plenary debriefings which lasted 5 10 min after round one and 15 25 min after round two drawing from the guidelines for debriefing of peters and vissers 2004 and kriz 2010 the debriefing in between rounds focused on the perceptions of the participants a joint reconstruction of what happened and a discussion of further options for actions the final debriefing addressed the connection between the simulation and reality including speculation about hypothetical scenarios and exploration of pathways to put into practice the lessons that were learned on several occasions we deviated from the general approach for example the participants in workshops 3 and 4 opted to spend more time on plenary instruction at the expense of interactive simulation time in workshop 10 the available time was relatively limited so we economized on the time allocated to instruction by assigning a technical assistant to each stakeholder role to help the participants operate re peat the settings of re peat reflected peatland areas of 9 km2 and timeframes of 30 100 years in which the gradual impacts of soil subsidence become apparent for example due to differences in soil subsidence rates the differences in water levels between adjacent watercourses may increase at some moment in time this will require additional embankments to prevent the watercourses with higher water levels from slumping van hardeveld et al 2017 the exact moment in time that the embankments are needed depends on the characteristics of the peatland area the chosen timeframes were always sufficiently long to include the moment at which such impacts were manifested in the peatland areas that were considered to accurately assess the soil subsidence rates throughout the considered timeframes we took into account that the microbes that oxidize peat become more active when the temperature rises tate 1987 therefore we gradually adjusted the soil subsidence assessment to reflect a regional projection of 2 c global temperature rise van den hurk et al 2006 we included 3 5 stakeholder roles in workshop 1 9 each stakeholder role was allocated to pairs of workshop participants who shared a laptop computer workshop 10 was an exception with 5 6 participants per stakeholder role in this workshop the laptops were connected to large projection screens to assure that all participants could see the user interface fig 2 during the entire workshop due to the limited availability of hardware the ngo was only included in workshops 1 and 2 in workshops 3 and 4 the participants requested omitting the collective residents so as to focus more on the remaining three stakeholder roles to examine how re peat can improve the support of environmental management we experimented with the settings regarding the style of the governmental roles and the involvement of the workshop participants table 1 we used two styles of the governmental roles to examine the effect of interventions that stimulate deliberation in workshops 1 4 we allowed the municipality and the water authority to make top down decisions i e they did not require other stakeholder roles to consent to changing taxes and drainage strategies these workshops allowed for a top down implementation of drainage strategies similar to what was considered at the turn of the century in the dutch peatlands for workshops 5 10 we changed this set up forcing the governmental stakeholders to deliberate their decisions i e they could only implement taxes and drainage strategies after obtaining the consent of the affected stakeholder roles these workshops reflected the current ideas on peatland management which acknowledge that cooperation between stakeholders is needed to produce viable management strategies we also experimented with the involvement of the workshop participants to examine the effect of various application styles workshops 1 9 had a hands on approach with the participants operating re peat themselves these workshops reflected a common setting of multi player serious game sessions which had not been used before to support the management of the dutch peatlands workshop 10 had a guided approach with the technical assistants operating re peat on behalf of the participants these workshops reflected a common setting of touch table sessions which had been used on several occasions to support the management of the dutch peatlands before our experiment arciniegas et al 2013 brouns et al 2015 overall this resulted in three groups of workshops with different settings 1 workshops 1 4 had a top down government style and hands on workshop participants 2 workshops 5 9 had a deliberative governmental style and hands on workshop participants and 3 workshop 10 had a deliberative governmental style and guided workshop participants because the workshops varied regarding the number of stakeholder roles and the duration of the interactive simulation table 1 we performed a sensitivity analysis first we analyzed the sensitivity of the results to excluding stakeholder roles that were not included in all workshops i e the collective residents not included in workshops 3 and 4 and the ngo not included in workshops 3 10 second we analyzed the results sensitivity to excluding workshops with less than 1 h allocated to interactive simulation i e workshops 3 4 and 9 2 4 perceiving added values we used post workshop questionnaires to enquire about the workshop participants perception of the added value of re peat to overcome implementation challenges of site specific collaborative management strategies to reduce soil subsidence in particular we enquired about a enhancing cooperation among them and b increasing their understanding of problems and action perspectives regarding the peatlands we used five point likert scales to measure their perceptions ranging from 2 very negative to 2 very positive we used pairwise mann whitney tests to assess statistical differences between the three groups of workshops in addition we included an open question in the post workshop questionnaires enquiring about arguments to elucidate the perceptions of added values afterwards we classified the 166 responses about the perceived added values into six categories we derived categories 1 4 from pelzer et al 2014 who distinguished between added values of iss on 1 the individual level regarding learning about the nature of the planning object 2 the individual level regarding learning about the perspective of other stakeholders 3 the group level i e the improvement of collaboration communication consensus and efficiency and 4 the outcome level i e better informed decisions in a follow up study pelzer et al 2016 found that participants in iss workshops perceived the added values at individual level to be key we therefore selected both individual values as separate categories in addition we included categories for 5 the context of the application e g the characteristics of the participants the policy process and the political context geertman 2006 and 6 the usability of re peat e g transparency user friendliness calculation time and integrality pelzer et al 2016 we used fisher s exact tests to assess statistical differences between the three groups of workshops 2 5 exploring different uses to reveal how the workshop participants used re peat we used logfiles that recorded all the actions of the stakeholder roles during the simulation in addition we used multiple video cameras to capture the activities of the actual workshop participants too afterwards we synchronized the videos and time coded the activities of each workshop participant drawing on the system for coding group working relations developed by nyerges et al 2006 we used four codes to annotate the activities of the workshop participants 1 inactive e g checking a cell phone or pouring a glass of water 2 reflective i e a getting support from technical assistants and b observing the interaction between other stakeholder roles 3 interactive i e discussion with other stakeholder roles and 4 explorative i e a focusing on the computer screen and b discussion with participants within the same stakeholder role for each participant we logged the cumulative number of actions and interactions hour 1 and the cumulative time spent on all coded activities we also used the logfiles to examine to what extent the workshop participants reached their goals and to what extent their own actions and the actions of other participants contributed to their overall progress score we used pairwise mann whitney tests to assess statistical differences between the groups of workshops regarding the time codes we excluded the inactive episodes time code 1 which on average accounted for 3 8 of the time we used post workshop questionnaires to enquire about the participants perception of their attitude we used seven point likert scales to measure these perceptions ranging from 3 very uncooperative to 3 very cooperative in addition we included an open question enquiring about the strategies they employed during the workshop afterwards the 210 responses to the open question were grouped into five categories 1 influencing the social system e g brokering deals with other stakeholders 2 influencing the physical system e g implementing measures 3 improving personal welfare either by maximizing profits or by minimizing costs 4 improving the peatlands e g by minimizing the soil subsidence and 5 no clear strategy we used pairwise mann whitney tests to assess statistical differences between the three groups of workshops regarding the attitudes of the workshop participants to assess differences in their strategies we used fisher s exact tests 3 results 3 1 differences in the perceived added values in post workshop questionnaires the workshop participants clearly stated they perceived re peat to be of high added value for enhancing cooperation among them and increasing their understanding of the social ecological system table 2 the perceptions of the groups were consistent with only small differences between them the proportion of workshop participants who elucidated their perceived added values with arguments regarding the outcome level was low differences between the groups were not significant arguments regarding the added value at group level and individual level were more common the argument more awareness of other perspectives was used by the largest proportion of workshop participants to explain their perception of the added values the proportion using this argument differed significantly between the groups in workshops 1 4 and workshop 10 p 0 037 the group of workshop 10 also stood out regarding the proportion that used the argument improved understanding of the peatlands it was significantly lower than the proportions for the groups of workshops 1 4 p 0 017 and workshops 5 9 p 0 045 approximately half of the participants remarked that the added values strongly depend on the context of the application such as the workshop setting and the characteristics of the participants some of them elucidated their remark by suggesting that the absence of conflicts was an important precondition for the added values their general perception was that although conflicts have not disappeared there is a trend toward consensus and cooperation among the stakeholders in dutch peatlands only for such contexts did they perceive high added values interestingly the participants in workshop 10 seemed less troubled by such considerations the proportion making such remarks was significantly smaller than in workshops 1 4 p 0 006 and workshops 5 9 p 0 001 almost half of the workshop participants mentioned that the usability of re peat contributed to their perception of the added values specifically they mentioned the credible results the abundance of detailed information and the realistic visual quality of the user interface for example the impact of site specific soil subsidence rates on the length of watercourses that required embankments to prevent them from slumping or the impact of site specific groundwater tables on the net value added of dairy farms some of them acknowledged that in general they struggled to comprehend the full complexity of peatland management they found re peat very useful because it presented a clear overview of all the aspects 3 2 behavioral differences during workshops logfiles and video recordings of the workshops revealed that on average the workshop participants simulated 12 4 actions hour 1 interacted with other participants 30 4 times hour 1 and spent most of their time on exploration table 3 per individual the number of actions and interactions hour 1 differed markedly with ranges of 2 38 actions hour 1 and 6 74 interactions with other participants hour 1 fig 3 the average ratio of actions hour 1 to interactions hour 1 was 0 6 with only 16 of the participants exhibiting ratios greater than 1 0 i e engaging in more actions hour 1 how the individual participants spent their time in the workshop also differed markedly with ranges of 4 65 for time spent on reflection 16 79 for time spent on exploration and 5 52 on time spent interacting with other participants fig 4 to some extent the variety in the behavior of the workshop participants related to the workshop settings the participants in workshop 10 spent much time on dialog within their group with the technical assistants assigned to their stakeholder role therefore they embarked on relatively few actions and interactions hour 1 consequently their results differed statistically from both other workshop groups in terms of their average proportion of time spent on reflection workshops 1 4 group u 17 0 p 0 000 workshops 5 9 group u 24 0 p 0 000 their average proportion of time spent on exploration workshops 1 4 group u 75 0 p 0 000 workshops 5 9 group u 87 0 p 0 000 their average proportion of time spent on interaction workshops 1 4 group u 192 0 p 0 005 workshops 5 9 group u 144 0 p 0 000 and their average number of actions hour 1 workshops 1 4 group u 84 0 p 0 003 workshops 5 9 group u 31 0 p 0 000 regarding the number of interactions hour 1 workshops 5 9 were statistically different workshops 1 4 group u 233 0 p 0 000 workshop 10 group u 80 0 p 0 000 the governmental decisions in workshops 1 4 did not require consent from other stakeholder roles consequently compared with the participants in workshops 5 9 the participants in workshops 1 4 had fewer interactions hour 1 and more actions hour 1 u 258 0 p 0 030 although the governmental decisions in workshop 10 also required consent from other stakeholder roles this did not result in markedly more interactions hour 1 than in workshops 1 4 because the participants in workshop 10 spent much time on discussions among the participants who shared their stakeholder role on average the overall progress score during the workshops was 22 table 4 this may seem rather modest but it must be noted that due to opposite effects of actions high scores were very difficult to realize for example a raise in surface water levels would decrease the soil subsidence rate which would increase the overall progress score however the frequency of flooding would increase as well which would lower the overall progress score due to such opposite effects only two pairs of workshop participants achieved an overall progress score of more than 50 remarkably in workshops 5 10 the average progress was mainly caused by the actions of other participants whereas in workshops 1 4 the progress was caused by the actions of the participants themselves in this group of workshops the progress due to the actions of other participants was significantly lower than in workshops 5 9 u 83 0 p 0 014 the difference coincides with the significantly lower number of interactions hour 1 table 3 post workshop questionnaires revealed that the participants used four types of strategies table 5 on average to almost the same extent significant differences were only found regarding the proportion of strategies aiming to influence the physical system this was lower in workshop 10 than in workshops 1 4 p 0 008 the difference coincides with the significantly lower number of actions hour 1 table 3 and the significantly lower proportion of participants using improved understanding of the peatlands as an argument to explain their perception of added values table 2 it seems that the participants in workshop 10 focused more on the social system dynamics than on the physical system dynamics consequently they might not have increased their understanding of the social ecological system as comprehensively as the participants in the other workshops however their appreciation of the added value of re peat did not reflect this shortcoming either because the effect was limited or because they were unaware of it on comparing participants attitude during the workshops we found that for workshops 1 4 in which governmental stakeholders were able to enforce top down decisions scores were significantly lower than in workshops 5 9 u 268 0 p 0 016 and workshop 10 u 109 5 p 0 012 in which governmental stakeholders needed to deliberate their decisions with the other stakeholders note that on average the lower scores did not reflect uncooperative impressions but impressions that were neutral to slightly cooperative uncooperative attitudes were only expressed by the participants in workshops 2 4 in these workshops many participants behaved individualistically and exhibited ratios of actions hour 1 to interactions hour 1 of up to 4 2 i e strongly preferring individual actions over interaction and deliberated coordinated actions because in workshops 5 10 deliberation was mandatory for the governmental stakeholder roles cooperative attitudes prevailed with only one of the 67 participants expressing a slightly uncooperative attitude it is noteworthy that participants in workshop 1 spontaneously engaged in several coordinated actions that required much deliberation resulting in cooperative attitudes similar to workshops 5 10 an average attitude score of 1 5 3 3 sensitivities analyzed the sensitivity analysis see appendix revealed only minor changes in the results due to the exclusion of a the stakeholder roles that were not included in all workshops i e collective residents and ngo and b the workshops with less than 1 h allocated to interactive simulations i e workshops 3 4 and 9 any changes in statistically significant differences between groups of workshops primarily reflected the smaller group sizes resulting from the exclusions we therefore deem the results not biased by variations in the workshop settings 4 discussion 4 1 system design the system design of an iss determines to what extent it can be used as a boundary object for the management of social ecological systems the general design principles are that the iss should always promote communication and translation between experts and promote mediation to avoid tradeoffs among the salience credibility and legitimacy of the scientific information cash et al 2003 in our project we aimed to secure these functions at the developmental stage of re peat by recruiting several key dutch peatland experts to translate existing scientific knowledge into content that was salient from a stakeholder perspective understandable by non scientific participants yet scientifically credible much of the knowledge incorporated in re peat resulted from preceding science policy interfaces such as processes of participatory knowledge development van brouwershaven and lokker 2010 and a boundary organization for innovative peatland management we believe the embeddedness of re peat in preceding science policy interfaces was an important condition for credibility and legitimacy arguably without this embeddedness the incorporated knowledge would have been more uncertain and disputable which would have diminished the effectiveness of the iss the workshop participants mentioned that the good usability of re peat also resulted from the abundance of detailed information and the realistic visual quality of the user interface arguably these features enhanced the salience of the information for them throughout the simulation they were continuously presented with sufficient information to make decisions in an easily understandable format the extent to which these iss features can be enhanced is related to the resulting calculation times an important condition for iss is its ability to keep pace with stakeholder interactions during actual decision making processes eijkelboom and janssen 2013 leskens et al 2014a in our case we were able to enhance the information load and the visual quality of re peat quite extensively because the graphics processing unit was an integral part of its computing system therefore the maximum calculation times were limited to a few seconds per action the iss features that enhance its usability are strongly related to the main added values that were perceived by the workshop participants like pelzer et al 2016 we found that iss workshop participants perceived the added values at individual and group level to be key and the added value of a better informed outcome to be less important in the workshops we witnessed how the participants collaboratively designed adaptive drainage strategies that could slow down soil subsidence although the generic effects of these adaptive drainage strategies had been known for approximately a decade querner et al 2012 they had never been implemented on a large scale because re peat translated the generic scientific knowledge into site specific effects from multiple stakeholder perspectives it created an operational fit between knowledge supply and demand in addition because the re peat supported informed negotiations it also raised awareness of mutually beneficial strategies it is noteworthy that later several workshop participants initiated a collaborative process to implement the adaptive drainage strategy designed during the workshops in the same peatland area they explored with re peat moreover they intend to continue using re peat as a boundary object to support collaborative management decisions in the years to come we believe the results show that the general purpose of iss should reflect capacity building at the individual and group level striving for a site specific awareness of the effects of measures and strengthening the resolve of the stakeholders to collectively implement these measures iss applications should not primarily focus on better informed outcomes although credible results are obviously important the iterative and interactive exploration of the myriad of management options should be the key consideration in support systems for collaborative environmental management 4 2 workshop setting our experiment with different workshop settings revealed that all workshop participants explored the physical system dynamics implementing measures and the social system dynamics brokering deals with other stakeholders however the participants in the workshops with a top down government style implemented markedly more actions hour 1 than the participants in the other workshops in addition their strategies were primarily aimed at influencing the physical system and their attitude during the workshops was significantly less cooperative than that of the participants in the other workshops consequently they appeared not to have taken full advantage of the potential of re peat to enhance cooperation because of their focus on physical measures their awareness of the perspectives of other stakeholders was markedly lower than the corresponding awareness in the other workshops furthermore their overall progress was relatively limited with other participants contributing significantly less to their overall progress than in the workshops 5 9 our findings suggest that interventions that stimulate deliberation can prevent individualistic strategies and instead foster cooperative attitudes in our research we achieved this by requiring mandatory deliberation of governmental decisions which fostered cooperative attitudes in workshops 5 10 other examples of interventions that can enhance cooperation are scripted instructions rummel and spada 2005 the incorporation of sequential phases with mandatory group tasks hämäläinen 2011 and scripted collaboration with workshop participants papadopoulos et al 2013 or their virtual counterparts hummel et al 2011 our experiment with various application styles also produced some marked results the participants in the guided workshop spent significantly more time on reflection than the hands on participants in the other workshops consequently they implemented markedly less actions hour 1 and employed strategies that were less often aimed at influencing the physical system in addition in the post workshop questionnaires they claimed significantly less often that their understanding of the peatlands had improved this might suggest that when the iss application is primarily aimed at increasing the participants understanding of the social ecological system a hands on approach such as in multi player serious gaming sessions is preferable to a guided approach such as in touch table sessions however we only regarded one workshop with a guided setting which does not suffice to draw valid conclusions further research is needed to examine to what extent the results might have been caused by other factors such as the limited time that was allocated to the plenary introduction and the debriefing in between rounds furthermore the impact of the facilitator and the technical assistants was underexposed in our research the facilitator and technical assistants of workshop 10 had contributed to 5 9 previous workshops in which they acquired the skills to effectively support an iss workshop arguably their performance contributed considerably to the added values that were perceived by the participants further research might increase our understanding of how iss facilitators can help improve the support of environmental management 4 3 context van enst et al 2014 identify two main contextual factors that define the structuredness of policy problems consensus on relevant norms and values and certainty about relevant knowledge on the one hand unstructured or wicked problems are deficient on both accounts therefore they lack a definite solution and exhibit a range of strategical and operational science policy interaction problems on the other hand completely structured problems are characterized by consensus as well as certainty which makes science policy interactions relatively straightforward the context of dutch peatlands can be seen as a moderately structured policy problem regarding knowledge several science policy interfaces have increased the certainty of generic knowledge but some uncertainty still remains especially regarding site specific effects from specific stakeholder perspectives in addition the workshop participants perceived a trend toward consensus among the stakeholders in dutch peatlands for the workshop participants the absence of conflicts was an important condition for the high added values they perceived we therefore believe our results primarily demonstrate how iss can be of added value for moderately structured environmental management problems arguably iss might be less effective in unstructured contexts with less consensus on norms and values and less certainty about relevant knowledge we suggest that in these contexts efforts to support environmental management should primarily be aimed at science policy interfaces which are suited for such contexts such as processes of participatory knowledge development boundary organizations and individual science policy mediators van enst et al 2014 5 conclusions our research demonstrated that iss can improve the support of environmental management implementation challenges for collaborative management strategies can be overcome by translating generic scientific knowledge into site specific effects from multiple stakeholder perspectives and by raising awareness of mutually beneficial strategies in iss workshops all participants explored the physical system dynamics implementing measures and the social system dynamics brokering deals with other stakeholders as a result the iss workshops enhanced cooperation among them and increased their understanding of problems and action perspectives regarding the social ecological system interventions that stimulate deliberation during the iss workshops were shown to prevent individualistic strategies and instead foster cooperative attitudes the embeddedness of an iss in preceding science policy interfaces is an important condition for the iss s credibility and legitimacy important conditions for the salience of the iss are an abundance of detailed information realistic visual quality of the user interface and calculation times that are short enough to keep pace with stakeholder interactions during the decision making processes in addition the general purpose of iss should reflect capacity building at the individual and group level striving for a site specific awareness of the effects of measures and strengthening the resolve of the stakeholders to collectively implement these measures we suggest further research on interactive simulation systems should capitalize on the ability of our research approach to yield in depth understanding of how iss can improve environmental management cross analyzing questionnaires with logfiles and videos of workshop proceedings can pinpoint how iss can effectively harness science for complex environmental management tasks this will help us understand how sustainable management of social ecological systems can be put into practice acknowledgments this study was funded by water authority hoogheemraadschap de stichtse rijnlanden the provinces of utrecht and zuid holland and the stowa foundation for applied water research the authors would like to thank jan van den akker gert jan van den born ernst bos gilles erkens john lambert and everybody at the tygron company for their contributions to the development of re peat we especially thank marije van bergen astrid de boer riebel joris van dijk harm de jong rudolf koster simon troost epke van der werf and florian witsenburg for their contributions to the workshops and two anonymous reviewers for their thoughtful comments which helped to improve the paper joy burrough was the language editor of a near final draft of the paper appendix sensitivity analysis regarding the included stakeholder roles table a1 changes in the results due to the exclusion of the stakeholder roles collective residents and ngo in terms of the average added values perceived per group of workshop participants for enhancing cooperation and increasing understanding and the proportion per workshop group that used an argument to explain their perceptions the scale of added values ranges from 2 very negative to 2 very positive changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 2 for the results that include all stakeholder roles table a1 workshop group 1 4 5 9 10 all no of participants 24 26 16 66 added value for enhancing cooperation 0 0 0 0 0 2 0 0 added value for increasing understanding 0 0 0 0 0 1 0 0 more awareness of other people s perspectives 6 12 3 2 improved understanding of the peatlands 2 7 5 3 support of the group process 3 10 15 9 better informed decisions 2 1 0 0 context of the application 7 10 8 4 usability of re peat 11 1 16 1 table a2 changes in the results due to the exclusion of the stakeholder roles collective residents and ngo in terms of the average number of actions and interactions hour 1 per group of workshop participants and their average proportion of active time spent on exploration reflection and interaction changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 3 for the results that include all stakeholder roles table a2 workshop group 1 4 5 9 10 all no of participants 24 26 16 66 actions hour 1 0 3 0 8 0 3 0 2 interactions hour 1 0 4 3 7 4 0 2 6 time spent on reflection 1 2 4 2 time spent on exploration 0 1 1 0 time spent on interaction 1 3 3 2 table a3 changes in the results due to the exclusion of the stakeholder roles collective residents and ngo in terms of the average extent to which groups of workshop participants reached their goals their average progress score and the average extent to which their progress was caused by their own actions or the actions of other participants note that the significance of the differences between groups of workshops did not change see table 4 for the results that include all stakeholder roles table a3 workshop group 1 4 5 9 10 all no of participants 24 26 16 66 common goal less soil subsidence 0 0 0 0 individual goals 0 7 4 4 overall progress score 0 4 3 2 progress due to own actions 1 1 3 0 progress due to actions of others 1 3 2 2 table a4 changes in the results after excluding the stakeholder roles collective residents and ngo in terms of the proportion per group of workshop participants that employed a strategy and their average attitude the scale of attitude ranges from 3 very uncooperative to 3 very cooperative changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 5 for the results that include all stakeholder roles table a4 workshop group 1 4 5 9 10 all no of participants 24 26 16 66 influence the social system 8 6 9 4 influence the physical system 4 9 12 0 improve the personal welfare 7 3 15 5 improve the peatlands 6 3 0 3 no strategy 2 2 12 1 attitude 0 1 0 0 0 3 0 0 we found only limited changes table a1 4 however on several occasions the limited changes in combination with the smaller group sizes affected the number of groups yielding significantly different results regarding the added value for cooperation the 4 increase in the average for group 10 resulted in significant differences workshops 1 4 u 38 0 p 0 018 workshops 5 9 u 44 0 p 0 019 regarding the argument more awareness of other people s perspectives the difference between workshops 1 4 and workshop 10 was no longer significant p 0 114 because a slightly larger proportion of participants in workshops 1 4 used this argument a similar effect was found regarding the argument improved understanding of the peatlands a slightly larger proportion of participants in workshop 10 who used this argument rendered the difference with the other workshops statistically insignificant workshops 1 4 p 0 114 workshops 5 9 p 0 054 an opposite effect was found regarding the argument support of the group process because none of the remaining participants in workshop 10 used this argument the difference between workshops 1 4 and workshop 10 was found to be statistically significant p 0 030 regarding the proportion of time spent on interaction the averages of workshops 1 4 and workshop 10 became slightly more similar as a consequence for the smaller group sizes the difference between workshops 1 4 and workshop 10 was not statistically significant u 57 0 p 0 103 regarding the proportion of workshop participants employing a certain strategy we found that because the proportion of participants who employed strategies aimed at influencing the system was 9 higher in workshops 5 9 and 12 lower in workshop 10 the difference between these groups was statistically significant p 0 004 although the changes regarding the attitudes and the impressions of interaction with other participants were also limited they affected the number of groups with significantly different results on the one hand the average attitude of the participants in workshops 1 4 became slightly less cooperative which was enough to prevent a significant difference vis à vis workshops 5 9 u 108 0 p 0 128 on the other hand the average attitude of the participants in workshop 10 became slightly more cooperative resulting in a significant difference vis à vis workshops 5 9 u 36 0 p 0 038 the average impression of the interaction was also more cooperative which resulted in a significant difference vis à vis workshops 1 4 u 24 0 p 0 010 sensitivity analysis regarding the duration of the interactive simulation table a5 changes in the results after excluding short workshops in terms of the average added values perceived per group of workshop participants for enhancing cooperation and increasing understanding and the proportions of the groups that used an argument to explain their perceptions the scale of added values ranges from 2 very negative to 2 very positive changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 2 for the results that include all workshops table a5 workshop group 1 4 5 9 10 all no of participants 20 25 22 67 added value for enhancing cooperation 0 0 0 1 0 0 0 0 added value for increasing understanding 0 0 0 0 0 0 0 0 more awareness of other people s perspectives 3 2 0 0 improved understanding of the peatlands 7 7 0 2 support of the group process 8 0 0 2 better informed decisions 2 1 0 1 context of the application 5 9 0 8 usability of re peat 5 11 0 3 we found only limited changes tables a5 8 however on four occasions the limited changes in combination with the smaller group sizes affected the number of groups with significantly different results first the difference between workshops 5 9 and workshop 10 regarding the proportion of participants that used the arguments improved understanding peatlands was no longer significant p 0 140 second the proportion of workshop participants employing strategies aimed at influencing the physical system differed significantly between workshops 5 9 and workshop 10 p 0 028 third because the reduction of soil subsidence was less in workshops 1 4 and more in workshops 5 9 the difference between these groups was statistically significant u 40 0 p 0 032 fourth the average attitude in workshops 1 4 was more cooperative as a result the difference vis à vis workshops 5 9 was no longer significant u 149 0 p 0 342 this effect can be explained by the exclusion of the negative attitudes prevailing in workshops 3 and 4 table a6 changes in the results after excluding short workshops in terms of the average number of actions and interactions hour 1 per group of workshop participants and their average proportion of active time spent on exploration reflection and interaction note that the significance of the differences between groups of workshops did not change see table 3 for the results that include all workshops table a6 workshop group 1 4 5 9 10 all no of participants 20 25 22 67 actions hour 1 0 7 3 0 0 0 0 4 interactions hour 1 1 5 1 6 0 0 0 3 time spent on reflection 1 0 0 2 time spent on exploration 1 1 0 1 time spent on interaction 1 1 0 1 table a7 changes in the results after excluding short workshops in terms of the average extent to which groups of workshop participants reached their goals their average progress score and the average extent to which their progress was caused by their own actions or the actions of other participants note that the significance of the differences between groups of workshops did not change see table 4 for the results that include all workshops table a7 workshop group 1 4 5 9 10 all no of participants 20 25 22 67 common goal less soil subsidence 3 10 0 5 individual goals 10 2 0 4 overall progress score 6 5 0 4 progress due to own actions 4 1 0 1 progress due to actions of others 2 3 0 4 table a8 changes in the results after excluding short workshops in terms of proportions of groups of workshop participants that employed a strategy and their average attitude the scale of attitude ranges from 3 very uncooperative to 3 very cooperative changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 5 for the results that include all workshops table a8 workshop group 1 4 5 9 10 all no of workshop participants 20 25 22 67 influence the social system 1 3 0 1 influence the physical system 3 13 0 4 improve the personal welfare 10 6 0 5 improve the peatlands 9 10 0 7 no strategy 1 6 0 2 attitude 0 3 0 1 0 0 0 2 
26174,how interactive simulations can improve the support of environmental management lessons from the dutch peatlands h a van hardeveld a b p p j driessen a p p schot a m j wassen a a copernicus institute of sustainable development utrecht university p o box 80 115 3508 tc utrecht the netherlands copernicus institute of sustainable development utrecht university p o box 80 115 utrecht 3508 tc the netherlands copernicus institute of sustainable development utrecht university p o box 80 115 3508 tc utrecht the netherlands b hoogheemraadschap de stichtse rijnlanden p o box 550 3990 gj houten the netherlands hoogheemraadschap de stichtse rijnlanden p o box 550 houten 3990 gj the netherlands hoogheemraadschap de stichtse rijnlanden p o box 550 3990 gj houten the netherlands corresponding author copernicus institute of sustainable development utrecht university p o box 80 115 3508 tc utrecht netherlands copernicus institute of sustainable development utrecht university p o box 80 115 utrecht 3508 tc netherlands how interactive simulation systems can improve the support of environmental management is not fully understood we therefore cross analyzed questionnaires with logfiles and videos of workshops in which an interactive simulation system for peatland management was applied to derive an in depth perspective of its added values the workshop participants explored the physical system dynamics implementing measures and the social system dynamics brokering deals with other stakeholders the system enabled capacity building at individual and group level through iterative exploration of possible measures as a result cooperation among the stakeholders was enhanced and their understanding of problems and action perspectives regarding the peatlands was increased interventions that stimulated deliberation during the workshops were shown to prevent individualistic strategies and instead fostered cooperative attitudes the embeddedness in preceding science policy interfaces enhanced the credibility and legitimacy of the system whereas salience was strengthened by abundant detailed information realistic visual quality and short calculation times keywords interactive simulation serious gaming peatlands soil subsidence sustainable water management participatory modeling 1 introduction the sustainable management of social ecological systems is notoriously complex because management strategies must address a set of interrelated environmental political and economic variables with impacts across multiple spatial and temporal scales that are often nonlinear and highly uncertain walker et al 2002 ostrom 2009 it is therefore widely acknowledged that management strategies must move beyond panaceas instead adopting a perspective that embraces complexity folke 2006 ostrom 2007 to effectively harness science for this challenge interfaces are needed that promote communication and translation between experts and decision makers and enable mediation to avoid tradeoffs between the salience credibility and legitimacy of the scientific information cash et al 2003 these science policy interfaces are essentially social processes with the aim of enriching decision making van den hove 2007 they encompass a variety of typologies such as individual mediators processes of participatory knowledge development and boundary organizations van enst et al 2014 to bridge the gap between science and policy many science policy interfaces use boundary objects i e collaborative outputs that are both adaptable to different viewpoints and robust enough to maintain identity across them star and griesemer 1989 examples range from gis technology harvey and chrisman 1998 and simulation models white et al 2010 to multifaceted concepts like ecosystem services abson et al 2014 cash et al 2003 suggest that collaborative efforts to produce boundary objects are likely to result in credible legitimate and salient information however van enst et al 2014 point out that science policy interfaces may encounter several interaction problems that diminish their effectiveness in their paper they illustrate how operational misfits between the demand and supply of knowledge will reduce the salience of information and how strategic production and or use of knowledge will also negatively impact the credibility and legitimacy of knowledge the strategical interaction problems mainly occur when the knowledge is uncertain and or consensus on norms and values is lacking the operational misfits occur more often for example uran and janssen 2003 describe how many decision support systems failed to provide salient information for their users and were therefore not used as effective boundary objects leskens et al 2014a describe how simulation models for flood disaster management encountered similar predicaments mainly because the models needed experts to run them and could not keep pace with the speed of interactions in the decision making processes in an extended literature review mayer 2009 describes how operational misfits and the accompanying critiques stimulated many developers of simulation models to create more transparent and interactive models that are more likely to become effective boundary objects he argues that serious games can be regarded as the most promising exponent of this new generation of computer mediated support systems because they are able to integrate the technical physical and the social political complexities of policy problems in addition serious gaming is known to be an effective technique for learning and retention hofstede et al 2010 connolly et al 2012 wouters et al 2013 cheng et al 2017 with proven abilities to engage stakeholders and allow them to experience the complexity of collaborative management tasks bekebrede 2010 vervoort et al 2014 not surprisingly serious games are increasingly being used to support the management of social ecological systems e g van der wal et al 2016 voinov et al 2016 craven et al 2017 for similar reasons many contemporary decision support systems allow for interactive simulations too they include spatial decision support tools such as touch tables arciniegas et al 2013 eijkelboom and janssen 2013 pelzer et al 2016 and flood simulation models leskens et al 2014b for terminological clarity in this paper we will refer to all interactive computer mediated support systems as interactive simulation systems iss despite the potential benefits of iss it is unclear to what extent they effectively support management decisions because much iss research is hampered by one or more limitations first many iss are tested with the help of students instead of real world stakeholders e g hummel et al 2011 poplin 2012 arciniegas et al 2013 schulze et al 2015 this raises the issue of external validity it remains uncertain to what extent these settings reflect real world practices moreover the iss test results of students have been shown to differ significantly from those of professional stakeholders bekebrede et al 2015 second most studies consider only one or a limited number of workshops although this might provide valuable results it remains uncertain to what extent the results can be generalized to other circumstances third most studies focus on opinions voiced by the participants in workshops in which the iss was tested without considering logfiles and or video recordings of the workshops the disadvantage of stated opinions is that answers can be biased e g by socially preferred answers logfiles and or video recordings lack these possible biases because they reveal not what people say but how they actually behave in the workshops in which the iss is tested in this paper we report research that aimed to overcome the research limitations mentioned above we tested an iss with real world stakeholders in multiple workshops using questionnaires as well as logfiles and video recordings of the workshops the guiding research question was how can iss improve the support of environmental management 2 method 2 1 outline research the case we used for our research was the collective management of dutch peatlands at the turn of the century it was suggested to raise the surface water levels which would decrease the soil subsidence rates although profitable dairy farming would no longer be possible and large scale transitions from dairy farming to nature restoration would be necessary this disadvantage would be outweighed by a decrease of management costs van brouwers haven and lokker 2010 however projects aimed at a top down implementation of this strategy met with resistance from agricultural stakeholders a lock in situation developed which raised awareness that more effective stakeholder collaboration was needed to produce legitimate results and develop viable management strategies to aid this resolve various processes of participatory knowledge development have been instigated van brouwershaven and lokker 2010 a boundary organization for innovative peatland management was created and alternatives to a top down mode of environmental governance were explored den uyl and driessen 2015 nevertheless at most locations the soil subsidence rates have remained unsustainably high although it has been shown that innovative applications of field drains can reduce soil subsidence and improve the conditions for all stakeholders this requires a a clear understanding of their site specific impacts and b consensus on a fair distribution of their costs among the stakeholders van hardeveld et al 2018 these implementation challenges are not easily overcome because site specific collaborative management strategies to reduce soil subsidence have not become commonplace for this context we developed re peat an iss for the collaborative management of peatlands which accurately assessed the site specific impacts of management strategies and supported negotiation processes on goals means and implementation pathways next we applied re peat in ten workshops in which the participants faced the assignment of improving the future conditions of a specific site in the dutch peatlands all participants could influence the simulation by stakeholder specific actions and transactions with other stakeholders we used post workshop questionnaires to enquire about the workshop participants perceptions of the added values of re peat to overcome the aforementioned implementation challenges for collaborative management strategies to reveal how the participants used re peat we also enquired about their attitude and their strategies and recorded the workshop proceedings on logfiles and video in addition we experimented with different workshop settings and analyzed how these settings influenced the outcomes of the workshops the combined results of our experiment were used to derive an in depth perspective on how iss can improve the support of environmental management 2 2 developing re peat aided by several key experts on the dutch peatlands we developed an iss for peatland management the core of the iss consisted of a spatially and temporally explicit modeling framework that simulates the interrelated dynamics of surface water levels phreatic groundwater tables and soil subsidence as well as the ensuing effects on embankments and hydraulic structures real estate co2 emissions and crop yield van hardeveld et al 2017 following the cost benefit analysis approach of van hardeveld et al 2018 we combined the modeling framework with empirical economic data to simulate the investment sums and maintenance costs required for the water system field drainage real estate gardens and roads and sewers as well as the net value added of the agricultural production and the agricultural supply chain we combined the expanded modeling framework with the tygron geodesign platform an interactive software platform for accurate 3d modeling of spatial development projects warmerdam et al 2006 bekebrede et al 2015 the combination with the tygron geodesign platform transformed most scenario settings of the extended modeling framework e g the drainage strategy or the land use into actions that allowed users to influence the simulation in addition the tygron geodesign platform allowed for monetary transactions during the simulation as well as the levying of taxes as we wanted the resulting iss to reflect the entire range of land uses in dutch peatlands i e dairy farming and other forms of agriculture villages and nature reserves we expanded the iss with several additional effects that we deemed relevant for these land uses we used empirical data from water authorities so as to include the water supply required by drainage strategies the amounts of dredged material the numerous ditches and waterways in the dutch landscape must be dredged regularly and the amount of nutrients that drain to the water system due to soil subsidence and farm management the water quality was included by comparing the simulated nutrient loads with threshold values for nutrient loads above which ditches become choked with duckweed the threshold values were obtained by 1638 runs of the pcditch model janse and van puijenbroek 1998 allowing for variations in a soil properties b water depth determined by the surface water level and c water discharge drawing on the approach of sijtsma et al 2011 we included an ecological quality score derived from land use and groundwater tables which we extended to reflect the effects of water quality and farm management too we also included scores for urban quality and cultural heritage which we derived from stakeholder actions for example demolishing old real estate diminished the cultural heritage score and increasing the maintenance of gardens increased the village quality score flooding was included by using raster based rainfall runoff computations based on the diffusive wave approximation horritt and bates 2001 which adequately compared with the analytical solutions of overland flow presented by di giammarco et al 1998 with nash sutcliffe efficiencies exceeding 0 99 the resulting iss called re peat an acronym derived from platform for evaluating and anticipating trends in peatlands can be used iteratively to explore the myriad of options in collaborative environmental management it is shown in fig 1 we included five stakeholder roles in re peat 1 the municipality which manages the infrastructure of roads and sewers 2 the water authority which manages the water system 3 the collective farmers who own most of the rural area 4 the collective residents who own most of the real estate in the villages and 5 an ngo which manages the nature reserves note that in reality farmers and residents predominantly operate individually however because their individual stakes were similar for the sake of clarity and effectiveness they were included collectively all stakeholder roles had a main individual goal and several accompanying goals e g the main goal for the water authority was the reduction of management costs with as accompanying goals the improvement of water quality and the reduction of water demand and flood damage in addition all stakeholder roles shared the common goal of reducing soil subsidence several stakeholder roles shared accompanying goals as well e g improving the quality of the villages was important for both the municipality and the collective residents all stakeholder roles had a personalized graphical user interface available which contained action menus thematic maps information panels and a view on the 3d simulation from their own perspective fig 2 the information panels showed their budgets and the extent to which their goals were reached for all goals the panels compared the results with and without actions in addition an overall information panel showed a graph of their progress score throughout the simulation the progress score was derived from the main individual goal 40 the accompanying individual goals 30 and the common goal of reducing soil subsidence 30 2 3 comparing workshops we organized ten workshops in which we applied re peat from march 10th till november 29th 2016 the workshops were attended by a total of 89 participants who were professionally involved in the management of dutch peatlands in addition each workshop was attended by a facilitator who oversaw the overall workshop process and by 2 4 assistants who could provide technical support if needed all workshops alternated rounds of interactive simulation with plenary moments of instruction and reflection the time spent on these activities varied table 1 in general the workshops started with 30 90 min of plenary instruction followed by two rounds of interactive simulation which both lasted 30 45 min the rounds of interactive simulation were followed by plenary debriefings which lasted 5 10 min after round one and 15 25 min after round two drawing from the guidelines for debriefing of peters and vissers 2004 and kriz 2010 the debriefing in between rounds focused on the perceptions of the participants a joint reconstruction of what happened and a discussion of further options for actions the final debriefing addressed the connection between the simulation and reality including speculation about hypothetical scenarios and exploration of pathways to put into practice the lessons that were learned on several occasions we deviated from the general approach for example the participants in workshops 3 and 4 opted to spend more time on plenary instruction at the expense of interactive simulation time in workshop 10 the available time was relatively limited so we economized on the time allocated to instruction by assigning a technical assistant to each stakeholder role to help the participants operate re peat the settings of re peat reflected peatland areas of 9 km2 and timeframes of 30 100 years in which the gradual impacts of soil subsidence become apparent for example due to differences in soil subsidence rates the differences in water levels between adjacent watercourses may increase at some moment in time this will require additional embankments to prevent the watercourses with higher water levels from slumping van hardeveld et al 2017 the exact moment in time that the embankments are needed depends on the characteristics of the peatland area the chosen timeframes were always sufficiently long to include the moment at which such impacts were manifested in the peatland areas that were considered to accurately assess the soil subsidence rates throughout the considered timeframes we took into account that the microbes that oxidize peat become more active when the temperature rises tate 1987 therefore we gradually adjusted the soil subsidence assessment to reflect a regional projection of 2 c global temperature rise van den hurk et al 2006 we included 3 5 stakeholder roles in workshop 1 9 each stakeholder role was allocated to pairs of workshop participants who shared a laptop computer workshop 10 was an exception with 5 6 participants per stakeholder role in this workshop the laptops were connected to large projection screens to assure that all participants could see the user interface fig 2 during the entire workshop due to the limited availability of hardware the ngo was only included in workshops 1 and 2 in workshops 3 and 4 the participants requested omitting the collective residents so as to focus more on the remaining three stakeholder roles to examine how re peat can improve the support of environmental management we experimented with the settings regarding the style of the governmental roles and the involvement of the workshop participants table 1 we used two styles of the governmental roles to examine the effect of interventions that stimulate deliberation in workshops 1 4 we allowed the municipality and the water authority to make top down decisions i e they did not require other stakeholder roles to consent to changing taxes and drainage strategies these workshops allowed for a top down implementation of drainage strategies similar to what was considered at the turn of the century in the dutch peatlands for workshops 5 10 we changed this set up forcing the governmental stakeholders to deliberate their decisions i e they could only implement taxes and drainage strategies after obtaining the consent of the affected stakeholder roles these workshops reflected the current ideas on peatland management which acknowledge that cooperation between stakeholders is needed to produce viable management strategies we also experimented with the involvement of the workshop participants to examine the effect of various application styles workshops 1 9 had a hands on approach with the participants operating re peat themselves these workshops reflected a common setting of multi player serious game sessions which had not been used before to support the management of the dutch peatlands workshop 10 had a guided approach with the technical assistants operating re peat on behalf of the participants these workshops reflected a common setting of touch table sessions which had been used on several occasions to support the management of the dutch peatlands before our experiment arciniegas et al 2013 brouns et al 2015 overall this resulted in three groups of workshops with different settings 1 workshops 1 4 had a top down government style and hands on workshop participants 2 workshops 5 9 had a deliberative governmental style and hands on workshop participants and 3 workshop 10 had a deliberative governmental style and guided workshop participants because the workshops varied regarding the number of stakeholder roles and the duration of the interactive simulation table 1 we performed a sensitivity analysis first we analyzed the sensitivity of the results to excluding stakeholder roles that were not included in all workshops i e the collective residents not included in workshops 3 and 4 and the ngo not included in workshops 3 10 second we analyzed the results sensitivity to excluding workshops with less than 1 h allocated to interactive simulation i e workshops 3 4 and 9 2 4 perceiving added values we used post workshop questionnaires to enquire about the workshop participants perception of the added value of re peat to overcome implementation challenges of site specific collaborative management strategies to reduce soil subsidence in particular we enquired about a enhancing cooperation among them and b increasing their understanding of problems and action perspectives regarding the peatlands we used five point likert scales to measure their perceptions ranging from 2 very negative to 2 very positive we used pairwise mann whitney tests to assess statistical differences between the three groups of workshops in addition we included an open question in the post workshop questionnaires enquiring about arguments to elucidate the perceptions of added values afterwards we classified the 166 responses about the perceived added values into six categories we derived categories 1 4 from pelzer et al 2014 who distinguished between added values of iss on 1 the individual level regarding learning about the nature of the planning object 2 the individual level regarding learning about the perspective of other stakeholders 3 the group level i e the improvement of collaboration communication consensus and efficiency and 4 the outcome level i e better informed decisions in a follow up study pelzer et al 2016 found that participants in iss workshops perceived the added values at individual level to be key we therefore selected both individual values as separate categories in addition we included categories for 5 the context of the application e g the characteristics of the participants the policy process and the political context geertman 2006 and 6 the usability of re peat e g transparency user friendliness calculation time and integrality pelzer et al 2016 we used fisher s exact tests to assess statistical differences between the three groups of workshops 2 5 exploring different uses to reveal how the workshop participants used re peat we used logfiles that recorded all the actions of the stakeholder roles during the simulation in addition we used multiple video cameras to capture the activities of the actual workshop participants too afterwards we synchronized the videos and time coded the activities of each workshop participant drawing on the system for coding group working relations developed by nyerges et al 2006 we used four codes to annotate the activities of the workshop participants 1 inactive e g checking a cell phone or pouring a glass of water 2 reflective i e a getting support from technical assistants and b observing the interaction between other stakeholder roles 3 interactive i e discussion with other stakeholder roles and 4 explorative i e a focusing on the computer screen and b discussion with participants within the same stakeholder role for each participant we logged the cumulative number of actions and interactions hour 1 and the cumulative time spent on all coded activities we also used the logfiles to examine to what extent the workshop participants reached their goals and to what extent their own actions and the actions of other participants contributed to their overall progress score we used pairwise mann whitney tests to assess statistical differences between the groups of workshops regarding the time codes we excluded the inactive episodes time code 1 which on average accounted for 3 8 of the time we used post workshop questionnaires to enquire about the participants perception of their attitude we used seven point likert scales to measure these perceptions ranging from 3 very uncooperative to 3 very cooperative in addition we included an open question enquiring about the strategies they employed during the workshop afterwards the 210 responses to the open question were grouped into five categories 1 influencing the social system e g brokering deals with other stakeholders 2 influencing the physical system e g implementing measures 3 improving personal welfare either by maximizing profits or by minimizing costs 4 improving the peatlands e g by minimizing the soil subsidence and 5 no clear strategy we used pairwise mann whitney tests to assess statistical differences between the three groups of workshops regarding the attitudes of the workshop participants to assess differences in their strategies we used fisher s exact tests 3 results 3 1 differences in the perceived added values in post workshop questionnaires the workshop participants clearly stated they perceived re peat to be of high added value for enhancing cooperation among them and increasing their understanding of the social ecological system table 2 the perceptions of the groups were consistent with only small differences between them the proportion of workshop participants who elucidated their perceived added values with arguments regarding the outcome level was low differences between the groups were not significant arguments regarding the added value at group level and individual level were more common the argument more awareness of other perspectives was used by the largest proportion of workshop participants to explain their perception of the added values the proportion using this argument differed significantly between the groups in workshops 1 4 and workshop 10 p 0 037 the group of workshop 10 also stood out regarding the proportion that used the argument improved understanding of the peatlands it was significantly lower than the proportions for the groups of workshops 1 4 p 0 017 and workshops 5 9 p 0 045 approximately half of the participants remarked that the added values strongly depend on the context of the application such as the workshop setting and the characteristics of the participants some of them elucidated their remark by suggesting that the absence of conflicts was an important precondition for the added values their general perception was that although conflicts have not disappeared there is a trend toward consensus and cooperation among the stakeholders in dutch peatlands only for such contexts did they perceive high added values interestingly the participants in workshop 10 seemed less troubled by such considerations the proportion making such remarks was significantly smaller than in workshops 1 4 p 0 006 and workshops 5 9 p 0 001 almost half of the workshop participants mentioned that the usability of re peat contributed to their perception of the added values specifically they mentioned the credible results the abundance of detailed information and the realistic visual quality of the user interface for example the impact of site specific soil subsidence rates on the length of watercourses that required embankments to prevent them from slumping or the impact of site specific groundwater tables on the net value added of dairy farms some of them acknowledged that in general they struggled to comprehend the full complexity of peatland management they found re peat very useful because it presented a clear overview of all the aspects 3 2 behavioral differences during workshops logfiles and video recordings of the workshops revealed that on average the workshop participants simulated 12 4 actions hour 1 interacted with other participants 30 4 times hour 1 and spent most of their time on exploration table 3 per individual the number of actions and interactions hour 1 differed markedly with ranges of 2 38 actions hour 1 and 6 74 interactions with other participants hour 1 fig 3 the average ratio of actions hour 1 to interactions hour 1 was 0 6 with only 16 of the participants exhibiting ratios greater than 1 0 i e engaging in more actions hour 1 how the individual participants spent their time in the workshop also differed markedly with ranges of 4 65 for time spent on reflection 16 79 for time spent on exploration and 5 52 on time spent interacting with other participants fig 4 to some extent the variety in the behavior of the workshop participants related to the workshop settings the participants in workshop 10 spent much time on dialog within their group with the technical assistants assigned to their stakeholder role therefore they embarked on relatively few actions and interactions hour 1 consequently their results differed statistically from both other workshop groups in terms of their average proportion of time spent on reflection workshops 1 4 group u 17 0 p 0 000 workshops 5 9 group u 24 0 p 0 000 their average proportion of time spent on exploration workshops 1 4 group u 75 0 p 0 000 workshops 5 9 group u 87 0 p 0 000 their average proportion of time spent on interaction workshops 1 4 group u 192 0 p 0 005 workshops 5 9 group u 144 0 p 0 000 and their average number of actions hour 1 workshops 1 4 group u 84 0 p 0 003 workshops 5 9 group u 31 0 p 0 000 regarding the number of interactions hour 1 workshops 5 9 were statistically different workshops 1 4 group u 233 0 p 0 000 workshop 10 group u 80 0 p 0 000 the governmental decisions in workshops 1 4 did not require consent from other stakeholder roles consequently compared with the participants in workshops 5 9 the participants in workshops 1 4 had fewer interactions hour 1 and more actions hour 1 u 258 0 p 0 030 although the governmental decisions in workshop 10 also required consent from other stakeholder roles this did not result in markedly more interactions hour 1 than in workshops 1 4 because the participants in workshop 10 spent much time on discussions among the participants who shared their stakeholder role on average the overall progress score during the workshops was 22 table 4 this may seem rather modest but it must be noted that due to opposite effects of actions high scores were very difficult to realize for example a raise in surface water levels would decrease the soil subsidence rate which would increase the overall progress score however the frequency of flooding would increase as well which would lower the overall progress score due to such opposite effects only two pairs of workshop participants achieved an overall progress score of more than 50 remarkably in workshops 5 10 the average progress was mainly caused by the actions of other participants whereas in workshops 1 4 the progress was caused by the actions of the participants themselves in this group of workshops the progress due to the actions of other participants was significantly lower than in workshops 5 9 u 83 0 p 0 014 the difference coincides with the significantly lower number of interactions hour 1 table 3 post workshop questionnaires revealed that the participants used four types of strategies table 5 on average to almost the same extent significant differences were only found regarding the proportion of strategies aiming to influence the physical system this was lower in workshop 10 than in workshops 1 4 p 0 008 the difference coincides with the significantly lower number of actions hour 1 table 3 and the significantly lower proportion of participants using improved understanding of the peatlands as an argument to explain their perception of added values table 2 it seems that the participants in workshop 10 focused more on the social system dynamics than on the physical system dynamics consequently they might not have increased their understanding of the social ecological system as comprehensively as the participants in the other workshops however their appreciation of the added value of re peat did not reflect this shortcoming either because the effect was limited or because they were unaware of it on comparing participants attitude during the workshops we found that for workshops 1 4 in which governmental stakeholders were able to enforce top down decisions scores were significantly lower than in workshops 5 9 u 268 0 p 0 016 and workshop 10 u 109 5 p 0 012 in which governmental stakeholders needed to deliberate their decisions with the other stakeholders note that on average the lower scores did not reflect uncooperative impressions but impressions that were neutral to slightly cooperative uncooperative attitudes were only expressed by the participants in workshops 2 4 in these workshops many participants behaved individualistically and exhibited ratios of actions hour 1 to interactions hour 1 of up to 4 2 i e strongly preferring individual actions over interaction and deliberated coordinated actions because in workshops 5 10 deliberation was mandatory for the governmental stakeholder roles cooperative attitudes prevailed with only one of the 67 participants expressing a slightly uncooperative attitude it is noteworthy that participants in workshop 1 spontaneously engaged in several coordinated actions that required much deliberation resulting in cooperative attitudes similar to workshops 5 10 an average attitude score of 1 5 3 3 sensitivities analyzed the sensitivity analysis see appendix revealed only minor changes in the results due to the exclusion of a the stakeholder roles that were not included in all workshops i e collective residents and ngo and b the workshops with less than 1 h allocated to interactive simulations i e workshops 3 4 and 9 any changes in statistically significant differences between groups of workshops primarily reflected the smaller group sizes resulting from the exclusions we therefore deem the results not biased by variations in the workshop settings 4 discussion 4 1 system design the system design of an iss determines to what extent it can be used as a boundary object for the management of social ecological systems the general design principles are that the iss should always promote communication and translation between experts and promote mediation to avoid tradeoffs among the salience credibility and legitimacy of the scientific information cash et al 2003 in our project we aimed to secure these functions at the developmental stage of re peat by recruiting several key dutch peatland experts to translate existing scientific knowledge into content that was salient from a stakeholder perspective understandable by non scientific participants yet scientifically credible much of the knowledge incorporated in re peat resulted from preceding science policy interfaces such as processes of participatory knowledge development van brouwershaven and lokker 2010 and a boundary organization for innovative peatland management we believe the embeddedness of re peat in preceding science policy interfaces was an important condition for credibility and legitimacy arguably without this embeddedness the incorporated knowledge would have been more uncertain and disputable which would have diminished the effectiveness of the iss the workshop participants mentioned that the good usability of re peat also resulted from the abundance of detailed information and the realistic visual quality of the user interface arguably these features enhanced the salience of the information for them throughout the simulation they were continuously presented with sufficient information to make decisions in an easily understandable format the extent to which these iss features can be enhanced is related to the resulting calculation times an important condition for iss is its ability to keep pace with stakeholder interactions during actual decision making processes eijkelboom and janssen 2013 leskens et al 2014a in our case we were able to enhance the information load and the visual quality of re peat quite extensively because the graphics processing unit was an integral part of its computing system therefore the maximum calculation times were limited to a few seconds per action the iss features that enhance its usability are strongly related to the main added values that were perceived by the workshop participants like pelzer et al 2016 we found that iss workshop participants perceived the added values at individual and group level to be key and the added value of a better informed outcome to be less important in the workshops we witnessed how the participants collaboratively designed adaptive drainage strategies that could slow down soil subsidence although the generic effects of these adaptive drainage strategies had been known for approximately a decade querner et al 2012 they had never been implemented on a large scale because re peat translated the generic scientific knowledge into site specific effects from multiple stakeholder perspectives it created an operational fit between knowledge supply and demand in addition because the re peat supported informed negotiations it also raised awareness of mutually beneficial strategies it is noteworthy that later several workshop participants initiated a collaborative process to implement the adaptive drainage strategy designed during the workshops in the same peatland area they explored with re peat moreover they intend to continue using re peat as a boundary object to support collaborative management decisions in the years to come we believe the results show that the general purpose of iss should reflect capacity building at the individual and group level striving for a site specific awareness of the effects of measures and strengthening the resolve of the stakeholders to collectively implement these measures iss applications should not primarily focus on better informed outcomes although credible results are obviously important the iterative and interactive exploration of the myriad of management options should be the key consideration in support systems for collaborative environmental management 4 2 workshop setting our experiment with different workshop settings revealed that all workshop participants explored the physical system dynamics implementing measures and the social system dynamics brokering deals with other stakeholders however the participants in the workshops with a top down government style implemented markedly more actions hour 1 than the participants in the other workshops in addition their strategies were primarily aimed at influencing the physical system and their attitude during the workshops was significantly less cooperative than that of the participants in the other workshops consequently they appeared not to have taken full advantage of the potential of re peat to enhance cooperation because of their focus on physical measures their awareness of the perspectives of other stakeholders was markedly lower than the corresponding awareness in the other workshops furthermore their overall progress was relatively limited with other participants contributing significantly less to their overall progress than in the workshops 5 9 our findings suggest that interventions that stimulate deliberation can prevent individualistic strategies and instead foster cooperative attitudes in our research we achieved this by requiring mandatory deliberation of governmental decisions which fostered cooperative attitudes in workshops 5 10 other examples of interventions that can enhance cooperation are scripted instructions rummel and spada 2005 the incorporation of sequential phases with mandatory group tasks hämäläinen 2011 and scripted collaboration with workshop participants papadopoulos et al 2013 or their virtual counterparts hummel et al 2011 our experiment with various application styles also produced some marked results the participants in the guided workshop spent significantly more time on reflection than the hands on participants in the other workshops consequently they implemented markedly less actions hour 1 and employed strategies that were less often aimed at influencing the physical system in addition in the post workshop questionnaires they claimed significantly less often that their understanding of the peatlands had improved this might suggest that when the iss application is primarily aimed at increasing the participants understanding of the social ecological system a hands on approach such as in multi player serious gaming sessions is preferable to a guided approach such as in touch table sessions however we only regarded one workshop with a guided setting which does not suffice to draw valid conclusions further research is needed to examine to what extent the results might have been caused by other factors such as the limited time that was allocated to the plenary introduction and the debriefing in between rounds furthermore the impact of the facilitator and the technical assistants was underexposed in our research the facilitator and technical assistants of workshop 10 had contributed to 5 9 previous workshops in which they acquired the skills to effectively support an iss workshop arguably their performance contributed considerably to the added values that were perceived by the participants further research might increase our understanding of how iss facilitators can help improve the support of environmental management 4 3 context van enst et al 2014 identify two main contextual factors that define the structuredness of policy problems consensus on relevant norms and values and certainty about relevant knowledge on the one hand unstructured or wicked problems are deficient on both accounts therefore they lack a definite solution and exhibit a range of strategical and operational science policy interaction problems on the other hand completely structured problems are characterized by consensus as well as certainty which makes science policy interactions relatively straightforward the context of dutch peatlands can be seen as a moderately structured policy problem regarding knowledge several science policy interfaces have increased the certainty of generic knowledge but some uncertainty still remains especially regarding site specific effects from specific stakeholder perspectives in addition the workshop participants perceived a trend toward consensus among the stakeholders in dutch peatlands for the workshop participants the absence of conflicts was an important condition for the high added values they perceived we therefore believe our results primarily demonstrate how iss can be of added value for moderately structured environmental management problems arguably iss might be less effective in unstructured contexts with less consensus on norms and values and less certainty about relevant knowledge we suggest that in these contexts efforts to support environmental management should primarily be aimed at science policy interfaces which are suited for such contexts such as processes of participatory knowledge development boundary organizations and individual science policy mediators van enst et al 2014 5 conclusions our research demonstrated that iss can improve the support of environmental management implementation challenges for collaborative management strategies can be overcome by translating generic scientific knowledge into site specific effects from multiple stakeholder perspectives and by raising awareness of mutually beneficial strategies in iss workshops all participants explored the physical system dynamics implementing measures and the social system dynamics brokering deals with other stakeholders as a result the iss workshops enhanced cooperation among them and increased their understanding of problems and action perspectives regarding the social ecological system interventions that stimulate deliberation during the iss workshops were shown to prevent individualistic strategies and instead foster cooperative attitudes the embeddedness of an iss in preceding science policy interfaces is an important condition for the iss s credibility and legitimacy important conditions for the salience of the iss are an abundance of detailed information realistic visual quality of the user interface and calculation times that are short enough to keep pace with stakeholder interactions during the decision making processes in addition the general purpose of iss should reflect capacity building at the individual and group level striving for a site specific awareness of the effects of measures and strengthening the resolve of the stakeholders to collectively implement these measures we suggest further research on interactive simulation systems should capitalize on the ability of our research approach to yield in depth understanding of how iss can improve environmental management cross analyzing questionnaires with logfiles and videos of workshop proceedings can pinpoint how iss can effectively harness science for complex environmental management tasks this will help us understand how sustainable management of social ecological systems can be put into practice acknowledgments this study was funded by water authority hoogheemraadschap de stichtse rijnlanden the provinces of utrecht and zuid holland and the stowa foundation for applied water research the authors would like to thank jan van den akker gert jan van den born ernst bos gilles erkens john lambert and everybody at the tygron company for their contributions to the development of re peat we especially thank marije van bergen astrid de boer riebel joris van dijk harm de jong rudolf koster simon troost epke van der werf and florian witsenburg for their contributions to the workshops and two anonymous reviewers for their thoughtful comments which helped to improve the paper joy burrough was the language editor of a near final draft of the paper appendix sensitivity analysis regarding the included stakeholder roles table a1 changes in the results due to the exclusion of the stakeholder roles collective residents and ngo in terms of the average added values perceived per group of workshop participants for enhancing cooperation and increasing understanding and the proportion per workshop group that used an argument to explain their perceptions the scale of added values ranges from 2 very negative to 2 very positive changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 2 for the results that include all stakeholder roles table a1 workshop group 1 4 5 9 10 all no of participants 24 26 16 66 added value for enhancing cooperation 0 0 0 0 0 2 0 0 added value for increasing understanding 0 0 0 0 0 1 0 0 more awareness of other people s perspectives 6 12 3 2 improved understanding of the peatlands 2 7 5 3 support of the group process 3 10 15 9 better informed decisions 2 1 0 0 context of the application 7 10 8 4 usability of re peat 11 1 16 1 table a2 changes in the results due to the exclusion of the stakeholder roles collective residents and ngo in terms of the average number of actions and interactions hour 1 per group of workshop participants and their average proportion of active time spent on exploration reflection and interaction changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 3 for the results that include all stakeholder roles table a2 workshop group 1 4 5 9 10 all no of participants 24 26 16 66 actions hour 1 0 3 0 8 0 3 0 2 interactions hour 1 0 4 3 7 4 0 2 6 time spent on reflection 1 2 4 2 time spent on exploration 0 1 1 0 time spent on interaction 1 3 3 2 table a3 changes in the results due to the exclusion of the stakeholder roles collective residents and ngo in terms of the average extent to which groups of workshop participants reached their goals their average progress score and the average extent to which their progress was caused by their own actions or the actions of other participants note that the significance of the differences between groups of workshops did not change see table 4 for the results that include all stakeholder roles table a3 workshop group 1 4 5 9 10 all no of participants 24 26 16 66 common goal less soil subsidence 0 0 0 0 individual goals 0 7 4 4 overall progress score 0 4 3 2 progress due to own actions 1 1 3 0 progress due to actions of others 1 3 2 2 table a4 changes in the results after excluding the stakeholder roles collective residents and ngo in terms of the proportion per group of workshop participants that employed a strategy and their average attitude the scale of attitude ranges from 3 very uncooperative to 3 very cooperative changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 5 for the results that include all stakeholder roles table a4 workshop group 1 4 5 9 10 all no of participants 24 26 16 66 influence the social system 8 6 9 4 influence the physical system 4 9 12 0 improve the personal welfare 7 3 15 5 improve the peatlands 6 3 0 3 no strategy 2 2 12 1 attitude 0 1 0 0 0 3 0 0 we found only limited changes table a1 4 however on several occasions the limited changes in combination with the smaller group sizes affected the number of groups yielding significantly different results regarding the added value for cooperation the 4 increase in the average for group 10 resulted in significant differences workshops 1 4 u 38 0 p 0 018 workshops 5 9 u 44 0 p 0 019 regarding the argument more awareness of other people s perspectives the difference between workshops 1 4 and workshop 10 was no longer significant p 0 114 because a slightly larger proportion of participants in workshops 1 4 used this argument a similar effect was found regarding the argument improved understanding of the peatlands a slightly larger proportion of participants in workshop 10 who used this argument rendered the difference with the other workshops statistically insignificant workshops 1 4 p 0 114 workshops 5 9 p 0 054 an opposite effect was found regarding the argument support of the group process because none of the remaining participants in workshop 10 used this argument the difference between workshops 1 4 and workshop 10 was found to be statistically significant p 0 030 regarding the proportion of time spent on interaction the averages of workshops 1 4 and workshop 10 became slightly more similar as a consequence for the smaller group sizes the difference between workshops 1 4 and workshop 10 was not statistically significant u 57 0 p 0 103 regarding the proportion of workshop participants employing a certain strategy we found that because the proportion of participants who employed strategies aimed at influencing the system was 9 higher in workshops 5 9 and 12 lower in workshop 10 the difference between these groups was statistically significant p 0 004 although the changes regarding the attitudes and the impressions of interaction with other participants were also limited they affected the number of groups with significantly different results on the one hand the average attitude of the participants in workshops 1 4 became slightly less cooperative which was enough to prevent a significant difference vis à vis workshops 5 9 u 108 0 p 0 128 on the other hand the average attitude of the participants in workshop 10 became slightly more cooperative resulting in a significant difference vis à vis workshops 5 9 u 36 0 p 0 038 the average impression of the interaction was also more cooperative which resulted in a significant difference vis à vis workshops 1 4 u 24 0 p 0 010 sensitivity analysis regarding the duration of the interactive simulation table a5 changes in the results after excluding short workshops in terms of the average added values perceived per group of workshop participants for enhancing cooperation and increasing understanding and the proportions of the groups that used an argument to explain their perceptions the scale of added values ranges from 2 very negative to 2 very positive changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 2 for the results that include all workshops table a5 workshop group 1 4 5 9 10 all no of participants 20 25 22 67 added value for enhancing cooperation 0 0 0 1 0 0 0 0 added value for increasing understanding 0 0 0 0 0 0 0 0 more awareness of other people s perspectives 3 2 0 0 improved understanding of the peatlands 7 7 0 2 support of the group process 8 0 0 2 better informed decisions 2 1 0 1 context of the application 5 9 0 8 usability of re peat 5 11 0 3 we found only limited changes tables a5 8 however on four occasions the limited changes in combination with the smaller group sizes affected the number of groups with significantly different results first the difference between workshops 5 9 and workshop 10 regarding the proportion of participants that used the arguments improved understanding peatlands was no longer significant p 0 140 second the proportion of workshop participants employing strategies aimed at influencing the physical system differed significantly between workshops 5 9 and workshop 10 p 0 028 third because the reduction of soil subsidence was less in workshops 1 4 and more in workshops 5 9 the difference between these groups was statistically significant u 40 0 p 0 032 fourth the average attitude in workshops 1 4 was more cooperative as a result the difference vis à vis workshops 5 9 was no longer significant u 149 0 p 0 342 this effect can be explained by the exclusion of the negative attitudes prevailing in workshops 3 and 4 table a6 changes in the results after excluding short workshops in terms of the average number of actions and interactions hour 1 per group of workshop participants and their average proportion of active time spent on exploration reflection and interaction note that the significance of the differences between groups of workshops did not change see table 3 for the results that include all workshops table a6 workshop group 1 4 5 9 10 all no of participants 20 25 22 67 actions hour 1 0 7 3 0 0 0 0 4 interactions hour 1 1 5 1 6 0 0 0 3 time spent on reflection 1 0 0 2 time spent on exploration 1 1 0 1 time spent on interaction 1 1 0 1 table a7 changes in the results after excluding short workshops in terms of the average extent to which groups of workshop participants reached their goals their average progress score and the average extent to which their progress was caused by their own actions or the actions of other participants note that the significance of the differences between groups of workshops did not change see table 4 for the results that include all workshops table a7 workshop group 1 4 5 9 10 all no of participants 20 25 22 67 common goal less soil subsidence 3 10 0 5 individual goals 10 2 0 4 overall progress score 6 5 0 4 progress due to own actions 4 1 0 1 progress due to actions of others 2 3 0 4 table a8 changes in the results after excluding short workshops in terms of proportions of groups of workshop participants that employed a strategy and their average attitude the scale of attitude ranges from 3 very uncooperative to 3 very cooperative changes in the significance of the differences between groups of workshops p 0 05 are asterisked see table 5 for the results that include all workshops table a8 workshop group 1 4 5 9 10 all no of workshop participants 20 25 22 67 influence the social system 1 3 0 1 influence the physical system 3 13 0 4 improve the personal welfare 10 6 0 5 improve the peatlands 9 10 0 7 no strategy 1 6 0 2 attitude 0 3 0 1 0 0 0 2 
