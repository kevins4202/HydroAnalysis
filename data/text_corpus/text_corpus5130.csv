index,text
25650,an open source tool has been developed to facilitate constrained single and multi objective optimization under uncertainty cmou analyses the tool uses the well known pest interface protocols to communicate with the underlying forward simulation making it non intrusive the tool contains a built in parallel run manager to make use of heterogeneous and distributed computing resources several popular and well known evolutionary algorithms are implemented and can be combined with a range of approaches to represent uncertainty in model derived constraint objective values these attributes serve to address the current barrier to adopt advanced cmou analyses for a wide range of decision support problems across the environmental modeling spectrum we demonstrate the capabilities of the cmou tool on a well known analytical benchmark problem that we augmented to include uncertainty as well as on a synthetic density dependent coastal groundwater management benchmark problem both demonstrations highlight the importance of explicitly accounting for uncertainty to convey risk and reliability in pareto optimal design keywords resource management decision support optimization under uncertainty multi objective optimization non intrusive 1 introduction increasingly decision making for environmental resource management is informed by predictive models model based decision support regarding unseen future outcomes can be achieved in several ways including with predictive scenario analyses or through formal management optimization analyses among others formal management optimization analyses are sometimes favored as they can offer a more direct means of predictive decision support ahlfeld and mulligan 2000 maier et al 2014 gorelick and zheng 2015 danapour et al 2021 that is while predictive scenarios are typically focused on simulating the outcome of what if conditions related to unseen changes in forcings optimization analyses can extend the utility of predictive scenario analyses for example optimization analyses can be used to identify optimal resource management decisions that avoid unwanted outcomes while minimizing costs and or maximizing resource allocation optimization analyses can vary significantly in their formulation including factors such as dimensionality decision variables objectives and constraints global versus local search methods non linear versus linear problem formulations etc in many decision support settings how best to conceptualize and formulate an optimization problem is subjective and requires investigation and testing for example a constrained management optimization problem may be better represented as a constrained multi objective management optimization problem where one or more inequality constraints are treated as an objective that can be traded off with other objectives deb 2001 nouiri et al 2015 siade et al 2019 paulinski et al 2018 the interplay between constraints and objectives warrants efficient reformulating management optimization problems by treating one or more constraints as objectives and vice versa this leads to constrained multi objective optimization fonseca and fleming 1998 when models are used to guide environmental resource decision making the uncertainty associated with decision relevant simulated outcomes e g predictions forecasts constraints objectives etc must be expressed in many modeling settings particularly subsurface settings the inputs to the model are highly dimensional and highly uncertain and there are typically few data available to condition uncertain model inputs meaning that uncertainty in decision relevant simulated outputs can be significant moore and doherty 2006 herckenrath et al 2011 brakefield et al 2015 knowling et al 2020a the need to quantify uncertainty in decision relevant simulated outcomes as well as to flexibly represent modeling analyses as constrained multi objective optimization problems gives rise to a complex coupled analysis referred to herein as constrained multi objective optimization under uncertainty cmou several previous works have investigated various aspects of cmou in environmental modeling settings see for example singh and minsker 2008 luo et al 2016 singh and minsker 2004 mantoglou and kourakos 2007 rezaei et al 2020 these studies collectively a demonstrate the value that cmou brings to decision support b identify the broad range of settings that cmou may be applied to and c highlight the complexity and user requirements that implementing cmou is likely to entail pertinent to this last point is that to our knowledge there exists no generally applicable tool that facilitates cmou analyses in a non intrusive i e model independent and flexible way to support the wide range of environmental modeling settings that could benefit from cmou to enable practitioners the ability to realize the benefits of cmou analyses we have developed pestpp mou pestpp mou is an open source tool that pairs several existing and well known single objective and multi objective evolutionary algorithms with flexible approaches to represent uncertainties in model derived outputs that serve as constraints and or objectives in the optimization solution process pestpp mou has been designed to function in a non intrusive and flexible way using the well known pest model interface protocols so that it can be easily applied to a wide range of cmou problems that require different underlying simulation engines to make use of parallel computation pestpp mou includes a built in fault tolerant parallel run manager for heterogeneous and distributed computing systems although there are some already available software tools that implement certain forms of multi objective optimization see for example kwakkel 2017 matott 2005 and hadka and reed 2013 among others pestpp mou is unique in that it implements constrained single objective and multi objective optimization under uncertainty in a non intrusive and easy to deploy e g statically linked stand alone software tool 2 background and conceptual overview as discussed in white et al 2018 there is nuanced terminology associated with both uncertainty risk analysis and management optimization analysis particularly multi objective optimization analysis herein we define the following terms decision variable an input to the model that is controlled by management action the optimization process adjusts these quantities in seeking optimal solutions simple constraint an inequality quantity that is derived from combinations of decision variables because this quantity is derived purely from decision variable values it is not subject to uncertainty simple objective a quantity that is derived from combinations of decision variables that is treated as an objective to minimize or maximize because this quantity is derived purely from decision variable values it is not subject to uncertainty model derived constraint an inequality quantity that is at least partially dependent on model outputs because this quantity depends on simulation results it is subject to uncertainty in as much as it is sensitive to uncertain model inputs model derived objective a quantity that is at least partially dependent on model outputs and that is treated as an objective to minimize or maximize because this quantity depends on simulation results it is subject to uncertainty in as much as it is sensitive to uncertain model inputs parameter a quantity representing uncertain model inputs collectively the uncertainty in parameters may induce uncertainty in model derived constraints objectives individual a single set of decision variable values population a collection of individuals each of which has a value for each decision variable the population is evolved through each generation of the cmou solution algorithm towards an optimal set of solutions generation an iteration of the cmou algorithm that consists of generating a new candidate population evaluating the population by running each individual in the population through the model and evaluating the resulting feasibility and fitness of each individual across multiple generations the population evolves emulating the processes of natural selection feasible an optimization solution e g an individual that satisfies all constraints fitness the metric used to rank individuals in a population in the case of single objective optimization fitness is typically defined only by feasibility and objective function value in multi objective optimization fitness includes feasibility as well as pareto optimality dominance and diversity uniqueness elements generator an algorithmic process that emulates part of the evolutionary process by generating offspring from one or more parent individuals in the current population and selector an algorithmic process that emulated part of the evolutionary process by comparing all members in a population to determine fitness which can then be used by the generator to propagate desirable characteristics to the next generation 2 1 evolutionary meta heuristic multi objective optimization evolutionary algorithms are an important and exciting branch of research that have applications across the computational sciences e g michalewicz et al 2003 in the most general sense this class of search algorithms employs the darwinian concept of survival of the fittest or natural selection and uses characteristics and behaviors observed in nature to generate and evolve a population of individuals towards optimality herein we only briefly review the concepts related to multi objective optimization interested readers are referred to maier et al 2014 siade et al 2019 for more detailed information regarding evolutionary computation in the context of environmental resource management how optimality is defined is a critical aspect of any multi objective evolutionary algorithm and may include elements of feasibility diversity and pareto optimality pareto optimality is a central concept in multi objective optimization we define an individual i of a population as pareto optimal or pareto dominant compared to individual j if for all objectives i has an equal or better value compared to j and i has a strictly better value than j for at least one objective e g zitzler et al 2003 emmerich and deutz 2018 conceptually a solution is pareto optimal if for all objectives no single objective can be improved further without compromising the value of another objective the goal then of a multi objective optimization algorithm is to find a population of pareto optimal individuals collectively a population of pareto optimal individuals map the optimal trade off between non commensurate and competing objectives this trade off surface is referred to as the pareto frontier 2 2 chance constraints and objectives because model derived constraints objectives may depend at least partially on model outputs and because all models of environmental systems are subject to uncertainty it follows that these model derived quantities are also uncertain the extent to which they are uncertain depends on many factors including the system that is being modeled the model itself and the available information both prior estimates of the model inputs and historic state observations to be assimilated etc as well as many other factors the focus of predictive uncertainty analysis is to quantify uncertainties in unmeasured decision relevant model outputs and possibly also to reduce these uncertainties through data assimilation e g from prior to posterior uncertainty estimates within cmou analyses the uncertainties in the model derived constraints objectives are the same uncertainties that would arise if the model derived constraints objectives were instead considered predictions within an uncertainty data assimilation analysis herein to account for the range of plausible constraint objective values arising from model input uncertainty within the constrained multi objective optimization solution process we employ the concept of chance constraints and or chance objectives the chance in chance constraints objectives represents the combination of model derived constraint objective uncertainty with a specified risk value to collapse the constraint objective uncertainty to a single representative value the value of risk expresses the level of aversion to an unwanted outcome for example a risk value of 0 95 expresses a 95 chance of avoiding an unwanted outcome a risk averse high reliability scenario while a risk value of 0 05 expresses only a 5 chance of avoiding an unwanted outcome a risk tolerant low reliability scenario conceptually the desired risk value is combined with the uncertainty in model derived constraints objectives by shifting the simulated values toward or away from the desired outcomes along the range of possible values interested readers are referred to wagner and gorelick 1987 white et al 2018 2020 and the references cited therein for more background on the concept of chance constraints and risk 2 3 coupling in cmou analyses models are used to evaluate two fundamental relations a the relation between decision variables and model derived constraints objectives the decision variable relation and b the relation between uncertain parameters and model derived constraints objectives the parameter relation if changes in the decision variable values influence the parameter relation or changes in the parameters influence the decision variable relation then this interaction should be considered within the cmou analysis because it may affect the optimal risk based solution s herein we refer to the interaction between the uncertainty risk analysis which relies on the parameter relation and the management optimization analysis which relies on the decision variable relation within the larger cmou analysis as coupling lopez and beck 2012 hamzehkolaei et al 2016 if no coupling exists then the chance risk estimates are independent of the decision variable values meaning the chance risk estimates can be used across multiple generations of the cmou process and shared between individuals within a population the uncertainty estimates calculated at one point in decision variable space are valid across decision variable space however if a coupling exists then as the values of decision variables change across generations and or across decision variable space the relation between the uncertain parameters and model derived constraint objective values may need to be re evaluated for example in designing a groundwater contaminant transport treatment system as the treatment rate and location of extraction and injection wells changes as part of an optimal design search the relation between uncertain properties e g hydraulic conductivity porosity and the simulated concentration at compliance points e g constraints objectives will be affected because the flow field induced by differing treatment designs may activate the influence of previously insensitive uncertain parameters knowling et al 2020b given the problem specific nature and wide range of coupling possibilities we recognize three difference approaches to represent uncertainty in model derived constraints objectives 1 specified uncertainty approach specified 2 first order second moment fosm approach fosm based 3 stack based approach stack based these approaches imply different uncertainty conceptualizations as well as different representations of the coupling between the uncertainty risk analysis and the management optimization analysis within the larger cmou analysis flexibility in how uncertainty and the corresponding chances and risk are represented in the cmou analysis is a key consideration for undertaking cmou analyses because each approach represents a trade off between computational burden and rigorous representation of uncertainty in model derived constraints objectives we note that each of the following approaches can be employed as either prior or posterior formulations depending on the availability of state observations to condition parameters it is expected that in practice most applications of cmou will use posterior uncertainty estimates and that these posterior estimates will be derived from an upstream e g previously completed data assimilation and uncertainty analysis 2 3 1 specified approach the simplest and most computationally efficient uncertainty representation is the specified approach as the name implies this approach relies on directly specifying the uncertainty e g standard deviation of model derived constraints objectives with an assumed gaussian probability distribution with the mean of the distribution being the simulated value for the constraint objective by specifying constraint objective uncertainties no additional model evaluations are required in the cmou solution process however this approach is only appropriate if there is no expected coupling between the uncertainty risk analysis and the management optimization analysis the specified standard deviations would typically be obtained from a previous uncertainty data assimilation analysis parameters are not explicitly used in this formulation 2 3 2 fosm based approach a more robust approach for representing constraint objective uncertainty is with fosm analyses see e g white et al 2018 white et al 2016 doherty 2015 menke 1989 goldstein and wooff 2007 like the specified approach the fosm representation also implies the uncertainty in the model derived constraints objectives can be approximated by a gaussian distribution and the mean values for these implied gaussian distributions are the simulated values however the fosm approach involves calculation of these uncertainties via conditional covariance propagation from uncertain parameters to the model derived constraints objectives in this way users provide the uncertainties of the parameters via a statistical parameter distribution and the parameter uncertainties are propagated to the model derived constraints objectives on the fly during the cmou analysis the fosm approach requires a jacobian matrix of sensitivities of model derived constraints objectives to the uncertain parameters furthermore the fosm based approach allows for some accommodation of coupling between the risk chance analysis and the management optimization analysis since the jacobian matrix and the corresponding fosm based constraint objective uncertainties can be updated as the cmou analysis moves through generations and the region of focus in decision variable space changes however the cost of repeated evaluation of the jacobian matrix especially when the number of parameters is large is computationally expensive 2 3 3 stack based approach the most robust approach for representing uncertainties in the model derived constraints objectives is with stacks bayer et al 2008 in this representation an ensemble or stack of parameter realizations is used to represent model input uncertainty essentially using monte carlo approach to account for these uncertainties compared to the previous two approaches the stack based approach makes no assumption about the statistical distribution of the model derived constraint objective uncertainties that is the stack based approach is a non parametric representation of constraint objective uncertainties the stack based approach uses the model derived constraint objective cumulative distribution function arising from evaluating the parameter stack at a given point in decision variable space once the stack of parameter realizations is evaluated each model derived constraint objective has a stack of possible values this set of possible values represents the range of values that model derived constraints objectives may take 3 methods and implementation pestpp mou has been designed to implement a wide range of cmou analyses to support environmental resource decision making it is a statically linked c program that uses the well known pest model interface protocols doherty 2015 to communicate with the underlying simulation in a non intrusive and flexible way given that cmou analyses require many model forward evaluations and inevitably some of these evaluations may lead to forward model convergence failure or other model instability issues pestpp mou includes a built in fault tolerant parallel run manager the run manager uses tcp ip to communicate a lower level and more common network protocol than the message passing interface clarke et al 1994 which allows pestpp mou to be deployed on a wide range of heterogeneous computing clusters including ad hoc networks and in cloud settings see fienen and hunt 2015 for more information on this important feature the non intrusive design and flexible parallel run management capabilities of pestpp mou are key contributions of this work as these aspects enable cmou analyses for many environmental management settings 3 1 generators and environmental selectors the field of evolutionary multi objective optimization is rapidly evolving and several tools that allow practitioners and researchers to explore capabilities of these algorithms already exist e g durillo and nebro 2011 kwakkel 2017 hadka and reed 2013 rather than develop yet another entry in this already crowded field we have focused instead on developing a production level tool that implements several commonly used and previously vetted evolutionary algorithms with flexible and generic uncertainty risk evaluation in non intrusive way with built in fault tolerant parallel run management as presently implemented pestpp mou offers several algorithmic choices for evolutionary generation and selection all of which have displayed strong multi objective optimization performance in the literature e g reed and hadka 2014 durillo and nebro 2011 pestpp mou purposely separates the generator and selector essentially breaking apart the two primary evolutionary mechanisms so that users have greater flexibility on how to combine these two algorithmic elements to be clear the generator produces new individuals from the existing population these new individuals are then evaluated through the model and their fitness is calculated by the selector choices for evolutionary generators include the following differential evolution de storn and price 1997 including a self adaptive variant georgioudakis and plevris 2020 whereby the crossover and mutation rates are treated as decision variables particle swarm optimization pso kennedy and eberhart 1995 siade et al 2019 simulated binary crossover sbx deb and agrawal 1995 and highly disruptive polynomial mutation plm hamdan 2012 pestpp mou users can also employ combinations of these generators this is achieved by simply specifying more than one generator we have also included the reducing set concurrent simplex lewis et al 2006 simplex a version of the nelder mead simplex olsson and nelson 1975 lagarias et al 1998 with increased parallelism as a gradient based generator in pestpp mou as presently implemented pestpp mou uses the constrained nsga ii environmental selector proposed in deb et al 2002 the code also implements the unconstrained spea 2 environmental selector described in zitzler et al 2001 just as with the generators we recognize that much past and current research has been and is being devoted to studying constraint handling in multi objective evolutionary optimization e g kramer 2010 huang and wang 2021 mezura montes 2009 fonseca and fleming 1998 aguirre et al 2004 liu et al 2021 as such we have relied on previously published approaches that have demonstrated success in constraint handling in applied environmental modeling studies to guide the current implementation in pestpp mou e g majedi et al 2021 fan et al 2020 mirzaie nodoushan et al 2017 naghdi et al 2021 raei et al 2017 we note that if a single objective is specified pestpp mou seamlessly functions as a constrained single objective evolutionary optimization tool and the same uncertainty risk functionality is still available for the single objective and any constraints that are included provided these quantities are model derived this seamless flexibility allows users to efficiently move between constraints and objectives to evaluate different optimization problem formulations 3 2 chance constraints objectives pestpp mou offers the three previously described approaches to represent model derived constraint objective uncertainty specified fosm based or stack based pestpp mou can automatically draw the parameter stack for the listed parameters using either basic initialization information e g initial values and parameter bounds by assuming the parameters are statistically independent in the case that pestpp mou is being used downstream from a data assimilation uncertainty analysis users can provide a posterior parameter covariance matrix e g doherty 2015 white et al 2016 white 2018 for stack generation and or for fosm uncertainty representation users can also supply a previously generated stack such as a parameter ensemble derived from a posterior uncertainty analysis pestpp mou also automates the process of populating and evaluating the jacobian matrix needed in the fosm based approach through shared file types related to the chance process pestpp mou provides a seamless transition from uncertainty data assimilation analyses undertaken with other tools in the pest and pest suites to cmou analyses we note that the entire risk uncertainty process is not required to apply pestpp mou and is only activated by specifying a risk value not equal to 0 5 pestpp mou includes two options for where in decision variable space to evaluate stack based chances through the chance points option namely at a single representative point single a computationally cheap but less rigorous option or to evaluate the stack at all individuals in a population all a computationally expensive but more rigorous option following the concepts presented in deb et al 2007 pestpp mou also provides the functionality to treat risk as an objective to be maximized i e driven towards a risk averse high reliability stance in this way if decision makers are unsure what risk stance is appropriate a priori or users are unsure what maximum value of risk may lead to infeasibility then specifying risk as a formal objective alleviates the requirement to specify a single risk value note however this option will likely lead to an increased computational burden as additional generations will likely be required to accommodate an additional objective 3 3 coupling approaches how and when the chance risk estimates should be updated in the course of a cmou analysis is likely to be problem specific given this pestpp mou strives to provide users with flexibility to accommodate a range of coupling approaches users can choose to re use or update the risk uncertainty estimates for a given generation flexibly with pestpp mou to lower the computational burden of applying pestpp mou for the fosm based approach only the second moment is used so this implies constraint objective uncertainty estimates can be broadcast across the population by simply applying the fosm estimated uncertainties in model derived constraints objectives to each individual s simulated values for these quantities to update the fosm estimates the jacobian matrix must be re evaluated which when requested is completed using the decision variable values nearest the current generation mean decision variable values in a euclidean distance sense in the case of a strong coupling within the cmou analysis using stack based uncertainty representation the stack can be evaluated at each individual in the population or if a weaker coupling is present the stack can be evaluated at a single representative individual point in decision variable space and broadcast to the remaining individuals as an option to reduce the computational burden the former requires many repeated stack evaluations with a complete stack evaluation required at each individual in the current population number of model evaluations equals the number of realizations in the stack times the number of individuals in the population while the latter requires a single stack evaluation the number of model evaluations equals the number of realizations in the stack as such the implementation of flexible coupling schemes with the stack based approach requires additional consideration to re use stack based uncertainty risk estimates across multiple generations pestpp mou maps each current individual solution to the nearest stack evaluation point in decision variable space in a minimum euclidean distance sense the nearest stack results are then translated to each corresponding current population individual by differencing the mean value of the stack results with the simulation results of the current individual in this way the available stack results nearest each individual are used to represent constraint objective uncertainty at that individual without requiring a re evaluation of the stack again reducing the computational burden 4 example applications we demonstrate the functionality of pestpp mou through two example applications the first is a well known analytical constrained multi objective benchmark problem that we augmented in order to transform this problem into chance constrained chance objective problem this problem is used to benchmark pestpp mou facilitate comparisons between the performance of different evolutionary generators as well as demonstrate how chances and risk affect the shape and orientation of the constrained pareto frontier in objective space the second example application is a variant of a popular saltwater intrusion benchmark problem developed for the purpose of designing a hypothetical artificial recharge basin to maintain viability of coastal groundwater extraction wells this problem is used to demonstrate decision support cmou capabilities that pestpp mou enables 4 1 benchmark problem the analytical constrained multi objective problem that we used for benchmarking is known as constr deb 2001 deb et al 2002 the constr problem is formally defined as follows minimize f 1 x x 1 θ 1 f 2 x 1 x 2 x 1 θ 2 subject to x 1 0 1 1 0 x 2 0 5 and g 1 x x 2 9x 1 6 g 2 x x 2 9x 1 1 where θ 1 and θ 2 are additive uncertain parameters as shown on fig 1 the pareto frontier between f 1 and f 2 red dashed is truncated by the edge of the feasible region shaded region as a result the optimal pareto frontier is partially defined by the trade off between f 1 and f 2 and partially defined by constraints we modified the constr problem to include two uncertain parameters θ 1 and θ 2 whose action is additive for each of the two objectives f 1 and f 2 respectively this introduces uncertainty into the calculated f 1 and f 2 objective values to be clear this additive uncertainty results in no interaction between the risk uncertainty analysis and the optimization analysis so we use this uncoupled problem to verify the behavior of the various uncertainty representations and coupling strategies for this problem we conducted several experiments specified risk values of 0 05 0 5 and 0 95 using a specified uncertainty representation 100 individual population and 50 generations risk as an objective using a specified uncertainty representation 100 individual population and 150 generations risk as an objective using a stack based uncertainty representation evaluating the stack at a single chance point and reusing this single stack across all generations 100 individual population and 150 generations and risk as an objective using a stack based uncertainty representation evaluating the stack at all individuals in the initial population and reusing these stack evaluations across all generations 100 individual population and 150 generations the specified risk analyses represent a risk tolerant 0 05 risk neutral 0 5 and risk averse 0 95 stance respectively treating risk as an objective will likely increase the complexity of the cmou analyses so the number of generations was increased to 150 making the results of the risk as an objective analyses comparable to the combined results of the three specified risk analyses with respect to computational budget 4 1 1 results the application of pestpp mou to the modified constr benchmark verifies certain aspects of the cmou solution process encoded in pestpp mou and also demonstrates some of the more advanced capabilities of pestpp mou fig 1 a and b show that for the risk neutral case risk value of 0 5 the algorithm encoded in pestpp mou finds the correct pareto frontier with either the de or pso generator furthermore the risk averse and risk tolerance solutions fig 1 a through f are located in objective function space as expected that is they are shifted in the appropriate direction with respect to the risk neutral pareto frontier the risk tolerant solutions occupying a larger feasibility region while the risk averse solutions occupy a smaller feasible region even in this analytical benchmark problem with simple additive objective uncertainty the importance of accounting for uncertainty and ultimately risk is demonstrated by the change in pareto optimal solutions as a function of risk the de generator appears to have found more pareto optimal solutions than the pso generator for the specified risk analyses the de pareto solution set point are more dense however we caution readers not to draw general conclusions regarding the relative performance of de and pso from these analyses because it is well known that there is no single algorithm e g generator selector etc that is optimal for all possible optimization problems the use of the risk as an objective option available in pestpp mou is shown to perform as expected specifically the results of the risk as an objective approach fig 1 c d e f appear to correspond appropriately to explicit risk averse and risk tolerant pareto frontiers fig 1 a and b additionally given that this problem was designed to have no coupling between the risk uncertainty analysis and the optimization analysis we expect the outcome of varying the uncertainty representation and coupling strategies to yield similar results verifying certain aspects of pestpp mou functionality for example the results of specified uncertainty approach fig 1 c and d appear very similar to the stack based results fig 1 e and f 4 2 henry problem to demonstrate the risk based decision support capabilities of cmou via pestpp mou in the context of coastal groundwater management we evaluate a modified form of the cross section henry saltwater intrusion problem henry 1964 the henry problem is patterned after the coastal aquifer conditions in south florida while its spatial and temporal dimensions are representative of a laboratory scale experiment the henry problem is well known e g herckenrath et al 2011 simpson and clement 2004 zidane et al 2012 holzbecher 2016 croucher and o sullivan 1995 and provides a computationally efficient test problem for density dependent groundwater flow that captures many relevant aspects of coastal saltwater intrusion phenomena the density dependent characteristics of the henry problem combined with non linear relations between model inputs and model outputs makes the henry problem an illustrative benchmark for evaluating the cmou algorithm encoded in pestpp mou starting with the henry model design available with modflow 6 hughes et al 2017 langevin et al 2017 see the modflow 6 software release langevin et al 2021 for details we endeavored to design an artificial recharge basin to mitigate landward migration of the fresh water saltwater interface under increased groundwater extraction for water supply the model used herein has the following details 160 columns and 40 layers of 0 025 m spacing columns are numbered starting farthest from the coastal boundary condition fig 2 note the water table is at or near 1 0 m in elevation three vertically aligned groundwater extraction wells located at column 60 in layers 10 20 and 30 fig 2 upgradient specified inflow boundary condition in column 1 and all layers 0 071 275 m 3 d per layer e g simpson and clement 2004 with a concentration of 0 0 g l salinity and general head boundary condition specified in column 160 and all layers to represent a coastal boundary with a stage of 1 0 m and concentration of 35 0 g l salinity three distinct periods of time were simulated pre development period of 2 5 days to establish a stable freshwater saltwater interface position without any groundwater extraction historical groundwater use period of 0 3 day which induces landward movement of the freshwater saltwater interface towards the groundwater extraction wells fig 2 and a predictive period of 2 5 days where the artificial recharge basin is in operation while groundwater extraction continues or increases depending on the selected decision variable values within this simulation we want to design a single artificial recharge basin located on the surface layer 1 between the coastal boundary and the upgradient inflow boundary to help maintain viability of the extraction wells to produce potable water e g salinity less than or equal to 0 5 g l the following decision variables subject to optimization were defined location of the left upgradient edge of the recharge basin column number from the left edge of the model domain minimum 1 maximum 140 note the width of the recharge basin was fixed at 10 model cells wide the rate of artificial recharge applied to the basin m d minimum 0 001 maximum 12 0 the concentration of recharge applied to the basin g l minimum 0 5 maximum 17 0 and the extraction rate of each of the three groundwater extraction wells m 3 d minimum 0 0 maximum 3 0 the objectives of this paired recharge basin groundwater extraction design cmou analysis are maximize the artificial recharge basin distance from the upgradient inflow prefer a more coastal location for the artificial recharge basin minimize the artificial recharge rate applied to the basin prefer to apply less artificial recharge maximize the concentration of artificial recharge applied to the basin prefer to apply low quality water and maximize the combined groundwater extracted from the three extraction wells the problem is subject to the following constraints the simulated combined flux weighted mean salinity of the three extraction wells is less than 0 5 g l e g potable water the simulated maximum water level within the recharge basin is less then 1 025 m 0 025 m allowable increase in water table elevation resulting from the recharge basin and the combined flux from the three extraction wells is greater than 0 7 m 3 d the combined historical extraction rate uncertainty arises in this optimization problem formulation through the concentration and groundwater level constraints because the model is used to evaluate the concentration at the extraction wells and groundwater level within the artificial recharge basin for a given set of decision variables these simulated concentrations and groundwater levels are uncertain inasmuch as the uncertain model inputs influence these simulated outputs we defined the following parameters to represent model input uncertainty hydraulic conductivity in every active model cell m d mean 864 minimum 86 4 maximum 8640 0 porosity in every active model cell dimensionless mean 0 35 minimum 0 3 maximum 0 4 and upgradient inflow column 1 every layer m 3 d mean 0 071 275 minimum 0 064 147 5 maximum 0 784 025 a prior uncertainty stance was used in this demonstration correlation in the spatially distributed parameter types was defined using an exponential variogram with a range of 0 1 m and sill proportional to the parameter bound range a stack of 100 parameter realizations was used to represent parameter uncertainty in the pestpp mou analyses the coupling between the risk chance estimates and the optimization problem was defined such that the stack was evaluated at every individual of the first generation and then reused for all subsequent generations two pestpp mou analyses were completed with the modified henry model artificial recharge design problem first a risk neutral analysis was completed to better understand possible feasible artificial recharge basin designs and trade offs assuming the model derived constraints are subject to no uncertainty the risk neutral analysis used 100 individuals and was evolved across 100 generations a second pestpp mou analysis was completed with risk as an objective using 100 individuals and 500 generations the 100 realization parameter stack was used to represent model input uncertainty in this analysis the henry based optimization analysis used the de generator and the constrained nsga ii environmental selector note the optimization problem occurs solely on the predictive period however to include the stack based chances and to ensure these chance estimates are free from temporal artifacts all simulation periods were evaluated for each model run the pestpp mou input datasets for this analysis were generated programmatically using pyemu white et al 2016 using the pstfrom automation process white et al 2021 4 2 1 results several interesting insights are gained by inspecting risk neutral outcomes fig 3 first we note that through the use of an artificial recharge basin combined with formal optimization substantially larger quantities of potable water can be produced compared to the more naive production strategy used during the historical period a strategy that led to landward migration of the freshwater saltwater interface only solutions that exceed the naive production are shown on fig 3 among the most interesting insights gained from the risk neutral analysis is the pareto optimal relation between combined groundwater extraction rate and artificial recharge rate fig 3 m and p there is an apparent bimodal relation between these two important objectives seen as two distinct lobes of optimal solutions one mode involves an approximately low ratio of recharge to extraction and uses relatively low salinity water whereas the other mode involves a much higher ratio of recharge to extraction and uses high salinity water fig 3 a the physical location aspect of these modes is key to understanding the low ratio high quality water mode includes artificial recharge basins located landward of the extraction wells fig 3 a essentially augmenting fresh upgradient inflows with high quality low salinity water to mitigate landward movement of the fresh water saltwater interface fig 3 c f the high ratio low quality water mode includes recharge basins that are located seaward of the extraction wells where large volumes relative to the combined extraction rate of low quality high salinity water are injected to vertically depress the freshwater saltwater interface and establish a hydraulic divide preventing high salinity seawater from moving landward in all but the deepest parts of the simulated aquifer these solutions generally involve shifting extraction to the shallowest extraction well located in layer 10 this dual mode behavior highlights the capabilities of constrained evolutionary multi objective optimization to identify less intuitive but feasible and pareto optimal solutions analyzing risk and chance for the henry design problem brings substantially more context and insight first by allowing risk tolerant solutions risk less than 0 5 we see a wider range of possible extraction rates for example fig 4 k and w versus fig 3 e and n we also now see that the low ratio high quality water design mode is generally lower in reliability than the high ratio low quality water design mode fig 4 u and y this is likely because the low ratio design is sensitive to both the subsurface properties of the system e g hydraulic conductivity and porosity and to upgradient inflow quantities all of which were specified as uncertain inputs to the model not surprisingly we also see that optimal solutions with very high combined groundwater extractions rates are also associated with lower reliability values fig 4 f and v these solutions rely on a delicate balance between extraction rate and recharge rate which can be disrupted by the uncertain parameters allowing for landward migration of the freshwater saltwater interface ultimately the most reliable solutions use a seaward recharge basin fig 4 d and l with high ratios of recharge to extraction rate fig 4 u and y and relatively low extraction rate fig 4 f and v note only solutions that are three times or greater than the historical extraction rate are shown the salinity of the recharging water can increase with increasing seaward distance of the basin fig 4 k and w these important insights which are not available with risk neutral analyses highlight the importance of the cmou workflow that pestpp mou enables 5 discussion and conclusions we have presented the cmou workflow enabling tool pestpp mou pestpp mou implements several popular evolutionary algorithms and pairs these algorithms with several options for representing uncertainties in model derived constraints objectives we have designed pestpp mou to work in a downstream sense of the existing uncertainty and data assimilation tools in the pest and pest suites so that practitioners have more options to seamlessly transition from more traditional uncertainty analyses and data assimilation to cmou analyses ultimately leading to better model based decision support this aspect of the pestpp mou design paired with the wide spread use of the pest model interface and the fault tolerant parallel run manager embedded within pestpp mou makes this tool unique among others the ability to formulate and evaluate complex cmou problems in flexible and general ways allows pestpp mou to be deployed to a wide range of decision support analyses across the environmental modeling spectrum the choice of how to best couple the risk chance analyses with the constrained and or multi objective management optimization analyses is a complex subject matter and will almost certainly be application specific while practitioners may be tempted to err on the side of caution and re evaluate the interaction between parameter and decision variable relations at every individual for every generation the computational burden of this formulation is very high practitioners are instead encouraged to proceed in a stepwise testing fashion to evaluate the importance or otherwise of increased rigor in the coupling strategy and representation of risk pestpp mou provides for the first time a means to systematically evaluate these coupling strategies the object oriented and encapsulated design and implementation of the population generators and environmental selectors within pestpp mou allow for additional generators and selectors to be added with relative ease this is important as the field of multi objective optimization is the focus of much research and it is expected that more efficient formulations will continue to be developed disclaimer any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government software availability the software tool presented herein is available within the open source pest code suite available at https github com usgs pestpp as a c code named pestpp mou pre compiled statically linked e g stand alone binaries for windows linux and macos are available as are cmake make and visual studio solutions the analytical benchmark application and the density dependent groundwater flow application input files are available in the pestpp mou continuous integration testing repository at https github com pestpp pestpp mou benchmarks we note the continuous integration testing suite for pestpp mou includes many more benchmark problems including stochastic variants of many of the test problems presented in zitzler et al 2000 and deb et al 2002 the code repository also includes a user s manual that provides additional guidance declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank chris langevin for thoughtful comments related to the henry problem demonstration and zak stanko for some early discussions on the design of pestpp mou we also would like acknowledge eric morway and paul barlow for peer review partial funding for this work was provided by intera inc and gns strategic science investment fund ssif groundwater 
25650,an open source tool has been developed to facilitate constrained single and multi objective optimization under uncertainty cmou analyses the tool uses the well known pest interface protocols to communicate with the underlying forward simulation making it non intrusive the tool contains a built in parallel run manager to make use of heterogeneous and distributed computing resources several popular and well known evolutionary algorithms are implemented and can be combined with a range of approaches to represent uncertainty in model derived constraint objective values these attributes serve to address the current barrier to adopt advanced cmou analyses for a wide range of decision support problems across the environmental modeling spectrum we demonstrate the capabilities of the cmou tool on a well known analytical benchmark problem that we augmented to include uncertainty as well as on a synthetic density dependent coastal groundwater management benchmark problem both demonstrations highlight the importance of explicitly accounting for uncertainty to convey risk and reliability in pareto optimal design keywords resource management decision support optimization under uncertainty multi objective optimization non intrusive 1 introduction increasingly decision making for environmental resource management is informed by predictive models model based decision support regarding unseen future outcomes can be achieved in several ways including with predictive scenario analyses or through formal management optimization analyses among others formal management optimization analyses are sometimes favored as they can offer a more direct means of predictive decision support ahlfeld and mulligan 2000 maier et al 2014 gorelick and zheng 2015 danapour et al 2021 that is while predictive scenarios are typically focused on simulating the outcome of what if conditions related to unseen changes in forcings optimization analyses can extend the utility of predictive scenario analyses for example optimization analyses can be used to identify optimal resource management decisions that avoid unwanted outcomes while minimizing costs and or maximizing resource allocation optimization analyses can vary significantly in their formulation including factors such as dimensionality decision variables objectives and constraints global versus local search methods non linear versus linear problem formulations etc in many decision support settings how best to conceptualize and formulate an optimization problem is subjective and requires investigation and testing for example a constrained management optimization problem may be better represented as a constrained multi objective management optimization problem where one or more inequality constraints are treated as an objective that can be traded off with other objectives deb 2001 nouiri et al 2015 siade et al 2019 paulinski et al 2018 the interplay between constraints and objectives warrants efficient reformulating management optimization problems by treating one or more constraints as objectives and vice versa this leads to constrained multi objective optimization fonseca and fleming 1998 when models are used to guide environmental resource decision making the uncertainty associated with decision relevant simulated outcomes e g predictions forecasts constraints objectives etc must be expressed in many modeling settings particularly subsurface settings the inputs to the model are highly dimensional and highly uncertain and there are typically few data available to condition uncertain model inputs meaning that uncertainty in decision relevant simulated outputs can be significant moore and doherty 2006 herckenrath et al 2011 brakefield et al 2015 knowling et al 2020a the need to quantify uncertainty in decision relevant simulated outcomes as well as to flexibly represent modeling analyses as constrained multi objective optimization problems gives rise to a complex coupled analysis referred to herein as constrained multi objective optimization under uncertainty cmou several previous works have investigated various aspects of cmou in environmental modeling settings see for example singh and minsker 2008 luo et al 2016 singh and minsker 2004 mantoglou and kourakos 2007 rezaei et al 2020 these studies collectively a demonstrate the value that cmou brings to decision support b identify the broad range of settings that cmou may be applied to and c highlight the complexity and user requirements that implementing cmou is likely to entail pertinent to this last point is that to our knowledge there exists no generally applicable tool that facilitates cmou analyses in a non intrusive i e model independent and flexible way to support the wide range of environmental modeling settings that could benefit from cmou to enable practitioners the ability to realize the benefits of cmou analyses we have developed pestpp mou pestpp mou is an open source tool that pairs several existing and well known single objective and multi objective evolutionary algorithms with flexible approaches to represent uncertainties in model derived outputs that serve as constraints and or objectives in the optimization solution process pestpp mou has been designed to function in a non intrusive and flexible way using the well known pest model interface protocols so that it can be easily applied to a wide range of cmou problems that require different underlying simulation engines to make use of parallel computation pestpp mou includes a built in fault tolerant parallel run manager for heterogeneous and distributed computing systems although there are some already available software tools that implement certain forms of multi objective optimization see for example kwakkel 2017 matott 2005 and hadka and reed 2013 among others pestpp mou is unique in that it implements constrained single objective and multi objective optimization under uncertainty in a non intrusive and easy to deploy e g statically linked stand alone software tool 2 background and conceptual overview as discussed in white et al 2018 there is nuanced terminology associated with both uncertainty risk analysis and management optimization analysis particularly multi objective optimization analysis herein we define the following terms decision variable an input to the model that is controlled by management action the optimization process adjusts these quantities in seeking optimal solutions simple constraint an inequality quantity that is derived from combinations of decision variables because this quantity is derived purely from decision variable values it is not subject to uncertainty simple objective a quantity that is derived from combinations of decision variables that is treated as an objective to minimize or maximize because this quantity is derived purely from decision variable values it is not subject to uncertainty model derived constraint an inequality quantity that is at least partially dependent on model outputs because this quantity depends on simulation results it is subject to uncertainty in as much as it is sensitive to uncertain model inputs model derived objective a quantity that is at least partially dependent on model outputs and that is treated as an objective to minimize or maximize because this quantity depends on simulation results it is subject to uncertainty in as much as it is sensitive to uncertain model inputs parameter a quantity representing uncertain model inputs collectively the uncertainty in parameters may induce uncertainty in model derived constraints objectives individual a single set of decision variable values population a collection of individuals each of which has a value for each decision variable the population is evolved through each generation of the cmou solution algorithm towards an optimal set of solutions generation an iteration of the cmou algorithm that consists of generating a new candidate population evaluating the population by running each individual in the population through the model and evaluating the resulting feasibility and fitness of each individual across multiple generations the population evolves emulating the processes of natural selection feasible an optimization solution e g an individual that satisfies all constraints fitness the metric used to rank individuals in a population in the case of single objective optimization fitness is typically defined only by feasibility and objective function value in multi objective optimization fitness includes feasibility as well as pareto optimality dominance and diversity uniqueness elements generator an algorithmic process that emulates part of the evolutionary process by generating offspring from one or more parent individuals in the current population and selector an algorithmic process that emulated part of the evolutionary process by comparing all members in a population to determine fitness which can then be used by the generator to propagate desirable characteristics to the next generation 2 1 evolutionary meta heuristic multi objective optimization evolutionary algorithms are an important and exciting branch of research that have applications across the computational sciences e g michalewicz et al 2003 in the most general sense this class of search algorithms employs the darwinian concept of survival of the fittest or natural selection and uses characteristics and behaviors observed in nature to generate and evolve a population of individuals towards optimality herein we only briefly review the concepts related to multi objective optimization interested readers are referred to maier et al 2014 siade et al 2019 for more detailed information regarding evolutionary computation in the context of environmental resource management how optimality is defined is a critical aspect of any multi objective evolutionary algorithm and may include elements of feasibility diversity and pareto optimality pareto optimality is a central concept in multi objective optimization we define an individual i of a population as pareto optimal or pareto dominant compared to individual j if for all objectives i has an equal or better value compared to j and i has a strictly better value than j for at least one objective e g zitzler et al 2003 emmerich and deutz 2018 conceptually a solution is pareto optimal if for all objectives no single objective can be improved further without compromising the value of another objective the goal then of a multi objective optimization algorithm is to find a population of pareto optimal individuals collectively a population of pareto optimal individuals map the optimal trade off between non commensurate and competing objectives this trade off surface is referred to as the pareto frontier 2 2 chance constraints and objectives because model derived constraints objectives may depend at least partially on model outputs and because all models of environmental systems are subject to uncertainty it follows that these model derived quantities are also uncertain the extent to which they are uncertain depends on many factors including the system that is being modeled the model itself and the available information both prior estimates of the model inputs and historic state observations to be assimilated etc as well as many other factors the focus of predictive uncertainty analysis is to quantify uncertainties in unmeasured decision relevant model outputs and possibly also to reduce these uncertainties through data assimilation e g from prior to posterior uncertainty estimates within cmou analyses the uncertainties in the model derived constraints objectives are the same uncertainties that would arise if the model derived constraints objectives were instead considered predictions within an uncertainty data assimilation analysis herein to account for the range of plausible constraint objective values arising from model input uncertainty within the constrained multi objective optimization solution process we employ the concept of chance constraints and or chance objectives the chance in chance constraints objectives represents the combination of model derived constraint objective uncertainty with a specified risk value to collapse the constraint objective uncertainty to a single representative value the value of risk expresses the level of aversion to an unwanted outcome for example a risk value of 0 95 expresses a 95 chance of avoiding an unwanted outcome a risk averse high reliability scenario while a risk value of 0 05 expresses only a 5 chance of avoiding an unwanted outcome a risk tolerant low reliability scenario conceptually the desired risk value is combined with the uncertainty in model derived constraints objectives by shifting the simulated values toward or away from the desired outcomes along the range of possible values interested readers are referred to wagner and gorelick 1987 white et al 2018 2020 and the references cited therein for more background on the concept of chance constraints and risk 2 3 coupling in cmou analyses models are used to evaluate two fundamental relations a the relation between decision variables and model derived constraints objectives the decision variable relation and b the relation between uncertain parameters and model derived constraints objectives the parameter relation if changes in the decision variable values influence the parameter relation or changes in the parameters influence the decision variable relation then this interaction should be considered within the cmou analysis because it may affect the optimal risk based solution s herein we refer to the interaction between the uncertainty risk analysis which relies on the parameter relation and the management optimization analysis which relies on the decision variable relation within the larger cmou analysis as coupling lopez and beck 2012 hamzehkolaei et al 2016 if no coupling exists then the chance risk estimates are independent of the decision variable values meaning the chance risk estimates can be used across multiple generations of the cmou process and shared between individuals within a population the uncertainty estimates calculated at one point in decision variable space are valid across decision variable space however if a coupling exists then as the values of decision variables change across generations and or across decision variable space the relation between the uncertain parameters and model derived constraint objective values may need to be re evaluated for example in designing a groundwater contaminant transport treatment system as the treatment rate and location of extraction and injection wells changes as part of an optimal design search the relation between uncertain properties e g hydraulic conductivity porosity and the simulated concentration at compliance points e g constraints objectives will be affected because the flow field induced by differing treatment designs may activate the influence of previously insensitive uncertain parameters knowling et al 2020b given the problem specific nature and wide range of coupling possibilities we recognize three difference approaches to represent uncertainty in model derived constraints objectives 1 specified uncertainty approach specified 2 first order second moment fosm approach fosm based 3 stack based approach stack based these approaches imply different uncertainty conceptualizations as well as different representations of the coupling between the uncertainty risk analysis and the management optimization analysis within the larger cmou analysis flexibility in how uncertainty and the corresponding chances and risk are represented in the cmou analysis is a key consideration for undertaking cmou analyses because each approach represents a trade off between computational burden and rigorous representation of uncertainty in model derived constraints objectives we note that each of the following approaches can be employed as either prior or posterior formulations depending on the availability of state observations to condition parameters it is expected that in practice most applications of cmou will use posterior uncertainty estimates and that these posterior estimates will be derived from an upstream e g previously completed data assimilation and uncertainty analysis 2 3 1 specified approach the simplest and most computationally efficient uncertainty representation is the specified approach as the name implies this approach relies on directly specifying the uncertainty e g standard deviation of model derived constraints objectives with an assumed gaussian probability distribution with the mean of the distribution being the simulated value for the constraint objective by specifying constraint objective uncertainties no additional model evaluations are required in the cmou solution process however this approach is only appropriate if there is no expected coupling between the uncertainty risk analysis and the management optimization analysis the specified standard deviations would typically be obtained from a previous uncertainty data assimilation analysis parameters are not explicitly used in this formulation 2 3 2 fosm based approach a more robust approach for representing constraint objective uncertainty is with fosm analyses see e g white et al 2018 white et al 2016 doherty 2015 menke 1989 goldstein and wooff 2007 like the specified approach the fosm representation also implies the uncertainty in the model derived constraints objectives can be approximated by a gaussian distribution and the mean values for these implied gaussian distributions are the simulated values however the fosm approach involves calculation of these uncertainties via conditional covariance propagation from uncertain parameters to the model derived constraints objectives in this way users provide the uncertainties of the parameters via a statistical parameter distribution and the parameter uncertainties are propagated to the model derived constraints objectives on the fly during the cmou analysis the fosm approach requires a jacobian matrix of sensitivities of model derived constraints objectives to the uncertain parameters furthermore the fosm based approach allows for some accommodation of coupling between the risk chance analysis and the management optimization analysis since the jacobian matrix and the corresponding fosm based constraint objective uncertainties can be updated as the cmou analysis moves through generations and the region of focus in decision variable space changes however the cost of repeated evaluation of the jacobian matrix especially when the number of parameters is large is computationally expensive 2 3 3 stack based approach the most robust approach for representing uncertainties in the model derived constraints objectives is with stacks bayer et al 2008 in this representation an ensemble or stack of parameter realizations is used to represent model input uncertainty essentially using monte carlo approach to account for these uncertainties compared to the previous two approaches the stack based approach makes no assumption about the statistical distribution of the model derived constraint objective uncertainties that is the stack based approach is a non parametric representation of constraint objective uncertainties the stack based approach uses the model derived constraint objective cumulative distribution function arising from evaluating the parameter stack at a given point in decision variable space once the stack of parameter realizations is evaluated each model derived constraint objective has a stack of possible values this set of possible values represents the range of values that model derived constraints objectives may take 3 methods and implementation pestpp mou has been designed to implement a wide range of cmou analyses to support environmental resource decision making it is a statically linked c program that uses the well known pest model interface protocols doherty 2015 to communicate with the underlying simulation in a non intrusive and flexible way given that cmou analyses require many model forward evaluations and inevitably some of these evaluations may lead to forward model convergence failure or other model instability issues pestpp mou includes a built in fault tolerant parallel run manager the run manager uses tcp ip to communicate a lower level and more common network protocol than the message passing interface clarke et al 1994 which allows pestpp mou to be deployed on a wide range of heterogeneous computing clusters including ad hoc networks and in cloud settings see fienen and hunt 2015 for more information on this important feature the non intrusive design and flexible parallel run management capabilities of pestpp mou are key contributions of this work as these aspects enable cmou analyses for many environmental management settings 3 1 generators and environmental selectors the field of evolutionary multi objective optimization is rapidly evolving and several tools that allow practitioners and researchers to explore capabilities of these algorithms already exist e g durillo and nebro 2011 kwakkel 2017 hadka and reed 2013 rather than develop yet another entry in this already crowded field we have focused instead on developing a production level tool that implements several commonly used and previously vetted evolutionary algorithms with flexible and generic uncertainty risk evaluation in non intrusive way with built in fault tolerant parallel run management as presently implemented pestpp mou offers several algorithmic choices for evolutionary generation and selection all of which have displayed strong multi objective optimization performance in the literature e g reed and hadka 2014 durillo and nebro 2011 pestpp mou purposely separates the generator and selector essentially breaking apart the two primary evolutionary mechanisms so that users have greater flexibility on how to combine these two algorithmic elements to be clear the generator produces new individuals from the existing population these new individuals are then evaluated through the model and their fitness is calculated by the selector choices for evolutionary generators include the following differential evolution de storn and price 1997 including a self adaptive variant georgioudakis and plevris 2020 whereby the crossover and mutation rates are treated as decision variables particle swarm optimization pso kennedy and eberhart 1995 siade et al 2019 simulated binary crossover sbx deb and agrawal 1995 and highly disruptive polynomial mutation plm hamdan 2012 pestpp mou users can also employ combinations of these generators this is achieved by simply specifying more than one generator we have also included the reducing set concurrent simplex lewis et al 2006 simplex a version of the nelder mead simplex olsson and nelson 1975 lagarias et al 1998 with increased parallelism as a gradient based generator in pestpp mou as presently implemented pestpp mou uses the constrained nsga ii environmental selector proposed in deb et al 2002 the code also implements the unconstrained spea 2 environmental selector described in zitzler et al 2001 just as with the generators we recognize that much past and current research has been and is being devoted to studying constraint handling in multi objective evolutionary optimization e g kramer 2010 huang and wang 2021 mezura montes 2009 fonseca and fleming 1998 aguirre et al 2004 liu et al 2021 as such we have relied on previously published approaches that have demonstrated success in constraint handling in applied environmental modeling studies to guide the current implementation in pestpp mou e g majedi et al 2021 fan et al 2020 mirzaie nodoushan et al 2017 naghdi et al 2021 raei et al 2017 we note that if a single objective is specified pestpp mou seamlessly functions as a constrained single objective evolutionary optimization tool and the same uncertainty risk functionality is still available for the single objective and any constraints that are included provided these quantities are model derived this seamless flexibility allows users to efficiently move between constraints and objectives to evaluate different optimization problem formulations 3 2 chance constraints objectives pestpp mou offers the three previously described approaches to represent model derived constraint objective uncertainty specified fosm based or stack based pestpp mou can automatically draw the parameter stack for the listed parameters using either basic initialization information e g initial values and parameter bounds by assuming the parameters are statistically independent in the case that pestpp mou is being used downstream from a data assimilation uncertainty analysis users can provide a posterior parameter covariance matrix e g doherty 2015 white et al 2016 white 2018 for stack generation and or for fosm uncertainty representation users can also supply a previously generated stack such as a parameter ensemble derived from a posterior uncertainty analysis pestpp mou also automates the process of populating and evaluating the jacobian matrix needed in the fosm based approach through shared file types related to the chance process pestpp mou provides a seamless transition from uncertainty data assimilation analyses undertaken with other tools in the pest and pest suites to cmou analyses we note that the entire risk uncertainty process is not required to apply pestpp mou and is only activated by specifying a risk value not equal to 0 5 pestpp mou includes two options for where in decision variable space to evaluate stack based chances through the chance points option namely at a single representative point single a computationally cheap but less rigorous option or to evaluate the stack at all individuals in a population all a computationally expensive but more rigorous option following the concepts presented in deb et al 2007 pestpp mou also provides the functionality to treat risk as an objective to be maximized i e driven towards a risk averse high reliability stance in this way if decision makers are unsure what risk stance is appropriate a priori or users are unsure what maximum value of risk may lead to infeasibility then specifying risk as a formal objective alleviates the requirement to specify a single risk value note however this option will likely lead to an increased computational burden as additional generations will likely be required to accommodate an additional objective 3 3 coupling approaches how and when the chance risk estimates should be updated in the course of a cmou analysis is likely to be problem specific given this pestpp mou strives to provide users with flexibility to accommodate a range of coupling approaches users can choose to re use or update the risk uncertainty estimates for a given generation flexibly with pestpp mou to lower the computational burden of applying pestpp mou for the fosm based approach only the second moment is used so this implies constraint objective uncertainty estimates can be broadcast across the population by simply applying the fosm estimated uncertainties in model derived constraints objectives to each individual s simulated values for these quantities to update the fosm estimates the jacobian matrix must be re evaluated which when requested is completed using the decision variable values nearest the current generation mean decision variable values in a euclidean distance sense in the case of a strong coupling within the cmou analysis using stack based uncertainty representation the stack can be evaluated at each individual in the population or if a weaker coupling is present the stack can be evaluated at a single representative individual point in decision variable space and broadcast to the remaining individuals as an option to reduce the computational burden the former requires many repeated stack evaluations with a complete stack evaluation required at each individual in the current population number of model evaluations equals the number of realizations in the stack times the number of individuals in the population while the latter requires a single stack evaluation the number of model evaluations equals the number of realizations in the stack as such the implementation of flexible coupling schemes with the stack based approach requires additional consideration to re use stack based uncertainty risk estimates across multiple generations pestpp mou maps each current individual solution to the nearest stack evaluation point in decision variable space in a minimum euclidean distance sense the nearest stack results are then translated to each corresponding current population individual by differencing the mean value of the stack results with the simulation results of the current individual in this way the available stack results nearest each individual are used to represent constraint objective uncertainty at that individual without requiring a re evaluation of the stack again reducing the computational burden 4 example applications we demonstrate the functionality of pestpp mou through two example applications the first is a well known analytical constrained multi objective benchmark problem that we augmented in order to transform this problem into chance constrained chance objective problem this problem is used to benchmark pestpp mou facilitate comparisons between the performance of different evolutionary generators as well as demonstrate how chances and risk affect the shape and orientation of the constrained pareto frontier in objective space the second example application is a variant of a popular saltwater intrusion benchmark problem developed for the purpose of designing a hypothetical artificial recharge basin to maintain viability of coastal groundwater extraction wells this problem is used to demonstrate decision support cmou capabilities that pestpp mou enables 4 1 benchmark problem the analytical constrained multi objective problem that we used for benchmarking is known as constr deb 2001 deb et al 2002 the constr problem is formally defined as follows minimize f 1 x x 1 θ 1 f 2 x 1 x 2 x 1 θ 2 subject to x 1 0 1 1 0 x 2 0 5 and g 1 x x 2 9x 1 6 g 2 x x 2 9x 1 1 where θ 1 and θ 2 are additive uncertain parameters as shown on fig 1 the pareto frontier between f 1 and f 2 red dashed is truncated by the edge of the feasible region shaded region as a result the optimal pareto frontier is partially defined by the trade off between f 1 and f 2 and partially defined by constraints we modified the constr problem to include two uncertain parameters θ 1 and θ 2 whose action is additive for each of the two objectives f 1 and f 2 respectively this introduces uncertainty into the calculated f 1 and f 2 objective values to be clear this additive uncertainty results in no interaction between the risk uncertainty analysis and the optimization analysis so we use this uncoupled problem to verify the behavior of the various uncertainty representations and coupling strategies for this problem we conducted several experiments specified risk values of 0 05 0 5 and 0 95 using a specified uncertainty representation 100 individual population and 50 generations risk as an objective using a specified uncertainty representation 100 individual population and 150 generations risk as an objective using a stack based uncertainty representation evaluating the stack at a single chance point and reusing this single stack across all generations 100 individual population and 150 generations and risk as an objective using a stack based uncertainty representation evaluating the stack at all individuals in the initial population and reusing these stack evaluations across all generations 100 individual population and 150 generations the specified risk analyses represent a risk tolerant 0 05 risk neutral 0 5 and risk averse 0 95 stance respectively treating risk as an objective will likely increase the complexity of the cmou analyses so the number of generations was increased to 150 making the results of the risk as an objective analyses comparable to the combined results of the three specified risk analyses with respect to computational budget 4 1 1 results the application of pestpp mou to the modified constr benchmark verifies certain aspects of the cmou solution process encoded in pestpp mou and also demonstrates some of the more advanced capabilities of pestpp mou fig 1 a and b show that for the risk neutral case risk value of 0 5 the algorithm encoded in pestpp mou finds the correct pareto frontier with either the de or pso generator furthermore the risk averse and risk tolerance solutions fig 1 a through f are located in objective function space as expected that is they are shifted in the appropriate direction with respect to the risk neutral pareto frontier the risk tolerant solutions occupying a larger feasibility region while the risk averse solutions occupy a smaller feasible region even in this analytical benchmark problem with simple additive objective uncertainty the importance of accounting for uncertainty and ultimately risk is demonstrated by the change in pareto optimal solutions as a function of risk the de generator appears to have found more pareto optimal solutions than the pso generator for the specified risk analyses the de pareto solution set point are more dense however we caution readers not to draw general conclusions regarding the relative performance of de and pso from these analyses because it is well known that there is no single algorithm e g generator selector etc that is optimal for all possible optimization problems the use of the risk as an objective option available in pestpp mou is shown to perform as expected specifically the results of the risk as an objective approach fig 1 c d e f appear to correspond appropriately to explicit risk averse and risk tolerant pareto frontiers fig 1 a and b additionally given that this problem was designed to have no coupling between the risk uncertainty analysis and the optimization analysis we expect the outcome of varying the uncertainty representation and coupling strategies to yield similar results verifying certain aspects of pestpp mou functionality for example the results of specified uncertainty approach fig 1 c and d appear very similar to the stack based results fig 1 e and f 4 2 henry problem to demonstrate the risk based decision support capabilities of cmou via pestpp mou in the context of coastal groundwater management we evaluate a modified form of the cross section henry saltwater intrusion problem henry 1964 the henry problem is patterned after the coastal aquifer conditions in south florida while its spatial and temporal dimensions are representative of a laboratory scale experiment the henry problem is well known e g herckenrath et al 2011 simpson and clement 2004 zidane et al 2012 holzbecher 2016 croucher and o sullivan 1995 and provides a computationally efficient test problem for density dependent groundwater flow that captures many relevant aspects of coastal saltwater intrusion phenomena the density dependent characteristics of the henry problem combined with non linear relations between model inputs and model outputs makes the henry problem an illustrative benchmark for evaluating the cmou algorithm encoded in pestpp mou starting with the henry model design available with modflow 6 hughes et al 2017 langevin et al 2017 see the modflow 6 software release langevin et al 2021 for details we endeavored to design an artificial recharge basin to mitigate landward migration of the fresh water saltwater interface under increased groundwater extraction for water supply the model used herein has the following details 160 columns and 40 layers of 0 025 m spacing columns are numbered starting farthest from the coastal boundary condition fig 2 note the water table is at or near 1 0 m in elevation three vertically aligned groundwater extraction wells located at column 60 in layers 10 20 and 30 fig 2 upgradient specified inflow boundary condition in column 1 and all layers 0 071 275 m 3 d per layer e g simpson and clement 2004 with a concentration of 0 0 g l salinity and general head boundary condition specified in column 160 and all layers to represent a coastal boundary with a stage of 1 0 m and concentration of 35 0 g l salinity three distinct periods of time were simulated pre development period of 2 5 days to establish a stable freshwater saltwater interface position without any groundwater extraction historical groundwater use period of 0 3 day which induces landward movement of the freshwater saltwater interface towards the groundwater extraction wells fig 2 and a predictive period of 2 5 days where the artificial recharge basin is in operation while groundwater extraction continues or increases depending on the selected decision variable values within this simulation we want to design a single artificial recharge basin located on the surface layer 1 between the coastal boundary and the upgradient inflow boundary to help maintain viability of the extraction wells to produce potable water e g salinity less than or equal to 0 5 g l the following decision variables subject to optimization were defined location of the left upgradient edge of the recharge basin column number from the left edge of the model domain minimum 1 maximum 140 note the width of the recharge basin was fixed at 10 model cells wide the rate of artificial recharge applied to the basin m d minimum 0 001 maximum 12 0 the concentration of recharge applied to the basin g l minimum 0 5 maximum 17 0 and the extraction rate of each of the three groundwater extraction wells m 3 d minimum 0 0 maximum 3 0 the objectives of this paired recharge basin groundwater extraction design cmou analysis are maximize the artificial recharge basin distance from the upgradient inflow prefer a more coastal location for the artificial recharge basin minimize the artificial recharge rate applied to the basin prefer to apply less artificial recharge maximize the concentration of artificial recharge applied to the basin prefer to apply low quality water and maximize the combined groundwater extracted from the three extraction wells the problem is subject to the following constraints the simulated combined flux weighted mean salinity of the three extraction wells is less than 0 5 g l e g potable water the simulated maximum water level within the recharge basin is less then 1 025 m 0 025 m allowable increase in water table elevation resulting from the recharge basin and the combined flux from the three extraction wells is greater than 0 7 m 3 d the combined historical extraction rate uncertainty arises in this optimization problem formulation through the concentration and groundwater level constraints because the model is used to evaluate the concentration at the extraction wells and groundwater level within the artificial recharge basin for a given set of decision variables these simulated concentrations and groundwater levels are uncertain inasmuch as the uncertain model inputs influence these simulated outputs we defined the following parameters to represent model input uncertainty hydraulic conductivity in every active model cell m d mean 864 minimum 86 4 maximum 8640 0 porosity in every active model cell dimensionless mean 0 35 minimum 0 3 maximum 0 4 and upgradient inflow column 1 every layer m 3 d mean 0 071 275 minimum 0 064 147 5 maximum 0 784 025 a prior uncertainty stance was used in this demonstration correlation in the spatially distributed parameter types was defined using an exponential variogram with a range of 0 1 m and sill proportional to the parameter bound range a stack of 100 parameter realizations was used to represent parameter uncertainty in the pestpp mou analyses the coupling between the risk chance estimates and the optimization problem was defined such that the stack was evaluated at every individual of the first generation and then reused for all subsequent generations two pestpp mou analyses were completed with the modified henry model artificial recharge design problem first a risk neutral analysis was completed to better understand possible feasible artificial recharge basin designs and trade offs assuming the model derived constraints are subject to no uncertainty the risk neutral analysis used 100 individuals and was evolved across 100 generations a second pestpp mou analysis was completed with risk as an objective using 100 individuals and 500 generations the 100 realization parameter stack was used to represent model input uncertainty in this analysis the henry based optimization analysis used the de generator and the constrained nsga ii environmental selector note the optimization problem occurs solely on the predictive period however to include the stack based chances and to ensure these chance estimates are free from temporal artifacts all simulation periods were evaluated for each model run the pestpp mou input datasets for this analysis were generated programmatically using pyemu white et al 2016 using the pstfrom automation process white et al 2021 4 2 1 results several interesting insights are gained by inspecting risk neutral outcomes fig 3 first we note that through the use of an artificial recharge basin combined with formal optimization substantially larger quantities of potable water can be produced compared to the more naive production strategy used during the historical period a strategy that led to landward migration of the freshwater saltwater interface only solutions that exceed the naive production are shown on fig 3 among the most interesting insights gained from the risk neutral analysis is the pareto optimal relation between combined groundwater extraction rate and artificial recharge rate fig 3 m and p there is an apparent bimodal relation between these two important objectives seen as two distinct lobes of optimal solutions one mode involves an approximately low ratio of recharge to extraction and uses relatively low salinity water whereas the other mode involves a much higher ratio of recharge to extraction and uses high salinity water fig 3 a the physical location aspect of these modes is key to understanding the low ratio high quality water mode includes artificial recharge basins located landward of the extraction wells fig 3 a essentially augmenting fresh upgradient inflows with high quality low salinity water to mitigate landward movement of the fresh water saltwater interface fig 3 c f the high ratio low quality water mode includes recharge basins that are located seaward of the extraction wells where large volumes relative to the combined extraction rate of low quality high salinity water are injected to vertically depress the freshwater saltwater interface and establish a hydraulic divide preventing high salinity seawater from moving landward in all but the deepest parts of the simulated aquifer these solutions generally involve shifting extraction to the shallowest extraction well located in layer 10 this dual mode behavior highlights the capabilities of constrained evolutionary multi objective optimization to identify less intuitive but feasible and pareto optimal solutions analyzing risk and chance for the henry design problem brings substantially more context and insight first by allowing risk tolerant solutions risk less than 0 5 we see a wider range of possible extraction rates for example fig 4 k and w versus fig 3 e and n we also now see that the low ratio high quality water design mode is generally lower in reliability than the high ratio low quality water design mode fig 4 u and y this is likely because the low ratio design is sensitive to both the subsurface properties of the system e g hydraulic conductivity and porosity and to upgradient inflow quantities all of which were specified as uncertain inputs to the model not surprisingly we also see that optimal solutions with very high combined groundwater extractions rates are also associated with lower reliability values fig 4 f and v these solutions rely on a delicate balance between extraction rate and recharge rate which can be disrupted by the uncertain parameters allowing for landward migration of the freshwater saltwater interface ultimately the most reliable solutions use a seaward recharge basin fig 4 d and l with high ratios of recharge to extraction rate fig 4 u and y and relatively low extraction rate fig 4 f and v note only solutions that are three times or greater than the historical extraction rate are shown the salinity of the recharging water can increase with increasing seaward distance of the basin fig 4 k and w these important insights which are not available with risk neutral analyses highlight the importance of the cmou workflow that pestpp mou enables 5 discussion and conclusions we have presented the cmou workflow enabling tool pestpp mou pestpp mou implements several popular evolutionary algorithms and pairs these algorithms with several options for representing uncertainties in model derived constraints objectives we have designed pestpp mou to work in a downstream sense of the existing uncertainty and data assimilation tools in the pest and pest suites so that practitioners have more options to seamlessly transition from more traditional uncertainty analyses and data assimilation to cmou analyses ultimately leading to better model based decision support this aspect of the pestpp mou design paired with the wide spread use of the pest model interface and the fault tolerant parallel run manager embedded within pestpp mou makes this tool unique among others the ability to formulate and evaluate complex cmou problems in flexible and general ways allows pestpp mou to be deployed to a wide range of decision support analyses across the environmental modeling spectrum the choice of how to best couple the risk chance analyses with the constrained and or multi objective management optimization analyses is a complex subject matter and will almost certainly be application specific while practitioners may be tempted to err on the side of caution and re evaluate the interaction between parameter and decision variable relations at every individual for every generation the computational burden of this formulation is very high practitioners are instead encouraged to proceed in a stepwise testing fashion to evaluate the importance or otherwise of increased rigor in the coupling strategy and representation of risk pestpp mou provides for the first time a means to systematically evaluate these coupling strategies the object oriented and encapsulated design and implementation of the population generators and environmental selectors within pestpp mou allow for additional generators and selectors to be added with relative ease this is important as the field of multi objective optimization is the focus of much research and it is expected that more efficient formulations will continue to be developed disclaimer any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government software availability the software tool presented herein is available within the open source pest code suite available at https github com usgs pestpp as a c code named pestpp mou pre compiled statically linked e g stand alone binaries for windows linux and macos are available as are cmake make and visual studio solutions the analytical benchmark application and the density dependent groundwater flow application input files are available in the pestpp mou continuous integration testing repository at https github com pestpp pestpp mou benchmarks we note the continuous integration testing suite for pestpp mou includes many more benchmark problems including stochastic variants of many of the test problems presented in zitzler et al 2000 and deb et al 2002 the code repository also includes a user s manual that provides additional guidance declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank chris langevin for thoughtful comments related to the henry problem demonstration and zak stanko for some early discussions on the design of pestpp mou we also would like acknowledge eric morway and paul barlow for peer review partial funding for this work was provided by intera inc and gns strategic science investment fund ssif groundwater 
25651,the flow routing module in several large scale erosion and sediment transport models generally adopts simplifications that ignore important physical aspects of river dynamics in this study we compare for the first time the results generated by a large scale erosion and sediment transport model using two different flow routing approaches a combined saint venant and muskingum cunge method and the inertial flow routing for this assessment simulations are performed with the mgb sed model we evaluate sediment characteristics such as suspended sediment load and concentration floodplain effects and the flux of suspended sediment within the madeira river basin the simulation with the inertial approach presents satisfactory results similar to those achieved with the combined saint venant approach for lowland rivers and the muskingum cunge method for steeper rivers also inertial module is capable to represent the complex processes of the basin but with easier preprocessing and numerical features thus promising expanded capabilities for large scale simulations keywords hydrosedimentological modeling large basins madeira river 1 introduction on a global scale tropical basins are the main sources of suspended sediments to the oceans filizola and guyot 2011 milliman and farnsworth 2011 sediment transport from the rivers to the ocean plays an important role in the global geochemical cycle beusen et al 2005 naipal et al 2018 van oost et al 2007 south america which possesses a drainage area of 17 8 106 km2 12 of the global terrestrial surface transfers to the ocean a large amount of suspended sediment 2446 mt year syvitski et al 2005 the three major rivers of south america are the amazon orinoco and paraná which together transport 24 of the global water flux and 13 of the global load of suspended sediment to the ocean restrepo et al 2006 nearly 50 of the sediment load transported by the amazon river amounting to approximately 240 000 tons of sediment per year originates from the madeira river the largest amazon tributary in terms of sediment load fagundes et al 2021 filizola and guyot 2011 guyot 1993 laraque et al 2005 vauchel et al 2017 anthropogenic changes in the environment new datasets and methods make it important to continuously evaluate suspended sediments due to its importance for geomorphological and ecological issues understanding the fluvial sediment transport processes that occur in large scale basins has great research value for the sustainable development of local and global ecosystems li et al 2020 and knowledge of how sediment behaves within a watershed is important to natural resource management gilbert and wilcox 2020 however scarcity of sediment data remains a problem for many locations in the world mouyen et al 2018 even in the era of big data and big science best 2019 fewer than 10 of global rivers are being monitored near their discharge to the ocean syvitski et al 2005 to assist researchers in dealing with data scarcity numerical models have become fundamental tools for understanding past and future sediment trends cohen et al 2014 erosion and sediment transport models differ widely in complexity input requirements represented processes the scale of usage and types of output information provided merritt et al 2003 the measurement and control of processes related to the erosion and transport of sediments in large basins become more complex as the area of the basin increases and the interactions between different erosive and depositional processes along with their impacts become more numerous de vente et al 2007 thus on large scales distributed or semi distributed models are preferred because they have a greater capacity to represent the spatial variability of the modeled processes and the heterogeneity of the basin these models are often composed of a hydrological and a sediment module which together consider soil erosion and river sediment transport the hydrological module is a key component as hydrological input data significantly affects the results of an overall model of sediment transport some examples are swat arnold et al 1998 swat g eckhardt et al 2002 swim krysanova et al 1996 sednet prosser et al 2001 wasa sed mueller et al 2010 lascam sivapalan et al 1996 viney and sivapalan 1999 and mgb sed buarque 2015 fagundes et al 2019 2020 2021 however compared to the more rapidly advancing area of large scale hydrological and hydrodynamic modeling the field of large scale sediment modeling still faces many barriers fagundes et al 2021 especially in the areas of spatially temporally distributed and continuous time step modeling the bqart model syvitski and milliman 2007 provides a mechanism to quantify the influences that control sediment flux to the coastal zones but does not provide details about erosion and sediment transport occurring in the inner part of the watershed the wbmsed cohen et al 2013 is a global scale sediment model that estimates suspended sediment flow at a daily timescale and its updated version cohen et al 2014 was validated in several locations including the madeira river however the model is focused on estimating long term average using a simplified flow routing method simplified flow routing methods such as the muskingum cunge mc may not take into account certain important physical aspects of river dynamics at large scales e g backwater effects floodplains non dendritic river networks neal et al 2012 paiva et al 2013a floodplains have strong impacts on streamflow and sediment dynamics and provide connections between habitat areas sediment retention biodiversity and nutrient cycling hupp et al 2009 mcintyre and thorne 2013 the floodplain environment typically has lower energy and sediment aggradation occurs through diffusion and convection processes over time pizzuto 1987 also floodplains are complex environments formed by several fluvial processes that depend on the channel morphology stream power sediment properties and hydrology among other things lauer and parker 2008 nanson and croke 1992 due to these characteristics many authors have reported that floodplains despite their importance are problematic areas for large scale modeling oki et al 1999 pedinotti et al 2012 tshimanga and hughes 2014 yamazaki et al 2011 it implies that representing more complex sediment phenomena in large rivers remains a challenge hydrodynamic hd approaches may use the full saint venant equations which have a complex formulation and require a large amount of data and computational effort pontes et al 2015 an alternative method of intermediate complexity is inertial flow routing almeida and bates 2013 bates et al 2010 yamazaki et al 2013 this approach can be used in regions with low slope rivers vast floodplains reservoirs and backwater effects it can provide similar results to models that use the full solution of the saint venant equations with a simpler algorithm fan et al 2014 a recent study by hatono and yoshimura 2020 presented a global sediment dynamics model that considers suspended sediment and bedload at short timescales the authors validated suspended sediment from four observation stations in the amazon river basin and in several locations around the world the model used the local inertial equation proposed by bates et al 2010 to calculate river discharge the results showed that the representation of individual rivers was satisfactory but that the global sum of suspended sediment flow was largely underestimated fagundes et al 2021 developed and evaluated the performance of a sediment erosion and transport model mgb sed as for the entire south america domain they coupled the sediment module from mgb sed model buarque 2015 to the hydrologic hydrodynamic model developed by siqueira et al 2018 for the whole south america fagundes et al 2021 evaluate the coupled model results against daily in situ data from 595 stations information from regional studies and from the global model wbmsed cohen et al 2014 these efforts represent great advances in sediment modeling studies improving the comprehension and comparison of spatial and temporal dynamics however these works do not clarify exactly how the sediment modeling and routing can be affected by different hydrodynamic approaches this paper aims to help fill this gap as we present and evaluate two flow routing methods applied on a large scale erosion and sediment transport model we present comparisons of results obtained with the inertial in approach to those acquired with a mixed flow routing method hd mc in the madeira river basin considered as a geographically and geologically complex basin for its features to the best of the authors knowledge this is the first time these approaches have been compared 2 methods 2 1 study area the madeira river basin is the most important southern component of the amazon basin guyot et al 1999 and it covers approximately 20 of the amazon basin having a surface area of 1 4 106 km2 and encompassing bolivia 51 brazil 42 and peru 7 fig 1 the madeira river is the longest southern tributary of the amazon basin contributing 15 of the total water discharge from the amazon river to the atlantic ocean goulding et al 2003 the basin has a complex conformation and drains three important geomorphological structures the andes the brazilian shield and the amazonian plains the basin encompasses high altitudes in upstream regions large floodplains in flatter areas and waterfalls within the brazilian shield such peculiarities make the madeira river a tributary of great geographical importance guyot et al 1999 vieira et al 2018 the annual mean concentration of suspended sediments in the madeira river is 360 mg l 1 at porto velho rivera et al 2019 this high value is typical of tropical rivers known as whitewater rivers fassoni andrade and paiva 2019 filizola et al 2009 ribeiro neto et al 2006 the andes mountains are the most significant source of sediments for the madeira river due to their characteristic steep slopes and the nature of their igneous rocks and sedimentary coverage which are highly susceptible to weathering in turn promoted by local tectonism low vegetal coverage and high precipitation levels sediment production in the region is controlled by massive erosion processes filizola and guyot 2011 and sediment transport occurs mainly via two tributaries of the madeira river the madre de dios and mamoré rivers bernini et al 2016 2 2 the mgb sed model the mgb sed model buarque 2015 is a coupling of an erosion and sediment transport module to the large scale mgb hydrological model collischonn et al 2007 paiva et al 2011 which is a semi distributed conceptual model with daily time steps and unit catchments discretization that utilizes the concept of hydrologic response units hru the normal river segmentation method considered by the mgb model defines each river reach as a drainage segment located between two successive confluences or between the beginning of a river stretch and the first confluence downstream a unit catchment is then defined as the area that contributes directly to this river stretch the mgb model collischonn et al 2007 was initially developed using the muskingum cunge mc flow routing method later paiva et al 2011 developed a version that utilized the full solution of the saint venant equations focusing on representing more complex environments such as the amazon basin floodplains however this version of the mgb model was used only in the amazon basin buarque 2015 föeger et al 2018 paiva et al 2011 ribeiro neto et al 2006 since its flow routing requires extensive preprocessing steps to generate input data making it a complex choice for modeling this version allows for the a priori selection of flow routing method by river section the method selected may be the muskingun cunge method mc the full hydrodynamic flow routing method hd or a mixed flow routing method hd mc that employs the hd method in steep reaches of the main rivers and the mc method in the headwater regions of the drainage network paiva et al 2011 the most recent version of the mgb model has been updated to reduce its cumbersomeness and improve the model s capacity to simulate large river basins with extensive floodplains and backwater effects the updated version by pontes et al 2017 replaces the hd flow routing with an inertial method in this version of the mgb model has been applied in many basins such as the araguaia pontes et al 2017 congo paris et al 2017 and niger fleischmann et al 2018 as well as to the south american continent as a whole siqueira et al 2018 the in flow routing method used in the mgb hydrologic model has been shown capable of representing complex hydraulic processes in lowland areas pontes et al 2017 it has less computational effort than the prior version of the mgb model described by paiva et al 2011 which uses the full saint venant equations this latest version also has greater ease of applicability than the paiva et al 2011 version due to its provision of tools iph hydro tools to perform data preprocessing siqueira et al 2016 the simulated hydrological processes of the mgb model include interception and soil infiltration energy budget and evapotranspiration using the penman monteith approach shuttleworth 1993 soil water budget using bucket model surface runoff based on the variable contributing area concept and subsurface and groundwater flow generation the water volumes generated in each hru are propagated to the stream network using three linear reservoirs representing surface subsurface and groundwater flows in the inertial approach the dynamic equation from the saint venant equations equations 1 and 2 is simplified disregarding the second term of the left side of the equation pontes et al 2017 known as the advective inertial term the module assumes a rectangular river cross section and employs a discretization approach to compute discharge using the numerical approximation proposed by bates et al 2010 1 a t q x 0 2 q t i q 2 a x ii g a h x iii g a s o iv g a s f v where i is the local inertia term ii is the advective inertia term iii is the pressure differential term iv is the bed slope term and v is the friction term q is the flow m3 s 1 a is the cross sectional area of the flow m2 x is the longitudinal distance m t is the time s q is the flow per unit width m2 s 1 h is the river depth m s 0 is the bottom slope m m 1 g is the gravitational acceleration m s 2 and s f is the water surface slope m m 1 estimated using the manning equation and the assumption that river width b is larger than river depth h which approximates the hydraulic radius to the depth according to 3 s f n 2 q 0 q 0 h f l o w 10 3 where h f l o w m refers to the mean depth of the catchment river and q 0 m3 s 1 m 1 is the ratio between the catchment flow and the channel width buarque 2015 coupled to the mgb model paiva et al 2011 a module that estimates the transport and redistribution of sediments creating the mgb sed despite the mgb sed having already been applied in several basins with good results buarque 2015 fagundes et al 2020 limitations to its application in large basins with complex hydrodynamic processes remains because the use of the hd flow routing method is only available for the amazon basin the mgb sed is divided into three components basin river and floodplain the basin component estimates the amount of sediment generated in each catchment and models its transport to the rivers in the representation of the basin the mgb sed uses unit catchments which are subdivided into hrus in which the sediment routing is performed using linear reservoirs combined with a lag time coefficient the modified universal soil loss equation musle williams 1975 is used to estimate the soil loss in every unit catchment 4 s e d α q s u r q p e a k a β k c p l s f g where s e d t is the resulting sediment load of the soil erosion q s u r mm ha 1 is the runoff surface volume q p e a k m³ s 1 is the peak superficial flow rate a ha is the superficial area k 0 013 t m2 h m³ t cm 1 is the soil erodibility factor c is the coverage and management factor p is the conservationist practices factor l s is the topographic factor and f g is the coarse fragment factor the coefficients α and β used in the mgb sed were originally estimated by williams 1975 having values of 11 8 and 0 56 respectively they are considered as coefficients that can be calibrated following fagundes et al 2019 sediment time series graphs are amplified or reduced proportional to the variation of parameter α while parameter β intensifies the peaks and valleys as its value decreases the sediment load estimated in each catchment is divided into three sediment fractions silt clay and sand defined by the mean texture of the soil classes of each hru since not all the sediment generated during a model time step daily reaches the river at the same time the model uses a linear reservoir for each soil fraction to delay the sediment volume transferred to the river buarque 2015 thus the total sediment volume generated in each hru of a catchment is stored in a reservoir and the total solid discharge q s t s 1 at the reservoir outlet is computed as a linear function of its stored load and the retardation time t k s s for the linear reservoir the solid discharge from each hru transferred to the catchment river is estimated using 5 q s i j t 1 t k s i v s e d i j t as suggested by fagundes et al 2019 the tks parameter corresponds to the lag time associated with the reservoir and is considered as one calibratable parameter of the sediment module proposed in this study the lower the adopted value for the tks the more intense the peaks and valleys of sediment time series graphs the parameter also directly influences the displacement of sediment graphs for the present work the calibration of the α β and tks parameters was not performed so that the results of inertial propagation and hydrodynamics results could be compared according to the criteria adopted by buarque 2015 the sediment contribution to the rivers occurs with the use of a simple linear reservoir for each soil fraction considered silt sand or clay the sediment volume is subject to a delay reflecting the fact that not all sediments within the generated volume necessarily reach the river simultaneously buarque 2015 the sediment transport module utilizes the results of flow from the flow routing modeling and the sediment contribution from the unit catchments to perform the sediment routing along the rivers considering both erosion and deposition phenomena the sediment transport at the river is performed using a diffusion advection equation for fine particles clay and silt these particles do not undergo deposition or erosion within the river channel and are considered to comprise the suspension load the model does not consider the resuspension of deposited sediments the mgb sed allows for the lateral exchange of sediments between the rivers and their respective floodplains where the deposition of these fractions can be estimated although as stated the model does not consider the resuspension phenomenon in addition the floodplain module allows for the reduction and delay of sediment concentration peaks in the channel buarque 2015 when the variation in level between time steps is positive it indicates elevation and water exchange from the river to the floodplain if the value is negative the opposite occurs with the water exchange from the floodplain to the river in both cases the volume is transferred at the direction of the flow considering the concentrations at the source when the flow leaves the river to the plain the transferred volume will depend on the sediment concentration in the river and in the opposite direction the transferred volume will depend on the concentration remaining in the plain considering silt and clay deposition in it the exner equation is used to compute the transport of the sediment sand fraction considered as bedload bedload erosion and deposition are calculated as a function of the sediment transport capacity using the yang equation yang 1979 the requirement of relatively few input parameters can be considered one of the biggest advantages of the mgb sed the interaction of the model and a geographic information system gis environment as well as the simple methodology for estimating the spatial and temporal distribution of sediments coupled to the mgb model allows for an easier estimation of sediment loads and concentration even for a large basin 2 3 datasets datasets and inputs for the hydrological and sediment simulation were the same as those adopted by buarque 2015 the hydrological data used in the mgb model are precipitation and flow data in this study the precipitation data was from the satellite trmm 3b42 huffman et al 2007 trmm 3b42 data is available from 1998 with a spatial resolution of 0 25 0 25 many studies have used this data to perform hydrological simulations buarque et al 2011 clarke et al 2011 collischonn et al 2008 paiva et al 2011 pontes et al 2017 soares et al 2016 the flow data was from the brazil national water agency ana streamflow gauge station the selected gauges contain daily data from 1985 to 2005 and at least 80 of monthly data is without gaps a month without gaps signifying a maximum of 5 days without information the total number of stations selected was 25 as listed in table 1 of the supplementary material section the meteorological data consists of air temperature atmospheric pressure solar radiation relative humidity and wind speed information the data used is a combination of results from climate models and information observed at meteorological gauge stations this data was selected because of the lack of precipitation monitoring data in the madeira river basin especially on the non brazilian territory the data belongs to the national centers for environmental prediction national center for atmospheric research ncep ncar as described by kalnay et al 1996 the hrus were used as presented by buarque 2015 the sediment data used for this work belongs to the sediment gauge stations of the observation service so hybam and consists of suspended sediment concentration data three gauging stations were selected based on the available data 1998 2005 the gauging stations are presented in table 1 and fig 2 2 4 sediment modeling erosion and sediment transport were simulated with daily time steps the parameters of musle considered were those adopted by buarque 2015 table 2 to establish a direct comparison between the results of the two versions of mgb sed as there is a lack of detailed and spatially distributed information about soil conservation measures mainly due to the extent of the basin the conservationist practices factor p was adopted as 1 for all hydrological response units buarque 2015 de vente et al 2008 fagundes 2018 wilkinson et al 2009 the same relationship was adopted for the f g throughout the basin in the present work the mgb sed was coupled to the latest version of the mgb model that uses the in flow routing pontes et al 2017 simulation a and it was compared directly with the sediment modeling made by buarque 2015 using the previous version of the mgb model with the hd mc flow routing paiva et al 2011 simulation b the discretization and adjustment of the calibration parameters was set as presented by buarque 2015 allowing for the direct comparison of results from the different flow routing methods the parameters related to hydrological phenomena were the ones fitted first since most of the results of a sediment model are a consequence of the hydrological information that fed it which usually results from a hydrological module the madeira river basin was subdivided into 1443 catchments once the hydrological and hydrodynamic models parameters were estimated it was possible to perform simulations coupling the sediment module thus modifications were made to the module proposed by buarque 2015 aiming to make it compatible with the model of pontes et al 2017 in the floodplain module some changes were made one of them was the calculation of the floodplain area which is useful for determining the mean depth of the floodplain in the mgb with hd flow routing the calculation of the floodplain area is performed by multiplying the floodplain width by the length of the river stretch the variable that represents the floodplain width is extracted in data preprocessing and in the model input there is a table that relates levels with their respective plain widths in the mgb with in flow routing the floodplain area is calculated by interpolation according to pontes et al 2017 this interpolation first generates a level volume relationship from the input dimension area hypsometric curve it then relates this volume to the volume calculated by the continuity equation to find the corresponding average area a r e a 2 which is stored as a variable after this the area of the river stretch is subtracted from the a r e a 2 if the result is greater than zero it means that there is a flooded area the flooded area estimated in the floodplain module is 6 a t f l m a x 0 a r e a 2 1000000 b r i o s r i o 1000 where a t f l m2 is the flooded area a r e a 2 km2 is the resulting interpolation area b r i o m is the width of the river and s r i o km is the length of the river in the same routine the floodplain volume of each river reach is calculated in the hd version the volume is calculated by interpolation between level and volume in the in version the floodplain volume is calculated by the continuity equation that describes the total volume of water in the catchment at a given moment thus the flooded volume can be estimated by 7 v f l m a x 0 v o l 2 v t a b 2 where v f l m³ is the floodplain volume v o l 2 m³ is the volume calculated by the continuity equation total volume in the catchment and v t a b 2 m³ is the volume calculated by the inertial method which corresponds to the river channel when completely full 2 5 model performance and other analysis after performing the simulations with the new proposed model the metrics used to evaluate the model performance were the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 and the total volume error pbias gupta et al 1999 the reference thresholds for the evaluation are presented by moriasi et al 2007 these criteria were used for both hydrological and sediment transport simulation for the transport of suspended sediments results of simulations a in flow routing and b hd mc flow routing were checked against the observed sediment data the model efficiency was estimated by the comparison of simulated discharge data and the data observed at so hybam stations the evaluation of the weight of the advective inertial term ii of dynamic equation 2 at the madeira river basin in comparison to the other terms i iii iv and v of dynamic equation 2 was performed to verify if the inertial flow routing approach would be an appropriate simplification for this assessment for each catchment the average values of the arithmetic mean of all terms the arithmetic mean of the advective inertial term the standard deviation of the advective inertial term and the percentual contribution of the advective inertial term were calculated for the evaluation the new mgb sed coupling was evaluated by a qualitative analysis of the net erosion of river reaches in which an analysis of the tendency towards erosion or deposition processes was performed the annual suspended sediment load and the role of floodplains in sediment storage were also evaluated these results were presented in maps comparing simulations a and b to evaluate model estimation regarding the behavior of floodplains in the madeira river basin we performed a simulation in which the effect of sediment exchange between rivers and their respective floodplains was not considered it should be noted that simulation b considered a pre selection of stretches for the use of the hd model with the remaining stretches being simulated with the mc model the criteria of the pre selection is presented in buarque 2015 and paiva et al 2013b in the mgb sed model the friction slope of sections selected for simulation with the hd method is calculated in the same way as in the inertial method for sections where the simulation is performed using the mc approach however the friction slope is replaced by the channel slope which can produce discrepant results between models for the same river section 3 results and discussion 3 1 hydrological simulation aiming to evaluate the updated version of the sediment model we first performed a comparison of the hydrological simulations yielded by the inertial flow routing in version a and the prior combined flow routing hd mc version of the mgb sed buarque 2015 b fig 3 present the results of the spatially distributed statistics nse nselog and pbias for the basin each streamflow gauge station is represented by a circle where the left side of the circle indicates the classification resulting from model calibration using inertial flow routing a and the right side indicates the classification using the prior version of the mgb sed b results from fig 3 show that more than 40 of the stations presented results where nse 0 75 when considering the value of nash coefficient of the logarithm of the flow this percentage increases to 70 for the volume error pbias of simulation a 56 of stations presented errors of less than 5 for simulation b the percentage was 52 locations that presented a nse 0 50 for both simulations correspond to gauges that have a small drainage area in which the model performance is worse if compared to large areas an example is the acari br 230 gauge table 2 of supp mat section which has negative nse and nselog coefficients and a pbias of 41 3 2 evaluation of the importance of the hydrodynamic equations terms at the inertial flow routing approach the advective inertial term ii of dynamic equation 2 is not considered bates et al 2010 presenting smaller magnitudes at larger length scales when compared with the other terms of the dynamic equation hunter et al 2007 the five terms of dynamic equation 2 were calculated for each catchment of the basin and fig 4 presents three evaluation parameters it was possible to identify a pattern between the three parameters presented in fig 4 where the highest values of the advective inertial term seem to happen on regions of the basin with low slopes and the regions that correspond to the main rivers of the basin as presented on fig 2 the pattern also happens when all terms were analyzed together fig 4 1 so this pattern does not necessarily mean that the advective inertial term has greater importance than the other terms and this fact is confirmed by fig 5 fig 5 shows that the average percentage that is represented by the advective inertial term is 0 15 when the importance of the advective inertial term in terms of percentage was analyzed the pattern observed by fig 4 has not been reproduced this absence of a well defined pattern means that other hydrodynamic and hydrological aspects have greater importance even on places where the advective inertial terms were the highest lower slopes regions as represented by fig 5 5 the proportional importance of the term was not necessarily significant 13 therefore it is possible to say that the advective inertial term on this application can be disregarded and the inertial flow routing is a plausible option for the madeira river basin as it is expected that the simplification in the inertial flow routing process representation will lead to minor changes in the model results when compared with the full solution of the saint venant equations without compromising the quality of the assessment furthermore in sediment modeling we have two models running for daily time step the greater the number of simulated rivers reaches the more time consuming the simulation is and the inertial flow routing method becomes a great solution for modeling on large scales mainly for its simpler formulation 3 3 net erosion fig 6 presents the spatial pattern of the erosion and deposition trends this trend is expressed as the difference between the total eroded and deposited sediments for the entire simulation period positive values indicate sediment erosion while negative values indicate deposition in general the transport of silt and clay particles occurs in suspension especially at the scale of application of the model the possibility of silt deposition in times of low transport capacity exists but the deposited percentage of silt would be small when compared to the total in suspension in addition to being easily removable with variations in the flow so the mgb sed model only considers sand particles as bedload the authors consider such simplification as an initial approach that may impose limitations on the analysis of the results but it has been successful in other large scale applications buarque 2015 fagundes et al 2020 2021g a high erosion trend is visible in most parts of the andes fig 6 which may be related to the high slopes in this region however it is also possible to identify a deposition trend for some river reaches explained by the substantial sediment load yielded by the region bernini et al 2016 overcoming channel transport capacity in these areas simulation a presented more areas of deposition fig 6 which may be related to inertial flow routing being used in all river reaches while simulation b involved a combination of two flow routing methods buarque 2015 paiva et al 2011 the percentage of bedload concerning the total sediment load of the rivers was also estimated fig 7 according to the literature despite the lack of sediment bedload monitoring data it may be assumed that bedload corresponds to between 5 and 25 of the suspended sediment load wu 2008 the simulation results presented bedload values smaller than 10 on the main rivers of the basin fig 7 the pattern of bedload for both analyses on flat regions was also coherent whereas in lower slope regions the flow velocity and sediment transport capacity are reduced vauchel et al 2017 fig 7 shows that in simulation a bedload percentages were lower than in simulation b for smaller river reaches the pattern observed for the main rivers in both simulations is in the same range 10 in agreement with the literature the higher deposition values found in other river reaches 35 can be explained by the fact that coarse material sourced from the andean is trapped in plain regions or by limitations in the transport capacity equation adopted at these locations 3 4 annual suspended sediment load table 3 presents the annual suspended sediment load estimated at the outlets of the main tributaries of the madeira river basin guaporé mamoré beni and madre de dios rivers the other rivers of the basin did not present a considerable contribution of sediments to the madeira river loads lower than 1 106 t year 1 our results showed that the andean region is the main source of suspended sediment load transported by the madeira river as expected rivera et al 2019 vauchel et al 2017 simulation a estimated that around 74 4 of the suspended sediment load coming from the andes to the madeira river is carried by the beni river and 25 3 of the load is carried by the mamoré river according to simulation b the estimates were 76 and 24 respectively this estimation agrees with studies that report a contribution of 72 for the beni river basin and 28 for the mamoré river basin guyot 1993 guyot et al 1999 many studies report that at least 50 of the suspended sediment load originating in the andes is retained by floodplains baby and guyot 2009 guyot 1993 guyot et al 1999 the results for both a and b scenarios show that floodplains retain about 30 of suspended sediment load the result is under the estimate of 50 but is internally consistent similar to a and b the underestimation of floodplain sediment retention might be related to processes observed in the andean region such as landslide driven sediment flux that in turn are not well represented by the musle equation used by the mgb sed buarque 2015 the spatial pattern of the average annual suspended sediment load in the madeira river basin for simulations a and b is presented in fig 8 fig 9 presents the regions that most contribute to the suspended sediment load of the basin in this figure the annual sediment load of each river reach was divided by its drainage area to provide the specific average suspended sediment load fig 9 highlights that the highest average annual suspended sediment loads occur in the andean region where specific average sediment loads greater than 3 5 106 t year 1 km2 1 were estimated in simulations a and b fig 10 presents the percentage of suspended sediment load that is retained in the floodplains of river reaches for each river reach the percentage presented refers to the upstream accumulated load the analysis of fine sediment deposition in flat regions fig 10 shows that the percentage estimated to be deposited within the madeira river is higher in simulation b 30 40 than in simulation a 20 30 this can be explained by a limitation of simulation b in which the stretches not selected for hd propagation are simulated with the mc flow routing method which does not consider the effects of deposition on floodplains simulation a presented a greater number of river stretches with deposition since all stretches were simulated with the same flow routing method in 3 5 suspended sediment load transport in this section we present the results of simulations performed to evaluate daily suspended sediment dynamics for the main rivers of the madeira river basin using three sediment gauging stations figs 11 16 present the time series of daily sediment discharge simulated by the two models a and b at the three selected sediment stations along with observed data the figures also present the performance metrics calculated for comparisons between observed and simulated sediment data the rurrenabaque station is located on the beni river in a portion of the basin where the calibration of the hydrological model by buarque 2015 adopted a single set of parameters for the entire beni and madre de dios sub basin simulated hydrograph result was evaluated only at the border between brazil and bolivia although statistics calculated at the monitored site showed nse and nselog coefficients with values greater than 0 8 the entire sub basin was devoid of stations with which to calibrate the model parameter values thus in the simulation there was no means of controlling the generation of runoff distributed in this sub basin besides adjusting the results of the outlets therefore it is possible that sediment generation may be mismodeled by simulations throughout this region as the musle equation explicitly considers surface runoff to estimate the volume of eroded material this fact may explain the unsatisfactory results of the nash coefficient calculated for the station in simulation a which also presented a considerable volume error 33 6 despite considering river plain exchanges figs 11 16 show that the coupling of the sediment module to the mgb model with inertial flow routing simulation a presented similar results to simulation b this can be verified visually and by the performance statistics calculated for the models the slight differences between the two models results can be related to the spatial discretization adopted for the simulations the spatial discretization for simulation a should have been based on river length as recommended by pontes et al 2017 but it was made according to buarque 2015 which utilizes a different process in which each catchment corresponds to a single river stretch until it reaches a confluence when it is subdivided the discretization adopted was the same as that of buarque 2015 for the establishment of a comparison where the discretization did not become another variable for analysis or a source of uncertainty 4 conclusion the present study presented the development and evaluation of a version of the mgb sed model in which the flow routing scheme has been modified from the original mgb sed model buarque 2015 for the first time a comparison between the two flow routing schemes local inertial modified and combined saint venant and musking cunge method original was performed in a large scale basin simulation of sediment erosion and transport results showed that the proposed update to the mgb sed which consists of a replacement of the full saint venant equations approach with inertial flow routing was capable of representing sediment transport processes as effectively as the original version of the model in several analyses the results of the original and modified versions were similar and the representation of backwater effects and the flow exchange between rivers and floodplains in the madeira river were reasonable for both simulations the hydrodynamic equation terms evaluation allowed to better understand why the difference in the performance of the two flow routing models to be used as hydraulic conditions for sediment transport modeling was not significant the disregarded term in the inertial version in general represents 0 15 if compared against all terms regarding about places where the advective inertia term may be considered relevant the mgb sed model would not be the ideal alternative future research can be conducted to improve the model s performance in these situations such as adding a third flow propagation method to the model the authors advise caution to those working at smaller scales where such effects can be magnified the most important achievement of this work was to verify that for a large watershed the results of simulations using inertial flow routing and the full saint venant method are effectively the same thus an inertial approach would seem to be the preferred choice when considering data preparation and numerical stability when dealing with large scales some idealizations must be made since not all processes can be represented in detail the use of a model with a time step compatible with the temporal dynamics of the flow and transport of sediments requires input data that are also compatible with this scale which unfortunately is not yet a reality in several locations even so erosion and deposition processes that occur in small stretches where the propagation time is under 24 h might be well represented despite this limitation the model satisfactorily represented the suspended sediment flow the new coupling of the updated model offers greater simplicity in data acquisition through tools that aid in discretizing and preprocessing data such as iph hydro tools siqueira et al 2016 which makes the process more dynamic for the user without reducing the quality of the results another advantage of the inertial approach on the mgb model is its coupling with open source gis software pontes et al 2017 which makes it more user friendly finally we recommend using the inertial method considering river reaches of equal length because it is an explicit numerical model that is more sensitive to watershed discretization considering this recommendation can be important to achieve better results software availability name of software mgb sed version 2020 developer hydraulic research institute federal university of rio grande do sul brazil iph ufrgs postgraduate programme in environmental engineering federal university of espírito santo ppgea ufes first available year 2015 software requirements windows qgis programming language vb net fortran cost the application mgb sed is free source code is available at https github com lizandrabf mgb sed git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment to the research and innovation support foundation of espírito santo fapes for granting a scholarship to the first author this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105332 
25651,the flow routing module in several large scale erosion and sediment transport models generally adopts simplifications that ignore important physical aspects of river dynamics in this study we compare for the first time the results generated by a large scale erosion and sediment transport model using two different flow routing approaches a combined saint venant and muskingum cunge method and the inertial flow routing for this assessment simulations are performed with the mgb sed model we evaluate sediment characteristics such as suspended sediment load and concentration floodplain effects and the flux of suspended sediment within the madeira river basin the simulation with the inertial approach presents satisfactory results similar to those achieved with the combined saint venant approach for lowland rivers and the muskingum cunge method for steeper rivers also inertial module is capable to represent the complex processes of the basin but with easier preprocessing and numerical features thus promising expanded capabilities for large scale simulations keywords hydrosedimentological modeling large basins madeira river 1 introduction on a global scale tropical basins are the main sources of suspended sediments to the oceans filizola and guyot 2011 milliman and farnsworth 2011 sediment transport from the rivers to the ocean plays an important role in the global geochemical cycle beusen et al 2005 naipal et al 2018 van oost et al 2007 south america which possesses a drainage area of 17 8 106 km2 12 of the global terrestrial surface transfers to the ocean a large amount of suspended sediment 2446 mt year syvitski et al 2005 the three major rivers of south america are the amazon orinoco and paraná which together transport 24 of the global water flux and 13 of the global load of suspended sediment to the ocean restrepo et al 2006 nearly 50 of the sediment load transported by the amazon river amounting to approximately 240 000 tons of sediment per year originates from the madeira river the largest amazon tributary in terms of sediment load fagundes et al 2021 filizola and guyot 2011 guyot 1993 laraque et al 2005 vauchel et al 2017 anthropogenic changes in the environment new datasets and methods make it important to continuously evaluate suspended sediments due to its importance for geomorphological and ecological issues understanding the fluvial sediment transport processes that occur in large scale basins has great research value for the sustainable development of local and global ecosystems li et al 2020 and knowledge of how sediment behaves within a watershed is important to natural resource management gilbert and wilcox 2020 however scarcity of sediment data remains a problem for many locations in the world mouyen et al 2018 even in the era of big data and big science best 2019 fewer than 10 of global rivers are being monitored near their discharge to the ocean syvitski et al 2005 to assist researchers in dealing with data scarcity numerical models have become fundamental tools for understanding past and future sediment trends cohen et al 2014 erosion and sediment transport models differ widely in complexity input requirements represented processes the scale of usage and types of output information provided merritt et al 2003 the measurement and control of processes related to the erosion and transport of sediments in large basins become more complex as the area of the basin increases and the interactions between different erosive and depositional processes along with their impacts become more numerous de vente et al 2007 thus on large scales distributed or semi distributed models are preferred because they have a greater capacity to represent the spatial variability of the modeled processes and the heterogeneity of the basin these models are often composed of a hydrological and a sediment module which together consider soil erosion and river sediment transport the hydrological module is a key component as hydrological input data significantly affects the results of an overall model of sediment transport some examples are swat arnold et al 1998 swat g eckhardt et al 2002 swim krysanova et al 1996 sednet prosser et al 2001 wasa sed mueller et al 2010 lascam sivapalan et al 1996 viney and sivapalan 1999 and mgb sed buarque 2015 fagundes et al 2019 2020 2021 however compared to the more rapidly advancing area of large scale hydrological and hydrodynamic modeling the field of large scale sediment modeling still faces many barriers fagundes et al 2021 especially in the areas of spatially temporally distributed and continuous time step modeling the bqart model syvitski and milliman 2007 provides a mechanism to quantify the influences that control sediment flux to the coastal zones but does not provide details about erosion and sediment transport occurring in the inner part of the watershed the wbmsed cohen et al 2013 is a global scale sediment model that estimates suspended sediment flow at a daily timescale and its updated version cohen et al 2014 was validated in several locations including the madeira river however the model is focused on estimating long term average using a simplified flow routing method simplified flow routing methods such as the muskingum cunge mc may not take into account certain important physical aspects of river dynamics at large scales e g backwater effects floodplains non dendritic river networks neal et al 2012 paiva et al 2013a floodplains have strong impacts on streamflow and sediment dynamics and provide connections between habitat areas sediment retention biodiversity and nutrient cycling hupp et al 2009 mcintyre and thorne 2013 the floodplain environment typically has lower energy and sediment aggradation occurs through diffusion and convection processes over time pizzuto 1987 also floodplains are complex environments formed by several fluvial processes that depend on the channel morphology stream power sediment properties and hydrology among other things lauer and parker 2008 nanson and croke 1992 due to these characteristics many authors have reported that floodplains despite their importance are problematic areas for large scale modeling oki et al 1999 pedinotti et al 2012 tshimanga and hughes 2014 yamazaki et al 2011 it implies that representing more complex sediment phenomena in large rivers remains a challenge hydrodynamic hd approaches may use the full saint venant equations which have a complex formulation and require a large amount of data and computational effort pontes et al 2015 an alternative method of intermediate complexity is inertial flow routing almeida and bates 2013 bates et al 2010 yamazaki et al 2013 this approach can be used in regions with low slope rivers vast floodplains reservoirs and backwater effects it can provide similar results to models that use the full solution of the saint venant equations with a simpler algorithm fan et al 2014 a recent study by hatono and yoshimura 2020 presented a global sediment dynamics model that considers suspended sediment and bedload at short timescales the authors validated suspended sediment from four observation stations in the amazon river basin and in several locations around the world the model used the local inertial equation proposed by bates et al 2010 to calculate river discharge the results showed that the representation of individual rivers was satisfactory but that the global sum of suspended sediment flow was largely underestimated fagundes et al 2021 developed and evaluated the performance of a sediment erosion and transport model mgb sed as for the entire south america domain they coupled the sediment module from mgb sed model buarque 2015 to the hydrologic hydrodynamic model developed by siqueira et al 2018 for the whole south america fagundes et al 2021 evaluate the coupled model results against daily in situ data from 595 stations information from regional studies and from the global model wbmsed cohen et al 2014 these efforts represent great advances in sediment modeling studies improving the comprehension and comparison of spatial and temporal dynamics however these works do not clarify exactly how the sediment modeling and routing can be affected by different hydrodynamic approaches this paper aims to help fill this gap as we present and evaluate two flow routing methods applied on a large scale erosion and sediment transport model we present comparisons of results obtained with the inertial in approach to those acquired with a mixed flow routing method hd mc in the madeira river basin considered as a geographically and geologically complex basin for its features to the best of the authors knowledge this is the first time these approaches have been compared 2 methods 2 1 study area the madeira river basin is the most important southern component of the amazon basin guyot et al 1999 and it covers approximately 20 of the amazon basin having a surface area of 1 4 106 km2 and encompassing bolivia 51 brazil 42 and peru 7 fig 1 the madeira river is the longest southern tributary of the amazon basin contributing 15 of the total water discharge from the amazon river to the atlantic ocean goulding et al 2003 the basin has a complex conformation and drains three important geomorphological structures the andes the brazilian shield and the amazonian plains the basin encompasses high altitudes in upstream regions large floodplains in flatter areas and waterfalls within the brazilian shield such peculiarities make the madeira river a tributary of great geographical importance guyot et al 1999 vieira et al 2018 the annual mean concentration of suspended sediments in the madeira river is 360 mg l 1 at porto velho rivera et al 2019 this high value is typical of tropical rivers known as whitewater rivers fassoni andrade and paiva 2019 filizola et al 2009 ribeiro neto et al 2006 the andes mountains are the most significant source of sediments for the madeira river due to their characteristic steep slopes and the nature of their igneous rocks and sedimentary coverage which are highly susceptible to weathering in turn promoted by local tectonism low vegetal coverage and high precipitation levels sediment production in the region is controlled by massive erosion processes filizola and guyot 2011 and sediment transport occurs mainly via two tributaries of the madeira river the madre de dios and mamoré rivers bernini et al 2016 2 2 the mgb sed model the mgb sed model buarque 2015 is a coupling of an erosion and sediment transport module to the large scale mgb hydrological model collischonn et al 2007 paiva et al 2011 which is a semi distributed conceptual model with daily time steps and unit catchments discretization that utilizes the concept of hydrologic response units hru the normal river segmentation method considered by the mgb model defines each river reach as a drainage segment located between two successive confluences or between the beginning of a river stretch and the first confluence downstream a unit catchment is then defined as the area that contributes directly to this river stretch the mgb model collischonn et al 2007 was initially developed using the muskingum cunge mc flow routing method later paiva et al 2011 developed a version that utilized the full solution of the saint venant equations focusing on representing more complex environments such as the amazon basin floodplains however this version of the mgb model was used only in the amazon basin buarque 2015 föeger et al 2018 paiva et al 2011 ribeiro neto et al 2006 since its flow routing requires extensive preprocessing steps to generate input data making it a complex choice for modeling this version allows for the a priori selection of flow routing method by river section the method selected may be the muskingun cunge method mc the full hydrodynamic flow routing method hd or a mixed flow routing method hd mc that employs the hd method in steep reaches of the main rivers and the mc method in the headwater regions of the drainage network paiva et al 2011 the most recent version of the mgb model has been updated to reduce its cumbersomeness and improve the model s capacity to simulate large river basins with extensive floodplains and backwater effects the updated version by pontes et al 2017 replaces the hd flow routing with an inertial method in this version of the mgb model has been applied in many basins such as the araguaia pontes et al 2017 congo paris et al 2017 and niger fleischmann et al 2018 as well as to the south american continent as a whole siqueira et al 2018 the in flow routing method used in the mgb hydrologic model has been shown capable of representing complex hydraulic processes in lowland areas pontes et al 2017 it has less computational effort than the prior version of the mgb model described by paiva et al 2011 which uses the full saint venant equations this latest version also has greater ease of applicability than the paiva et al 2011 version due to its provision of tools iph hydro tools to perform data preprocessing siqueira et al 2016 the simulated hydrological processes of the mgb model include interception and soil infiltration energy budget and evapotranspiration using the penman monteith approach shuttleworth 1993 soil water budget using bucket model surface runoff based on the variable contributing area concept and subsurface and groundwater flow generation the water volumes generated in each hru are propagated to the stream network using three linear reservoirs representing surface subsurface and groundwater flows in the inertial approach the dynamic equation from the saint venant equations equations 1 and 2 is simplified disregarding the second term of the left side of the equation pontes et al 2017 known as the advective inertial term the module assumes a rectangular river cross section and employs a discretization approach to compute discharge using the numerical approximation proposed by bates et al 2010 1 a t q x 0 2 q t i q 2 a x ii g a h x iii g a s o iv g a s f v where i is the local inertia term ii is the advective inertia term iii is the pressure differential term iv is the bed slope term and v is the friction term q is the flow m3 s 1 a is the cross sectional area of the flow m2 x is the longitudinal distance m t is the time s q is the flow per unit width m2 s 1 h is the river depth m s 0 is the bottom slope m m 1 g is the gravitational acceleration m s 2 and s f is the water surface slope m m 1 estimated using the manning equation and the assumption that river width b is larger than river depth h which approximates the hydraulic radius to the depth according to 3 s f n 2 q 0 q 0 h f l o w 10 3 where h f l o w m refers to the mean depth of the catchment river and q 0 m3 s 1 m 1 is the ratio between the catchment flow and the channel width buarque 2015 coupled to the mgb model paiva et al 2011 a module that estimates the transport and redistribution of sediments creating the mgb sed despite the mgb sed having already been applied in several basins with good results buarque 2015 fagundes et al 2020 limitations to its application in large basins with complex hydrodynamic processes remains because the use of the hd flow routing method is only available for the amazon basin the mgb sed is divided into three components basin river and floodplain the basin component estimates the amount of sediment generated in each catchment and models its transport to the rivers in the representation of the basin the mgb sed uses unit catchments which are subdivided into hrus in which the sediment routing is performed using linear reservoirs combined with a lag time coefficient the modified universal soil loss equation musle williams 1975 is used to estimate the soil loss in every unit catchment 4 s e d α q s u r q p e a k a β k c p l s f g where s e d t is the resulting sediment load of the soil erosion q s u r mm ha 1 is the runoff surface volume q p e a k m³ s 1 is the peak superficial flow rate a ha is the superficial area k 0 013 t m2 h m³ t cm 1 is the soil erodibility factor c is the coverage and management factor p is the conservationist practices factor l s is the topographic factor and f g is the coarse fragment factor the coefficients α and β used in the mgb sed were originally estimated by williams 1975 having values of 11 8 and 0 56 respectively they are considered as coefficients that can be calibrated following fagundes et al 2019 sediment time series graphs are amplified or reduced proportional to the variation of parameter α while parameter β intensifies the peaks and valleys as its value decreases the sediment load estimated in each catchment is divided into three sediment fractions silt clay and sand defined by the mean texture of the soil classes of each hru since not all the sediment generated during a model time step daily reaches the river at the same time the model uses a linear reservoir for each soil fraction to delay the sediment volume transferred to the river buarque 2015 thus the total sediment volume generated in each hru of a catchment is stored in a reservoir and the total solid discharge q s t s 1 at the reservoir outlet is computed as a linear function of its stored load and the retardation time t k s s for the linear reservoir the solid discharge from each hru transferred to the catchment river is estimated using 5 q s i j t 1 t k s i v s e d i j t as suggested by fagundes et al 2019 the tks parameter corresponds to the lag time associated with the reservoir and is considered as one calibratable parameter of the sediment module proposed in this study the lower the adopted value for the tks the more intense the peaks and valleys of sediment time series graphs the parameter also directly influences the displacement of sediment graphs for the present work the calibration of the α β and tks parameters was not performed so that the results of inertial propagation and hydrodynamics results could be compared according to the criteria adopted by buarque 2015 the sediment contribution to the rivers occurs with the use of a simple linear reservoir for each soil fraction considered silt sand or clay the sediment volume is subject to a delay reflecting the fact that not all sediments within the generated volume necessarily reach the river simultaneously buarque 2015 the sediment transport module utilizes the results of flow from the flow routing modeling and the sediment contribution from the unit catchments to perform the sediment routing along the rivers considering both erosion and deposition phenomena the sediment transport at the river is performed using a diffusion advection equation for fine particles clay and silt these particles do not undergo deposition or erosion within the river channel and are considered to comprise the suspension load the model does not consider the resuspension of deposited sediments the mgb sed allows for the lateral exchange of sediments between the rivers and their respective floodplains where the deposition of these fractions can be estimated although as stated the model does not consider the resuspension phenomenon in addition the floodplain module allows for the reduction and delay of sediment concentration peaks in the channel buarque 2015 when the variation in level between time steps is positive it indicates elevation and water exchange from the river to the floodplain if the value is negative the opposite occurs with the water exchange from the floodplain to the river in both cases the volume is transferred at the direction of the flow considering the concentrations at the source when the flow leaves the river to the plain the transferred volume will depend on the sediment concentration in the river and in the opposite direction the transferred volume will depend on the concentration remaining in the plain considering silt and clay deposition in it the exner equation is used to compute the transport of the sediment sand fraction considered as bedload bedload erosion and deposition are calculated as a function of the sediment transport capacity using the yang equation yang 1979 the requirement of relatively few input parameters can be considered one of the biggest advantages of the mgb sed the interaction of the model and a geographic information system gis environment as well as the simple methodology for estimating the spatial and temporal distribution of sediments coupled to the mgb model allows for an easier estimation of sediment loads and concentration even for a large basin 2 3 datasets datasets and inputs for the hydrological and sediment simulation were the same as those adopted by buarque 2015 the hydrological data used in the mgb model are precipitation and flow data in this study the precipitation data was from the satellite trmm 3b42 huffman et al 2007 trmm 3b42 data is available from 1998 with a spatial resolution of 0 25 0 25 many studies have used this data to perform hydrological simulations buarque et al 2011 clarke et al 2011 collischonn et al 2008 paiva et al 2011 pontes et al 2017 soares et al 2016 the flow data was from the brazil national water agency ana streamflow gauge station the selected gauges contain daily data from 1985 to 2005 and at least 80 of monthly data is without gaps a month without gaps signifying a maximum of 5 days without information the total number of stations selected was 25 as listed in table 1 of the supplementary material section the meteorological data consists of air temperature atmospheric pressure solar radiation relative humidity and wind speed information the data used is a combination of results from climate models and information observed at meteorological gauge stations this data was selected because of the lack of precipitation monitoring data in the madeira river basin especially on the non brazilian territory the data belongs to the national centers for environmental prediction national center for atmospheric research ncep ncar as described by kalnay et al 1996 the hrus were used as presented by buarque 2015 the sediment data used for this work belongs to the sediment gauge stations of the observation service so hybam and consists of suspended sediment concentration data three gauging stations were selected based on the available data 1998 2005 the gauging stations are presented in table 1 and fig 2 2 4 sediment modeling erosion and sediment transport were simulated with daily time steps the parameters of musle considered were those adopted by buarque 2015 table 2 to establish a direct comparison between the results of the two versions of mgb sed as there is a lack of detailed and spatially distributed information about soil conservation measures mainly due to the extent of the basin the conservationist practices factor p was adopted as 1 for all hydrological response units buarque 2015 de vente et al 2008 fagundes 2018 wilkinson et al 2009 the same relationship was adopted for the f g throughout the basin in the present work the mgb sed was coupled to the latest version of the mgb model that uses the in flow routing pontes et al 2017 simulation a and it was compared directly with the sediment modeling made by buarque 2015 using the previous version of the mgb model with the hd mc flow routing paiva et al 2011 simulation b the discretization and adjustment of the calibration parameters was set as presented by buarque 2015 allowing for the direct comparison of results from the different flow routing methods the parameters related to hydrological phenomena were the ones fitted first since most of the results of a sediment model are a consequence of the hydrological information that fed it which usually results from a hydrological module the madeira river basin was subdivided into 1443 catchments once the hydrological and hydrodynamic models parameters were estimated it was possible to perform simulations coupling the sediment module thus modifications were made to the module proposed by buarque 2015 aiming to make it compatible with the model of pontes et al 2017 in the floodplain module some changes were made one of them was the calculation of the floodplain area which is useful for determining the mean depth of the floodplain in the mgb with hd flow routing the calculation of the floodplain area is performed by multiplying the floodplain width by the length of the river stretch the variable that represents the floodplain width is extracted in data preprocessing and in the model input there is a table that relates levels with their respective plain widths in the mgb with in flow routing the floodplain area is calculated by interpolation according to pontes et al 2017 this interpolation first generates a level volume relationship from the input dimension area hypsometric curve it then relates this volume to the volume calculated by the continuity equation to find the corresponding average area a r e a 2 which is stored as a variable after this the area of the river stretch is subtracted from the a r e a 2 if the result is greater than zero it means that there is a flooded area the flooded area estimated in the floodplain module is 6 a t f l m a x 0 a r e a 2 1000000 b r i o s r i o 1000 where a t f l m2 is the flooded area a r e a 2 km2 is the resulting interpolation area b r i o m is the width of the river and s r i o km is the length of the river in the same routine the floodplain volume of each river reach is calculated in the hd version the volume is calculated by interpolation between level and volume in the in version the floodplain volume is calculated by the continuity equation that describes the total volume of water in the catchment at a given moment thus the flooded volume can be estimated by 7 v f l m a x 0 v o l 2 v t a b 2 where v f l m³ is the floodplain volume v o l 2 m³ is the volume calculated by the continuity equation total volume in the catchment and v t a b 2 m³ is the volume calculated by the inertial method which corresponds to the river channel when completely full 2 5 model performance and other analysis after performing the simulations with the new proposed model the metrics used to evaluate the model performance were the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 and the total volume error pbias gupta et al 1999 the reference thresholds for the evaluation are presented by moriasi et al 2007 these criteria were used for both hydrological and sediment transport simulation for the transport of suspended sediments results of simulations a in flow routing and b hd mc flow routing were checked against the observed sediment data the model efficiency was estimated by the comparison of simulated discharge data and the data observed at so hybam stations the evaluation of the weight of the advective inertial term ii of dynamic equation 2 at the madeira river basin in comparison to the other terms i iii iv and v of dynamic equation 2 was performed to verify if the inertial flow routing approach would be an appropriate simplification for this assessment for each catchment the average values of the arithmetic mean of all terms the arithmetic mean of the advective inertial term the standard deviation of the advective inertial term and the percentual contribution of the advective inertial term were calculated for the evaluation the new mgb sed coupling was evaluated by a qualitative analysis of the net erosion of river reaches in which an analysis of the tendency towards erosion or deposition processes was performed the annual suspended sediment load and the role of floodplains in sediment storage were also evaluated these results were presented in maps comparing simulations a and b to evaluate model estimation regarding the behavior of floodplains in the madeira river basin we performed a simulation in which the effect of sediment exchange between rivers and their respective floodplains was not considered it should be noted that simulation b considered a pre selection of stretches for the use of the hd model with the remaining stretches being simulated with the mc model the criteria of the pre selection is presented in buarque 2015 and paiva et al 2013b in the mgb sed model the friction slope of sections selected for simulation with the hd method is calculated in the same way as in the inertial method for sections where the simulation is performed using the mc approach however the friction slope is replaced by the channel slope which can produce discrepant results between models for the same river section 3 results and discussion 3 1 hydrological simulation aiming to evaluate the updated version of the sediment model we first performed a comparison of the hydrological simulations yielded by the inertial flow routing in version a and the prior combined flow routing hd mc version of the mgb sed buarque 2015 b fig 3 present the results of the spatially distributed statistics nse nselog and pbias for the basin each streamflow gauge station is represented by a circle where the left side of the circle indicates the classification resulting from model calibration using inertial flow routing a and the right side indicates the classification using the prior version of the mgb sed b results from fig 3 show that more than 40 of the stations presented results where nse 0 75 when considering the value of nash coefficient of the logarithm of the flow this percentage increases to 70 for the volume error pbias of simulation a 56 of stations presented errors of less than 5 for simulation b the percentage was 52 locations that presented a nse 0 50 for both simulations correspond to gauges that have a small drainage area in which the model performance is worse if compared to large areas an example is the acari br 230 gauge table 2 of supp mat section which has negative nse and nselog coefficients and a pbias of 41 3 2 evaluation of the importance of the hydrodynamic equations terms at the inertial flow routing approach the advective inertial term ii of dynamic equation 2 is not considered bates et al 2010 presenting smaller magnitudes at larger length scales when compared with the other terms of the dynamic equation hunter et al 2007 the five terms of dynamic equation 2 were calculated for each catchment of the basin and fig 4 presents three evaluation parameters it was possible to identify a pattern between the three parameters presented in fig 4 where the highest values of the advective inertial term seem to happen on regions of the basin with low slopes and the regions that correspond to the main rivers of the basin as presented on fig 2 the pattern also happens when all terms were analyzed together fig 4 1 so this pattern does not necessarily mean that the advective inertial term has greater importance than the other terms and this fact is confirmed by fig 5 fig 5 shows that the average percentage that is represented by the advective inertial term is 0 15 when the importance of the advective inertial term in terms of percentage was analyzed the pattern observed by fig 4 has not been reproduced this absence of a well defined pattern means that other hydrodynamic and hydrological aspects have greater importance even on places where the advective inertial terms were the highest lower slopes regions as represented by fig 5 5 the proportional importance of the term was not necessarily significant 13 therefore it is possible to say that the advective inertial term on this application can be disregarded and the inertial flow routing is a plausible option for the madeira river basin as it is expected that the simplification in the inertial flow routing process representation will lead to minor changes in the model results when compared with the full solution of the saint venant equations without compromising the quality of the assessment furthermore in sediment modeling we have two models running for daily time step the greater the number of simulated rivers reaches the more time consuming the simulation is and the inertial flow routing method becomes a great solution for modeling on large scales mainly for its simpler formulation 3 3 net erosion fig 6 presents the spatial pattern of the erosion and deposition trends this trend is expressed as the difference between the total eroded and deposited sediments for the entire simulation period positive values indicate sediment erosion while negative values indicate deposition in general the transport of silt and clay particles occurs in suspension especially at the scale of application of the model the possibility of silt deposition in times of low transport capacity exists but the deposited percentage of silt would be small when compared to the total in suspension in addition to being easily removable with variations in the flow so the mgb sed model only considers sand particles as bedload the authors consider such simplification as an initial approach that may impose limitations on the analysis of the results but it has been successful in other large scale applications buarque 2015 fagundes et al 2020 2021g a high erosion trend is visible in most parts of the andes fig 6 which may be related to the high slopes in this region however it is also possible to identify a deposition trend for some river reaches explained by the substantial sediment load yielded by the region bernini et al 2016 overcoming channel transport capacity in these areas simulation a presented more areas of deposition fig 6 which may be related to inertial flow routing being used in all river reaches while simulation b involved a combination of two flow routing methods buarque 2015 paiva et al 2011 the percentage of bedload concerning the total sediment load of the rivers was also estimated fig 7 according to the literature despite the lack of sediment bedload monitoring data it may be assumed that bedload corresponds to between 5 and 25 of the suspended sediment load wu 2008 the simulation results presented bedload values smaller than 10 on the main rivers of the basin fig 7 the pattern of bedload for both analyses on flat regions was also coherent whereas in lower slope regions the flow velocity and sediment transport capacity are reduced vauchel et al 2017 fig 7 shows that in simulation a bedload percentages were lower than in simulation b for smaller river reaches the pattern observed for the main rivers in both simulations is in the same range 10 in agreement with the literature the higher deposition values found in other river reaches 35 can be explained by the fact that coarse material sourced from the andean is trapped in plain regions or by limitations in the transport capacity equation adopted at these locations 3 4 annual suspended sediment load table 3 presents the annual suspended sediment load estimated at the outlets of the main tributaries of the madeira river basin guaporé mamoré beni and madre de dios rivers the other rivers of the basin did not present a considerable contribution of sediments to the madeira river loads lower than 1 106 t year 1 our results showed that the andean region is the main source of suspended sediment load transported by the madeira river as expected rivera et al 2019 vauchel et al 2017 simulation a estimated that around 74 4 of the suspended sediment load coming from the andes to the madeira river is carried by the beni river and 25 3 of the load is carried by the mamoré river according to simulation b the estimates were 76 and 24 respectively this estimation agrees with studies that report a contribution of 72 for the beni river basin and 28 for the mamoré river basin guyot 1993 guyot et al 1999 many studies report that at least 50 of the suspended sediment load originating in the andes is retained by floodplains baby and guyot 2009 guyot 1993 guyot et al 1999 the results for both a and b scenarios show that floodplains retain about 30 of suspended sediment load the result is under the estimate of 50 but is internally consistent similar to a and b the underestimation of floodplain sediment retention might be related to processes observed in the andean region such as landslide driven sediment flux that in turn are not well represented by the musle equation used by the mgb sed buarque 2015 the spatial pattern of the average annual suspended sediment load in the madeira river basin for simulations a and b is presented in fig 8 fig 9 presents the regions that most contribute to the suspended sediment load of the basin in this figure the annual sediment load of each river reach was divided by its drainage area to provide the specific average suspended sediment load fig 9 highlights that the highest average annual suspended sediment loads occur in the andean region where specific average sediment loads greater than 3 5 106 t year 1 km2 1 were estimated in simulations a and b fig 10 presents the percentage of suspended sediment load that is retained in the floodplains of river reaches for each river reach the percentage presented refers to the upstream accumulated load the analysis of fine sediment deposition in flat regions fig 10 shows that the percentage estimated to be deposited within the madeira river is higher in simulation b 30 40 than in simulation a 20 30 this can be explained by a limitation of simulation b in which the stretches not selected for hd propagation are simulated with the mc flow routing method which does not consider the effects of deposition on floodplains simulation a presented a greater number of river stretches with deposition since all stretches were simulated with the same flow routing method in 3 5 suspended sediment load transport in this section we present the results of simulations performed to evaluate daily suspended sediment dynamics for the main rivers of the madeira river basin using three sediment gauging stations figs 11 16 present the time series of daily sediment discharge simulated by the two models a and b at the three selected sediment stations along with observed data the figures also present the performance metrics calculated for comparisons between observed and simulated sediment data the rurrenabaque station is located on the beni river in a portion of the basin where the calibration of the hydrological model by buarque 2015 adopted a single set of parameters for the entire beni and madre de dios sub basin simulated hydrograph result was evaluated only at the border between brazil and bolivia although statistics calculated at the monitored site showed nse and nselog coefficients with values greater than 0 8 the entire sub basin was devoid of stations with which to calibrate the model parameter values thus in the simulation there was no means of controlling the generation of runoff distributed in this sub basin besides adjusting the results of the outlets therefore it is possible that sediment generation may be mismodeled by simulations throughout this region as the musle equation explicitly considers surface runoff to estimate the volume of eroded material this fact may explain the unsatisfactory results of the nash coefficient calculated for the station in simulation a which also presented a considerable volume error 33 6 despite considering river plain exchanges figs 11 16 show that the coupling of the sediment module to the mgb model with inertial flow routing simulation a presented similar results to simulation b this can be verified visually and by the performance statistics calculated for the models the slight differences between the two models results can be related to the spatial discretization adopted for the simulations the spatial discretization for simulation a should have been based on river length as recommended by pontes et al 2017 but it was made according to buarque 2015 which utilizes a different process in which each catchment corresponds to a single river stretch until it reaches a confluence when it is subdivided the discretization adopted was the same as that of buarque 2015 for the establishment of a comparison where the discretization did not become another variable for analysis or a source of uncertainty 4 conclusion the present study presented the development and evaluation of a version of the mgb sed model in which the flow routing scheme has been modified from the original mgb sed model buarque 2015 for the first time a comparison between the two flow routing schemes local inertial modified and combined saint venant and musking cunge method original was performed in a large scale basin simulation of sediment erosion and transport results showed that the proposed update to the mgb sed which consists of a replacement of the full saint venant equations approach with inertial flow routing was capable of representing sediment transport processes as effectively as the original version of the model in several analyses the results of the original and modified versions were similar and the representation of backwater effects and the flow exchange between rivers and floodplains in the madeira river were reasonable for both simulations the hydrodynamic equation terms evaluation allowed to better understand why the difference in the performance of the two flow routing models to be used as hydraulic conditions for sediment transport modeling was not significant the disregarded term in the inertial version in general represents 0 15 if compared against all terms regarding about places where the advective inertia term may be considered relevant the mgb sed model would not be the ideal alternative future research can be conducted to improve the model s performance in these situations such as adding a third flow propagation method to the model the authors advise caution to those working at smaller scales where such effects can be magnified the most important achievement of this work was to verify that for a large watershed the results of simulations using inertial flow routing and the full saint venant method are effectively the same thus an inertial approach would seem to be the preferred choice when considering data preparation and numerical stability when dealing with large scales some idealizations must be made since not all processes can be represented in detail the use of a model with a time step compatible with the temporal dynamics of the flow and transport of sediments requires input data that are also compatible with this scale which unfortunately is not yet a reality in several locations even so erosion and deposition processes that occur in small stretches where the propagation time is under 24 h might be well represented despite this limitation the model satisfactorily represented the suspended sediment flow the new coupling of the updated model offers greater simplicity in data acquisition through tools that aid in discretizing and preprocessing data such as iph hydro tools siqueira et al 2016 which makes the process more dynamic for the user without reducing the quality of the results another advantage of the inertial approach on the mgb model is its coupling with open source gis software pontes et al 2017 which makes it more user friendly finally we recommend using the inertial method considering river reaches of equal length because it is an explicit numerical model that is more sensitive to watershed discretization considering this recommendation can be important to achieve better results software availability name of software mgb sed version 2020 developer hydraulic research institute federal university of rio grande do sul brazil iph ufrgs postgraduate programme in environmental engineering federal university of espírito santo ppgea ufes first available year 2015 software requirements windows qgis programming language vb net fortran cost the application mgb sed is free source code is available at https github com lizandrabf mgb sed git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment to the research and innovation support foundation of espírito santo fapes for granting a scholarship to the first author this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105332 
25652,currently raster based landscape indices lis that measures the landscape pattern of raster format land use data can be easily computed by relevant software e g fragstats unfortunately open access software for vector based lis often implement a small variety of metrics which cannot meet the growing demand of the gis and landscape design research the common approach often results in a loss of accuracy hence this paper presents the state of the art vecli framework for computing 217 vector based lis a parcel merging algorithm is proposed to address the impact of landscape fragmentation on vector based lis by considering the neighborhood effect a case study was conduct in shunde china the result shows that 80 of the lis from vecli are strongly correlated to fragstats s lis the patch perimeter related metrics from vecli portray a more realistic geographical pattern compared to those from fragstats moreover the vecli based software is developed for use by the gis and landscape design researchers keywords vector based landscape indices land parcel fragmentation urban environment landscape pattern fragstats 1 introduction landscape indices lis also call landscape metrics are the important basis in the field of landscape ecology frazier and kedron 2017 in landscape related studies landscape modeling often applies lis to measure landscape spatial patterns and analyze their temporal evolutions liu and yang 2015 sklenicka and zouhar 2018 with the application of lis in geographic information system gis and remote sensing more and more lis have been proposed to quantify the configuration of landscape patterns del castillo et al 2015 zhang and atkinson 2016 yu 2021 it has also driven the continuous development of related software for computing lis with rich types of lis implemented turner and gardner 2015 currently the most popular software for li computation available is fragstats mcgarigal 2015 frazier and kedron 2017 fragstats is a stand alone software for raster based lis with detailed instructions and metric descriptions as the pioneer of li software fragstats was the first to classify lis into three geographic scales i e landscape class and patch scales also to evaluate the similarities among lis fragstats classifies them into six classes including area edge shape core area contrast as well as aggregation to date fragstats still provides the largest number of raster based lis with 251 metrics in version 4 2 mcgarigal 2015 yu et al 2019 in second place is a package called landscapemetrics which only implements a total of 134 metrics hesselbarth et al 2019 however to date open access vector based software for computing lis still only offers a paucity of vector based metrics these vector based solutions are mainly presented in the form of plug ins for gis software e g arcgis based v late patch analyst 5 and arc lind among them v late and patch analyst 5 only provide a small number of lis mainly for quantifying ecological landscape conservation such as biodiversity lang and tiede 2003 rempel et al 2012 and arc lind provides 195 metrics and some solutions for the problems of computing vector based lis but is not currently open for use maclean and congalton 2013 yu et al 2019 currently raster based lis are widely used for studying landscape pattern land use and land cover lulc data are a prerequisite for the computation of lis gustafson 2019 lulc data are represented in two forms in geographic information system gis i e raster and vector formats mõisja et al 2016 and in the past the vast majority of lulc data was produced in the form of raster data with lower computing performance requirements and higher efficiency bober et al 2016 this is the reason why most of the existing lis computing frameworks are based on raster format lulc however most existing studies based on vector format lulc must follow the raster based routine by rasterizing the vector format data bosch 2019 fu et al 2021 data rasterization is prone to loss of graphical data resulting in loss of accuracy boongaling et al 2018 zhou et al 2018 the scale related parameter of the data rasterization can also make a significant difference to the results pan et al 2019 with the development of fine grained land use simulations vector data is valued for its ability to describe topographic data in great detail and accuracy o festus et al 2020 hence it is an urgent need for software to compute lis based on vector format lulc data in the past the development of vector based lis has encountered many limitations first vector based lis are much less efficient than raster based methods limited by the development level of computer technology lausch et al 2015 second vector data are prone to topological errors and cumbersome data processing when representing landscape patterns bubenik and dłotko 2017 at the same time parcel based vector landscape patterns are susceptible to landscape fragmentation due to human activities such as urban expansion and road construction which is not conducive to quantifying vector landscape patterns dadashpoor et al 2019 kumar et al 2018 nowadays the efficiency of vector data processing has been guaranteed thanks to the development of computer hardware and specialist vector data processing packages hesselbarth et al 2019 and common topological errors of vector data can be easily corrected via gis software kukulska et al 2018 martinez llario et al 2017 landscape fragmentation causes excessive fragmentation of parcels and affects the quantification of vector landscape patterns yao et al 2021 due to the lack of research on this issue this becomes an urgent problem for the current computation of vector based lis this paper proposes a new unifying computational framework for vector based lis vecli which provides three scales and six types in total 217 lis with reference to fragstats we adopt a parcel merging approach to address the problem of excessive land parcel fragmentation in the vector format a case study is conducted taking shunde guangdong province china as the study area computing 40 lis at the landscape scale to compare them with the current mainstream raster li software fragstats v4 2 the efficiency of vecli framework is verified by conducting a consistency analysis with pearson coefficients and t tests 2 methodology the proposed vecli framework in this paper can be divided into two steps 1 first the optimal neighborhood radius of the study area will be obtained and then a parcel merging algorithm based on this radius will be implemented to eliminate the effects of landscape fragmentation next the new landscape pattern will be quantified as vector based lis based on the merged vector lulc data 2 the vector format lulc will be rasterized and a set of its raster based lis will be computed via fragstats v4 2 as a baseline 3 a consistency analysis will be conducted between the vector and raster based lis by using two tailed paired t tests and pearson r correlation coefficients specifically the impacts of three aspects on the consistency i e the spatial resolutions of the rasterization the types of lis and individual metrics will be analyzed respectively 2 1 1 parcel merging algorithm considering landscape fragmentation landscape fragmentation often causes over division of parcels li et al 2017 yao et al 2021 particularly parcels are the basic spatial unit of the vector based landscape patterns and the effect of landscape fragmentation makes it difficult to accurately quantify vector based landscape patterns hence it is important to merge the adjacent parcels of the same land use type before quantifying the vector landscape patterns here we propose a parcel merging algorithm to address the effect of landscape fragmentation as follows fig 1 first a breadth first search is conducted to collect the adjacent parcels of a parcel by setting up a searching radius the indices of the adjacent parcels are recorded if they are of the same land use type at the end of these parcels are merged in order it is readily apparent that the searching radius significantly affects the result of the parcel merging algorithm to investigate the optimal searching radius we first compute the number of patches np within the completed landscape pattern for different neighborhood radius before performing merging based on the pre defined neighborhood extent and interval to extract the optimal searching radius we also introduce the concept of the number of patches within the neighborhood npn to assess the effect of the neighborhood on the parcel merging as it is necessary to determine whether the patches within the neighborhood are contiguous during this process an increase in npn means that more parcels are considered and more time is required to complete the process 2 2 2 comparison with vecli s metrics and raster based lis via fragstats this paper compares two types of lis i e vector and raster based lis for the study area we used the field of the land use type as the raster value field to rasterize the vector format lulc data and then the fragstats is used to compute the raster based lis to investigate the effect of the spatial resolutions of the rasterization on the result we also obtained raster format lulc data with diverse spatial resolutions to compute the corresponding raster based lis and vecli framework is also used to directly quantify the vector based landscape pattern of the study area in this paper 217 vector based lis are implemented in the vecli framework with reference to fragstats table 1 three scales of lis are included of which 14 lis are at patch scale to describe patches patterns 98 lis are at class scale to quantify land use types and 105 lis are landscape level describing the whole landscape patterns the lis can also be classified into six types area edge metrics and shape metrics are computed based on the area and perimeter of the patch the former describes the basis of the landscape structure while the latter measure the complexity of the patch shape core area metrics and contrast metrics are calculated based on the patch boundaries describing the area without the edge effects and the quantification edge effects respectively aggregation metrics are computed based on the distance between patches they reveal the distribution and aggregation of patches on the landscape diversity metrics are calculated based on the number and area of patches they are used to measure landscape structure mcgarigal et al 2012 park and guldmann 2020 slattery and fenner 2021 among them some of the shape and aggregation metrics contain parameters related to the raster format structure table 2 which indicates that they cannot be directly computed from vector format lulc data here the metrics in table 3 are not included in the vecli framework leaving a total of 217 metrics in addition some of the indices need corrections when computing based on vector format lulc data the radius of gyration gyrate of area edge metrics is strongly raster format related here we proposed a vector formula based on the definition of the gyrate and modified the meaning of the parameter table 4 also shape frac lsi of shape and aggregation metrics contain the raster based adjustment parameter of 0 25 was changed to 0 282 table 5 in reference to yu s approach yu et al 2019 prox and simi of aggregation metrics need to compute the edge to edge distance between patches while the raster based method is based on the centers of raster pixels and this distance is taken as 1 by default when using fragstats in the vecli framework the parameter is calculated directly based on the edge to edge distance and when plaque adjacency is present the and the parameter in vecli is also set as 1 2 3 3 consistency analysis between the vector and the raster based landscape indices a consistency analysis is conducted for the evolution of the landscape pattern in the study area t tests and pearson r correlation coefficients are applied to measure the consistency between the vector based lis and the raster based lis at diverse scales the two tailed paired t test is a method used to measure the average difference between two sets of samples doehl et al 2017 this paper applies this test to detect whether there is variability between the means of the vector based lis and the raster based lis here a two tailed p value of less than 0 05 is regarded that significant variability existed the pearson r correlation coefficient is a commonly used to measure the correlation between two sets of samples weaver and wuensch 2013 here we use it to analyze the correlation between the vector based lis and the raster based lis when the absolute value of pearson r is greater than 0 8 between 0 6 and 0 8 or less than 0 6 we consider the correlation to be strong moderately strong or weak respectively 3 results 3 1 1 study area shunde guangdong province china is selected as the study area shunde covers an area of approximately 806 km2 with four streets and six towns under its jurisdiction shunde shows an increasing trend of land use parcels number with a total of 16 611 20 865 and 23 336 parcels in 2012 2015 and 2018 an increasing ratio of 25 61 and 11 84 respectively because the rapid land use changes have resulted in a heavily fragmented landscape in shunde according to shunde s statistical yearbook shunde has been ranked as the first of the top 100 districts in terms of comprehensive strength since 2012 to 2018 the diverse industries there has led to a variety of land use types in shunde here we summarize the land use types of lulc data into four classes i e unused land farmland road and construction land we also rasterize the vector format lulc data into raster format data at three spatial resolutions of 10m 20m and 30m in shunde 3 2 2 the optimal searching radius based on the np and npn metrics this section explores the optimal searching radius of shunde in 2012 2015 and 2018 we first calculated the np and npn of the landscape pattern from a range of 0 3000 m searching radius at a 50 m interval fig 2 trends of np and npn in the study area as shown in fig 3 the trends of np and npn in shunde both remains basically the same in different years as the radius increases the np gradually decreases and tends to converge the npn gradually increases and at an ever increasing rate because the number of adjacent parcels increases as the area of the predefined neighborhood grows however the actual neighborhood is fixed which indicates that the np eventually remains constant in this paper the searching radius when the np is just constant is regarded as the optimal searching radius by setting the optimal searching radius the result of parcel merging algorithm as well as the computational efficiency are both the best the optimal radiuses in 2012 2015 and 2018 are 1750 m 1700 m and 1500 m respectively which indicates that the parcels are gradually concentrated and the urban functional areas show the trend of clustering it verifies that the parcel merging algorithm adopted in this paper can eliminate internally fragmented parcels by accurately quantifying the neighborhood effect and it can also effectively mine the landscape pattern of urban functional areas 3 3 3 consistency between vector and raster based lis the vector based lis of shunde are compared with the raster based lis at different scales via the consistency analysis for the same metrics pearson correlation coefficients and p values for t tests are computed based on the results of 2012 2015 and 2018 see table 9 for the null values in table 6 we compare them with the ground truth for the analysis table 7 it can be noticed that the metrics of the pr prd and rpr for all three years maintain constants moreover the vector based metrics are the same as the raster based lis thus these metrics are not considered in the following analysis 74 7 of the results obtained from fragstats and vecli are significantly different but 79 3 are strongly correlated it indicates that there is an objective quantitative difference between the vector and raster based lis due to different data structures however they show a high degree of consistency in the linear relationship reflecting the validity of the proposed vecli framework in terms of the spatial resolutions of the rasterization p values and r values always vary with the resolutions table 8 which indicates that almost all metrics except pr prd and rpr are sensitive to the spatial resolution related parameter of the data rasterizations at the same time variability is minimally affected by changes in resolution but correlation is strongly affected and proportional to resolution thus although the raster based lis differ numerically from the vector based metrics the higher the spatial resolution is set the more the landscape pattern described is correlated with the vector based landscape due to its ability to portray geographic entities it suggests that the vecli framework for vector based lis is significantly more realistic than the conventional raster based way in terms of index type both area edge and diversity metrics show a higher variability in average values compared to the other types with 38 9 and 55 6 significant as weak differences respectively it suggests that the mean values of the raster and vector based lis are closer to each other in measuring the pattern of the landscape in terms of the linear correlation both core area and contrast have a strong correlation of 100 0 indicating that the metrics based on patch edges are almost independent of the structure of the data in terms of individual metrics the range of split and mesh is limited by the number of raster pixels although their vector based metrics are achievable the correlation between vector and raster based metrics fluctuates significantly as the parameter of spatial resolution changes the metrics related to the perimeter of the parcels e g pafrac para mn frac show low correlations and their significance fluctuates greatly because the raster data structure represents the parcel in such a way that the edges are described as larger than the reality due to the jaggedness and the lower the resolution of the raster data is set the greater the edge length of the pixel leading to a greater lack of accuracy while the vector format data can more accurately portray the edge of the parcels and their edge lengths are more closely matched to the ground truth it highlights vecli s excellent ability to quantify the edge of the parcels finely 4 discussion we summarize the existing limitations of lis and propose an effective solution for the issue of landscape fragmentation previous studies often overlooked this problem this paper is the first to presents the np and npn based parcel merging algorithm it considers the neighborhood effect by merging adjacent parcels of the same land use type we find that the searching radius of neighborhood tends to decrease year by year as the clustering trend of urban functional area becomes more apparent it is consistent with the results of previous studies yao et al 2017 zhai et al 2020 indicating the feasibility of our idea this paper provides reasonable corrections for vector based lis computation referring to the framework of raster based lis provided by fragstats this paper proposes new equations for metrics without vector based equations and adjusts some of the metrics parameters by combining the study of yu et al and the equations of fragstats yu et al 2019 currently the proposed vecli framework provides the most index types and the richest number of vector based lis in a reasonable computation framework this paper examines the effect of data rasterization on the accuracy of lis and considers the relationship between the parameter of raster resolution and the vector based landscape pattern we found that all metrics vary with raster resolution except for a few indices of diversity where the results are constant this result is consistent with previous studies we also found that the higher the raster resolution is set the stronger the correlation between the calculated lis and the vector based lis it indicates that the refined raster landscape pattern is closer to the vector landscape pattern proving the authenticity of vecli in the representation of landscape patterns overall the vecli and fragstats results are significantly different in mean values of lis but maintain a high correlation 74 7 of the landscape indices are significantly different in mean values however the metrics measuring the overall structure of the landscape from vecli and fragstats both are close in mean values which is different from the overall trend there is an extremely high correlation between the vector and raster based lis in terms of linearity with strong correlations reaching even 100 in the metrics related to the parcel edge effect according to the overall trends of lis variability and correlation we found that the patch perimeter dependent metrics are mainly influenced by edge jaggedness with raster data describing patch edge lengths that are larger than the actual edge lengths and proportional to the size of the raster pixel the vector based lis are more realistic because the vector data structure accurately describes the geographic entity and the calculated perimeter matches the ground truth accurately thus the vector based lis implemented by vecli framework compensates for the lack of fragstats the lack of open access software for computing vector based lis has been considered in the design of the vecli framework and the development of its software firstly a wide range of lis is a prerequisite for the software to be widely used the vecli based software can implement 217 indices which is perfectly suitable for the main scenarios of fragstats secondly the software should be architecturally independent and free to use the vecli based software has its own underlying layer and the user does not need to obtain any commercial license we also provide comprehensive documentation and index descriptions to further help users understand and access the lis they need the proposed vecli framework still has shortcomings first the efficiency of the algorithm is still lacking compared to fragstats when dealing with vector format lulc data with more than 100 000 parcels at once thus the algorithm still needs to be optimized in the future second in terms of the richness of the lis considering that the variety of lis will be diversified in the future and not only limited to fragstats but we will also consider opening up the software code and provide the interface of the corresponding functions in the future so that users can develop the indices they need finally with the increasing demand for lis from disciplines such as urban planning resources and environment and biological sciences it is a direction of our later research to adapt the quantification of metrics to diverse application scenarios 5 conclusion this paper proposes a framework for computing the vector based lis vecli considering the effect of the landscape fragmentation in this paper we not only summarize the vector based lis and make vector based corrections in conjunction with previous research but also innovatively propose a parcel merging algorithm that considers the neighborhood effects effectively solving the problem of landscape fragmentation on quantifying vector based landscape patterns the overall experimental results from the consistency analysis show both consistency and variability between results of vecli and fragstats the consistency is evidenced by a strong linear relationship between the results for 2012 2015 and 2018 while the variability is evidenced by a significant difference in the mean values of the metrics specifically vector based lis are more accurate in measuring the perimeter of patches compared to raster based lis therefore the perimeter based lis should be implemented through a vector based way the vecli software for computing vector based li released in this paper makes up for the shortcomings of the existing vector based software the independent architecture and rich landscape indices of vecli can meet the needs of the geographic information and landscape design fields making it a professional software for computing vector based lis credit authorship contribution statement yao yao conceptualization methodology project administration resources funding acquisition writing original draft software tao cheng writing original draft writing review editing software methodology zhenhui sun writing original draft writing review editing software methodology linlong li writing original draft writing review editing software methodology dongsheng chen writing original draft supervision conceptualization software ziheng chen writing review editing software validation jianglin wei writing review editing software validation qingfeng guan supervision funding acquisition data curation declaration of competing interest no conflict of interest exists in the submission of this manuscript and manuscript is approved by all authors for publication i would like to declare on behalf of my co authors that the work described was original research that has not been published previously and not under consideration for publication elsewhere in whole or in part acknowledgements this work was supported by the national key research and development program of china 2019yfb2102903 the national natural science foundation of china 42171466 41801306 and the scientific research program of the department of natural resources of hubei province zrzy2021kj02 software availability software name vecli v2 0 0 developer yao yao tao cheng zhenhui sun linlong li dongsheng chen year first official release 2021 hardware requirements pc system requirements windows program language c program size 159 4 mb availability this paper the version 2 0 mentioned in this paper https urbancomp net archives vecli200 the lastest version 3 0 beta https www urbancomp net archives vecliv3beta license gpl 3 0 documentation documentation can be downloaded from the website https urbancomp oss cn hangzhou aliyuncs com blog vecli v2 manuals en 1629191061075 pdf appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105325 
25652,currently raster based landscape indices lis that measures the landscape pattern of raster format land use data can be easily computed by relevant software e g fragstats unfortunately open access software for vector based lis often implement a small variety of metrics which cannot meet the growing demand of the gis and landscape design research the common approach often results in a loss of accuracy hence this paper presents the state of the art vecli framework for computing 217 vector based lis a parcel merging algorithm is proposed to address the impact of landscape fragmentation on vector based lis by considering the neighborhood effect a case study was conduct in shunde china the result shows that 80 of the lis from vecli are strongly correlated to fragstats s lis the patch perimeter related metrics from vecli portray a more realistic geographical pattern compared to those from fragstats moreover the vecli based software is developed for use by the gis and landscape design researchers keywords vector based landscape indices land parcel fragmentation urban environment landscape pattern fragstats 1 introduction landscape indices lis also call landscape metrics are the important basis in the field of landscape ecology frazier and kedron 2017 in landscape related studies landscape modeling often applies lis to measure landscape spatial patterns and analyze their temporal evolutions liu and yang 2015 sklenicka and zouhar 2018 with the application of lis in geographic information system gis and remote sensing more and more lis have been proposed to quantify the configuration of landscape patterns del castillo et al 2015 zhang and atkinson 2016 yu 2021 it has also driven the continuous development of related software for computing lis with rich types of lis implemented turner and gardner 2015 currently the most popular software for li computation available is fragstats mcgarigal 2015 frazier and kedron 2017 fragstats is a stand alone software for raster based lis with detailed instructions and metric descriptions as the pioneer of li software fragstats was the first to classify lis into three geographic scales i e landscape class and patch scales also to evaluate the similarities among lis fragstats classifies them into six classes including area edge shape core area contrast as well as aggregation to date fragstats still provides the largest number of raster based lis with 251 metrics in version 4 2 mcgarigal 2015 yu et al 2019 in second place is a package called landscapemetrics which only implements a total of 134 metrics hesselbarth et al 2019 however to date open access vector based software for computing lis still only offers a paucity of vector based metrics these vector based solutions are mainly presented in the form of plug ins for gis software e g arcgis based v late patch analyst 5 and arc lind among them v late and patch analyst 5 only provide a small number of lis mainly for quantifying ecological landscape conservation such as biodiversity lang and tiede 2003 rempel et al 2012 and arc lind provides 195 metrics and some solutions for the problems of computing vector based lis but is not currently open for use maclean and congalton 2013 yu et al 2019 currently raster based lis are widely used for studying landscape pattern land use and land cover lulc data are a prerequisite for the computation of lis gustafson 2019 lulc data are represented in two forms in geographic information system gis i e raster and vector formats mõisja et al 2016 and in the past the vast majority of lulc data was produced in the form of raster data with lower computing performance requirements and higher efficiency bober et al 2016 this is the reason why most of the existing lis computing frameworks are based on raster format lulc however most existing studies based on vector format lulc must follow the raster based routine by rasterizing the vector format data bosch 2019 fu et al 2021 data rasterization is prone to loss of graphical data resulting in loss of accuracy boongaling et al 2018 zhou et al 2018 the scale related parameter of the data rasterization can also make a significant difference to the results pan et al 2019 with the development of fine grained land use simulations vector data is valued for its ability to describe topographic data in great detail and accuracy o festus et al 2020 hence it is an urgent need for software to compute lis based on vector format lulc data in the past the development of vector based lis has encountered many limitations first vector based lis are much less efficient than raster based methods limited by the development level of computer technology lausch et al 2015 second vector data are prone to topological errors and cumbersome data processing when representing landscape patterns bubenik and dłotko 2017 at the same time parcel based vector landscape patterns are susceptible to landscape fragmentation due to human activities such as urban expansion and road construction which is not conducive to quantifying vector landscape patterns dadashpoor et al 2019 kumar et al 2018 nowadays the efficiency of vector data processing has been guaranteed thanks to the development of computer hardware and specialist vector data processing packages hesselbarth et al 2019 and common topological errors of vector data can be easily corrected via gis software kukulska et al 2018 martinez llario et al 2017 landscape fragmentation causes excessive fragmentation of parcels and affects the quantification of vector landscape patterns yao et al 2021 due to the lack of research on this issue this becomes an urgent problem for the current computation of vector based lis this paper proposes a new unifying computational framework for vector based lis vecli which provides three scales and six types in total 217 lis with reference to fragstats we adopt a parcel merging approach to address the problem of excessive land parcel fragmentation in the vector format a case study is conducted taking shunde guangdong province china as the study area computing 40 lis at the landscape scale to compare them with the current mainstream raster li software fragstats v4 2 the efficiency of vecli framework is verified by conducting a consistency analysis with pearson coefficients and t tests 2 methodology the proposed vecli framework in this paper can be divided into two steps 1 first the optimal neighborhood radius of the study area will be obtained and then a parcel merging algorithm based on this radius will be implemented to eliminate the effects of landscape fragmentation next the new landscape pattern will be quantified as vector based lis based on the merged vector lulc data 2 the vector format lulc will be rasterized and a set of its raster based lis will be computed via fragstats v4 2 as a baseline 3 a consistency analysis will be conducted between the vector and raster based lis by using two tailed paired t tests and pearson r correlation coefficients specifically the impacts of three aspects on the consistency i e the spatial resolutions of the rasterization the types of lis and individual metrics will be analyzed respectively 2 1 1 parcel merging algorithm considering landscape fragmentation landscape fragmentation often causes over division of parcels li et al 2017 yao et al 2021 particularly parcels are the basic spatial unit of the vector based landscape patterns and the effect of landscape fragmentation makes it difficult to accurately quantify vector based landscape patterns hence it is important to merge the adjacent parcels of the same land use type before quantifying the vector landscape patterns here we propose a parcel merging algorithm to address the effect of landscape fragmentation as follows fig 1 first a breadth first search is conducted to collect the adjacent parcels of a parcel by setting up a searching radius the indices of the adjacent parcels are recorded if they are of the same land use type at the end of these parcels are merged in order it is readily apparent that the searching radius significantly affects the result of the parcel merging algorithm to investigate the optimal searching radius we first compute the number of patches np within the completed landscape pattern for different neighborhood radius before performing merging based on the pre defined neighborhood extent and interval to extract the optimal searching radius we also introduce the concept of the number of patches within the neighborhood npn to assess the effect of the neighborhood on the parcel merging as it is necessary to determine whether the patches within the neighborhood are contiguous during this process an increase in npn means that more parcels are considered and more time is required to complete the process 2 2 2 comparison with vecli s metrics and raster based lis via fragstats this paper compares two types of lis i e vector and raster based lis for the study area we used the field of the land use type as the raster value field to rasterize the vector format lulc data and then the fragstats is used to compute the raster based lis to investigate the effect of the spatial resolutions of the rasterization on the result we also obtained raster format lulc data with diverse spatial resolutions to compute the corresponding raster based lis and vecli framework is also used to directly quantify the vector based landscape pattern of the study area in this paper 217 vector based lis are implemented in the vecli framework with reference to fragstats table 1 three scales of lis are included of which 14 lis are at patch scale to describe patches patterns 98 lis are at class scale to quantify land use types and 105 lis are landscape level describing the whole landscape patterns the lis can also be classified into six types area edge metrics and shape metrics are computed based on the area and perimeter of the patch the former describes the basis of the landscape structure while the latter measure the complexity of the patch shape core area metrics and contrast metrics are calculated based on the patch boundaries describing the area without the edge effects and the quantification edge effects respectively aggregation metrics are computed based on the distance between patches they reveal the distribution and aggregation of patches on the landscape diversity metrics are calculated based on the number and area of patches they are used to measure landscape structure mcgarigal et al 2012 park and guldmann 2020 slattery and fenner 2021 among them some of the shape and aggregation metrics contain parameters related to the raster format structure table 2 which indicates that they cannot be directly computed from vector format lulc data here the metrics in table 3 are not included in the vecli framework leaving a total of 217 metrics in addition some of the indices need corrections when computing based on vector format lulc data the radius of gyration gyrate of area edge metrics is strongly raster format related here we proposed a vector formula based on the definition of the gyrate and modified the meaning of the parameter table 4 also shape frac lsi of shape and aggregation metrics contain the raster based adjustment parameter of 0 25 was changed to 0 282 table 5 in reference to yu s approach yu et al 2019 prox and simi of aggregation metrics need to compute the edge to edge distance between patches while the raster based method is based on the centers of raster pixels and this distance is taken as 1 by default when using fragstats in the vecli framework the parameter is calculated directly based on the edge to edge distance and when plaque adjacency is present the and the parameter in vecli is also set as 1 2 3 3 consistency analysis between the vector and the raster based landscape indices a consistency analysis is conducted for the evolution of the landscape pattern in the study area t tests and pearson r correlation coefficients are applied to measure the consistency between the vector based lis and the raster based lis at diverse scales the two tailed paired t test is a method used to measure the average difference between two sets of samples doehl et al 2017 this paper applies this test to detect whether there is variability between the means of the vector based lis and the raster based lis here a two tailed p value of less than 0 05 is regarded that significant variability existed the pearson r correlation coefficient is a commonly used to measure the correlation between two sets of samples weaver and wuensch 2013 here we use it to analyze the correlation between the vector based lis and the raster based lis when the absolute value of pearson r is greater than 0 8 between 0 6 and 0 8 or less than 0 6 we consider the correlation to be strong moderately strong or weak respectively 3 results 3 1 1 study area shunde guangdong province china is selected as the study area shunde covers an area of approximately 806 km2 with four streets and six towns under its jurisdiction shunde shows an increasing trend of land use parcels number with a total of 16 611 20 865 and 23 336 parcels in 2012 2015 and 2018 an increasing ratio of 25 61 and 11 84 respectively because the rapid land use changes have resulted in a heavily fragmented landscape in shunde according to shunde s statistical yearbook shunde has been ranked as the first of the top 100 districts in terms of comprehensive strength since 2012 to 2018 the diverse industries there has led to a variety of land use types in shunde here we summarize the land use types of lulc data into four classes i e unused land farmland road and construction land we also rasterize the vector format lulc data into raster format data at three spatial resolutions of 10m 20m and 30m in shunde 3 2 2 the optimal searching radius based on the np and npn metrics this section explores the optimal searching radius of shunde in 2012 2015 and 2018 we first calculated the np and npn of the landscape pattern from a range of 0 3000 m searching radius at a 50 m interval fig 2 trends of np and npn in the study area as shown in fig 3 the trends of np and npn in shunde both remains basically the same in different years as the radius increases the np gradually decreases and tends to converge the npn gradually increases and at an ever increasing rate because the number of adjacent parcels increases as the area of the predefined neighborhood grows however the actual neighborhood is fixed which indicates that the np eventually remains constant in this paper the searching radius when the np is just constant is regarded as the optimal searching radius by setting the optimal searching radius the result of parcel merging algorithm as well as the computational efficiency are both the best the optimal radiuses in 2012 2015 and 2018 are 1750 m 1700 m and 1500 m respectively which indicates that the parcels are gradually concentrated and the urban functional areas show the trend of clustering it verifies that the parcel merging algorithm adopted in this paper can eliminate internally fragmented parcels by accurately quantifying the neighborhood effect and it can also effectively mine the landscape pattern of urban functional areas 3 3 3 consistency between vector and raster based lis the vector based lis of shunde are compared with the raster based lis at different scales via the consistency analysis for the same metrics pearson correlation coefficients and p values for t tests are computed based on the results of 2012 2015 and 2018 see table 9 for the null values in table 6 we compare them with the ground truth for the analysis table 7 it can be noticed that the metrics of the pr prd and rpr for all three years maintain constants moreover the vector based metrics are the same as the raster based lis thus these metrics are not considered in the following analysis 74 7 of the results obtained from fragstats and vecli are significantly different but 79 3 are strongly correlated it indicates that there is an objective quantitative difference between the vector and raster based lis due to different data structures however they show a high degree of consistency in the linear relationship reflecting the validity of the proposed vecli framework in terms of the spatial resolutions of the rasterization p values and r values always vary with the resolutions table 8 which indicates that almost all metrics except pr prd and rpr are sensitive to the spatial resolution related parameter of the data rasterizations at the same time variability is minimally affected by changes in resolution but correlation is strongly affected and proportional to resolution thus although the raster based lis differ numerically from the vector based metrics the higher the spatial resolution is set the more the landscape pattern described is correlated with the vector based landscape due to its ability to portray geographic entities it suggests that the vecli framework for vector based lis is significantly more realistic than the conventional raster based way in terms of index type both area edge and diversity metrics show a higher variability in average values compared to the other types with 38 9 and 55 6 significant as weak differences respectively it suggests that the mean values of the raster and vector based lis are closer to each other in measuring the pattern of the landscape in terms of the linear correlation both core area and contrast have a strong correlation of 100 0 indicating that the metrics based on patch edges are almost independent of the structure of the data in terms of individual metrics the range of split and mesh is limited by the number of raster pixels although their vector based metrics are achievable the correlation between vector and raster based metrics fluctuates significantly as the parameter of spatial resolution changes the metrics related to the perimeter of the parcels e g pafrac para mn frac show low correlations and their significance fluctuates greatly because the raster data structure represents the parcel in such a way that the edges are described as larger than the reality due to the jaggedness and the lower the resolution of the raster data is set the greater the edge length of the pixel leading to a greater lack of accuracy while the vector format data can more accurately portray the edge of the parcels and their edge lengths are more closely matched to the ground truth it highlights vecli s excellent ability to quantify the edge of the parcels finely 4 discussion we summarize the existing limitations of lis and propose an effective solution for the issue of landscape fragmentation previous studies often overlooked this problem this paper is the first to presents the np and npn based parcel merging algorithm it considers the neighborhood effect by merging adjacent parcels of the same land use type we find that the searching radius of neighborhood tends to decrease year by year as the clustering trend of urban functional area becomes more apparent it is consistent with the results of previous studies yao et al 2017 zhai et al 2020 indicating the feasibility of our idea this paper provides reasonable corrections for vector based lis computation referring to the framework of raster based lis provided by fragstats this paper proposes new equations for metrics without vector based equations and adjusts some of the metrics parameters by combining the study of yu et al and the equations of fragstats yu et al 2019 currently the proposed vecli framework provides the most index types and the richest number of vector based lis in a reasonable computation framework this paper examines the effect of data rasterization on the accuracy of lis and considers the relationship between the parameter of raster resolution and the vector based landscape pattern we found that all metrics vary with raster resolution except for a few indices of diversity where the results are constant this result is consistent with previous studies we also found that the higher the raster resolution is set the stronger the correlation between the calculated lis and the vector based lis it indicates that the refined raster landscape pattern is closer to the vector landscape pattern proving the authenticity of vecli in the representation of landscape patterns overall the vecli and fragstats results are significantly different in mean values of lis but maintain a high correlation 74 7 of the landscape indices are significantly different in mean values however the metrics measuring the overall structure of the landscape from vecli and fragstats both are close in mean values which is different from the overall trend there is an extremely high correlation between the vector and raster based lis in terms of linearity with strong correlations reaching even 100 in the metrics related to the parcel edge effect according to the overall trends of lis variability and correlation we found that the patch perimeter dependent metrics are mainly influenced by edge jaggedness with raster data describing patch edge lengths that are larger than the actual edge lengths and proportional to the size of the raster pixel the vector based lis are more realistic because the vector data structure accurately describes the geographic entity and the calculated perimeter matches the ground truth accurately thus the vector based lis implemented by vecli framework compensates for the lack of fragstats the lack of open access software for computing vector based lis has been considered in the design of the vecli framework and the development of its software firstly a wide range of lis is a prerequisite for the software to be widely used the vecli based software can implement 217 indices which is perfectly suitable for the main scenarios of fragstats secondly the software should be architecturally independent and free to use the vecli based software has its own underlying layer and the user does not need to obtain any commercial license we also provide comprehensive documentation and index descriptions to further help users understand and access the lis they need the proposed vecli framework still has shortcomings first the efficiency of the algorithm is still lacking compared to fragstats when dealing with vector format lulc data with more than 100 000 parcels at once thus the algorithm still needs to be optimized in the future second in terms of the richness of the lis considering that the variety of lis will be diversified in the future and not only limited to fragstats but we will also consider opening up the software code and provide the interface of the corresponding functions in the future so that users can develop the indices they need finally with the increasing demand for lis from disciplines such as urban planning resources and environment and biological sciences it is a direction of our later research to adapt the quantification of metrics to diverse application scenarios 5 conclusion this paper proposes a framework for computing the vector based lis vecli considering the effect of the landscape fragmentation in this paper we not only summarize the vector based lis and make vector based corrections in conjunction with previous research but also innovatively propose a parcel merging algorithm that considers the neighborhood effects effectively solving the problem of landscape fragmentation on quantifying vector based landscape patterns the overall experimental results from the consistency analysis show both consistency and variability between results of vecli and fragstats the consistency is evidenced by a strong linear relationship between the results for 2012 2015 and 2018 while the variability is evidenced by a significant difference in the mean values of the metrics specifically vector based lis are more accurate in measuring the perimeter of patches compared to raster based lis therefore the perimeter based lis should be implemented through a vector based way the vecli software for computing vector based li released in this paper makes up for the shortcomings of the existing vector based software the independent architecture and rich landscape indices of vecli can meet the needs of the geographic information and landscape design fields making it a professional software for computing vector based lis credit authorship contribution statement yao yao conceptualization methodology project administration resources funding acquisition writing original draft software tao cheng writing original draft writing review editing software methodology zhenhui sun writing original draft writing review editing software methodology linlong li writing original draft writing review editing software methodology dongsheng chen writing original draft supervision conceptualization software ziheng chen writing review editing software validation jianglin wei writing review editing software validation qingfeng guan supervision funding acquisition data curation declaration of competing interest no conflict of interest exists in the submission of this manuscript and manuscript is approved by all authors for publication i would like to declare on behalf of my co authors that the work described was original research that has not been published previously and not under consideration for publication elsewhere in whole or in part acknowledgements this work was supported by the national key research and development program of china 2019yfb2102903 the national natural science foundation of china 42171466 41801306 and the scientific research program of the department of natural resources of hubei province zrzy2021kj02 software availability software name vecli v2 0 0 developer yao yao tao cheng zhenhui sun linlong li dongsheng chen year first official release 2021 hardware requirements pc system requirements windows program language c program size 159 4 mb availability this paper the version 2 0 mentioned in this paper https urbancomp net archives vecli200 the lastest version 3 0 beta https www urbancomp net archives vecliv3beta license gpl 3 0 documentation documentation can be downloaded from the website https urbancomp oss cn hangzhou aliyuncs com blog vecli v2 manuals en 1629191061075 pdf appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105325 
25653,networks of coauthorship for scientific publication well represent the collaboration dynamics among members of a scientific community here we employed a coauthor network analysis to identify research communities and their evolutionary patterns as the first attempt to characterize the dynamics of collaboration in environmental engineering using bibliometric data from 2007 to 2018 in environmental engineering 20 research communities were identified and two major drivers of scientific collaboration were revealed research interest and country of one s affiliation we developed a novel methodology that enables tracking cluster s evolutionary patterns this methodology allowed systematic stereotyping of each cluster as mature or emerging the two stereotypes did not show a significant difference in the annual growth rate of the volume of publications implying that comparable attention has been paid to advancing the knowledge of formerly established subjects and exploring innovative subjects of environmental engineering keywords bibliometric analysis coauthor network community evolution environmental engineering research pattern trend analysis 1 introduction environmental engineering is an academic discipline that aims to satisfy human needs by protecting the environment with resolution of environmental issues reible 1999 because problems of this discipline are not intentionally designed they are inherently complicated and working with them requires a wide breadth of scientific knowledge in most cases for example domestic sewage reclamation processes require knowledge that stems from all basic disciplines of natural science including physics chemistry and biology due to the complexity of sewage constituents that should be removed hazardous chemicals released into the environment can cause multi media pollution problems assessing human health risks imposed by this pollution problem requires multiple fields of applied science including analytical chemistry fluid mechanics meteorology toxicology and so forth the wide breadth of scientific knowledge required to work with environmental engineering problems implies that advance in the academic discipline of environmental engineering necessitates collaboration among scholars with different academic backgrounds davis and masten 2020 khan et al 2019 thus tracking and characterizing collaboration among scholars in environmental engineering should provide valuable information for community members of this discipline to help understand the status quo of their current research topics explore directions of their future research and build a strategic plan for future collaboration scientific collaboration is one of principal strategies for scholars to improve scientific productivity and quality of their research and to efficiently disseminate the knowledge barabás et al 2017 mccarty et al 2013 analyzing scientific collaboration as a network enables disclosing hidden relationships between scientists among the currently available approaches of scientific collaboration network analyses the coauthor network analysis which represents authors and their coauthoring as nodes and edges respectively is regarded to be the most powerful providing the best projection of real research collaboration patterns glänzel and schubert 2004 in addition to recording the comprehensive behavior of researchers regarding how often and with how many colleagues they coauthor communities of authors who have common denominators can be marked by identifying densely connected groups within the network called clusters porter et al 2009 herein for the first time we applied coauthor network analysis to the international scientific literature of environmental engineering the primary objective of the current work was to provide a scientific systematic and unbiased assessment of gross features of collaboration in the academic field of environmental engineering we employed coauthor network analysis to identify subgroups of intimate intra collaboration which we referred to as research communities of environmental engineering in this study characterized each subgroup and investigated their evolutionary dynamics to the best of our knowledge except for our recent paper on domestic environmental engineering in south korea park et al 2022 any peer reviewed article that analyzes the scientific literature of environmental engineering in the context of coauthor network has not been reported yet widening our search to studies that attempted such analysis in any context returned very few works an example is a recent work that has analyzed papers published in environmental science technology from 2000 to 2019 zhu et al 2021 this recent work focused on extracting and analyzing keywords while lacking any discussion on collaboration among researchers another unique feature of the current work was that in order to closely represent the gross academic field of environmental engineering a relatively large set of data 142 129 authors with 57 534 papers in total were collected for analysis coauthor network analysis has previously been employed to identify subdivisions of manifold academic disciplines barabás et al 2017 borrett et al 2018 huang et al 2008 newman 2001 2004 settle et al 2020 velden et al 2010 and to offer their prospects morel et al 2009 evidently subject disciplines of these previous works including ecological engineering borrett et al 2018 epidemiology morel et al 2009 and nanotechnology rueda et al 2007 were much smaller in volume than environmental engineering relatively small volumes of data used in these previous works allowed clusters of the coauthor networks to be well evaluated for their evolution dynamics by tracking down to an individual level huang et al 2008 tomassini and luthi 2007 on the other hand clusters identified in the current study were too large to be studied by this simple qualitative approach for this reason a novel method was developed and employed to semi quantitatively evaluate the evolutionary pattern of clusters of coauthors 2 methods 2 1 data and network for the current analysis we collected all 68 811 papers published from 2007 to 2019 in eight journals table s1 in supplementary material whose impact factors were within the top 10 in the category of engineering environmental or environmental sciences in the 2018 journal citation reports issued by the web of science wos and at the same time whose official websites explicitly stated their scopes to be topics of environmental engineering the wos database was used to extract bibliographical data of papers including title keyword author name country of author s affiliation journal name and year of publication data collection was commenced in 2007 because before then names of authors were recorded with their given names abbreviated which imposed a significant challenge in author identification a coauthor network was built using data from papers published between 2007 and 2018 the network had a single giant component i e the one in which the majority of the volume of the whole network was found among connected subsets of a network newman 2001 the presence of the giant component accounting for 79 of all authors of the constructed network and its scale free degree distribution fig s1 featured a typical mature social network barabási and albert 1999 newman 2001 the basic properties of the giant component are summarized in table s2 data collection and network construction were performed using python code written by us network characterization was performed using gephi software bastian et al 2009 data from papers published in 2019 were used as a validation set for the cluster growth model described below 2 2 cluster identification and analysis clustering was executed for the giant component we adopted leiden algorithm one of the fastest and best performing cluster identification algorithms blondel et al 2008 lancichinetti and fortunato 2009 traag et al 2019 yang et al 2016 using gephi we adjusted the criterion for clustering as density with a resolution set as the universal value of the giant component 5 348 10 5 this enabled the identification of clusters or research communities whose members collaborated more intensively with those who belonged to the same cluster than the rest traag et al 2019 among 821 clusters identified by this means we limited our definition of research communities to those containing over 1 of whole network members 1 006 this resulted in a total of 20 research communities they were named from a to t in descending order of the number of members a novel methodology was developed to explore evolutionary patterns of research communities over time sub datasets were created using snowball sampling huang et al 2008 i e by collecting data that corresponded to cumulative time intervals of 2007 2007 2007 2008 2007 2009 and so forth the clustering algorithm described above was applied to each sub dataset yielding a set of clusters for each cumulative time interval the ratio of pre clustered members r i t and the normalized time interval x t were obtained using the following eqs 1 and 2 respectively 1 r i t m a x n c i t c t j n c i t i a b c j a b c 2 x t t t 0 t t 0 where t was the last calendar year of the analyzed time interval t 0 and t were the first and the last calendar year of the whole dataset respectively i indicated research communities identified for the whole dataset c i t was the group of members of a research community i who had published at least one article by year t c t j was the group of cluster members labeled j which was identified using a sub dataset of publications between year t 0 and t and n was the number of members among intersecting groups between multiple c t j s and c i t members in the largest group were marked with pre clustered and the rest with complementary r i t indicated the ratio of already clustered members among existing members in cluster i at time interval t 0 t of course more c t j s at each sub dataset sharing less numbers of members with c i t may exist however some c t j s did not retain a sufficient size to represent explicit characteristics in further keyword analysis therefore we only focused on the maximally sharing group to allow tracking in our analysis considering complementary members would be clustered in the forthcoming period tracking research interests of pre clustered and complementary members could reveal the exhaustive evolutionary process of the research community the process of determining r i t is illustrated in fig 1 annual growth dynamics of research communities was characterized using the following power function 3 y i t x t b i where y i t was the normalized publication volume referring to the fraction of research community i s publications published between t 0 and t relative to those between t 0 and t and b i was the power function exponent it has been demonstrated that power function can well reproduce the temporal growth of a cumulative quantity with the value of the exponent representing the rapidness of growth manchein et al 2020 merrin 2020 zang et al 2016 zhu et al 2013 3 results and discussion 3 1 research community characterization table 1 summarizes key features of each research community including keywords of articles and countries of affiliations of authors with the most frequent appearance an extended list of major keywords is given in table s3 these major keywords were analyzed to define major research interests of each cluster which is given in the third column of table 1 keywords well representing research interests are marked in bold in table 1 and table s3 evaluation of research interests of the most influential researchers in each community table s4 justified our definition of major research interests countries that showed a notably i e more than 2 fold greater country occupancy ratio for a given cluster than that for the giant component are also marked in bold in table 1 we defined country occupancy ratio as the ratio of the number of author appearances with a given country of affiliation relative to the total number of author appearances the giant component s country occupancy ratios were in the order of usa 30 7 china 27 3 canada 6 1 spain 5 8 france 5 1 and so forth keywords of articles and affiliations of the authors were two determinants that defined the identity of each research community keyword analysis revealed that most research communities represented distinct research topics of environmental engineering these spanned from classical topics such as persistent organic pollutants pops communities b and i and biological water and wastewater treatment j to those that had recently become popular such as nanoparticles electrochemistry and photochemistry k and o in fact in many cases keywords and in turn research topics alone were sufficient to differentiate a research community from others research interest was also observed to be the major driver of collaborative research in a domestic environmental engineering network park et al 2022 common research interests of each research community identified in the current work were scientific or technical subjects whereas those reported in the previous study would be better defined as environmental problems e g inorganic waste management indoor water quality this is not surprising because international journals as sources of data sets for the current study highly prefer papers that discuss readily generalizable scientific and or technical findings whereas domestic journals would welcome submissions that address regional local matters countries of authors host institutions also made a significant contribution to the formation of research communities for instance communities a b n p and s showed unique country occupancy ratio profiles for china canada germany south korea and usa respectively as primary countries there were several pairs of communities whose research topics were analogous these research topics could not easily be distinguished from each other without introducing country of the host institution these included b and i pair and o and p pair major keywords of community a represented a much wider range of environmental engineering research topics than those of others community a had an extraordinarily large size of 13 510 members which was 3 8 fold greater than that of the second largest community the uniqueness of community a might be attributed to an occurrence of strong driving force for intra country collaboration in a country that a large number of authors belong to that is china the return of young chinese scientists who had studied abroad to their homeland might be one of the primary reasons for the formation of this community relevant studies have shown that the proportion of chinese graduates from u s universities who return to china has substantially increased since the beginning of the 21st century kellogg 2012 and that these early returners strongly prefer domestic collaboration over international one jonkers and tijssen 2008 we also tested if other variables such as published year table s5 and published journals table s6 were effective descriptors of each community s identity and thus probable drivers of research collaboration communities did not show any distinct fingerprints regarding the composition of published years of their articles unique published journal composition was observed for certain communities which could well be explained by the research interests of the community table s6 this indicates that keyword appearance and published journal profile strongly depend on each other the former should be a more effective descriptor of a community s identity than the latter because it provides much more affluent and specific information on research interest overall the current study and a previous study demonstrate that there are two major drivers of collaboration for research publication in environmental engineering common research interests and cultural linguistic geographic proximity forming or participating in a working group sharing common research interests has been recognized as the most widespread and efficient way to improve one s scientific productivity and impact borrett et al 2018 ductor et al 2014 glänzel and schubert 2004 mccarty et al 2013 previous studies have demonstrated that cultural and linguistic proximities are strong drivers of scientific coauthorship nagtegaal and de bruin 1994 narin et al 1991 geographical proximity has also been shown to be a key driver of coauthorship because it is one of the major motivations for initiating collaborative research in pursuit of resolving regional local environmental problems park et al 2022 3 2 evolution pattern the ratio of pre clustered members among existing members in each research community r i t is plotted against the normalized time interval x t in fig 2 data for periods of 2007 2007 and 2007 2008 were not shown in the figure because coauthor networks created for these periods were too small to conduct clustering evidently r i t is a function of time it is low at the beginning reaching unity as the time window expands closer to x t 1 however its trace over x t can vary considerably community by community groups whose members have a relatively long history of intensive collaboration i e mature groups are expected to consistently maintain a high value of r i t whereas relatively new collaboration groups should show a low r i t value at earlier time windows for example community a had r a 2009 0 8 meaning that 80 of members of a were in the same community at the time window of 2007 2009 this indicated that this community the one dominated by chinese authors with broad research interests had been constituted in or prior to 2009 it was recognized as early as in 2006 that chinese scientists became major players in scientific publications zhou and leydesdorff 2006 an analogous pattern is seen in other mature communities such as b and c addressing classical environmental engineering topics of pops and atmospheric pollutants e g particulate matter and organic aerosols respectively conversely the r i t value of g showed an abrupt increase from 0 4 to 1 between the last two years i e 2017 and 2018 this dramatic change can be explained by unification of separate communities in 2018 i e the emergence of this research community from convergence of different fields tracking r i t values of all research communities allows systematically stereotyping subjects of each into two categories mature and emerging examining the r i t value for the earliest time window i e r i 2009 along with the major research interest for each cluster suggests that r i 2009 0 6 is a reasonable threshold to categorize communities result of this categorization can be found in the second last column of table 1 3 3 emergence of fields we demonstrated above that the change of r i t with x t might be used to reckon how a research community emerged over time community g exhibited a unique x t r i t relationship allowing this task to be intuitive however the pattern of change of r i t for emerging communities showed a wide variation in addition many of these communities did not show such clear x t r i t relationship thus additional information was necessary to evaluate drivers of emergence to this end we recalled keyword analysis top 5 keywords of the pre clustered and complementary groups at different time windows were identified for all research communities as shown in table s7 tracking changes in top keywords for the pre clustered and complementary groups over time enabled us to investigate if a research community emerged by convergence of relevant fields or by congregation of researchers who worked in various fields to seek for a new field of research groups of g in 2017 had keywords of toxicity wastewater plastic debris drinking water degradation and toxicity wastewater exposure pahs united states for the pre clustered and complementary respectively given that the r i t value of g remarkably increased in 2018 it could be deduced that two different groups of toxicity concerned with plastics and pahs respectively converged at around the year of 2018 in a similar fashion with respect keywords variations in table s7 the rest of emerging communities had been investigated excluding r s and t which were not large enough to arrange data over time in h the r i t value increased from 2013 to 2014 two groups with research interest in exposure to air and water had converged in that period in i groups of pops working with different classes of contaminants merged in 2017 in n groups were integrated in 2015 due to clustering of two contaminated site remediation groups of groundwater and soil in o a group of aops using nanoparticles was combined with a group with a wider interest in wastewater treatment in p researchers interested in aops technologies were integrated in 2018 3 4 relative growth rate annual growth dynamics of each community in publication volume i e the x t y i t relationship was excellently reproduced by the power function of eq 3 fig s2 the growth rate exponent b i for each cluster is given in the last column of table 1 validity of the model and exponents were further examined by an extrapolation to year 2019 correlation between model predicted and real y i 2019 values showed statistical significance at a threshold of 0 01 as determined by the spearman correlation analysis these results demonstrate that the growth rate exponent i e b value can serve as an effective indicator to compare growth rates of the publication volume of a research community all communities exhibited b values greater than unity indicating that they were all consistently growing in the annual volume of publications the high growth rates of a and d can be ascribed by the remarkable growth of chinese research output zhou and leydesdorff 2006 and increasing interest in health concerns related to air quality respectively communities with pops and bioremediation as major research interests b and n exemplify those with relatively slow growth rates despite the distinct difference in evolutionary patterns between mature and emerging community categories observed in cluster analysis fig 2 average growth rates for these two categories were not statistically significantly different from each other p 0 35 student s t test 1 26 0 14 and 1 21 0 10 respectively mean standard deviation this result suggests that the emergence of a research community of environmental engineering does not necessarily involve extraordinary growth in research output 4 implications we conducted a coauthor network analysis for a significant portion of the international literature of environmental engineering research to identify twenty spontaneous communities with a relatively intense coauthorship in depth characterization of these communities revealed that major drivers of scientific collaboration through coauthoring were research interests and the country of the author s affiliation a novel methodology to investigate the evolutionary behavior of research communities at the mesoscopic level arranged different evolutionary types of research communities such as mature and emerging all research communities have been steadily growing whether a related research topic is classic or novel guaranteeing continuous development of environmental engineering as a promising discipline this study is a pioneering study performing a community evolution analysis and scrutinizing collaboration in the field of environmental engineering the analytical method developed herein allows investigation of collaborative behaviors of individuals in a research community and prediction of temporal trend of research topics in numerous complex multidisciplinary fields as community evolution research is still in its infancy follow up studies are needed to further advance relevant analytical means that will help characterize various evolutionary patterns in detail and track maturation of a field after its emergence by focusing on collaboration among individuals through scientific publications that eventually lead to a research community formation this study allows ones to develop integrative understanding on contemporary research of environmental engineering and widen their insights on the field to strategize future contribution it is expected that the current work will be combined with potential future reviews of environmental engineering bibliography focusing on different aspects of research activities to further assist players of the field in seeing not only trees but also woods declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper relied on web of science wos database supplied by the clarivate analytics this research was supported by the bk21 four research program of the national research foundation of korea nrf jeryang park was supported by basic science research program through the national research foundation of korea nrf funded by the ministry of science and ict nrf 2019r1c1c1008017 jaebeom park and yongju choi would like to thank the institute of engineering research at seoul national university for technical assistance appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105320 
25653,networks of coauthorship for scientific publication well represent the collaboration dynamics among members of a scientific community here we employed a coauthor network analysis to identify research communities and their evolutionary patterns as the first attempt to characterize the dynamics of collaboration in environmental engineering using bibliometric data from 2007 to 2018 in environmental engineering 20 research communities were identified and two major drivers of scientific collaboration were revealed research interest and country of one s affiliation we developed a novel methodology that enables tracking cluster s evolutionary patterns this methodology allowed systematic stereotyping of each cluster as mature or emerging the two stereotypes did not show a significant difference in the annual growth rate of the volume of publications implying that comparable attention has been paid to advancing the knowledge of formerly established subjects and exploring innovative subjects of environmental engineering keywords bibliometric analysis coauthor network community evolution environmental engineering research pattern trend analysis 1 introduction environmental engineering is an academic discipline that aims to satisfy human needs by protecting the environment with resolution of environmental issues reible 1999 because problems of this discipline are not intentionally designed they are inherently complicated and working with them requires a wide breadth of scientific knowledge in most cases for example domestic sewage reclamation processes require knowledge that stems from all basic disciplines of natural science including physics chemistry and biology due to the complexity of sewage constituents that should be removed hazardous chemicals released into the environment can cause multi media pollution problems assessing human health risks imposed by this pollution problem requires multiple fields of applied science including analytical chemistry fluid mechanics meteorology toxicology and so forth the wide breadth of scientific knowledge required to work with environmental engineering problems implies that advance in the academic discipline of environmental engineering necessitates collaboration among scholars with different academic backgrounds davis and masten 2020 khan et al 2019 thus tracking and characterizing collaboration among scholars in environmental engineering should provide valuable information for community members of this discipline to help understand the status quo of their current research topics explore directions of their future research and build a strategic plan for future collaboration scientific collaboration is one of principal strategies for scholars to improve scientific productivity and quality of their research and to efficiently disseminate the knowledge barabás et al 2017 mccarty et al 2013 analyzing scientific collaboration as a network enables disclosing hidden relationships between scientists among the currently available approaches of scientific collaboration network analyses the coauthor network analysis which represents authors and their coauthoring as nodes and edges respectively is regarded to be the most powerful providing the best projection of real research collaboration patterns glänzel and schubert 2004 in addition to recording the comprehensive behavior of researchers regarding how often and with how many colleagues they coauthor communities of authors who have common denominators can be marked by identifying densely connected groups within the network called clusters porter et al 2009 herein for the first time we applied coauthor network analysis to the international scientific literature of environmental engineering the primary objective of the current work was to provide a scientific systematic and unbiased assessment of gross features of collaboration in the academic field of environmental engineering we employed coauthor network analysis to identify subgroups of intimate intra collaboration which we referred to as research communities of environmental engineering in this study characterized each subgroup and investigated their evolutionary dynamics to the best of our knowledge except for our recent paper on domestic environmental engineering in south korea park et al 2022 any peer reviewed article that analyzes the scientific literature of environmental engineering in the context of coauthor network has not been reported yet widening our search to studies that attempted such analysis in any context returned very few works an example is a recent work that has analyzed papers published in environmental science technology from 2000 to 2019 zhu et al 2021 this recent work focused on extracting and analyzing keywords while lacking any discussion on collaboration among researchers another unique feature of the current work was that in order to closely represent the gross academic field of environmental engineering a relatively large set of data 142 129 authors with 57 534 papers in total were collected for analysis coauthor network analysis has previously been employed to identify subdivisions of manifold academic disciplines barabás et al 2017 borrett et al 2018 huang et al 2008 newman 2001 2004 settle et al 2020 velden et al 2010 and to offer their prospects morel et al 2009 evidently subject disciplines of these previous works including ecological engineering borrett et al 2018 epidemiology morel et al 2009 and nanotechnology rueda et al 2007 were much smaller in volume than environmental engineering relatively small volumes of data used in these previous works allowed clusters of the coauthor networks to be well evaluated for their evolution dynamics by tracking down to an individual level huang et al 2008 tomassini and luthi 2007 on the other hand clusters identified in the current study were too large to be studied by this simple qualitative approach for this reason a novel method was developed and employed to semi quantitatively evaluate the evolutionary pattern of clusters of coauthors 2 methods 2 1 data and network for the current analysis we collected all 68 811 papers published from 2007 to 2019 in eight journals table s1 in supplementary material whose impact factors were within the top 10 in the category of engineering environmental or environmental sciences in the 2018 journal citation reports issued by the web of science wos and at the same time whose official websites explicitly stated their scopes to be topics of environmental engineering the wos database was used to extract bibliographical data of papers including title keyword author name country of author s affiliation journal name and year of publication data collection was commenced in 2007 because before then names of authors were recorded with their given names abbreviated which imposed a significant challenge in author identification a coauthor network was built using data from papers published between 2007 and 2018 the network had a single giant component i e the one in which the majority of the volume of the whole network was found among connected subsets of a network newman 2001 the presence of the giant component accounting for 79 of all authors of the constructed network and its scale free degree distribution fig s1 featured a typical mature social network barabási and albert 1999 newman 2001 the basic properties of the giant component are summarized in table s2 data collection and network construction were performed using python code written by us network characterization was performed using gephi software bastian et al 2009 data from papers published in 2019 were used as a validation set for the cluster growth model described below 2 2 cluster identification and analysis clustering was executed for the giant component we adopted leiden algorithm one of the fastest and best performing cluster identification algorithms blondel et al 2008 lancichinetti and fortunato 2009 traag et al 2019 yang et al 2016 using gephi we adjusted the criterion for clustering as density with a resolution set as the universal value of the giant component 5 348 10 5 this enabled the identification of clusters or research communities whose members collaborated more intensively with those who belonged to the same cluster than the rest traag et al 2019 among 821 clusters identified by this means we limited our definition of research communities to those containing over 1 of whole network members 1 006 this resulted in a total of 20 research communities they were named from a to t in descending order of the number of members a novel methodology was developed to explore evolutionary patterns of research communities over time sub datasets were created using snowball sampling huang et al 2008 i e by collecting data that corresponded to cumulative time intervals of 2007 2007 2007 2008 2007 2009 and so forth the clustering algorithm described above was applied to each sub dataset yielding a set of clusters for each cumulative time interval the ratio of pre clustered members r i t and the normalized time interval x t were obtained using the following eqs 1 and 2 respectively 1 r i t m a x n c i t c t j n c i t i a b c j a b c 2 x t t t 0 t t 0 where t was the last calendar year of the analyzed time interval t 0 and t were the first and the last calendar year of the whole dataset respectively i indicated research communities identified for the whole dataset c i t was the group of members of a research community i who had published at least one article by year t c t j was the group of cluster members labeled j which was identified using a sub dataset of publications between year t 0 and t and n was the number of members among intersecting groups between multiple c t j s and c i t members in the largest group were marked with pre clustered and the rest with complementary r i t indicated the ratio of already clustered members among existing members in cluster i at time interval t 0 t of course more c t j s at each sub dataset sharing less numbers of members with c i t may exist however some c t j s did not retain a sufficient size to represent explicit characteristics in further keyword analysis therefore we only focused on the maximally sharing group to allow tracking in our analysis considering complementary members would be clustered in the forthcoming period tracking research interests of pre clustered and complementary members could reveal the exhaustive evolutionary process of the research community the process of determining r i t is illustrated in fig 1 annual growth dynamics of research communities was characterized using the following power function 3 y i t x t b i where y i t was the normalized publication volume referring to the fraction of research community i s publications published between t 0 and t relative to those between t 0 and t and b i was the power function exponent it has been demonstrated that power function can well reproduce the temporal growth of a cumulative quantity with the value of the exponent representing the rapidness of growth manchein et al 2020 merrin 2020 zang et al 2016 zhu et al 2013 3 results and discussion 3 1 research community characterization table 1 summarizes key features of each research community including keywords of articles and countries of affiliations of authors with the most frequent appearance an extended list of major keywords is given in table s3 these major keywords were analyzed to define major research interests of each cluster which is given in the third column of table 1 keywords well representing research interests are marked in bold in table 1 and table s3 evaluation of research interests of the most influential researchers in each community table s4 justified our definition of major research interests countries that showed a notably i e more than 2 fold greater country occupancy ratio for a given cluster than that for the giant component are also marked in bold in table 1 we defined country occupancy ratio as the ratio of the number of author appearances with a given country of affiliation relative to the total number of author appearances the giant component s country occupancy ratios were in the order of usa 30 7 china 27 3 canada 6 1 spain 5 8 france 5 1 and so forth keywords of articles and affiliations of the authors were two determinants that defined the identity of each research community keyword analysis revealed that most research communities represented distinct research topics of environmental engineering these spanned from classical topics such as persistent organic pollutants pops communities b and i and biological water and wastewater treatment j to those that had recently become popular such as nanoparticles electrochemistry and photochemistry k and o in fact in many cases keywords and in turn research topics alone were sufficient to differentiate a research community from others research interest was also observed to be the major driver of collaborative research in a domestic environmental engineering network park et al 2022 common research interests of each research community identified in the current work were scientific or technical subjects whereas those reported in the previous study would be better defined as environmental problems e g inorganic waste management indoor water quality this is not surprising because international journals as sources of data sets for the current study highly prefer papers that discuss readily generalizable scientific and or technical findings whereas domestic journals would welcome submissions that address regional local matters countries of authors host institutions also made a significant contribution to the formation of research communities for instance communities a b n p and s showed unique country occupancy ratio profiles for china canada germany south korea and usa respectively as primary countries there were several pairs of communities whose research topics were analogous these research topics could not easily be distinguished from each other without introducing country of the host institution these included b and i pair and o and p pair major keywords of community a represented a much wider range of environmental engineering research topics than those of others community a had an extraordinarily large size of 13 510 members which was 3 8 fold greater than that of the second largest community the uniqueness of community a might be attributed to an occurrence of strong driving force for intra country collaboration in a country that a large number of authors belong to that is china the return of young chinese scientists who had studied abroad to their homeland might be one of the primary reasons for the formation of this community relevant studies have shown that the proportion of chinese graduates from u s universities who return to china has substantially increased since the beginning of the 21st century kellogg 2012 and that these early returners strongly prefer domestic collaboration over international one jonkers and tijssen 2008 we also tested if other variables such as published year table s5 and published journals table s6 were effective descriptors of each community s identity and thus probable drivers of research collaboration communities did not show any distinct fingerprints regarding the composition of published years of their articles unique published journal composition was observed for certain communities which could well be explained by the research interests of the community table s6 this indicates that keyword appearance and published journal profile strongly depend on each other the former should be a more effective descriptor of a community s identity than the latter because it provides much more affluent and specific information on research interest overall the current study and a previous study demonstrate that there are two major drivers of collaboration for research publication in environmental engineering common research interests and cultural linguistic geographic proximity forming or participating in a working group sharing common research interests has been recognized as the most widespread and efficient way to improve one s scientific productivity and impact borrett et al 2018 ductor et al 2014 glänzel and schubert 2004 mccarty et al 2013 previous studies have demonstrated that cultural and linguistic proximities are strong drivers of scientific coauthorship nagtegaal and de bruin 1994 narin et al 1991 geographical proximity has also been shown to be a key driver of coauthorship because it is one of the major motivations for initiating collaborative research in pursuit of resolving regional local environmental problems park et al 2022 3 2 evolution pattern the ratio of pre clustered members among existing members in each research community r i t is plotted against the normalized time interval x t in fig 2 data for periods of 2007 2007 and 2007 2008 were not shown in the figure because coauthor networks created for these periods were too small to conduct clustering evidently r i t is a function of time it is low at the beginning reaching unity as the time window expands closer to x t 1 however its trace over x t can vary considerably community by community groups whose members have a relatively long history of intensive collaboration i e mature groups are expected to consistently maintain a high value of r i t whereas relatively new collaboration groups should show a low r i t value at earlier time windows for example community a had r a 2009 0 8 meaning that 80 of members of a were in the same community at the time window of 2007 2009 this indicated that this community the one dominated by chinese authors with broad research interests had been constituted in or prior to 2009 it was recognized as early as in 2006 that chinese scientists became major players in scientific publications zhou and leydesdorff 2006 an analogous pattern is seen in other mature communities such as b and c addressing classical environmental engineering topics of pops and atmospheric pollutants e g particulate matter and organic aerosols respectively conversely the r i t value of g showed an abrupt increase from 0 4 to 1 between the last two years i e 2017 and 2018 this dramatic change can be explained by unification of separate communities in 2018 i e the emergence of this research community from convergence of different fields tracking r i t values of all research communities allows systematically stereotyping subjects of each into two categories mature and emerging examining the r i t value for the earliest time window i e r i 2009 along with the major research interest for each cluster suggests that r i 2009 0 6 is a reasonable threshold to categorize communities result of this categorization can be found in the second last column of table 1 3 3 emergence of fields we demonstrated above that the change of r i t with x t might be used to reckon how a research community emerged over time community g exhibited a unique x t r i t relationship allowing this task to be intuitive however the pattern of change of r i t for emerging communities showed a wide variation in addition many of these communities did not show such clear x t r i t relationship thus additional information was necessary to evaluate drivers of emergence to this end we recalled keyword analysis top 5 keywords of the pre clustered and complementary groups at different time windows were identified for all research communities as shown in table s7 tracking changes in top keywords for the pre clustered and complementary groups over time enabled us to investigate if a research community emerged by convergence of relevant fields or by congregation of researchers who worked in various fields to seek for a new field of research groups of g in 2017 had keywords of toxicity wastewater plastic debris drinking water degradation and toxicity wastewater exposure pahs united states for the pre clustered and complementary respectively given that the r i t value of g remarkably increased in 2018 it could be deduced that two different groups of toxicity concerned with plastics and pahs respectively converged at around the year of 2018 in a similar fashion with respect keywords variations in table s7 the rest of emerging communities had been investigated excluding r s and t which were not large enough to arrange data over time in h the r i t value increased from 2013 to 2014 two groups with research interest in exposure to air and water had converged in that period in i groups of pops working with different classes of contaminants merged in 2017 in n groups were integrated in 2015 due to clustering of two contaminated site remediation groups of groundwater and soil in o a group of aops using nanoparticles was combined with a group with a wider interest in wastewater treatment in p researchers interested in aops technologies were integrated in 2018 3 4 relative growth rate annual growth dynamics of each community in publication volume i e the x t y i t relationship was excellently reproduced by the power function of eq 3 fig s2 the growth rate exponent b i for each cluster is given in the last column of table 1 validity of the model and exponents were further examined by an extrapolation to year 2019 correlation between model predicted and real y i 2019 values showed statistical significance at a threshold of 0 01 as determined by the spearman correlation analysis these results demonstrate that the growth rate exponent i e b value can serve as an effective indicator to compare growth rates of the publication volume of a research community all communities exhibited b values greater than unity indicating that they were all consistently growing in the annual volume of publications the high growth rates of a and d can be ascribed by the remarkable growth of chinese research output zhou and leydesdorff 2006 and increasing interest in health concerns related to air quality respectively communities with pops and bioremediation as major research interests b and n exemplify those with relatively slow growth rates despite the distinct difference in evolutionary patterns between mature and emerging community categories observed in cluster analysis fig 2 average growth rates for these two categories were not statistically significantly different from each other p 0 35 student s t test 1 26 0 14 and 1 21 0 10 respectively mean standard deviation this result suggests that the emergence of a research community of environmental engineering does not necessarily involve extraordinary growth in research output 4 implications we conducted a coauthor network analysis for a significant portion of the international literature of environmental engineering research to identify twenty spontaneous communities with a relatively intense coauthorship in depth characterization of these communities revealed that major drivers of scientific collaboration through coauthoring were research interests and the country of the author s affiliation a novel methodology to investigate the evolutionary behavior of research communities at the mesoscopic level arranged different evolutionary types of research communities such as mature and emerging all research communities have been steadily growing whether a related research topic is classic or novel guaranteeing continuous development of environmental engineering as a promising discipline this study is a pioneering study performing a community evolution analysis and scrutinizing collaboration in the field of environmental engineering the analytical method developed herein allows investigation of collaborative behaviors of individuals in a research community and prediction of temporal trend of research topics in numerous complex multidisciplinary fields as community evolution research is still in its infancy follow up studies are needed to further advance relevant analytical means that will help characterize various evolutionary patterns in detail and track maturation of a field after its emergence by focusing on collaboration among individuals through scientific publications that eventually lead to a research community formation this study allows ones to develop integrative understanding on contemporary research of environmental engineering and widen their insights on the field to strategize future contribution it is expected that the current work will be combined with potential future reviews of environmental engineering bibliography focusing on different aspects of research activities to further assist players of the field in seeing not only trees but also woods declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper relied on web of science wos database supplied by the clarivate analytics this research was supported by the bk21 four research program of the national research foundation of korea nrf jeryang park was supported by basic science research program through the national research foundation of korea nrf funded by the ministry of science and ict nrf 2019r1c1c1008017 jaebeom park and yongju choi would like to thank the institute of engineering research at seoul national university for technical assistance appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105320 
25654,we developed statistical models to generate runoff time series at national hydrography dataset plus version 2 nhdplusv2 catchment scale for the continental united states conus the models use normalized difference vegetation index ndvi based curve number cn to generate initial runoff time series which then is corrected using statistical models to improve accuracy we used the north american land data assimilation system 2 nldas 2 catchment scale runoff time series as the reference data for model training and validation we used 17 years of 16 day 250 m resolution ndvi data as a proxy for hydrologic conditions during a representative year to calculate 23 ndvi based cn ndvi cn values for each of 2 65 million nhdplusv2 catchments for the contiguous u s to maximize predictive accuracy while avoiding optimistically biased model validation results we developed a spatio temporal cross validation framework for estimating selecting and validating the statistical correction models we found that in many of the physiographic sections comprising conus even simple linear regression models were highly effective at correcting ndvi cn runoff to achieve nash sutcliffe efficiency values above 0 5 however all models showed poor performance in physiographic sections that experience significant snow accumulation data availability statement statement the code used to develop this paper and the appendix can be found at the following sites https github com quanted hms handler curve number and ndvi dataset can be found at the following site ftp newftp epa gov exposure curvenumberndvi average curve number dataset for nhdplus catchments can be found at the following site https qed epa gov hms rest api info catchment cn true comid comid where comid is nhdplusv2 catchment id 1 introduction effective management of hydrologic resources and hazards often depends on accurate simulations of runoff for example runoff time series can be combined with other environmental data to characterize how a system responds to various climate and land use scenarios to facilitate the work of researchers and managers seeking to understand and manage hydrologic systems we developed an automated curve number cn based technique for estimating catchment level runoff that allows for the use of either simulated or historical data for precipitation and landcover to facilitate validation of the models developed in this paper for various applications we designed and implemented a machine learning accuracy assessment framework that withholds validation data from statistical model training both spatially and temporally to build confidence that the resulting accuracy measures truly characterize how the models generalize to other catchments and time periods to implement the accuracy assessment this paper presents a machine learning framework for a state of the art approximately unbiased approach to quantifying predictive accuracy in a hydrologic spatio temporal context using an existing automated technique for quantifying hydrologic condition muche et al 2019a 2019b we generated nhdplusv2 catchment scale cns for conus and applied the framework to estimate and evaluate a variety of relatively simple cn generated runoff time series correction models that often dramatically improve runoff accuracy additionally because the technique we employed for automating cn generation adheres to the conventional cn approach we expect that the correction models are likely to enhance the accuracy of runoff time series generated by any of the variants of cn such as the recent gcn250 jaafar et al 2019 1 1 hydrologic runoff modeling a variety of research topics involve data modeling that requires information about how precipitation patterns translate into measures such as runoff and streamflow historical runoff estimates utilizing a robust set of environmental forcing variables have been made readily available through the north american land data assimilation system nldas xia et al 2013 and the global land data assimilation system gldas rodell et al 2004 land surface model lsm projects for example historical data are useful for assessing and training models but these projects do not provide a means for simulating runoff in counterfactual or future conditions a variety of approaches have been developed to estimate the relationship between hydrologically relevant environmental variables such as precipitation and runoff sitterson et al 2017 provide a taxonomy of rainfall runoff models based primarily on the correspondence of the model with physical reality and spatial resolution at one end of the spectrum the cn methods of runoff modeling offer analysts one of the simplest approaches to runoff modeling at the other end of the spectrum are multi input multi output lsms such as nldas 2 henceforth referred to as nldas which itself is an ensemble of lsms xia et al 2012a 2012b other runoff modeling approaches include the geomorphological instantaneous unit hydrograph approach a more recent and a more technically sophisticated approach to rainfall runoff modeling rigon et al 2016 recently oppel and schumann 2020 applied machine learning estimators to explore transferability of geomorphological instant unit hydrograph runoff models between catchments based on catchment characteristics and a basin classification scheme derived from their models fractal geometry has been applied to modeling surface runoff gires et al 2018 another group of runoff models is the gr chain ficchì et al 2019 that vary in temporal resolution which ficchì et al 2019 extended to include flux matching criteria the soil conservation service curve number also widely referred to as the curve number method was developed by the united states department of agriculture usda in the 1950s to predict direct runoff from rainfall events and it is a widely adopted method in surface runoff estimation hawkins et al 2008 hawkins 2014 lian et al 2020 the method was developed using measured rainfall and runoff data from several agricultural research watersheds primarily in the eastern midwestern and southern u s the rainfall runoff relationship in the study watersheds was extrapolated to an empirical number the curve number using land use cover hydrologic soil groups and hydrologic conditions of watersheds hawkins et al 2008 muche et al 2019a rallison 1980 the cn method has been globally adapted to areas with varying land use cover soil properties and climatic conditions it has also been incorporated into various continuous hydrologic watershed models though the method was originally devised for event based rainfall runoff modeling garen and moore 2005 hawkins 1996 kennan et al 2007 muche et al 2019a despite wide applications of the cn method some watersheds have been found to exhibit significant differences between observed and predicted runoff using the cn method hawkins 2014 muche et al 2019a the effects of rainfall volume intensity and frequency muche et al 2019a wang and bi 2020 in addition to the seasonality of rainfall runoff relationship could be among the contributing factors to the cn method s low accuracy in those watersheds rodríguez blanco et al 2012 to increase the accuracy of cn generated runoff silveira et al 2000 incorporated automated estimation of antecedent moisture using five days of lagged rainfall recently advancements in geographic information science gis created the opportunity to account for seasonality in the rainfall runoff relationship by using remote sensed data to flexibly approximate hydrologic condition muche 2016 nasiri and alipur 2014 singh and khare 2021 moderate resolution imaging spectroradiometer normalized difference vegetation index modis ndvi were applied to cn estimation by several authors gandini and usunoff 2004 muche et al 2019a 2019b nasiri and alipur 2014 singh and khare 2021 muche et al 2019a used modis ndvi to estimate cn using 12 years of observed rainfall and runoff at four small watersheds in the konza prairie long term ecological research site muche et al 2019b extended this work using modis ndvi for catchment level cn development spanning conus as part of usepa s hydrologic micro services hms computational platform which is used in the results below 1 2 model validation and selection validation is generally regarded as an important step in modeling though it is not clear what exactly is meant by validation and what one must do to achieve it schlesinger 1979 defined validation for computerized simulations only in terms of comparison to reality in the domain of applicability schruben 1980 discussed simulation credibility as a more practical standard to meet than strict validation requiring simulation output to be indistinguishable from observations of reality by a human in the same manner as a turing test for artificial human intelligence sargent 2013 discusses validation broadly for empirical studies and describes several validation techniques including inner validation as an assessment process using data resampling and historical data validation as a process of splitting data into building and testing sets klemeš 1986 offers an early and widely cited guide to validation of hydrologic models that generally corresponds well with modern machine learning approaches to model validation biondi et al 2012 review and refine validation concepts in hydrology and they discuss performance or model validation which includes qualitative assessment of graphs and quantitative assessments of model metrics on split samples they also discuss a distinct type of validation scientific validation wherein one considers the theoretical underpinnings of the model common machine learning terminology contrasts with the above uses of validation hastie et al 2017 uses the validation set for model selection and a final testing set for quantifying generalization error which is called validation in the above contexts other than machine learning in machine learning a great deal of attention is paid to using validation like metrics for both model selection and validation typically with distinct treatments of data common statistical approaches to model selection that rely on error estimates from training observations can lead to substantial downward bias in error metrics leading to overly optimistic conclusions about model accuracy picard and cook 1984 cross validation approaches to model validation split a dataset into n folds use n 1 of the folds to train a model quantify predictive accuracy on the withheld fold and cycle through all n folds generating an empirical distribution of model accuracy that approximates expected prediction error hastie et al 2017 p254 hawkins et al 2003 favor cross validation approaches to model selection and validation rather than a single hold out test set because cross validation balances the desire for more data with the need for quantifying predictive accuracy however cross validation when done incorrectly can still lead to biased performance measures and inferior model selection cawley and talbot 2010 hastie et al 2017 p245 guyon et al 2010 provide a useful discussion of the nested relationship between the two or more types of parameters in the context of diverse statistical learners in this process of multi level inference that guyon et al 2010 describe parameters are chosen in an inner estimation step by analytically efficient learning algorithms e g linear algebra solutions for ordinary least squares ols linear regression and hyper parameters are chosen by repeatedly invoking the efficient algorithms with different hyper parameter values for multi level inference approaches cross validation serves as the framework for each level of inference to avoid over fitting cawley and talbot 2010 fushiki 2011 found for regression problems that n fold cross validation may bias prediction error upwards while training error is a downward biased estimate of prediction error hastie et al 2017 p254 characterize 10 fold cross validation as an approximately unbiased means of quantifying expected or extra sample error techniques such as 5 fold and 10 fold cross validation are computationally efficient approaches for quantifying prediction error with generally lower variance than leave one out cross validation hastie et al 2017 p255 in classification problems n fold cross validation has exhibited reduced bias and computational complexity relative to the bootstrap kim 2009 an alternative to cross validation time series datasets require additional assumptions to justify cross validation more conservative approaches to validation of time series models typically require withholding later observations in the dataset from training for testing the model s performance with new data this process has been called last block validation bergmeir and benítez 2012 or out of sample evaluation bergmeir et al 2018 recent advances have opened up possibilities for efficient cross validation with some time series estimators which is particularly adventitious for small datasets bergmeir et al 2018 because all data can be used for validation data can be grouped for modeling and assessment in a variety of ways in machine learning the term slice chung et al 2019 refers to divisions in the data by predictor variables that can be used to assess model performance with greater granularity validation metrics that group slices together can potentially obscure poorly performing slices chung et al 2019 the idea of assessing slice performance is closely related to the idea of transportability in hydrology as discussed by klemeš 1986 who described an early cross validation like testing procedure for detecting poor performing members of a group of catchments to validate simulations in ungauged members of the group the model scoring or loss metric also plays an important role in model selection and validation gupta et al 2009 applied and refined decompositions of the popular nash sutcliffe efficiency nse model scoring metric in the context of runoff simulations criticizing models optimized with nse as the score for being of use only in normal conditions this problem can be remediated to some extent by their proposed kling gupta efficiency kge metrics gupta et al 2009 knoben et al 2019 point out interpretation issues associated with several parametric and non parametric variants of kge they emphasize a lack of a clear benchmark or cutoff value with kge metrics while the nse value of zero benchmarks simulation performance against the mean of the observed series 1 3 corrective modeling models that correct simulations have been developed extensively in the earth sciences watson 2019 discusses the tradeoffs associated with physical versus purely data driven approaches to increasing predictive accuracy at short and long timescales in the context of climate simulations dinge et al 2019 distinguish between point to point correction models and models that use time series characteristics to increase performance in the context of applying error correction models to wind speed prediction zjavka 2015 applies a polynomial neural network to correct wind speed using lagged measures of nearby environmental variables 1 4 regression prediction we use a broad definition of regression from hastie et al 2017 p10 which encompasses any statistical learner that makes quantitative predictions in practice regression models have a continuous dependent variable in contrast to classification and ordered categorical models with discrete dependent variables accordingly the linear regressions regularized linear regressions and gradient boosting ensembles are all referred to as regressors or regression models regularized linear regression estimators share similarities with ols but with additional structure to reduce the variability increase the stability of the parameter estimates at the expense of increased bias frank and friedman 1993 the lasso regularized regression effectively selects predictor variables pushing some coefficient estimates to zero while the ridge regularized regression tends to push regression coefficients towards equality with each other tibshirani 1996 the group lasso was developed to select groups of dummy variables for multi category predictors and extensions to the group lasso have been developed to preserve hierarchal connection between interaction and main effects in lasso regression models lim and hastie 2015 in contrast to ols the basic lasso estimator can more generally estimate flexible dummy variable specifications with overlapping categories as discussed in lim and hastie 2015 the elastic net regressor combines the strengths of the lasso and ridge regression estimators with the ability to model high collinearity among variables like ridge and the ability to do variable selection like lasso zou and hastie 2005 the gradient boosting regressor is an extension of the gradient boosting classifier and has been described in detail by friedman 2001 and by hastie et al 2017 the algorithm fits an additive sequence of simple regression tree estimators with the gradient of the previous estimator s loss function used as the dependent variable for training the subsequent tree in an iterative procedure according to guyon et al 2010 boosting methods of regression are less vulnerable to overfitting because they minimize a guaranteed risk function hastie et al 2017 p340 quote others in describing the classifier version of gradient boosting as the best off the shelf classifier in the world 2 methods 2 1 ndvi based automated curve number development the primary challenge in automating the generation of runoff time series using the curve number method is the selection of the hydrologic condition the hydrologic condition functions as a categorical variable taking into consideration several possible influencing factors mainly related to land cover type at the time of precipitation event the customary approach to specifying hydrologic condition requires site specific expert analysis that hinders scaling the approach to larger areas remote sensing data has been shown to be a viable approach to specifying hydrologic condition facilitating automated estimation of cn values we followed the work of muche et al 2019a 2019b and used 250 m 16 day resolution modis ndvi didan 2015 data along with land cover and soil data to quantify hydrologic condition and create a time series of twenty three cn values spanning an average year for each of approximately 2 65 million nhdplusv2 catchments in conus to compute these numerous cn values we first needed to quantify the corresponding hydrologic condition we used google earth engine to obtain and spatially average seventeen years 2001 2017 of modis ndvi satellite raster data to the nhdplusv2 catchment next for each catchment and each of twenty three annual sixteen day timesteps we temporally averaged the 17 observations of spatially averaged ndvi we then used these time and space averaged ndvi values along with nlcd land cover data discussed next for each catchment and time period to determine the hydrologic condition as poor normal or good based on the ranges specified in table 1 we obtained catchment level nlcd 2011 land cover data and statsgo derived sand and clay soil composition percentages from the epa streamcat dataset hill et al 2016 we used the statsgo percentages to determine the hydrologic soil group of each catchment finally for each timestep and each catchment we used the land cover hydrologic soil group and hydrologic condition values along with the usda s soil conservation service curve number tables to obtain ndvi cn values for each catchment and each of the 23 annual 16 day time periods for each catchment and each timestep the ndvi and cn values as well as annual average cn values can be obtained at ftp newftp epa gov exposure curvenumberndvi additionally the spatially and temporally averaged ndvi data can be obtained for each catchment at https qed epa gov hms rest api info catchment cn true comid comid where comid is replaced by a nhdplus catchment id e g https qed epa gov hms rest api info catchment cn true comid 331416 2 2 accuracy assessment we used the cn values described above to develop a runoff database to investigate the accuracy of ndvi cn generated daily runoff using nldas runoff as the target for the spatial units in our database we randomly selected 5 nhdplus catchments in each united states geologic survey usgs physiographic section fenneman and johnson 1946 next we retrieved 17 years of nldas runoff data and ndvi cn runoff data forced by nldas precipitation data for each catchment we also retrieved the gldas runoff and ndvi cn runoff forced by gldas precipitation data and present parallel condensed results based on that data in appendix 1 in the discussion of model selection and validation below we follow the bulk of the empirical literature and reserve the term validation to describe the final split of data that is not used for any kind of model selection in this paper but only for reporting a final estimate of predictive accuracy this decision contrasts with recent trends in machine learning research where validation data are used for model selection and testing data are used for quantifying predictive accuracy of the selected model e g hastie et al 2017 we use the term validate or validation to characterize the final chronological split of data as well as the subsequent accuracy analysis this validation stage provides information not used for model selection in this paper but that is developed for use by end users who require information about expected predictive accuracy we use the term test or testing more generically to refer to any accuracy assessment such as those performed during model selection validation is thus a special kind of model testing deliberately designed to avoid data leakage that can optimistically bias results we adopt this terminology for consistency with the hydrologic literature and a number of other empirical sciences as well the purpose of assessing the accuracy of simulated runoff relative to the target runoff is to provide information to users about the likely quality of future simulations that may include times and locations not present in our database to achieve this we develop a validation approach with a resampling design based on holding out observations for testing based on both time and space first because some of our runoff simulation models use lengthy time series of runoff data for training we employ a traditional three part temporal splitting of each time series as can be seen in fig 1 the first half of a runoff time series is reserved for training the models and the second half is split into a testing series used for final model selection and a validation series used for quantifying predictive accuracy this approach helps ensure that the validation accuracy assessment of the models generalizes to other time periods particularly the near future this type of validation is an example of what klemeš 1986 called a split sample test second for the runoff correction models that we develop below there is also a potential concern about the ability of the accuracy metrics to generalize to catchments excluded from model training such as those not in the runoff database accordingly we evaluate predictive accuracy of each correction model using the average of a leave one catchment out of each section repeated cross validation approach as shown in fig 2 the splitting algorithm we developed takes each physiographic section and places the five sampled catchments in a list for a single repetition of cross validation each list is shuffled and then the first catchment in each list is excluded from the training data of a sub model and reserved for testing that model the next training test split comes from excluding the second catchment in each list and so on until each of the 5 catchments in each section have been reserved from training a model and used for testing and validation of that model this procedure is repeated to ensure the accuracy metric does not depend on any patterns in the test data from a single shuffling of the catchments in a physiographic section this approach to geographic data splitting is in addition to the single chronological split discussed above and is an example of what klemeš 1986 referred to as a proxy basin test the result is that no catchment or time period is ever present in both training and test or validation data for any of the prediction accuracy measures that we report the simulated runoff series and the target or observed runoff series yobs we are comparing are continuous and non negative allowing for a wide range of accuracy measures based on differences or similarities between the two series we select pearson s correlation coefficient rp and nash sutcliffe efficiency nse because these two measures are popular in both machine learning and hydrology applications and because the two measures inform us about distinct aspects of model fit nash sutcliffe efficiency is identical to the familiar coefficient of determination or r2 from a linear regression where yobs is the dependent variable and ysim is treated as the regression prediction nse can be applied to non linear models with a potential range from minus infinity to positive one negative values for nse indicate that the mean of the target is a better predictor of the target than the simulated series the magnitude of rp can range from zero to one measuring how close a linear transformation of the simulation is to the target comparing nse to rp is helpful for illustrating the contrasting properties of these two measures nse effectively benchmarks the simulation against the target s mean and importantly the mean of the target is not known at the time of the simulation further the nse does not center or rescale the simulation series to help it match the target series in contrast rp benefits from a linear transformation of the data i e standardization of both ysim and yobs that uses information about the target series thus the nse conservatively uses information about the target s mean to penalize the measure of a simulation s performance while rp optimistically uses similar information to effectively augment the simulation when assessing its performance 2 3 cn runoff correction modeling as shown in the results below the high values of rp for the average catchment in most sections along with the low nse values for the same catchments could indicate that the ndvi cn runoff series were not very close to the nldas runoff series but that nonetheless the two series contained much of the same information this situation is analogous to comparing measurements taken in the wrong units e g celsius vs fahrenheit accordingly we developed correction models to investigate if we could reliably correct the ndvi cn runoff time series using nldas runoff as the target catchments with fewer than 100 ndvi cn event days were excluded from the analysis to ensure sufficient data across all three temporal splits we use the python version 3 8 programming language to develop software implementing a modeling framework that would allow for a flexible and repeatable analysis of a variety of approaches for generating a set of one or more ndvi cn correction models because our database includes runoff time series for numerous locations characterized as catchments sections provinces and divisions we allow for distinct models to be estimated for each item in a geographic grouping or level e g one conus model or eight physiographic division models etc we refer to this as the geographic modeling scope additionally we allow for models to estimate runoff corrections conditional on a different finer geographic level e g physiographic section within a model of a single physiographic province or a physiographic province within a model of a single physiographic domain etc we refer to this as the geographic modeling level in the results below we set geographic modeling scope to physiographic division and we set the geographic modeling level to physiographic section we develop correction models of ndvi cn ycn to predict the nldas target runoff values ynldas according to y n l d a s t g f g y c n t g g e c n t g where t indexes time in days g indexes the geographic modeling scope of the transformation f g indexes the geographic modeling level i e physiographic section associated with each runoff value and e c n t g is the error for the correction function f we develop the option for employing several statistical regression techniques utilizing pipelines transformers and estimators from python s scikit learn version 0 24 1 machine learning package pedregosa et al 2011 pipelines are a sequence of data transformers and statistical estimators that can be fit to training data to estimate a model that model can then be used to predict with potentially different data pipelines always end with an estimator e g linear regression and may include data transformation steps such as for standardization and creation of interaction and polynomial terms conveniently transformations such as standardization that are estimated during model training are stored for use with future predictions for this paper we developed pipelines implementing the following statistical estimators ols linear regression lin reg lasso regularized linear regression lasso ridge regularized linear regression ridge elastic net regularized linear regression elastic net and a gradient boosting regression tree ensemble gbr each of the pipelines is preceded by a global step where the geographic modeling level is used to create a set of binary dummy variables identifying the geographic membership of each runoff value when creating dummy variables no values were dropped from each level so no constant term was included as a regressor this approach avoids perfect multicollinearity and allows for interpretation of coefficients on dummy variables that does not rely on comparison to an excluded category each of the four linear regression pipelines include polynomial terms for the continuous ndvi cn runoff series interacted with each of the geographic modeling level dummy variables to avoid perfect multicollinearity these polynomial terms are not included as non interacted standalone variables we also developed the option to use nested cross validation to choose the optimal polynomial degree for each model though we did not use that setting in the results presented in this paper in favor of a more geographically specific model selection approach discussed below to maintain the integrity of pipelines we used python objects to create wrappers that use multi indexed pandas dataframes mckinney 2011 to retain information about the comid associated with each corrected or uncorrected runoff value this extra programming step was helpful for maintaining data integrity because the scikit learn estimators and transformers used in the pipelines utilize numpy array objects pedregosa et al 2011 harris et al 2020 when used for cross validation each pipeline wrapper also automatically checks for logs and removes any catchment ids comids used at training to guard against data leakage which could lead to downward biased prediction error estimates and incorrect inference in the model selection process when creating a correction model we have the opportunity for dividing up the data into different models for different values of uncorrected runoff for example because the cn method tends to predict no runoff for days with low values of precipitation the optimal correction model for those values is likely quite different than for days when precipitation is high for this paper we only develop the capability of splitting uncorrected ndvi cn runoff values based on whether they are equal to zero but it would be simple to add other approaches such as a quantile based split the split we chose makes sense particularly because zero and nonzero runoff values have distinct data generating processes due to the event based nature of the cn method in our simple point to point correction method there are two straightforward approaches to correcting zero runoff predictions from the ndvi cn model first we consider the mean of observed runoff conditional on the geographic modeling level and use that value as the correction second we consider an otherwise identical model that retains the uncorrected value of zero labeled as flat0 in the results below the positive ndvi cn values are used to train the pipeline creating a model that can be used for prediction at the time of prediction ndvi cn values are divided into zero and nonzero rows fed into the appropriate model and reassembled into a dataframe with each row indexed by comid and date each of the pipelines we created present several opportunities for hyper parameter tuning we develop the option to use nested cross validation for assessing the predictive performance of all hyper parameter combinations using scikit learn s optimized nested cross validation estimators e g lassocv instead of lasso when available and the scikit learn nested gridsearchcv tool otherwise these nested cross validation estimators utilize repeated k fold cross validation on the training data passed when fitting a pipeline which itself may be part of a broader cross validation assessment the tool chooses the combination of hyperparameters that has the highest average test r2 i e nse across inner cross validation folds and refits the pipeline using those values on the training data passed to the estimator fig 2 illustrates the nested cross validation process for a single leave one catchment out split for a single physiographic section the four comids in each split of nested cross validation in fig 2 correspond to the 4 of 5 catchments for used for training in fig 1 the nested cross validation approach to hyper parameter tuning is computationally intensive at the time of fitting a model because the geographic modeling scope can be broader than the geographic modeling level it may be the case that the best combination of hyper parameters varies from one location level to the next to allow for flexible hyper parameter selection across levels we developed the option for running and selecting from multiple pipelines with varying hyper parameter values this is an alternative to tuning the hyper parameter values through nested cross validation in a single pipeline this alternative approach to handling hyper parameter values leads to a much longer list of models to select from in comparison to hyper parameter tuning with nested cross validation this approach is less computationally intensive at the time of fitting the models and more computationally intensive at the time of testing the models in the ols linear regression and regularized linear regression models presented in the results below we created separate pipelines for each maximum polynomial degree from one to five we used nested cross validation to choose hyper parameters for regularization strength in each pipeline and this particular division balances the computational efficiency of the scikit learn cross validated regularization estimators e g lassocv and the flexibility of separate models for each maximum polynomial degree while double cross validation can produce an approximately unbiased estimate of prediction error choosing from many models the one that seems to have the best prediction error can potentially lead to an optimistic estimate of prediction error because we have eighteen years of data in our runoff database we developed and used in the results below the option for using a separate validation set for reporting the final model accuracy as discussed in the introduction we use the term validation to describe the final assessment of the accuracy of the selected model once the cross validation assessment is complete for each pipeline all results are compared and the software selects for each location in the geographic modeling level the pipeline with the best average leave one catchment out cross validation nse over the test data the uncorrected ndvi cn runoff series is also considered as a candidate in the model selection process there is no temporal or spatial cross validation necessary for the ndvi cn series due to the lack of a correction model but to ensure comparability accuracy is assessed on the same chronological split of test data after model selection and model validation are complete the statistical pipelines associated with each selected model are refit using all of the data in the runoff database to obtain a final model for production use the resulting model has not been validated in a strict sense but the modeling approach as implemented in the statistical pipeline has been validated additionally due to the larger training dataset the refit model is likely to have less bias and variance than the sub models estimated during the cross validation experiment that informs our accuracy assessment presented below 3 results and discussion 3 1 ndvi cn runoff to better understand geographic variability in the accuracy of the uncorrected ndvi cn runoff time series relative to the nldas runoff time series we calculated nse and rp for each catchment then we grouped the sampled catchments by physiographic section and averaged each accuracy measure within each section the resulting maps can be seen in fig 3 because we are not interested in physiographic sections where the model performs worse than a simple average we censored locations with negative average accuracy metrics when shading the maps this leaves more room in the map s polygon fill gradient to facilitate interpretation for physiographic sections where the simulation has some credibility i e nse 0 we also use identical scaling of the fill gradient to the accuracy metrics across all maps in this paper to facilitate comparisons among figures notably the gldas accuracy maps in appendix 1 share their own distinct scaling fig 3 shows overwhelmingly higher values for the averaged rp relative to averaged nse the juxtaposition of high rp and low nse for the same simulated and observed runoff values can be explained by at least two possibilities 1 physiographic sections contain catchments that perform very differently across the models estimated during the leave one catchment out cross validation experiment one large negative nse value for a single catchment in a section can dominate the average value of nse for rp a poor prediction can t be below minus one so a single poorly performing catchment cannot dominate the average rp 2 the ndvi cn runoff simulations correlate with the nldas runoff values similarly across catchments in a physiographic section but the ndvi cn values suffer from a scaling problem this explanation hints at a possibility of correcting this scaling problem to obtain precise automated cn generated runoff predictions 3 2 cn runoff correction modeling for this paper we estimate correction models using physiographic domain as the geographic modeling scope and physiographic section as the geographic modeling level a visual comparison of the validation average nse scores of the leave one catchment out cross validation assessment for each pipeline are presented in fig 4 in this figure physiographic domains are arranged in order of decreasing nse averaged across sections and sections are arranged in order of decreasing nse averaged across catchments each point for each statistical estimator is the validation nse of the best performing model of that type where selection is based on the test data not validation data the numbering of the sorted physiographic domains can be found in table 2 along with the validation nse and model details for the model that scored highest on the test data in fig 4 one of the more remarkable patterns is that there appears to be no relationship between the uncorrected nse and the corrected nse this indicates that ndvi cn generated runoff time series diverge from the nldas runoff time series quite differently across sections even in the same domain fig 4 is also useful for assessing the relative strengths of the different statistical techniques from a machine learning perspective it is interesting that the regularized regression methods are frequently dominated by the ols linear regression models a finer spaced grid of regularization hyper parameters that include smaller values for regularization strength may lead to improved performance however regularization is intended to reduce over fitting and the large number of observations and relatively few parameters in the models may prevent over fitting without regularization penalties fig 5 shows the validation scores for the selected estimator for each physiographic section the nse results provide the best indication of the likely accuracy of future runoff simulations from the models considered in this analysis in this map rp has visibly increased relative to the same measure for uncorrected ndvi cn runoff from fig 3 suggesting accuracy improvements come partially from the estimator learning the nldas series beyond just learning how to rescale the uncorrected ndvi cn runoff values the rp values can rise when a model with a non linear transformation including higher than first degree polynomials is selected or because of the improvement in correlation from the correction for ndvi cn non event predictions of zero by considering the patterns in rp in fig 6 relative to fig 3 both of which benefit from the rescaling inherent to rp the improvement from the transformation of nonzero runoff values can be distinguished from improvements that come from correcting the non event values from zero to the mean of the zero runoff days in the training data the similarity in rp across the correction models suggest a substantial bulk of improvement in rp is due to the non event zero runoff corrections by comparing the nse scores for the best correction models in fig 5 with the nse scores from the first order linear regression models in fig 6 and the original nse scores for the uncorrected ndvi cn runoff series in fig 3 we can see that the bulk of the improvement in accuracy in the correction models is attainable with a simple linear rescaling of the nonzero event values and a simple shift of the non event zero values it is useful to compare the values of rp in fig 3 to the corrected nse values shown in fig 6 the rp benefits from an in sample transformation to account for differences in the mean of each series an inherent part of the rp metric in contrast the coefficients from the first order linear regression correction models also implement a linear transformation but from out of sample relative to the testing and validation splits over which the metrics are calculated training data the rp is also not squared like nse but otherwise the measures are similar and comparing them indicates how well a simple linear correction can generalize to leverage the available information to correct linear scaling problems for future predictions from visual inspection of the accuracy maps it appears physiographic sections with more snowfall or less precipitation tend to perform poorly across the various runoff simulations we conducted because none of the models we developed for this paper use information about time for training or prediction simple augmentations like monthly or bi weekly time dummy variables may increase runoff accuracy particularly in snowy areas with consistent annual runoff patterns fig 7 contains runoff and simulation data for the last year of the validation split for the physiographic section in each physiographic domain with the best corrected runoff nse each of these sections is the first section in each division in table 2 and in fig 4 each pane includes the following three runoff series uncorrected ndvi cn generated best correction model generated and nldas generated for all runoff time series plots in this paper the runoff values on the vertical axes are transformed by taking the natural logarithm of one plus runoff the logarithmic transformation makes it easier to see patterns at both low and high values of runoff and adding 1 prior to the logarithmic transformation keeps runoff values of zero at zero fig 7 is useful for developing an understanding of the overall behavior of the runoff series across the diverse physiographic domains the impact of poor event detection is particularly visible in the bottom two series where the nonzero correction model predictions are markedly above zero it is also readily apparent from several panes that the correction models reduce the frequency of dramatic runoff overpredictions generated by ndvi cn to better distinguish between runoff performance during ndvi cn nonzero runoff events we created separate runoff plots spanning the validation temporal split for ndvi cn events with nonzero uncorrected simulated runoff and ndvi cn non events with zero uncorrected simulated runoff fig 8 shows only the event days when ndvi cn predicts positive runoff across the top of each pane in the figure is an index that numbers the days of nonzero ndvi cn runoff in each physiographic division it is helpful to consider these same nonzero runoff values but ordered by ascending observed runoff i e nldas runoff as illustrated in fig 9 here the reader can see patterns of under or overprediction of the final corrected runoff models which can be used to validate or invalidate the use of these models for various real world decision making applications it is interesting to compare the runoff predictions for the mississippi alluvial plain map and the arkansas valley av the top 2 panes in fig 9 as can be seen in table 2 both sections have high nse values and the selected model for the map is a first order linear regression while the av correction is a 3rd order linear regression for the days in the av with the highest runoff the polynomial correction appears to help achieve a close fit while for the map the corrected runoff values seem to have a downward bias when observed runoff is highest the av has nearly twice as many observations which may be important for estimating higher order polynomial terms with sufficient precision to enhance predictive accuracy over simpler models as indicated by table 2 the plains border section uses gbr as the selected correction estimator a close examination of the differences between the uncorrected runoff values and corrected runoff values in fig 9 reveals several instances of non monotonic transformations where ndvi cn runoff falls and corrected runoff falls but then rises this can be seen around day 80 for example in the same figure the lower californian section with relatively few ndvi cn nonzero event days shows a marked improvement in accuracy at high runoff values while reducing the variability of ndvi cn runoff this last pattern a reduction in corrected runoff variability relative to ncvi cn variability is the most discernible feature of the first five panes in fig 9 to better understand the days when ndvi cn predicts zero runoff we also developed fig 10 showing the ndvi cn zero runoff days in the validation split it is important to note that the vertical scale on these graphs varies widely the ndvi cn method fails to detect substantial runoff events in a seasonal pattern in nldas runoff in the superior upland and northern rocky mountains physiographic sections there also are likely statistically significant seasonal patterns in the ndvi cn zero runoff days that could be addressed by adding additional complexity to the zero runoff correction models while the validation and estimation framework we used for developing the ndvi cn correction models is complex the underlying models are relatively simple because they lack an awareness of time broadening the information set available for prediction in both the zero runoff and nonzero runoff models to include past time periods would potentially overcome limitations associated with the event based nature of ndvi cn including precipitation and lagged precipitation similarly would likely provide opportunities for increasing model skill variables for seasonality or more sophisticated time series approaches such as wavelets also would likely increase model accuracy particularly for locations with substantial snow melt and accumulation however in the context of the curve number methodology a simple and effective linear correction is particularly appealing in these results we considered only a narrow range of the possibilities for grouping the data by selecting a broad geographic modeling scope physiographic division and a narrow geographic modeling level physiographic section at the same time we used a short list of explanatory variables so the structure of the matrix of regressors is block diagonal and thus relatively little information is shared between physiographic sections in each physiographic division for use by the statistical estimators more complex structures such as overlapping dummy variables lim and hastie 2015 and geographic regressors such as those in streamcat may help identify stronger patterns in the data a finer selection for the geographic modeling scope would also potentially improve model accuracy by reducing the tendency to lump together physiographic sections or more generally slices with contrasting snow fall melt patterns 4 conclusions by developing and applying a carefully designed model estimation selection and accuracy assessment framework we have developed correction models to enhance ndvi cn rainfall runoff model the result of our accuracy assessment is a set of validation nse values that can be used by practitioners who need runoff time series estimates to appropriately curate their data sources and quantify sources of error in downstream modeling applications because the curve number approach to runoff modeling is one of the simplest and least data intensive approaches it is fascinating that runoff estimated using a somewhat inflexible automated approach to quantifying hydrologic condition i e ndvi cn has such high linear correlation with an ensemble of state of the art lsms further for much of the country this relationship is stable and can be leveraged into simple first order linear regression correction models with skillful predictions as judged by nse and illustrated in fig 6 during this research we became aware of the importance of including a simple metric like rp to contrast more stringent accuracy measures like nse or kge while time series plots of simulated and observed runoff can help the analyst spot information patterns that can be leveraged to build correction models it is useful to have a metric that quantifies these patterns because rp r2 and nse have so much in common there may be a need for a generalized version of rp in the same manner that kge generalizes nse non linear correlation measures like spearman s rank order correlation coefficient may also be useful for spotting non linear patterns in the data disclaimer this article has been reviewed by the u s environmental protection agency and approved for publication mention of trade names or commercial products does not constitute endorsement or recommendation for use by the u s government the views expressed in this article are those of the authors and do not necessarily reflect the views or policies of the u s epa notation rp pearson s correlation coefficient nse nash sutcliffe efficiency r2 coefficient of determination declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported in part by an appointment to the orise fellowship program at the u s epa office of research and development athens ga administered by the oak ridge institute for science and education through interagency agreement no dw8992298301 between the u s department of energy and the u s environmental protection agency appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105321 
25654,we developed statistical models to generate runoff time series at national hydrography dataset plus version 2 nhdplusv2 catchment scale for the continental united states conus the models use normalized difference vegetation index ndvi based curve number cn to generate initial runoff time series which then is corrected using statistical models to improve accuracy we used the north american land data assimilation system 2 nldas 2 catchment scale runoff time series as the reference data for model training and validation we used 17 years of 16 day 250 m resolution ndvi data as a proxy for hydrologic conditions during a representative year to calculate 23 ndvi based cn ndvi cn values for each of 2 65 million nhdplusv2 catchments for the contiguous u s to maximize predictive accuracy while avoiding optimistically biased model validation results we developed a spatio temporal cross validation framework for estimating selecting and validating the statistical correction models we found that in many of the physiographic sections comprising conus even simple linear regression models were highly effective at correcting ndvi cn runoff to achieve nash sutcliffe efficiency values above 0 5 however all models showed poor performance in physiographic sections that experience significant snow accumulation data availability statement statement the code used to develop this paper and the appendix can be found at the following sites https github com quanted hms handler curve number and ndvi dataset can be found at the following site ftp newftp epa gov exposure curvenumberndvi average curve number dataset for nhdplus catchments can be found at the following site https qed epa gov hms rest api info catchment cn true comid comid where comid is nhdplusv2 catchment id 1 introduction effective management of hydrologic resources and hazards often depends on accurate simulations of runoff for example runoff time series can be combined with other environmental data to characterize how a system responds to various climate and land use scenarios to facilitate the work of researchers and managers seeking to understand and manage hydrologic systems we developed an automated curve number cn based technique for estimating catchment level runoff that allows for the use of either simulated or historical data for precipitation and landcover to facilitate validation of the models developed in this paper for various applications we designed and implemented a machine learning accuracy assessment framework that withholds validation data from statistical model training both spatially and temporally to build confidence that the resulting accuracy measures truly characterize how the models generalize to other catchments and time periods to implement the accuracy assessment this paper presents a machine learning framework for a state of the art approximately unbiased approach to quantifying predictive accuracy in a hydrologic spatio temporal context using an existing automated technique for quantifying hydrologic condition muche et al 2019a 2019b we generated nhdplusv2 catchment scale cns for conus and applied the framework to estimate and evaluate a variety of relatively simple cn generated runoff time series correction models that often dramatically improve runoff accuracy additionally because the technique we employed for automating cn generation adheres to the conventional cn approach we expect that the correction models are likely to enhance the accuracy of runoff time series generated by any of the variants of cn such as the recent gcn250 jaafar et al 2019 1 1 hydrologic runoff modeling a variety of research topics involve data modeling that requires information about how precipitation patterns translate into measures such as runoff and streamflow historical runoff estimates utilizing a robust set of environmental forcing variables have been made readily available through the north american land data assimilation system nldas xia et al 2013 and the global land data assimilation system gldas rodell et al 2004 land surface model lsm projects for example historical data are useful for assessing and training models but these projects do not provide a means for simulating runoff in counterfactual or future conditions a variety of approaches have been developed to estimate the relationship between hydrologically relevant environmental variables such as precipitation and runoff sitterson et al 2017 provide a taxonomy of rainfall runoff models based primarily on the correspondence of the model with physical reality and spatial resolution at one end of the spectrum the cn methods of runoff modeling offer analysts one of the simplest approaches to runoff modeling at the other end of the spectrum are multi input multi output lsms such as nldas 2 henceforth referred to as nldas which itself is an ensemble of lsms xia et al 2012a 2012b other runoff modeling approaches include the geomorphological instantaneous unit hydrograph approach a more recent and a more technically sophisticated approach to rainfall runoff modeling rigon et al 2016 recently oppel and schumann 2020 applied machine learning estimators to explore transferability of geomorphological instant unit hydrograph runoff models between catchments based on catchment characteristics and a basin classification scheme derived from their models fractal geometry has been applied to modeling surface runoff gires et al 2018 another group of runoff models is the gr chain ficchì et al 2019 that vary in temporal resolution which ficchì et al 2019 extended to include flux matching criteria the soil conservation service curve number also widely referred to as the curve number method was developed by the united states department of agriculture usda in the 1950s to predict direct runoff from rainfall events and it is a widely adopted method in surface runoff estimation hawkins et al 2008 hawkins 2014 lian et al 2020 the method was developed using measured rainfall and runoff data from several agricultural research watersheds primarily in the eastern midwestern and southern u s the rainfall runoff relationship in the study watersheds was extrapolated to an empirical number the curve number using land use cover hydrologic soil groups and hydrologic conditions of watersheds hawkins et al 2008 muche et al 2019a rallison 1980 the cn method has been globally adapted to areas with varying land use cover soil properties and climatic conditions it has also been incorporated into various continuous hydrologic watershed models though the method was originally devised for event based rainfall runoff modeling garen and moore 2005 hawkins 1996 kennan et al 2007 muche et al 2019a despite wide applications of the cn method some watersheds have been found to exhibit significant differences between observed and predicted runoff using the cn method hawkins 2014 muche et al 2019a the effects of rainfall volume intensity and frequency muche et al 2019a wang and bi 2020 in addition to the seasonality of rainfall runoff relationship could be among the contributing factors to the cn method s low accuracy in those watersheds rodríguez blanco et al 2012 to increase the accuracy of cn generated runoff silveira et al 2000 incorporated automated estimation of antecedent moisture using five days of lagged rainfall recently advancements in geographic information science gis created the opportunity to account for seasonality in the rainfall runoff relationship by using remote sensed data to flexibly approximate hydrologic condition muche 2016 nasiri and alipur 2014 singh and khare 2021 moderate resolution imaging spectroradiometer normalized difference vegetation index modis ndvi were applied to cn estimation by several authors gandini and usunoff 2004 muche et al 2019a 2019b nasiri and alipur 2014 singh and khare 2021 muche et al 2019a used modis ndvi to estimate cn using 12 years of observed rainfall and runoff at four small watersheds in the konza prairie long term ecological research site muche et al 2019b extended this work using modis ndvi for catchment level cn development spanning conus as part of usepa s hydrologic micro services hms computational platform which is used in the results below 1 2 model validation and selection validation is generally regarded as an important step in modeling though it is not clear what exactly is meant by validation and what one must do to achieve it schlesinger 1979 defined validation for computerized simulations only in terms of comparison to reality in the domain of applicability schruben 1980 discussed simulation credibility as a more practical standard to meet than strict validation requiring simulation output to be indistinguishable from observations of reality by a human in the same manner as a turing test for artificial human intelligence sargent 2013 discusses validation broadly for empirical studies and describes several validation techniques including inner validation as an assessment process using data resampling and historical data validation as a process of splitting data into building and testing sets klemeš 1986 offers an early and widely cited guide to validation of hydrologic models that generally corresponds well with modern machine learning approaches to model validation biondi et al 2012 review and refine validation concepts in hydrology and they discuss performance or model validation which includes qualitative assessment of graphs and quantitative assessments of model metrics on split samples they also discuss a distinct type of validation scientific validation wherein one considers the theoretical underpinnings of the model common machine learning terminology contrasts with the above uses of validation hastie et al 2017 uses the validation set for model selection and a final testing set for quantifying generalization error which is called validation in the above contexts other than machine learning in machine learning a great deal of attention is paid to using validation like metrics for both model selection and validation typically with distinct treatments of data common statistical approaches to model selection that rely on error estimates from training observations can lead to substantial downward bias in error metrics leading to overly optimistic conclusions about model accuracy picard and cook 1984 cross validation approaches to model validation split a dataset into n folds use n 1 of the folds to train a model quantify predictive accuracy on the withheld fold and cycle through all n folds generating an empirical distribution of model accuracy that approximates expected prediction error hastie et al 2017 p254 hawkins et al 2003 favor cross validation approaches to model selection and validation rather than a single hold out test set because cross validation balances the desire for more data with the need for quantifying predictive accuracy however cross validation when done incorrectly can still lead to biased performance measures and inferior model selection cawley and talbot 2010 hastie et al 2017 p245 guyon et al 2010 provide a useful discussion of the nested relationship between the two or more types of parameters in the context of diverse statistical learners in this process of multi level inference that guyon et al 2010 describe parameters are chosen in an inner estimation step by analytically efficient learning algorithms e g linear algebra solutions for ordinary least squares ols linear regression and hyper parameters are chosen by repeatedly invoking the efficient algorithms with different hyper parameter values for multi level inference approaches cross validation serves as the framework for each level of inference to avoid over fitting cawley and talbot 2010 fushiki 2011 found for regression problems that n fold cross validation may bias prediction error upwards while training error is a downward biased estimate of prediction error hastie et al 2017 p254 characterize 10 fold cross validation as an approximately unbiased means of quantifying expected or extra sample error techniques such as 5 fold and 10 fold cross validation are computationally efficient approaches for quantifying prediction error with generally lower variance than leave one out cross validation hastie et al 2017 p255 in classification problems n fold cross validation has exhibited reduced bias and computational complexity relative to the bootstrap kim 2009 an alternative to cross validation time series datasets require additional assumptions to justify cross validation more conservative approaches to validation of time series models typically require withholding later observations in the dataset from training for testing the model s performance with new data this process has been called last block validation bergmeir and benítez 2012 or out of sample evaluation bergmeir et al 2018 recent advances have opened up possibilities for efficient cross validation with some time series estimators which is particularly adventitious for small datasets bergmeir et al 2018 because all data can be used for validation data can be grouped for modeling and assessment in a variety of ways in machine learning the term slice chung et al 2019 refers to divisions in the data by predictor variables that can be used to assess model performance with greater granularity validation metrics that group slices together can potentially obscure poorly performing slices chung et al 2019 the idea of assessing slice performance is closely related to the idea of transportability in hydrology as discussed by klemeš 1986 who described an early cross validation like testing procedure for detecting poor performing members of a group of catchments to validate simulations in ungauged members of the group the model scoring or loss metric also plays an important role in model selection and validation gupta et al 2009 applied and refined decompositions of the popular nash sutcliffe efficiency nse model scoring metric in the context of runoff simulations criticizing models optimized with nse as the score for being of use only in normal conditions this problem can be remediated to some extent by their proposed kling gupta efficiency kge metrics gupta et al 2009 knoben et al 2019 point out interpretation issues associated with several parametric and non parametric variants of kge they emphasize a lack of a clear benchmark or cutoff value with kge metrics while the nse value of zero benchmarks simulation performance against the mean of the observed series 1 3 corrective modeling models that correct simulations have been developed extensively in the earth sciences watson 2019 discusses the tradeoffs associated with physical versus purely data driven approaches to increasing predictive accuracy at short and long timescales in the context of climate simulations dinge et al 2019 distinguish between point to point correction models and models that use time series characteristics to increase performance in the context of applying error correction models to wind speed prediction zjavka 2015 applies a polynomial neural network to correct wind speed using lagged measures of nearby environmental variables 1 4 regression prediction we use a broad definition of regression from hastie et al 2017 p10 which encompasses any statistical learner that makes quantitative predictions in practice regression models have a continuous dependent variable in contrast to classification and ordered categorical models with discrete dependent variables accordingly the linear regressions regularized linear regressions and gradient boosting ensembles are all referred to as regressors or regression models regularized linear regression estimators share similarities with ols but with additional structure to reduce the variability increase the stability of the parameter estimates at the expense of increased bias frank and friedman 1993 the lasso regularized regression effectively selects predictor variables pushing some coefficient estimates to zero while the ridge regularized regression tends to push regression coefficients towards equality with each other tibshirani 1996 the group lasso was developed to select groups of dummy variables for multi category predictors and extensions to the group lasso have been developed to preserve hierarchal connection between interaction and main effects in lasso regression models lim and hastie 2015 in contrast to ols the basic lasso estimator can more generally estimate flexible dummy variable specifications with overlapping categories as discussed in lim and hastie 2015 the elastic net regressor combines the strengths of the lasso and ridge regression estimators with the ability to model high collinearity among variables like ridge and the ability to do variable selection like lasso zou and hastie 2005 the gradient boosting regressor is an extension of the gradient boosting classifier and has been described in detail by friedman 2001 and by hastie et al 2017 the algorithm fits an additive sequence of simple regression tree estimators with the gradient of the previous estimator s loss function used as the dependent variable for training the subsequent tree in an iterative procedure according to guyon et al 2010 boosting methods of regression are less vulnerable to overfitting because they minimize a guaranteed risk function hastie et al 2017 p340 quote others in describing the classifier version of gradient boosting as the best off the shelf classifier in the world 2 methods 2 1 ndvi based automated curve number development the primary challenge in automating the generation of runoff time series using the curve number method is the selection of the hydrologic condition the hydrologic condition functions as a categorical variable taking into consideration several possible influencing factors mainly related to land cover type at the time of precipitation event the customary approach to specifying hydrologic condition requires site specific expert analysis that hinders scaling the approach to larger areas remote sensing data has been shown to be a viable approach to specifying hydrologic condition facilitating automated estimation of cn values we followed the work of muche et al 2019a 2019b and used 250 m 16 day resolution modis ndvi didan 2015 data along with land cover and soil data to quantify hydrologic condition and create a time series of twenty three cn values spanning an average year for each of approximately 2 65 million nhdplusv2 catchments in conus to compute these numerous cn values we first needed to quantify the corresponding hydrologic condition we used google earth engine to obtain and spatially average seventeen years 2001 2017 of modis ndvi satellite raster data to the nhdplusv2 catchment next for each catchment and each of twenty three annual sixteen day timesteps we temporally averaged the 17 observations of spatially averaged ndvi we then used these time and space averaged ndvi values along with nlcd land cover data discussed next for each catchment and time period to determine the hydrologic condition as poor normal or good based on the ranges specified in table 1 we obtained catchment level nlcd 2011 land cover data and statsgo derived sand and clay soil composition percentages from the epa streamcat dataset hill et al 2016 we used the statsgo percentages to determine the hydrologic soil group of each catchment finally for each timestep and each catchment we used the land cover hydrologic soil group and hydrologic condition values along with the usda s soil conservation service curve number tables to obtain ndvi cn values for each catchment and each of the 23 annual 16 day time periods for each catchment and each timestep the ndvi and cn values as well as annual average cn values can be obtained at ftp newftp epa gov exposure curvenumberndvi additionally the spatially and temporally averaged ndvi data can be obtained for each catchment at https qed epa gov hms rest api info catchment cn true comid comid where comid is replaced by a nhdplus catchment id e g https qed epa gov hms rest api info catchment cn true comid 331416 2 2 accuracy assessment we used the cn values described above to develop a runoff database to investigate the accuracy of ndvi cn generated daily runoff using nldas runoff as the target for the spatial units in our database we randomly selected 5 nhdplus catchments in each united states geologic survey usgs physiographic section fenneman and johnson 1946 next we retrieved 17 years of nldas runoff data and ndvi cn runoff data forced by nldas precipitation data for each catchment we also retrieved the gldas runoff and ndvi cn runoff forced by gldas precipitation data and present parallel condensed results based on that data in appendix 1 in the discussion of model selection and validation below we follow the bulk of the empirical literature and reserve the term validation to describe the final split of data that is not used for any kind of model selection in this paper but only for reporting a final estimate of predictive accuracy this decision contrasts with recent trends in machine learning research where validation data are used for model selection and testing data are used for quantifying predictive accuracy of the selected model e g hastie et al 2017 we use the term validate or validation to characterize the final chronological split of data as well as the subsequent accuracy analysis this validation stage provides information not used for model selection in this paper but that is developed for use by end users who require information about expected predictive accuracy we use the term test or testing more generically to refer to any accuracy assessment such as those performed during model selection validation is thus a special kind of model testing deliberately designed to avoid data leakage that can optimistically bias results we adopt this terminology for consistency with the hydrologic literature and a number of other empirical sciences as well the purpose of assessing the accuracy of simulated runoff relative to the target runoff is to provide information to users about the likely quality of future simulations that may include times and locations not present in our database to achieve this we develop a validation approach with a resampling design based on holding out observations for testing based on both time and space first because some of our runoff simulation models use lengthy time series of runoff data for training we employ a traditional three part temporal splitting of each time series as can be seen in fig 1 the first half of a runoff time series is reserved for training the models and the second half is split into a testing series used for final model selection and a validation series used for quantifying predictive accuracy this approach helps ensure that the validation accuracy assessment of the models generalizes to other time periods particularly the near future this type of validation is an example of what klemeš 1986 called a split sample test second for the runoff correction models that we develop below there is also a potential concern about the ability of the accuracy metrics to generalize to catchments excluded from model training such as those not in the runoff database accordingly we evaluate predictive accuracy of each correction model using the average of a leave one catchment out of each section repeated cross validation approach as shown in fig 2 the splitting algorithm we developed takes each physiographic section and places the five sampled catchments in a list for a single repetition of cross validation each list is shuffled and then the first catchment in each list is excluded from the training data of a sub model and reserved for testing that model the next training test split comes from excluding the second catchment in each list and so on until each of the 5 catchments in each section have been reserved from training a model and used for testing and validation of that model this procedure is repeated to ensure the accuracy metric does not depend on any patterns in the test data from a single shuffling of the catchments in a physiographic section this approach to geographic data splitting is in addition to the single chronological split discussed above and is an example of what klemeš 1986 referred to as a proxy basin test the result is that no catchment or time period is ever present in both training and test or validation data for any of the prediction accuracy measures that we report the simulated runoff series and the target or observed runoff series yobs we are comparing are continuous and non negative allowing for a wide range of accuracy measures based on differences or similarities between the two series we select pearson s correlation coefficient rp and nash sutcliffe efficiency nse because these two measures are popular in both machine learning and hydrology applications and because the two measures inform us about distinct aspects of model fit nash sutcliffe efficiency is identical to the familiar coefficient of determination or r2 from a linear regression where yobs is the dependent variable and ysim is treated as the regression prediction nse can be applied to non linear models with a potential range from minus infinity to positive one negative values for nse indicate that the mean of the target is a better predictor of the target than the simulated series the magnitude of rp can range from zero to one measuring how close a linear transformation of the simulation is to the target comparing nse to rp is helpful for illustrating the contrasting properties of these two measures nse effectively benchmarks the simulation against the target s mean and importantly the mean of the target is not known at the time of the simulation further the nse does not center or rescale the simulation series to help it match the target series in contrast rp benefits from a linear transformation of the data i e standardization of both ysim and yobs that uses information about the target series thus the nse conservatively uses information about the target s mean to penalize the measure of a simulation s performance while rp optimistically uses similar information to effectively augment the simulation when assessing its performance 2 3 cn runoff correction modeling as shown in the results below the high values of rp for the average catchment in most sections along with the low nse values for the same catchments could indicate that the ndvi cn runoff series were not very close to the nldas runoff series but that nonetheless the two series contained much of the same information this situation is analogous to comparing measurements taken in the wrong units e g celsius vs fahrenheit accordingly we developed correction models to investigate if we could reliably correct the ndvi cn runoff time series using nldas runoff as the target catchments with fewer than 100 ndvi cn event days were excluded from the analysis to ensure sufficient data across all three temporal splits we use the python version 3 8 programming language to develop software implementing a modeling framework that would allow for a flexible and repeatable analysis of a variety of approaches for generating a set of one or more ndvi cn correction models because our database includes runoff time series for numerous locations characterized as catchments sections provinces and divisions we allow for distinct models to be estimated for each item in a geographic grouping or level e g one conus model or eight physiographic division models etc we refer to this as the geographic modeling scope additionally we allow for models to estimate runoff corrections conditional on a different finer geographic level e g physiographic section within a model of a single physiographic province or a physiographic province within a model of a single physiographic domain etc we refer to this as the geographic modeling level in the results below we set geographic modeling scope to physiographic division and we set the geographic modeling level to physiographic section we develop correction models of ndvi cn ycn to predict the nldas target runoff values ynldas according to y n l d a s t g f g y c n t g g e c n t g where t indexes time in days g indexes the geographic modeling scope of the transformation f g indexes the geographic modeling level i e physiographic section associated with each runoff value and e c n t g is the error for the correction function f we develop the option for employing several statistical regression techniques utilizing pipelines transformers and estimators from python s scikit learn version 0 24 1 machine learning package pedregosa et al 2011 pipelines are a sequence of data transformers and statistical estimators that can be fit to training data to estimate a model that model can then be used to predict with potentially different data pipelines always end with an estimator e g linear regression and may include data transformation steps such as for standardization and creation of interaction and polynomial terms conveniently transformations such as standardization that are estimated during model training are stored for use with future predictions for this paper we developed pipelines implementing the following statistical estimators ols linear regression lin reg lasso regularized linear regression lasso ridge regularized linear regression ridge elastic net regularized linear regression elastic net and a gradient boosting regression tree ensemble gbr each of the pipelines is preceded by a global step where the geographic modeling level is used to create a set of binary dummy variables identifying the geographic membership of each runoff value when creating dummy variables no values were dropped from each level so no constant term was included as a regressor this approach avoids perfect multicollinearity and allows for interpretation of coefficients on dummy variables that does not rely on comparison to an excluded category each of the four linear regression pipelines include polynomial terms for the continuous ndvi cn runoff series interacted with each of the geographic modeling level dummy variables to avoid perfect multicollinearity these polynomial terms are not included as non interacted standalone variables we also developed the option to use nested cross validation to choose the optimal polynomial degree for each model though we did not use that setting in the results presented in this paper in favor of a more geographically specific model selection approach discussed below to maintain the integrity of pipelines we used python objects to create wrappers that use multi indexed pandas dataframes mckinney 2011 to retain information about the comid associated with each corrected or uncorrected runoff value this extra programming step was helpful for maintaining data integrity because the scikit learn estimators and transformers used in the pipelines utilize numpy array objects pedregosa et al 2011 harris et al 2020 when used for cross validation each pipeline wrapper also automatically checks for logs and removes any catchment ids comids used at training to guard against data leakage which could lead to downward biased prediction error estimates and incorrect inference in the model selection process when creating a correction model we have the opportunity for dividing up the data into different models for different values of uncorrected runoff for example because the cn method tends to predict no runoff for days with low values of precipitation the optimal correction model for those values is likely quite different than for days when precipitation is high for this paper we only develop the capability of splitting uncorrected ndvi cn runoff values based on whether they are equal to zero but it would be simple to add other approaches such as a quantile based split the split we chose makes sense particularly because zero and nonzero runoff values have distinct data generating processes due to the event based nature of the cn method in our simple point to point correction method there are two straightforward approaches to correcting zero runoff predictions from the ndvi cn model first we consider the mean of observed runoff conditional on the geographic modeling level and use that value as the correction second we consider an otherwise identical model that retains the uncorrected value of zero labeled as flat0 in the results below the positive ndvi cn values are used to train the pipeline creating a model that can be used for prediction at the time of prediction ndvi cn values are divided into zero and nonzero rows fed into the appropriate model and reassembled into a dataframe with each row indexed by comid and date each of the pipelines we created present several opportunities for hyper parameter tuning we develop the option to use nested cross validation for assessing the predictive performance of all hyper parameter combinations using scikit learn s optimized nested cross validation estimators e g lassocv instead of lasso when available and the scikit learn nested gridsearchcv tool otherwise these nested cross validation estimators utilize repeated k fold cross validation on the training data passed when fitting a pipeline which itself may be part of a broader cross validation assessment the tool chooses the combination of hyperparameters that has the highest average test r2 i e nse across inner cross validation folds and refits the pipeline using those values on the training data passed to the estimator fig 2 illustrates the nested cross validation process for a single leave one catchment out split for a single physiographic section the four comids in each split of nested cross validation in fig 2 correspond to the 4 of 5 catchments for used for training in fig 1 the nested cross validation approach to hyper parameter tuning is computationally intensive at the time of fitting a model because the geographic modeling scope can be broader than the geographic modeling level it may be the case that the best combination of hyper parameters varies from one location level to the next to allow for flexible hyper parameter selection across levels we developed the option for running and selecting from multiple pipelines with varying hyper parameter values this is an alternative to tuning the hyper parameter values through nested cross validation in a single pipeline this alternative approach to handling hyper parameter values leads to a much longer list of models to select from in comparison to hyper parameter tuning with nested cross validation this approach is less computationally intensive at the time of fitting the models and more computationally intensive at the time of testing the models in the ols linear regression and regularized linear regression models presented in the results below we created separate pipelines for each maximum polynomial degree from one to five we used nested cross validation to choose hyper parameters for regularization strength in each pipeline and this particular division balances the computational efficiency of the scikit learn cross validated regularization estimators e g lassocv and the flexibility of separate models for each maximum polynomial degree while double cross validation can produce an approximately unbiased estimate of prediction error choosing from many models the one that seems to have the best prediction error can potentially lead to an optimistic estimate of prediction error because we have eighteen years of data in our runoff database we developed and used in the results below the option for using a separate validation set for reporting the final model accuracy as discussed in the introduction we use the term validation to describe the final assessment of the accuracy of the selected model once the cross validation assessment is complete for each pipeline all results are compared and the software selects for each location in the geographic modeling level the pipeline with the best average leave one catchment out cross validation nse over the test data the uncorrected ndvi cn runoff series is also considered as a candidate in the model selection process there is no temporal or spatial cross validation necessary for the ndvi cn series due to the lack of a correction model but to ensure comparability accuracy is assessed on the same chronological split of test data after model selection and model validation are complete the statistical pipelines associated with each selected model are refit using all of the data in the runoff database to obtain a final model for production use the resulting model has not been validated in a strict sense but the modeling approach as implemented in the statistical pipeline has been validated additionally due to the larger training dataset the refit model is likely to have less bias and variance than the sub models estimated during the cross validation experiment that informs our accuracy assessment presented below 3 results and discussion 3 1 ndvi cn runoff to better understand geographic variability in the accuracy of the uncorrected ndvi cn runoff time series relative to the nldas runoff time series we calculated nse and rp for each catchment then we grouped the sampled catchments by physiographic section and averaged each accuracy measure within each section the resulting maps can be seen in fig 3 because we are not interested in physiographic sections where the model performs worse than a simple average we censored locations with negative average accuracy metrics when shading the maps this leaves more room in the map s polygon fill gradient to facilitate interpretation for physiographic sections where the simulation has some credibility i e nse 0 we also use identical scaling of the fill gradient to the accuracy metrics across all maps in this paper to facilitate comparisons among figures notably the gldas accuracy maps in appendix 1 share their own distinct scaling fig 3 shows overwhelmingly higher values for the averaged rp relative to averaged nse the juxtaposition of high rp and low nse for the same simulated and observed runoff values can be explained by at least two possibilities 1 physiographic sections contain catchments that perform very differently across the models estimated during the leave one catchment out cross validation experiment one large negative nse value for a single catchment in a section can dominate the average value of nse for rp a poor prediction can t be below minus one so a single poorly performing catchment cannot dominate the average rp 2 the ndvi cn runoff simulations correlate with the nldas runoff values similarly across catchments in a physiographic section but the ndvi cn values suffer from a scaling problem this explanation hints at a possibility of correcting this scaling problem to obtain precise automated cn generated runoff predictions 3 2 cn runoff correction modeling for this paper we estimate correction models using physiographic domain as the geographic modeling scope and physiographic section as the geographic modeling level a visual comparison of the validation average nse scores of the leave one catchment out cross validation assessment for each pipeline are presented in fig 4 in this figure physiographic domains are arranged in order of decreasing nse averaged across sections and sections are arranged in order of decreasing nse averaged across catchments each point for each statistical estimator is the validation nse of the best performing model of that type where selection is based on the test data not validation data the numbering of the sorted physiographic domains can be found in table 2 along with the validation nse and model details for the model that scored highest on the test data in fig 4 one of the more remarkable patterns is that there appears to be no relationship between the uncorrected nse and the corrected nse this indicates that ndvi cn generated runoff time series diverge from the nldas runoff time series quite differently across sections even in the same domain fig 4 is also useful for assessing the relative strengths of the different statistical techniques from a machine learning perspective it is interesting that the regularized regression methods are frequently dominated by the ols linear regression models a finer spaced grid of regularization hyper parameters that include smaller values for regularization strength may lead to improved performance however regularization is intended to reduce over fitting and the large number of observations and relatively few parameters in the models may prevent over fitting without regularization penalties fig 5 shows the validation scores for the selected estimator for each physiographic section the nse results provide the best indication of the likely accuracy of future runoff simulations from the models considered in this analysis in this map rp has visibly increased relative to the same measure for uncorrected ndvi cn runoff from fig 3 suggesting accuracy improvements come partially from the estimator learning the nldas series beyond just learning how to rescale the uncorrected ndvi cn runoff values the rp values can rise when a model with a non linear transformation including higher than first degree polynomials is selected or because of the improvement in correlation from the correction for ndvi cn non event predictions of zero by considering the patterns in rp in fig 6 relative to fig 3 both of which benefit from the rescaling inherent to rp the improvement from the transformation of nonzero runoff values can be distinguished from improvements that come from correcting the non event values from zero to the mean of the zero runoff days in the training data the similarity in rp across the correction models suggest a substantial bulk of improvement in rp is due to the non event zero runoff corrections by comparing the nse scores for the best correction models in fig 5 with the nse scores from the first order linear regression models in fig 6 and the original nse scores for the uncorrected ndvi cn runoff series in fig 3 we can see that the bulk of the improvement in accuracy in the correction models is attainable with a simple linear rescaling of the nonzero event values and a simple shift of the non event zero values it is useful to compare the values of rp in fig 3 to the corrected nse values shown in fig 6 the rp benefits from an in sample transformation to account for differences in the mean of each series an inherent part of the rp metric in contrast the coefficients from the first order linear regression correction models also implement a linear transformation but from out of sample relative to the testing and validation splits over which the metrics are calculated training data the rp is also not squared like nse but otherwise the measures are similar and comparing them indicates how well a simple linear correction can generalize to leverage the available information to correct linear scaling problems for future predictions from visual inspection of the accuracy maps it appears physiographic sections with more snowfall or less precipitation tend to perform poorly across the various runoff simulations we conducted because none of the models we developed for this paper use information about time for training or prediction simple augmentations like monthly or bi weekly time dummy variables may increase runoff accuracy particularly in snowy areas with consistent annual runoff patterns fig 7 contains runoff and simulation data for the last year of the validation split for the physiographic section in each physiographic domain with the best corrected runoff nse each of these sections is the first section in each division in table 2 and in fig 4 each pane includes the following three runoff series uncorrected ndvi cn generated best correction model generated and nldas generated for all runoff time series plots in this paper the runoff values on the vertical axes are transformed by taking the natural logarithm of one plus runoff the logarithmic transformation makes it easier to see patterns at both low and high values of runoff and adding 1 prior to the logarithmic transformation keeps runoff values of zero at zero fig 7 is useful for developing an understanding of the overall behavior of the runoff series across the diverse physiographic domains the impact of poor event detection is particularly visible in the bottom two series where the nonzero correction model predictions are markedly above zero it is also readily apparent from several panes that the correction models reduce the frequency of dramatic runoff overpredictions generated by ndvi cn to better distinguish between runoff performance during ndvi cn nonzero runoff events we created separate runoff plots spanning the validation temporal split for ndvi cn events with nonzero uncorrected simulated runoff and ndvi cn non events with zero uncorrected simulated runoff fig 8 shows only the event days when ndvi cn predicts positive runoff across the top of each pane in the figure is an index that numbers the days of nonzero ndvi cn runoff in each physiographic division it is helpful to consider these same nonzero runoff values but ordered by ascending observed runoff i e nldas runoff as illustrated in fig 9 here the reader can see patterns of under or overprediction of the final corrected runoff models which can be used to validate or invalidate the use of these models for various real world decision making applications it is interesting to compare the runoff predictions for the mississippi alluvial plain map and the arkansas valley av the top 2 panes in fig 9 as can be seen in table 2 both sections have high nse values and the selected model for the map is a first order linear regression while the av correction is a 3rd order linear regression for the days in the av with the highest runoff the polynomial correction appears to help achieve a close fit while for the map the corrected runoff values seem to have a downward bias when observed runoff is highest the av has nearly twice as many observations which may be important for estimating higher order polynomial terms with sufficient precision to enhance predictive accuracy over simpler models as indicated by table 2 the plains border section uses gbr as the selected correction estimator a close examination of the differences between the uncorrected runoff values and corrected runoff values in fig 9 reveals several instances of non monotonic transformations where ndvi cn runoff falls and corrected runoff falls but then rises this can be seen around day 80 for example in the same figure the lower californian section with relatively few ndvi cn nonzero event days shows a marked improvement in accuracy at high runoff values while reducing the variability of ndvi cn runoff this last pattern a reduction in corrected runoff variability relative to ncvi cn variability is the most discernible feature of the first five panes in fig 9 to better understand the days when ndvi cn predicts zero runoff we also developed fig 10 showing the ndvi cn zero runoff days in the validation split it is important to note that the vertical scale on these graphs varies widely the ndvi cn method fails to detect substantial runoff events in a seasonal pattern in nldas runoff in the superior upland and northern rocky mountains physiographic sections there also are likely statistically significant seasonal patterns in the ndvi cn zero runoff days that could be addressed by adding additional complexity to the zero runoff correction models while the validation and estimation framework we used for developing the ndvi cn correction models is complex the underlying models are relatively simple because they lack an awareness of time broadening the information set available for prediction in both the zero runoff and nonzero runoff models to include past time periods would potentially overcome limitations associated with the event based nature of ndvi cn including precipitation and lagged precipitation similarly would likely provide opportunities for increasing model skill variables for seasonality or more sophisticated time series approaches such as wavelets also would likely increase model accuracy particularly for locations with substantial snow melt and accumulation however in the context of the curve number methodology a simple and effective linear correction is particularly appealing in these results we considered only a narrow range of the possibilities for grouping the data by selecting a broad geographic modeling scope physiographic division and a narrow geographic modeling level physiographic section at the same time we used a short list of explanatory variables so the structure of the matrix of regressors is block diagonal and thus relatively little information is shared between physiographic sections in each physiographic division for use by the statistical estimators more complex structures such as overlapping dummy variables lim and hastie 2015 and geographic regressors such as those in streamcat may help identify stronger patterns in the data a finer selection for the geographic modeling scope would also potentially improve model accuracy by reducing the tendency to lump together physiographic sections or more generally slices with contrasting snow fall melt patterns 4 conclusions by developing and applying a carefully designed model estimation selection and accuracy assessment framework we have developed correction models to enhance ndvi cn rainfall runoff model the result of our accuracy assessment is a set of validation nse values that can be used by practitioners who need runoff time series estimates to appropriately curate their data sources and quantify sources of error in downstream modeling applications because the curve number approach to runoff modeling is one of the simplest and least data intensive approaches it is fascinating that runoff estimated using a somewhat inflexible automated approach to quantifying hydrologic condition i e ndvi cn has such high linear correlation with an ensemble of state of the art lsms further for much of the country this relationship is stable and can be leveraged into simple first order linear regression correction models with skillful predictions as judged by nse and illustrated in fig 6 during this research we became aware of the importance of including a simple metric like rp to contrast more stringent accuracy measures like nse or kge while time series plots of simulated and observed runoff can help the analyst spot information patterns that can be leveraged to build correction models it is useful to have a metric that quantifies these patterns because rp r2 and nse have so much in common there may be a need for a generalized version of rp in the same manner that kge generalizes nse non linear correlation measures like spearman s rank order correlation coefficient may also be useful for spotting non linear patterns in the data disclaimer this article has been reviewed by the u s environmental protection agency and approved for publication mention of trade names or commercial products does not constitute endorsement or recommendation for use by the u s government the views expressed in this article are those of the authors and do not necessarily reflect the views or policies of the u s epa notation rp pearson s correlation coefficient nse nash sutcliffe efficiency r2 coefficient of determination declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported in part by an appointment to the orise fellowship program at the u s epa office of research and development athens ga administered by the oak ridge institute for science and education through interagency agreement no dw8992298301 between the u s department of energy and the u s environmental protection agency appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105321 
