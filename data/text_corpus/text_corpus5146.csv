index,text
25730,visualization of the outputs from sensitivity analyses of spatially explicit models can potentially enhance the perception of the uncertainty in model outputs however spatial global sensitivity analysis gsa produces multiple output maps twice as many as the number of input parameters under investigation which can result in an overwhelming visual load to interpret in this research we tested the visual stacking of coincident maps to handle the visual complexity for an application of an urban land use change model the coincident maps were compared with the adjacent representation both types of maps were tested using a web based survey to understand the efficacy in supporting the comprehension of the gsa results the majority of the questions showed no statistically significant difference between visualization techniques however the difficulty of interpreting the adjacent maps was observed among novice users which suggesting the need for an update for the future representations using coincident maps keywords visualization global sensitivity analysis uncertainty analysis spatial models land use change 1 introduction land use change is a complex and multi dimensional process nevertheless thanks to complex modeling approaches e g cellular automata we have a better understanding of the forecasted future ca and its ability to represent the state of a spatial unit in a lattice form can be easily adopted to geographic processes and therefore is applicable for representing urban systems clarke 2017 2019 since its introduction in spatial science by tobler 1979 ca has been a major modeling paradigm in land use change modeling couclelis 1985 gazulis and clarke 2006 verburg et al 2006 yet the increased complexity requires more information as model inputs consequently the challenge becomes how to address multiple parameters and their interactions in space and time yuan and hornsby 2008 all process representations in land use change models are subject to uncertainty forecasting with a model can represent only a sample of the known unknowns the model parameters orrell 2007 therefore the question becomes how to understand the unknown uncertainty of the unknowns the model parameters as a solution to this problem a land use change models reliability can be assessed by quantifying and representing model output uncertainty followed by a sensitivity analysis sa uncertainty analysis ua quantifies the model uncertainty and sa enables exploring the space of input factors to find out which contribute the most to the error and uncertainty in modeling sa is a particularly essential element when the output of a model feeds into policy description and planning guided by urban growth modeling pianosi et al 2016 saltelli et al 2019 the end users who will use the land use change model output can be numerous and from different backgrounds therefore the question becomes how best to convey the sa results to a diverse audience the selection of a visualization technique should 1 optimize the visual load 2 effectively summarize the output of sa and 3 be suitable for a diverse set of users global sa gsa is a widely accepted and applied sa approach to analyze model input parameters and their interactions within complex models and during spatial decision making e g ligmann zielinska and sun 2010 sheikholeslami et al 2019 song et al 2015 wu et al 2012 since most land use change models produce spatial outputs sa should be applied spatially to depict the uncertainty and sensitivity information aligned with the decision related output ligmann zielinska jankowski 2014 şalap ayça et al 2018 xu and zhang 2013 when gsa is applied spatially several sensitivity maps are produced to depict the contributions of individual model input factors to the model output uncertainty main effect and contributions due to their interactions interaction effects the number of output maps equals twice the number of input parameters under investigation the sum of individual variances and the variances due to the interactions among the factors ligmann zielinska and jankowski 2014 şalap ayça et al 2018 for spatio temporal models along with the number of model variables and the degree of interactions a time dimension is also added to spatial representations precisely for n parameters resulting in 2n interactions for a simulation duration of t 2 nt maps must be produced and examined to depict the contributions of the main and interaction effects of the model parameters consequently the choice of appropriate techniques for visualizing the input parameter sensitivity becomes more challenging with increasing model complexity systematic evaluations of the effectiveness of sa visualization techniques are rare compared to the uncertainty visualization literature most of the existing visualization efforts are mostly about aspatial data visualization butler et al 2014 kelleher et al 2013 pianosi et al 2015 van werkhoven et al 2008 the effectiveness of communicating model output uncertainty to decision makers or even analysts is of paramount importance especially for spatial decision making problems if the model uncertainty or reliability falsely conveyed model results may fail to disclose their full information one can assess the effectiveness of communicating uncertainty for example by administering to a selected group of model users a questionnaire designed to elicit their comprehension of data and information depicted on visuals çöltekin et al 2009 slocum et al 2003 aerts et al 2003 used a web survey of 66 participants to assess the effectiveness of two visualization techniques static comparison and toggling for spatial decision making purposes a chi square test was used to analyze the survey results and compare the two visualization techniques the results showed some degree of difference in the preferences of novices versus expert users static comparison was preferred over toggling by both expert and novice users the results were also similar to those obtained by leitner and buttenfield 1997 2000 who found that representing uncertainty as color lightness saturation and texture integrated with certainty information was clearer to end users when compared with a complex graphical representation aerts et al 2003 leitner and buttenfield 1997 2000 sanyal et al 2009 evaluated four different visualization techniques for one dimensional and two dimensional outputs with 36 participants for the analysis they computed full factorial anova from the survey results to assess the effectiveness of the different techniques according to their findings error bars had the poorest results among the four techniques they also concluded that the comprehension of uncertainty in 1d model output data i e scalar output was higher than in 2d outputs sanyal et al 2009 bisantz et al 2011 tested the effect of numeric annotations accompanying the probabilities of each category s membership e g high or low and compared their comprehension effectiveness among the experiment participants with the comprehension of graphical icons e g an upward arrow representing a high value their experimental design included twelve randomly assigned participants and they analyzed the survey results using a normalized score and the chi square statistic the results indicated a better outcome when the participants toggled between numerical and graphical representations than when using only one or the other our title asks is less more we tested whether showing less can have more impact than showing a lot namely we will investigate if using less dimension in visualization decreasing the visual load will effectively convey the message from gsa or not in spatial and spatial temporal models gsa results are traditionally presented with help of adjacent maps where the gsa output maps are arranged side by side and sometimes depending on the number of model input factors in a tabular layout consisting of several rows and columns this type of visualization can be effective in models with a small number of input factors subject to gsa however arranging 2 n maps next to each other in a tabular layout and visually comparing them can be cognitively a very demanding task hence in order to reduce the cognitive load we have proposed a coincident map which collapses the stack of 2 n values corresponding to each spatial unit in a model to a single representative value section 3 2 a coincident map is a single layer representation of a stack of maps where each location on the layer e g a raster map cell is assigned one value representing the variability of values in the stack various descriptive statistical measures such as maximum mean mode or the range of output values can be used to compile coincident maps for this experiment we kept the time component aspatial and depicted it in a stream graph form to represent the relative fluctuations in the simulation period this coincident visualization was tested against the ongoing practice in the gsa literature adjacent visualization to understand the applicability of the coincident technique section 3 3 finally the findings of the study are discussed along with the lessons learned to identify the areas of improvement in visualization techniques section 4 5 2 background 2 1 global sensitivity analysis for urban growth modeling validation of a land use change model simply focuses on the performance of the calibrated model for assessment purposes rather than its underlying forecasting on uncertainty clarke 2018 clarke and johnson 2020 van vliet et al 2016 in this sense sa for a land use change model is a useful method to assess the model s reliability by identifying input factors that contribute to the model s output uncertainty land use change modeling has always been shaped by the definition of candidate variables processes and system boundaries akyol and tuncay 2016 van vliet et al 2016 cellular automata ca models are no exception the most influential input parameters will most definitely determine the output of the model in this sense visualizing and understanding how sensitive the model is to these parameters is important for determining confidence in the model s outputs moreover in complex systems where individual model parts can have complex interdependencies and non linear feedbacks the effects of interactions among the components on the variability of the model output require further study li et al 2014 ligmann zielinska and sun 2010 moreau et al 2013 the sleuth land use change and urban growth model is a widely used example of a ca based land use change simulation model clarke et al 1997 sleuth is the acronym for slope land use exclusion urban extent over time transportation and the hill shaded backdrop layers which are the input layers used in the model along with its 6 grided raster layers sleuth is initiated by a scenario file which contains forecast parameters and the forecast period used in cellular automata for the urban growth phase at least four historic urban maps are used to derive the historic pattern of growth during the model s calibration and application chaudhuri and clarke 2013 once the model calibration is achieved sleuth simulates urban growth by using four transition rules which include spontaneous growth f s δ γ addition of new spreading centers f n s β γ edge growth f e χ γ and road influenced growth f r β ρ δ γ where δ symbolizes ca rules diffusion γ slope resistance β breed χ spread and ρ road gravity coefficients given as forecast parameters candau et al 2000 the diffusion coefficient determines the urbanization probability the slope coefficient determines the weighted probability of the local slope and the breed coefficient defines the probability for each new urbanized cell to become a new spreading center during the urban growth simulation the urbanization results from both new centers and more established centers from earlier simulation time steps if a non urbanized center cell has at least three urbanized neighboring cells it has a certain global probability to transition defined by the spread and slope coefficients various approaches to gsa have been thoroughly discussed in ravazi et al 2021 each approach distribution based variogram based regression based response surface assisted sa and sa with correlated inputs has had widespread applications across many fields among all of them surrogate modeling strategies offer dimension reduction with great computational gains and are preferable especially in computationally expensive models although there are several robust response surface methods described in the literature razavi and gupta 2016 here we adopted pce as it is easier to integrate into our model and doesn t require any additional computational cost for calculating sobol indices once a pce representation is available most sensitivity maps show the variability of first order index values over the study area these maps help to visualize the input factor s effects i e parameter effects on the model output gaining popularity among the methods of sa applied to complex models gsa addresses both interactions among model inputs and non linear output responses such that the sensitivity of model input factors is not only determined individually main effects but also as a result of their interactions crosetto et al 2000 marrel et al 2011 saltelli et al 2008 specifically the model s input factor influences on model output variability is represented by first order s i and total order s t sensitivity indices the interaction maps consisting of total order sensitivity indices show the total contribution of groups of parameters expressed by a total sensitivity index s t for example the sensitivity index s t 1 represents the total contribution of the 1st parameter to the model variance resulting from the parameter s interactions with other model parameters a low s 1 and high s t 1 mean that the contribution of the 1st parameter to the overall variability of model output is more due to this parameter s interactions with other parameters than to this parameter treated alone examples of spatial application of gsa can be seen in multiple studies abily et al 2016 gómez delgado et al 2004 ligmann zielinska and jankowski 2014 ligmann zielinska and sun 2010 marrel et al 2011 moreau et al 2013 pena et al 2014 plata rocha et al 2012 xu and zhang 2013 the majority of these applications use adjacent map representations which yield numbers of parameter maps ranging from 8 to 20 marrel et al 2011 moreau et al 2013 roura pascual et al 2010 xu and zhang 2013 ligmann zielinska and jankowski 2014 and abily et al 2016 applied a ranking method by defining the dominant parameters to prioritize the input factors here we seek to experiment with a similar coincident approach by testing its visual effectiveness 2 2 visualizing results of spatially explicit global sensitivity analysis multivariate data representation is a challenging task due to the attribute space having a higher dimension than cartesian space such as when representing urban growth by time and location one solution is to compress combine the multivariate information into a static non spatial point source representation by using bar charts pie charts glyphs or star plots spense 2014 a second option is to reduce dimensionality by using one of the following scatterplots which can be further extended to scatterplot matrices cleveland 1993 parallel coordinate plots where the multiple variables are displayed on parallel axes inselberg 1985 or principal components analysis pca where the data is projected to the axes of greatest variance jolliffe 2002 however each technique suffers from limitations in usability affected by the number of variables or visual clutter although the usability of different visualization techniques for depicting uncertainty has been investigated by many scholars brodlie et al 2012 hullman et al 2019 kinkeldey et al 2015 maceachren et al 2005 potter et al 2012 slocum et al 2003 the literature on sensitivity visualization is scarce kinkeldey et al 2014 provided a comprehensive review on uncertainty visualization and systematically summarized techniques into three dichotomies as an uncertainty visualization cube three axes were given as intrinsic extrinsic static dynamic and coincident adjacent intrinsic visualization technique alters the existing map symbology by altering color hue value or saturation in concert with other visual variables such as transparency blur or resolution although the intrinsic method gives a better overview of the uncertainty the extrinsic technique enables better in depth analysis dynamic representation is based on user control interaction although kinkeldey et al 2014 stated that animated views have a potential for uncertainty visualization they do not show any evidence that they perform better adjacent representation is found to be useful for retrieval of single values but less effective when a task becomes more complex and more eye movements are needed hence none of these techniques solves the dimensionality problem especially for raster data sets therefore we selected coincident mapping from these six techniques due to its flexibility for visually stacking of the spatial gsa attributes 3 methods 3 1 spatial gsa for land use change modeling and its visual load the urban growth component of the sleuth model was used for this study urban growth is forecasted through four growth cycles each cell in the study area goes through these cycles the outcome is depicted as the probability of that cell s becoming urban or not the san diego county dataset compiled and tested by syphard et al 2011 was used in this study for model calibration and initialization the meta modeling framework developed by şalap ayça et al 2018 was used for the spatial gsa application this framework is a spatial implementation of polynomial chaos expansion pce with variance based decomposition sobol 2001 sudret 2008 unlike the other full order variance based decomposition techniques where full order model evaluation is required to explore the full factor space pianosi et al 2016 saltelli et al 2010 in pce the minimum required sample size is based on the selected experimental design degree and model parameters this eventually yields a smaller number of samples and the sampling derivation can be found in şalap ayça and jankowski 2017 this makes pce based variance decomposition more attractive specifically for computationally expensive complex models for this experiment for 5 input forecast parameters diffusion breed spread slope and road gravity a set of 252 quasi random numbers were generated based on a triangular distribution see table 1 for ranges of model parameters then by using these samples as the input parameters the sleuth model simulated the yearly urban growth from 2001 to 2050 yielding 252 50 output forecast maps for the fifty year period these maps are used to calculate 5 50 main and 5 50 interaction effect maps when aspatial gsa is applied for n parameters the analysis output yields 2n sa index values to interpret if gsa is applied spatially the scalar index values are represented by 2n maps one index value for each mapping unit to produce and interpret for example as illustrated in fig 1 when we run the spatial gsa for urban growth each pixel has a probability of being urban along with how this probability is sensitive to the transition parameters sdiffusion sspread sroad gravity sbreed sslope and their interactions stdiffusion stspread stroad gravity stbreed stslope additionally when the model has temporal components i e urban growth forecasting a spatially distributed output is produced for each year during the simulation span as a summary each grid cell has 2 nt attribute values which can be mapped separately 3 2 coincident maps coincident methods are preferred when the primary map is insufficient to accommodate the high visual dimensionality like other multivariate data sensitivity information can also be combined with the other visualized data in a map or compared by visualizing it in a separate map view therefore adding an overlay would make the map difficult to comprehend suggested methodology for coincident maps for main and interaction effects are as follows compute the sensitivity indices of each grid cell define the maximum among all to define the most important parameter dominating factor color code the final grid cells using the value of the index by following this iterative process for each pixel in the maps of the study area it is possible to stack the n dimensional data space information into a 2d visualization space 3 3 experiment using adjacent coincident visualization in order to understand the effectiveness of the visualization a web survey was published through the san diego state university sdsu department of geography web host and was active from april 25th to june 12th 2018 each participant who accepted the consent form was directed to the first section of the survey which had an imaginary case story composed of a brief explanation of the urban growth modeling the meaning of sensitivity maps and solicited participant s responses in reaction to viewing the model output maps the participants were also reminded that full confidence in responses was neither a requirement nor an expectation in their answers in the second section of the survey the participants viewed sensitivity maps produced with a randomly assigned visualization technique and answered questions related to the maps ethical clearance for the survey was obtained from the sdsu institutional review board for human subject research 3 3 1 survey participants 65 volunteer participants responded to the survey recruited by distributing the questionnaire in selected upper division geography courses and via email listservs the participants included undergraduate and graduate students from geography departments at sdsu and the university of california santa barbara university of southern california engineering graduate students and professionals in urban planning gis environmental management engineering and data analysis reached through listservs the participant responses were grouped according to the visualization technique adjacent versus coincident the assignment of participants to groups was randomly made based on the equal chance of selection for each group and equal representation of educational levels each response was also later grouped based on the responders educational level and professional background classifying those with a background in urban planning gis and map visualization as experts and those with a background in engineering and others as non experts 3 3 2 survey instrument all participants were given a set of 12 questions including 10 map related questions and 2 demographic questions the initial 5 questions for each group were related to the visualization of spatial sensitivity analysis only the sensitivity maps resulting from the 2050 urban growth forecast output were used in these questions the fifty year period sensitivity analysis results were summarized for question 6 to depict temporal change questions 7 to 10 solicited the participants general opinion on the given visualization technique and sensitivity visualization in general questions 11 and 12 were about the education level and professional specialty area table 2 summarizes the questions variables investigated question purpose and provides the wording of the questions along with possible answer options we analyzed the responses to the above questions to answer the following research questions 1 how effective are different visual representations of input factor sensitivity in a land use change model 2 how do interaction maps showing the result of gsa inform the interpretation of land use model results 3 are the level of expertise and the choice of preferred visualization techniques independent of each other for the first research question it is hypothesized that participants who receive coincident representation will exhibit higher confidence in the sensitivity map interpretation participants who received adjacent representation are expected to exhibit less confidence in interpreting the model output in order to answer the second research question it is hypothesized that participants will be using interaction maps in their interpretation of the model output this study also collected information on the level of expertise in spatial decision making urban planning map visualization decision support gis environmental management or data analytics to determine whether the level of expertise expert versus novice user bears on the choice of visualization techniques 3 3 3 adjacent maps with static graphs for the adjacent representation the main effect sensitivity maps fig 2 were produced with matlab s plotting function to depict the result of the 2050 forecast for each model parameter s 1 diffusion s 2 breed s 3 spread s 4 slope and s 5 road gravity the maps show sensitivity analysis results for a selected sub region in san diego county the background color grey indicates no data values and the color bar shows the range of change in the sensitivity values for each parameter figs 2 and 3 were used for questions q1 to q5 in the adjacent representation group the main effect sensitivity maps fig 2 show the contribution of single input parameters including s 1 s 2 s 3 s 4 and s 5 to output variability the higher order indices which account for the interaction effects and their contribution to output variability are referred to as s t 1 s t 2 s t 3 s t 4 and s t 5 and are depicted on the corresponding maps fig 3 when only the main effect sensitivity maps are examined the ranking of sensitivity index values for the parameters with respect to their contribution to model output variability yields s 3 s 1 s 2 with two additional parameters regarded as equally important s 4 s 5 and contributing relatively little to the output variability fig 3 shows the interaction maps for the 2050 forecast for each forecast parameter representing the total contribution of the pairs of parameters expressed by the total sensitivity index st although the dominating region does not necessitate the full color spectrum it has been applied to reflect all the varying cells in the selected region moreover color value varying schemes were not sufficient to produce the acceptable color distance see brychtova and coltekin 2015 for details of color distance in sequential schemes therefore a hue varying scheme where spectral ordered hues include blue cyan green yellow and red has been selected since it is better for reading specific details gołębiowska and çöltekin 2020 the rank order of the sensitivity index values for the parameters with respect to their output variability contribution yields s t 1 s t 5 s t 3 s t 2 s t 4 therefore considering the overall variance of a single parameter s 1 should be interpreted as the most influential parameter for the selected study area to represent the change in time of model sensitivity all sensitivity maps for the forecast period 2001 2050 were summarized in a single measure which is the mean sensitivity index value per simulation year and the results were plotted on a static x y graph in fig 4 each plot represents a course of change for a parameter s sensitivity index value first order indices over the simulation time s 1 s 2 s 4 and s 5 show a relatively similar increase of the influence over the simulation time whereas the influence of s 3 shows the reverse trend of waning variability in the model output 3 3 4 coincident maps with stream graphs for the coincident representation of simulation results the main effect sensitivity maps were arranged to emphasize at each pixel the dominant factor among the five model factors fig 5 fig 5 was used for questions q1 to q5 in the coincident representation group the parallel coordinate graphs produced with matlab s parallel coordinates plot function show how the sensitivity index values change for each location the blue lines represent the change in sensitivity values for each pixel and the red line represents the mean of the sensitivity values for each parameter although it is not possible to derive the rank order of model parameters by looking at the main effect dominance map the parallel coordinates graph shows the general trend in the ranks the interpretation of the lower left part in fig 5 depicting the interaction dominance map is the same as for the main effect dominance map the only difference is that it refers to the higher order interaction between parameters to analyze the temporal variability in coincident representation a stream graph depicting the change in the parameter s sensitivity index was produced in r with the ggplot2 and ggtimeseries libraries fig 6 shows the range rank and value rank of the five model parameters the range rank determines the placement of a parameter in the graph and it is based on the difference between the maximum and minimum observed variance for example has the 1st rank the highest range among the five parameters and s 2 has the 2nd rank fig 6 left color bar hence these two parameters and their corresponding values are placed on the edges of the graph s 3 on the upper edge and s 2 on the lower edge the value rank is used for guidance in interpreting the thickness in the graph s 3 has the highest variance value and s 4 has the lowest value therefore s 3 is the thickest whereas s 4 occupies the thinnest stream in fig 6 right color bar fig 7 accompanied question q6 in the coincident representation group in order to introduce this visualization technique a brief explanation about stream graphs was given to the survey participants the participants were informed that the most varying parameter was placed on the graph s edges and the thickness of the stream corresponded to the parameter s value implying that the closer a parameter s position is to the center of the stream graph the less variability due to this parameter can be observed in the model output the increase in the fluctuations of the parameter values is represented by peaks and jagged boundary lines whereas the smooth edges are symptomatic of steady parameter values although the axes of the stream graph do not represent the exact measured values they are used as a reference and the relative rank of parameters can be established over the simulation time period 4 results a total of 65 completed survey forms of which 32 pertained to adjacent and 33 to coincident visualizations depending on the answers to two categorical questions about educational level q11 and professional expertise q12 participants were divided into novice and expert categories the participants who had at least an undergraduate college degree and a background in one or a combination of the following disciplines urban planning geography gis environmental management environmental sciences or statistics data analytics were treated as experts participants with an engineering background or lacking a college degree were designated as novices table 3 overall 48 of the participants were classified as expert and 52 as novice users the majority of participants had a positive attitude toward sensitivity visualization q10 table 2 the results showed that 56 of the participants sum of responders who answered yes to q10 thought that sensitivity visualization helped interpret the urban growth model output fig 8 we hypothesized that the effectiveness of sensitivity visualization for interpreting urban growth would differ between adjacent versus coincident maps and expert versus novice the difference in q10 with respect to adjacent and coincident maps although significant at the p value 0 10 was insignificant at the p value 0 05 adopted for the analysis of survey results chi square 5 419 p 0 067 cramer s v 0 289 and expert versus novice respondents chi square 0 682 p 0 711 cramer s v 0 102 for q10 the lack of opinion not sure response was also included in the response choices in order to help respondents who might not wish to appear uninformed or did not have an opinion on the subject krosnick and presser 2010 among the respondents who did not have a positive attitude toward sa visualization for both levels of expertise the selection between a disagreement no and uncertain answer not sure had an almost equal percentage distribution we tested the responses for all of the survey questions with the shapiro wilk test and found that the distribution of responses significantly deviated from a normal distribution given that skew and kurtosis of responses for each question also indicated a non normal distribution and that the dependent variable was not normally distributed a non parametric test was selected for the analyses the mann whitney u test was used to determine the significance of 1 dependence on the level of expertise expert versus novice users independent variables and 2 type of visualization adjacent versus coincident independent variables on confidence in sa maps dependent variables the relationship between the two groups expert versus novice and adjacent versus coincident was addressed by using ordinal regression additionally in order to understand the usage and benefit of gsa the results of q1 and q4 were analyzed to determine which representation was more effective in correctly interpreting the sensitivity maps 4 1 importance of expertise level in sa result interpretations the frequency of response categories based on respondent s expertise level is illustrated in fig 9 the divergence bars represent the levels of agreement ranging from disagreement not at all to complete agreement completely bars starting to the right of the center zero line mark 0 0 represent questions that received more positive than negative responses for example the most positive response 66 was received from novice users to q5 in general the novice users had more confidence in their answers than the expert users however this difference was not statistically significant when tested with the mann whitney u test table 4 4 2 preference of visualization techniques q7 measured the participants preference for a visualization technique almost a quarter of participants assigned the coincident representation 11 of experts and 11 of novices were uncertain in their responses to q7 of their preferences for this visualization technique whereas a similar percentage of participants 9 experts and 14 novices preferred the adjacent visualization fig 10 however the difference in preferences for these two visual representations of sa results was not statistically significant chi square 4 877 p 0 087 cramer s v 0 274 a similar frequency distribution of responses based on the dichotomy between adjacent versus coincident representations is present in fig 11 in their responses to q2 q3 q8 and q9 the participants showed more confidence in the coincident than the adjacent representation conversely with respect to q5 and q6 the participants had more confidence in the adjacent representation the greatest difference in preferences for adjacent versus coincident representations was in q9 which asked whether the sensitivity visualization made the interpretation of the model output too difficult and complex to use the coincident representation was deemed by 62 of participants as too complex and difficult to use in interpreting the results of sa versus 36 of participants indicating the complexity of adjacent representation this difference was statistically significant p 0 009 table 5 4 3 the relationship between two categorical variables expert versus novice participants and adjacent versus coincident representations we applied an ordinal regression to examine the relationship between the dependent confidence in sa map interpretation and the independent visualization technique and expertise level variables the resulting statistics are given in table 6 and the significant values are indicated in bold the base category is represented by adjacent novice interaction the odds ratio column gives the odds of an outcome increasing or decreasing when the explanatory variables change for example for q9 for coincident representation with an increase in expertise level we expect a 42 decrease 2 36 5 64 in the ordered log odds of higher confidence level model fitting in ordinal regression estimates whether the regression model improves our ability to predict the participant confidence in sa interpretation this was done by comparing the base model against the final model with all of the explanatory variables thus one can observe whether the final model has significantly improved the data fit in this study for all confidence level measures except for the information acquisition q9 the final model did not significantly improve over the baseline model model fit values in table 6 however the goodness of fit was significant indicating that the data and the model predictions were similar marking a good model r2 given by the nagelkerke statistic is the coefficient of determination and larger values indicate more variation in the confidence levels for example for q2 1 9 of the variance in the confidence level is due to model interaction table 6 all of the calculated nagelkerke statistics except for q9 are relatively small values indicating less variation in the confidence levels for the final statistics in table 6 the test of parallel lines the significance values greater than 0 05 mean that there is no difference in location parameters interaction between two independent variables given in exploratory variable column in table 6 groups across the response categories 4 4 interpretation of interaction maps q1 was analyzed using a simple percentage of correct versus incorrect answers with 65 participants almost 37 answered the question correctly by selecting s 1 as the most influential model parameter and considering both main and interaction effects fig 12 which shows that those participants were able to interpret both maps correctly however the ability to interpret maps differed by visual representation type 44 of correct answers for adjacent versus 30 correct answers for coincident maps which means that the visualization assignment highlighted a difference in terms of map reading skills moreover there was also a difference in correct answers among the different levels of expertise 25 correct answers from experts versus 12 correct answers from novices q4 asked about the usage of interaction maps during the interpretation of sensitivity index measures figs 13 and 14 show the drill downs by visualization category and expertise level in figs 13 and 14 the column values yes and no represent answers to q4 and the row values 1 5 correspond to sensitivity index values for the respective parameters s 1 s 2 s 3 s 4 and s 5 for example the first column and the first row in fig 13 the orange color bubble 38 means that 38 of the participants assigned to the adjacent map group 12 out of 32 people selected the 1st parameter s 1 and used interaction maps 61 of the experts 19 out of 31 people stated that they used the interaction maps whereas this ratio was 79 27 out of 34 people for the novice users among the 44 6 of participants 29 out of 65 people who selected s 3 as the most influential parameter 41 3 12 out of 29 stated that they did not use the interaction maps this is the expected outcome since s 3 is the obvious choice when only the main effect sensitivity maps are considered similarly out of 70 7 46 out of 65 people of the participants who used the interaction maps 43 5 20 out of 46 people selected s 1 as the most influential parameter which means they not only used the interaction maps but also successfully interpreted the map content 5 discussion this study aimed to introduce a visual load reduction method and assess the effectiveness of the proposed method by comparing the existing approach for sa visualization fig 15 summarizes the confidence level frequencies of the level of agreement questions grouped by expertise level and visualization type only for q9 was there a statistically significant association between visualization technique and agreement with the statement in the question given the border line significance of some differences between the groups concerning map preferences and map effectiveness it would be worthwhile to repeat the study using a larger group of participants for the first research question it was hypothesized that the participants assigned a coincident representation would exhibit higher confidence in the map interpretation of sa results than the participants assigned a single adjacent representation based on the survey results this hypothesis must be rejected the effectiveness of the two selected visual representations of input factor sensitivity in the land use change model does not significantly differ between coincident and adjacent representations for q9 asserting that sensitivity visualization makes the interpretation of the model output too difficult and complex to use the negative answer implies a positive attitude in this respect less than half of the expert users 47 5 the sum of 17 22 and 8 5 for q9 expert coincident in fig 15 regarded the coincident method as uncomplicated whereas only 26 5 4 out of 15 of novices did fig 15 this could be interpreted as the coincident maps being too complex for novice users despite the intent to synthesize multiple maps into one map differences in perception between the novice and expert users can be explained by the visualization concepts introduced by dibiase 1990 and further developed as cartography cubes by maceachren and taylor 1994 and maceachren and kraak 1997 2001 the commonality in these studies is that they all interpret the importance of visualization as a communication tool to reach the audience and as an analytic support device for private visual thinking maceachren and kraak 2001 transitioning from exploratory maps typically used by scientists and experts to communication maps used by stakeholders students or citizens the communication effectiveness mostly depends on the cartographic language used the adjacent and coincident maps used in this study can be arguably characterized as exploratory it follows to reason that they would be more effectively used by experts than novices although this expectation has only been confirmed by the study results with respect to the complexity of usage q9 one could argue that there is a need to strengthen the communication effectiveness of maps employed to visually represent the results of uncertainty and sensitivity analysis this need becomes even more apparent if uncertainty and sensitivity measures are communicated to stakeholders and decision makers as part of participatory decision making processes it also follows from the survey results that using a single type of visualization may not always be effective for all levels of expertise the second research question focuses on the usage of interaction maps while for the majority of the survey questions there was no one correct answer there was an expected correct answer for the first question participants who opted for using the interaction maps were also expected to give different answers from the participants who decided not to use the interaction maps the usage and the understanding of interaction maps which depict the results of global sensitivity analysis can be inferred from analyzing the responses to q1 when the responses are compared across the groups the highest rate of correct answers comes from experts who used coincident representations for the adjacent visualization the expertise level does not seem to affect the rate of correct answers however the difference between the correct answers under the use of coincident versus the use of adjacent representation becomes 10 times higher for experts than for novices 46 out of 65 participants stated that they used interaction maps in their interpretation for the participants who selected s 1 as the most influential factor and confirmed the use of interaction maps 19 out of 65 the interaction maps proved to help interpret the results of sa however the majority of the participants who used the interaction maps could not interpret the sa results correctly this may be due to interpreting the main effects and interactions of model input factors separately and the difficulty in effectively integrating these two sources of information one future solution to this problem could be integrating the main and interaction effects s 1 and s t into a single visual representation the last research question assessed the level of expertise and its relation to visualization technique preference in reference to the uncertain endeavour maceachren 1992 p 17 uncertainty or sensitivity information can already be a burden for some users especially novices simple interpretation tasks which do not involve complex decision making can be carried out with adjacent representations however for complex multi location comparisons an adjacent map comparison may introduce a cognitive overload harrower 2003 viard et al 2011 in this study we had only 5 input factors which resulted in 10 maps to process the outcomes could vary dramatically if the number of inputs increased to for example 20 or 30 which would likely result in more eye movement and comparison difficulty in such cases we expect the superiority of coincident maps to be more obvious hence there is a need to create an effective design of coincident representation different from the one used in this study and cognitively easier for novice users the analysis of the interaction effect by expertise level and the choice of preferred visualization on the confidence in sa map interpretation table 6 revealed that only the responses to q9 showed a significant difference between experts and novices in the visualization choice since the combination of adjacent novice was selected as the baseline for the regression depending on the odds ratio we can say that the odds of a novice user who chose the coincident representation achieving a high level of agreement for q9 are approximately 6 times the odds of novice users who selected the adjacent representation this means that a novice user is expected to deem a coincident representation as more difficult for sa visualization than an adjacent representation considering the skills of novice users their preference for adjacent representation might be predictable since the adjacent map representation is still the most widely used form of spatial visualization in the case of several parameters spatial variables 6 conclusion limitations and future work in most decision making problems the end users do not always have to know nor do they care about the details of model output variability yet they might be open to learning about model output reliability if they could visualizing sensitivity maps therefore sensitivity visualization is one of the emerging research frontiers in spatial decision making especially important for addressing effective ways to represent model variances linked to parameter uncertainties malczewski and jankowski 2020 in this paper we presented the visual stacking approach of coincident maps as a solution to multi dimensional visual load for gsa output visualization below is a summary of findings and their limitations 1 coincident maps help to simplify the visualization effort into a 2d representation however there is still room for improvement of the effective visualization techniques and tools to support decision making by showing land use change model sensitivity 2 parallel plots are effective in revealing the relationships between adjacent variables as well as showing the distribution of attributes however overlapping of line segments is a frequent occurrence since many data records have the same or similar values or the number of data records is large relative to the display 3 results underlined a difficulty among novice users in integrating the separate information coming from gsa therefore for future studies we suggest possible research directions to overcome the above limitations and move beyond them 1 the maps depicting information about model uncertainty and sensitivity should be simple but informative consequently simplifying visualization forms further and making them interactive should be the next step in improving visual representations for sa mapping 2 future research efforts should focus on how the relative rankings of sensitivity indices can be addressed one possible solution could consider a grouping strategy as in sheikholeslami et al 2019 and huo et al 2019 3 in this study we had a research perspective of the visualization problem in a follow up to this study one could include domain experts and their preferences to analyze their specific requirements and re structure the research questions to better consider existing knowledge finally an interesting open research question about the spatial gsa for spatially explicit complex models is about the unknown unknowns the performance of gsa methods can be sensitive to the randomness in sampling variability which is only amplified by spatial and or spatial temporal variability future studies could investigate other sampling strategies as the one presented in this paper that in addition to sampling and spatial variabilities also account for the dynamic properties of environmental models as in the variogram analysis of response surface vars razavi and gupta 2016 therefore a future study accounting for the randomness of sample points in spatially explicit dynamic models by either experimenting with different random seeds or using methods such as bootstrapping can help to better understand the unknown unknowns in spatial models funding this work was supported by the national science foundation grant numbers bcs 1263071 cns 0960316 dmr 1720256 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment we acknowledge support from the center for earth systems analysis research department of geography at san diego state university and the university of california santa barbara center for scientific computing from the cnsi mrl an nsf mrsec grant no dmr 1720256 and nsf grant no cns 0960316 this research was supported in part by the national science foundation geography and spatial sciences program grant no bcs 1263071 any opinion findings conclusions and recommendations expressed in this paper are those of the authors s and do not necessarily reflect the views of the national science foundation we also appreciate the feedback provided by the anonymous reviewers and the special issue editors on the earlier version of the manuscript the corresponding author is also grateful for the support of her writing accountability group during the covid 19 pandemic appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105181 
25730,visualization of the outputs from sensitivity analyses of spatially explicit models can potentially enhance the perception of the uncertainty in model outputs however spatial global sensitivity analysis gsa produces multiple output maps twice as many as the number of input parameters under investigation which can result in an overwhelming visual load to interpret in this research we tested the visual stacking of coincident maps to handle the visual complexity for an application of an urban land use change model the coincident maps were compared with the adjacent representation both types of maps were tested using a web based survey to understand the efficacy in supporting the comprehension of the gsa results the majority of the questions showed no statistically significant difference between visualization techniques however the difficulty of interpreting the adjacent maps was observed among novice users which suggesting the need for an update for the future representations using coincident maps keywords visualization global sensitivity analysis uncertainty analysis spatial models land use change 1 introduction land use change is a complex and multi dimensional process nevertheless thanks to complex modeling approaches e g cellular automata we have a better understanding of the forecasted future ca and its ability to represent the state of a spatial unit in a lattice form can be easily adopted to geographic processes and therefore is applicable for representing urban systems clarke 2017 2019 since its introduction in spatial science by tobler 1979 ca has been a major modeling paradigm in land use change modeling couclelis 1985 gazulis and clarke 2006 verburg et al 2006 yet the increased complexity requires more information as model inputs consequently the challenge becomes how to address multiple parameters and their interactions in space and time yuan and hornsby 2008 all process representations in land use change models are subject to uncertainty forecasting with a model can represent only a sample of the known unknowns the model parameters orrell 2007 therefore the question becomes how to understand the unknown uncertainty of the unknowns the model parameters as a solution to this problem a land use change models reliability can be assessed by quantifying and representing model output uncertainty followed by a sensitivity analysis sa uncertainty analysis ua quantifies the model uncertainty and sa enables exploring the space of input factors to find out which contribute the most to the error and uncertainty in modeling sa is a particularly essential element when the output of a model feeds into policy description and planning guided by urban growth modeling pianosi et al 2016 saltelli et al 2019 the end users who will use the land use change model output can be numerous and from different backgrounds therefore the question becomes how best to convey the sa results to a diverse audience the selection of a visualization technique should 1 optimize the visual load 2 effectively summarize the output of sa and 3 be suitable for a diverse set of users global sa gsa is a widely accepted and applied sa approach to analyze model input parameters and their interactions within complex models and during spatial decision making e g ligmann zielinska and sun 2010 sheikholeslami et al 2019 song et al 2015 wu et al 2012 since most land use change models produce spatial outputs sa should be applied spatially to depict the uncertainty and sensitivity information aligned with the decision related output ligmann zielinska jankowski 2014 şalap ayça et al 2018 xu and zhang 2013 when gsa is applied spatially several sensitivity maps are produced to depict the contributions of individual model input factors to the model output uncertainty main effect and contributions due to their interactions interaction effects the number of output maps equals twice the number of input parameters under investigation the sum of individual variances and the variances due to the interactions among the factors ligmann zielinska and jankowski 2014 şalap ayça et al 2018 for spatio temporal models along with the number of model variables and the degree of interactions a time dimension is also added to spatial representations precisely for n parameters resulting in 2n interactions for a simulation duration of t 2 nt maps must be produced and examined to depict the contributions of the main and interaction effects of the model parameters consequently the choice of appropriate techniques for visualizing the input parameter sensitivity becomes more challenging with increasing model complexity systematic evaluations of the effectiveness of sa visualization techniques are rare compared to the uncertainty visualization literature most of the existing visualization efforts are mostly about aspatial data visualization butler et al 2014 kelleher et al 2013 pianosi et al 2015 van werkhoven et al 2008 the effectiveness of communicating model output uncertainty to decision makers or even analysts is of paramount importance especially for spatial decision making problems if the model uncertainty or reliability falsely conveyed model results may fail to disclose their full information one can assess the effectiveness of communicating uncertainty for example by administering to a selected group of model users a questionnaire designed to elicit their comprehension of data and information depicted on visuals çöltekin et al 2009 slocum et al 2003 aerts et al 2003 used a web survey of 66 participants to assess the effectiveness of two visualization techniques static comparison and toggling for spatial decision making purposes a chi square test was used to analyze the survey results and compare the two visualization techniques the results showed some degree of difference in the preferences of novices versus expert users static comparison was preferred over toggling by both expert and novice users the results were also similar to those obtained by leitner and buttenfield 1997 2000 who found that representing uncertainty as color lightness saturation and texture integrated with certainty information was clearer to end users when compared with a complex graphical representation aerts et al 2003 leitner and buttenfield 1997 2000 sanyal et al 2009 evaluated four different visualization techniques for one dimensional and two dimensional outputs with 36 participants for the analysis they computed full factorial anova from the survey results to assess the effectiveness of the different techniques according to their findings error bars had the poorest results among the four techniques they also concluded that the comprehension of uncertainty in 1d model output data i e scalar output was higher than in 2d outputs sanyal et al 2009 bisantz et al 2011 tested the effect of numeric annotations accompanying the probabilities of each category s membership e g high or low and compared their comprehension effectiveness among the experiment participants with the comprehension of graphical icons e g an upward arrow representing a high value their experimental design included twelve randomly assigned participants and they analyzed the survey results using a normalized score and the chi square statistic the results indicated a better outcome when the participants toggled between numerical and graphical representations than when using only one or the other our title asks is less more we tested whether showing less can have more impact than showing a lot namely we will investigate if using less dimension in visualization decreasing the visual load will effectively convey the message from gsa or not in spatial and spatial temporal models gsa results are traditionally presented with help of adjacent maps where the gsa output maps are arranged side by side and sometimes depending on the number of model input factors in a tabular layout consisting of several rows and columns this type of visualization can be effective in models with a small number of input factors subject to gsa however arranging 2 n maps next to each other in a tabular layout and visually comparing them can be cognitively a very demanding task hence in order to reduce the cognitive load we have proposed a coincident map which collapses the stack of 2 n values corresponding to each spatial unit in a model to a single representative value section 3 2 a coincident map is a single layer representation of a stack of maps where each location on the layer e g a raster map cell is assigned one value representing the variability of values in the stack various descriptive statistical measures such as maximum mean mode or the range of output values can be used to compile coincident maps for this experiment we kept the time component aspatial and depicted it in a stream graph form to represent the relative fluctuations in the simulation period this coincident visualization was tested against the ongoing practice in the gsa literature adjacent visualization to understand the applicability of the coincident technique section 3 3 finally the findings of the study are discussed along with the lessons learned to identify the areas of improvement in visualization techniques section 4 5 2 background 2 1 global sensitivity analysis for urban growth modeling validation of a land use change model simply focuses on the performance of the calibrated model for assessment purposes rather than its underlying forecasting on uncertainty clarke 2018 clarke and johnson 2020 van vliet et al 2016 in this sense sa for a land use change model is a useful method to assess the model s reliability by identifying input factors that contribute to the model s output uncertainty land use change modeling has always been shaped by the definition of candidate variables processes and system boundaries akyol and tuncay 2016 van vliet et al 2016 cellular automata ca models are no exception the most influential input parameters will most definitely determine the output of the model in this sense visualizing and understanding how sensitive the model is to these parameters is important for determining confidence in the model s outputs moreover in complex systems where individual model parts can have complex interdependencies and non linear feedbacks the effects of interactions among the components on the variability of the model output require further study li et al 2014 ligmann zielinska and sun 2010 moreau et al 2013 the sleuth land use change and urban growth model is a widely used example of a ca based land use change simulation model clarke et al 1997 sleuth is the acronym for slope land use exclusion urban extent over time transportation and the hill shaded backdrop layers which are the input layers used in the model along with its 6 grided raster layers sleuth is initiated by a scenario file which contains forecast parameters and the forecast period used in cellular automata for the urban growth phase at least four historic urban maps are used to derive the historic pattern of growth during the model s calibration and application chaudhuri and clarke 2013 once the model calibration is achieved sleuth simulates urban growth by using four transition rules which include spontaneous growth f s δ γ addition of new spreading centers f n s β γ edge growth f e χ γ and road influenced growth f r β ρ δ γ where δ symbolizes ca rules diffusion γ slope resistance β breed χ spread and ρ road gravity coefficients given as forecast parameters candau et al 2000 the diffusion coefficient determines the urbanization probability the slope coefficient determines the weighted probability of the local slope and the breed coefficient defines the probability for each new urbanized cell to become a new spreading center during the urban growth simulation the urbanization results from both new centers and more established centers from earlier simulation time steps if a non urbanized center cell has at least three urbanized neighboring cells it has a certain global probability to transition defined by the spread and slope coefficients various approaches to gsa have been thoroughly discussed in ravazi et al 2021 each approach distribution based variogram based regression based response surface assisted sa and sa with correlated inputs has had widespread applications across many fields among all of them surrogate modeling strategies offer dimension reduction with great computational gains and are preferable especially in computationally expensive models although there are several robust response surface methods described in the literature razavi and gupta 2016 here we adopted pce as it is easier to integrate into our model and doesn t require any additional computational cost for calculating sobol indices once a pce representation is available most sensitivity maps show the variability of first order index values over the study area these maps help to visualize the input factor s effects i e parameter effects on the model output gaining popularity among the methods of sa applied to complex models gsa addresses both interactions among model inputs and non linear output responses such that the sensitivity of model input factors is not only determined individually main effects but also as a result of their interactions crosetto et al 2000 marrel et al 2011 saltelli et al 2008 specifically the model s input factor influences on model output variability is represented by first order s i and total order s t sensitivity indices the interaction maps consisting of total order sensitivity indices show the total contribution of groups of parameters expressed by a total sensitivity index s t for example the sensitivity index s t 1 represents the total contribution of the 1st parameter to the model variance resulting from the parameter s interactions with other model parameters a low s 1 and high s t 1 mean that the contribution of the 1st parameter to the overall variability of model output is more due to this parameter s interactions with other parameters than to this parameter treated alone examples of spatial application of gsa can be seen in multiple studies abily et al 2016 gómez delgado et al 2004 ligmann zielinska and jankowski 2014 ligmann zielinska and sun 2010 marrel et al 2011 moreau et al 2013 pena et al 2014 plata rocha et al 2012 xu and zhang 2013 the majority of these applications use adjacent map representations which yield numbers of parameter maps ranging from 8 to 20 marrel et al 2011 moreau et al 2013 roura pascual et al 2010 xu and zhang 2013 ligmann zielinska and jankowski 2014 and abily et al 2016 applied a ranking method by defining the dominant parameters to prioritize the input factors here we seek to experiment with a similar coincident approach by testing its visual effectiveness 2 2 visualizing results of spatially explicit global sensitivity analysis multivariate data representation is a challenging task due to the attribute space having a higher dimension than cartesian space such as when representing urban growth by time and location one solution is to compress combine the multivariate information into a static non spatial point source representation by using bar charts pie charts glyphs or star plots spense 2014 a second option is to reduce dimensionality by using one of the following scatterplots which can be further extended to scatterplot matrices cleveland 1993 parallel coordinate plots where the multiple variables are displayed on parallel axes inselberg 1985 or principal components analysis pca where the data is projected to the axes of greatest variance jolliffe 2002 however each technique suffers from limitations in usability affected by the number of variables or visual clutter although the usability of different visualization techniques for depicting uncertainty has been investigated by many scholars brodlie et al 2012 hullman et al 2019 kinkeldey et al 2015 maceachren et al 2005 potter et al 2012 slocum et al 2003 the literature on sensitivity visualization is scarce kinkeldey et al 2014 provided a comprehensive review on uncertainty visualization and systematically summarized techniques into three dichotomies as an uncertainty visualization cube three axes were given as intrinsic extrinsic static dynamic and coincident adjacent intrinsic visualization technique alters the existing map symbology by altering color hue value or saturation in concert with other visual variables such as transparency blur or resolution although the intrinsic method gives a better overview of the uncertainty the extrinsic technique enables better in depth analysis dynamic representation is based on user control interaction although kinkeldey et al 2014 stated that animated views have a potential for uncertainty visualization they do not show any evidence that they perform better adjacent representation is found to be useful for retrieval of single values but less effective when a task becomes more complex and more eye movements are needed hence none of these techniques solves the dimensionality problem especially for raster data sets therefore we selected coincident mapping from these six techniques due to its flexibility for visually stacking of the spatial gsa attributes 3 methods 3 1 spatial gsa for land use change modeling and its visual load the urban growth component of the sleuth model was used for this study urban growth is forecasted through four growth cycles each cell in the study area goes through these cycles the outcome is depicted as the probability of that cell s becoming urban or not the san diego county dataset compiled and tested by syphard et al 2011 was used in this study for model calibration and initialization the meta modeling framework developed by şalap ayça et al 2018 was used for the spatial gsa application this framework is a spatial implementation of polynomial chaos expansion pce with variance based decomposition sobol 2001 sudret 2008 unlike the other full order variance based decomposition techniques where full order model evaluation is required to explore the full factor space pianosi et al 2016 saltelli et al 2010 in pce the minimum required sample size is based on the selected experimental design degree and model parameters this eventually yields a smaller number of samples and the sampling derivation can be found in şalap ayça and jankowski 2017 this makes pce based variance decomposition more attractive specifically for computationally expensive complex models for this experiment for 5 input forecast parameters diffusion breed spread slope and road gravity a set of 252 quasi random numbers were generated based on a triangular distribution see table 1 for ranges of model parameters then by using these samples as the input parameters the sleuth model simulated the yearly urban growth from 2001 to 2050 yielding 252 50 output forecast maps for the fifty year period these maps are used to calculate 5 50 main and 5 50 interaction effect maps when aspatial gsa is applied for n parameters the analysis output yields 2n sa index values to interpret if gsa is applied spatially the scalar index values are represented by 2n maps one index value for each mapping unit to produce and interpret for example as illustrated in fig 1 when we run the spatial gsa for urban growth each pixel has a probability of being urban along with how this probability is sensitive to the transition parameters sdiffusion sspread sroad gravity sbreed sslope and their interactions stdiffusion stspread stroad gravity stbreed stslope additionally when the model has temporal components i e urban growth forecasting a spatially distributed output is produced for each year during the simulation span as a summary each grid cell has 2 nt attribute values which can be mapped separately 3 2 coincident maps coincident methods are preferred when the primary map is insufficient to accommodate the high visual dimensionality like other multivariate data sensitivity information can also be combined with the other visualized data in a map or compared by visualizing it in a separate map view therefore adding an overlay would make the map difficult to comprehend suggested methodology for coincident maps for main and interaction effects are as follows compute the sensitivity indices of each grid cell define the maximum among all to define the most important parameter dominating factor color code the final grid cells using the value of the index by following this iterative process for each pixel in the maps of the study area it is possible to stack the n dimensional data space information into a 2d visualization space 3 3 experiment using adjacent coincident visualization in order to understand the effectiveness of the visualization a web survey was published through the san diego state university sdsu department of geography web host and was active from april 25th to june 12th 2018 each participant who accepted the consent form was directed to the first section of the survey which had an imaginary case story composed of a brief explanation of the urban growth modeling the meaning of sensitivity maps and solicited participant s responses in reaction to viewing the model output maps the participants were also reminded that full confidence in responses was neither a requirement nor an expectation in their answers in the second section of the survey the participants viewed sensitivity maps produced with a randomly assigned visualization technique and answered questions related to the maps ethical clearance for the survey was obtained from the sdsu institutional review board for human subject research 3 3 1 survey participants 65 volunteer participants responded to the survey recruited by distributing the questionnaire in selected upper division geography courses and via email listservs the participants included undergraduate and graduate students from geography departments at sdsu and the university of california santa barbara university of southern california engineering graduate students and professionals in urban planning gis environmental management engineering and data analysis reached through listservs the participant responses were grouped according to the visualization technique adjacent versus coincident the assignment of participants to groups was randomly made based on the equal chance of selection for each group and equal representation of educational levels each response was also later grouped based on the responders educational level and professional background classifying those with a background in urban planning gis and map visualization as experts and those with a background in engineering and others as non experts 3 3 2 survey instrument all participants were given a set of 12 questions including 10 map related questions and 2 demographic questions the initial 5 questions for each group were related to the visualization of spatial sensitivity analysis only the sensitivity maps resulting from the 2050 urban growth forecast output were used in these questions the fifty year period sensitivity analysis results were summarized for question 6 to depict temporal change questions 7 to 10 solicited the participants general opinion on the given visualization technique and sensitivity visualization in general questions 11 and 12 were about the education level and professional specialty area table 2 summarizes the questions variables investigated question purpose and provides the wording of the questions along with possible answer options we analyzed the responses to the above questions to answer the following research questions 1 how effective are different visual representations of input factor sensitivity in a land use change model 2 how do interaction maps showing the result of gsa inform the interpretation of land use model results 3 are the level of expertise and the choice of preferred visualization techniques independent of each other for the first research question it is hypothesized that participants who receive coincident representation will exhibit higher confidence in the sensitivity map interpretation participants who received adjacent representation are expected to exhibit less confidence in interpreting the model output in order to answer the second research question it is hypothesized that participants will be using interaction maps in their interpretation of the model output this study also collected information on the level of expertise in spatial decision making urban planning map visualization decision support gis environmental management or data analytics to determine whether the level of expertise expert versus novice user bears on the choice of visualization techniques 3 3 3 adjacent maps with static graphs for the adjacent representation the main effect sensitivity maps fig 2 were produced with matlab s plotting function to depict the result of the 2050 forecast for each model parameter s 1 diffusion s 2 breed s 3 spread s 4 slope and s 5 road gravity the maps show sensitivity analysis results for a selected sub region in san diego county the background color grey indicates no data values and the color bar shows the range of change in the sensitivity values for each parameter figs 2 and 3 were used for questions q1 to q5 in the adjacent representation group the main effect sensitivity maps fig 2 show the contribution of single input parameters including s 1 s 2 s 3 s 4 and s 5 to output variability the higher order indices which account for the interaction effects and their contribution to output variability are referred to as s t 1 s t 2 s t 3 s t 4 and s t 5 and are depicted on the corresponding maps fig 3 when only the main effect sensitivity maps are examined the ranking of sensitivity index values for the parameters with respect to their contribution to model output variability yields s 3 s 1 s 2 with two additional parameters regarded as equally important s 4 s 5 and contributing relatively little to the output variability fig 3 shows the interaction maps for the 2050 forecast for each forecast parameter representing the total contribution of the pairs of parameters expressed by the total sensitivity index st although the dominating region does not necessitate the full color spectrum it has been applied to reflect all the varying cells in the selected region moreover color value varying schemes were not sufficient to produce the acceptable color distance see brychtova and coltekin 2015 for details of color distance in sequential schemes therefore a hue varying scheme where spectral ordered hues include blue cyan green yellow and red has been selected since it is better for reading specific details gołębiowska and çöltekin 2020 the rank order of the sensitivity index values for the parameters with respect to their output variability contribution yields s t 1 s t 5 s t 3 s t 2 s t 4 therefore considering the overall variance of a single parameter s 1 should be interpreted as the most influential parameter for the selected study area to represent the change in time of model sensitivity all sensitivity maps for the forecast period 2001 2050 were summarized in a single measure which is the mean sensitivity index value per simulation year and the results were plotted on a static x y graph in fig 4 each plot represents a course of change for a parameter s sensitivity index value first order indices over the simulation time s 1 s 2 s 4 and s 5 show a relatively similar increase of the influence over the simulation time whereas the influence of s 3 shows the reverse trend of waning variability in the model output 3 3 4 coincident maps with stream graphs for the coincident representation of simulation results the main effect sensitivity maps were arranged to emphasize at each pixel the dominant factor among the five model factors fig 5 fig 5 was used for questions q1 to q5 in the coincident representation group the parallel coordinate graphs produced with matlab s parallel coordinates plot function show how the sensitivity index values change for each location the blue lines represent the change in sensitivity values for each pixel and the red line represents the mean of the sensitivity values for each parameter although it is not possible to derive the rank order of model parameters by looking at the main effect dominance map the parallel coordinates graph shows the general trend in the ranks the interpretation of the lower left part in fig 5 depicting the interaction dominance map is the same as for the main effect dominance map the only difference is that it refers to the higher order interaction between parameters to analyze the temporal variability in coincident representation a stream graph depicting the change in the parameter s sensitivity index was produced in r with the ggplot2 and ggtimeseries libraries fig 6 shows the range rank and value rank of the five model parameters the range rank determines the placement of a parameter in the graph and it is based on the difference between the maximum and minimum observed variance for example has the 1st rank the highest range among the five parameters and s 2 has the 2nd rank fig 6 left color bar hence these two parameters and their corresponding values are placed on the edges of the graph s 3 on the upper edge and s 2 on the lower edge the value rank is used for guidance in interpreting the thickness in the graph s 3 has the highest variance value and s 4 has the lowest value therefore s 3 is the thickest whereas s 4 occupies the thinnest stream in fig 6 right color bar fig 7 accompanied question q6 in the coincident representation group in order to introduce this visualization technique a brief explanation about stream graphs was given to the survey participants the participants were informed that the most varying parameter was placed on the graph s edges and the thickness of the stream corresponded to the parameter s value implying that the closer a parameter s position is to the center of the stream graph the less variability due to this parameter can be observed in the model output the increase in the fluctuations of the parameter values is represented by peaks and jagged boundary lines whereas the smooth edges are symptomatic of steady parameter values although the axes of the stream graph do not represent the exact measured values they are used as a reference and the relative rank of parameters can be established over the simulation time period 4 results a total of 65 completed survey forms of which 32 pertained to adjacent and 33 to coincident visualizations depending on the answers to two categorical questions about educational level q11 and professional expertise q12 participants were divided into novice and expert categories the participants who had at least an undergraduate college degree and a background in one or a combination of the following disciplines urban planning geography gis environmental management environmental sciences or statistics data analytics were treated as experts participants with an engineering background or lacking a college degree were designated as novices table 3 overall 48 of the participants were classified as expert and 52 as novice users the majority of participants had a positive attitude toward sensitivity visualization q10 table 2 the results showed that 56 of the participants sum of responders who answered yes to q10 thought that sensitivity visualization helped interpret the urban growth model output fig 8 we hypothesized that the effectiveness of sensitivity visualization for interpreting urban growth would differ between adjacent versus coincident maps and expert versus novice the difference in q10 with respect to adjacent and coincident maps although significant at the p value 0 10 was insignificant at the p value 0 05 adopted for the analysis of survey results chi square 5 419 p 0 067 cramer s v 0 289 and expert versus novice respondents chi square 0 682 p 0 711 cramer s v 0 102 for q10 the lack of opinion not sure response was also included in the response choices in order to help respondents who might not wish to appear uninformed or did not have an opinion on the subject krosnick and presser 2010 among the respondents who did not have a positive attitude toward sa visualization for both levels of expertise the selection between a disagreement no and uncertain answer not sure had an almost equal percentage distribution we tested the responses for all of the survey questions with the shapiro wilk test and found that the distribution of responses significantly deviated from a normal distribution given that skew and kurtosis of responses for each question also indicated a non normal distribution and that the dependent variable was not normally distributed a non parametric test was selected for the analyses the mann whitney u test was used to determine the significance of 1 dependence on the level of expertise expert versus novice users independent variables and 2 type of visualization adjacent versus coincident independent variables on confidence in sa maps dependent variables the relationship between the two groups expert versus novice and adjacent versus coincident was addressed by using ordinal regression additionally in order to understand the usage and benefit of gsa the results of q1 and q4 were analyzed to determine which representation was more effective in correctly interpreting the sensitivity maps 4 1 importance of expertise level in sa result interpretations the frequency of response categories based on respondent s expertise level is illustrated in fig 9 the divergence bars represent the levels of agreement ranging from disagreement not at all to complete agreement completely bars starting to the right of the center zero line mark 0 0 represent questions that received more positive than negative responses for example the most positive response 66 was received from novice users to q5 in general the novice users had more confidence in their answers than the expert users however this difference was not statistically significant when tested with the mann whitney u test table 4 4 2 preference of visualization techniques q7 measured the participants preference for a visualization technique almost a quarter of participants assigned the coincident representation 11 of experts and 11 of novices were uncertain in their responses to q7 of their preferences for this visualization technique whereas a similar percentage of participants 9 experts and 14 novices preferred the adjacent visualization fig 10 however the difference in preferences for these two visual representations of sa results was not statistically significant chi square 4 877 p 0 087 cramer s v 0 274 a similar frequency distribution of responses based on the dichotomy between adjacent versus coincident representations is present in fig 11 in their responses to q2 q3 q8 and q9 the participants showed more confidence in the coincident than the adjacent representation conversely with respect to q5 and q6 the participants had more confidence in the adjacent representation the greatest difference in preferences for adjacent versus coincident representations was in q9 which asked whether the sensitivity visualization made the interpretation of the model output too difficult and complex to use the coincident representation was deemed by 62 of participants as too complex and difficult to use in interpreting the results of sa versus 36 of participants indicating the complexity of adjacent representation this difference was statistically significant p 0 009 table 5 4 3 the relationship between two categorical variables expert versus novice participants and adjacent versus coincident representations we applied an ordinal regression to examine the relationship between the dependent confidence in sa map interpretation and the independent visualization technique and expertise level variables the resulting statistics are given in table 6 and the significant values are indicated in bold the base category is represented by adjacent novice interaction the odds ratio column gives the odds of an outcome increasing or decreasing when the explanatory variables change for example for q9 for coincident representation with an increase in expertise level we expect a 42 decrease 2 36 5 64 in the ordered log odds of higher confidence level model fitting in ordinal regression estimates whether the regression model improves our ability to predict the participant confidence in sa interpretation this was done by comparing the base model against the final model with all of the explanatory variables thus one can observe whether the final model has significantly improved the data fit in this study for all confidence level measures except for the information acquisition q9 the final model did not significantly improve over the baseline model model fit values in table 6 however the goodness of fit was significant indicating that the data and the model predictions were similar marking a good model r2 given by the nagelkerke statistic is the coefficient of determination and larger values indicate more variation in the confidence levels for example for q2 1 9 of the variance in the confidence level is due to model interaction table 6 all of the calculated nagelkerke statistics except for q9 are relatively small values indicating less variation in the confidence levels for the final statistics in table 6 the test of parallel lines the significance values greater than 0 05 mean that there is no difference in location parameters interaction between two independent variables given in exploratory variable column in table 6 groups across the response categories 4 4 interpretation of interaction maps q1 was analyzed using a simple percentage of correct versus incorrect answers with 65 participants almost 37 answered the question correctly by selecting s 1 as the most influential model parameter and considering both main and interaction effects fig 12 which shows that those participants were able to interpret both maps correctly however the ability to interpret maps differed by visual representation type 44 of correct answers for adjacent versus 30 correct answers for coincident maps which means that the visualization assignment highlighted a difference in terms of map reading skills moreover there was also a difference in correct answers among the different levels of expertise 25 correct answers from experts versus 12 correct answers from novices q4 asked about the usage of interaction maps during the interpretation of sensitivity index measures figs 13 and 14 show the drill downs by visualization category and expertise level in figs 13 and 14 the column values yes and no represent answers to q4 and the row values 1 5 correspond to sensitivity index values for the respective parameters s 1 s 2 s 3 s 4 and s 5 for example the first column and the first row in fig 13 the orange color bubble 38 means that 38 of the participants assigned to the adjacent map group 12 out of 32 people selected the 1st parameter s 1 and used interaction maps 61 of the experts 19 out of 31 people stated that they used the interaction maps whereas this ratio was 79 27 out of 34 people for the novice users among the 44 6 of participants 29 out of 65 people who selected s 3 as the most influential parameter 41 3 12 out of 29 stated that they did not use the interaction maps this is the expected outcome since s 3 is the obvious choice when only the main effect sensitivity maps are considered similarly out of 70 7 46 out of 65 people of the participants who used the interaction maps 43 5 20 out of 46 people selected s 1 as the most influential parameter which means they not only used the interaction maps but also successfully interpreted the map content 5 discussion this study aimed to introduce a visual load reduction method and assess the effectiveness of the proposed method by comparing the existing approach for sa visualization fig 15 summarizes the confidence level frequencies of the level of agreement questions grouped by expertise level and visualization type only for q9 was there a statistically significant association between visualization technique and agreement with the statement in the question given the border line significance of some differences between the groups concerning map preferences and map effectiveness it would be worthwhile to repeat the study using a larger group of participants for the first research question it was hypothesized that the participants assigned a coincident representation would exhibit higher confidence in the map interpretation of sa results than the participants assigned a single adjacent representation based on the survey results this hypothesis must be rejected the effectiveness of the two selected visual representations of input factor sensitivity in the land use change model does not significantly differ between coincident and adjacent representations for q9 asserting that sensitivity visualization makes the interpretation of the model output too difficult and complex to use the negative answer implies a positive attitude in this respect less than half of the expert users 47 5 the sum of 17 22 and 8 5 for q9 expert coincident in fig 15 regarded the coincident method as uncomplicated whereas only 26 5 4 out of 15 of novices did fig 15 this could be interpreted as the coincident maps being too complex for novice users despite the intent to synthesize multiple maps into one map differences in perception between the novice and expert users can be explained by the visualization concepts introduced by dibiase 1990 and further developed as cartography cubes by maceachren and taylor 1994 and maceachren and kraak 1997 2001 the commonality in these studies is that they all interpret the importance of visualization as a communication tool to reach the audience and as an analytic support device for private visual thinking maceachren and kraak 2001 transitioning from exploratory maps typically used by scientists and experts to communication maps used by stakeholders students or citizens the communication effectiveness mostly depends on the cartographic language used the adjacent and coincident maps used in this study can be arguably characterized as exploratory it follows to reason that they would be more effectively used by experts than novices although this expectation has only been confirmed by the study results with respect to the complexity of usage q9 one could argue that there is a need to strengthen the communication effectiveness of maps employed to visually represent the results of uncertainty and sensitivity analysis this need becomes even more apparent if uncertainty and sensitivity measures are communicated to stakeholders and decision makers as part of participatory decision making processes it also follows from the survey results that using a single type of visualization may not always be effective for all levels of expertise the second research question focuses on the usage of interaction maps while for the majority of the survey questions there was no one correct answer there was an expected correct answer for the first question participants who opted for using the interaction maps were also expected to give different answers from the participants who decided not to use the interaction maps the usage and the understanding of interaction maps which depict the results of global sensitivity analysis can be inferred from analyzing the responses to q1 when the responses are compared across the groups the highest rate of correct answers comes from experts who used coincident representations for the adjacent visualization the expertise level does not seem to affect the rate of correct answers however the difference between the correct answers under the use of coincident versus the use of adjacent representation becomes 10 times higher for experts than for novices 46 out of 65 participants stated that they used interaction maps in their interpretation for the participants who selected s 1 as the most influential factor and confirmed the use of interaction maps 19 out of 65 the interaction maps proved to help interpret the results of sa however the majority of the participants who used the interaction maps could not interpret the sa results correctly this may be due to interpreting the main effects and interactions of model input factors separately and the difficulty in effectively integrating these two sources of information one future solution to this problem could be integrating the main and interaction effects s 1 and s t into a single visual representation the last research question assessed the level of expertise and its relation to visualization technique preference in reference to the uncertain endeavour maceachren 1992 p 17 uncertainty or sensitivity information can already be a burden for some users especially novices simple interpretation tasks which do not involve complex decision making can be carried out with adjacent representations however for complex multi location comparisons an adjacent map comparison may introduce a cognitive overload harrower 2003 viard et al 2011 in this study we had only 5 input factors which resulted in 10 maps to process the outcomes could vary dramatically if the number of inputs increased to for example 20 or 30 which would likely result in more eye movement and comparison difficulty in such cases we expect the superiority of coincident maps to be more obvious hence there is a need to create an effective design of coincident representation different from the one used in this study and cognitively easier for novice users the analysis of the interaction effect by expertise level and the choice of preferred visualization on the confidence in sa map interpretation table 6 revealed that only the responses to q9 showed a significant difference between experts and novices in the visualization choice since the combination of adjacent novice was selected as the baseline for the regression depending on the odds ratio we can say that the odds of a novice user who chose the coincident representation achieving a high level of agreement for q9 are approximately 6 times the odds of novice users who selected the adjacent representation this means that a novice user is expected to deem a coincident representation as more difficult for sa visualization than an adjacent representation considering the skills of novice users their preference for adjacent representation might be predictable since the adjacent map representation is still the most widely used form of spatial visualization in the case of several parameters spatial variables 6 conclusion limitations and future work in most decision making problems the end users do not always have to know nor do they care about the details of model output variability yet they might be open to learning about model output reliability if they could visualizing sensitivity maps therefore sensitivity visualization is one of the emerging research frontiers in spatial decision making especially important for addressing effective ways to represent model variances linked to parameter uncertainties malczewski and jankowski 2020 in this paper we presented the visual stacking approach of coincident maps as a solution to multi dimensional visual load for gsa output visualization below is a summary of findings and their limitations 1 coincident maps help to simplify the visualization effort into a 2d representation however there is still room for improvement of the effective visualization techniques and tools to support decision making by showing land use change model sensitivity 2 parallel plots are effective in revealing the relationships between adjacent variables as well as showing the distribution of attributes however overlapping of line segments is a frequent occurrence since many data records have the same or similar values or the number of data records is large relative to the display 3 results underlined a difficulty among novice users in integrating the separate information coming from gsa therefore for future studies we suggest possible research directions to overcome the above limitations and move beyond them 1 the maps depicting information about model uncertainty and sensitivity should be simple but informative consequently simplifying visualization forms further and making them interactive should be the next step in improving visual representations for sa mapping 2 future research efforts should focus on how the relative rankings of sensitivity indices can be addressed one possible solution could consider a grouping strategy as in sheikholeslami et al 2019 and huo et al 2019 3 in this study we had a research perspective of the visualization problem in a follow up to this study one could include domain experts and their preferences to analyze their specific requirements and re structure the research questions to better consider existing knowledge finally an interesting open research question about the spatial gsa for spatially explicit complex models is about the unknown unknowns the performance of gsa methods can be sensitive to the randomness in sampling variability which is only amplified by spatial and or spatial temporal variability future studies could investigate other sampling strategies as the one presented in this paper that in addition to sampling and spatial variabilities also account for the dynamic properties of environmental models as in the variogram analysis of response surface vars razavi and gupta 2016 therefore a future study accounting for the randomness of sample points in spatially explicit dynamic models by either experimenting with different random seeds or using methods such as bootstrapping can help to better understand the unknown unknowns in spatial models funding this work was supported by the national science foundation grant numbers bcs 1263071 cns 0960316 dmr 1720256 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment we acknowledge support from the center for earth systems analysis research department of geography at san diego state university and the university of california santa barbara center for scientific computing from the cnsi mrl an nsf mrsec grant no dmr 1720256 and nsf grant no cns 0960316 this research was supported in part by the national science foundation geography and spatial sciences program grant no bcs 1263071 any opinion findings conclusions and recommendations expressed in this paper are those of the authors s and do not necessarily reflect the views of the national science foundation we also appreciate the feedback provided by the anonymous reviewers and the special issue editors on the earlier version of the manuscript the corresponding author is also grateful for the support of her writing accountability group during the covid 19 pandemic appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105181 
25731,porousmedia4foam is a package for solving flow and transport in porous media using openfoam a popular open source numerical toolbox we introduce and highlight the features of a new generation open source hydro geochemical module implemented within porousmedia4foam which relies on micro continuum concept and which makes it possible to investigate hydro geochemical processes occurring at multiple scales i e at the pore scale reservoir scale and at the hybrid scale geochemistry is handled by a third party package e g phreeqc that is coupled to the flow and transport solver of openfoam we conducted benchmarks across different scales to validate the accuracy of our simulator we further looked at the evolution of mineral dissolution precipitation in a fractured porous system application fields of this new package include the investigation of hydro bio geochemical processes in the critical zone the modelling of contaminant transport in aquifers as well as and the assessment of confinement performance for geological barriers keywords reactive transport modelling multi scale simulations micro continuum openfoam phreeqc 1 introduction over the last decades reactive transport modelling rtm has become an essential tool for the study of subsurface processes involving flow transport and geochemical reactions steefel et al 2015a this discipline is at the junction of two scientific communities namely geochemistry and transport in porous media rtm consists of computational models that describe the coupled physical chemical and biological processes that interact with each other over a broad range of spatial and temporal scales rtm modelling tools enable the prediction of contaminant migration in polluted aquifers and are used to design enhanced remediation techniques integration of physical and biogeochemical processes makes rtm also an ideal research instrument for elucidating the complex and non linear interactions between roots micro organisms water composition and minerals in the critical zone li et al 2017 other applications include the assessment of the long term integrity of reservoirs for storing carbon dioxide hydrogen or nuclear wastes in deep geological formations depaolo and cole 2013 claret et al 2018 in practice three different kinds of models are used to describe reactive transport in porous media as illustrated in fig 1 i continuum models ii pore scale models and iii hybrid models that combine both former approaches continuum models see fig 1c are representative of the historical and standard approach for solving reactive transport in large scale natural porous systems lichtner 1985 continuum scale rtm codes include among others min3p mayer et al 2002 crunchflow steefel et al 2015a toughreact xu et al 2006 pflotran lichtner et al 2015 or hp1 jacques et al 2008 in such codes flow and transport equations are formulated in terms of volume averaged equations with respect to a representative elementary volume rev of the porous structure bear 1972 and are coupled with geochemical reactions see steefel et al 2015a for an comprehensive description of the coupling the topology of the rock micro structures is described using effective properties including porosity tortuosity permeability or hydraulic conductivity and specific surface area flow is usually modelled using darcy s law and multi component species transport relies on a set of advection dispersion reaction equations in addition to the classic challenges related to transport in porous media including the medium heterogeneity awareness and the description of hydrodynamic dispersion rtm has to consider the variation of rock properties in response to chemical reactions indeed by enlarging or clogging pore throats and fractures geochemical processes such as minerals dissolution and precipitation can alter the local flowfield and subsequently modify the rock properties e g permeability tortuosity accessible reactive surface area poonoosamy et al 2020 seigneur et al 2019 changes in rock properties with chemical reaction is usually described in continuum scale rtm as heuristic functions of the porosity for example in most of reactive transport codes tortuosity is described using archies s law permeability change is modelled using kozeny carman relationship and mineral surface areas evolve as a two third power law of the porosity xie et al 2015 however the complex interplay between reactions advection and diffusion can lead to highly nonlinear porosity feedback that is poorly captured using this kind of relationships that were not built on a strong theoretical background daccord and lenormand 1987 garing et al 2015 the limiting factor of the continuum scale models is therefore the determination of empirical parameters and their evolution as a function of the progress of geochemical processes to circumvent these challenges recent efforts have focused on the numerical modelling of coupled hydro geochemical processes at the pore scale békri et al 1997 chen et al 2013 tartakovsky et al 2007 molins et al 2014 molins et al 2017 in pore scale models see fig 1a the pore network is fully resolved i e each point of space is occupied by either a fluid or solid phase as the exact knowledge of the phase distribution is known continuum scale concepts such as porosity permeability and reactive surface area do not apply at the pore scale they can be obtained however by averaging pore scale simulation results if the computational domain is large enough to reach the size of a rev whitaker 1999 soulaine et al 2013 the strategy that consists in simulating flow and transport in a three dimensional image of a porous sample to characterize its continuum scale properties has become an independent scientific discipline sometimes referred to as digital rock physics blunt et al 2013 andrä et al 2013a andrä et al 2013b soulaine et al 2021 most of the efforts so far have been devoted to solve the navier stokes equations under single spanne et al 1994 bijeljic et al 2013 guibert et al 2015 soulaine et al 2016 and two phase flow conditions horgue et al 2013 raeini et al 2014 graveleau et al 2017 maes and soulaine 2018 pavuluri et al 2020 to compute absolute and relative permeabilities despite the growing investment in the development of rtm at the pore scale pioneer simulators date back to the late 90s békri et al 1997 the pore scale rtm field is still emerging one of the main challenge of this approach consists in moving the fluid solid boundary with respect to chemical reactions at the mineral boundaries noiriel and soulaine 2021 a comprehensive review of the different approaches used to solve this problem can be found in molins et al molins et al 2020 it is only very recently that pore scale simulators have been proved mature for reproducing accurately and without any adjusting parameters the dissolution of a calcite crystal soulaine et al 2017 molins et al 2020 or of a gypsum grain dutka et al 2020 actually the benchmark presented in molins et al molins et al 2020 is a first effort based on a relatively simple geochemical reaction a single component that reacts with a single mineral using a first order kinetics to demonstrate the ability of current codes to accurately simulate mineral dissolution at the pore scale in a reproducible manner with several codes further developments and verifications still need to be done for simulating multi component aqueous solutions interacting with heterogeneous multi mineral media using comprehensive reaction networks naturally occurring porous media involve a wide range of spatial scales for example the important contrast in pore size distributions in fractured porous rocks comes from much larger characteristic lengths for the fractures than for the surrounding porous matrix therefore the domain size required to reach a rev limits the use of pure pore scale modelling hybrid scale models have been proposed to describe systems that include multiple characteristic length scales for which some regions are described using pore scale modelling while others are modelled with continuum approaches see fig 1b liu and ortoleva 1996 liu et al 1997 two different approaches have been developed to solve hybrid scale problems on the one hand the domain decomposition technique solves different physics on separate domains one for darcy flow another for stokes flow linked together through appropriate boundary conditions molins et al 2019 including beaver joseph and ochoa tapia whitaker conditions beavers and joseph 1967 ochoa tapia and whitaker 1995 on the other hand micro continuum models use a single set of partial differential equations throughout the computational domain regardless of the content of a grid block steefel et al 2015b soulaine and tchelepi 2016 the latter approach is particularly well suited to capture the dynamic displacement of the interface between the porous and solid free regions without involving complex re meshing strategies for example micro continuum models have been used successfully to simulate the formation and growth of wormholes in acidic environments ormond and ortoleva 2000 golfier et al 2002 soulaine and tchelepi 2016 hybrid scale modelling is also a powerful tool in image based simulations to account for microscale features that are not visible in the images because they are smaller than the imaging instrument resolution arns et al 2005 apourvari and arns 2004 scheibe et al 2015 soulaine et al 2016 soulaine et al 2019 in this study we developed a comprehensive open source simulator to model coupled hydro geochemical processes at continuum pore and hybrid scales this unique multi scale framework relies on the micro continuum model and its ability to tend asymptotically toward continuum scale models if grid block contains solid content 0 φ 1 or towards pore scale models otherwise φ 1 soulaine and tchelepi 2016 the resulting advanced rtm allows the treatment of complex reactions network as a function of flow conditions water composition and minerals distribution within the rock including the complex porosity feedback between flow and chemistry it is part of porousmedia4foam an open source package developed by the authors to solve flow and transport in porous media within the popular simulation platform openfoam because of its versatility and advanced features such as three dimensional unstructured grids dynamic meshes and high performance computing there is a growing interest in the community to develop mathematical models for solving flow and transport in porous media within openfoam horgue et al 2015 maes and geiger 2018 orgogozo et al 2014 soulaine et al 2017 we developed a generic interface to combine flow and transport models with existing geochemical packages we illustrate the versatility of our coupling interface by combining flow models with geochemical models using phreeqc parkhurst and wissmeier 2015 an open source and popular geochemistry package that is used in many continuum scale rtm rolle et al 2018 healy et al 2018 muniruzzaman and rolle 2019 muniruzzaman et al 2020 moortgat et al 2020 in section 2 we present the mathematical models implemented in porousmedia4foam including a multi scale and a continuum scale flow solver a wide range of porous properties models and the packages used to perform geochemical calculations in section 3 we verify the robustness of the coupled hydro geochemical platform by simulating cases for which reference solutions exist both at the continuum scale and at the pore scale simulation results are compared with the results obtained with state of the art reactive transport codes then in section 4 we use the simulation framework to illustrate the potential of porousmedia4foam to model hybrid scale cases 2 the porousmedia4foam package the multi scale solver for simulating hydro geochemical problems is part of porousmedia4foam https github com csoulain porousmedia4foam a generic platform for solving flow and transport in porous media at various scales of interest porousmedia4foam is an open source platform developed by the authors using the c library openfoam http www openfoam org hence the package benefits from all the features of openfoam including the solution of partial differential equations using the finite volume method on three dimensional unstructured grids as well as high performance computing although porousmedia4foam has capabilities for solving two phase flow liquid liquid and liquid gas in porous systems the geochemistry coupling that is introduced in this paper only considers single phase flow the code is organized in three interacting parts a class that describes porous media properties section 2 4 the flow solvers section 2 2 and the geochemical packages section 2 3 as porousmedia4foam intends to be a versatile platform it is designed in a such way that other porous media models other flow solvers or other geochemical packages can be easily implemented using the c code architecture in this section we introduce the models and their numerical implementation in the code it is organized as a multiple entry document in which each model is presented in a self contained manner so that users can directly refer to the subsection associated with the said model 2 1 mineral distribution and porosity a geological medium is made of an assembly of n s minerals whose porous properties are defined in section 2 4 the distribution of each mineral i on the computational grid is determined by the volume fraction 1 y s i x y z t with i 1 n s in each grid block the n s solid volume fraction fields are dimensionless they can be initialized with uniform or distributed values the evolution of y s i due to geochemical reactions is dictated by the geochemical packages that are described in section 2 3 in some simulations it is relevant to define an inert mineral y s inert that is not part of the geochemical calculation the porosity field is computed by 2 φ 1 i n s 1 y s i y s i n e r t and can be updated at any moment following dissolution or precipitation processes the porosity update at every time step is optional 2 2 flow solvers porousmedia4foam for hydro geochemical simulations includes three flow models a multi scale flow solver based on the micro continuum approach a continuum scale darcy solver and a constant velocity solver see table 1 2 2 1 db sfoam multi scale micro continuum flow model d bsfoam is a multi scale flow solver based on the micro continuum modelling approach developed in soulaine and tchelepi soulaine and tchelepi 2016 micro continuum approaches are intermediate between a pure navier stokes description of the transport for which all the porosity is fully resolved see fig 1a and a pure continuum scale modelling for which the flow is governed by darcy s law see fig 1c this hybrid scale approach relies on the darcy brinkman stokes dbs equation brinkman 1947 that allows for the modelling of flow and transport in regions free of solid and porous regions in a single framework neale and nader 1974 soulaine and tchelepi 2016 dbs equation arises from the integration of navier stokes equations over a control volume vafai and tien 1981 hsu and cheng 1990 bousquet melou et al 2002 goyeau et al 2003 the momentum equation reads 3 1 φ ρ f v f t ρ f φ v f v f p f ρ f g μ f φ v f t v f μ f k 1 v f where φ is the porosity v f is the seepage velocity p f is the fluid pressure g is the gravity ρ f is the fluid density μ f is the fluid viscosity and k is the cell permeability the porous media properties including porosity and permeability change dynamically with geochemical processes and are updated at every time steps eq 3 is valid throughout the computational domain regardless the content of a cell in regions that contain fluid only φ 1 and the drag force μ f k 1 v f vanishes so that the momentum equation tends to the navier stokes equation in regions that contain an aggregate of fluid and solid 0 φ 1 and the drag force is dominant over the inertial and viscous forces so that eq 3 tends asymptotically to darcy s law the momentum equation eq 3 can be used to model pore scale flows indeed if a solid region is approximated by a low permeability low porosity matrix the velocity in this region goes to near zero which forces a no slip boundary condition at the fluid solid interface this feature of the dbs equation is particularly relevant to solve navier stokes problems using cartesian grids only also called penalized approach angot et al 1999 soulaine and tchelepi 2016 and to move the fluid solid interface with respect to geochemical processes such as precipitation dissolution soulaine et al 2017 molins et al 2020 or swelling by using the local porosity field φ as a phase indicator function carrillo and bourg 2019 the pressure velocity coupling is achieved by solving the momentum equation along with the micro continuum continuity equation for multiple minerals for an incompressible newtonian aqueous fluid the latter reads 4 v f i 1 n s m s i 1 ρ f 1 ρ s i where ρ s i is the solid density of mineral i and m s i represents the rate of phase change of solid into fluid or of a fluid into solid for example it can represent the rate of solid minerals that is dissolved into aqueous solution inversely it can describe an amount of fluid that is removed of a control volume because it has precipitated the right hand side of eq 4 is provided by the geochemistry calculation section 2 3 although this term if often neglected in continuum scale models it ensures the mass balance at the fluid solid interface in pore scale simulations soulaine et al 2017 as well as in continuum scale simulations seigneur et al 2018 the flow model formed by eqs 3 and 4 is discretized using the finite volume method and solved sequentially the pressure velocity coupling is handled by a predictor corrector strategy based on the pimple algorithm implemented in openfoam it consists in a combination of piso pressure implicit with splitting of operator issa 1985 and simple semi implicit method for pressure linked equations patankar 1980 pimple algorithm allows both transient and steady state simulations moreover pimple enables larger time steps than piso further information regarding the numerics is found in soulaine and tchelepi 2016 2 2 2 darcyfoam darcy s law d arcyfoam is a standard continuum scale solver that is based on darcy s law 5 v f k μ f p f ρ f g for describing flow in porous media numerically eq 5 is combined along with eq 4 to form a laplace equation solving implicitly for the pressure field p f then the velocity field is calculated point wise using eq 5 and p f if activateporosityfeedback is switched on darcy s law is recalculated at every time steps to update the velocity and pressure fields according to the new permeability value boundary conditions can be described by imposing fixed pressure or fixed velocity values on the domain edges however as darcyfoam solves implicitly for the pressure field the boundary conditions on the velocity are transformed into pressure gradient conditions using darcy s law 6 n p f n μ f k 1 v f ρ f g where n is the vector normal to the domain boundary in the code eq 6 is achieved using the boundary condition darcygradpressure horgue et al 2015 2 2 3 constantvelocityfoam constant flow rate the flow solver constantvelocityfoam is used to model cases in which the chemical species are transported using a steady state velocity field v f uniform or non uniform provided as an input data that can come from a separate flow simulation c onstantvelocityfoam is particularly useful if the feedback between geochemical reactions and the flow is negligible indeed in such a case the characteristic timescale of flow changes is much longer than the characteristic time of species transport and the calculation of the velocity profile can be decoupled from the species transport 2 3 geochemical packages in porousmedia4foam complex reaction networks are handled by geochemical packages the aqueous components are transported using the velocity profile v f computed by the flow solver see section 2 2 and surface reactions rely on the reactive surface area a e calculated with the porous media models see section 2 4 2 the code architecture of porousmedia4foam is generic so that a wide variety of third party geochemical packages can be coupled with our platform for solving hydro geochemical processes at the pore scale and at the continuum scale in this paper we illustrate the potential of the coupled simulation framework using the popular geochemistry package phreeqc parkhurst and appelo 2013 parkhurst and wissmeier 2015 models currently implemented in porousmedia4foam to account for geochemistry are summarized in table 2 the geochemical packages update the water composition c j and the distribution of the solid minerals y s i and return the rate of solid changes 7 m s i ρ s i y s i t where ρ s i is the density of solid mineral i 2 3 1 phreeqcrm the phreeqcrm class calls the general purpose geochemical reaction model phreeqc through the phreeqcrm module it carries out the transport of the aqueous solution composition c j in mol kgwater along with equilibrium and kinetic reactions with the solid minerals described by y s i p hreeqcrm is set with components i e solution master species total concentration the geochemistry setup is carried out using an input file that follows phreeqc format hence the aqueous composition is defined in the block solution 0 for the composition of the injected fluid at the inlet boundary 1 for the initial aqueous composition in the bulk the equilibrium phases and kinetics blocks are generated automatically within the code and the user only has to assign before the calculation which mode of reactions is used for each mineral moreover porousmedia4foam can load any customized database using phreeqc format the coupling between transport and reactions relies on an operator splitting approach based on the strang s algorithm strang 1968 first all species concentration fields c j are transported sequentially using the advection dispersion equations 8 φ c j t v f c j φ d j c j 0 where v f is the fluid velocity computed with the flow solver see section 2 2 and d j is an effective diffusion tensor that accounts for tortuosity and hydrodynamic dispersion effects see section 2 4 3 the transport equation is discretized on the computational domain using the finite volume method and solved implicitly using openfoam s engines then the volume fractions of solid minerals y s i are updated according to phase equilibrium and or kinetic reaction calculations provided by phreeqc reaction kinetics use the surface area computed at every time steps using the surface area models in section 2 4 2 it corresponds to the surface area per volume and its units are m2 m3 or m 1 hence the rate block provided in phreeqc database to describe reaction rates has to be defined accordingly 2 3 2 simplefirstorderkineticmole s implefirstorderkineticmole is a simple geochemical engine for solving the transport of a single species labelled a that reacts with solid minerals using first order kinetic reactions it is an extension to multiple minerals of the model used in the benchmark presented in molins et al 2020 in which pore scale simulators were used to model the dissolution of a calcite crystal by hydrochloric acid the chemical reaction reads 9 m i n e r a l i s a b i the mass balance equation for species a reads 10 φ c a t v f c a φ d j c a j 1 n s a s j k j a γ a c a where v f is the fluid velocity d j is an effective diffusion tensor a s j is the reactive surface area of mineral j in m 1 and k j a γ a is the constant of reaction of the species a with the mineral j in m s following the notations adopted in molins et al 2020 in simplefirstorderkineticmole the concentration field c j is defined in mol m3 the equation is discretized on the computational grid using the finite volume method and solved implicitly the distribution of solid minerals evolves according to 11 y s i t a s a k i a γ a v m s i c a where v m s i in m3 mol is the molar volume of the reacting mineral 2 3 3 transportonly t ransportonly solves the advection dispersion equation 12 φ c j t v f c j φ d j c j 0 without considering geochemistry it allows the transport of species using the dispersion models implemented in porousmedia4foam see table 5 2 3 4 flowonly f lowonly is an empty class for computing velocity profiles without species transport nor geochemistry for example poonoosamy et al 2020 used the multi scale flow solver of porousmedia4foam to compute the steady state velocity profile in absence of geochemical reactions within a two scale domain i e a domain that contains both solid free regions and porous regions see fig 1b this option is particularly interesting in cases for which geochemistry and flow can be treated independently from each other 2 4 porous media models the flow solvers and geochemistry modules rely on porous media properties including absolute permeability specific surface area and dispersion tensor these properties describe pore scale effects related to the micro structure geometry of the porous medium hence they may change if the micro structure evolves with geochemical reactions 2 4 1 absolute permeability models the absolute permeability describes the ability of a porous medium to conduct the flow this property is intrinsic to the medium micro structure and therefore evolves with geochemical processes including precipitation and dissolution porousmedia4foam includes several porosity permeability relationships summarized in table 3 2 4 2 surface area models the estimation of the reactive surface area is crucial to model geochemical processes described by kinetic reactions actually reactive surface area is a difficult quantity to assess as only a portion of the geometric surface area is accessible to the reactants for example in an advection dominated transport only the surfaces at the vicinity of the faster flowlines react soulaine et al 2017 leading to a reactive surface area smaller than the geometric surface area moreover the evolution of the specific surface area as the mineral volume fractions change due to dissolution or precipitation is not necessarily monotonic noiriel et al 2009 table 4 summarizes the models implemented in porousmedia4foam to describe the surface area as a function of the mineral volume fraction unlike continuum scale simulations the surface area in pore scale modelling is not an input parameter but is a direct output of the simulation indeed at the pore scale the micro structure of the porous medium is fully resolved in the computational grid and there is a sharp interface between the fluid and the solid mineral the volume of solid model computes the surface area of an explicit fluid solid interface using the gradient of the volume fraction of mineral see soulaine et al 2017 for additional details on the technique 2 4 3 dispersion models in porous media the spreading of a solute is not governed only by molecular diffusion d i but also by the micro structure and the local velocity field on the one hand the tortuosity of the porous structure tends to slow down the spreading on the other hand hydrodynamic dispersion stretches a solute band in the flow direction during its transport in porousmedia4foam a single effective diffusion tensor d i is used to represent both mechanisms the models implemented in the code are summarized in table 5 3 verification of the hydro geochemical simulation platform in this section the multi scale hydro geochemical simulation package porousmedia4foam introduced in section 2 is used along with phreeqc to investigate several scenarios for which reference solutions exist the verification of the results is achieved by comparison against benchmarks published in literature both at the continuum scale and at the pore scale essential files required to run all the test cases presented in this section are available as examples within the package all simulations were run on intel xeon with 2 60 ghz 3 1 verification at the continuum scale we verify the ability of porousmedia4foam to simulate coupled hydro geochemical processes that include porosity feedback on the transport properties at the continuum scale we also verify that the multi scale solver db sfoam tends asymptotically towards darcy s law in porous domains the case is based on the benchmark 1 described in xie et al 2015 it consists of a 2 meters long 1d column initially filled with 35 of inert mineral and 30 of calcite an acid ph 3 is continuously injected at the inlet to initiate the dissolution of calcite according to 13 c a c o 3 h c a 2 h c o 3 h c o 3 h c o 3 2 table 6 provides the initial and boundary conditions data specific to the primary components a difference of 0 007 m in hydraulic head is applied between the inlet and outlet xie et al 2015 by fixing the pressure at 70 pa and 0 pa respectively at the inlet and outlet boundaries throughout the simulation the calcite dissolution with porosity feedback is simulated using both the multi scale solver db sfoam and the continuum scale solver darcyfoam the aqueous species are transported by advection only the calcite dissolution is modelled using a kinetic rate of reaction with k calcite 5 10 5 mol m2 s and the initial specific area a 0 1 m2 m3 as the calcite dissolves the surface area decreases according to a power law function with n 2 3 see table 4 the porosity permeability relationship is described by the kozeny carman equation table 3 the initial permeability of the column is set to k 0 1 186 10 11 m2 the column is spatially discretized with δx 25 mm 80 cells 150 years are simulated with δt 21600 s there is a perfect match between db sfoam and darcyfoam confirming that the multi scale solver converges well to the continuum scale solution predicted by darcy s law fig 2 the analysis of results includes the evolution of porosity fig 2a calcite volume fraction fig 2b hydraulic head fig 2c along the column length and the outflux over time fig 2d to be consistent with the benchmark of xie et al 2015 we consider the cross sectional area of the column to be 1 m2 as the dissolution of calcite occurs the calcite volume fraction decreases and the porosity increases over time in fig 2c we notice different slopes of hydraulic head at different times the slope is minimal where porosity is large and vice versa the velocity and therefore the outflux increases over time as the permeability and porosity of the system increases the evolution of porosity calcite volume fraction hydraulic head and outflux predicted by porousmedia4foam solvers are in close agreement with those of min3p which demonstrates the ability of our platform to simulate hydro geochemical processes with porosity feedback 3 2 verification at the pore scale in this section we highlight the capabilities of our openfoam package to model hydro geochemical interactions occurring at the pore scale using phreeqc in porousmedia4foam pore scale simulations are run using the micro continuum approach through the flow solver db sfoam at the pore scale the reaction rates are directly applied at the fluid mineral interface that is described explicitly in the computational grid using the mineral volume fraction the micro continuum approach has been used to simulate the dissolution of a calcite crystal at the pore scale and compared successfully with microfluidic experiments soulaine et al 2017 in molins et al 2020 the approach is compared with state of the art rtm at the pore scale using various numerical techniques including chombo cruch with level set molins et al 2017 lattice boltzmann method prasianakis et al 2018 dissolfoam moving grids with conformal mapping starchenko et al 2016 and vortex methods sanchez et al 2019 the benchmark consists of a 0 2 mm diameter calcite crystal posted in a 1 mm long 0 5 width channel see fig 3 an acidic solution of ph 2 is continuously injected from the inlet at a rate of u inj 1 2 10 3 m s the calcite crystal dissolution is described considering a kinetic rate 14 r a calcite k calcite γ c h where r is the reaction rate in mol m3 s a calcite is the reactive surface area in m2 m3 computed using the volume of solid approach see table 4 k calcite γ 0 89 1 0 3 m s is the reaction rate constant of calcite and c h is the concentration of h in mol m3 this reaction rate may not be fully representative of the underlying geochemical processes it has been chosen in molins et al 2020 to demonstraste the ability of various approaches to move the fluid mineral interface according to geochemical processes all the numerical methods were able to capture accurately the shape evolution of the calcite crystal giving confidence in pore scale simulators for moving fluid mineral interfaces along with geochemical processes actually in soulaine et al 2017 and molins et al 2020 the micro continuum approach db sfoam was combined with the geochemical package simplefirstorderkineticsmole see table 2 that solves eq 14 using openfoam s internal engines this limits drastically the applicability of the approach to comprehensive reaction networks in this section we reproduce the two dimensional case presented in molins et al 2020 using db sfoam and phreeqcrm to demonstrate the robustness of our coupling between openfoam and phreeqc at the pore scale the kinetic rate in phreeqc input file has been modified to match eq 14 the system is spatially discretized using a cartesian mesh of 128 64 cells the simulation is run for 45 minutes using a time step size δt 5 ms the shape evolution of the calcite grain determined by the two approaches matches perfectly fig 4 which verifies therefore that in our modelling plateform phreeqc can be used to model hydro geochemical interactions occurring at the pore scale this gives us confidence for further investigations that rely on more complex reactive transport phenomena occurring at the pore scale 4 hybrid scale simulation in fractured porous media in most subsurface environments fractures intercept porous media domains these fractures act as free flow zones transporting substantial quantities of fluids alongside chemical species compared to the flow and transport that occur within the porous medium noiriel et al 2007 ajo franklin et al 2018 the complex interplay between advection diffusion and reaction can lead to very different dissolution and precipitation patterns for example poonoosamy et al 2020 uses micro raman spectroscopy to visualise the replacement of celestite with barite in a fractured porous media flooded with a solution that contains barium ions they observe that the mineral replacement occurs either uniformly or at the vicinity of the fracture matrix interface this difference in the mineral distribution was attributed to the injection flow rates leading to advection or diffusion dominated transport we investigate such a multiscale system where a fracture is sandwiched in between a reactive porous matrix made of 50 celestite srso4 having specific reactive surface area of a 0 20000 m2 m3 as shown in fig 5 the fracture has a length of ℓ 0 03 m and height h 0 002 m transport phenomena in the fracture is fully resolved i e the flow is governed by navier stokes equations whereas the flow in the matrix is described by darcy s law this hybrid scale case is modelled using the db sfoam solver the initial porosity and permeability of the porous medium are φ 0 0 5 and k 0 10 12 m2 respectively a solution containing 300 mol m3 of barium ba2 is continuously injected through the inlet at a constant velocity u inj for 200 hours the dispersivity of species within the porous matrix are taken into account considering linear dispersion law table 5 with molecular diffusion set to d j 10 9 m2 s hydrodynamic dispersion coefficient set to α l 10 5 m and tortuosity exponent set to n 2 once the barium ions reach the porous matrix celestite dissociates into strontium sr2 and sulphate s o 4 2 ions the barium ions react with sulphate ions resulting in the precipitation of barite baso4 according to the following reaction poonoosamy et al 2018 15 b a 2 s r s o 4 b a s o 4 s r 2 celestite dissolution is taken into account considering kinetics with k celestite 10 5 66 mol m2 s whereas barite precipitation is accounted considering phase equilibrium celestite reactive surface area evolves linearly with its volume fraction the matrix permeability is updated according to kozeny carman we investigate the ongoing hydrogeochemistry within this system considering two different injection velocities u inj 10 2 m s and u inj 10 6 m s the péclet number pe u inj ℓ d j where the reference length scale is the fracture aperture characterizes the importance of advection with respect to diffusion within the fracture the highest velocity corresponds to advection dominated transport pe 104 while the lowest corresponds to diffusion dominated regime pe 1 for both cases the second damkhöler number that determines the timescale of reaction with respect to the timescale of species diffusion at the mineral surface is da i i k celestite ℓ c b a 2 d j 3 6 1 0 4 where the reference length scale is the inverse of the specific surface area ℓ a 0 1 according to soulaine et al 2017 we notice differences in the pattern of celestite dissolution and barite precipitation whether the transport in the fracture is dominated by advection or by diffusion in agreement with poonoosamy et al 2020 observations fig 6 for advection dominated regime pe 1 there are two characteristic timescales for the solute transport first the barium ions flow through the fracture by advection then they diffuse laterally into the matrix because of the timescale contrast between the two processes the concentration profile of barium ions is uniform along the porous matrix which leads to a uniform pattern of celestite dissolution and barite precipitation as seen in figs 6 and 7 for diffusion dominated regimes pe 1 the characteristic transport timescales both in the fracture and in the matrix are of the same order of magnitude therefore the front of barium ions in the matrix follows the diffusive front within the fracture subsequently we observe mineral dissolution celestite and precipitation barite fronts within the porous matrix figs 6 and 7 this illustration emphasizes the capabilities of porousmedia4foam to model dual porosity systems in reactive environments using hybrid scale approach our platform is therefore a powerful tool to complement and augment reactive transport experiments including high resolution imaging of the evolution the fracture aperture including the effect of the weathered zone noiriel et al 2007 noiriel et al 2009 ajo franklin et al 2018 deng et al 2020 and two scale reactive microfluidic experiments poonoosamy et al 2020 nissan et al 2021 osselin et al 2016 in a logic of cascade of scales nested within each other fractured porous media can be modelled by i pore scale approaches ii discrete fracture networks iii dual porosity models the hybrid scale approach that we propose in this paper is intermediate between a full pore scale description in which the fracture and the pores in the matrix are fully resolved and a discrete fracture network in which the matrix is modelled as a porous medium and the fractures as discrete elements that exchange matter with the matrix the hybrid scale approach is therefore crucial to characterize and improve the effective parameters e g transfer functions between the porous matrix and the fracture used in discrete fracture networks and larger scale models 5 conclusion we developed an integrated open source simulator to model hydro geochemical processes at various scales of interest including pore scale and reservoir scale the simulation platform is part of porousmedia4foam a package that solves flow and transport in porous media using the open source library openfoam the modelling framework handles complex reactions network as a function of flow conditions water composition and minerals distribution within the rock including the complex porosity feedback between flow and chemistry moreover porousmedia4foam benefits from all features of openfoam libraries hence the code is fully parallel and handles structured as well as unstructured grids in one two and three dimensions the interface between the flow simulator and the geochemistry is generic and can be used to couple a large variety of geochemical packages in this paper we illustrated the hydro geochemical capabilities of the coupled solver using phreeqc unlike other reactive transport simulators porousmedia4foam is multi scale i e a unique flow solver describes transport processes both at the continuum scale and the pore scale importantly the two scales can be solved simultaneously in geological formations that feature large contrast of permeability and porosity for example in fractured rocks porousmedia4foam solves stokes flow in the fracture network and darcy s law in the porous matrix this multi scale model is achieved using the micro continuum approach hybrid scale approach based on the darcy brinkman stokes equation indeed this approach is intermediate between a pure navier stokes description of the transport for which all the porosity is fully resolved and pure continuum scale modeling based on darcy s law besides this hybrid scale approach porousmedia4foam also includes a standard darcy solver for continuum scale simulations therefore the same simulator can be used to simulate flow transport and geochemical reactions in an reservoir and in 3d micro tomography images the coupled hydro geochemical simulator was verified by running cases for which reference solutions exist these solutions are well established and used in the reactive transport community to benchmark state of the art codes available both at the continuum scale xie et al 2015 and at the pore scale molins et al 2020 finally we demonstrated the ability of our advanced modelling framework to simulate dissolution and precipitation processes in fractured porous media at the pore scale using the hybrid scale approach here the reactive medium consisted of celestite grains that reacted with a barium chloride solution injected into the system leading to the dissolution of celestite and the growth of barite we observed differences in mineral precipitation dissolution patterns by varying the injection rates because porousmedia4foam has already capabilities for modelling two phase flow in porous media both at the pore and darcy s scales using two phase micro continuum technique soulaine et al 2019 soulaine et al 2018 carrillo et al 2020 true multiscale and multiphase rtm is envisioned to be implemented in porousmedia4foam framework software and data availability porousmedia4foam the software introduced in this paper is built using open source libraries including openfoam and phreeqc the source code and the cases presented in the paper are available on github https github com csoulain porousmedia4foam contributions of the authors cs is the porousmedia4foam architect cs and sp implemented new models in porousmedia4foam and corrected bugs sp and ct designed and setup the benchmark problems sp run the simulations cs sp ct and fc discussed interpreted the results and wrote the paper cs ct and fc applied for funding declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the research leading to these results has received funding from the french agency for research agence nationale de la recherche anr through the labex voltaire anr 10 labx 100 01 the grant catch anr 18 ce05 0035 and through the framati project under contract anr 19 ce05 0002 it has also received financial support from the cnrs through the miti interdisciplinary programs this project has received funding from the european union s horizon 2020 research and innovation programme under grant agreements no 847593 wp donut and no 850626 reflect project sp postdoctoral fellowship was funded by brgm through the pore reactif project from the alliance nationale de coordination de la recherche pour l energie ancre the authors benefitted from the use of the cluster at the centre de calcul scientifique en région centre val de loire 
25731,porousmedia4foam is a package for solving flow and transport in porous media using openfoam a popular open source numerical toolbox we introduce and highlight the features of a new generation open source hydro geochemical module implemented within porousmedia4foam which relies on micro continuum concept and which makes it possible to investigate hydro geochemical processes occurring at multiple scales i e at the pore scale reservoir scale and at the hybrid scale geochemistry is handled by a third party package e g phreeqc that is coupled to the flow and transport solver of openfoam we conducted benchmarks across different scales to validate the accuracy of our simulator we further looked at the evolution of mineral dissolution precipitation in a fractured porous system application fields of this new package include the investigation of hydro bio geochemical processes in the critical zone the modelling of contaminant transport in aquifers as well as and the assessment of confinement performance for geological barriers keywords reactive transport modelling multi scale simulations micro continuum openfoam phreeqc 1 introduction over the last decades reactive transport modelling rtm has become an essential tool for the study of subsurface processes involving flow transport and geochemical reactions steefel et al 2015a this discipline is at the junction of two scientific communities namely geochemistry and transport in porous media rtm consists of computational models that describe the coupled physical chemical and biological processes that interact with each other over a broad range of spatial and temporal scales rtm modelling tools enable the prediction of contaminant migration in polluted aquifers and are used to design enhanced remediation techniques integration of physical and biogeochemical processes makes rtm also an ideal research instrument for elucidating the complex and non linear interactions between roots micro organisms water composition and minerals in the critical zone li et al 2017 other applications include the assessment of the long term integrity of reservoirs for storing carbon dioxide hydrogen or nuclear wastes in deep geological formations depaolo and cole 2013 claret et al 2018 in practice three different kinds of models are used to describe reactive transport in porous media as illustrated in fig 1 i continuum models ii pore scale models and iii hybrid models that combine both former approaches continuum models see fig 1c are representative of the historical and standard approach for solving reactive transport in large scale natural porous systems lichtner 1985 continuum scale rtm codes include among others min3p mayer et al 2002 crunchflow steefel et al 2015a toughreact xu et al 2006 pflotran lichtner et al 2015 or hp1 jacques et al 2008 in such codes flow and transport equations are formulated in terms of volume averaged equations with respect to a representative elementary volume rev of the porous structure bear 1972 and are coupled with geochemical reactions see steefel et al 2015a for an comprehensive description of the coupling the topology of the rock micro structures is described using effective properties including porosity tortuosity permeability or hydraulic conductivity and specific surface area flow is usually modelled using darcy s law and multi component species transport relies on a set of advection dispersion reaction equations in addition to the classic challenges related to transport in porous media including the medium heterogeneity awareness and the description of hydrodynamic dispersion rtm has to consider the variation of rock properties in response to chemical reactions indeed by enlarging or clogging pore throats and fractures geochemical processes such as minerals dissolution and precipitation can alter the local flowfield and subsequently modify the rock properties e g permeability tortuosity accessible reactive surface area poonoosamy et al 2020 seigneur et al 2019 changes in rock properties with chemical reaction is usually described in continuum scale rtm as heuristic functions of the porosity for example in most of reactive transport codes tortuosity is described using archies s law permeability change is modelled using kozeny carman relationship and mineral surface areas evolve as a two third power law of the porosity xie et al 2015 however the complex interplay between reactions advection and diffusion can lead to highly nonlinear porosity feedback that is poorly captured using this kind of relationships that were not built on a strong theoretical background daccord and lenormand 1987 garing et al 2015 the limiting factor of the continuum scale models is therefore the determination of empirical parameters and their evolution as a function of the progress of geochemical processes to circumvent these challenges recent efforts have focused on the numerical modelling of coupled hydro geochemical processes at the pore scale békri et al 1997 chen et al 2013 tartakovsky et al 2007 molins et al 2014 molins et al 2017 in pore scale models see fig 1a the pore network is fully resolved i e each point of space is occupied by either a fluid or solid phase as the exact knowledge of the phase distribution is known continuum scale concepts such as porosity permeability and reactive surface area do not apply at the pore scale they can be obtained however by averaging pore scale simulation results if the computational domain is large enough to reach the size of a rev whitaker 1999 soulaine et al 2013 the strategy that consists in simulating flow and transport in a three dimensional image of a porous sample to characterize its continuum scale properties has become an independent scientific discipline sometimes referred to as digital rock physics blunt et al 2013 andrä et al 2013a andrä et al 2013b soulaine et al 2021 most of the efforts so far have been devoted to solve the navier stokes equations under single spanne et al 1994 bijeljic et al 2013 guibert et al 2015 soulaine et al 2016 and two phase flow conditions horgue et al 2013 raeini et al 2014 graveleau et al 2017 maes and soulaine 2018 pavuluri et al 2020 to compute absolute and relative permeabilities despite the growing investment in the development of rtm at the pore scale pioneer simulators date back to the late 90s békri et al 1997 the pore scale rtm field is still emerging one of the main challenge of this approach consists in moving the fluid solid boundary with respect to chemical reactions at the mineral boundaries noiriel and soulaine 2021 a comprehensive review of the different approaches used to solve this problem can be found in molins et al molins et al 2020 it is only very recently that pore scale simulators have been proved mature for reproducing accurately and without any adjusting parameters the dissolution of a calcite crystal soulaine et al 2017 molins et al 2020 or of a gypsum grain dutka et al 2020 actually the benchmark presented in molins et al molins et al 2020 is a first effort based on a relatively simple geochemical reaction a single component that reacts with a single mineral using a first order kinetics to demonstrate the ability of current codes to accurately simulate mineral dissolution at the pore scale in a reproducible manner with several codes further developments and verifications still need to be done for simulating multi component aqueous solutions interacting with heterogeneous multi mineral media using comprehensive reaction networks naturally occurring porous media involve a wide range of spatial scales for example the important contrast in pore size distributions in fractured porous rocks comes from much larger characteristic lengths for the fractures than for the surrounding porous matrix therefore the domain size required to reach a rev limits the use of pure pore scale modelling hybrid scale models have been proposed to describe systems that include multiple characteristic length scales for which some regions are described using pore scale modelling while others are modelled with continuum approaches see fig 1b liu and ortoleva 1996 liu et al 1997 two different approaches have been developed to solve hybrid scale problems on the one hand the domain decomposition technique solves different physics on separate domains one for darcy flow another for stokes flow linked together through appropriate boundary conditions molins et al 2019 including beaver joseph and ochoa tapia whitaker conditions beavers and joseph 1967 ochoa tapia and whitaker 1995 on the other hand micro continuum models use a single set of partial differential equations throughout the computational domain regardless of the content of a grid block steefel et al 2015b soulaine and tchelepi 2016 the latter approach is particularly well suited to capture the dynamic displacement of the interface between the porous and solid free regions without involving complex re meshing strategies for example micro continuum models have been used successfully to simulate the formation and growth of wormholes in acidic environments ormond and ortoleva 2000 golfier et al 2002 soulaine and tchelepi 2016 hybrid scale modelling is also a powerful tool in image based simulations to account for microscale features that are not visible in the images because they are smaller than the imaging instrument resolution arns et al 2005 apourvari and arns 2004 scheibe et al 2015 soulaine et al 2016 soulaine et al 2019 in this study we developed a comprehensive open source simulator to model coupled hydro geochemical processes at continuum pore and hybrid scales this unique multi scale framework relies on the micro continuum model and its ability to tend asymptotically toward continuum scale models if grid block contains solid content 0 φ 1 or towards pore scale models otherwise φ 1 soulaine and tchelepi 2016 the resulting advanced rtm allows the treatment of complex reactions network as a function of flow conditions water composition and minerals distribution within the rock including the complex porosity feedback between flow and chemistry it is part of porousmedia4foam an open source package developed by the authors to solve flow and transport in porous media within the popular simulation platform openfoam because of its versatility and advanced features such as three dimensional unstructured grids dynamic meshes and high performance computing there is a growing interest in the community to develop mathematical models for solving flow and transport in porous media within openfoam horgue et al 2015 maes and geiger 2018 orgogozo et al 2014 soulaine et al 2017 we developed a generic interface to combine flow and transport models with existing geochemical packages we illustrate the versatility of our coupling interface by combining flow models with geochemical models using phreeqc parkhurst and wissmeier 2015 an open source and popular geochemistry package that is used in many continuum scale rtm rolle et al 2018 healy et al 2018 muniruzzaman and rolle 2019 muniruzzaman et al 2020 moortgat et al 2020 in section 2 we present the mathematical models implemented in porousmedia4foam including a multi scale and a continuum scale flow solver a wide range of porous properties models and the packages used to perform geochemical calculations in section 3 we verify the robustness of the coupled hydro geochemical platform by simulating cases for which reference solutions exist both at the continuum scale and at the pore scale simulation results are compared with the results obtained with state of the art reactive transport codes then in section 4 we use the simulation framework to illustrate the potential of porousmedia4foam to model hybrid scale cases 2 the porousmedia4foam package the multi scale solver for simulating hydro geochemical problems is part of porousmedia4foam https github com csoulain porousmedia4foam a generic platform for solving flow and transport in porous media at various scales of interest porousmedia4foam is an open source platform developed by the authors using the c library openfoam http www openfoam org hence the package benefits from all the features of openfoam including the solution of partial differential equations using the finite volume method on three dimensional unstructured grids as well as high performance computing although porousmedia4foam has capabilities for solving two phase flow liquid liquid and liquid gas in porous systems the geochemistry coupling that is introduced in this paper only considers single phase flow the code is organized in three interacting parts a class that describes porous media properties section 2 4 the flow solvers section 2 2 and the geochemical packages section 2 3 as porousmedia4foam intends to be a versatile platform it is designed in a such way that other porous media models other flow solvers or other geochemical packages can be easily implemented using the c code architecture in this section we introduce the models and their numerical implementation in the code it is organized as a multiple entry document in which each model is presented in a self contained manner so that users can directly refer to the subsection associated with the said model 2 1 mineral distribution and porosity a geological medium is made of an assembly of n s minerals whose porous properties are defined in section 2 4 the distribution of each mineral i on the computational grid is determined by the volume fraction 1 y s i x y z t with i 1 n s in each grid block the n s solid volume fraction fields are dimensionless they can be initialized with uniform or distributed values the evolution of y s i due to geochemical reactions is dictated by the geochemical packages that are described in section 2 3 in some simulations it is relevant to define an inert mineral y s inert that is not part of the geochemical calculation the porosity field is computed by 2 φ 1 i n s 1 y s i y s i n e r t and can be updated at any moment following dissolution or precipitation processes the porosity update at every time step is optional 2 2 flow solvers porousmedia4foam for hydro geochemical simulations includes three flow models a multi scale flow solver based on the micro continuum approach a continuum scale darcy solver and a constant velocity solver see table 1 2 2 1 db sfoam multi scale micro continuum flow model d bsfoam is a multi scale flow solver based on the micro continuum modelling approach developed in soulaine and tchelepi soulaine and tchelepi 2016 micro continuum approaches are intermediate between a pure navier stokes description of the transport for which all the porosity is fully resolved see fig 1a and a pure continuum scale modelling for which the flow is governed by darcy s law see fig 1c this hybrid scale approach relies on the darcy brinkman stokes dbs equation brinkman 1947 that allows for the modelling of flow and transport in regions free of solid and porous regions in a single framework neale and nader 1974 soulaine and tchelepi 2016 dbs equation arises from the integration of navier stokes equations over a control volume vafai and tien 1981 hsu and cheng 1990 bousquet melou et al 2002 goyeau et al 2003 the momentum equation reads 3 1 φ ρ f v f t ρ f φ v f v f p f ρ f g μ f φ v f t v f μ f k 1 v f where φ is the porosity v f is the seepage velocity p f is the fluid pressure g is the gravity ρ f is the fluid density μ f is the fluid viscosity and k is the cell permeability the porous media properties including porosity and permeability change dynamically with geochemical processes and are updated at every time steps eq 3 is valid throughout the computational domain regardless the content of a cell in regions that contain fluid only φ 1 and the drag force μ f k 1 v f vanishes so that the momentum equation tends to the navier stokes equation in regions that contain an aggregate of fluid and solid 0 φ 1 and the drag force is dominant over the inertial and viscous forces so that eq 3 tends asymptotically to darcy s law the momentum equation eq 3 can be used to model pore scale flows indeed if a solid region is approximated by a low permeability low porosity matrix the velocity in this region goes to near zero which forces a no slip boundary condition at the fluid solid interface this feature of the dbs equation is particularly relevant to solve navier stokes problems using cartesian grids only also called penalized approach angot et al 1999 soulaine and tchelepi 2016 and to move the fluid solid interface with respect to geochemical processes such as precipitation dissolution soulaine et al 2017 molins et al 2020 or swelling by using the local porosity field φ as a phase indicator function carrillo and bourg 2019 the pressure velocity coupling is achieved by solving the momentum equation along with the micro continuum continuity equation for multiple minerals for an incompressible newtonian aqueous fluid the latter reads 4 v f i 1 n s m s i 1 ρ f 1 ρ s i where ρ s i is the solid density of mineral i and m s i represents the rate of phase change of solid into fluid or of a fluid into solid for example it can represent the rate of solid minerals that is dissolved into aqueous solution inversely it can describe an amount of fluid that is removed of a control volume because it has precipitated the right hand side of eq 4 is provided by the geochemistry calculation section 2 3 although this term if often neglected in continuum scale models it ensures the mass balance at the fluid solid interface in pore scale simulations soulaine et al 2017 as well as in continuum scale simulations seigneur et al 2018 the flow model formed by eqs 3 and 4 is discretized using the finite volume method and solved sequentially the pressure velocity coupling is handled by a predictor corrector strategy based on the pimple algorithm implemented in openfoam it consists in a combination of piso pressure implicit with splitting of operator issa 1985 and simple semi implicit method for pressure linked equations patankar 1980 pimple algorithm allows both transient and steady state simulations moreover pimple enables larger time steps than piso further information regarding the numerics is found in soulaine and tchelepi 2016 2 2 2 darcyfoam darcy s law d arcyfoam is a standard continuum scale solver that is based on darcy s law 5 v f k μ f p f ρ f g for describing flow in porous media numerically eq 5 is combined along with eq 4 to form a laplace equation solving implicitly for the pressure field p f then the velocity field is calculated point wise using eq 5 and p f if activateporosityfeedback is switched on darcy s law is recalculated at every time steps to update the velocity and pressure fields according to the new permeability value boundary conditions can be described by imposing fixed pressure or fixed velocity values on the domain edges however as darcyfoam solves implicitly for the pressure field the boundary conditions on the velocity are transformed into pressure gradient conditions using darcy s law 6 n p f n μ f k 1 v f ρ f g where n is the vector normal to the domain boundary in the code eq 6 is achieved using the boundary condition darcygradpressure horgue et al 2015 2 2 3 constantvelocityfoam constant flow rate the flow solver constantvelocityfoam is used to model cases in which the chemical species are transported using a steady state velocity field v f uniform or non uniform provided as an input data that can come from a separate flow simulation c onstantvelocityfoam is particularly useful if the feedback between geochemical reactions and the flow is negligible indeed in such a case the characteristic timescale of flow changes is much longer than the characteristic time of species transport and the calculation of the velocity profile can be decoupled from the species transport 2 3 geochemical packages in porousmedia4foam complex reaction networks are handled by geochemical packages the aqueous components are transported using the velocity profile v f computed by the flow solver see section 2 2 and surface reactions rely on the reactive surface area a e calculated with the porous media models see section 2 4 2 the code architecture of porousmedia4foam is generic so that a wide variety of third party geochemical packages can be coupled with our platform for solving hydro geochemical processes at the pore scale and at the continuum scale in this paper we illustrate the potential of the coupled simulation framework using the popular geochemistry package phreeqc parkhurst and appelo 2013 parkhurst and wissmeier 2015 models currently implemented in porousmedia4foam to account for geochemistry are summarized in table 2 the geochemical packages update the water composition c j and the distribution of the solid minerals y s i and return the rate of solid changes 7 m s i ρ s i y s i t where ρ s i is the density of solid mineral i 2 3 1 phreeqcrm the phreeqcrm class calls the general purpose geochemical reaction model phreeqc through the phreeqcrm module it carries out the transport of the aqueous solution composition c j in mol kgwater along with equilibrium and kinetic reactions with the solid minerals described by y s i p hreeqcrm is set with components i e solution master species total concentration the geochemistry setup is carried out using an input file that follows phreeqc format hence the aqueous composition is defined in the block solution 0 for the composition of the injected fluid at the inlet boundary 1 for the initial aqueous composition in the bulk the equilibrium phases and kinetics blocks are generated automatically within the code and the user only has to assign before the calculation which mode of reactions is used for each mineral moreover porousmedia4foam can load any customized database using phreeqc format the coupling between transport and reactions relies on an operator splitting approach based on the strang s algorithm strang 1968 first all species concentration fields c j are transported sequentially using the advection dispersion equations 8 φ c j t v f c j φ d j c j 0 where v f is the fluid velocity computed with the flow solver see section 2 2 and d j is an effective diffusion tensor that accounts for tortuosity and hydrodynamic dispersion effects see section 2 4 3 the transport equation is discretized on the computational domain using the finite volume method and solved implicitly using openfoam s engines then the volume fractions of solid minerals y s i are updated according to phase equilibrium and or kinetic reaction calculations provided by phreeqc reaction kinetics use the surface area computed at every time steps using the surface area models in section 2 4 2 it corresponds to the surface area per volume and its units are m2 m3 or m 1 hence the rate block provided in phreeqc database to describe reaction rates has to be defined accordingly 2 3 2 simplefirstorderkineticmole s implefirstorderkineticmole is a simple geochemical engine for solving the transport of a single species labelled a that reacts with solid minerals using first order kinetic reactions it is an extension to multiple minerals of the model used in the benchmark presented in molins et al 2020 in which pore scale simulators were used to model the dissolution of a calcite crystal by hydrochloric acid the chemical reaction reads 9 m i n e r a l i s a b i the mass balance equation for species a reads 10 φ c a t v f c a φ d j c a j 1 n s a s j k j a γ a c a where v f is the fluid velocity d j is an effective diffusion tensor a s j is the reactive surface area of mineral j in m 1 and k j a γ a is the constant of reaction of the species a with the mineral j in m s following the notations adopted in molins et al 2020 in simplefirstorderkineticmole the concentration field c j is defined in mol m3 the equation is discretized on the computational grid using the finite volume method and solved implicitly the distribution of solid minerals evolves according to 11 y s i t a s a k i a γ a v m s i c a where v m s i in m3 mol is the molar volume of the reacting mineral 2 3 3 transportonly t ransportonly solves the advection dispersion equation 12 φ c j t v f c j φ d j c j 0 without considering geochemistry it allows the transport of species using the dispersion models implemented in porousmedia4foam see table 5 2 3 4 flowonly f lowonly is an empty class for computing velocity profiles without species transport nor geochemistry for example poonoosamy et al 2020 used the multi scale flow solver of porousmedia4foam to compute the steady state velocity profile in absence of geochemical reactions within a two scale domain i e a domain that contains both solid free regions and porous regions see fig 1b this option is particularly interesting in cases for which geochemistry and flow can be treated independently from each other 2 4 porous media models the flow solvers and geochemistry modules rely on porous media properties including absolute permeability specific surface area and dispersion tensor these properties describe pore scale effects related to the micro structure geometry of the porous medium hence they may change if the micro structure evolves with geochemical reactions 2 4 1 absolute permeability models the absolute permeability describes the ability of a porous medium to conduct the flow this property is intrinsic to the medium micro structure and therefore evolves with geochemical processes including precipitation and dissolution porousmedia4foam includes several porosity permeability relationships summarized in table 3 2 4 2 surface area models the estimation of the reactive surface area is crucial to model geochemical processes described by kinetic reactions actually reactive surface area is a difficult quantity to assess as only a portion of the geometric surface area is accessible to the reactants for example in an advection dominated transport only the surfaces at the vicinity of the faster flowlines react soulaine et al 2017 leading to a reactive surface area smaller than the geometric surface area moreover the evolution of the specific surface area as the mineral volume fractions change due to dissolution or precipitation is not necessarily monotonic noiriel et al 2009 table 4 summarizes the models implemented in porousmedia4foam to describe the surface area as a function of the mineral volume fraction unlike continuum scale simulations the surface area in pore scale modelling is not an input parameter but is a direct output of the simulation indeed at the pore scale the micro structure of the porous medium is fully resolved in the computational grid and there is a sharp interface between the fluid and the solid mineral the volume of solid model computes the surface area of an explicit fluid solid interface using the gradient of the volume fraction of mineral see soulaine et al 2017 for additional details on the technique 2 4 3 dispersion models in porous media the spreading of a solute is not governed only by molecular diffusion d i but also by the micro structure and the local velocity field on the one hand the tortuosity of the porous structure tends to slow down the spreading on the other hand hydrodynamic dispersion stretches a solute band in the flow direction during its transport in porousmedia4foam a single effective diffusion tensor d i is used to represent both mechanisms the models implemented in the code are summarized in table 5 3 verification of the hydro geochemical simulation platform in this section the multi scale hydro geochemical simulation package porousmedia4foam introduced in section 2 is used along with phreeqc to investigate several scenarios for which reference solutions exist the verification of the results is achieved by comparison against benchmarks published in literature both at the continuum scale and at the pore scale essential files required to run all the test cases presented in this section are available as examples within the package all simulations were run on intel xeon with 2 60 ghz 3 1 verification at the continuum scale we verify the ability of porousmedia4foam to simulate coupled hydro geochemical processes that include porosity feedback on the transport properties at the continuum scale we also verify that the multi scale solver db sfoam tends asymptotically towards darcy s law in porous domains the case is based on the benchmark 1 described in xie et al 2015 it consists of a 2 meters long 1d column initially filled with 35 of inert mineral and 30 of calcite an acid ph 3 is continuously injected at the inlet to initiate the dissolution of calcite according to 13 c a c o 3 h c a 2 h c o 3 h c o 3 h c o 3 2 table 6 provides the initial and boundary conditions data specific to the primary components a difference of 0 007 m in hydraulic head is applied between the inlet and outlet xie et al 2015 by fixing the pressure at 70 pa and 0 pa respectively at the inlet and outlet boundaries throughout the simulation the calcite dissolution with porosity feedback is simulated using both the multi scale solver db sfoam and the continuum scale solver darcyfoam the aqueous species are transported by advection only the calcite dissolution is modelled using a kinetic rate of reaction with k calcite 5 10 5 mol m2 s and the initial specific area a 0 1 m2 m3 as the calcite dissolves the surface area decreases according to a power law function with n 2 3 see table 4 the porosity permeability relationship is described by the kozeny carman equation table 3 the initial permeability of the column is set to k 0 1 186 10 11 m2 the column is spatially discretized with δx 25 mm 80 cells 150 years are simulated with δt 21600 s there is a perfect match between db sfoam and darcyfoam confirming that the multi scale solver converges well to the continuum scale solution predicted by darcy s law fig 2 the analysis of results includes the evolution of porosity fig 2a calcite volume fraction fig 2b hydraulic head fig 2c along the column length and the outflux over time fig 2d to be consistent with the benchmark of xie et al 2015 we consider the cross sectional area of the column to be 1 m2 as the dissolution of calcite occurs the calcite volume fraction decreases and the porosity increases over time in fig 2c we notice different slopes of hydraulic head at different times the slope is minimal where porosity is large and vice versa the velocity and therefore the outflux increases over time as the permeability and porosity of the system increases the evolution of porosity calcite volume fraction hydraulic head and outflux predicted by porousmedia4foam solvers are in close agreement with those of min3p which demonstrates the ability of our platform to simulate hydro geochemical processes with porosity feedback 3 2 verification at the pore scale in this section we highlight the capabilities of our openfoam package to model hydro geochemical interactions occurring at the pore scale using phreeqc in porousmedia4foam pore scale simulations are run using the micro continuum approach through the flow solver db sfoam at the pore scale the reaction rates are directly applied at the fluid mineral interface that is described explicitly in the computational grid using the mineral volume fraction the micro continuum approach has been used to simulate the dissolution of a calcite crystal at the pore scale and compared successfully with microfluidic experiments soulaine et al 2017 in molins et al 2020 the approach is compared with state of the art rtm at the pore scale using various numerical techniques including chombo cruch with level set molins et al 2017 lattice boltzmann method prasianakis et al 2018 dissolfoam moving grids with conformal mapping starchenko et al 2016 and vortex methods sanchez et al 2019 the benchmark consists of a 0 2 mm diameter calcite crystal posted in a 1 mm long 0 5 width channel see fig 3 an acidic solution of ph 2 is continuously injected from the inlet at a rate of u inj 1 2 10 3 m s the calcite crystal dissolution is described considering a kinetic rate 14 r a calcite k calcite γ c h where r is the reaction rate in mol m3 s a calcite is the reactive surface area in m2 m3 computed using the volume of solid approach see table 4 k calcite γ 0 89 1 0 3 m s is the reaction rate constant of calcite and c h is the concentration of h in mol m3 this reaction rate may not be fully representative of the underlying geochemical processes it has been chosen in molins et al 2020 to demonstraste the ability of various approaches to move the fluid mineral interface according to geochemical processes all the numerical methods were able to capture accurately the shape evolution of the calcite crystal giving confidence in pore scale simulators for moving fluid mineral interfaces along with geochemical processes actually in soulaine et al 2017 and molins et al 2020 the micro continuum approach db sfoam was combined with the geochemical package simplefirstorderkineticsmole see table 2 that solves eq 14 using openfoam s internal engines this limits drastically the applicability of the approach to comprehensive reaction networks in this section we reproduce the two dimensional case presented in molins et al 2020 using db sfoam and phreeqcrm to demonstrate the robustness of our coupling between openfoam and phreeqc at the pore scale the kinetic rate in phreeqc input file has been modified to match eq 14 the system is spatially discretized using a cartesian mesh of 128 64 cells the simulation is run for 45 minutes using a time step size δt 5 ms the shape evolution of the calcite grain determined by the two approaches matches perfectly fig 4 which verifies therefore that in our modelling plateform phreeqc can be used to model hydro geochemical interactions occurring at the pore scale this gives us confidence for further investigations that rely on more complex reactive transport phenomena occurring at the pore scale 4 hybrid scale simulation in fractured porous media in most subsurface environments fractures intercept porous media domains these fractures act as free flow zones transporting substantial quantities of fluids alongside chemical species compared to the flow and transport that occur within the porous medium noiriel et al 2007 ajo franklin et al 2018 the complex interplay between advection diffusion and reaction can lead to very different dissolution and precipitation patterns for example poonoosamy et al 2020 uses micro raman spectroscopy to visualise the replacement of celestite with barite in a fractured porous media flooded with a solution that contains barium ions they observe that the mineral replacement occurs either uniformly or at the vicinity of the fracture matrix interface this difference in the mineral distribution was attributed to the injection flow rates leading to advection or diffusion dominated transport we investigate such a multiscale system where a fracture is sandwiched in between a reactive porous matrix made of 50 celestite srso4 having specific reactive surface area of a 0 20000 m2 m3 as shown in fig 5 the fracture has a length of ℓ 0 03 m and height h 0 002 m transport phenomena in the fracture is fully resolved i e the flow is governed by navier stokes equations whereas the flow in the matrix is described by darcy s law this hybrid scale case is modelled using the db sfoam solver the initial porosity and permeability of the porous medium are φ 0 0 5 and k 0 10 12 m2 respectively a solution containing 300 mol m3 of barium ba2 is continuously injected through the inlet at a constant velocity u inj for 200 hours the dispersivity of species within the porous matrix are taken into account considering linear dispersion law table 5 with molecular diffusion set to d j 10 9 m2 s hydrodynamic dispersion coefficient set to α l 10 5 m and tortuosity exponent set to n 2 once the barium ions reach the porous matrix celestite dissociates into strontium sr2 and sulphate s o 4 2 ions the barium ions react with sulphate ions resulting in the precipitation of barite baso4 according to the following reaction poonoosamy et al 2018 15 b a 2 s r s o 4 b a s o 4 s r 2 celestite dissolution is taken into account considering kinetics with k celestite 10 5 66 mol m2 s whereas barite precipitation is accounted considering phase equilibrium celestite reactive surface area evolves linearly with its volume fraction the matrix permeability is updated according to kozeny carman we investigate the ongoing hydrogeochemistry within this system considering two different injection velocities u inj 10 2 m s and u inj 10 6 m s the péclet number pe u inj ℓ d j where the reference length scale is the fracture aperture characterizes the importance of advection with respect to diffusion within the fracture the highest velocity corresponds to advection dominated transport pe 104 while the lowest corresponds to diffusion dominated regime pe 1 for both cases the second damkhöler number that determines the timescale of reaction with respect to the timescale of species diffusion at the mineral surface is da i i k celestite ℓ c b a 2 d j 3 6 1 0 4 where the reference length scale is the inverse of the specific surface area ℓ a 0 1 according to soulaine et al 2017 we notice differences in the pattern of celestite dissolution and barite precipitation whether the transport in the fracture is dominated by advection or by diffusion in agreement with poonoosamy et al 2020 observations fig 6 for advection dominated regime pe 1 there are two characteristic timescales for the solute transport first the barium ions flow through the fracture by advection then they diffuse laterally into the matrix because of the timescale contrast between the two processes the concentration profile of barium ions is uniform along the porous matrix which leads to a uniform pattern of celestite dissolution and barite precipitation as seen in figs 6 and 7 for diffusion dominated regimes pe 1 the characteristic transport timescales both in the fracture and in the matrix are of the same order of magnitude therefore the front of barium ions in the matrix follows the diffusive front within the fracture subsequently we observe mineral dissolution celestite and precipitation barite fronts within the porous matrix figs 6 and 7 this illustration emphasizes the capabilities of porousmedia4foam to model dual porosity systems in reactive environments using hybrid scale approach our platform is therefore a powerful tool to complement and augment reactive transport experiments including high resolution imaging of the evolution the fracture aperture including the effect of the weathered zone noiriel et al 2007 noiriel et al 2009 ajo franklin et al 2018 deng et al 2020 and two scale reactive microfluidic experiments poonoosamy et al 2020 nissan et al 2021 osselin et al 2016 in a logic of cascade of scales nested within each other fractured porous media can be modelled by i pore scale approaches ii discrete fracture networks iii dual porosity models the hybrid scale approach that we propose in this paper is intermediate between a full pore scale description in which the fracture and the pores in the matrix are fully resolved and a discrete fracture network in which the matrix is modelled as a porous medium and the fractures as discrete elements that exchange matter with the matrix the hybrid scale approach is therefore crucial to characterize and improve the effective parameters e g transfer functions between the porous matrix and the fracture used in discrete fracture networks and larger scale models 5 conclusion we developed an integrated open source simulator to model hydro geochemical processes at various scales of interest including pore scale and reservoir scale the simulation platform is part of porousmedia4foam a package that solves flow and transport in porous media using the open source library openfoam the modelling framework handles complex reactions network as a function of flow conditions water composition and minerals distribution within the rock including the complex porosity feedback between flow and chemistry moreover porousmedia4foam benefits from all features of openfoam libraries hence the code is fully parallel and handles structured as well as unstructured grids in one two and three dimensions the interface between the flow simulator and the geochemistry is generic and can be used to couple a large variety of geochemical packages in this paper we illustrated the hydro geochemical capabilities of the coupled solver using phreeqc unlike other reactive transport simulators porousmedia4foam is multi scale i e a unique flow solver describes transport processes both at the continuum scale and the pore scale importantly the two scales can be solved simultaneously in geological formations that feature large contrast of permeability and porosity for example in fractured rocks porousmedia4foam solves stokes flow in the fracture network and darcy s law in the porous matrix this multi scale model is achieved using the micro continuum approach hybrid scale approach based on the darcy brinkman stokes equation indeed this approach is intermediate between a pure navier stokes description of the transport for which all the porosity is fully resolved and pure continuum scale modeling based on darcy s law besides this hybrid scale approach porousmedia4foam also includes a standard darcy solver for continuum scale simulations therefore the same simulator can be used to simulate flow transport and geochemical reactions in an reservoir and in 3d micro tomography images the coupled hydro geochemical simulator was verified by running cases for which reference solutions exist these solutions are well established and used in the reactive transport community to benchmark state of the art codes available both at the continuum scale xie et al 2015 and at the pore scale molins et al 2020 finally we demonstrated the ability of our advanced modelling framework to simulate dissolution and precipitation processes in fractured porous media at the pore scale using the hybrid scale approach here the reactive medium consisted of celestite grains that reacted with a barium chloride solution injected into the system leading to the dissolution of celestite and the growth of barite we observed differences in mineral precipitation dissolution patterns by varying the injection rates because porousmedia4foam has already capabilities for modelling two phase flow in porous media both at the pore and darcy s scales using two phase micro continuum technique soulaine et al 2019 soulaine et al 2018 carrillo et al 2020 true multiscale and multiphase rtm is envisioned to be implemented in porousmedia4foam framework software and data availability porousmedia4foam the software introduced in this paper is built using open source libraries including openfoam and phreeqc the source code and the cases presented in the paper are available on github https github com csoulain porousmedia4foam contributions of the authors cs is the porousmedia4foam architect cs and sp implemented new models in porousmedia4foam and corrected bugs sp and ct designed and setup the benchmark problems sp run the simulations cs sp ct and fc discussed interpreted the results and wrote the paper cs ct and fc applied for funding declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the research leading to these results has received funding from the french agency for research agence nationale de la recherche anr through the labex voltaire anr 10 labx 100 01 the grant catch anr 18 ce05 0035 and through the framati project under contract anr 19 ce05 0002 it has also received financial support from the cnrs through the miti interdisciplinary programs this project has received funding from the european union s horizon 2020 research and innovation programme under grant agreements no 847593 wp donut and no 850626 reflect project sp postdoctoral fellowship was funded by brgm through the pore reactif project from the alliance nationale de coordination de la recherche pour l energie ancre the authors benefitted from the use of the cluster at the centre de calcul scientifique en région centre val de loire 
25732,this paper compares the well known genetic algorithm ga and pattern search ps optimization methods for forecasting optimal flow releases in a multi storage system for flood control the simulation models used by the optimization models include a a batch of scripts for data acquisition of forecasted precipitation and their automated post processing b a hydrological model for rainfall runoff conversion and c a hydraulic model for simulating river inundation this paper focuses on 1 demonstrating the application of the framework by applying it to the operation of a hypothetical eight wetland system in the cypress creek watershed in houston texas and 2 comparing and discussing the performance of the two optimization methods under consideration the results show that the ga and ps optimal solutions are very similar however the computational time required by ps is significantly shorter than that required by ga the results also show that optimal dynamic water management can significantly mitigate flooding compared to the case without management keywords flood control flood management forecast optimization real time 1 introduction inland flooding produces more damage annually than any other weather event in the united states noaa 2016 it is expected that global warming along increasing trends in urban development will make the problem worse nasa 2017 multiple strategies to mitigate floods have been developed in the last few decades in particular flood mitigation at the watershed scale is receiving increasing attention kusler 2004 flotemersch et al 2016 within this context flood control can be improved by operating detention ponds reservoirs and other storage systems in an integrated and coordinated manner according to precipitation forecasts leon et al 2020 for instance flood control can be improved by partially emptying wetlands ahead of e g a few hours or a couple of days before a heavy rainfall that would produce flooding in this case the storage made available by the early release would provide extra water storage during the heavy rainfall thus mitigating floods even though a few numerical frameworks were proposed for near real time flood control e g wei and hsu 2008 vermuyten et al 2020 tang et al 2020a there are very few papers comparing the numerical performance of optimization algorithms the present work compares the performance of the well known genetic algorithm ga and pattern search ps for forecasting optimal flow releases in a multi storage system for flood control this paper is organized as follows 1 the simulation and optimization models are briefly described 2 the objective function and constraints are presented 3 the case study is presented and discussed finally the key results are summarized in the conclusion 2 model description a numerical framework for forecasting hourly flow releases in a multi storage system for flood control needs to include an array of models intended for data acquisition of forecasted precipitation landscape rainfall runoff conversion level pool routing in storage systems river inundation modeling and optimization for each of these components there are an array of options available in the literature below it is briefly described the models used in the paper and the justification for their use 2 1 acquisition of precipitation forecast and conversion to dss format the acquisition of precipitation forecasts is obtained using our scripts that are provided in github see appendix a the scripts also include code to convert the data to dss format which is the file format used by hec hms and hec ras for storing time series data such as precipitation and discharge over time and other types of data such as unit hydrographs elevation area curves and elevation discharge curves as an illustration fig 1 presents the precipitation forecast for the south east area of the united states this figure depicts the precipitation forecast for april 13 2020 and was generated using the code on april 08 2020 5 days lead time 2 2 hydrological and hydraulic routing as discussed in leon et al 2020 the u s army corps of engineers hydrologic modeling system hec hms hydrologic engineering center 2017 is a good alternative for the hydrologic modeling and the u s army corps of engineers hydrologic engineering center s river analysis system hec ras hydrologic engineering center 2016a hydrologic engineering center 2016b is a good option for inundation modeling the version of the models used herein are hec hms 4 3 and hec ras 5 0 7 2 3 optimal schedules of flow releases in a multi storage system for forecasting optimal schedules of flow releases for flood control an optimization solver is needed the number of decision variables in the optimization is directly proportional to the number of storage systems and the number of time intervals e g hourly releases used in the optimization for instance if the number of storage systems is 20 and optimal schedules of flow releases are needed for a period of 5 days at hourly time intervals the number of decision variables would be 2400 5x24x20 thus a near real time flood control framework requires an optimization solver suitable for large scale problems herein the performance of two state of the art optimization solvers are compared and discussed within the context of flood control due to the availability of these solvers within the matlab optimization toolbox chipperfield and fleming 1995 this toolbox was used herein the version of the matlab model used herein is matlab r2021a the two used solvers are briefly described next 2 3 1 genetic algorithm ga the genetic algorithm ga solves constrained and unconstrained optimization problems based on a natural selection process that mimics biological evolution chipperfield and fleming 1995 the ga repeatedly changes a population of individual solutions at each generation the ga randomly selects individuals from the current population and uses them as parents to produce children for the next generation after several generations the population is expected to evolve toward an optimal solution the ga is recommended to solve problems that are not well suited for standard optimization algorithms including problems in which the objective function is discontinuous nondifferentiable stochastic or highly non linear chipperfield and fleming 1995 for more details about the genetic algorithm and its application to water resources the reader is referred to wardlaw and sharif 1999 leon and kanashiro 2010 leon et al 2014 lerma et al 2015 yang et al 2015 and chen et al 2016 2 3 2 pattern search ps optimization the pattern search method is an efficient algorithm for solving smooth and nonsmooth optimization problems mathworks 2020 at each iteration the pattern search method searches a set of points called a mesh around the current point looking for one where the value of the objective function is lower than the value at the current point the pattern search method forms the mesh by mathworks 2020 1 generating a set of vectors by multiplying each pattern vector by the mesh size and 2 adding the set of vectors to the current point which is the point with the best objective function value found at the previous step the set of pattern vectors is defined by the number of decision variables in the objective function e g n and the positive basis set two commonly used positive basis sets in pattern search algorithms are the maximal basis with 2n vectors and the minimal basis with n 1 vectors for example if there are two independent variables in the optimization problem the default for a 2n positive basis consists of the following pattern vectors v 1 0 1 v 2 1 0 v 3 0 1 and v 4 1 0 the reader is referred to kolda et al 2006 for a description of the way in which the pattern search method forms a pattern with linear constraints for more details about the pattern search algorithm the reader is referred to lewis et al 2007 and abramson et al 2009 3 objective function and constraints 3 1 objective function a typical watershed may experience flooding only a few times per year during flooding conditions the water level at control cross sections of the rivers and creeks should be maintained below the respective pre specified maximum water level a control cross section can be specified for instance at densely populated areas the maximum water level specified at a control cross section corresponds to a level where inundation is imminent the objective function f for flooding conditions can be written as follows 1 f i 1 cs w i j 1 p e i j e m a x i 2 where the summation in eq 1 is included for all e i j e m a x i and 0 otherwise in eq 1 cs and p are the number of control river cross sections at which the water level constraint is checked and the number of time intervals e g hourly flow releases for each managed wetland respectively also e i j is the water level at control river cross section i and at time interval j e g hour j and e m a x i is the specified maximum water level constraint at control river cross section i also in eq 1 w i is the weight of the importance of maintaining the water level in control river cross section i if the weights are equally important all w i can be set equal to 1 3 2 constraints the optimization may be subject to linear equality a eq x b eq and inequality constraints a ineq x b ineq the equality constraint needs to be specified when for instance a certain water level needs to be maintained in the wetlands at a given time for brevity let s consider only two wetlands and three time intervals e g three decision variables for each wetland for this case the vector of decisions variables x would consist of 6 variables if a certain water storage s end needs to be maintaned at the end of the optimization the matrix a eq and vector b eq would be defined as a e q 1 1 1 0 0 0 0 0 0 1 1 1 b e q s o s e n d 1 δ t σ i 1 s o s e n d 2 δ t σ i 2 where s o i is the initial storage at wetland i and σi i is the sum of inflows that enters wetland i the optimization would also be subject to several linear inequality constraints for instance from the operational point of view it may be desirable that the change of two consecutive flow releases are within a certain value mathematically this means that the absolute value of the difference of two consecutive flow releases are within a certain value e g c note that the absolute value is equivalent to two linear inequality constraints x k x k 1 c and x k x k 1 c another inequality constraint can be defined to maintain the water storage in each wetland above a minimum wetland storage which may be required for ecological purposes s ecol another inequality constraint can be defined to keep the water storage in each wetland below its maximum storage capacity s max as an illustration for the two wetlands and the three time intervals mentioned above the matrix a ineq and the vector b ineq for the three aforementioned inequality constraints in the presented order can be written as a i n e q 1 1 0 0 0 0 01 1000 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 b i n e q c c c c c c c c a i n e q 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 b i n e q 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 i 1 1 i 1 2 i 1 3 i 2 1 i 2 2 i 2 3 s e c o l s o 1 δ t s e c o l s o 1 δ t s e c o l s o 1 δ t s e c o l s o 2 δ t s e c o l s o 2 δ t s e c o l s o 2 δ t a i n e q 100000 1 1 1 10000 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 b i n e q 100000 1 1 1 10000 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 i 1 1 i 1 2 i 1 3 i 2 1 i 2 2 i 2 3 s m a x s o 1 δ t s m a x s o 1 δ t s m a x s o 1 δ t s m a x s o 2 δ t s m a x s o 2 δ t s m a x s o 2 δ t where i i k indicates the inflow that enters wetland i at time interval k 4 case study a hypothetical eight wetland system in the cypress creek watershed houston tx the coupled optimization simulation model is applied to the operation of a hypothetical eight wetland system in the cypress creek watershed which is located in houston texas see fig 2 the characteristics of this watershed are described in tang et al 2020b the cypress creek watershed which has a total area of 8 33 108 m2 experienced devastating floods during hurricane harvey in august 2017 the upper half of the cypress creek watershed was historically covered by wetlands and rice farms and as such there are a multitude of existing levees that can be easily repaired to restore the function of wetlands tang et al 2020b to help in flood mitigation tang et al 2020b considered eight hypothetical wetlands wl 300 wl 310 wl 330 wl 380 wl 390 wl 400 wl 410 wl 420 that are placed in the midstream portion of the watershed these eight wetlands are depicted as yellow clouds in fig 3 this case study considers a single control cross section station 42006 23 in the lower reach of the cypress creek river to track the water level the maximum desired water elevation at this river station was set to 37 and 37 5 m it is noted that according to eq 1 the objective function is the sum of the square of the difference between the water level in the control cross section and the maximum desired water elevation thus specifying a higher inundation level will result in less flooding and in more operation flexibility before and during the flood the flow chart of the fully coupled optimization simulation model for forecasting optimal flow releases in a multi storage system for flood control is presented in fig 4 as shown in this figure the hec hms and hec ras models are prepared validated and linked offline the linking consists on using the outflows of hec hms managed wetlands and unmanaged basins as inflows for the hec ras model via dss filepaths after the hec hms and hec ras models are specified the user needs to specify the optimization parameters the initial water levels in the managed wetlands and the initial flow conditions in the river then for a given precipitation historical or forecasted the optimization model generates an schedule of outflows for each wetland at each generation in ga or at each iteration in ps the schedule of wetland outflows is then used by hec hms to update the water levels in the wetlands then the outflows from hec hms which could be unmanaged flows sub basins without managed storage or managed flows sub basins with managed storage enter the streams in hec ras the water levels in the control cross sections in hec ras are used to evaluate the objective function given in eq 1 the linear inequality and equality constraints are satisfied at each generation or iteration in both ga and ps the process is repeated until the optimization stop criteria is satisfied once the optimization is completed the process can be repeated for another precipitation the matlab and python scripts for the coupled optimization simulation are provided in github see appendix b the hydrologic model of the cypress creek watershed was created in hec hms the details of the hec hms model construction calibration and validation are discussed in tang et al 2020b it is noted that the present paper used gridded precipitation instead of time series precipitation used in tang et al 2020b for details of the gridded precipitation the reader is referred to bian et al 2021 our python and matlab scripts for obtaining gridded precipitation are provided in github see appendix a for the present demonstration the eight hypothetical wetlands wl 300 wl 310 wl 330 wl 380 wl 390 wl 400 wl 410 wl 420 have a total combined area of about 3 5 of the whole watershed area and each wetland has a maximum depth of 1 m the hydraulic model of the major streams of the cypress creek watershed was created in hec ras using the hec georas tool within arcgis the details of the hec ras model construction calibration and validation are discussed in tang et al 2020b the optimization period considered in this case study is 14 days 336 h resulting in a total of 2688 optimal hourly flows for the eight wetlands the optimization parameters specified for the ga are as follows population 128 function tolerance 1e 4 the optimization parameters specified for the ps are as follows initial mesh size 0 5 m3 s maximum number of iterations 1000 mesh tolerance 1e 4 function tolerance 1e 4 the lower limit for the flow releases at all eight managed wetlands was set to 0 m3 s the upper limit for the flow releases was set to 25 12 15 15 25 10 10 and 10 m3 s for wetlands wl 300 wl 310 wl 330 wl 380 wl 390 wl 400 wl 410 wl 420 respectively to speed up the computations all hec ras simulations are performed in a vectorized manner e g hec ras simulations are computed in parallel herein we have used 18 available processors in the 8th generation intel core i7 8700 18 parallel computations equality and inequality constraints were specified for all eight wetlands two constraint scenarios were specified in the optimization the first constraint scenario considered one equality constraint and two inequality constraints the equality constraint specified that 72 h 3 days after the beginning of the optimization which was also before the beginning of the rainfall event the water level in all wetlands be at its ecological depth assumed to be 0 3 m for all wetlands the first inequality constraint is that the maximum change between two consecutive hourly flow releases is 5 m3 s the second inequality constraint specified that the water depth in each wetland needs to be maintained above the minimum ecological depth at all times the second constraint scenario includes all constraints of the first constraint scenario plus a no overflow constraint the no overflow constraint specified that the water depth in all wetlands at all times need to be maintained below the respective maximum wetland depth 1 m this inequality constraint was set to avoid overflows at the wetlands the typical convergence process for the ga and ps are shown in figs 5 and 6 respectively after the stopping criteria of the ga and ps is satisfied our framework automatically generates a plot for the best optimal solution for each managed wetland each plot includes the optimal trace of outflows the corresponding time trace of the water surface elevation and storage in the wetland and the time trace of total inflow wetland spill flow and total outflow spill flow managed outflow a plot produced for wetland wl 390 with ga and ps for inundation elevation of 37 5 m and for the first constraint scenario is shown in figs 7 and 8 respectively as shown in figs 7 and 8 the pattern of outflows produced with both algorithms are very similar as also shown in these figures the optimization releases water from the wetlands before the rainfall and during the initial rainfall period this initial rainfall period corresponds to the period before the control cross section is about to be inundated four optimization conditions were simulated the conditions were obtained by utilizing two inundation elevations at control cross section 42006 23 37 and 37 5 m and the two aforementioned constraint scenarios figs 9 12 show the time traces of the water elevation and discharge at the control cross section for the above mentioned optimization conditions for the best solutions obtained with the ga and ps methods and those without any water management overall the optimization aims to release water from the wetlands before the rainfall and during the initial rainfall period this initial rainfall period corresponds to the period before the control cross section is about to exceed the pre specified level of inundation during the later rainfall period there is no significant change in the objective function and as such no significant flood mitigation this is because during most of this period the wetlands are full and the river is flowing near maximum capacity the results in figs 9 12 indicate that the results produced by the ga and ps are very similar however the computational time required by ps is significantly smaller than that required by ga for instance the results in fig 9 required a runtime of 16 h for the ps and about 5 days for the ga the results also show that the simulation without water management exceed more significantly the specified inundation level 37 or 37 5 m and for longer periods of time for the same inundation level 37 or 37 5 m the results for the two aforementioned constraint scenarios are also very similar it is clear that in the second constraint scenario the flow releases at the managed wetlands will be continuous even during the entire rainfall period however the total outflow flow release spill flow for both constraint scenarios are essentially the same thus the results are very similar 5 conclusions this paper compares the performance of the well known genetic algorithm and pattern search methods for forecasting optimal flow releases in a multi storage system for flood control this framework combines hec hms hec ras the matlab optimization toolbox and a batch of scripts to integrate these models all scripts used are made available in github see appendices a and b the case study is illustrated using the operation of a hypothetical eight wetland system in the cypress creek in houston texas the key results are as follows 1 the results produced by the genetic algorithm ga and pattern search ps methods are very similar however the computational time required by ps is significantly smaller than that required by ga 2 in general the results show that the dynamic water management according to the optimization results can help to significantly mitigate flooding compared to the case without management e g uncontrolled water release of wetlands 3 the results without any water management exceed more significantly the maximum water level at the control cross section and for longer periods of time 4 a key factor for flood control is to partially empty the storage systems before the rainfall event and during the initial rainfall period this initial rainfall period corresponds to the period before the pre specified inundation level at the control cross section is exceeded during the later rainfall period the optimization doesn t play a significant role because the wetlands are full and the river is flowing near maximum capacity software and data availability all scripts used in this paper are made available in github see appendices a and b declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first and second author was partially supported by the national science foundation through nsf eng cbet under award no 1805417 and through nsf dbi bio under award no 1820778 appendix a automated acquisition of precipitation forecast and conversion to dss for its use in hec hms the acquisition of precipitation forecast and the automated conversion of the acquired data to dss format is performed using a batch of scripts available at https web eng fiu edu arleon code precip forecast dss html and the github repository https github com artuleon automate precipitation forecast git the acquired precipitation is the bias corrected global forecast system gfs for a lead time of 5 days today s time is april 04 of 2021 and a time interval of 6 h the acquired data is automatically projected to the cypress creek watershed the precipitation map for a lead time of 5 days is shown in fig 13 this file is automatically generated in the folder forecast gfs with the name precip plot pdf the dss file is automatically generated in the folder forecast gfs with the name gfs dss the script forecast gfs py re samples the precipitation to a 1000 m 1000 m grid cell and 1 h time interval an example of gridded precipitation converted to dss for its use in hec hms is shown in fig 14 fig 13 snapshot of bias corrected global forecast system gfs acquired by python and projected to the cypress creek watershed fig 13 fig 14 snapshot of gridded precipitation converted to dss for its use in hec hms fig 14 appendix b coupled simulation optimization model for forecasting optimal flow releases in a multi storage system for flood control our scripts for the coupled optimization simulation used in this paper can be found at https web eng fiu edu arleon code flood control dss html and the github repository https github com artuleon flood control dss 
25732,this paper compares the well known genetic algorithm ga and pattern search ps optimization methods for forecasting optimal flow releases in a multi storage system for flood control the simulation models used by the optimization models include a a batch of scripts for data acquisition of forecasted precipitation and their automated post processing b a hydrological model for rainfall runoff conversion and c a hydraulic model for simulating river inundation this paper focuses on 1 demonstrating the application of the framework by applying it to the operation of a hypothetical eight wetland system in the cypress creek watershed in houston texas and 2 comparing and discussing the performance of the two optimization methods under consideration the results show that the ga and ps optimal solutions are very similar however the computational time required by ps is significantly shorter than that required by ga the results also show that optimal dynamic water management can significantly mitigate flooding compared to the case without management keywords flood control flood management forecast optimization real time 1 introduction inland flooding produces more damage annually than any other weather event in the united states noaa 2016 it is expected that global warming along increasing trends in urban development will make the problem worse nasa 2017 multiple strategies to mitigate floods have been developed in the last few decades in particular flood mitigation at the watershed scale is receiving increasing attention kusler 2004 flotemersch et al 2016 within this context flood control can be improved by operating detention ponds reservoirs and other storage systems in an integrated and coordinated manner according to precipitation forecasts leon et al 2020 for instance flood control can be improved by partially emptying wetlands ahead of e g a few hours or a couple of days before a heavy rainfall that would produce flooding in this case the storage made available by the early release would provide extra water storage during the heavy rainfall thus mitigating floods even though a few numerical frameworks were proposed for near real time flood control e g wei and hsu 2008 vermuyten et al 2020 tang et al 2020a there are very few papers comparing the numerical performance of optimization algorithms the present work compares the performance of the well known genetic algorithm ga and pattern search ps for forecasting optimal flow releases in a multi storage system for flood control this paper is organized as follows 1 the simulation and optimization models are briefly described 2 the objective function and constraints are presented 3 the case study is presented and discussed finally the key results are summarized in the conclusion 2 model description a numerical framework for forecasting hourly flow releases in a multi storage system for flood control needs to include an array of models intended for data acquisition of forecasted precipitation landscape rainfall runoff conversion level pool routing in storage systems river inundation modeling and optimization for each of these components there are an array of options available in the literature below it is briefly described the models used in the paper and the justification for their use 2 1 acquisition of precipitation forecast and conversion to dss format the acquisition of precipitation forecasts is obtained using our scripts that are provided in github see appendix a the scripts also include code to convert the data to dss format which is the file format used by hec hms and hec ras for storing time series data such as precipitation and discharge over time and other types of data such as unit hydrographs elevation area curves and elevation discharge curves as an illustration fig 1 presents the precipitation forecast for the south east area of the united states this figure depicts the precipitation forecast for april 13 2020 and was generated using the code on april 08 2020 5 days lead time 2 2 hydrological and hydraulic routing as discussed in leon et al 2020 the u s army corps of engineers hydrologic modeling system hec hms hydrologic engineering center 2017 is a good alternative for the hydrologic modeling and the u s army corps of engineers hydrologic engineering center s river analysis system hec ras hydrologic engineering center 2016a hydrologic engineering center 2016b is a good option for inundation modeling the version of the models used herein are hec hms 4 3 and hec ras 5 0 7 2 3 optimal schedules of flow releases in a multi storage system for forecasting optimal schedules of flow releases for flood control an optimization solver is needed the number of decision variables in the optimization is directly proportional to the number of storage systems and the number of time intervals e g hourly releases used in the optimization for instance if the number of storage systems is 20 and optimal schedules of flow releases are needed for a period of 5 days at hourly time intervals the number of decision variables would be 2400 5x24x20 thus a near real time flood control framework requires an optimization solver suitable for large scale problems herein the performance of two state of the art optimization solvers are compared and discussed within the context of flood control due to the availability of these solvers within the matlab optimization toolbox chipperfield and fleming 1995 this toolbox was used herein the version of the matlab model used herein is matlab r2021a the two used solvers are briefly described next 2 3 1 genetic algorithm ga the genetic algorithm ga solves constrained and unconstrained optimization problems based on a natural selection process that mimics biological evolution chipperfield and fleming 1995 the ga repeatedly changes a population of individual solutions at each generation the ga randomly selects individuals from the current population and uses them as parents to produce children for the next generation after several generations the population is expected to evolve toward an optimal solution the ga is recommended to solve problems that are not well suited for standard optimization algorithms including problems in which the objective function is discontinuous nondifferentiable stochastic or highly non linear chipperfield and fleming 1995 for more details about the genetic algorithm and its application to water resources the reader is referred to wardlaw and sharif 1999 leon and kanashiro 2010 leon et al 2014 lerma et al 2015 yang et al 2015 and chen et al 2016 2 3 2 pattern search ps optimization the pattern search method is an efficient algorithm for solving smooth and nonsmooth optimization problems mathworks 2020 at each iteration the pattern search method searches a set of points called a mesh around the current point looking for one where the value of the objective function is lower than the value at the current point the pattern search method forms the mesh by mathworks 2020 1 generating a set of vectors by multiplying each pattern vector by the mesh size and 2 adding the set of vectors to the current point which is the point with the best objective function value found at the previous step the set of pattern vectors is defined by the number of decision variables in the objective function e g n and the positive basis set two commonly used positive basis sets in pattern search algorithms are the maximal basis with 2n vectors and the minimal basis with n 1 vectors for example if there are two independent variables in the optimization problem the default for a 2n positive basis consists of the following pattern vectors v 1 0 1 v 2 1 0 v 3 0 1 and v 4 1 0 the reader is referred to kolda et al 2006 for a description of the way in which the pattern search method forms a pattern with linear constraints for more details about the pattern search algorithm the reader is referred to lewis et al 2007 and abramson et al 2009 3 objective function and constraints 3 1 objective function a typical watershed may experience flooding only a few times per year during flooding conditions the water level at control cross sections of the rivers and creeks should be maintained below the respective pre specified maximum water level a control cross section can be specified for instance at densely populated areas the maximum water level specified at a control cross section corresponds to a level where inundation is imminent the objective function f for flooding conditions can be written as follows 1 f i 1 cs w i j 1 p e i j e m a x i 2 where the summation in eq 1 is included for all e i j e m a x i and 0 otherwise in eq 1 cs and p are the number of control river cross sections at which the water level constraint is checked and the number of time intervals e g hourly flow releases for each managed wetland respectively also e i j is the water level at control river cross section i and at time interval j e g hour j and e m a x i is the specified maximum water level constraint at control river cross section i also in eq 1 w i is the weight of the importance of maintaining the water level in control river cross section i if the weights are equally important all w i can be set equal to 1 3 2 constraints the optimization may be subject to linear equality a eq x b eq and inequality constraints a ineq x b ineq the equality constraint needs to be specified when for instance a certain water level needs to be maintained in the wetlands at a given time for brevity let s consider only two wetlands and three time intervals e g three decision variables for each wetland for this case the vector of decisions variables x would consist of 6 variables if a certain water storage s end needs to be maintaned at the end of the optimization the matrix a eq and vector b eq would be defined as a e q 1 1 1 0 0 0 0 0 0 1 1 1 b e q s o s e n d 1 δ t σ i 1 s o s e n d 2 δ t σ i 2 where s o i is the initial storage at wetland i and σi i is the sum of inflows that enters wetland i the optimization would also be subject to several linear inequality constraints for instance from the operational point of view it may be desirable that the change of two consecutive flow releases are within a certain value mathematically this means that the absolute value of the difference of two consecutive flow releases are within a certain value e g c note that the absolute value is equivalent to two linear inequality constraints x k x k 1 c and x k x k 1 c another inequality constraint can be defined to maintain the water storage in each wetland above a minimum wetland storage which may be required for ecological purposes s ecol another inequality constraint can be defined to keep the water storage in each wetland below its maximum storage capacity s max as an illustration for the two wetlands and the three time intervals mentioned above the matrix a ineq and the vector b ineq for the three aforementioned inequality constraints in the presented order can be written as a i n e q 1 1 0 0 0 0 01 1000 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 b i n e q c c c c c c c c a i n e q 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 b i n e q 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 i 1 1 i 1 2 i 1 3 i 2 1 i 2 2 i 2 3 s e c o l s o 1 δ t s e c o l s o 1 δ t s e c o l s o 1 δ t s e c o l s o 2 δ t s e c o l s o 2 δ t s e c o l s o 2 δ t a i n e q 100000 1 1 1 10000 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 b i n e q 100000 1 1 1 10000 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 i 1 1 i 1 2 i 1 3 i 2 1 i 2 2 i 2 3 s m a x s o 1 δ t s m a x s o 1 δ t s m a x s o 1 δ t s m a x s o 2 δ t s m a x s o 2 δ t s m a x s o 2 δ t where i i k indicates the inflow that enters wetland i at time interval k 4 case study a hypothetical eight wetland system in the cypress creek watershed houston tx the coupled optimization simulation model is applied to the operation of a hypothetical eight wetland system in the cypress creek watershed which is located in houston texas see fig 2 the characteristics of this watershed are described in tang et al 2020b the cypress creek watershed which has a total area of 8 33 108 m2 experienced devastating floods during hurricane harvey in august 2017 the upper half of the cypress creek watershed was historically covered by wetlands and rice farms and as such there are a multitude of existing levees that can be easily repaired to restore the function of wetlands tang et al 2020b to help in flood mitigation tang et al 2020b considered eight hypothetical wetlands wl 300 wl 310 wl 330 wl 380 wl 390 wl 400 wl 410 wl 420 that are placed in the midstream portion of the watershed these eight wetlands are depicted as yellow clouds in fig 3 this case study considers a single control cross section station 42006 23 in the lower reach of the cypress creek river to track the water level the maximum desired water elevation at this river station was set to 37 and 37 5 m it is noted that according to eq 1 the objective function is the sum of the square of the difference between the water level in the control cross section and the maximum desired water elevation thus specifying a higher inundation level will result in less flooding and in more operation flexibility before and during the flood the flow chart of the fully coupled optimization simulation model for forecasting optimal flow releases in a multi storage system for flood control is presented in fig 4 as shown in this figure the hec hms and hec ras models are prepared validated and linked offline the linking consists on using the outflows of hec hms managed wetlands and unmanaged basins as inflows for the hec ras model via dss filepaths after the hec hms and hec ras models are specified the user needs to specify the optimization parameters the initial water levels in the managed wetlands and the initial flow conditions in the river then for a given precipitation historical or forecasted the optimization model generates an schedule of outflows for each wetland at each generation in ga or at each iteration in ps the schedule of wetland outflows is then used by hec hms to update the water levels in the wetlands then the outflows from hec hms which could be unmanaged flows sub basins without managed storage or managed flows sub basins with managed storage enter the streams in hec ras the water levels in the control cross sections in hec ras are used to evaluate the objective function given in eq 1 the linear inequality and equality constraints are satisfied at each generation or iteration in both ga and ps the process is repeated until the optimization stop criteria is satisfied once the optimization is completed the process can be repeated for another precipitation the matlab and python scripts for the coupled optimization simulation are provided in github see appendix b the hydrologic model of the cypress creek watershed was created in hec hms the details of the hec hms model construction calibration and validation are discussed in tang et al 2020b it is noted that the present paper used gridded precipitation instead of time series precipitation used in tang et al 2020b for details of the gridded precipitation the reader is referred to bian et al 2021 our python and matlab scripts for obtaining gridded precipitation are provided in github see appendix a for the present demonstration the eight hypothetical wetlands wl 300 wl 310 wl 330 wl 380 wl 390 wl 400 wl 410 wl 420 have a total combined area of about 3 5 of the whole watershed area and each wetland has a maximum depth of 1 m the hydraulic model of the major streams of the cypress creek watershed was created in hec ras using the hec georas tool within arcgis the details of the hec ras model construction calibration and validation are discussed in tang et al 2020b the optimization period considered in this case study is 14 days 336 h resulting in a total of 2688 optimal hourly flows for the eight wetlands the optimization parameters specified for the ga are as follows population 128 function tolerance 1e 4 the optimization parameters specified for the ps are as follows initial mesh size 0 5 m3 s maximum number of iterations 1000 mesh tolerance 1e 4 function tolerance 1e 4 the lower limit for the flow releases at all eight managed wetlands was set to 0 m3 s the upper limit for the flow releases was set to 25 12 15 15 25 10 10 and 10 m3 s for wetlands wl 300 wl 310 wl 330 wl 380 wl 390 wl 400 wl 410 wl 420 respectively to speed up the computations all hec ras simulations are performed in a vectorized manner e g hec ras simulations are computed in parallel herein we have used 18 available processors in the 8th generation intel core i7 8700 18 parallel computations equality and inequality constraints were specified for all eight wetlands two constraint scenarios were specified in the optimization the first constraint scenario considered one equality constraint and two inequality constraints the equality constraint specified that 72 h 3 days after the beginning of the optimization which was also before the beginning of the rainfall event the water level in all wetlands be at its ecological depth assumed to be 0 3 m for all wetlands the first inequality constraint is that the maximum change between two consecutive hourly flow releases is 5 m3 s the second inequality constraint specified that the water depth in each wetland needs to be maintained above the minimum ecological depth at all times the second constraint scenario includes all constraints of the first constraint scenario plus a no overflow constraint the no overflow constraint specified that the water depth in all wetlands at all times need to be maintained below the respective maximum wetland depth 1 m this inequality constraint was set to avoid overflows at the wetlands the typical convergence process for the ga and ps are shown in figs 5 and 6 respectively after the stopping criteria of the ga and ps is satisfied our framework automatically generates a plot for the best optimal solution for each managed wetland each plot includes the optimal trace of outflows the corresponding time trace of the water surface elevation and storage in the wetland and the time trace of total inflow wetland spill flow and total outflow spill flow managed outflow a plot produced for wetland wl 390 with ga and ps for inundation elevation of 37 5 m and for the first constraint scenario is shown in figs 7 and 8 respectively as shown in figs 7 and 8 the pattern of outflows produced with both algorithms are very similar as also shown in these figures the optimization releases water from the wetlands before the rainfall and during the initial rainfall period this initial rainfall period corresponds to the period before the control cross section is about to be inundated four optimization conditions were simulated the conditions were obtained by utilizing two inundation elevations at control cross section 42006 23 37 and 37 5 m and the two aforementioned constraint scenarios figs 9 12 show the time traces of the water elevation and discharge at the control cross section for the above mentioned optimization conditions for the best solutions obtained with the ga and ps methods and those without any water management overall the optimization aims to release water from the wetlands before the rainfall and during the initial rainfall period this initial rainfall period corresponds to the period before the control cross section is about to exceed the pre specified level of inundation during the later rainfall period there is no significant change in the objective function and as such no significant flood mitigation this is because during most of this period the wetlands are full and the river is flowing near maximum capacity the results in figs 9 12 indicate that the results produced by the ga and ps are very similar however the computational time required by ps is significantly smaller than that required by ga for instance the results in fig 9 required a runtime of 16 h for the ps and about 5 days for the ga the results also show that the simulation without water management exceed more significantly the specified inundation level 37 or 37 5 m and for longer periods of time for the same inundation level 37 or 37 5 m the results for the two aforementioned constraint scenarios are also very similar it is clear that in the second constraint scenario the flow releases at the managed wetlands will be continuous even during the entire rainfall period however the total outflow flow release spill flow for both constraint scenarios are essentially the same thus the results are very similar 5 conclusions this paper compares the performance of the well known genetic algorithm and pattern search methods for forecasting optimal flow releases in a multi storage system for flood control this framework combines hec hms hec ras the matlab optimization toolbox and a batch of scripts to integrate these models all scripts used are made available in github see appendices a and b the case study is illustrated using the operation of a hypothetical eight wetland system in the cypress creek in houston texas the key results are as follows 1 the results produced by the genetic algorithm ga and pattern search ps methods are very similar however the computational time required by ps is significantly smaller than that required by ga 2 in general the results show that the dynamic water management according to the optimization results can help to significantly mitigate flooding compared to the case without management e g uncontrolled water release of wetlands 3 the results without any water management exceed more significantly the maximum water level at the control cross section and for longer periods of time 4 a key factor for flood control is to partially empty the storage systems before the rainfall event and during the initial rainfall period this initial rainfall period corresponds to the period before the pre specified inundation level at the control cross section is exceeded during the later rainfall period the optimization doesn t play a significant role because the wetlands are full and the river is flowing near maximum capacity software and data availability all scripts used in this paper are made available in github see appendices a and b declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first and second author was partially supported by the national science foundation through nsf eng cbet under award no 1805417 and through nsf dbi bio under award no 1820778 appendix a automated acquisition of precipitation forecast and conversion to dss for its use in hec hms the acquisition of precipitation forecast and the automated conversion of the acquired data to dss format is performed using a batch of scripts available at https web eng fiu edu arleon code precip forecast dss html and the github repository https github com artuleon automate precipitation forecast git the acquired precipitation is the bias corrected global forecast system gfs for a lead time of 5 days today s time is april 04 of 2021 and a time interval of 6 h the acquired data is automatically projected to the cypress creek watershed the precipitation map for a lead time of 5 days is shown in fig 13 this file is automatically generated in the folder forecast gfs with the name precip plot pdf the dss file is automatically generated in the folder forecast gfs with the name gfs dss the script forecast gfs py re samples the precipitation to a 1000 m 1000 m grid cell and 1 h time interval an example of gridded precipitation converted to dss for its use in hec hms is shown in fig 14 fig 13 snapshot of bias corrected global forecast system gfs acquired by python and projected to the cypress creek watershed fig 13 fig 14 snapshot of gridded precipitation converted to dss for its use in hec hms fig 14 appendix b coupled simulation optimization model for forecasting optimal flow releases in a multi storage system for flood control our scripts for the coupled optimization simulation used in this paper can be found at https web eng fiu edu arleon code flood control dss html and the github repository https github com artuleon flood control dss 
25733,shallow water models are used for simulating flood and lake hydrodynamics however the computational cost of those models is often high and require high performance computing we present the sw2d gpu a two dimensional shallow water model accelerated by general purpose graphics processing unit the model is implemented in parallel using cuda c c we exemplify the use of the model with two case studies i flood simulation in an urban area and ii water level simulation in a lake catchment we have included potential evaporation in the formulation which expands its application to water level simulations in lakes and reservoirs the sw2d gpu model is approximately 34 times faster than its equivalent sequential version the model can be run in any computer equipped with a nvidia gpu integrated simulations of surface waters in lake watershed and simulations of floods caused by dam break are some of the potential applications keywords shallow water equation high performance computing lake ecosystem parallel computing flood software availability name of code sw2d gpu developer tomas carlotto contact address tomas carlotto posgrad ufsc br t carlotto and pedro chaffe ufsc br p l b chaffe year first available 2021 hardware required graphics processing unit gpu with cuda technology software required cuda toolkit 8 0 or later in windows operating system visual studio community 2013 or later is required program language cuda c c program size 23 7 mb visual studio version 13 2 mb linux version availability https github com labhidro sw2d gpu cost free of charge 1 introduction the shallow water equations are the mathematical basis for many hydrodynamic models their applications ranging from the estimation of water level in rivers and lakes yu et al 2019 ferrarin et al 2020 to the spatiotemporal evolution of floods liu et al 2019 however model instability in complex topographies and long simulation times hinder their application in real cases studies horváth et al 2014 liu et al 2018 a common way to reduce simulation time is to use the two dimensional 2d formulation of the shallow water equations thang et al 2004 lee et al 2014 2016 yet even 2d formulations might be prohibitive in computational domains typical of hydrological applications liu et al 2019 echeverribar et al 2019 high performance computing hpc and codes suitable for parallel processing are the best alternative for accelerating numerical solutions smari et al 2016 most codes that approximate the solution to the shallow water equations are developed using sequential processing or parallelization schemes based on message passing interface mpi or open multi processing openmp compatible with clusters composed of several central processing units cpu o donncha et al 2014 2019 anguita et al 2015 noh et al 2018 2019 recently massively parallel devices such as the general purpose graphics processing unit gpgpu have been shown highly efficient to accelerate the solution of shallow water equations and environmental models with application in real world phenomena brodtkorb et al 2010 ransom and younis 2016 vacondio et al 2017 carlotto et al 2019 dazzi et al 2020 gpus are found on most personal computers and offer great computing power with thousands of processing cores brodtkorb et al 2013 smari et al 2016 consequently systems adapted for the use of languages libraries and tools e g cuda c c cuda fortran for computing in gpus have evolved considerably in the last decade brodtkorb et al 2013 xia et al 2019 cuda technology facilitates code parallelization and use of gpgpu to accelerate numerical solutions equally important are the unified api for hpc code development such as the one presented by o donncha et al 2019 used for parallelization and development of complex code in large computational domains however each parallelization method has positive and negative aspects openmp is simple to implement but has limited scalability due to the size of shared memory mpi is highly scalable for large problems but increased communication between cpus reduces computational performance cuda language for gpu computing is easy to implement and provides great parallel computing power at low cost but processing on gpu is limited by memory constraints noh et al 2018 xia et al 2019 several model implementations harness the computing power of the gpu to approximate the solution of equations that govern the water flow in earth system park et al 2019 used cuda fortran to implement a 2d diffusive wave model resulting in a speed up of approximately 150 times le and kumar 2017 used hybrid cpu gpu parallel computing to develop the dhara model that couples an integrated surface and subsurface flow model numerically solved by alternating direction implicit adi scheme gcs flow le et al 2015 implemented in cuda c c and an ecophysiological multilayer canopy model mlcan drewry et al 2010 implemented in mpi for parallel processing in cpu the dhara model combines the effects of microtopography and climate change on small scale eco hydrological dynamics using the advantages of mpi and gpu parallel computing for high performance in large scale simulations ming et al 2020 developed a real time flood prediction system by coupling a numerical weather prediction model with a hydrodynamic model based on 2d shallow water equations solved using godunov type finite volume scheme with gpu implementation in their approach parallel computing on multiple gpus was used to overcome the physical memory limitations of a single gpu xia et al 2019 in hydrological and hydrodynamic modeling gpgpu accelerated models allow high resolution simulations which can lead to better process representation in the case of lake ecosystems for example rainfall runoff is usually modeled separate from lake hydrodynamics and it is imposed as boundary conditions due to computational constraints acosta et al 2015 janssen et al 2019 in flood inundation forecasting high resolution computational grids are necessary noh et al 2018 2019 however simplifications adopted to reduce the computational cost usually neglect the need for high spatial resolution to simulate flooding in complex terrains kim et al 2012 with the significant gain in processing power offered by gpu such simplifications may no longer be necessary the adaptation of models based on shallow water equations that can account for meteorological variables and the interaction of surface water dynamics between hillslopes streams and lakes are a valuable tool for water resources management especially in watersheds with limited data gu et al 2016 munar et al 2018 jiang et al 2019 moreover these solutions are relevant for problems such as parallelizing and updating models with thousands of lines of code e g weather research and forecasting wrf model and to improve the representation of hydrological process in earth system models schneider et al 2017 despite the advances in computing technologies that provide great parallel processing power at low cost effort is still needed to update and develop hydrological and hydrodynamic model codes compatible with multiple processors and massively parallel devices o donncha et al 2014 le et al 2015 kuffour et al 2020 in order to obtain a high performance 2d shallow water model that simulates hydrological processes in realistic systems using low cost computers we have developed sw2d gpu a two dimensional shallow water model accelerated by gpgpu the accelerated model is based on a sequential version previously implemented in fortran lee 2013 noh et al 2016 the new version is suitable for processing in massively parallel devices with cuda technology we highlight the applicability and performance of the model using two case studies i flood simulation in an urban area and ii simulation of integrated watershed and lake water levels in response to meteorological data and human interferences performance analysis is based on i comparison between the total time that each version of the model parallel and sequential takes to complete the test simulations and ii comparison of the computation time of the cuda kernels processed in the gpu with the equivalent functions of the sequential model executed in the cpu 2 model implementation we base our parallel model on a sequential version of a two dimensional shallow water model previously implemented in fortran lee 2013 noh et al 2016 we have parallelized and completely rewritten the numerical code in the cuda c c programming language the parallel version of the model maintains the same order of tasks as the original sequential implementation to provide a performance comparison between the two versions we propose and implement a formulation to estimate potential evaporation as a function of temperature and solar radiation moreover we include the option of adding an unlimited number of streamflow time series in different locations of the drainage network 2 1 the numerical model the numerical model is based on the 2d shallow water equations lee et al 2014 noh et al 2016 in this formulation the coriolis strength the wind resistance and the viscosity terms are neglected so that the continuity equation of the two dimensional shallow water model are 1 h t m x n y r e q ex e 2 r e r r i n f i n t l w l the momentum equations are 3 m t u m x v m y g h h x f 1 4 n t u n x v n y g h h y f 2 and the friction terms are 5 f 1 g n 2 m u 2 v 2 h 4 3 6 f 2 g n 2 n u 2 v 2 h 4 3 where h is the water depth h h z where z is the topographic elevation u and v are the velocities in the x and y directions respectively m is the flow in the x direction given by uh n is the flow in the y direction given by vh r e is the effective rainfall r is the rainfall inf is the percentage of rainfall infiltration loss int is the percentage of rainfall interception loss lwl is the lake water loses only in the cells in the lake area and water bodies q ex is the term that defines the flow into the drainage network height of water per unit time in a given network cell e is the potential evaporation t is the time f 1 and f 2 are the friction components where g is the acceleration of gravity and n is manning s roughness coefficient the height of water h d that drains in a given section of the channel i e an inlet or outlet of the system is defined by the following equation 7 h d t q d a q ex where q d is accumulated flow at the entrance to the channel and a is the area of the receiving cell potential evaporation is calculated at each time step based on the energy balance method as a function of temperature and solar radiation chow et al 1988 written as follows 8 e r s l v ρ w t k 1000 h h min 0 h h min where r s 1 α r g where r s is net radiation wm 2 r g is global solar radiation wm 2 α is albedo l v is the latent heat of vaporization jkg 1 ρ w is the density of the water kgm 3 t k is the monitoring time interval of the data s and h min is the minimum water depth to activate potential evaporation the constant 1000 is used to convert the results to millimeters and t k adjusts the results to the time interval of the input data to estimate the latent heat of vaporization as a function of temperature we use the equation presented in the work of henderson sellers 1984 written as follows 9 l v 1 91846 1 0 6 t t 273 15 t t 273 15 33 91 2 where t t is the air temperature c observed in time t the density of water is calculated as a function of temperature jones and harris 1992 this way 10 ρ w 999 85308 6 32693 1 0 2 t t 8 523829 1 0 3 t t 2 6 943248 1 0 5 t t 3 3 821216 1 0 7 t t 4 equations 8 10 allow estimating potential evaporation as a function of temperature and solar radiation the formulation of the model is suitable for simulating the dynamics of surface waters in order to study the interactions between the dynamics of surface waters in the watershed and the variation of the water level in the water bodies e g streams lakes and lagoons study the influences of the evaporation process on the water level verify in a simplified way how restrictions on water availability due infiltration losses interception and processes within the lake affect the hydrodynamics of the lake study surface water runoff phenomena e g floods where this 2d formulation of shallow water equations is valid 2 1 1 time space discretization scheme the time space discretization of the model uses the leapfrog method and a finite difference scheme lee 2013 noh et al 2016 fig 1 shows the alternate calculation procedure in which the water depth is calculated at the center of the mesh and the flows at the boundaries of a adjacent meshes through this scheme the continuity equation equation 1 can be written as follows 11 h i 1 2 j 1 2 t 3 h i 1 2 j 1 2 t 1 2 t m i 1 j 1 2 t 2 m i j 1 2 t 2 x n i 1 2 j 1 t 2 n i 1 2 j t 2 y r e i 1 2 j 1 2 t 2 q e x i 1 2 j 1 2 t 2 e i 1 2 j 1 2 t 2 where i and j are the spatial variation indexes in the x and y directions respectively a detailed description of the discretization of momentum equations and friction terms can be found in lee 2013 and noh et al 2016 2 2 parallel implementation in gpgpu we use parallel computing in gpgpu with the cuda c c programming language and implement in the visual studio community 2013 software programming environment with cuda toolkit version 8 0 we test the model using a geforce gtx 1060 graphic card with 1280 cuda cores and 6 gb of memory running in an intel core i7 7700k cpu with 16 gb of memory an implementation of the sw2d gpu model is also available for use with the linux operational system 2 2 1 code structure in cuda c c the main steps of the code in cuda c c are shown in fig 2 the parts of the code that demand massive calculations are performed on the gpu with parallel processing and the other parts are performed on the cpu with sequential processing the sequential version of the model has the same sequence of operations but all steps are processed in the cpu both the parallelized sw2d gpu model and the sequential version are implemented with double precision the diagram in fig 2 illustrates the model structure and the sequence of operations to approximate the solution of the shallow water equations using the leapfrog method first the input data is read and stored in the cpu memory and then data used in parallel processing in the kernels implemented in cuda c c is copied to gpu memory next the solution of the shallow water equations is approached iteratively in which the calculations distributed in space are performed in parallel on the gpu but the temporal evolution occurs sequentially on the cpu so that the calculations of u v and h are performed for all matrix positions alternately according to the leapfrog method in this procedure the cuda kernels processed on the gpu are called successive times from the cpu iteratively until the solution is approximated using 1 2 t iterations for each second of numerical integration as in each iteration the calculations are performed on t t and t 2 t to enable alternate calculations of the leapfrog method according to fig 1 a courant friedrichs levy cfl condition is required to define the time step t that provides the stability of the numerical solution according to altaie 2016 for the 2d shallow water equations assuming a courant number c less than 1 the stability condition can be defined according to the following equation 12 12 t 1 v max g h x 1 y 1 where g is the acceleration of gravity h is the water depth t is the time step x and y are the spatial resolutions in the x and y directions respectively parallel calculations are performed by cuda kernels through processing entities organized in grids blocks and threads the grid size is equal to the number of blocks needed to cover the entire computational domain each block contains multiple threads e g 512 1024 depending on the gpu each thread on the gpu is associated with a specific index so it can access memory locations in an array in the cuda kernels of the sw2d gpu model we use a 1d indexing scheme in which 2d matrices of m rows and n columns receive linear indexing in row major order and are treated as vectors of m n elements the linear row major indexing structure favors the transfer of coalesced memory in which consecutive threads access consecutive memory addresses so that all threads in a warp access global memory at the same time the linear row major indexing structure of a 2d array and the distribution of blocks and threads in the parallel code are illustrated in fig 3 in the gpu used in this work each block can have a maximum of 1024 threads therefore for an array with less than 1024 elements a single block with the number of threads equivalent to the number of elements in the array is used for arrays with more than 1024 elements multiple blocks with 1024 threads each can be used the determination of the number of blocks and threads and the calling of a cuda kernel are exemplified in the code shown in fig 4 a the structure of a cuda kernel with 1d indexing used in the implementation of the sw2d gpu model kernels is presented in fig 4b thread indices are calculated as 13 t h r i d b l o c k d i m x b l o c k i d x x t h r e a d i d x x where blockdim x is the x dimension of the block dimension blockidx x is the x dimension block identifier and threadidx x is the x dimension thread identifier the while loop shown in fig 4b is used to increment the threads index one grid size griddim x blockdim x at a time this procedure is called a grid stride loop which guarantees the uniform distribution of work to the threads and obtaining maximum memory coalescence the steps with parallel processing are shown on the right side of fig 2 the main cuda kernels involved in each stage are gpu evaporation calc calculates the potential evaporation from temperature and solar radiation data flux calculates flows by approximating the solution of the momentum equations continuity performs the solution of the continuity equation treat error performs the correction of small errors in the flow components at the dry wet boundaries hm hn uu1 vv1 uua vva perform the calculation of depth values and water velocity components in the x and y directions forward updates the initial conditions for solving the shallow water equation in the next time step 2 2 2 input and output data the sw2d gpu has four categories of input data topographic data a file containing the elevations of the terrain is a mandatory entry for the model bathymetry must be included in areas where there are lakes or water bodies digital elevation model dem m model works with dem in a wide range of spatial resolutions hydrometeorological data temperature and solar radiation data is necessary if evaporation process is considered rainfall mm the duration of the simulation and the temporal resolution will be defined based on the period and the temporal resolution of the rainfall data series temperatures c temperature data is only needed when the evaporation process is active solar radiation wm 2 solar radiation data is only needed when the evaporation process is active inflow and outflow m3s 1 the model allows to inform water inlet or outlet at points where monitored data are available water flows entering the computational domain are defined as positive and water flows exiting the domain are negative initial and boundary conditions initial water level m to start the simulation it is necessary to inform the initial water level in the lake this entry is made using a raster map with the same dimensions as the dem boundary condition of known value dirichlet boundary condition m a known water level value is reported in a given location which can be a point where the water level remains constant after reaching a threshold usually at the outlet of the watershed or at some point of interest with a specified value parameters manning roughness coefficient this coefficient controls the friction terms of the shallow water model it can be a value chosen according to the characteristics of the basin aspects of the channel through which the water flows and the soil cover albedo this parameter is variable according to the ability of the medium to reflect incident radiation the albedo is less than 1 and depends not only on the characteristics of the surface but also on the angle of incidence of light lake water losses percentage of water losses inside the lake during rainfall events e g seepage or evaporation during rainfall rainfall interception loss percentage of evaporation loss due to rainfall intercepted by vegetation infiltration percentage of rainfall that infiltrates into the watershed and does not contribute to overland flow or the inflow of water into the lake the outputs of the 2d shallow water model are saved in the vtk format data file format used in the visualization toolkit vtk https vtk org which is designed to provide a consistent data representation scheme for a variety of data set types the main outputs are velocity in the x direction m matrix that stores velocity values in the x direction velocity in the y direction m matrix that stores velocity values in the y direction water level m matrix that stores the values of the water level h above the soil surface the results are saved in a sequence of files called results file number vtk file numbering is sequential and unitary 0 1 2 3 n the real time is obtained by the product between the file number and the time interval defined for saving the results 2 2 3 post processing post processing and visualization of model results saved in vtk files can be performed with any open source software that offers high performance features for viewing large data sets such as visit https wci llnl gov simulation computer codes visit or paraview software https www paraview org 3 case study 1 urban inundation we applied the sw2d gpu model for the simulation of flooding in the urban area hatched area in fig 5 c of the federal university of santa catarina ufsc campus the ufsc catchment is approximately 4 09 km2 with eight contributing sub basins ranging from 0 07 km2 to 1 19 km2 the low area of the basin is urbanized with main drainage channels rectified with concrete walls and bottom in this application the sw2d gpu model was configured to consider a precipitation event homogeneously distributed in space and eight points with known streamflows over the drainage network that supplies the ufsc campus area 3 1 input data the digital elevation model dem in this case study fig 6 a has a spatial resolution of 1 m sistema de informações geográficas de santa catarina sigsc sds 2016 fig 6a shows the dem for the active cells that define the computational domain with 893330 grid points in a matrix of 1588 rows and 1189 columns to obtain the initial condition of the water levels in the study area we perform a simulation of a period of 1 h with constant baseflow values in the 8 points distributed according to fig 6a for the simulation of the flooding process we use the unit hydrograph to derive the streamflow for the eight sub basins that drain to the study area fig 6b the rainfall and streamflow in fig 6b are the main inputs for this case study streamflow is inserted into the system at the locations defined by points p1 p2 p3 p4 p5 p6 p7 and p8 fig 6a at locations near the junctions of the channels we define points wl1 wl2 and wl3 to record changes in water level during the simulation at the outlet of the drainage network near point wl1 we define a boundary in which all the water comes out of the system as the channels walls and bottom is concrete we adopt the manning s coefficient of 0 012 the data is of a period of 4 h and 41 min with rainfall homogeneously distributed in the study area during the first 180 min we use a 0 01 s time step for the simulation 3 2 results the results of the flood simulation in the ufsc campus area are in fig 7 the sw2d gpu model simulates the variation of water levels in response to the contribution of eight sub basins that drain to the study area fig 7a shows the simulated water level at points wl1 wl2 and wl3 choosing the positions of these points fig 6a helps to show how the water level varies along the main channel of the drainage network for example point wl1 near the channel outlet reaches higher levels than the intermediate point wl2 and the point wl3 located near the entrance of the channel these values can help to predict the time when flooding is likely to occur in these regions the spatio temporal variation of water levels is shown in fig 7b g beginning with the spatial distribution of water levels after 90 min of simulation with low intensity rainfall at which the level maximum water level reaches 1 30 m as the rainfall becomes more intense there is an increase in water levels as shown in fig 7 after 165 min of simulation the most critical situation occurs in which some regions of the ufsc campus would be inundated 4 case study 2 integrated lake water level simulation the peri lake experimental catchment is located in santa catarina island perez et al 2020 chaffe et al 2021 santos et al 2021 southern brazil fig 5d is the largest source of water supply on the island and an important ecosystem for biodiversity preservation sbroglia and beltrame 2012 it is located in the transition between tropical and temperate climates with hot summer no dry season with average annual precipitation of 1500 mm peri lake watershed is surrounded by hillslopes covered by some of the remains of subtropical atlantic rain forest and a sandy restinga with a coastal lake with a surface area of 5 7 km2 the maximum depth of the lake is approximately 11 0 m in the central portion and an average depth of 7 0 m and no direct seawater influence hennemann and petrucio 2011 the only outflow communication between the lake and the atlantic ocean is via the sangradouro river in which the main freshwater is collected to supply the florianópolis island oliveira 2002 in this case study the sw2d gpu is used to model the water level variation in peri lake we consider six main fluxes in the water balance i rainfall r ii evaporation ev iii water abstraction for human consumption wa iv forest interception int v infiltration inf vi lake water losses lwl fig 8 presents a conceptual model of the main processes and hydrometeorological variables in this case study in the energy balance part of the solar radiation is reflected by the surface depending on the albedo the rest composes the net radiation that combined with the temperature provides the potential evaporation estimate in the lake part of the rainfall is intercepted by forest part is retained in the watershed and infiltrates the soil and the difference becomes runoff flowing into the lake fig 8 water output from the lake is due to evaporation process and water abstraction for the water treatment plant next we use the sw2d gpu model to simulate the influence of each of these factors on the variation of the water level in the peri lake 4 1 input data the terrain elevation data in the watershed and the bathymetry of the lake area are merged into the same raster map fig 9 a the digital elevation model has a spatial resolution of 12 5 m dataset asf daac 2018 the bathymetry of peri lake is provided by the sanitation company casan rainfall temperature and solar radiation data are monitored in a meteorological station installed in the peri lake expererimental catchment chaffe et al 2021 fig 5e the data period used starts on 02 01 2020 and ends on 05 02 2020 as shown in fig 9b lake level data is provided by casan and the evaporation is estimated using equations 8 10 we adopted a water abstraction from the peri lake of 0 12 m3s 1 the manning s coefficient is defined as 0 12 considering the presence of boulders manning s coefficient 0 040 0 070 with correction for channels covered by forests correction value for the manning s coefficient 0 025 0 050 arcement and schneider 1989 we use a 0 04 s time step for the simulation we assume that about 30 of the direct rainfall in the lake is lost due to different mechanisms transformation into water vapor infiltration in the hyporheic zone and others here called lake water losses lwl and about 20 of the rainfall in the watershed is retained through the infiltration process inf not contributing to the lake supply the water intercepted by the forest during the rainfall event corresponds to 45 sá et al 2019 and albedo to 0 25 albedo values can vary considerably during the day rutan et al 2014 and values between 0 15 and 0 25 are reasonable for a coastal lake in latitude 27 s potential evaporation is applied to water in the lake and where the water level exceeded a defined threshold which we define here as 5 cm 4 2 results the sw2d gpu model integrates modeling of the hillslope streams and lake to consider the diffuse water inputs to the lake as shown in fig 10 b and d in this case study we evaluate the influence of the different processes and parameters that make up the model variation in the water level fig 10a in the results of the simulation considering only the rainfall r and no water outlet from the system black dotted line it can be verified that the model satisfies the principle of conservation of the mass since there are no gains or significant losses of water during dry periods in the results of the simulation considering rainfall r lake water losses lwl 30 infiltration inf 20 and interception int 45 it can be seen that these parameters define the amount of water that is available to supply the watershed and consequently the lake level during the rainfall when evaporation ev is included the lake and the water balance depend on the temperature and solar radiation finally the water abstraction for human consumption is activated these results confirm the sensitivity of the model to the different parameters and variables and how each of them influences the variation in lake water levels the diffuse characteristic of water inputs to the lake during an accumulated rainfall equal to 38 8 mm at the time of the simulation corresponding to 217h of the rainfall data can be seen in fig 10b which also shows that the contributions to the level of water in the lake occur by main channels and also by the runoff on the hillslopes the locations where the water remains stored in the watershed can be seen in fig 10c which shows the watershed after a dry period of approximately 11 days the stored water will contribute to a faster response to the generation of runoff in the next rainfall as these spaces will already be filled with water with an accumulated rain of approximately 4 8 mm there is generation of runoff in which the preferred paths are the main channels of the basin as shown in fig 10d in a scenario with no water outlet from the system fig 10a the 187 4 mm of accumulated rainfall would provide an increase of approximately 55 cm in the water level in the lake however in a scenario considering lake water losses lwl infiltration inf interception int evaporation ev and water abstraction wa the water level in the lake decreases by 5 cm in agreement with the level data monitored in the lake fig 10a 5 performance of the parallel and sequential models here we compare the computation times of the sw2d gpu model with the sequential version as a function of the increase in the number of grid points in the computational domain the numerical tests were performed with 1 min simulations just to obtain computation times in all performance comparisons we consider double precision and the same time step for the two versions cpu and gpu of the model fig 11 the parallel model significantly decreases the simulation times of the 2d shallow water model the speed up comparing the two versions of the model increases as a function of the increase in the number of grid points in the computational domain fig 11 this is due not only to the proper use of memory space but also to the large number of calculations that can be performed in parallel on the gpu the sequential model needs to perform a large number of iterations to traverse the entire computational domain a time consuming process with calculations performed one after the other repeatedly however in very small computational domains the time spent transferring data between the cpu and the gpu can make the model implemented in cuda less advantageous compared to the sequential version processed in the cpu i e the advantages gained from faster calculations done in parallel are lost when transferring data between cpu and gpu the speed up in case study 1 fig 11b is between 10 and 32 times with the number of grid points of 5 104 to 6 106 and the speed up in case study 2 is between 19 and 34 times with the number of grid points of 1 105 to 7 106 fig 11d the performance gains in the simulations are a result of the parallel processing performed in the cuda kernels to show the computation times of the main cuda kernels implemented in the sw2d gpu model compared to the computation times of the equivalent functions of the sequential model processed in the cpu we use the original domains of case studies 1 with 893330 grid points and 2 with 122307 grid points the results are shown in fig 12 the flux continuity and forward cuda kernels have the greatest computational demands fig 12 these are the functions in which the greatest amount of calculations occur to approximate the solution of 2d shallow water equations the performance gain is homogeneous compared to the different cuda kernels but flux continuity and forward are more time consuming 6 conclusions in this work we parallelize and implement a 2d shallow water model using cuda c c and show that parallel computing in the general purpose graphics processing unit gpgpu is a powerful way to accelerate this model the sw2d gpu model was able to perform the simulations 34 times faster than the sequential model in addition to having a significant improvement in performance the model can now also represent the evaporation process in water bodies the model can simulate flood inundation in urban areas and can consider water inputs from different sub basins the model simulates in an integrated manner the surface water dynamics in the hillslope streams and lake as well as human interference through water abstraction this model allows diffuse inflows of water in the lake and is an interesting option especially to study the hydrology of lake ecosystems with little availability of hydrological data using low cost computers equipped with gpu with cuda technology while the interactions between surface and subsurface waters and the water movement induced by the wind were neglected the parallel formulation presented in this work can be easily integrated with other models or receive new modules for the calculation of those processes the sw2d gpu model code is simple and short so it can be used for learning activities and programming studies in cuda language and it can also help with planning for larger code parallelization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank brazilian national council for scientific and technological development cnpq for the scholarship of the first author under the grant number 168003 2018 0 and capes for the scholarship of the third author under the grant number 88887 343151 2019 00 
25733,shallow water models are used for simulating flood and lake hydrodynamics however the computational cost of those models is often high and require high performance computing we present the sw2d gpu a two dimensional shallow water model accelerated by general purpose graphics processing unit the model is implemented in parallel using cuda c c we exemplify the use of the model with two case studies i flood simulation in an urban area and ii water level simulation in a lake catchment we have included potential evaporation in the formulation which expands its application to water level simulations in lakes and reservoirs the sw2d gpu model is approximately 34 times faster than its equivalent sequential version the model can be run in any computer equipped with a nvidia gpu integrated simulations of surface waters in lake watershed and simulations of floods caused by dam break are some of the potential applications keywords shallow water equation high performance computing lake ecosystem parallel computing flood software availability name of code sw2d gpu developer tomas carlotto contact address tomas carlotto posgrad ufsc br t carlotto and pedro chaffe ufsc br p l b chaffe year first available 2021 hardware required graphics processing unit gpu with cuda technology software required cuda toolkit 8 0 or later in windows operating system visual studio community 2013 or later is required program language cuda c c program size 23 7 mb visual studio version 13 2 mb linux version availability https github com labhidro sw2d gpu cost free of charge 1 introduction the shallow water equations are the mathematical basis for many hydrodynamic models their applications ranging from the estimation of water level in rivers and lakes yu et al 2019 ferrarin et al 2020 to the spatiotemporal evolution of floods liu et al 2019 however model instability in complex topographies and long simulation times hinder their application in real cases studies horváth et al 2014 liu et al 2018 a common way to reduce simulation time is to use the two dimensional 2d formulation of the shallow water equations thang et al 2004 lee et al 2014 2016 yet even 2d formulations might be prohibitive in computational domains typical of hydrological applications liu et al 2019 echeverribar et al 2019 high performance computing hpc and codes suitable for parallel processing are the best alternative for accelerating numerical solutions smari et al 2016 most codes that approximate the solution to the shallow water equations are developed using sequential processing or parallelization schemes based on message passing interface mpi or open multi processing openmp compatible with clusters composed of several central processing units cpu o donncha et al 2014 2019 anguita et al 2015 noh et al 2018 2019 recently massively parallel devices such as the general purpose graphics processing unit gpgpu have been shown highly efficient to accelerate the solution of shallow water equations and environmental models with application in real world phenomena brodtkorb et al 2010 ransom and younis 2016 vacondio et al 2017 carlotto et al 2019 dazzi et al 2020 gpus are found on most personal computers and offer great computing power with thousands of processing cores brodtkorb et al 2013 smari et al 2016 consequently systems adapted for the use of languages libraries and tools e g cuda c c cuda fortran for computing in gpus have evolved considerably in the last decade brodtkorb et al 2013 xia et al 2019 cuda technology facilitates code parallelization and use of gpgpu to accelerate numerical solutions equally important are the unified api for hpc code development such as the one presented by o donncha et al 2019 used for parallelization and development of complex code in large computational domains however each parallelization method has positive and negative aspects openmp is simple to implement but has limited scalability due to the size of shared memory mpi is highly scalable for large problems but increased communication between cpus reduces computational performance cuda language for gpu computing is easy to implement and provides great parallel computing power at low cost but processing on gpu is limited by memory constraints noh et al 2018 xia et al 2019 several model implementations harness the computing power of the gpu to approximate the solution of equations that govern the water flow in earth system park et al 2019 used cuda fortran to implement a 2d diffusive wave model resulting in a speed up of approximately 150 times le and kumar 2017 used hybrid cpu gpu parallel computing to develop the dhara model that couples an integrated surface and subsurface flow model numerically solved by alternating direction implicit adi scheme gcs flow le et al 2015 implemented in cuda c c and an ecophysiological multilayer canopy model mlcan drewry et al 2010 implemented in mpi for parallel processing in cpu the dhara model combines the effects of microtopography and climate change on small scale eco hydrological dynamics using the advantages of mpi and gpu parallel computing for high performance in large scale simulations ming et al 2020 developed a real time flood prediction system by coupling a numerical weather prediction model with a hydrodynamic model based on 2d shallow water equations solved using godunov type finite volume scheme with gpu implementation in their approach parallel computing on multiple gpus was used to overcome the physical memory limitations of a single gpu xia et al 2019 in hydrological and hydrodynamic modeling gpgpu accelerated models allow high resolution simulations which can lead to better process representation in the case of lake ecosystems for example rainfall runoff is usually modeled separate from lake hydrodynamics and it is imposed as boundary conditions due to computational constraints acosta et al 2015 janssen et al 2019 in flood inundation forecasting high resolution computational grids are necessary noh et al 2018 2019 however simplifications adopted to reduce the computational cost usually neglect the need for high spatial resolution to simulate flooding in complex terrains kim et al 2012 with the significant gain in processing power offered by gpu such simplifications may no longer be necessary the adaptation of models based on shallow water equations that can account for meteorological variables and the interaction of surface water dynamics between hillslopes streams and lakes are a valuable tool for water resources management especially in watersheds with limited data gu et al 2016 munar et al 2018 jiang et al 2019 moreover these solutions are relevant for problems such as parallelizing and updating models with thousands of lines of code e g weather research and forecasting wrf model and to improve the representation of hydrological process in earth system models schneider et al 2017 despite the advances in computing technologies that provide great parallel processing power at low cost effort is still needed to update and develop hydrological and hydrodynamic model codes compatible with multiple processors and massively parallel devices o donncha et al 2014 le et al 2015 kuffour et al 2020 in order to obtain a high performance 2d shallow water model that simulates hydrological processes in realistic systems using low cost computers we have developed sw2d gpu a two dimensional shallow water model accelerated by gpgpu the accelerated model is based on a sequential version previously implemented in fortran lee 2013 noh et al 2016 the new version is suitable for processing in massively parallel devices with cuda technology we highlight the applicability and performance of the model using two case studies i flood simulation in an urban area and ii simulation of integrated watershed and lake water levels in response to meteorological data and human interferences performance analysis is based on i comparison between the total time that each version of the model parallel and sequential takes to complete the test simulations and ii comparison of the computation time of the cuda kernels processed in the gpu with the equivalent functions of the sequential model executed in the cpu 2 model implementation we base our parallel model on a sequential version of a two dimensional shallow water model previously implemented in fortran lee 2013 noh et al 2016 we have parallelized and completely rewritten the numerical code in the cuda c c programming language the parallel version of the model maintains the same order of tasks as the original sequential implementation to provide a performance comparison between the two versions we propose and implement a formulation to estimate potential evaporation as a function of temperature and solar radiation moreover we include the option of adding an unlimited number of streamflow time series in different locations of the drainage network 2 1 the numerical model the numerical model is based on the 2d shallow water equations lee et al 2014 noh et al 2016 in this formulation the coriolis strength the wind resistance and the viscosity terms are neglected so that the continuity equation of the two dimensional shallow water model are 1 h t m x n y r e q ex e 2 r e r r i n f i n t l w l the momentum equations are 3 m t u m x v m y g h h x f 1 4 n t u n x v n y g h h y f 2 and the friction terms are 5 f 1 g n 2 m u 2 v 2 h 4 3 6 f 2 g n 2 n u 2 v 2 h 4 3 where h is the water depth h h z where z is the topographic elevation u and v are the velocities in the x and y directions respectively m is the flow in the x direction given by uh n is the flow in the y direction given by vh r e is the effective rainfall r is the rainfall inf is the percentage of rainfall infiltration loss int is the percentage of rainfall interception loss lwl is the lake water loses only in the cells in the lake area and water bodies q ex is the term that defines the flow into the drainage network height of water per unit time in a given network cell e is the potential evaporation t is the time f 1 and f 2 are the friction components where g is the acceleration of gravity and n is manning s roughness coefficient the height of water h d that drains in a given section of the channel i e an inlet or outlet of the system is defined by the following equation 7 h d t q d a q ex where q d is accumulated flow at the entrance to the channel and a is the area of the receiving cell potential evaporation is calculated at each time step based on the energy balance method as a function of temperature and solar radiation chow et al 1988 written as follows 8 e r s l v ρ w t k 1000 h h min 0 h h min where r s 1 α r g where r s is net radiation wm 2 r g is global solar radiation wm 2 α is albedo l v is the latent heat of vaporization jkg 1 ρ w is the density of the water kgm 3 t k is the monitoring time interval of the data s and h min is the minimum water depth to activate potential evaporation the constant 1000 is used to convert the results to millimeters and t k adjusts the results to the time interval of the input data to estimate the latent heat of vaporization as a function of temperature we use the equation presented in the work of henderson sellers 1984 written as follows 9 l v 1 91846 1 0 6 t t 273 15 t t 273 15 33 91 2 where t t is the air temperature c observed in time t the density of water is calculated as a function of temperature jones and harris 1992 this way 10 ρ w 999 85308 6 32693 1 0 2 t t 8 523829 1 0 3 t t 2 6 943248 1 0 5 t t 3 3 821216 1 0 7 t t 4 equations 8 10 allow estimating potential evaporation as a function of temperature and solar radiation the formulation of the model is suitable for simulating the dynamics of surface waters in order to study the interactions between the dynamics of surface waters in the watershed and the variation of the water level in the water bodies e g streams lakes and lagoons study the influences of the evaporation process on the water level verify in a simplified way how restrictions on water availability due infiltration losses interception and processes within the lake affect the hydrodynamics of the lake study surface water runoff phenomena e g floods where this 2d formulation of shallow water equations is valid 2 1 1 time space discretization scheme the time space discretization of the model uses the leapfrog method and a finite difference scheme lee 2013 noh et al 2016 fig 1 shows the alternate calculation procedure in which the water depth is calculated at the center of the mesh and the flows at the boundaries of a adjacent meshes through this scheme the continuity equation equation 1 can be written as follows 11 h i 1 2 j 1 2 t 3 h i 1 2 j 1 2 t 1 2 t m i 1 j 1 2 t 2 m i j 1 2 t 2 x n i 1 2 j 1 t 2 n i 1 2 j t 2 y r e i 1 2 j 1 2 t 2 q e x i 1 2 j 1 2 t 2 e i 1 2 j 1 2 t 2 where i and j are the spatial variation indexes in the x and y directions respectively a detailed description of the discretization of momentum equations and friction terms can be found in lee 2013 and noh et al 2016 2 2 parallel implementation in gpgpu we use parallel computing in gpgpu with the cuda c c programming language and implement in the visual studio community 2013 software programming environment with cuda toolkit version 8 0 we test the model using a geforce gtx 1060 graphic card with 1280 cuda cores and 6 gb of memory running in an intel core i7 7700k cpu with 16 gb of memory an implementation of the sw2d gpu model is also available for use with the linux operational system 2 2 1 code structure in cuda c c the main steps of the code in cuda c c are shown in fig 2 the parts of the code that demand massive calculations are performed on the gpu with parallel processing and the other parts are performed on the cpu with sequential processing the sequential version of the model has the same sequence of operations but all steps are processed in the cpu both the parallelized sw2d gpu model and the sequential version are implemented with double precision the diagram in fig 2 illustrates the model structure and the sequence of operations to approximate the solution of the shallow water equations using the leapfrog method first the input data is read and stored in the cpu memory and then data used in parallel processing in the kernels implemented in cuda c c is copied to gpu memory next the solution of the shallow water equations is approached iteratively in which the calculations distributed in space are performed in parallel on the gpu but the temporal evolution occurs sequentially on the cpu so that the calculations of u v and h are performed for all matrix positions alternately according to the leapfrog method in this procedure the cuda kernels processed on the gpu are called successive times from the cpu iteratively until the solution is approximated using 1 2 t iterations for each second of numerical integration as in each iteration the calculations are performed on t t and t 2 t to enable alternate calculations of the leapfrog method according to fig 1 a courant friedrichs levy cfl condition is required to define the time step t that provides the stability of the numerical solution according to altaie 2016 for the 2d shallow water equations assuming a courant number c less than 1 the stability condition can be defined according to the following equation 12 12 t 1 v max g h x 1 y 1 where g is the acceleration of gravity h is the water depth t is the time step x and y are the spatial resolutions in the x and y directions respectively parallel calculations are performed by cuda kernels through processing entities organized in grids blocks and threads the grid size is equal to the number of blocks needed to cover the entire computational domain each block contains multiple threads e g 512 1024 depending on the gpu each thread on the gpu is associated with a specific index so it can access memory locations in an array in the cuda kernels of the sw2d gpu model we use a 1d indexing scheme in which 2d matrices of m rows and n columns receive linear indexing in row major order and are treated as vectors of m n elements the linear row major indexing structure favors the transfer of coalesced memory in which consecutive threads access consecutive memory addresses so that all threads in a warp access global memory at the same time the linear row major indexing structure of a 2d array and the distribution of blocks and threads in the parallel code are illustrated in fig 3 in the gpu used in this work each block can have a maximum of 1024 threads therefore for an array with less than 1024 elements a single block with the number of threads equivalent to the number of elements in the array is used for arrays with more than 1024 elements multiple blocks with 1024 threads each can be used the determination of the number of blocks and threads and the calling of a cuda kernel are exemplified in the code shown in fig 4 a the structure of a cuda kernel with 1d indexing used in the implementation of the sw2d gpu model kernels is presented in fig 4b thread indices are calculated as 13 t h r i d b l o c k d i m x b l o c k i d x x t h r e a d i d x x where blockdim x is the x dimension of the block dimension blockidx x is the x dimension block identifier and threadidx x is the x dimension thread identifier the while loop shown in fig 4b is used to increment the threads index one grid size griddim x blockdim x at a time this procedure is called a grid stride loop which guarantees the uniform distribution of work to the threads and obtaining maximum memory coalescence the steps with parallel processing are shown on the right side of fig 2 the main cuda kernels involved in each stage are gpu evaporation calc calculates the potential evaporation from temperature and solar radiation data flux calculates flows by approximating the solution of the momentum equations continuity performs the solution of the continuity equation treat error performs the correction of small errors in the flow components at the dry wet boundaries hm hn uu1 vv1 uua vva perform the calculation of depth values and water velocity components in the x and y directions forward updates the initial conditions for solving the shallow water equation in the next time step 2 2 2 input and output data the sw2d gpu has four categories of input data topographic data a file containing the elevations of the terrain is a mandatory entry for the model bathymetry must be included in areas where there are lakes or water bodies digital elevation model dem m model works with dem in a wide range of spatial resolutions hydrometeorological data temperature and solar radiation data is necessary if evaporation process is considered rainfall mm the duration of the simulation and the temporal resolution will be defined based on the period and the temporal resolution of the rainfall data series temperatures c temperature data is only needed when the evaporation process is active solar radiation wm 2 solar radiation data is only needed when the evaporation process is active inflow and outflow m3s 1 the model allows to inform water inlet or outlet at points where monitored data are available water flows entering the computational domain are defined as positive and water flows exiting the domain are negative initial and boundary conditions initial water level m to start the simulation it is necessary to inform the initial water level in the lake this entry is made using a raster map with the same dimensions as the dem boundary condition of known value dirichlet boundary condition m a known water level value is reported in a given location which can be a point where the water level remains constant after reaching a threshold usually at the outlet of the watershed or at some point of interest with a specified value parameters manning roughness coefficient this coefficient controls the friction terms of the shallow water model it can be a value chosen according to the characteristics of the basin aspects of the channel through which the water flows and the soil cover albedo this parameter is variable according to the ability of the medium to reflect incident radiation the albedo is less than 1 and depends not only on the characteristics of the surface but also on the angle of incidence of light lake water losses percentage of water losses inside the lake during rainfall events e g seepage or evaporation during rainfall rainfall interception loss percentage of evaporation loss due to rainfall intercepted by vegetation infiltration percentage of rainfall that infiltrates into the watershed and does not contribute to overland flow or the inflow of water into the lake the outputs of the 2d shallow water model are saved in the vtk format data file format used in the visualization toolkit vtk https vtk org which is designed to provide a consistent data representation scheme for a variety of data set types the main outputs are velocity in the x direction m matrix that stores velocity values in the x direction velocity in the y direction m matrix that stores velocity values in the y direction water level m matrix that stores the values of the water level h above the soil surface the results are saved in a sequence of files called results file number vtk file numbering is sequential and unitary 0 1 2 3 n the real time is obtained by the product between the file number and the time interval defined for saving the results 2 2 3 post processing post processing and visualization of model results saved in vtk files can be performed with any open source software that offers high performance features for viewing large data sets such as visit https wci llnl gov simulation computer codes visit or paraview software https www paraview org 3 case study 1 urban inundation we applied the sw2d gpu model for the simulation of flooding in the urban area hatched area in fig 5 c of the federal university of santa catarina ufsc campus the ufsc catchment is approximately 4 09 km2 with eight contributing sub basins ranging from 0 07 km2 to 1 19 km2 the low area of the basin is urbanized with main drainage channels rectified with concrete walls and bottom in this application the sw2d gpu model was configured to consider a precipitation event homogeneously distributed in space and eight points with known streamflows over the drainage network that supplies the ufsc campus area 3 1 input data the digital elevation model dem in this case study fig 6 a has a spatial resolution of 1 m sistema de informações geográficas de santa catarina sigsc sds 2016 fig 6a shows the dem for the active cells that define the computational domain with 893330 grid points in a matrix of 1588 rows and 1189 columns to obtain the initial condition of the water levels in the study area we perform a simulation of a period of 1 h with constant baseflow values in the 8 points distributed according to fig 6a for the simulation of the flooding process we use the unit hydrograph to derive the streamflow for the eight sub basins that drain to the study area fig 6b the rainfall and streamflow in fig 6b are the main inputs for this case study streamflow is inserted into the system at the locations defined by points p1 p2 p3 p4 p5 p6 p7 and p8 fig 6a at locations near the junctions of the channels we define points wl1 wl2 and wl3 to record changes in water level during the simulation at the outlet of the drainage network near point wl1 we define a boundary in which all the water comes out of the system as the channels walls and bottom is concrete we adopt the manning s coefficient of 0 012 the data is of a period of 4 h and 41 min with rainfall homogeneously distributed in the study area during the first 180 min we use a 0 01 s time step for the simulation 3 2 results the results of the flood simulation in the ufsc campus area are in fig 7 the sw2d gpu model simulates the variation of water levels in response to the contribution of eight sub basins that drain to the study area fig 7a shows the simulated water level at points wl1 wl2 and wl3 choosing the positions of these points fig 6a helps to show how the water level varies along the main channel of the drainage network for example point wl1 near the channel outlet reaches higher levels than the intermediate point wl2 and the point wl3 located near the entrance of the channel these values can help to predict the time when flooding is likely to occur in these regions the spatio temporal variation of water levels is shown in fig 7b g beginning with the spatial distribution of water levels after 90 min of simulation with low intensity rainfall at which the level maximum water level reaches 1 30 m as the rainfall becomes more intense there is an increase in water levels as shown in fig 7 after 165 min of simulation the most critical situation occurs in which some regions of the ufsc campus would be inundated 4 case study 2 integrated lake water level simulation the peri lake experimental catchment is located in santa catarina island perez et al 2020 chaffe et al 2021 santos et al 2021 southern brazil fig 5d is the largest source of water supply on the island and an important ecosystem for biodiversity preservation sbroglia and beltrame 2012 it is located in the transition between tropical and temperate climates with hot summer no dry season with average annual precipitation of 1500 mm peri lake watershed is surrounded by hillslopes covered by some of the remains of subtropical atlantic rain forest and a sandy restinga with a coastal lake with a surface area of 5 7 km2 the maximum depth of the lake is approximately 11 0 m in the central portion and an average depth of 7 0 m and no direct seawater influence hennemann and petrucio 2011 the only outflow communication between the lake and the atlantic ocean is via the sangradouro river in which the main freshwater is collected to supply the florianópolis island oliveira 2002 in this case study the sw2d gpu is used to model the water level variation in peri lake we consider six main fluxes in the water balance i rainfall r ii evaporation ev iii water abstraction for human consumption wa iv forest interception int v infiltration inf vi lake water losses lwl fig 8 presents a conceptual model of the main processes and hydrometeorological variables in this case study in the energy balance part of the solar radiation is reflected by the surface depending on the albedo the rest composes the net radiation that combined with the temperature provides the potential evaporation estimate in the lake part of the rainfall is intercepted by forest part is retained in the watershed and infiltrates the soil and the difference becomes runoff flowing into the lake fig 8 water output from the lake is due to evaporation process and water abstraction for the water treatment plant next we use the sw2d gpu model to simulate the influence of each of these factors on the variation of the water level in the peri lake 4 1 input data the terrain elevation data in the watershed and the bathymetry of the lake area are merged into the same raster map fig 9 a the digital elevation model has a spatial resolution of 12 5 m dataset asf daac 2018 the bathymetry of peri lake is provided by the sanitation company casan rainfall temperature and solar radiation data are monitored in a meteorological station installed in the peri lake expererimental catchment chaffe et al 2021 fig 5e the data period used starts on 02 01 2020 and ends on 05 02 2020 as shown in fig 9b lake level data is provided by casan and the evaporation is estimated using equations 8 10 we adopted a water abstraction from the peri lake of 0 12 m3s 1 the manning s coefficient is defined as 0 12 considering the presence of boulders manning s coefficient 0 040 0 070 with correction for channels covered by forests correction value for the manning s coefficient 0 025 0 050 arcement and schneider 1989 we use a 0 04 s time step for the simulation we assume that about 30 of the direct rainfall in the lake is lost due to different mechanisms transformation into water vapor infiltration in the hyporheic zone and others here called lake water losses lwl and about 20 of the rainfall in the watershed is retained through the infiltration process inf not contributing to the lake supply the water intercepted by the forest during the rainfall event corresponds to 45 sá et al 2019 and albedo to 0 25 albedo values can vary considerably during the day rutan et al 2014 and values between 0 15 and 0 25 are reasonable for a coastal lake in latitude 27 s potential evaporation is applied to water in the lake and where the water level exceeded a defined threshold which we define here as 5 cm 4 2 results the sw2d gpu model integrates modeling of the hillslope streams and lake to consider the diffuse water inputs to the lake as shown in fig 10 b and d in this case study we evaluate the influence of the different processes and parameters that make up the model variation in the water level fig 10a in the results of the simulation considering only the rainfall r and no water outlet from the system black dotted line it can be verified that the model satisfies the principle of conservation of the mass since there are no gains or significant losses of water during dry periods in the results of the simulation considering rainfall r lake water losses lwl 30 infiltration inf 20 and interception int 45 it can be seen that these parameters define the amount of water that is available to supply the watershed and consequently the lake level during the rainfall when evaporation ev is included the lake and the water balance depend on the temperature and solar radiation finally the water abstraction for human consumption is activated these results confirm the sensitivity of the model to the different parameters and variables and how each of them influences the variation in lake water levels the diffuse characteristic of water inputs to the lake during an accumulated rainfall equal to 38 8 mm at the time of the simulation corresponding to 217h of the rainfall data can be seen in fig 10b which also shows that the contributions to the level of water in the lake occur by main channels and also by the runoff on the hillslopes the locations where the water remains stored in the watershed can be seen in fig 10c which shows the watershed after a dry period of approximately 11 days the stored water will contribute to a faster response to the generation of runoff in the next rainfall as these spaces will already be filled with water with an accumulated rain of approximately 4 8 mm there is generation of runoff in which the preferred paths are the main channels of the basin as shown in fig 10d in a scenario with no water outlet from the system fig 10a the 187 4 mm of accumulated rainfall would provide an increase of approximately 55 cm in the water level in the lake however in a scenario considering lake water losses lwl infiltration inf interception int evaporation ev and water abstraction wa the water level in the lake decreases by 5 cm in agreement with the level data monitored in the lake fig 10a 5 performance of the parallel and sequential models here we compare the computation times of the sw2d gpu model with the sequential version as a function of the increase in the number of grid points in the computational domain the numerical tests were performed with 1 min simulations just to obtain computation times in all performance comparisons we consider double precision and the same time step for the two versions cpu and gpu of the model fig 11 the parallel model significantly decreases the simulation times of the 2d shallow water model the speed up comparing the two versions of the model increases as a function of the increase in the number of grid points in the computational domain fig 11 this is due not only to the proper use of memory space but also to the large number of calculations that can be performed in parallel on the gpu the sequential model needs to perform a large number of iterations to traverse the entire computational domain a time consuming process with calculations performed one after the other repeatedly however in very small computational domains the time spent transferring data between the cpu and the gpu can make the model implemented in cuda less advantageous compared to the sequential version processed in the cpu i e the advantages gained from faster calculations done in parallel are lost when transferring data between cpu and gpu the speed up in case study 1 fig 11b is between 10 and 32 times with the number of grid points of 5 104 to 6 106 and the speed up in case study 2 is between 19 and 34 times with the number of grid points of 1 105 to 7 106 fig 11d the performance gains in the simulations are a result of the parallel processing performed in the cuda kernels to show the computation times of the main cuda kernels implemented in the sw2d gpu model compared to the computation times of the equivalent functions of the sequential model processed in the cpu we use the original domains of case studies 1 with 893330 grid points and 2 with 122307 grid points the results are shown in fig 12 the flux continuity and forward cuda kernels have the greatest computational demands fig 12 these are the functions in which the greatest amount of calculations occur to approximate the solution of 2d shallow water equations the performance gain is homogeneous compared to the different cuda kernels but flux continuity and forward are more time consuming 6 conclusions in this work we parallelize and implement a 2d shallow water model using cuda c c and show that parallel computing in the general purpose graphics processing unit gpgpu is a powerful way to accelerate this model the sw2d gpu model was able to perform the simulations 34 times faster than the sequential model in addition to having a significant improvement in performance the model can now also represent the evaporation process in water bodies the model can simulate flood inundation in urban areas and can consider water inputs from different sub basins the model simulates in an integrated manner the surface water dynamics in the hillslope streams and lake as well as human interference through water abstraction this model allows diffuse inflows of water in the lake and is an interesting option especially to study the hydrology of lake ecosystems with little availability of hydrological data using low cost computers equipped with gpu with cuda technology while the interactions between surface and subsurface waters and the water movement induced by the wind were neglected the parallel formulation presented in this work can be easily integrated with other models or receive new modules for the calculation of those processes the sw2d gpu model code is simple and short so it can be used for learning activities and programming studies in cuda language and it can also help with planning for larger code parallelization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank brazilian national council for scientific and technological development cnpq for the scholarship of the first author under the grant number 168003 2018 0 and capes for the scholarship of the third author under the grant number 88887 343151 2019 00 
25734,in the last decades the number of publications dedicated to the application of species distribution models sdms to invasive alien plants iaps has constantly increased although recent reviews have addressed very relevant issues in the application of sdms the modelling approaches i e algorithms applied to iaps have not been systematized therefore we undertook a bibliographic review of articles devoted to sdms and iaps from 1996 to 2019 our results indicate that maximum entropy generalized linear models boosted regression trees and random forest were the four most frequent types of modelling approaches it was clear that there was a variety of different approaches regarding the type of algorithm to be used we discuss the characteristics of the most cited algorithms providing examples of their application in sdms dedicated to iaps we advocate the use of a combination of different algorithms an intensive evaluation of predictors a thorough validation process and a critical analysis of model predictions keywords biotic data ecological modelling invasive alien plants iaps modelling approaches predictions species distribution models sdms 1 introduction 1 1 sdms and ecological niche the development ecology was largely influenced by gause s competitive exclusion principle and hutchinson s and macarthur s works on the ecological niche and species packing gause and witt 1935 andrewartha and birch 1953 hutchinson 1957 macarthur 1957 bruno et al 2003 ecological theory indicates that species show a unimodal reaction to the source of constraint in an ecological n dimensional space whittaker 1975 austin 1985 ter braak 1986 the volume of this ecological space in which an organism can survive and reproduce defines its environmental niche hegel et al 2010 a concept was originally defined by grinnell 1917 and elton 1927 hutchinson 1957 defined the realized niche as a subset of the fundamental niche limited by biotic interactions such as the presence of competitors or the lack of mutualists pulliam 2000 revised that definition to allow for source sink theory and dispersal limitations thus sink populations could allow the realized niche to be larger than the fundamental niche while restrictions on dispersal and previous disturbance could restrict the realized niche beyond the impacts of biotic interactions the differences between the two visions may be relevant to invasive species principally when considering the realized niche in native ranges versus the potential global niche elith et al 2006 many species distribution models sdms are based on the idea that the niche is an n dimensional hypervolume where axes represent the n resources that limit the fitness of an organism hegel et al 2010 the dimensions of the ecological niche are limited to that subset of all possible conditions that directly affect the organism s fitness kearney 2006 therefore the first step to predict the distribution occurrence or abundance of a species is the quantification of its niche space 1 2 environmental predictors a key challenge to predict habitat suitability and occurrence in complex landscapes is to link the non spatial niche relationships with the complex patterns representing the way how heterogeneous landscapes overlap the environmental gradients cushman et al 2007 hossard and chopin 2019 ecophysiology and biophysics can be used to select variables that might describe and define the target species distributions huntley et al 1995 guisan and zimmermann 2000 austin 2005 in practice modellers often focus on species response to climate however this might not be the essential or most relevant factor for some species and spatial extents hulme 2003 spatial and temporal complexity require a mechanistic understanding of the key drivers that limit species distributions and abundances including the spatial scales at which they operate and their temporal gaps and effects cushman et al 2007 a careful selection of predictors utilizing the knowledge gained from physiology and the study of environmental processes should be considered to improve the interpretability and quality of statistical models austin 2007 nevertheless identifying the factors affecting species distribution is an unresolved ecological issue araújo and guisan 2006 there are often many combinations of environmental factors that can explain species distributions particularly when correlated leading to an uncertainty about the impact of each factor when taken individually platts et al 2008 murray and conner 2009 ashcroft et al 2011 sdm researchers have used a wide variety of predictors driven by their availability and the conviction that the model will identify the most relevant variables elith and leathwick 2009 as necessary guidelines for the definition of biologically informative and generalizable sdms authors have stressed the use ecologically and biophysically meaningful predictors and the consideration of their direct and indirect impacts on species distributions austin 2007 newbold 2010 bucklin et al 2015 climate is a dominant driver of species distributions and recent climate change became a widely acknowledged global change factor pearson and dawson 2003 lebreton 2011 ranjitkar et al 2016 however climate only predictors should be considered as incomplete representations of complex environmental systems because other factors may affect species distributions heikkinen et al 2006 beale et al 2008 araújo and peterson 2012 bucklin et al 2015 and should be combined with climate variables austin and van niel 2011 including land cover and human influence e g blach overgaard et al 2010 costa et al 2012 2013 variables such as altitude and latitude have a relationship with the organisms presence through their correlation with climate austin 2007 nonetheless the correlation between indirect and direct factors is location specific and might not be linear complicating the form of the species responses austin 2007 some studies have shown that potential errors in predictors can have profound effects on model outcomes e g van neil et al 2004 and that the accuracy of predictions depends on the selected variables thus clear justifications for their use should be provided e g stockwell and peterson 2002 austin and van niel 2011 predictors have been often selected based on their correlation with the target species distribution guisan and thuiller 2005 syphard and franklin 2009 therefore presuming that the identified relationship remains unchanged the future distribution can be projected over different environmental conditions either found in the future or at other locations harris et al 2013 however if the variables entering the model are not causally linked to the species distribution predictive models can be incorrect when transferred to other scenarios varela et al 2011 another key factor is the selection of a minimum number of feasible predictors since as more variables are included in the model the narrower its predictions will be bullock et al 2000 mac nally 2000 burnham and anderson 2004 beaumont et al 2005 sobéron 2010 trabucco et al 2010 braunisch et al 2013 cauwer et al 2014 moreover other reasons might justify a reduction in the number of predictors including minimization of computation time avoidance of overfitting increase in transferability improvement in the understanding of the causal relationships guisan and thuiller 2005 barbet massin et al 2010 austin and van niel 2011 braunisch et al 2013 petitpierre et al 2017 1 3 species data the nature of the dependent biotic variable is crucial to model structure austin 2007 meier et al 2010 with three types of biotic information generally regarded austin 2007 wisz et al 2013 abundance measurements presence absence data and presence only data identifying the presence of a species or individual at a location is relatively straightforward however the identification or confirmation of its absence is much more complicated mackenzie 2005 absence data are viewed as misleading because the species might not be at equilibrium with the environment or might be difficult to detect václavík and meentemeyer 2009 elith et al 2010 hegel et al 2010 the drawbacks of presence only data have been widely debated and include the possibility of failure to identify presences due to detection bias the overrepresentation of rare species and underrepresentation of common species and the erroneous application of statistical methods relying on the use of true absences zaniewski et al 2002 brotons et al 2004 elith et al 2006 elith and graham 2009 guillera arroita et al 2015 one approach to deal with these situations is to use pseudo absences representing the habitat available for an organism elith et al 2006 williams et al 2009 abundance based on the frequency of reporting rates or on an estimate of true population size harrison and cherry 1997 renwick et al 2011 howard et al 2014 may be considered as an indicator of habitat quality pearce and ferrier 2001 most research has focused on modelling presence absence or abundance data hirzel and guisan 2002 cawsey et al 2002 the former has been used to estimate the probability of occupancy for a sample location corrected for imperfect detection mackenzie et al 2006 maher et al 2014 ramsey et al 2015 and has numerous applications mcpherson et al 2004 allouche et al 2006 guisan et al 2006 foody 2011 variable evaluation prioritization of sites for conservation monitoring of species decline or of range expansion nevertheless there has been an increasing interest in the use of presence only data pearce and boyce 2006 gathered from biological collections and utilized for conservation invasive species and climate change modelling graham et al 2004 this has often been justified by the lack of systematic survey data linked to a pervasive demand for mapped predictions elith et al 2006 2011 gormley et al 2011 as a consequence a wide discussion opposing the use of presence only and presence absence data has ensued e g soberón and peterson 2005 hirzel and le lay 2008 jiménez valverde et al 2008 soberón and nakamura 2009 lobo et al 2010 with variable attitudes towards presence only data elith and leathwick 2009 elith et al 2011 bird et al 2014 presence absence data for individual species such as from quadrat data elith and franklin 2013 have the advantage of providing information on what has been surveyed however the meaning of an absence can be unclear and can vary with the spatiotemporal resolution of sampling green 1971 pulliam 2000 hirzel et al 2001 hirzel and guisan 2002 le maitre et al 2008 kéry et al 2010 elith and franklin 2013 unaccounted abiotic or biotic factors e g type of substrate soil ph absence of dispersers presence of predators could have prevented successful establishment the species has not been able to reach that locality yet or it was not detected 1 4 sdms and biological invasions it has been shown that the successful application of models and the possibility of determining a significant effect is largely dependent on the goal and also that a careful consideration in model selection is required if a long term forecasting applicability is to be achieved zenni et al 2014 tredennick et al 2016 makridakis et al 2018 thus despite the limitations of sdms their use in the design of reliable biodiversity conservation strategies management strategies and in decision making has been largely accepted thuiller et al 2008 ausseil et al 2019 eker et al 2019 srivastava et al 2019 in recent years there has been a strong drive by modellers to adopt predictive tools that would support measures to reduce the impact of biological invasions research devoted to invasive alien species ias using open source approaches to enhance model development has facilitated the use of statistical algorithms by a broader subset of the ecosystem modelling community a better understanding of these tools enabled the conceptualization of practical applications however although modellers are now able to explain how and why many biological invasions have occurred we are still unable to fully forecast ias expansion zenni et al 2014 nonetheless ias have been widely recognized as a major environmental problem at global and local scales zedda et al 2010 posing serious threats to ecosystem structure diversity and function olden et al 2008 ehrenfeld 2010 hernández et al 2014 plant invasions can have powerful effects on ecosystems and human welfare pimentel et al 2005 pejchar and mooney 2009 van kleunen et al 2010 and negatively affect indigenous plant populations by becoming either monopolisers or resource limiting donors or by changing soil stability increasing erosion colonizing open substrates changing litter accumulation or other soil traits vitousek 1990 richardson et al 2000 brooks et al 2004 presently invasive alien plants iaps constitute a relevant component of the vegetation in urban ecosystems throughout the globe alston and richardson 2006 lima et al 2013 kendal et al 2014 although much of the global research has been dedicated to invasive plants in forested and agricultural landscapes woody plant invasions have been associated with a variety of socio ecological factors including land use change binggelli 1996 binggeli et al 1998 richardson 1998 mack et al 2000 richardson and rejmánek 2004 2011 silva and smith 2004 2006 williams and cameron 2006 silva et al 2008 costa et al 2012 2013 staudhammer et al 2015 a large body of literature has aimed to identify and classify invasive species determinants levine et al 2003 van kleunen et al 2010 causes of invasiveness are an important research topic in ecology and invasion biology and effective management is facilitated when the mechanisms of invasion and their ecological impacts are understood lowry et al 2013 studies about woody plant invaders have shed light on several aspects such as functional traits related with physiology biomass allocation growth rate size and fitness kolar and lodge 2001 grotkopp et al 2002 2004 petit et al 2004 van kleunen and richardson 2007 however there has been a considerable debate about the traits that could potentially increase invasiveness i e a high probability of successful establishment since the characteristics of the invaded ecosystem are also relevant crawley 1987 roy 1990 rejmánek et al 2005 leishman et al 2007 pyšek and richardson 2008 mason et al 2008 gordon et al 2010 richardson and rejmánek 2011 a relevant component of ecological research has been directed towards a practical knowledge about the distribution of threatened species their habitat requirement and to the assessment of the impacts by ias alpert et al 2000 d antonio and meyerson 2002 levin et al 2003 engler et al 2004 silva et al 2015 understanding the factors associated with ias expansion is the first step towards the development of effective control measures minimizing negative environmental impacts that often rely on habitat suitability modelling and on decision analysis le lay et al 2001 hirzel et al 2001 cacho et al 2010 tonini et al 2017 fortunately the fast development of statistical techniques and computing power is facilitating the design of tools that can support pro active strategies to avoid the introduction or to guide screening to impede the establishment of new ias guisan and zimmermann 2000 peterson 2003 drake and lodge 2006 roura pascual et al 2009 within this framework sdms have been increasingly used to predict ias distribution and to better understand the drivers underlaying biological invasions elith et al 2010 2011 elith and franklin 2013 the successes of these modelling tools have allowed to correlate ias distribution with environmental variables both in their native range and in the invaded areas thereby facilitating early detection of new invasions and maximizing the success of risk assessment protocols richardson et al 2000 peterson 2003 elith et al 2006 hulme 2006 bradley 2013 1 5 why a literature review a wide variety of statistical and machine learning methods has been introduced to highlight unresolved issues in sdms austin 2002 2007 guisan and thuiller 2005 elith and graham 2009 elith and leathwick 2009 meanwhile the large number of different approaches and techniques has raised several methodological questions i the implementation of new statistical methods phillips et al 2006 elith et al 2008 ii the evaluation of sampling design on model performance edwards et al 2006 guisan et al 2006 iii the evaluation of sample size on model accuracy zimmermann et al 2007 wisz et al 2008 iv the removal of spatial autocorrelation from model fitting dormann et al 2007 zimmermann et al 2010 v the comparison of statistical results for different models elith et al 2006 and vi general model evaluation procedures allouche et al 2006 smulders et al 2010 in summary many efforts have been made to improve the statistical basis of the application of sdms as shown by the large number of published articles over the last decades zimmermann et al 2010 araújo et al 2019 there have been continuous advances in species distribution modelling and in computation power as well as a growing need to better understand and predict biological invasions particularly in the case of iaps with their possible association with climate change moreover despite the existence of recent reviews and general guidelines e g araújo et al 2019 zurell et al 2020 it is not clear if general trends exist in the use of different modelling approaches i e algorithms along time therefore we think that a literature review analysing a broad compilation of scientific literature devoted to iap distribution modelling and addressing the different modelling approaches applied in sdms would be relevant in fact a new researcher in the field facing the wide diversity of modelling algorithms might wonder about the way to proceed our aim was to provide a comprehensive literature review about the current state of the art considering the different algorithms starting with a structured literature search combined with a bibliometric analysis we highlight the more cited algorithms we present an overview of the main characteristics advantages and limitations of each modelling approach and we conclude with examples of their application to the study of iaps finally we present suggestions for general guidelines when applying sdms to iaps fig 1 illustrates the flowchart used to structure the proposed analysis 2 methods to produce a knowledge synthesis we used a systematic search of literature together with a bibliometric approach in order to identify the literature s substance as well as its context and processes e g analytical and focused trends janssen 2007 tricco et al 2010 all documents used in this research were based on two online academic citations indexes for all available years google scholar and web of science the datasets were accessed in the period between march 2019 and march 2020 we used the following terms and their combinations as keywords to search titles part of titles abstracts or keywords invasive plant s alien plant s invader plant s exotic plant s non native plant s non indigenous plant s woody invasive plant woody alien plant s woody exotic plant s detection habitat habitat suitability invasions invasiveness land management potential distribution predicting predictive species distribution generalized linear models or glm generalized additive models or gam ecological niche factor analysis or enfa surface range envelope or sre quantile regression or qr structural equation modelling or sem geographically weighted regression or gwr multivariate adaptive regression splines or mars artificial neural networks or ann classification and regression trees or cart classification tree analysis or cta boosted regression trees or brt random forest or rf bayesian analysis and approach bayesian methods generalized dissimilarity modelling or gdm entropy maximization maxent and genetic algorithm for rule set production or garp an example used for a combination of search terms was alien plant potential distribution glm after searching the literature related with iaps and sdms we reviewed the title and abstract of each article and rejected any articles that did not meet our targets we organized and classified articles based on the following classification scheme i type of modelling algorithm ii taxonomic group plant species iii year of publication iv number of citations v journal vi journal impact factor if and vii geographical location of the study area the journal if was determined for each article as reported in the year 2018 which was the latest data available in this study we only discuss the articles published in the period beginning in 1996 since this was the earliest date found in our research 3 results 3 1 bibliometric analysis and basic statistics fig 2 displays the number of publications covering iaps and sdms between 1996 and 2019 the total number of articles was 526 corresponding to 22699 citations table 1 the annual number of articles the number of cited references and their percentage increased considerably along time when considering the total number of citations for the period the number of citations per year of publication and per article varied over the years from 63 to a maximum of 4024 table 1 the earliest article was published in 1996 higgins and richardson 1996 with no publications in 1998 2001 and 2002 only 5 articles published before 2003 and with 521 publications from 2003 to 2019 fig 2 a total of 123 journals were involved appendix s1 see table 2 with the top 20 journals biological invasions ranked first with 71 articles 13 5 and an impact factor if of 2 897 diversity and distributions if 4 092 ranked second with 45 articles 8 6 plos one if 2 776 ranked third with 19 articles 3 6 global change biology with 7 articles 1 331 should be noted because of its high if 8 880 table 3 shows the frequency use of each of 17 algorithms including the number of articles and the corresponding percentage between 1996 and 2019 maxent 21 5 glm 15 0 brt 10 3 and rf 8 7 were the four most frequent types of algorithms although those percentages fluctuated maxent has consistently become more frequently used fig 3 shows the evolution of the percentage of articles and citations along time reflecting the diversity of modelling techniques applied to iaps the first article recognized was in the field of glm in 1996 in that early period most articles and citations demonstrated the use of a low variety of algorithms however starting in 2004 2007 there was an increase in the variety of algorithms applied to iaps which was accentuated in more recent periods fig 3 3 2 overview of the geographical distribution of sdm use we should point out a high incidence of studies in europe 301 asia 165 north america 119 africa 82 oceania 62 and south america 25 there was a wide geographical variation regarding the use of the 17 types of modelling techniques see supplementary fig 1 amongst the geographic regions usa spain and austria received much attention having more case studies that applied maxent bm glm brt and gam qr and gwr were targeted by a few publications with specific application to south africa and usa respectively globally it was found that brt bm maxent and garp were used more broadly while enfa qr and gwr showed a more restricted use in terms of article number and geographic range see supplementary fig 1 4 discussion in this section we discuss the characteristics and potential application of the different algorithms found in the literature review starting from the most cited ones we also provide examples of the use of each algorithm in sdms applied to iaps we conclude with the suggestion of several guidelines for the application of sdms to the study of iaps 4 1 entropy maximization maxent maximum entropy models are density estimation methods used in a range of natural language processing tasks berger et al 1996 rosenfeld 1996 pietra et al 1997 malouf 2002 manning and klein 2003 ratnaparkhi 2010 convertino et al 2014 and in many other applications such as in habitat and species distribution models phillips et al 2004 2006 dudík et al 2007 elith et al 2006 2011 hegel et al 2010 elith et al 2006 entropy maximization has been increasingly used as in ecology phillips et al 2004 2006 harte et al 2008 cunze and tackenberg 2015 as an approach for modelling species distributions developed in association with a software called maxent phillips and dudík 2008 hegel et al 2010 maxent models are based on the density estimation principle advocated by jaynes 1957 which consists of selecting the distribution that is the closest to the uniform distribution across the study area hernandez et al 2006 however since the derived probability distribution is dependent on the observed data sample bias can add error to the resulting predictions phillips et al 2006 hegel et al 2010 the model assumes that used locations are compared to a sample of available locations across the landscape pearce and boyce 2006 hegel et al 2010 therefore presence only data can be used in the analysis rupprech et al 2011 thus maxent is a machine learning process employing a statistical mechanism that creates a probability distribution across the entire study area with each location receiving a non negative value such that the sum of values for all locations in the study area equals one hegel et al 2010 rebelo et al 2010 while this technique can be seen as a general purpose method for making predictions from incomplete information phillips et al 2006 phillips and dudík 2008 elith et al 2011 the use of this algorithm to model species distributions from presence only data has been shown to perform well in comparison with other algorithms elith et al 2006 hernandez et al 2006 the invasiveness of acacia paradoxa dc fabaceae in south africa was evaluated by zenni et al 2009 for bioclimatic modelling they used maxent to estimate the climatic niche of a paradoxa in australia and to project its potential range in south africa phillips et al 2006 zenni et al 2009 2014 lemke et al 2011 implemented a simulation method to determine the future spread of japanese honeysuckle lonicera japonica thunb caprifoliaceae a forest invasive plant in the south eastern united states maxent runs were separately validated and tested to check modelling quality and species distribution forecasting based on presence absence or presence only data knowledge of the probable areas that would become under the invasion of bushmint hyptis suaveolens l poit lamiaceae was viewed as immensely important for rapid response and mitigation thus padalia et al 2014 modelled india s possible bushmint intrusion array by exploring maxent s predictive capability wedelia trilobata l a hitchc asteraceae an ornamental ground cover plant introduced from central america to areas around the world has become invasive in many regions to predict its spread in central america qin et al 2015 used maxent to create models based on occurrence records from the full distribution range or from the native range only roger et al 2015 aimed to predict the influence of climate change on the potential distribution of naturalized alien plant species they developed an australia wide screening tool and used maxent to build models of abiotically suitable habitat for 292 alien plant species in australia qin et al 2018 used the maxent cycle intrusion method to study spatiotemporal variation in amaranthus retroflexus l amaranthaceae distribution in china arogoundade et al 2019 analysed the habitats susceptible to parthenium hysterophorus l asteraceae invasion in kwazulu natal using environmental data and sentinel 2 multi spectral imagery as input to maxent models 4 2 generalized linear models the use of contemporary approaches to regression has provided a helpful tool for modelling the spatial distribution of species and communities guisan and zimmermann 2000 scott et al 2002 logistic regression is a relatively straightforward technique to regress a binary response variable presence absence against climatic variables jeschke and strayer 2008 this is a particular case of generalized linear models an important statistical development of the last 30 years glm mccullagh and nelder 1989 glms are mathematical extensions of linear models based on an assumed relationship link function between the mean response variable and the linear combination of the explanatory variables guisan et al 2002 indeed this sort of connection does not force information into unnatural scales and allows non linearity and heteroscedasticity in data hastie and tibshirani 1990 moreover data from several families of probability distributions can be handled such as normal binomial poisson negative binomial or gamma distributions hence glms are viewed as parametric models guisan and zimmermann 2000 guisan et al 2002 model parameters or coefficients are estimated via maximum likelihood and specify the change in the response following a one unit change in predictor hegel et al 2010 clearly selecting predictors and their possible transformations e g polynomial terms is the most significant and difficult step when fitting a glm guisan and zimmermann 2000 engler et al 2004 since a sum of weighted ecogeographical predictors explains the dependent variable presence absence hirzel et al 2001 the weights are tuned to generate the best fit between the model and the calibration data set nicholls 1989 jongman et al 1995 the application of regression tools for species distribution modelling assumes the existence of a pseudo equilibrium between the organism and the environment guisan and theurillat 2000 consequently the use of such methods to classify environmental factors responsible for the distribution of species for example that tend to extend their range in the study area will lead to biased results such as truncated ecological response curves hirzel et al 2001 guisan et al 2002 nevertheless logistic regression has been commonly used to forecast the distribution of species for reserve layout environmental impact assessment and the understanding of the factors influencing species distribution nicholls 1989 austin et al 1990 pereira and itami 1991 brown 1994 leathwick 1995 glms were used in early analyses of presence absence and count data often with simple additive combinations of linear terms elith and leathwick 2009 as the common occurrence of nonlinear species responses to the environment was recognized more studies included quadratic cubic or other parametric transforms austin et al 1990 elith and leathwick 2009 gams see section 4 5 for more details hastie and tibshirani 1990 are like glms but use data defined scatter plot smoothers to describe nonlinear responses elith and leathwick 2009 consequently gams have been recommended for data exploration because these models allow the data to determine the shape of the response and enable the detection of skewed or bimodal responses yee and mitchell 1991 both for vegetation mapping and conservation purposes numerical regression models such as glms austin et al 1990 and gams yee and mitchell 1991 can be used to estimate species distributions austin et al 2006 kleinbauer et al 2010 used niche based predictive modelling glm and gam to determine the degree to which the austrian natura 2000 network and a variety of other habitats with conservation interest could be vulnerable to warming leading to an increased risk of invasion by robinia pseudoacacia l fabaceae one of the most problematic iaps in europe smolik et al 2010 combined glm and gam to model the spread of iaps in austria including ambrosia artemisiifolia l asteraceae le roux et al 2013 used glms to demonstrate the implications of human activities in resource availability for 17 alien plants in prince edward islands more recently to select predictors maximizing the transferability of sdms applied to 50 holarctic iaps in north america eurasia and australia petitpierre et al 2017 combined three frequently used algorithms including glms shiferaw et al 2019 studied the performance of machine learning algorithms when mapping the cover of the iap prosopis juliflora sw dc fabaceae in a dryland ecosystem in ethiopia and found that glms did not perform as well as other algorithms 4 3 boosted regression trees boosted regression trees brt also called generalized boosted models gbm are a combination of a statistical and a machine learning algorithm developed by friedman et al 2000 it is one of several methods enhancing model efficiency by fitting and combining several models to improve forecast quality freund et al 1999 schapire 2003 one factor to highlight is its ability to deal with complex responses including nonlinearities and interactions elith et al 2008 brt combines two algorithms the regression from the cart group of models and boosting which builds and combines a collection of models breiman 2001 aertsen et al 2010 the regression trees are sequentially matched and a gradient descent algorithm is used to iteratively model the residuals that reflect the inadequacy of the prior collection of trees friedman 2001 elith et al 2008 graham et al 2008 regression trees are used because they are good at selecting significant variables while accommodating interactions wearne et al 2013 boosting is used to overcome the inaccuracies inherent to a single tree elith et al 2006 brt is one of methods that uses the previous model in the ensemble to minimize error thus boosting accuracy popp et al 2007 craig and huettmann 2008 hegel et al 2010 commonly the cart algorithm is used in brt hegel et al 2010 buston and elith 2011 by assigning a weight to each model based on the respective classification error cushman and huettmann 2010 at each iteration weights are increased on the incorrectly classified classes to focus the algorithm on these cases de ath and fabricius 2000 cushman and huettmann 2010 catford et al 2011 used brt to model the cover of alien plants in south east australia as a function of the environmental conditions using spatially referenced data coutts et al 2011 used brt to determine the most important drivers of spread for several iaps in australia geerts et al 2013 used brt for species distribution modelling including an assessment of invasiveness and options for the management of genista monspessulana l l a s johnson fabaceae and spartium junceum l fabaceae in south africa uddin et al 2013 examined the diversity traits and biogeography of 31 alien species in bangladesh s north eastern region and used brt to assess the effect of different environmental predictors protective regimes and disturbance factors in their distribution fan et al 2018 assessed the transferability of models based on environmental predictors in the study of the iap flaveria bidentis l kuntze asteraceae in china variables were categorized using the hierarchical framework available in brt 4 4 random forest random forest rf is an ensemble of unpruned classification or regression trees generated using bootstrap information samples and random selection of tree induction features breiman 2001 svetnik et al 2003 prasad et al 2006 breiman 2001 suggested the random forest approach by adding to bagging an extra layer of randomness successive trees do not rely on previous trees in bagging each is built separately by using dataset bootstrap sampling and by taking a straightforward majority vote as a forecast liaw and wiener 2002 the rf algorithm generates various cart like trees breiman et al 1984 each trained on a bootstrapped sample of the original training data and searches for a split at each node only through a randomly chosen subset of the input variables gislason et al 2006 the use of rf in sdms has proven robust and stable because it can handle abundance data as well as presence absence data hegel et al 2010 prasad et al 2006 working with pinus taeda l pinaceae acer saccharum marshall sapindaceae fagus grandifolia ehrh fagaceae and quercus alba l fagaceae in the eastern usa and rehfeldt et al 2006 working with 25 biotic communities of the western usa demonstrated the utility of rf in the large scale prediction of tree species distribution rf has been used to model the distribution of iaps such as euphorbia esula l euphorbiaceae and centaurea maculosa lam asteraceae in the usa lawrence et al 2006 of pinus sylvestris l pinaceae in the iberian peninsula gárzon et al 2006 and of larix spp pinaceae in northeastern china prasad et al 2006 leng et al 2008 using rf kleinbauer et al 2010 found that the area potentially invaded by r pseudacacia will increase considerably under a warmer climate in austria to recognize areas of high conservation value in georgia thalmann et al 2015 used the ensemble forecasting approach implemented in the biomod package including rf they aimed to identify the most prominent iaps and to predict their distribution under current and future climatic conditions chapman et al 2019 followed a novel modelling strategy including rf and other algorithms to predict the potential range expansion of iaps such as humulus scandens lour merr cannabaceae lygodium japonicum thunb sw lygodiaceae lespedeza cuneata fabaceae triadica sebifera small euphorbiaceae and cinnamomum camphora l j presl lauraceae at a global level dutra silva et al 2019 modelled the distribution of native morella faya aiton wilbur myricaceae and alien trees acacia melanoxylon r br fabaceae and pittosporum undulatum vent pittosporaceae in the azores under current and future climatic conditions rf models showed the highest values for true skill statistic tss and area under the curve auc 4 5 generalized additive models models including quadratic cubic or other parametric transforms emerged with the common occurrence of nonlinear species responses to the predictors austin et al 2006 generalized additive models gam hastie and tibshirani 1990 have been commonly used to implement non parametric smoothers in regression models yee and mitchell 1991 brown 1994 austin and meyers 1996 bio et al 1998 franklin 1998 lehman 1998 leathwick and austin 2001 glms are somewhat restrictive in their assumption of a linear relationship between response and predictors while gams establish a correlation between the mean of the response variable and a smoothed function of the predictor denoël and lehmann 2006 hegel et al 2010 this technique applies cubic or thin plate regression splines and do not assume a linear relationship hegel et al 2010 each model uses data defined scatter plot smoothers to describe nonlinear responses elith and leathwick 2009 gams have been extensively used in different applications in ecology and in model fitting see yee and mitchell 1991 they provide useful additional flexibility for fitting ecologically realistic relationships in sdms elith and leathwick 2009 for adjustment procedures the analyst s main decision is how smooth the relationship between result and predictor should be hegel et al 2010 the smoothest relationship is a simple straight line df 1 but as the response tracks the values of the predictors more closely the function becomes less smooth and the degrees of freedom rise yee and mitchell 1991 hegel et al 2010 nonetheless gams are considered as more capable of modelling complex ecological response shapes than glms and allow a better data fit although with less transparency and interpretability guisan et al 2002 wintle et al 2005 jeschke and strayer 2008 however with gaussian responses and direct gradient predictors both glms and gams perform well austin et al 2006 robertson et al 2004 predicted the potential distribution of three iaps lantana camara l verbenaceae ricinus communis l euphrobiaceae and solanum mauritianum scop solanaceae in south africa lesotho and swaziland their results suggested that in cases where pseudo absence records can be generated with confidence glm or gam may perform better than fuzzy envelope models the distribution of two widespread iaps genista monspessulana l l a s johnson and spartium junceum l fabaceae was modelled at various spatial scales in south africa with different algorithms e g gam to measure invasiveness and evaluate management options geerts et al 2013 bazzichetto et al 2018 predicted that carpobrotus sp aizoaceae would occur on the coastal dune network of the tyrrhenian using gam 4 6 bayesian models advances in computational statistics have generated new inference and prediction tools over the last decades clark 2005 the bayesian statistical approach is an important tool that is being increasingly used in ecological research and in species distribution modelling ellison 2004 clark 2005 latimer et al 2006 royle and dorazio 2008 aguilera et al 2010 kéry 2010 halstead et al 2012 gelman et al 2014 bensadoun et al 2016 in bayesian analysis emphasis is put on the distinct components and ecosystem processes leading to hierarchical models that incorporate the variance components originating from the various data levels gelman 2006 royle and dorazio 2008 tenan et al 2014 the application of bayesian inference based on bayes theorem bayes 1763 represents a different school of statistical thinking which provides an alternative way to analyse data and statistical philosophy differing from the standard frequentist statistics that most scientists are taught jeffreys 1961 berger 1985 lee 1989 press 1989 howson and urbach 1991 1993 not all ecologists appreciate the philosophy of bayesian inference ellison 2004 particularly bayesians and frequentists differ in the definition of probability and in the treatment of model parameters as random variables or as true value estimates ellison 2004 bayesian methods calculate the probability of the value of a parameter given the observed data wade 2000 in contrast frequentist statistical analysis calculates the probability of observing the data or more extreme values if the null hypothesis is true lindley 1986 wade 2000 bayesian models combine a priori probabilities of observing species or communities with their probabilities of occurrence conditional to the value of each predictor guisan and zimmermann 2000 ellison 2004 conditional probabilities can be the relative frequencies of species occurrence within discrete classes of a nominal predictor guisan and zimmermann 2000 bayesian techniques are described by posterior model probabilities and provide a way of selecting or averaging models where more complex models are penalized as in maximum likelihood model selection bolker et al 2009 modelling using bayesian inference can increase the precision of model parameter estimates once it allies previous knowledge a prior with newly collected data the likelihood to produce a posterior distribution morris et al 2015 increased accuracy of estimates from the use of informative priors is well established and improvement has been proved in ecological contexts e g mccarthy and masters 2005 mccarthy et al 2008 morris et al 2013 2015 marcot et al 2019 indeed the increase in precision is an inherent feature of using informative priors morris et al 2015 one of the complexities in bayesian data processing relies in the estimation of the model parameters posterior distribution hegel et al 2010 markov chain monte carlo mcmc techniques can however be used to estimate the final distribution of a species under scenarios of environmental change besag 1986 heikkinen and hogmander 1994 besag and kooperberg 1995 cook et al 2007 improvement of these methods has also made it possible to estimate the posterior distributions of bayesian models from flat prior distributions hegel et al 2010 a flexible approach is the use of mcmc methods with gibbs sampling to estimate the posterior distributions of model parameters casella and george 1992 gibbs sampling generates samples from the joint probability of two or more variables which is useful when the joint distribution is not explicitly known hegel et al 2010 also to sample the posterior distribution along a mcmc the specific algorithm is adapted to each situation lunn et al 2000 one of the challenges to the enhancement of sdm efficiency is to consider spatial and temporal correlations and how spatial inference about species distribution could be affected eidsvik et al 2012 however the increasing availability of geocoded scientific data and the emergence of hierarchical spatial models e g bayesian implementation have led to the acceleration of research in this area and to an increased attention from many ecologists beguin et al 2012 martins et al 2013 namely the use of gaussian random fields has recently obtained significant attention as it is ideally suited for fitting complex models based on the numerous spatial data sets that are presently available blangiardo et al 2013 illian et al 2013 the integrated nested laplace approximation inla rue et al 2009 was designed for latent gaussian models a very wide and flexible class of models including generalized linear mixed models e g spatial and spatiotemporal models that can be used in a wide variety of applications martino et al 2011 riebler et al 2012 blangiardo et al 2013 inla makes use of deterministic nested laplace approximations as an algorithm tailored to the class of latent gaussian models lgms tierney and kadane 1986 rue et al 2009 eidsvik et al 2012 in fitting these models inla calculates precise deterministic approximations to subsequent marginal distributions instead of lengthy mcmc simulations beguin et al 2012 although the requirement of specialized mathematics and programming might be apparent disadvantages bayesian models are being used more often besides bringing sophisticated model fitting abilities that can incorporate process based information latimer et al 2006 hooten et al 2007 bayesian models are able to incorporate data from multiple data sources and handle the uncertainty associated with underreporting measurement or classification error location error and incomplete datasets also being able to evaluate the impacts of different scenarios on predictions barber et al 2006 wilson et al 2008 chakraborty et al 2010 liedloff and smith 2010 powers et al 2010 herron et al 2007 employed bayesian hierarchical models to explore whether there were multiple discrete sets of invasive traits that would confer invasive success to 248 alien woody plants in a single region new england usa dawson et al 2011 followed a bayesian approach by using generalized linear mixed models with the r package mcmcglmm to study the invasiveness of 105 plant species in the united kingdom hadfield 2010 based on an extensive database of vegetation plots gonzález moreno et al 2014 used hierarchical bayesian models to measure the context dependent landscape scale correlation of weather and human activity with the rate of local plant invasion at various stages of invasion hui et al 2011 also used bayesian methods to monitor the abundance and spatial structure of 29 iaps in south africa demonstrating their importance in the design and assessment of long term management strategies link et al 2018 performed an observational study on the proliferation of seedlings in woodland plots usa invaded or not by berberis thunbergii dc berberidaceae and used survey data to calculate bayesian models the findings indicated that invaded plots harboured significantly lower native tree seedling densities with potentially significant long term forest integrity consequences to estimate the potential distribution of pittosporum undulatum vent pittosporaceae and morella faya aiton wilbur myricaceae dutra silva et al 2017a used geo referenced from pico and são miguel islands azores and topographic climatic and land use predictors fixed effects models run with maximum likelihood or the inla approach provided very similar results even when reducing the size of the dataset the addition of a gaussian random field to model spatial autocorrelation increased model adjustment lower deviance information criterion particularly for the less abundant tree m faya moreover the random field parameters were clearly affected by sample size and species distribution patterns 4 7 multivariate adaptive regression splines multivariate adaptive regression splines mars are a form of regression analysis introduced by friedman friedman 1991 sekulic and kowalski 1992 friedman and roosen 1995 put et al 2004 de cos juez et al 2009 garcía nieto et al 2012 álvarez antón et al 2013 it is a flexible non parametric technique that links explanatory and response variables in nonlinear relationships with similar levels of complexity to that of a gam hastie 1991 rigby and stasinopoulos 2005 the mars procedure builds models by fitting basis functions to distinct intervals of the independent variables jin et al 2001 generally piecewise polynomials also known as splines have smooth connections between parts schumaker 2007 garcía nieto et al 2012 this approach differs from classical splines where the knots joining points of polynomials are predetermined and spaced uniformly de andrés et al 2011 these will be denoted by the location and number of required knots in a forward backward stepwise fashion abraham and steinberg 2001 in other words a nonlinear mars feature comprises a sequence of linked straight line segments rather than the smooth curve of a gam and in several cases it has been shown that it is much faster than gam to model ecological information moisen and frescino 2002 leathwick et al 2005 a priori mars models are not designed for presence absence responses and the method has been used following indications from friedman 1991 and as implemented by leathwick et al 2005 the basic functions are extracted and fit as predictors in glm mccullagh and nelder 1989 using a binomial distribution of errors leathwick et al 2005 mars have been chosen for being a general additive boosting model with linear spline function which has proved very effective in parsimonious model selection and in modelling interactions with a performance comparable to that of neural networks muñoz and felicísimo 2004 mateo et al 2010 grenouillet et al 2011 as with gam mars is a non linear method that often better fits ecological reality austin 1999 2002 2007 stohlgren et al 2010 tested individual models including mars and ensemble models for selected alien species linaria dalmatica mill plantaginaceae carduus nutans l asteraceae bromus tectorum l poaceae melilotus officinalis pall fabaceae among other invasive species in yellowstone and grand teton national parks wyoming sequoia and kings canyon national parks california and areas of interior alaska vicente et al 2011 used mars for testing where there were conflicts between alien and rare species ruscus aculeatus l asparagaceae acacia dealbata link fabaceae and to predict their occurrence after climate and land use change in portugal in another study chen et al 2015 used mars to assess the relative importance of biogeographical and trait variables in the naturalization stage of 150 invasive and 87 non invasive herbs in china 4 8 genetic algorithm for rule set production genetic algorithms ga represent one iterative optimization strategy in the field of artificial intelligence based on evolutionary theory and inspired by genetic and evolutionary models holland 1975 they have been applied to different issues that are not suitable for traditional computational methods as the search for all possible alternatives is too large and varied to be searched for in an acceptable period of time stockwell and noble 1992 one of several current approaches to modelling species distributions is the genetic algorithm for rule set production garp stockwell and peterson 2002 garp is an iterative approach based on artificial intelligence involving several inferential tools mau crimmins et al 2006 that is a superset of modelling algorithms that allow the prediction of environmental niches through the identification of non random relationships between the environmental characteristics at presence locations as compared to the whole study region grinnell 1917 stockwell and noble 1992 stockwell 1999 mau crimmins et al 2006 hegel et al 2010 because the algorithm is stochastic it generates a random set of mathematical rules following an interactive process of rule selection by test incorporation and rejection peterson et al 2007 through which each run of the model can yield a unique solution anderson et al 2003 garp reduces the error in the predicted distributions by maximizing both significance and predictive accuracy stockwell 1999 without overfitting or overly specialized rules the algorithm is generally effective padalia et al 2014 peterson and vieglais 2001 and anderson et al 2002 provide general explanations of this type of modelling process an interpretation of the potential distribution and technical details underwood et al 2004 identified key environmental factors that were correlated with the percent cover of alien species in california and developed a predictive model using garp the potential distributions of the alien weeds brassica tournefortii gouan brassicaceae and schismus arabicus nees poaceae were predicted for the biosphere reserve el pinacate y gran desierto de altar in sonora mexico by applying garp to the repository of the geographic information system sánchez flores 2007 zhu et al 2007 used the datasets documenting eupatorium adenophorum spreng asteraceae invaded localities in china over the past 50 years and garp generated environmental variables to predict its distribution with a high degree of accuracy 4 9 classification and regression trees classification and regression trees cart date to morgan and sonquist 1963 and have received more recent attention through breiman et al 1984 it is a non parametric regression technique based on classification trees formed by a collection of rules bedia et al 2011 derived from the values of certain variables in the dataset and optimized following an efficient search process bedia et al 2011 loh 2014 cart is a powerful approach that can deal with mixed data types and do not violate assumptions of parametric statistics showing a very intuitive interpretation hegel et al 2010 fernandes et al 2013 candidate predictors require a detailed preparation but may be of any type numeric binary categorical and the model results are not affected by monotonous transformations and varying measurement scales elith et al 2008 regression trees are insensitive to outliers and provide omitted data in predictor variables by using surrogates breiman et al 1984 cart predicts unknown instances by judging which node they belong to and provides predictions based on other instance categories or response values yu et al 2015 aggregated cart models have been used by andrew and ustin 2009 to model the future distribution of lepidium latifolium l brassicaceae in the usa using separate stream length variables upland range elevation and topographical variables the density and demography of rubus phoenicolasius maxim rosaceae in the usa was investigated by gorchov et al 2011 in tree gaps of different sizes but identical in age to non gap areas cart analyses were used to identify the most important predictors fernandes et al 2013 devised a hierarchical procedure using the kruskal wallis test followed by cart to select the minimum number of optimum spectral bands that discriminated against the vegetation adjacent to arundo donax l poaceae dixon et al 2015 used cart modelling to analyse the efficacy of a single regional model in forecasting the naturalization of alien woody plants in five areas in the upper midwest usa to test the effect of environmental and socioeconomic predictors on species distribution the occurrence of 354 iaps in 45 russian regions was compiled and regression trees were built using cart vinogradova et al 2018 4 10 structural equation modelling structural equation modelling sem is a strong instrument for investigating causal relationship hypotheses among variables using observational data iriondo et al 2003 the use of sem has increased since its introduction in the 1980s by wright 1921 1934 who developed a way to break down the observed correlations into an equations scheme describing the causal relationships hypotheses iriondo et al 2003 later this method was rediscovered and developed by two economists and sociologists jöreskog 1973 1977 1981 and jöreskog and sörbom 1982 they transformed wright s original analysis into a combined factor analysis that was capable of testing rather than describing causal claims shipley 1999 frequently sem is used to confirm or disprove an a priori hypothesized causal model markus 2012 it can also be used to generate models in an exploratory sense kingsolver and schemske 1991 mitchell 1992 1994 sem is often used by its usefulness in researching complicated relationship networks but also by its commitment to represent theoretical concepts grace and pugesek 1997 grace et al 2010 sem assumes that the presence of a correlation between two variables does not necessarily imply the existence of a causal relationship between them or vice versa austin 2007 laliberté and tylianakis 2012 almeida neto et al 2010 used sem to examine the effects of invasive grass cover on native asteraceae richness by controlling specific variables grace and keeley 2006 they factored the total effect of each predictor into a direct component bangert and huntly 2010 tested three general hypotheses for the diversity and species composition of the vegetation of kipukas in the usa using sem to evaluate the ability of each hypothesis to predict the observed distributions of indigenous and non indigenous plants including the invasive grass bromus tectorum l poaceae maltez mouro et al 2010 illustrated the impact of carpobrotus edulis n e br aizoaceae on portuguese sand dune communities structure and composition they applied sem to determine the importance of alien plants and climatic variables as drivers of the sand dune community rijal et al 2017 assessed the relationship between invasibility and genetic diversity of the iap heracleum persicum desf apiaceae in 12 locations in northern norway 4 11 artificial neural networks the concept of artificial neural networks ann relates to a large group of models inspired by biological neural networks particularly the brain which consists of interconnected neuron networks for information processing lek and guégan 1999 olden and jackson 2002 ripley 2007 olden et al 2008 generally it corresponds to a set of simple nonlinear computing components whose inputs are linked together to form a network kuo et al 2001 anns are typically organized in layers with an input layer in which data is fed into the model several hidden layers and the output layer which represents model results lek and guégan 1999 olden and jackson 2002 olden et al 2008 franklin 2010 learning algorithms within anns are highly variable and include evolutionary models simulated annealing and nonparametric models hegel et al 2010 anns are a powerful tool for processing any kind of data and are used in ecological modelling with mathematical and software progress some ecological applications have already used ann for sdms and other purposes özesmi and özesmi 1999 yen et al 2004 parker allie et al 2009 addressed the impact of climate warming on the distribution of 29 invasive annual eurasian grasses using a modelling system that included several algorithms including ann they found that climate change would hamper the spread of european annual grasses in southern africa pěknicová and berchová bímovák 2016 have found that ann appeared most suitable to predict ias spread fallopia spp polygonaceae solidago spp and heracleum mantegazzianum sommier levier apiaceae in the kokorínsko protected landscape area czech republic according to shrestha and shrestha 2019 climate change amplifies plant invasion hotspots in nepal they used an ensemble of species distribution models e g ann and conducted hotspot analysis to highlight the distribution of 24 iaps in different climatic zones land cover categories ecoregions physiography types and federal states césar de sá et al 2019 modelled the potential distribution of acacia spp fabaceae in portugal using an ensemble approach biomod2 all sdms obtained very high accuracy including ann with the highest values being obtained in the models trained with researchers data data obtained from citizens alone could potentially overestimate species distributions 4 12 classification tree analysis classification tree analysis cta is a non linear statistical classification methodology that allows the study and analysis of the relationship between environmental factors and species distributions breiman et al 1984 providing an alternative to regression techniques e g vayssiéres et al 2000 thuiller et al 2003a they do not rely on a priori hypotheses about the relationship between independent and dependent variables thuiller et al 2003b according to a straightforward rule based on a single explanatory variable the tree is constructed by dividing the calibration information constantly the information being divided into two exclusive groups at each split as homogeneously as possible breiman et al 1984 thuiller et al 2003a the heterogeneity of a node is defined with a deviance notion that can be interpreted as the deviance of a multinomial model breiman et al 1984 to control the length of the tree a prune function is used venables and ripley 2002 the algorithm builds a nested series of subtrees by repeatedly snipping away the less significant divisions in terms of explained deviance thuiller et al 2003a cta is one of the few methods that can model interactive effects of two or more variables and can represent them in an easily readable fashion iverson and prasad 1998 rouget et al 2001 this modelling approach have received increased interest for biogeographical studies franklin 1998 iverson and prasad 1998 vayssières et al 2000 rouget et al 2001 as compared to other modelling techniques such as glm and gam thuiller et al 2003a to construct a bayesian model murray et al 2012 used a cta of the field data to predict the potential distribution of a riparian invasive plant phyla canescens kunth greene verbenaceae in australia another study tested the effect of the addition of variable sets to an sdm for heracleum mantegazzianum s l apiaceae in poland mędrzycki et al 2017 however the cta derived model quality was low with single models showing a higher level of variation in true skill statistic than ensemble models 4 13 ecological niche factor analysis perrin 1984 was the first to suggest the term ecological niche factor analysis enfa later developed by hausser 1995 and more recently implemented in the software biomapper hirzel et al 2000 2002 the enfa is a method based on a comparison between the environmental niche of the species and the environmental characteristics of the entire study area hirzel et al 2002 engler et al 2004 like the environmental envelope approach it presents the advantage of requiring presence data only i e no absences are required and a set of background gis predictors engler et al 2004 shirley et al 2013 enfa is like a kind of principal component analysis pca and summarizes all ecogeographical predictors into a few uncorrelated factors retaining most of the information cassinello et al 2004 2006 hengl et al 2009 moreover the extracted factors have an ecological meaning the first extracted factor provides the marginality coefficient describing the standardized difference between the average conditions at species sites and at the entire study area marginality ranges from 1 to 1 and indicates the rarity of the conditions selected by the target species within the study area chefaoui et al 2005 hortal et al 2005 acevedo et al 2007 positive values show a species optimum to be higher than the average conditions in the study area the second factor specialization varying from 0 to compares the variance of the environment in areas where the species is present with the global variance in the study area cassinello et al 2006 successive factors explain the remaining specialization in decreasing amounts a high value of specialization indicates a narrow niche breadth in comparison with the available conditions hirzel et al 2002 2004 2006 these extracted factors are used to compute a habitat suitability index for any set of predictor values hirzel et al 2002 enfa calculates a measure of habitat suitability based on the analyses of marginality how the species mean differs from the global mean and environmental tolerance how the species variance compares to the global variance hirzel et al 2000 2001 a habitat suitability map with values ranging from 0 to 100 is built comparing the position of each cell in the study area to the distribution of presence cells on the different factorial axes hirzel et al 2002 2007 hirzel et al 2001 compared two habitat suitability assessing methods enfa and glm to see how well they coped with three different scenarios the enfa method was preferred but the exact procedure used for the glm is difficult to determine austin et al 2006 hortal et al 2010 identified the areas under potential threat from the future spread of pittosporum undulatum vent in são miguel island azores using enfa to investigate the impact of geographical bias on the performance of ecological niche models for iaps wolmarans et al 2010 used enfa to compare the ecological niche occupied by 19 iaps in their native and introduced ranges in south africa and australia steiner et al 2008 costa et al 2012 used enfa to determine whether and where areas currently occupied by p undulatum might also be valuable habitat for m faya thus promoting potential management measures priyanka and joshi 2013 modelled the spatial distribution of l camara in india but of the statistical methods used enfa produced an odd pattern of under predictive models from a global multidimensional thermal niche viewpoint two alien grasses poa annua l and poa pratensis l poaceae were studied to evaluate the threat of biological invasion in antarctica pertierra et al 2017 enfa was performed to estimate the biological relevance of temperature related predictors hirzel et al 2002 dutra silva et al 2017b modelled the distribution of indigenous m faya and invasive woody species a melanoxylon and p undulatum in the azores applying enfa and maxent algorithms which generated similar predictions 4 14 surface range envelope surface range envelope sre is an envelope style technique that takes into consideration the entire range of environmental circumstances in which the species is present busby 1991 the envelope is defined by the minimum and maximum values for each predictor from the set of presences jiguet et al 2011 araújo and peterson 2012 thuiller et al 2012 any location with environmental conditions falling between these minimum and maximum limits is included in the potential range of the target species jiguet et al 2011 thuiller et al 2012 once the bioclimatic envelope of the species is recognized the model can be used to predict its distribution in other areas or climatic scenarios araújo and peterson 2012 this method is influenced by the data input and more specifically by the extremes thuiller et al 2012 to avoid the over predictive effects of outliers the envelope can be reduced at specified percentiles or standard deviations araújo and peterson 2012 thuiller et al 2012 the sre assumes that a linear model can distinguish extreme values from the primary trend as a result of which the predictions are provided directly in binary notation a site being classified as possibly suitable for all the variables araújo and peterson 2012 thuiller et al 2012 booth et al 2014 however treating all presence points as equal and trying to not discriminate the extreme values has been criticized by beaumont et al 2005 le maitre et al 2008 developed a method for describing the possible distribution of hakea proteaceae species in south africa wit the sre model which simply used the upper and lower limits of each of the weather variables in the presence records to establish a multidimensional framework melampyrum pratense l orobanchaceae was investigated in iceland to decide whether the species was a recent anthropogenic addition to the icelandic flora or whether its existence should be due to natural long distance dispersal activity wasowicz et al 2018 pseudo absences were randomly selected inside the geographical range of the species using the sre method le maitre et al 2008 barbet massin et al 2012 4 15 quantile regression quantile regression qr gradually emerged as an extensive strategy for the statistical analysis of linear and nonlinear response models in econometrics yu and moyeed 2001 however this technique has not received much attention in applications to sdms koenker 2005 austin 2007 hegel et al 2010 qr provides solid estimates of the impact of the predictors in cases where various limiting factors may interact some possibly unmeasured cade et al 1999 it assumes relationships between observations and predictors in a specific way over different ranges i e quantiles for predictors cade and noon 2003 cade et al 2005 bartomeus et al 2010 2012 hegel et al 2010 these semi parametric models estimate unique parameters to a response across different quantiles of data hegel et al 2010 having specific fits for specific data sections can be equated with other approaches such as mars muñoz and felicísimo 2004 hegel et al 2010 vaz et al 2008 used qr to calculate the upper boundaries of organism environment relationships and to identify those factors that limited species distribution hegel et al 2010 cade et al 1999 introduced the use of qr to estimate the curves of the envelope or factor ceiling reactions thomson et al 1996 in 2003 cade and noon provided an exposition of this statistical method and of its potential in the ecological analysis of observation data both for plants and animals austin 2007 qr can be useful in the study of iaps as most still have the potential to expand their distribution range guisan and thuiller 2005 ricotta et al 2010 analysed the relationship between indigenous and non indigenous plants in brussels with qr koenker and bassett 1978 koenker and hallock 2001 zedda et al 2010 studied the impact of alien plants and human disturbance on soil growing bryophyte and lichen diversity in coastal areas of sardinia italy qr was selected to test the comparisons between the cover of carpobrotus spp and of alien trees versus the cover and species diversity of bryophytes and lichens found in the parcels petty et al 2012 used a novel spatial autocorrelation qr technique to compare factors that influenced andropogon gayanus kunth poaceae abundance and distribution at two large scale invasion sites in australia s tropical savanna region 4 16 generalized dissimilarity modelling biodiversity spatial structure can be defined and explained by the evaluation of species composition turnover using beta diversity modelling approaches warren et al 2014 numerous studies have investigated the differences between the biotic assemblages found at different locations to develop predictive models for application in biodiversity and macroecology ferrier et al 2007 anderson et al 2011 initially generalized dissimilarity modelling gdm was developed to study turnover in species diversity in community ecology ferrier et al 2002 2007 hegel et al 2010 compositional turnover can be partitioned into components explained by environment location or both legendre et al 2005 gdm follows a generalized linear approach using an exponential link function to model the compositional dissimilarity between pairs of biological studies based on the bray curtis index as a function of environmental predictors ferrier and guisan 2006 ferrier et al 2007 overton et al 2009 it uses a matrix regression approach and models non linear relationships by fitting a linear combination of spline basis functions to the environmental variables in a linear predictor hegel et al 2010 elith et al 2006 extended this approach beyond modelling species diversity to modelling species distribution for this a kernel regression is used on the output of the transformed environmental variables from the gdm to predict the likelihood of species occurrence hegel et al 2010 data can be handled by using presence pseudo absence however this should be considered prudently ferrier et al 2007 jones et al 2013 used gdm to model the compositional turnover of trees 486 species and ferns 92 species in the drainage of the panama canal because of soil chemistry weather and regional separation vicente et al 2014 used this new framework of dissimilarity to examine and explain spatial dynamics of species diversity associated with iaps in northern portugal assessing compositional dissimilarity patterns between sites or regions using gdm as a tool has been suggested as a promising complementary approach to traditional correlative analysis ferrier et al 2007 vicente et al 2014 4 17 geographically weighted regression in spatial analysis one of the primary purposes is to define the nature of the interactions between distinct variables brundson et al 1996 traditional statistical methods can only produce average and global parameter estimators consequently they are unable to deal with the spatial autocorrelation existing in the variables gao and li 2011 however a comparatively easy method called geographically weighted regression gwr has recently been suggested to explore variable relationships spatially wheeler 2014 the method has been developed by brundson et al 2001 and fotheringham et al 2001 and proposed a refinement to normal regression methods dealing with the spatial non stationarity of empirical relationships fotheringham et al 2003 wang et al 2005 gwr is thus based on the appealing concept of a locally weighted regression that works by estimating local curve fitting models using subsets of focal point centred observations lü and fu 2001 gao and li 2011 this approach is an extension of traditional standard regression techniques such as ols ordinary least squares because it allows local rather than global parameter estimates fotheringham et al 2001 austin 2007 these can help reveal how a relationship varies over space to examine the spatial pattern of the local estimates and to get some understanding of the hidden possible causes of this pattern fotheringham et al 2003 it can help reveal spatial variations in the empirical relationships between variables that would otherwise be ignored in the overall analysis charlton et al 2009 despite some controversy over the relative importance of gwr versus global spatial regression see jetz and rahbek 2002 versus foody 2004 gwr has been widely applied in diverse fields such as ecology forestry geography and regional science wheeler 2014 an example of this approach was the study of urbanization that promoted the spread of 411 alien woody species and the establishment of diverse plant assemblages in the new york metropolitan region aronson et al 2015 gwr was used to detect non stationarity between the proportions of alien species in the flora to the proportion of urban land cover in the landscape latimer et al 2009 also used this methodology to model the distribution of the iap celastrus orbiculatus thunb celastraceae in the north eastern usa 4 18 ensemble modelling given the variability in results arising from the application of different algorithms the complementary use of several methods has been recommended e g ensemble modelling araújo et al 2005 araújo and new 2007 within a consensus modelling framework marmion et al 2009 grenouillet et al 2011 the principle of integrating predictions from various models into an ensemble has gained substantial notoriety hao et al 2019 many users cite the superior predictive performance of ensembles over individual models e g crossman and bass 2008 marmion et al 2009 grenouillet et al 2011 ensemble modelling is a method in which many different models are generated either by using several different modelling algorithms or different training data sets to predict an outcome kotu and deshpande 2014 the reason for using ensemble models is to decrease the forecast generalization error if the base models are diverse and distinct when the ensemble method is used the estimation error of the model decreases kotu and deshpande 2014 grenouillet et al 2011 investigated the utility of approaches to ensemble modelling and tested whether species characteristics affected the precision of those approaches their results illustrate the utility of ensemble models for the detection of geographical areas of consensus between forecasts and they considered that the application of a single modelling technique should be avoided especially for species with wide environmental ranges grenouillet et al 2011 however it has also been argued that individual models might perform better than ensemble models e g crimmins et al 2013 hao et al 2019 reported the breadth of ensemble applications and provided guidelines for comprehensive reporting practices in a biomod based ensemble workflow however they could not draw clear quantitative conclusions about a putative superiority in predictive performance of ensemble models di napoli et al 2020 tested ensembles of artificial neural networks generalized boosting and maximum entropy algorithms and found that ensemble modelling improved reliability originating higher scores and lower variation hao et al 2020 aimed to complement the existing knowledge on performance of ensemble models versus individual models and investigated how spatial blocking affected the understanding of model performance using a presence absence dataset they found that ensemble models performed slightly better than untuned individual models in most situations but not consistently better than tuned individual models on external validation therefore work is still in progress regarding the effective use of ensemble modelling 4 19 overview of modelling approaches as suggested in previous reviews austin et al 2006 austin 2007 our results showed that when modelling the environmental niche or the geographic distribution of iaps currently there is no general criterion for the selection of specific algorithms other aspects such as the ecological knowledge and statistical skills of the analyst might have affected the choices made by researchers more than a decade ago austin 2007 claimed that three potential areas of development included qr sem and gwr and that the methods with more predictive success were brt gdm and maxent followed by mars glm gam and garp however we did not confirm a growing use of qr sem and gwr probably because machine learning techniques are still gaining more attention in the area of sdms although their use is expected to increase in the forthcoming years humphries et al 2018 in the same sense brt and gdm have not become the dominant algorithms used for sdms although brt ranked as third in our analysis our findings showed that the annual number of articles addressing sdms developed for iaps and the number of cited references have both increased considerably from the late 1990 s up to the present this trend can be well explained by the development of computing technologies e g bennett and bierema 2010 michener et al 2012 tarng et al 2015 wang et al 2017 marcot and penman 2019 zellweger et al 2019 personalized for the advancement of modelling and by the diffusion of multiple analysis platforms particularly those based on r packages e g dismo biomod2 inla which are freely distributed with numerous available tutorials and discussion forums simultaneously the interest in research devoted to ias has also consistently increased see van wilgen et al 2012 early et al 2016 ramírez albores et al 2019 shackleton et al 2019 rai and kim 2020 many of the identified studies utilized maxent for model development this has been considered as a good modelling practice which has gained considerable attention however this practice does not fully address other prevailing modelling issues such as the use of pseudo absences elith et al 2011 guillera arroita et al 2014 and the difficulties when comparing output with other algorithms as maxent output provides environmental suitability rather than predicted probability of occurrence merow et al 2013 moreover many authors such as qiao et al 2015 concluded that niche or distribution modelling studies should begin by testing a suite of algorithms for predictive ability under the circumstances of each study case recently fletcher et al 2016 koshkina et al 2017 and schank et al 2017 have suggested the use of point process models as a way of integrating presence only and presence absence data meanwhile researchers in ecology could eventually broaden the scope of the methods applied for statistical modelling and prediction including applications focused in machine learning examples include mars with its ability for handling interactions between predictors moisen and frescino 2002 muñoz and felicísimo 2004 balshi et al 2009 zhang and goh 2013 kisi and parmar 2016 huang et al 2019 cart which is one of the most successful decision tree methods liu et al 2013 dixon et al 2015 chen et al 2017 gayen and pourghasemi 2019 brt that combines two algorithms elith et al 2008 stokland et al 2011 martin et al 2014 pourtaghi et al 2016 wu et al 2019 genetic algorithms like garp stockwell and noble 1992 stockwell 1999 yu and he 2009 sobek swant et al 2012 li et al 2017 moghaddam et al 2020 and gwr brundson et al 2001 fotheringham et al 2001 partridge et al 2008 propastin 2012 tenerelli et al 2016 yang et al 2019 although the use of those algorithms in forecasting the invasion success of iaps has been clearly established pyšek et al 2009 keller et al 2011 research by keller et al 2011 revealed that model quality from machine learning algorithms was not consistently superior to that of conventional statistical methods for a large dataset of up to 87 species nevertheless numerous applications of machine learning methods to ecological data have showed successful results e g dutra silva et al 2017b brodrick et al 2019 pichler et al 2020 therefore more work should be devoted to the comparison of results obtained with different and complementary approaches another finding was the geographical diversity regarding the use of different types of modelling techniques to predict the distribution of iaps in various parts of the world the interest in the study of biological invasions has substantially increased during the first decade of the 21st century and this trend continues today ramírez albores et al 2019 therefore the number of references has increased exponentially over time see ramírez albores et al 2019 shackleton et al 2019 rai and kim 2020 showing that iap studies may still be considered as an emerging area justifying the pronounced diversification of modelling approaches found in our research our results concerning the global distribution of sdm studies might be explained by the geographic occurrence of biological invasions and by the historical interest that this subject has stimulated in some regions thus according to tuberlin et al 2017 ias mainly occur in the global north and in some newly industrialized countries e g china india brazil another stusy revealed the engagement of stakeholders in the study and management of ias particularly in south africa usa australia canada india spain and the uk see shackleton et al 2019 these regions largely coincide with the areas that showed more diversified approaches to sdms applied to iaps in our research 5 conclusion our study presented an overview of the conceptual framework supporting the application of sdms a structured literature search combined with bibliometric analysis as well as the characteristics of the main modelling techniques and examples of their application to the study of iaps in the last decades the interest in sdms has increased dramatically as reflected by the number of publications this can be explained by the increased availability of software packages enabling modellers to create and train sdms with more extensive applications across different regions of the world considering the diverse array of modelling algorithms that have been used along the studied period this overview led to a crucial conclusion it is clear from the articles quoted that when modelling the environmental niche or geographic distribution of iaps presently there is no criterion for best practice in the selection of the modelling algorithms some general guidelines for improving predictions of sdms should include i the use of different algorithms to see if all perform consistently or if certain approaches provide more reliable results for instance maxent has been extensively used in many cases without a comparison with another modelling approach but has been shown to perform equally or below other algorithms in several cases ii the need to compare the results obtained using individual and ensemble models since this is an area still in development and involving a considerable uncertainty there are inconsistent reports favouring one approach or the other iii the use of several complementary parameters to evaluate model fit and prediction quality including tss auc boyce index and deviance measures such as aic waic or dic when possible it is common to get relatively high auc values even when tss is relatively small or when the analysis of the boyce index and curve suggests low quality modelling iv the use of different combinations of predictor variables since using methods such as variation inflation factor vif alone in many cases might preclude finding the most suitable model for instance using a combination of vif and pca to select predictors might provide more reliable models and the use ecologically and biophysically meaningful predictors v spatial autocorrelation is probably present in many systems and together with variation in sample size and with the specific distribution pattern of the species might have a decisive influence on model performance indeed the inclusion of a random gaussian field in mixed generalized linear models has showed the importance of autocorrelation and distributions patterns are often dependent on predictors such has land cover which largely results from human action and not directly from biophysical predictors vi the implementation of recent approaches with a very high potential in the developmental of geospatial models algorithms such as bayesian models calculated using inla have shown to be very powerful in modelling varied geospatial phenomena but are still seldom used in sdms most likely due to their complex implementation meanwhile sdms are important tools in applied ecology and in the recent years have revealed a tremendous progress in many aspects and applications with the rich diversity of biological and environmental settings philosophical and analytical approaches and research initiatives addressing many aspects and applications nevertheless improvement requires constant evaluation and future efforts should be focused on consolidating modelling frameworks the comparison of different modelling approaches is a challenge that should not be ignored by modellers since new methods methodologies and technologies are constantly being implemented in this field finally iaps constitute an important continuously growing and significant part of the global vegetation composition around the world this work highlights the various algorithms available for better understanding the conditions that promote the expansion of iaps and that could help in the design of more successful management strategies declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we are grateful to two anonymous reviewers who provided constructive remarks that allowed to improve the manuscript quality we also appreciated funding from project forest eco2 towards an ecological and economic valorisation of the azorean forest acores 01 0145 feder 000014 azores 2020 po 2016 2019 feder european regional development fund funds through the operational programme for competitiveness factors compete and by national funds through fct foundation for science and technology under the uid bia 50027 2019 and poci 01 0145 feder 006821 drct m1 1 a 005 funcionamento c 2019 cibio a secretaria regional do mar ciência e tecnologia governo dos açores appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105203 
25734,in the last decades the number of publications dedicated to the application of species distribution models sdms to invasive alien plants iaps has constantly increased although recent reviews have addressed very relevant issues in the application of sdms the modelling approaches i e algorithms applied to iaps have not been systematized therefore we undertook a bibliographic review of articles devoted to sdms and iaps from 1996 to 2019 our results indicate that maximum entropy generalized linear models boosted regression trees and random forest were the four most frequent types of modelling approaches it was clear that there was a variety of different approaches regarding the type of algorithm to be used we discuss the characteristics of the most cited algorithms providing examples of their application in sdms dedicated to iaps we advocate the use of a combination of different algorithms an intensive evaluation of predictors a thorough validation process and a critical analysis of model predictions keywords biotic data ecological modelling invasive alien plants iaps modelling approaches predictions species distribution models sdms 1 introduction 1 1 sdms and ecological niche the development ecology was largely influenced by gause s competitive exclusion principle and hutchinson s and macarthur s works on the ecological niche and species packing gause and witt 1935 andrewartha and birch 1953 hutchinson 1957 macarthur 1957 bruno et al 2003 ecological theory indicates that species show a unimodal reaction to the source of constraint in an ecological n dimensional space whittaker 1975 austin 1985 ter braak 1986 the volume of this ecological space in which an organism can survive and reproduce defines its environmental niche hegel et al 2010 a concept was originally defined by grinnell 1917 and elton 1927 hutchinson 1957 defined the realized niche as a subset of the fundamental niche limited by biotic interactions such as the presence of competitors or the lack of mutualists pulliam 2000 revised that definition to allow for source sink theory and dispersal limitations thus sink populations could allow the realized niche to be larger than the fundamental niche while restrictions on dispersal and previous disturbance could restrict the realized niche beyond the impacts of biotic interactions the differences between the two visions may be relevant to invasive species principally when considering the realized niche in native ranges versus the potential global niche elith et al 2006 many species distribution models sdms are based on the idea that the niche is an n dimensional hypervolume where axes represent the n resources that limit the fitness of an organism hegel et al 2010 the dimensions of the ecological niche are limited to that subset of all possible conditions that directly affect the organism s fitness kearney 2006 therefore the first step to predict the distribution occurrence or abundance of a species is the quantification of its niche space 1 2 environmental predictors a key challenge to predict habitat suitability and occurrence in complex landscapes is to link the non spatial niche relationships with the complex patterns representing the way how heterogeneous landscapes overlap the environmental gradients cushman et al 2007 hossard and chopin 2019 ecophysiology and biophysics can be used to select variables that might describe and define the target species distributions huntley et al 1995 guisan and zimmermann 2000 austin 2005 in practice modellers often focus on species response to climate however this might not be the essential or most relevant factor for some species and spatial extents hulme 2003 spatial and temporal complexity require a mechanistic understanding of the key drivers that limit species distributions and abundances including the spatial scales at which they operate and their temporal gaps and effects cushman et al 2007 a careful selection of predictors utilizing the knowledge gained from physiology and the study of environmental processes should be considered to improve the interpretability and quality of statistical models austin 2007 nevertheless identifying the factors affecting species distribution is an unresolved ecological issue araújo and guisan 2006 there are often many combinations of environmental factors that can explain species distributions particularly when correlated leading to an uncertainty about the impact of each factor when taken individually platts et al 2008 murray and conner 2009 ashcroft et al 2011 sdm researchers have used a wide variety of predictors driven by their availability and the conviction that the model will identify the most relevant variables elith and leathwick 2009 as necessary guidelines for the definition of biologically informative and generalizable sdms authors have stressed the use ecologically and biophysically meaningful predictors and the consideration of their direct and indirect impacts on species distributions austin 2007 newbold 2010 bucklin et al 2015 climate is a dominant driver of species distributions and recent climate change became a widely acknowledged global change factor pearson and dawson 2003 lebreton 2011 ranjitkar et al 2016 however climate only predictors should be considered as incomplete representations of complex environmental systems because other factors may affect species distributions heikkinen et al 2006 beale et al 2008 araújo and peterson 2012 bucklin et al 2015 and should be combined with climate variables austin and van niel 2011 including land cover and human influence e g blach overgaard et al 2010 costa et al 2012 2013 variables such as altitude and latitude have a relationship with the organisms presence through their correlation with climate austin 2007 nonetheless the correlation between indirect and direct factors is location specific and might not be linear complicating the form of the species responses austin 2007 some studies have shown that potential errors in predictors can have profound effects on model outcomes e g van neil et al 2004 and that the accuracy of predictions depends on the selected variables thus clear justifications for their use should be provided e g stockwell and peterson 2002 austin and van niel 2011 predictors have been often selected based on their correlation with the target species distribution guisan and thuiller 2005 syphard and franklin 2009 therefore presuming that the identified relationship remains unchanged the future distribution can be projected over different environmental conditions either found in the future or at other locations harris et al 2013 however if the variables entering the model are not causally linked to the species distribution predictive models can be incorrect when transferred to other scenarios varela et al 2011 another key factor is the selection of a minimum number of feasible predictors since as more variables are included in the model the narrower its predictions will be bullock et al 2000 mac nally 2000 burnham and anderson 2004 beaumont et al 2005 sobéron 2010 trabucco et al 2010 braunisch et al 2013 cauwer et al 2014 moreover other reasons might justify a reduction in the number of predictors including minimization of computation time avoidance of overfitting increase in transferability improvement in the understanding of the causal relationships guisan and thuiller 2005 barbet massin et al 2010 austin and van niel 2011 braunisch et al 2013 petitpierre et al 2017 1 3 species data the nature of the dependent biotic variable is crucial to model structure austin 2007 meier et al 2010 with three types of biotic information generally regarded austin 2007 wisz et al 2013 abundance measurements presence absence data and presence only data identifying the presence of a species or individual at a location is relatively straightforward however the identification or confirmation of its absence is much more complicated mackenzie 2005 absence data are viewed as misleading because the species might not be at equilibrium with the environment or might be difficult to detect václavík and meentemeyer 2009 elith et al 2010 hegel et al 2010 the drawbacks of presence only data have been widely debated and include the possibility of failure to identify presences due to detection bias the overrepresentation of rare species and underrepresentation of common species and the erroneous application of statistical methods relying on the use of true absences zaniewski et al 2002 brotons et al 2004 elith et al 2006 elith and graham 2009 guillera arroita et al 2015 one approach to deal with these situations is to use pseudo absences representing the habitat available for an organism elith et al 2006 williams et al 2009 abundance based on the frequency of reporting rates or on an estimate of true population size harrison and cherry 1997 renwick et al 2011 howard et al 2014 may be considered as an indicator of habitat quality pearce and ferrier 2001 most research has focused on modelling presence absence or abundance data hirzel and guisan 2002 cawsey et al 2002 the former has been used to estimate the probability of occupancy for a sample location corrected for imperfect detection mackenzie et al 2006 maher et al 2014 ramsey et al 2015 and has numerous applications mcpherson et al 2004 allouche et al 2006 guisan et al 2006 foody 2011 variable evaluation prioritization of sites for conservation monitoring of species decline or of range expansion nevertheless there has been an increasing interest in the use of presence only data pearce and boyce 2006 gathered from biological collections and utilized for conservation invasive species and climate change modelling graham et al 2004 this has often been justified by the lack of systematic survey data linked to a pervasive demand for mapped predictions elith et al 2006 2011 gormley et al 2011 as a consequence a wide discussion opposing the use of presence only and presence absence data has ensued e g soberón and peterson 2005 hirzel and le lay 2008 jiménez valverde et al 2008 soberón and nakamura 2009 lobo et al 2010 with variable attitudes towards presence only data elith and leathwick 2009 elith et al 2011 bird et al 2014 presence absence data for individual species such as from quadrat data elith and franklin 2013 have the advantage of providing information on what has been surveyed however the meaning of an absence can be unclear and can vary with the spatiotemporal resolution of sampling green 1971 pulliam 2000 hirzel et al 2001 hirzel and guisan 2002 le maitre et al 2008 kéry et al 2010 elith and franklin 2013 unaccounted abiotic or biotic factors e g type of substrate soil ph absence of dispersers presence of predators could have prevented successful establishment the species has not been able to reach that locality yet or it was not detected 1 4 sdms and biological invasions it has been shown that the successful application of models and the possibility of determining a significant effect is largely dependent on the goal and also that a careful consideration in model selection is required if a long term forecasting applicability is to be achieved zenni et al 2014 tredennick et al 2016 makridakis et al 2018 thus despite the limitations of sdms their use in the design of reliable biodiversity conservation strategies management strategies and in decision making has been largely accepted thuiller et al 2008 ausseil et al 2019 eker et al 2019 srivastava et al 2019 in recent years there has been a strong drive by modellers to adopt predictive tools that would support measures to reduce the impact of biological invasions research devoted to invasive alien species ias using open source approaches to enhance model development has facilitated the use of statistical algorithms by a broader subset of the ecosystem modelling community a better understanding of these tools enabled the conceptualization of practical applications however although modellers are now able to explain how and why many biological invasions have occurred we are still unable to fully forecast ias expansion zenni et al 2014 nonetheless ias have been widely recognized as a major environmental problem at global and local scales zedda et al 2010 posing serious threats to ecosystem structure diversity and function olden et al 2008 ehrenfeld 2010 hernández et al 2014 plant invasions can have powerful effects on ecosystems and human welfare pimentel et al 2005 pejchar and mooney 2009 van kleunen et al 2010 and negatively affect indigenous plant populations by becoming either monopolisers or resource limiting donors or by changing soil stability increasing erosion colonizing open substrates changing litter accumulation or other soil traits vitousek 1990 richardson et al 2000 brooks et al 2004 presently invasive alien plants iaps constitute a relevant component of the vegetation in urban ecosystems throughout the globe alston and richardson 2006 lima et al 2013 kendal et al 2014 although much of the global research has been dedicated to invasive plants in forested and agricultural landscapes woody plant invasions have been associated with a variety of socio ecological factors including land use change binggelli 1996 binggeli et al 1998 richardson 1998 mack et al 2000 richardson and rejmánek 2004 2011 silva and smith 2004 2006 williams and cameron 2006 silva et al 2008 costa et al 2012 2013 staudhammer et al 2015 a large body of literature has aimed to identify and classify invasive species determinants levine et al 2003 van kleunen et al 2010 causes of invasiveness are an important research topic in ecology and invasion biology and effective management is facilitated when the mechanisms of invasion and their ecological impacts are understood lowry et al 2013 studies about woody plant invaders have shed light on several aspects such as functional traits related with physiology biomass allocation growth rate size and fitness kolar and lodge 2001 grotkopp et al 2002 2004 petit et al 2004 van kleunen and richardson 2007 however there has been a considerable debate about the traits that could potentially increase invasiveness i e a high probability of successful establishment since the characteristics of the invaded ecosystem are also relevant crawley 1987 roy 1990 rejmánek et al 2005 leishman et al 2007 pyšek and richardson 2008 mason et al 2008 gordon et al 2010 richardson and rejmánek 2011 a relevant component of ecological research has been directed towards a practical knowledge about the distribution of threatened species their habitat requirement and to the assessment of the impacts by ias alpert et al 2000 d antonio and meyerson 2002 levin et al 2003 engler et al 2004 silva et al 2015 understanding the factors associated with ias expansion is the first step towards the development of effective control measures minimizing negative environmental impacts that often rely on habitat suitability modelling and on decision analysis le lay et al 2001 hirzel et al 2001 cacho et al 2010 tonini et al 2017 fortunately the fast development of statistical techniques and computing power is facilitating the design of tools that can support pro active strategies to avoid the introduction or to guide screening to impede the establishment of new ias guisan and zimmermann 2000 peterson 2003 drake and lodge 2006 roura pascual et al 2009 within this framework sdms have been increasingly used to predict ias distribution and to better understand the drivers underlaying biological invasions elith et al 2010 2011 elith and franklin 2013 the successes of these modelling tools have allowed to correlate ias distribution with environmental variables both in their native range and in the invaded areas thereby facilitating early detection of new invasions and maximizing the success of risk assessment protocols richardson et al 2000 peterson 2003 elith et al 2006 hulme 2006 bradley 2013 1 5 why a literature review a wide variety of statistical and machine learning methods has been introduced to highlight unresolved issues in sdms austin 2002 2007 guisan and thuiller 2005 elith and graham 2009 elith and leathwick 2009 meanwhile the large number of different approaches and techniques has raised several methodological questions i the implementation of new statistical methods phillips et al 2006 elith et al 2008 ii the evaluation of sampling design on model performance edwards et al 2006 guisan et al 2006 iii the evaluation of sample size on model accuracy zimmermann et al 2007 wisz et al 2008 iv the removal of spatial autocorrelation from model fitting dormann et al 2007 zimmermann et al 2010 v the comparison of statistical results for different models elith et al 2006 and vi general model evaluation procedures allouche et al 2006 smulders et al 2010 in summary many efforts have been made to improve the statistical basis of the application of sdms as shown by the large number of published articles over the last decades zimmermann et al 2010 araújo et al 2019 there have been continuous advances in species distribution modelling and in computation power as well as a growing need to better understand and predict biological invasions particularly in the case of iaps with their possible association with climate change moreover despite the existence of recent reviews and general guidelines e g araújo et al 2019 zurell et al 2020 it is not clear if general trends exist in the use of different modelling approaches i e algorithms along time therefore we think that a literature review analysing a broad compilation of scientific literature devoted to iap distribution modelling and addressing the different modelling approaches applied in sdms would be relevant in fact a new researcher in the field facing the wide diversity of modelling algorithms might wonder about the way to proceed our aim was to provide a comprehensive literature review about the current state of the art considering the different algorithms starting with a structured literature search combined with a bibliometric analysis we highlight the more cited algorithms we present an overview of the main characteristics advantages and limitations of each modelling approach and we conclude with examples of their application to the study of iaps finally we present suggestions for general guidelines when applying sdms to iaps fig 1 illustrates the flowchart used to structure the proposed analysis 2 methods to produce a knowledge synthesis we used a systematic search of literature together with a bibliometric approach in order to identify the literature s substance as well as its context and processes e g analytical and focused trends janssen 2007 tricco et al 2010 all documents used in this research were based on two online academic citations indexes for all available years google scholar and web of science the datasets were accessed in the period between march 2019 and march 2020 we used the following terms and their combinations as keywords to search titles part of titles abstracts or keywords invasive plant s alien plant s invader plant s exotic plant s non native plant s non indigenous plant s woody invasive plant woody alien plant s woody exotic plant s detection habitat habitat suitability invasions invasiveness land management potential distribution predicting predictive species distribution generalized linear models or glm generalized additive models or gam ecological niche factor analysis or enfa surface range envelope or sre quantile regression or qr structural equation modelling or sem geographically weighted regression or gwr multivariate adaptive regression splines or mars artificial neural networks or ann classification and regression trees or cart classification tree analysis or cta boosted regression trees or brt random forest or rf bayesian analysis and approach bayesian methods generalized dissimilarity modelling or gdm entropy maximization maxent and genetic algorithm for rule set production or garp an example used for a combination of search terms was alien plant potential distribution glm after searching the literature related with iaps and sdms we reviewed the title and abstract of each article and rejected any articles that did not meet our targets we organized and classified articles based on the following classification scheme i type of modelling algorithm ii taxonomic group plant species iii year of publication iv number of citations v journal vi journal impact factor if and vii geographical location of the study area the journal if was determined for each article as reported in the year 2018 which was the latest data available in this study we only discuss the articles published in the period beginning in 1996 since this was the earliest date found in our research 3 results 3 1 bibliometric analysis and basic statistics fig 2 displays the number of publications covering iaps and sdms between 1996 and 2019 the total number of articles was 526 corresponding to 22699 citations table 1 the annual number of articles the number of cited references and their percentage increased considerably along time when considering the total number of citations for the period the number of citations per year of publication and per article varied over the years from 63 to a maximum of 4024 table 1 the earliest article was published in 1996 higgins and richardson 1996 with no publications in 1998 2001 and 2002 only 5 articles published before 2003 and with 521 publications from 2003 to 2019 fig 2 a total of 123 journals were involved appendix s1 see table 2 with the top 20 journals biological invasions ranked first with 71 articles 13 5 and an impact factor if of 2 897 diversity and distributions if 4 092 ranked second with 45 articles 8 6 plos one if 2 776 ranked third with 19 articles 3 6 global change biology with 7 articles 1 331 should be noted because of its high if 8 880 table 3 shows the frequency use of each of 17 algorithms including the number of articles and the corresponding percentage between 1996 and 2019 maxent 21 5 glm 15 0 brt 10 3 and rf 8 7 were the four most frequent types of algorithms although those percentages fluctuated maxent has consistently become more frequently used fig 3 shows the evolution of the percentage of articles and citations along time reflecting the diversity of modelling techniques applied to iaps the first article recognized was in the field of glm in 1996 in that early period most articles and citations demonstrated the use of a low variety of algorithms however starting in 2004 2007 there was an increase in the variety of algorithms applied to iaps which was accentuated in more recent periods fig 3 3 2 overview of the geographical distribution of sdm use we should point out a high incidence of studies in europe 301 asia 165 north america 119 africa 82 oceania 62 and south america 25 there was a wide geographical variation regarding the use of the 17 types of modelling techniques see supplementary fig 1 amongst the geographic regions usa spain and austria received much attention having more case studies that applied maxent bm glm brt and gam qr and gwr were targeted by a few publications with specific application to south africa and usa respectively globally it was found that brt bm maxent and garp were used more broadly while enfa qr and gwr showed a more restricted use in terms of article number and geographic range see supplementary fig 1 4 discussion in this section we discuss the characteristics and potential application of the different algorithms found in the literature review starting from the most cited ones we also provide examples of the use of each algorithm in sdms applied to iaps we conclude with the suggestion of several guidelines for the application of sdms to the study of iaps 4 1 entropy maximization maxent maximum entropy models are density estimation methods used in a range of natural language processing tasks berger et al 1996 rosenfeld 1996 pietra et al 1997 malouf 2002 manning and klein 2003 ratnaparkhi 2010 convertino et al 2014 and in many other applications such as in habitat and species distribution models phillips et al 2004 2006 dudík et al 2007 elith et al 2006 2011 hegel et al 2010 elith et al 2006 entropy maximization has been increasingly used as in ecology phillips et al 2004 2006 harte et al 2008 cunze and tackenberg 2015 as an approach for modelling species distributions developed in association with a software called maxent phillips and dudík 2008 hegel et al 2010 maxent models are based on the density estimation principle advocated by jaynes 1957 which consists of selecting the distribution that is the closest to the uniform distribution across the study area hernandez et al 2006 however since the derived probability distribution is dependent on the observed data sample bias can add error to the resulting predictions phillips et al 2006 hegel et al 2010 the model assumes that used locations are compared to a sample of available locations across the landscape pearce and boyce 2006 hegel et al 2010 therefore presence only data can be used in the analysis rupprech et al 2011 thus maxent is a machine learning process employing a statistical mechanism that creates a probability distribution across the entire study area with each location receiving a non negative value such that the sum of values for all locations in the study area equals one hegel et al 2010 rebelo et al 2010 while this technique can be seen as a general purpose method for making predictions from incomplete information phillips et al 2006 phillips and dudík 2008 elith et al 2011 the use of this algorithm to model species distributions from presence only data has been shown to perform well in comparison with other algorithms elith et al 2006 hernandez et al 2006 the invasiveness of acacia paradoxa dc fabaceae in south africa was evaluated by zenni et al 2009 for bioclimatic modelling they used maxent to estimate the climatic niche of a paradoxa in australia and to project its potential range in south africa phillips et al 2006 zenni et al 2009 2014 lemke et al 2011 implemented a simulation method to determine the future spread of japanese honeysuckle lonicera japonica thunb caprifoliaceae a forest invasive plant in the south eastern united states maxent runs were separately validated and tested to check modelling quality and species distribution forecasting based on presence absence or presence only data knowledge of the probable areas that would become under the invasion of bushmint hyptis suaveolens l poit lamiaceae was viewed as immensely important for rapid response and mitigation thus padalia et al 2014 modelled india s possible bushmint intrusion array by exploring maxent s predictive capability wedelia trilobata l a hitchc asteraceae an ornamental ground cover plant introduced from central america to areas around the world has become invasive in many regions to predict its spread in central america qin et al 2015 used maxent to create models based on occurrence records from the full distribution range or from the native range only roger et al 2015 aimed to predict the influence of climate change on the potential distribution of naturalized alien plant species they developed an australia wide screening tool and used maxent to build models of abiotically suitable habitat for 292 alien plant species in australia qin et al 2018 used the maxent cycle intrusion method to study spatiotemporal variation in amaranthus retroflexus l amaranthaceae distribution in china arogoundade et al 2019 analysed the habitats susceptible to parthenium hysterophorus l asteraceae invasion in kwazulu natal using environmental data and sentinel 2 multi spectral imagery as input to maxent models 4 2 generalized linear models the use of contemporary approaches to regression has provided a helpful tool for modelling the spatial distribution of species and communities guisan and zimmermann 2000 scott et al 2002 logistic regression is a relatively straightforward technique to regress a binary response variable presence absence against climatic variables jeschke and strayer 2008 this is a particular case of generalized linear models an important statistical development of the last 30 years glm mccullagh and nelder 1989 glms are mathematical extensions of linear models based on an assumed relationship link function between the mean response variable and the linear combination of the explanatory variables guisan et al 2002 indeed this sort of connection does not force information into unnatural scales and allows non linearity and heteroscedasticity in data hastie and tibshirani 1990 moreover data from several families of probability distributions can be handled such as normal binomial poisson negative binomial or gamma distributions hence glms are viewed as parametric models guisan and zimmermann 2000 guisan et al 2002 model parameters or coefficients are estimated via maximum likelihood and specify the change in the response following a one unit change in predictor hegel et al 2010 clearly selecting predictors and their possible transformations e g polynomial terms is the most significant and difficult step when fitting a glm guisan and zimmermann 2000 engler et al 2004 since a sum of weighted ecogeographical predictors explains the dependent variable presence absence hirzel et al 2001 the weights are tuned to generate the best fit between the model and the calibration data set nicholls 1989 jongman et al 1995 the application of regression tools for species distribution modelling assumes the existence of a pseudo equilibrium between the organism and the environment guisan and theurillat 2000 consequently the use of such methods to classify environmental factors responsible for the distribution of species for example that tend to extend their range in the study area will lead to biased results such as truncated ecological response curves hirzel et al 2001 guisan et al 2002 nevertheless logistic regression has been commonly used to forecast the distribution of species for reserve layout environmental impact assessment and the understanding of the factors influencing species distribution nicholls 1989 austin et al 1990 pereira and itami 1991 brown 1994 leathwick 1995 glms were used in early analyses of presence absence and count data often with simple additive combinations of linear terms elith and leathwick 2009 as the common occurrence of nonlinear species responses to the environment was recognized more studies included quadratic cubic or other parametric transforms austin et al 1990 elith and leathwick 2009 gams see section 4 5 for more details hastie and tibshirani 1990 are like glms but use data defined scatter plot smoothers to describe nonlinear responses elith and leathwick 2009 consequently gams have been recommended for data exploration because these models allow the data to determine the shape of the response and enable the detection of skewed or bimodal responses yee and mitchell 1991 both for vegetation mapping and conservation purposes numerical regression models such as glms austin et al 1990 and gams yee and mitchell 1991 can be used to estimate species distributions austin et al 2006 kleinbauer et al 2010 used niche based predictive modelling glm and gam to determine the degree to which the austrian natura 2000 network and a variety of other habitats with conservation interest could be vulnerable to warming leading to an increased risk of invasion by robinia pseudoacacia l fabaceae one of the most problematic iaps in europe smolik et al 2010 combined glm and gam to model the spread of iaps in austria including ambrosia artemisiifolia l asteraceae le roux et al 2013 used glms to demonstrate the implications of human activities in resource availability for 17 alien plants in prince edward islands more recently to select predictors maximizing the transferability of sdms applied to 50 holarctic iaps in north america eurasia and australia petitpierre et al 2017 combined three frequently used algorithms including glms shiferaw et al 2019 studied the performance of machine learning algorithms when mapping the cover of the iap prosopis juliflora sw dc fabaceae in a dryland ecosystem in ethiopia and found that glms did not perform as well as other algorithms 4 3 boosted regression trees boosted regression trees brt also called generalized boosted models gbm are a combination of a statistical and a machine learning algorithm developed by friedman et al 2000 it is one of several methods enhancing model efficiency by fitting and combining several models to improve forecast quality freund et al 1999 schapire 2003 one factor to highlight is its ability to deal with complex responses including nonlinearities and interactions elith et al 2008 brt combines two algorithms the regression from the cart group of models and boosting which builds and combines a collection of models breiman 2001 aertsen et al 2010 the regression trees are sequentially matched and a gradient descent algorithm is used to iteratively model the residuals that reflect the inadequacy of the prior collection of trees friedman 2001 elith et al 2008 graham et al 2008 regression trees are used because they are good at selecting significant variables while accommodating interactions wearne et al 2013 boosting is used to overcome the inaccuracies inherent to a single tree elith et al 2006 brt is one of methods that uses the previous model in the ensemble to minimize error thus boosting accuracy popp et al 2007 craig and huettmann 2008 hegel et al 2010 commonly the cart algorithm is used in brt hegel et al 2010 buston and elith 2011 by assigning a weight to each model based on the respective classification error cushman and huettmann 2010 at each iteration weights are increased on the incorrectly classified classes to focus the algorithm on these cases de ath and fabricius 2000 cushman and huettmann 2010 catford et al 2011 used brt to model the cover of alien plants in south east australia as a function of the environmental conditions using spatially referenced data coutts et al 2011 used brt to determine the most important drivers of spread for several iaps in australia geerts et al 2013 used brt for species distribution modelling including an assessment of invasiveness and options for the management of genista monspessulana l l a s johnson fabaceae and spartium junceum l fabaceae in south africa uddin et al 2013 examined the diversity traits and biogeography of 31 alien species in bangladesh s north eastern region and used brt to assess the effect of different environmental predictors protective regimes and disturbance factors in their distribution fan et al 2018 assessed the transferability of models based on environmental predictors in the study of the iap flaveria bidentis l kuntze asteraceae in china variables were categorized using the hierarchical framework available in brt 4 4 random forest random forest rf is an ensemble of unpruned classification or regression trees generated using bootstrap information samples and random selection of tree induction features breiman 2001 svetnik et al 2003 prasad et al 2006 breiman 2001 suggested the random forest approach by adding to bagging an extra layer of randomness successive trees do not rely on previous trees in bagging each is built separately by using dataset bootstrap sampling and by taking a straightforward majority vote as a forecast liaw and wiener 2002 the rf algorithm generates various cart like trees breiman et al 1984 each trained on a bootstrapped sample of the original training data and searches for a split at each node only through a randomly chosen subset of the input variables gislason et al 2006 the use of rf in sdms has proven robust and stable because it can handle abundance data as well as presence absence data hegel et al 2010 prasad et al 2006 working with pinus taeda l pinaceae acer saccharum marshall sapindaceae fagus grandifolia ehrh fagaceae and quercus alba l fagaceae in the eastern usa and rehfeldt et al 2006 working with 25 biotic communities of the western usa demonstrated the utility of rf in the large scale prediction of tree species distribution rf has been used to model the distribution of iaps such as euphorbia esula l euphorbiaceae and centaurea maculosa lam asteraceae in the usa lawrence et al 2006 of pinus sylvestris l pinaceae in the iberian peninsula gárzon et al 2006 and of larix spp pinaceae in northeastern china prasad et al 2006 leng et al 2008 using rf kleinbauer et al 2010 found that the area potentially invaded by r pseudacacia will increase considerably under a warmer climate in austria to recognize areas of high conservation value in georgia thalmann et al 2015 used the ensemble forecasting approach implemented in the biomod package including rf they aimed to identify the most prominent iaps and to predict their distribution under current and future climatic conditions chapman et al 2019 followed a novel modelling strategy including rf and other algorithms to predict the potential range expansion of iaps such as humulus scandens lour merr cannabaceae lygodium japonicum thunb sw lygodiaceae lespedeza cuneata fabaceae triadica sebifera small euphorbiaceae and cinnamomum camphora l j presl lauraceae at a global level dutra silva et al 2019 modelled the distribution of native morella faya aiton wilbur myricaceae and alien trees acacia melanoxylon r br fabaceae and pittosporum undulatum vent pittosporaceae in the azores under current and future climatic conditions rf models showed the highest values for true skill statistic tss and area under the curve auc 4 5 generalized additive models models including quadratic cubic or other parametric transforms emerged with the common occurrence of nonlinear species responses to the predictors austin et al 2006 generalized additive models gam hastie and tibshirani 1990 have been commonly used to implement non parametric smoothers in regression models yee and mitchell 1991 brown 1994 austin and meyers 1996 bio et al 1998 franklin 1998 lehman 1998 leathwick and austin 2001 glms are somewhat restrictive in their assumption of a linear relationship between response and predictors while gams establish a correlation between the mean of the response variable and a smoothed function of the predictor denoël and lehmann 2006 hegel et al 2010 this technique applies cubic or thin plate regression splines and do not assume a linear relationship hegel et al 2010 each model uses data defined scatter plot smoothers to describe nonlinear responses elith and leathwick 2009 gams have been extensively used in different applications in ecology and in model fitting see yee and mitchell 1991 they provide useful additional flexibility for fitting ecologically realistic relationships in sdms elith and leathwick 2009 for adjustment procedures the analyst s main decision is how smooth the relationship between result and predictor should be hegel et al 2010 the smoothest relationship is a simple straight line df 1 but as the response tracks the values of the predictors more closely the function becomes less smooth and the degrees of freedom rise yee and mitchell 1991 hegel et al 2010 nonetheless gams are considered as more capable of modelling complex ecological response shapes than glms and allow a better data fit although with less transparency and interpretability guisan et al 2002 wintle et al 2005 jeschke and strayer 2008 however with gaussian responses and direct gradient predictors both glms and gams perform well austin et al 2006 robertson et al 2004 predicted the potential distribution of three iaps lantana camara l verbenaceae ricinus communis l euphrobiaceae and solanum mauritianum scop solanaceae in south africa lesotho and swaziland their results suggested that in cases where pseudo absence records can be generated with confidence glm or gam may perform better than fuzzy envelope models the distribution of two widespread iaps genista monspessulana l l a s johnson and spartium junceum l fabaceae was modelled at various spatial scales in south africa with different algorithms e g gam to measure invasiveness and evaluate management options geerts et al 2013 bazzichetto et al 2018 predicted that carpobrotus sp aizoaceae would occur on the coastal dune network of the tyrrhenian using gam 4 6 bayesian models advances in computational statistics have generated new inference and prediction tools over the last decades clark 2005 the bayesian statistical approach is an important tool that is being increasingly used in ecological research and in species distribution modelling ellison 2004 clark 2005 latimer et al 2006 royle and dorazio 2008 aguilera et al 2010 kéry 2010 halstead et al 2012 gelman et al 2014 bensadoun et al 2016 in bayesian analysis emphasis is put on the distinct components and ecosystem processes leading to hierarchical models that incorporate the variance components originating from the various data levels gelman 2006 royle and dorazio 2008 tenan et al 2014 the application of bayesian inference based on bayes theorem bayes 1763 represents a different school of statistical thinking which provides an alternative way to analyse data and statistical philosophy differing from the standard frequentist statistics that most scientists are taught jeffreys 1961 berger 1985 lee 1989 press 1989 howson and urbach 1991 1993 not all ecologists appreciate the philosophy of bayesian inference ellison 2004 particularly bayesians and frequentists differ in the definition of probability and in the treatment of model parameters as random variables or as true value estimates ellison 2004 bayesian methods calculate the probability of the value of a parameter given the observed data wade 2000 in contrast frequentist statistical analysis calculates the probability of observing the data or more extreme values if the null hypothesis is true lindley 1986 wade 2000 bayesian models combine a priori probabilities of observing species or communities with their probabilities of occurrence conditional to the value of each predictor guisan and zimmermann 2000 ellison 2004 conditional probabilities can be the relative frequencies of species occurrence within discrete classes of a nominal predictor guisan and zimmermann 2000 bayesian techniques are described by posterior model probabilities and provide a way of selecting or averaging models where more complex models are penalized as in maximum likelihood model selection bolker et al 2009 modelling using bayesian inference can increase the precision of model parameter estimates once it allies previous knowledge a prior with newly collected data the likelihood to produce a posterior distribution morris et al 2015 increased accuracy of estimates from the use of informative priors is well established and improvement has been proved in ecological contexts e g mccarthy and masters 2005 mccarthy et al 2008 morris et al 2013 2015 marcot et al 2019 indeed the increase in precision is an inherent feature of using informative priors morris et al 2015 one of the complexities in bayesian data processing relies in the estimation of the model parameters posterior distribution hegel et al 2010 markov chain monte carlo mcmc techniques can however be used to estimate the final distribution of a species under scenarios of environmental change besag 1986 heikkinen and hogmander 1994 besag and kooperberg 1995 cook et al 2007 improvement of these methods has also made it possible to estimate the posterior distributions of bayesian models from flat prior distributions hegel et al 2010 a flexible approach is the use of mcmc methods with gibbs sampling to estimate the posterior distributions of model parameters casella and george 1992 gibbs sampling generates samples from the joint probability of two or more variables which is useful when the joint distribution is not explicitly known hegel et al 2010 also to sample the posterior distribution along a mcmc the specific algorithm is adapted to each situation lunn et al 2000 one of the challenges to the enhancement of sdm efficiency is to consider spatial and temporal correlations and how spatial inference about species distribution could be affected eidsvik et al 2012 however the increasing availability of geocoded scientific data and the emergence of hierarchical spatial models e g bayesian implementation have led to the acceleration of research in this area and to an increased attention from many ecologists beguin et al 2012 martins et al 2013 namely the use of gaussian random fields has recently obtained significant attention as it is ideally suited for fitting complex models based on the numerous spatial data sets that are presently available blangiardo et al 2013 illian et al 2013 the integrated nested laplace approximation inla rue et al 2009 was designed for latent gaussian models a very wide and flexible class of models including generalized linear mixed models e g spatial and spatiotemporal models that can be used in a wide variety of applications martino et al 2011 riebler et al 2012 blangiardo et al 2013 inla makes use of deterministic nested laplace approximations as an algorithm tailored to the class of latent gaussian models lgms tierney and kadane 1986 rue et al 2009 eidsvik et al 2012 in fitting these models inla calculates precise deterministic approximations to subsequent marginal distributions instead of lengthy mcmc simulations beguin et al 2012 although the requirement of specialized mathematics and programming might be apparent disadvantages bayesian models are being used more often besides bringing sophisticated model fitting abilities that can incorporate process based information latimer et al 2006 hooten et al 2007 bayesian models are able to incorporate data from multiple data sources and handle the uncertainty associated with underreporting measurement or classification error location error and incomplete datasets also being able to evaluate the impacts of different scenarios on predictions barber et al 2006 wilson et al 2008 chakraborty et al 2010 liedloff and smith 2010 powers et al 2010 herron et al 2007 employed bayesian hierarchical models to explore whether there were multiple discrete sets of invasive traits that would confer invasive success to 248 alien woody plants in a single region new england usa dawson et al 2011 followed a bayesian approach by using generalized linear mixed models with the r package mcmcglmm to study the invasiveness of 105 plant species in the united kingdom hadfield 2010 based on an extensive database of vegetation plots gonzález moreno et al 2014 used hierarchical bayesian models to measure the context dependent landscape scale correlation of weather and human activity with the rate of local plant invasion at various stages of invasion hui et al 2011 also used bayesian methods to monitor the abundance and spatial structure of 29 iaps in south africa demonstrating their importance in the design and assessment of long term management strategies link et al 2018 performed an observational study on the proliferation of seedlings in woodland plots usa invaded or not by berberis thunbergii dc berberidaceae and used survey data to calculate bayesian models the findings indicated that invaded plots harboured significantly lower native tree seedling densities with potentially significant long term forest integrity consequences to estimate the potential distribution of pittosporum undulatum vent pittosporaceae and morella faya aiton wilbur myricaceae dutra silva et al 2017a used geo referenced from pico and são miguel islands azores and topographic climatic and land use predictors fixed effects models run with maximum likelihood or the inla approach provided very similar results even when reducing the size of the dataset the addition of a gaussian random field to model spatial autocorrelation increased model adjustment lower deviance information criterion particularly for the less abundant tree m faya moreover the random field parameters were clearly affected by sample size and species distribution patterns 4 7 multivariate adaptive regression splines multivariate adaptive regression splines mars are a form of regression analysis introduced by friedman friedman 1991 sekulic and kowalski 1992 friedman and roosen 1995 put et al 2004 de cos juez et al 2009 garcía nieto et al 2012 álvarez antón et al 2013 it is a flexible non parametric technique that links explanatory and response variables in nonlinear relationships with similar levels of complexity to that of a gam hastie 1991 rigby and stasinopoulos 2005 the mars procedure builds models by fitting basis functions to distinct intervals of the independent variables jin et al 2001 generally piecewise polynomials also known as splines have smooth connections between parts schumaker 2007 garcía nieto et al 2012 this approach differs from classical splines where the knots joining points of polynomials are predetermined and spaced uniformly de andrés et al 2011 these will be denoted by the location and number of required knots in a forward backward stepwise fashion abraham and steinberg 2001 in other words a nonlinear mars feature comprises a sequence of linked straight line segments rather than the smooth curve of a gam and in several cases it has been shown that it is much faster than gam to model ecological information moisen and frescino 2002 leathwick et al 2005 a priori mars models are not designed for presence absence responses and the method has been used following indications from friedman 1991 and as implemented by leathwick et al 2005 the basic functions are extracted and fit as predictors in glm mccullagh and nelder 1989 using a binomial distribution of errors leathwick et al 2005 mars have been chosen for being a general additive boosting model with linear spline function which has proved very effective in parsimonious model selection and in modelling interactions with a performance comparable to that of neural networks muñoz and felicísimo 2004 mateo et al 2010 grenouillet et al 2011 as with gam mars is a non linear method that often better fits ecological reality austin 1999 2002 2007 stohlgren et al 2010 tested individual models including mars and ensemble models for selected alien species linaria dalmatica mill plantaginaceae carduus nutans l asteraceae bromus tectorum l poaceae melilotus officinalis pall fabaceae among other invasive species in yellowstone and grand teton national parks wyoming sequoia and kings canyon national parks california and areas of interior alaska vicente et al 2011 used mars for testing where there were conflicts between alien and rare species ruscus aculeatus l asparagaceae acacia dealbata link fabaceae and to predict their occurrence after climate and land use change in portugal in another study chen et al 2015 used mars to assess the relative importance of biogeographical and trait variables in the naturalization stage of 150 invasive and 87 non invasive herbs in china 4 8 genetic algorithm for rule set production genetic algorithms ga represent one iterative optimization strategy in the field of artificial intelligence based on evolutionary theory and inspired by genetic and evolutionary models holland 1975 they have been applied to different issues that are not suitable for traditional computational methods as the search for all possible alternatives is too large and varied to be searched for in an acceptable period of time stockwell and noble 1992 one of several current approaches to modelling species distributions is the genetic algorithm for rule set production garp stockwell and peterson 2002 garp is an iterative approach based on artificial intelligence involving several inferential tools mau crimmins et al 2006 that is a superset of modelling algorithms that allow the prediction of environmental niches through the identification of non random relationships between the environmental characteristics at presence locations as compared to the whole study region grinnell 1917 stockwell and noble 1992 stockwell 1999 mau crimmins et al 2006 hegel et al 2010 because the algorithm is stochastic it generates a random set of mathematical rules following an interactive process of rule selection by test incorporation and rejection peterson et al 2007 through which each run of the model can yield a unique solution anderson et al 2003 garp reduces the error in the predicted distributions by maximizing both significance and predictive accuracy stockwell 1999 without overfitting or overly specialized rules the algorithm is generally effective padalia et al 2014 peterson and vieglais 2001 and anderson et al 2002 provide general explanations of this type of modelling process an interpretation of the potential distribution and technical details underwood et al 2004 identified key environmental factors that were correlated with the percent cover of alien species in california and developed a predictive model using garp the potential distributions of the alien weeds brassica tournefortii gouan brassicaceae and schismus arabicus nees poaceae were predicted for the biosphere reserve el pinacate y gran desierto de altar in sonora mexico by applying garp to the repository of the geographic information system sánchez flores 2007 zhu et al 2007 used the datasets documenting eupatorium adenophorum spreng asteraceae invaded localities in china over the past 50 years and garp generated environmental variables to predict its distribution with a high degree of accuracy 4 9 classification and regression trees classification and regression trees cart date to morgan and sonquist 1963 and have received more recent attention through breiman et al 1984 it is a non parametric regression technique based on classification trees formed by a collection of rules bedia et al 2011 derived from the values of certain variables in the dataset and optimized following an efficient search process bedia et al 2011 loh 2014 cart is a powerful approach that can deal with mixed data types and do not violate assumptions of parametric statistics showing a very intuitive interpretation hegel et al 2010 fernandes et al 2013 candidate predictors require a detailed preparation but may be of any type numeric binary categorical and the model results are not affected by monotonous transformations and varying measurement scales elith et al 2008 regression trees are insensitive to outliers and provide omitted data in predictor variables by using surrogates breiman et al 1984 cart predicts unknown instances by judging which node they belong to and provides predictions based on other instance categories or response values yu et al 2015 aggregated cart models have been used by andrew and ustin 2009 to model the future distribution of lepidium latifolium l brassicaceae in the usa using separate stream length variables upland range elevation and topographical variables the density and demography of rubus phoenicolasius maxim rosaceae in the usa was investigated by gorchov et al 2011 in tree gaps of different sizes but identical in age to non gap areas cart analyses were used to identify the most important predictors fernandes et al 2013 devised a hierarchical procedure using the kruskal wallis test followed by cart to select the minimum number of optimum spectral bands that discriminated against the vegetation adjacent to arundo donax l poaceae dixon et al 2015 used cart modelling to analyse the efficacy of a single regional model in forecasting the naturalization of alien woody plants in five areas in the upper midwest usa to test the effect of environmental and socioeconomic predictors on species distribution the occurrence of 354 iaps in 45 russian regions was compiled and regression trees were built using cart vinogradova et al 2018 4 10 structural equation modelling structural equation modelling sem is a strong instrument for investigating causal relationship hypotheses among variables using observational data iriondo et al 2003 the use of sem has increased since its introduction in the 1980s by wright 1921 1934 who developed a way to break down the observed correlations into an equations scheme describing the causal relationships hypotheses iriondo et al 2003 later this method was rediscovered and developed by two economists and sociologists jöreskog 1973 1977 1981 and jöreskog and sörbom 1982 they transformed wright s original analysis into a combined factor analysis that was capable of testing rather than describing causal claims shipley 1999 frequently sem is used to confirm or disprove an a priori hypothesized causal model markus 2012 it can also be used to generate models in an exploratory sense kingsolver and schemske 1991 mitchell 1992 1994 sem is often used by its usefulness in researching complicated relationship networks but also by its commitment to represent theoretical concepts grace and pugesek 1997 grace et al 2010 sem assumes that the presence of a correlation between two variables does not necessarily imply the existence of a causal relationship between them or vice versa austin 2007 laliberté and tylianakis 2012 almeida neto et al 2010 used sem to examine the effects of invasive grass cover on native asteraceae richness by controlling specific variables grace and keeley 2006 they factored the total effect of each predictor into a direct component bangert and huntly 2010 tested three general hypotheses for the diversity and species composition of the vegetation of kipukas in the usa using sem to evaluate the ability of each hypothesis to predict the observed distributions of indigenous and non indigenous plants including the invasive grass bromus tectorum l poaceae maltez mouro et al 2010 illustrated the impact of carpobrotus edulis n e br aizoaceae on portuguese sand dune communities structure and composition they applied sem to determine the importance of alien plants and climatic variables as drivers of the sand dune community rijal et al 2017 assessed the relationship between invasibility and genetic diversity of the iap heracleum persicum desf apiaceae in 12 locations in northern norway 4 11 artificial neural networks the concept of artificial neural networks ann relates to a large group of models inspired by biological neural networks particularly the brain which consists of interconnected neuron networks for information processing lek and guégan 1999 olden and jackson 2002 ripley 2007 olden et al 2008 generally it corresponds to a set of simple nonlinear computing components whose inputs are linked together to form a network kuo et al 2001 anns are typically organized in layers with an input layer in which data is fed into the model several hidden layers and the output layer which represents model results lek and guégan 1999 olden and jackson 2002 olden et al 2008 franklin 2010 learning algorithms within anns are highly variable and include evolutionary models simulated annealing and nonparametric models hegel et al 2010 anns are a powerful tool for processing any kind of data and are used in ecological modelling with mathematical and software progress some ecological applications have already used ann for sdms and other purposes özesmi and özesmi 1999 yen et al 2004 parker allie et al 2009 addressed the impact of climate warming on the distribution of 29 invasive annual eurasian grasses using a modelling system that included several algorithms including ann they found that climate change would hamper the spread of european annual grasses in southern africa pěknicová and berchová bímovák 2016 have found that ann appeared most suitable to predict ias spread fallopia spp polygonaceae solidago spp and heracleum mantegazzianum sommier levier apiaceae in the kokorínsko protected landscape area czech republic according to shrestha and shrestha 2019 climate change amplifies plant invasion hotspots in nepal they used an ensemble of species distribution models e g ann and conducted hotspot analysis to highlight the distribution of 24 iaps in different climatic zones land cover categories ecoregions physiography types and federal states césar de sá et al 2019 modelled the potential distribution of acacia spp fabaceae in portugal using an ensemble approach biomod2 all sdms obtained very high accuracy including ann with the highest values being obtained in the models trained with researchers data data obtained from citizens alone could potentially overestimate species distributions 4 12 classification tree analysis classification tree analysis cta is a non linear statistical classification methodology that allows the study and analysis of the relationship between environmental factors and species distributions breiman et al 1984 providing an alternative to regression techniques e g vayssiéres et al 2000 thuiller et al 2003a they do not rely on a priori hypotheses about the relationship between independent and dependent variables thuiller et al 2003b according to a straightforward rule based on a single explanatory variable the tree is constructed by dividing the calibration information constantly the information being divided into two exclusive groups at each split as homogeneously as possible breiman et al 1984 thuiller et al 2003a the heterogeneity of a node is defined with a deviance notion that can be interpreted as the deviance of a multinomial model breiman et al 1984 to control the length of the tree a prune function is used venables and ripley 2002 the algorithm builds a nested series of subtrees by repeatedly snipping away the less significant divisions in terms of explained deviance thuiller et al 2003a cta is one of the few methods that can model interactive effects of two or more variables and can represent them in an easily readable fashion iverson and prasad 1998 rouget et al 2001 this modelling approach have received increased interest for biogeographical studies franklin 1998 iverson and prasad 1998 vayssières et al 2000 rouget et al 2001 as compared to other modelling techniques such as glm and gam thuiller et al 2003a to construct a bayesian model murray et al 2012 used a cta of the field data to predict the potential distribution of a riparian invasive plant phyla canescens kunth greene verbenaceae in australia another study tested the effect of the addition of variable sets to an sdm for heracleum mantegazzianum s l apiaceae in poland mędrzycki et al 2017 however the cta derived model quality was low with single models showing a higher level of variation in true skill statistic than ensemble models 4 13 ecological niche factor analysis perrin 1984 was the first to suggest the term ecological niche factor analysis enfa later developed by hausser 1995 and more recently implemented in the software biomapper hirzel et al 2000 2002 the enfa is a method based on a comparison between the environmental niche of the species and the environmental characteristics of the entire study area hirzel et al 2002 engler et al 2004 like the environmental envelope approach it presents the advantage of requiring presence data only i e no absences are required and a set of background gis predictors engler et al 2004 shirley et al 2013 enfa is like a kind of principal component analysis pca and summarizes all ecogeographical predictors into a few uncorrelated factors retaining most of the information cassinello et al 2004 2006 hengl et al 2009 moreover the extracted factors have an ecological meaning the first extracted factor provides the marginality coefficient describing the standardized difference between the average conditions at species sites and at the entire study area marginality ranges from 1 to 1 and indicates the rarity of the conditions selected by the target species within the study area chefaoui et al 2005 hortal et al 2005 acevedo et al 2007 positive values show a species optimum to be higher than the average conditions in the study area the second factor specialization varying from 0 to compares the variance of the environment in areas where the species is present with the global variance in the study area cassinello et al 2006 successive factors explain the remaining specialization in decreasing amounts a high value of specialization indicates a narrow niche breadth in comparison with the available conditions hirzel et al 2002 2004 2006 these extracted factors are used to compute a habitat suitability index for any set of predictor values hirzel et al 2002 enfa calculates a measure of habitat suitability based on the analyses of marginality how the species mean differs from the global mean and environmental tolerance how the species variance compares to the global variance hirzel et al 2000 2001 a habitat suitability map with values ranging from 0 to 100 is built comparing the position of each cell in the study area to the distribution of presence cells on the different factorial axes hirzel et al 2002 2007 hirzel et al 2001 compared two habitat suitability assessing methods enfa and glm to see how well they coped with three different scenarios the enfa method was preferred but the exact procedure used for the glm is difficult to determine austin et al 2006 hortal et al 2010 identified the areas under potential threat from the future spread of pittosporum undulatum vent in são miguel island azores using enfa to investigate the impact of geographical bias on the performance of ecological niche models for iaps wolmarans et al 2010 used enfa to compare the ecological niche occupied by 19 iaps in their native and introduced ranges in south africa and australia steiner et al 2008 costa et al 2012 used enfa to determine whether and where areas currently occupied by p undulatum might also be valuable habitat for m faya thus promoting potential management measures priyanka and joshi 2013 modelled the spatial distribution of l camara in india but of the statistical methods used enfa produced an odd pattern of under predictive models from a global multidimensional thermal niche viewpoint two alien grasses poa annua l and poa pratensis l poaceae were studied to evaluate the threat of biological invasion in antarctica pertierra et al 2017 enfa was performed to estimate the biological relevance of temperature related predictors hirzel et al 2002 dutra silva et al 2017b modelled the distribution of indigenous m faya and invasive woody species a melanoxylon and p undulatum in the azores applying enfa and maxent algorithms which generated similar predictions 4 14 surface range envelope surface range envelope sre is an envelope style technique that takes into consideration the entire range of environmental circumstances in which the species is present busby 1991 the envelope is defined by the minimum and maximum values for each predictor from the set of presences jiguet et al 2011 araújo and peterson 2012 thuiller et al 2012 any location with environmental conditions falling between these minimum and maximum limits is included in the potential range of the target species jiguet et al 2011 thuiller et al 2012 once the bioclimatic envelope of the species is recognized the model can be used to predict its distribution in other areas or climatic scenarios araújo and peterson 2012 this method is influenced by the data input and more specifically by the extremes thuiller et al 2012 to avoid the over predictive effects of outliers the envelope can be reduced at specified percentiles or standard deviations araújo and peterson 2012 thuiller et al 2012 the sre assumes that a linear model can distinguish extreme values from the primary trend as a result of which the predictions are provided directly in binary notation a site being classified as possibly suitable for all the variables araújo and peterson 2012 thuiller et al 2012 booth et al 2014 however treating all presence points as equal and trying to not discriminate the extreme values has been criticized by beaumont et al 2005 le maitre et al 2008 developed a method for describing the possible distribution of hakea proteaceae species in south africa wit the sre model which simply used the upper and lower limits of each of the weather variables in the presence records to establish a multidimensional framework melampyrum pratense l orobanchaceae was investigated in iceland to decide whether the species was a recent anthropogenic addition to the icelandic flora or whether its existence should be due to natural long distance dispersal activity wasowicz et al 2018 pseudo absences were randomly selected inside the geographical range of the species using the sre method le maitre et al 2008 barbet massin et al 2012 4 15 quantile regression quantile regression qr gradually emerged as an extensive strategy for the statistical analysis of linear and nonlinear response models in econometrics yu and moyeed 2001 however this technique has not received much attention in applications to sdms koenker 2005 austin 2007 hegel et al 2010 qr provides solid estimates of the impact of the predictors in cases where various limiting factors may interact some possibly unmeasured cade et al 1999 it assumes relationships between observations and predictors in a specific way over different ranges i e quantiles for predictors cade and noon 2003 cade et al 2005 bartomeus et al 2010 2012 hegel et al 2010 these semi parametric models estimate unique parameters to a response across different quantiles of data hegel et al 2010 having specific fits for specific data sections can be equated with other approaches such as mars muñoz and felicísimo 2004 hegel et al 2010 vaz et al 2008 used qr to calculate the upper boundaries of organism environment relationships and to identify those factors that limited species distribution hegel et al 2010 cade et al 1999 introduced the use of qr to estimate the curves of the envelope or factor ceiling reactions thomson et al 1996 in 2003 cade and noon provided an exposition of this statistical method and of its potential in the ecological analysis of observation data both for plants and animals austin 2007 qr can be useful in the study of iaps as most still have the potential to expand their distribution range guisan and thuiller 2005 ricotta et al 2010 analysed the relationship between indigenous and non indigenous plants in brussels with qr koenker and bassett 1978 koenker and hallock 2001 zedda et al 2010 studied the impact of alien plants and human disturbance on soil growing bryophyte and lichen diversity in coastal areas of sardinia italy qr was selected to test the comparisons between the cover of carpobrotus spp and of alien trees versus the cover and species diversity of bryophytes and lichens found in the parcels petty et al 2012 used a novel spatial autocorrelation qr technique to compare factors that influenced andropogon gayanus kunth poaceae abundance and distribution at two large scale invasion sites in australia s tropical savanna region 4 16 generalized dissimilarity modelling biodiversity spatial structure can be defined and explained by the evaluation of species composition turnover using beta diversity modelling approaches warren et al 2014 numerous studies have investigated the differences between the biotic assemblages found at different locations to develop predictive models for application in biodiversity and macroecology ferrier et al 2007 anderson et al 2011 initially generalized dissimilarity modelling gdm was developed to study turnover in species diversity in community ecology ferrier et al 2002 2007 hegel et al 2010 compositional turnover can be partitioned into components explained by environment location or both legendre et al 2005 gdm follows a generalized linear approach using an exponential link function to model the compositional dissimilarity between pairs of biological studies based on the bray curtis index as a function of environmental predictors ferrier and guisan 2006 ferrier et al 2007 overton et al 2009 it uses a matrix regression approach and models non linear relationships by fitting a linear combination of spline basis functions to the environmental variables in a linear predictor hegel et al 2010 elith et al 2006 extended this approach beyond modelling species diversity to modelling species distribution for this a kernel regression is used on the output of the transformed environmental variables from the gdm to predict the likelihood of species occurrence hegel et al 2010 data can be handled by using presence pseudo absence however this should be considered prudently ferrier et al 2007 jones et al 2013 used gdm to model the compositional turnover of trees 486 species and ferns 92 species in the drainage of the panama canal because of soil chemistry weather and regional separation vicente et al 2014 used this new framework of dissimilarity to examine and explain spatial dynamics of species diversity associated with iaps in northern portugal assessing compositional dissimilarity patterns between sites or regions using gdm as a tool has been suggested as a promising complementary approach to traditional correlative analysis ferrier et al 2007 vicente et al 2014 4 17 geographically weighted regression in spatial analysis one of the primary purposes is to define the nature of the interactions between distinct variables brundson et al 1996 traditional statistical methods can only produce average and global parameter estimators consequently they are unable to deal with the spatial autocorrelation existing in the variables gao and li 2011 however a comparatively easy method called geographically weighted regression gwr has recently been suggested to explore variable relationships spatially wheeler 2014 the method has been developed by brundson et al 2001 and fotheringham et al 2001 and proposed a refinement to normal regression methods dealing with the spatial non stationarity of empirical relationships fotheringham et al 2003 wang et al 2005 gwr is thus based on the appealing concept of a locally weighted regression that works by estimating local curve fitting models using subsets of focal point centred observations lü and fu 2001 gao and li 2011 this approach is an extension of traditional standard regression techniques such as ols ordinary least squares because it allows local rather than global parameter estimates fotheringham et al 2001 austin 2007 these can help reveal how a relationship varies over space to examine the spatial pattern of the local estimates and to get some understanding of the hidden possible causes of this pattern fotheringham et al 2003 it can help reveal spatial variations in the empirical relationships between variables that would otherwise be ignored in the overall analysis charlton et al 2009 despite some controversy over the relative importance of gwr versus global spatial regression see jetz and rahbek 2002 versus foody 2004 gwr has been widely applied in diverse fields such as ecology forestry geography and regional science wheeler 2014 an example of this approach was the study of urbanization that promoted the spread of 411 alien woody species and the establishment of diverse plant assemblages in the new york metropolitan region aronson et al 2015 gwr was used to detect non stationarity between the proportions of alien species in the flora to the proportion of urban land cover in the landscape latimer et al 2009 also used this methodology to model the distribution of the iap celastrus orbiculatus thunb celastraceae in the north eastern usa 4 18 ensemble modelling given the variability in results arising from the application of different algorithms the complementary use of several methods has been recommended e g ensemble modelling araújo et al 2005 araújo and new 2007 within a consensus modelling framework marmion et al 2009 grenouillet et al 2011 the principle of integrating predictions from various models into an ensemble has gained substantial notoriety hao et al 2019 many users cite the superior predictive performance of ensembles over individual models e g crossman and bass 2008 marmion et al 2009 grenouillet et al 2011 ensemble modelling is a method in which many different models are generated either by using several different modelling algorithms or different training data sets to predict an outcome kotu and deshpande 2014 the reason for using ensemble models is to decrease the forecast generalization error if the base models are diverse and distinct when the ensemble method is used the estimation error of the model decreases kotu and deshpande 2014 grenouillet et al 2011 investigated the utility of approaches to ensemble modelling and tested whether species characteristics affected the precision of those approaches their results illustrate the utility of ensemble models for the detection of geographical areas of consensus between forecasts and they considered that the application of a single modelling technique should be avoided especially for species with wide environmental ranges grenouillet et al 2011 however it has also been argued that individual models might perform better than ensemble models e g crimmins et al 2013 hao et al 2019 reported the breadth of ensemble applications and provided guidelines for comprehensive reporting practices in a biomod based ensemble workflow however they could not draw clear quantitative conclusions about a putative superiority in predictive performance of ensemble models di napoli et al 2020 tested ensembles of artificial neural networks generalized boosting and maximum entropy algorithms and found that ensemble modelling improved reliability originating higher scores and lower variation hao et al 2020 aimed to complement the existing knowledge on performance of ensemble models versus individual models and investigated how spatial blocking affected the understanding of model performance using a presence absence dataset they found that ensemble models performed slightly better than untuned individual models in most situations but not consistently better than tuned individual models on external validation therefore work is still in progress regarding the effective use of ensemble modelling 4 19 overview of modelling approaches as suggested in previous reviews austin et al 2006 austin 2007 our results showed that when modelling the environmental niche or the geographic distribution of iaps currently there is no general criterion for the selection of specific algorithms other aspects such as the ecological knowledge and statistical skills of the analyst might have affected the choices made by researchers more than a decade ago austin 2007 claimed that three potential areas of development included qr sem and gwr and that the methods with more predictive success were brt gdm and maxent followed by mars glm gam and garp however we did not confirm a growing use of qr sem and gwr probably because machine learning techniques are still gaining more attention in the area of sdms although their use is expected to increase in the forthcoming years humphries et al 2018 in the same sense brt and gdm have not become the dominant algorithms used for sdms although brt ranked as third in our analysis our findings showed that the annual number of articles addressing sdms developed for iaps and the number of cited references have both increased considerably from the late 1990 s up to the present this trend can be well explained by the development of computing technologies e g bennett and bierema 2010 michener et al 2012 tarng et al 2015 wang et al 2017 marcot and penman 2019 zellweger et al 2019 personalized for the advancement of modelling and by the diffusion of multiple analysis platforms particularly those based on r packages e g dismo biomod2 inla which are freely distributed with numerous available tutorials and discussion forums simultaneously the interest in research devoted to ias has also consistently increased see van wilgen et al 2012 early et al 2016 ramírez albores et al 2019 shackleton et al 2019 rai and kim 2020 many of the identified studies utilized maxent for model development this has been considered as a good modelling practice which has gained considerable attention however this practice does not fully address other prevailing modelling issues such as the use of pseudo absences elith et al 2011 guillera arroita et al 2014 and the difficulties when comparing output with other algorithms as maxent output provides environmental suitability rather than predicted probability of occurrence merow et al 2013 moreover many authors such as qiao et al 2015 concluded that niche or distribution modelling studies should begin by testing a suite of algorithms for predictive ability under the circumstances of each study case recently fletcher et al 2016 koshkina et al 2017 and schank et al 2017 have suggested the use of point process models as a way of integrating presence only and presence absence data meanwhile researchers in ecology could eventually broaden the scope of the methods applied for statistical modelling and prediction including applications focused in machine learning examples include mars with its ability for handling interactions between predictors moisen and frescino 2002 muñoz and felicísimo 2004 balshi et al 2009 zhang and goh 2013 kisi and parmar 2016 huang et al 2019 cart which is one of the most successful decision tree methods liu et al 2013 dixon et al 2015 chen et al 2017 gayen and pourghasemi 2019 brt that combines two algorithms elith et al 2008 stokland et al 2011 martin et al 2014 pourtaghi et al 2016 wu et al 2019 genetic algorithms like garp stockwell and noble 1992 stockwell 1999 yu and he 2009 sobek swant et al 2012 li et al 2017 moghaddam et al 2020 and gwr brundson et al 2001 fotheringham et al 2001 partridge et al 2008 propastin 2012 tenerelli et al 2016 yang et al 2019 although the use of those algorithms in forecasting the invasion success of iaps has been clearly established pyšek et al 2009 keller et al 2011 research by keller et al 2011 revealed that model quality from machine learning algorithms was not consistently superior to that of conventional statistical methods for a large dataset of up to 87 species nevertheless numerous applications of machine learning methods to ecological data have showed successful results e g dutra silva et al 2017b brodrick et al 2019 pichler et al 2020 therefore more work should be devoted to the comparison of results obtained with different and complementary approaches another finding was the geographical diversity regarding the use of different types of modelling techniques to predict the distribution of iaps in various parts of the world the interest in the study of biological invasions has substantially increased during the first decade of the 21st century and this trend continues today ramírez albores et al 2019 therefore the number of references has increased exponentially over time see ramírez albores et al 2019 shackleton et al 2019 rai and kim 2020 showing that iap studies may still be considered as an emerging area justifying the pronounced diversification of modelling approaches found in our research our results concerning the global distribution of sdm studies might be explained by the geographic occurrence of biological invasions and by the historical interest that this subject has stimulated in some regions thus according to tuberlin et al 2017 ias mainly occur in the global north and in some newly industrialized countries e g china india brazil another stusy revealed the engagement of stakeholders in the study and management of ias particularly in south africa usa australia canada india spain and the uk see shackleton et al 2019 these regions largely coincide with the areas that showed more diversified approaches to sdms applied to iaps in our research 5 conclusion our study presented an overview of the conceptual framework supporting the application of sdms a structured literature search combined with bibliometric analysis as well as the characteristics of the main modelling techniques and examples of their application to the study of iaps in the last decades the interest in sdms has increased dramatically as reflected by the number of publications this can be explained by the increased availability of software packages enabling modellers to create and train sdms with more extensive applications across different regions of the world considering the diverse array of modelling algorithms that have been used along the studied period this overview led to a crucial conclusion it is clear from the articles quoted that when modelling the environmental niche or geographic distribution of iaps presently there is no criterion for best practice in the selection of the modelling algorithms some general guidelines for improving predictions of sdms should include i the use of different algorithms to see if all perform consistently or if certain approaches provide more reliable results for instance maxent has been extensively used in many cases without a comparison with another modelling approach but has been shown to perform equally or below other algorithms in several cases ii the need to compare the results obtained using individual and ensemble models since this is an area still in development and involving a considerable uncertainty there are inconsistent reports favouring one approach or the other iii the use of several complementary parameters to evaluate model fit and prediction quality including tss auc boyce index and deviance measures such as aic waic or dic when possible it is common to get relatively high auc values even when tss is relatively small or when the analysis of the boyce index and curve suggests low quality modelling iv the use of different combinations of predictor variables since using methods such as variation inflation factor vif alone in many cases might preclude finding the most suitable model for instance using a combination of vif and pca to select predictors might provide more reliable models and the use ecologically and biophysically meaningful predictors v spatial autocorrelation is probably present in many systems and together with variation in sample size and with the specific distribution pattern of the species might have a decisive influence on model performance indeed the inclusion of a random gaussian field in mixed generalized linear models has showed the importance of autocorrelation and distributions patterns are often dependent on predictors such has land cover which largely results from human action and not directly from biophysical predictors vi the implementation of recent approaches with a very high potential in the developmental of geospatial models algorithms such as bayesian models calculated using inla have shown to be very powerful in modelling varied geospatial phenomena but are still seldom used in sdms most likely due to their complex implementation meanwhile sdms are important tools in applied ecology and in the recent years have revealed a tremendous progress in many aspects and applications with the rich diversity of biological and environmental settings philosophical and analytical approaches and research initiatives addressing many aspects and applications nevertheless improvement requires constant evaluation and future efforts should be focused on consolidating modelling frameworks the comparison of different modelling approaches is a challenge that should not be ignored by modellers since new methods methodologies and technologies are constantly being implemented in this field finally iaps constitute an important continuously growing and significant part of the global vegetation composition around the world this work highlights the various algorithms available for better understanding the conditions that promote the expansion of iaps and that could help in the design of more successful management strategies declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we are grateful to two anonymous reviewers who provided constructive remarks that allowed to improve the manuscript quality we also appreciated funding from project forest eco2 towards an ecological and economic valorisation of the azorean forest acores 01 0145 feder 000014 azores 2020 po 2016 2019 feder european regional development fund funds through the operational programme for competitiveness factors compete and by national funds through fct foundation for science and technology under the uid bia 50027 2019 and poci 01 0145 feder 006821 drct m1 1 a 005 funcionamento c 2019 cibio a secretaria regional do mar ciência e tecnologia governo dos açores appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105203 
